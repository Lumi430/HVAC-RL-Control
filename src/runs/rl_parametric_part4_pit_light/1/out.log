Using TensorFlow backend.
[2019-04-01 15:38:04,646] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v1', activation='linear', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-v1', eval_act_func='part4_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=100000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=5000000, metric_func='part4_v1', model_dir='None', model_param=[12, 1], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_v1', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=10, test_env=['Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'], test_mode='Multiple', train_act_func='part4_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=40.0, weight_initer='glorot_uniform', window_len=20)
[2019-04-01 15:38:04,646] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-01 15:38:04.678055: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-01 15:38:19,460] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-01 15:38:19,461] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-v1', 'Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'] ...
[2019-04-01 15:38:19,490] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation worker starts!
[2019-04-01 15:38:19,505] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation worker starts!
[2019-04-01 15:38:19,520] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation worker starts!
[2019-04-01 15:38:19,520] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:19,520] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-01 15:38:19,594] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:19,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run1
[2019-04-01 15:38:20,521] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:20,524] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-01 15:38:20,610] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:20,611] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run1
[2019-04-01 15:38:21,525] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:21,526] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-01 15:38:21,603] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:21,604] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run1
[2019-04-01 15:38:22,527] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:22,528] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-01 15:38:22,615] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:22,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run1
[2019-04-01 15:38:23,514] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-01 15:38:23,514] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 15:38:23,515] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 15:38:23,515] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:23,515] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:23,515] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 15:38:23,516] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:23,518] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run1
[2019-04-01 15:38:23,525] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run1
[2019-04-01 15:38:23,532] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:23,538] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-04-01 15:38:23,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run1
[2019-04-01 15:38:23,674] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:23,676] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run1
[2019-04-01 15:38:24,539] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:24,540] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-01 15:38:24,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:24,610] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run1
[2019-04-01 15:38:25,541] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:25,542] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-01 15:38:25,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:25,671] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run1
[2019-04-01 15:38:26,543] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:26,543] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-01 15:38:26,726] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:26,728] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run1
[2019-04-01 15:38:27,544] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:27,545] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-01 15:38:27,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:27,710] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run1
[2019-04-01 15:38:28,546] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:28,547] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-01 15:38:28,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:28,687] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run1
[2019-04-01 15:38:29,548] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:29,548] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-01 15:38:29,622] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:29,623] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run1
[2019-04-01 15:38:30,549] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:30,550] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-01 15:38:30,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:30,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run1
[2019-04-01 15:38:31,551] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:31,551] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-01 15:38:31,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:31,647] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run1
[2019-04-01 15:38:32,552] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:32,553] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-01 15:38:32,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:32,672] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run1
[2019-04-01 15:38:33,554] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:33,556] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-01 15:38:33,716] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:33,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run1
[2019-04-01 15:38:34,557] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-01 15:38:34,558] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-01 15:38:34,652] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:38:34,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run1
[2019-04-01 15:38:48,817] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-01 15:38:48,818] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-10.1, 61.66666666666667, 162.3333333333333, 395.8333333333333, 24.0, 24.89125699323811, 6.415504438899482, 0.0, 1.0, 33.69838230052085]
[2019-04-01 15:38:48,818] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 15:38:48,819] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.02522191 0.21544899 0.06742159 0.02671085 0.06157259 0.37424156
 0.22938256], sampled 0.32324314070950066
[2019-04-01 15:40:05,097] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-01 15:40:05,097] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.0, 39.0, 38.5, 328.5, 26.0, 25.20592089766713, 8.902342051899467, 0.0, 1.0, 12.22839971825683]
[2019-04-01 15:40:05,097] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 15:40:05,098] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.03105908 0.23816435 0.07259246 0.02522927 0.0710124  0.28798848
 0.2739539 ], sampled 0.21542816343036186
[2019-04-01 15:40:11,706] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5711.4596 224621.2483 38641.3061
[2019-04-01 15:40:26,320] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-01 15:40:26,320] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-5.333333333333333, 82.0, 132.3333333333333, 419.3333333333334, 22.0, 23.70360955275909, 5.491502649405514, 0.0, 1.0, 26.66907552523093]
[2019-04-01 15:40:26,320] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 15:40:26,321] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.03114979 0.16729425 0.08521769 0.02177712 0.04236839 0.36500934
 0.2871834 ], sampled 0.7098456382969831
[2019-04-01 15:40:31,879] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5492.8609 249336.1916 35870.9688
[2019-04-01 15:40:34,502] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5888.0673 259703.9775 28455.6671
[2019-04-01 15:40:35,525] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 5492.860915343379, 249336.19156681094, 35870.968751545384, 5711.459556567306, 224621.2483171821, 38641.30609391998, 5888.0673370559125, 259703.97754511275, 28455.66711134116]
[2019-04-01 15:40:42,735] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.7568970e-05 9.3697596e-01 2.5666111e-05 2.5775216e-07 3.4030639e-02
 2.9211937e-05 2.8890714e-02], sum to 1.0000
[2019-04-01 15:40:42,735] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9097
[2019-04-01 15:40:42,809] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.4666666666666667, 95.83333333333333, 0.0, 0.0, 19.0, 21.23723899720954, 12.36081361811107, 0.0, 1.0, 6.202780106615739], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 79800.0000, 
sim time next is 80400.0000, 
raw observation next is [0.4333333333333333, 95.66666666666666, 0.0, 0.0, 19.0, 21.17248663346016, 12.73193406117193, 0.0, 1.0, 6.617119950855088], 
processed observation next is [0.0, 0.9565217391304348, 0.4746075715604802, 0.9566666666666666, 0.0, 0.0, 0.0, 0.31035523335145143, 0.12731934061171932, 0.0, 1.0, 0.04726514250610777], 
reward next is 0.9527, 
noisyNet noise sample is [array([-0.04465791], dtype=float32), 0.8684401]. 
=============================================
[2019-04-01 15:40:43,754] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8908400e-06 9.9440247e-01 5.3139735e-07 5.0164841e-09 2.4503295e-03
 5.3310492e-07 3.1412097e-03], sum to 1.0000
[2019-04-01 15:40:43,756] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5139
[2019-04-01 15:40:43,850] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.433333333333333, 88.33333333333334, 0.0, 0.0, 19.0, 19.5058854554183, 25.28741214469665, 0.0, 1.0, 9.370295080044468], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 96000.0000, 
sim time next is 96600.0000, 
raw observation next is [-2.616666666666667, 87.66666666666666, 0.0, 0.0, 19.0, 19.4012061146458, 25.56238968909497, 0.0, 1.0, 9.478278118416625], 
processed observation next is [1.0, 0.08695652173913043, 0.3901200369344414, 0.8766666666666666, 0.0, 0.0, 0.0, 0.05731515923511411, 0.2556238968909497, 0.0, 1.0, 0.06770198656011875], 
reward next is 0.9323, 
noisyNet noise sample is [array([-0.9967446], dtype=float32), -0.1189373]. 
=============================================
[2019-04-01 15:40:44,417] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.09129712e-06 9.97469425e-01 6.35934185e-08 1.75261555e-10
 1.11161196e-03 1.11111206e-07 1.41768507e-03], sum to 1.0000
[2019-04-01 15:40:44,421] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6942
[2019-04-01 15:40:44,509] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.300000000000001, 68.0, 0.0, 0.0, 19.0, 18.96403909580122, 29.692082823947, 1.0, 1.0, 22.33144278886905], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 112800.0000, 
sim time next is 113400.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 19.0, 18.91065468560151, 30.02386003636077, 1.0, 1.0, 21.2629241366828], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.0, -0.012763616342641322, 0.3002386003636077, 1.0, 1.0, 0.15187802954773427], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3206393], dtype=float32), -1.6795601]. 
=============================================
[2019-04-01 15:40:49,807] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.01897506e-07 6.49596681e-04 4.14957313e-09 4.46707948e-11
 9.48561549e-01 1.20517754e-07 5.07886261e-02], sum to 1.0000
[2019-04-01 15:40:49,810] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9846
[2019-04-01 15:40:49,907] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 74.66666666666667, 0.0, 0.0, 26.0, 22.00921974708141, 9.958449087777003, 0.0, 1.0, 44.58431551826342], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 180600.0000, 
sim time next is 181200.0000, 
raw observation next is [-8.900000000000002, 75.33333333333334, 0.0, 0.0, 26.0, 21.96687601866252, 9.83841238186674, 0.0, 1.0, 44.54417157631806], 
processed observation next is [1.0, 0.08695652173913043, 0.2160664819944598, 0.7533333333333334, 0.0, 0.0, 1.0, 0.42383943123750306, 0.0983841238186674, 0.0, 1.0, 0.31817265411655754], 
reward next is 0.6818, 
noisyNet noise sample is [array([1.3292606], dtype=float32), 0.4713506]. 
=============================================
[2019-04-01 15:40:52,749] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.5770423e-10 5.5898930e-04 1.4936019e-11 2.7926354e-15 9.8449326e-01
 6.7170497e-10 1.4947742e-02], sum to 1.0000
[2019-04-01 15:40:52,749] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9299
[2019-04-01 15:40:52,959] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.916666666666667, 65.0, 137.0, 0.0, 26.0, 25.65445087761933, 7.831091446108715, 1.0, 1.0, 44.46542607869796], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 216600.0000, 
sim time next is 217200.0000, 
raw observation next is [-4.833333333333334, 65.0, 133.0, 0.0, 26.0, 25.62232284169099, 7.006867585525427, 1.0, 1.0, 41.1006114327203], 
processed observation next is [1.0, 0.5217391304347826, 0.32871652816251157, 0.65, 0.44333333333333336, 0.0, 1.0, 0.9460461202415702, 0.07006867585525427, 1.0, 1.0, 0.29357579594800215], 
reward next is 0.7064, 
noisyNet noise sample is [array([0.3177791], dtype=float32), -0.23556785]. 
=============================================
[2019-04-01 15:40:58,982] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7674: loss 1.3518
[2019-04-01 15:40:59,029] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7674: learning rate 0.0010
[2019-04-01 15:40:59,084] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7691: loss 1.2834
[2019-04-01 15:40:59,084] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7691: learning rate 0.0010
[2019-04-01 15:40:59,506] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 7784: loss 0.8118
[2019-04-01 15:40:59,507] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 7785: learning rate 0.0010
[2019-04-01 15:40:59,865] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7877: loss 0.4511
[2019-04-01 15:40:59,866] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7877: learning rate 0.0010
[2019-04-01 15:40:59,924] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7889: loss 0.5706
[2019-04-01 15:40:59,925] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7889: learning rate 0.0010
[2019-04-01 15:41:00,304] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7972: loss 0.1962
[2019-04-01 15:41:00,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7972: learning rate 0.0010
[2019-04-01 15:41:00,336] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7978: loss 0.1963
[2019-04-01 15:41:00,336] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7978: learning rate 0.0010
[2019-04-01 15:41:00,403] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0304351e-10 2.9467896e-03 1.4890596e-11 9.7644095e-16 9.8074508e-01
 7.3361384e-10 1.6308093e-02], sum to 1.0000
[2019-04-01 15:41:00,403] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5897
[2019-04-01 15:41:00,404] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7995: loss 0.0987
[2019-04-01 15:41:00,407] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7995: learning rate 0.0010
[2019-04-01 15:41:00,542] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.6, 60.0, 96.5, 585.0, 26.0, 25.93949775043483, 8.72492285351309, 1.0, 1.0, 30.49595067957703], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 298800.0000, 
sim time next is 299400.0000, 
raw observation next is [-10.6, 58.16666666666666, 99.66666666666666, 609.3333333333334, 26.0, 25.93256653907019, 8.70970439612951, 1.0, 1.0, 28.77897042375645], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.5816666666666666, 0.3322222222222222, 0.6732965009208104, 1.0, 0.9903666484385987, 0.0870970439612951, 1.0, 1.0, 0.20556407445540323], 
reward next is 0.7944, 
noisyNet noise sample is [array([0.2878358], dtype=float32), 0.01739415]. 
=============================================
[2019-04-01 15:41:00,552] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8021: loss 0.1927
[2019-04-01 15:41:00,553] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8021: learning rate 0.0010
[2019-04-01 15:41:00,599] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 8025: loss 0.0142
[2019-04-01 15:41:00,600] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 8026: learning rate 0.0010
[2019-04-01 15:41:00,619] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8032: loss 0.0447
[2019-04-01 15:41:00,620] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8032: learning rate 0.0010
[2019-04-01 15:41:00,964] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 8092: loss 0.1483
[2019-04-01 15:41:00,965] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 8092: learning rate 0.0010
[2019-04-01 15:41:01,040] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 8105: loss 0.1662
[2019-04-01 15:41:01,041] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 8105: learning rate 0.0010
[2019-04-01 15:41:01,124] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8123: loss 0.1575
[2019-04-01 15:41:01,125] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8123: learning rate 0.0010
[2019-04-01 15:41:01,261] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8155: loss 0.3960
[2019-04-01 15:41:01,263] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8155: learning rate 0.0010
[2019-04-01 15:41:01,985] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8312: loss 0.4035
[2019-04-01 15:41:01,987] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8312: learning rate 0.0010
[2019-04-01 15:41:10,098] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3344930e-06 1.7035202e-04 2.6102260e-09 2.9242251e-08 9.9981624e-01
 8.3529233e-07 1.1148751e-05], sum to 1.0000
[2019-04-01 15:41:10,099] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0360
[2019-04-01 15:41:10,233] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.0, 36.66666666666666, 0.0, 0.0, 26.0, 25.68427726088716, 8.988788825122283, 1.0, 1.0, 47.70612656452568], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 407400.0000, 
sim time next is 408000.0000, 
raw observation next is [-9.100000000000001, 37.33333333333334, 0.0, 0.0, 26.0, 25.81100490506609, 8.982985884781831, 1.0, 1.0, 35.70302182289543], 
processed observation next is [1.0, 0.7391304347826086, 0.21052631578947364, 0.3733333333333334, 0.0, 0.0, 1.0, 0.9730007007237269, 0.08982985884781831, 1.0, 1.0, 0.25502158444925305], 
reward next is 0.7450, 
noisyNet noise sample is [array([0.37350073], dtype=float32), -1.0338442]. 
=============================================
[2019-04-01 15:41:10,241] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[29.222513]
 [28.818235]
 [28.293028]
 [27.980013]
 [27.865145]], R is [[30.24188042]
 [30.59870529]
 [30.90438652]
 [31.20801544]
 [31.76266098]].
[2019-04-01 15:41:12,643] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.4103103e-10 5.9330203e-05 3.0133701e-11 2.1242853e-13 9.9991727e-01
 9.1987051e-10 2.3404948e-05], sum to 1.0000
[2019-04-01 15:41:12,644] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8011
[2019-04-01 15:41:12,743] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.683333333333334, 45.33333333333334, 0.0, 0.0, 26.0, 22.37056031715739, 8.082053663685427, 0.0, 1.0, 46.09668603017125], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 453000.0000, 
sim time next is 453600.0000, 
raw observation next is [-9.5, 44.0, 0.0, 0.0, 26.0, 22.33294708141234, 8.11690988139427, 0.0, 1.0, 46.03453997960517], 
processed observation next is [1.0, 0.2608695652173913, 0.1994459833795014, 0.44, 0.0, 0.0, 1.0, 0.47613529734461985, 0.08116909881394269, 0.0, 1.0, 0.32881814271146553], 
reward next is 0.6712, 
noisyNet noise sample is [array([0.3754286], dtype=float32), -2.09769]. 
=============================================
[2019-04-01 15:41:19,630] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8376815e-13 2.9183184e-07 5.1947887e-15 1.0543657e-18 9.9999666e-01
 1.9112839e-13 3.1434718e-06], sum to 1.0000
[2019-04-01 15:41:19,632] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2288
[2019-04-01 15:41:19,760] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.266666666666667, 96.0, 0.0, 0.0, 26.0, 24.75398909768106, 6.860947891974779, 0.0, 1.0, 45.07710370338723], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 505200.0000, 
sim time next is 505800.0000, 
raw observation next is [1.35, 96.0, 0.0, 0.0, 26.0, 24.78280751095706, 6.917095040451923, 0.0, 1.0, 43.7955900921442], 
processed observation next is [1.0, 0.8695652173913043, 0.5000000000000001, 0.96, 0.0, 0.0, 1.0, 0.8261153587081514, 0.06917095040451923, 0.0, 1.0, 0.3128256435153157], 
reward next is 0.6872, 
noisyNet noise sample is [array([0.00748766], dtype=float32), -0.86097455]. 
=============================================
[2019-04-01 15:41:20,378] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.9904415e-09 6.7217089e-04 8.9403082e-11 5.5941115e-12 9.9929357e-01
 1.3488421e-08 3.4205212e-05], sum to 1.0000
[2019-04-01 15:41:20,378] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1173
[2019-04-01 15:41:20,395] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.433333333333334, 86.0, 0.0, 0.0, 26.0, 24.54229296581476, 5.904135641324267, 0.0, 1.0, 40.19147040461195], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 537600.0000, 
sim time next is 538200.0000, 
raw observation next is [1.35, 86.5, 0.0, 0.0, 26.0, 24.51749874515675, 5.868098211105604, 0.0, 1.0, 40.23627467684106], 
processed observation next is [0.0, 0.21739130434782608, 0.5000000000000001, 0.865, 0.0, 0.0, 1.0, 0.7882141064509643, 0.05868098211105604, 0.0, 1.0, 0.28740196197743617], 
reward next is 0.7126, 
noisyNet noise sample is [array([0.34552017], dtype=float32), 0.45991892]. 
=============================================
[2019-04-01 15:41:21,297] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.8634279e-11 4.7186841e-06 1.8689514e-12 1.3128200e-14 9.9999452e-01
 1.0164347e-10 7.7069200e-07], sum to 1.0000
[2019-04-01 15:41:21,298] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1599
[2019-04-01 15:41:21,341] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.2, 89.66666666666667, 125.6666666666667, 103.1666666666667, 26.0, 25.04164129541558, 7.007975721499015, 0.0, 1.0, 29.14152636347082], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 552000.0000, 
sim time next is 552600.0000, 
raw observation next is [-0.3, 89.0, 144.0, 103.0, 26.0, 24.96516976877571, 6.873953350057652, 0.0, 1.0, 27.48944799977207], 
processed observation next is [0.0, 0.391304347826087, 0.4542936288088643, 0.89, 0.48, 0.1138121546961326, 1.0, 0.8521671098251014, 0.06873953350057653, 0.0, 1.0, 0.19635319999837195], 
reward next is 0.8036, 
noisyNet noise sample is [array([-1.1317652], dtype=float32), 0.6209876]. 
=============================================
[2019-04-01 15:41:24,530] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.3318556e-12 1.9163875e-07 6.1984039e-13 8.7806434e-15 9.9999952e-01
 2.5280637e-11 2.0873372e-07], sum to 1.0000
[2019-04-01 15:41:24,531] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7966
[2019-04-01 15:41:24,558] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 26.0, 24.90191334185787, 6.756060975946539, 0.0, 1.0, 42.40437746997015], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 597000.0000, 
sim time next is 597600.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 26.0, 24.87022464037749, 6.679311627200744, 0.0, 1.0, 42.38348777220837], 
processed observation next is [0.0, 0.9565217391304348, 0.38504155124653744, 0.83, 0.0, 0.0, 1.0, 0.8386035200539271, 0.06679311627200744, 0.0, 1.0, 0.3027391983729169], 
reward next is 0.6973, 
noisyNet noise sample is [array([-0.2070742], dtype=float32), -0.62223077]. 
=============================================
[2019-04-01 15:41:24,667] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15413: loss 0.0185
[2019-04-01 15:41:24,688] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15413: learning rate 0.0010
[2019-04-01 15:41:25,409] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 15651: loss 0.0227
[2019-04-01 15:41:25,419] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 15651: learning rate 0.0010
[2019-04-01 15:41:25,457] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15668: loss 0.0340
[2019-04-01 15:41:25,458] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15668: learning rate 0.0010
[2019-04-01 15:41:25,799] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15816: loss 0.0362
[2019-04-01 15:41:25,801] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15819: learning rate 0.0010
[2019-04-01 15:41:25,863] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15849: loss 0.1007
[2019-04-01 15:41:25,865] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15849: learning rate 0.0010
[2019-04-01 15:41:25,893] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15864: loss 0.0834
[2019-04-01 15:41:25,898] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15868: learning rate 0.0010
[2019-04-01 15:41:25,926] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15885: loss 0.0144
[2019-04-01 15:41:25,928] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15885: learning rate 0.0010
[2019-04-01 15:41:26,096] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15972: loss 0.0823
[2019-04-01 15:41:26,096] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15973: learning rate 0.0010
[2019-04-01 15:41:26,162] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16006: loss 0.0859
[2019-04-01 15:41:26,163] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16006: learning rate 0.0010
[2019-04-01 15:41:26,277] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16068: loss 0.0102
[2019-04-01 15:41:26,277] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16068: learning rate 0.0010
[2019-04-01 15:41:26,344] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16104: loss 0.0530
[2019-04-01 15:41:26,346] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16105: learning rate 0.0010
[2019-04-01 15:41:26,563] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16221: loss 0.0120
[2019-04-01 15:41:26,565] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16221: learning rate 0.0010
[2019-04-01 15:41:26,723] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 16318: loss 0.0282
[2019-04-01 15:41:26,724] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 16319: learning rate 0.0010
[2019-04-01 15:41:26,952] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 16452: loss 0.0003
[2019-04-01 15:41:26,954] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 16452: learning rate 0.0010
[2019-04-01 15:41:27,058] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.8273247e-10 7.0327746e-07 2.3252086e-11 2.5011140e-13 9.9999857e-01
 2.0290047e-10 7.0758506e-07], sum to 1.0000
[2019-04-01 15:41:27,058] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7444
[2019-04-01 15:41:27,070] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.39609390635177, 5.691500272663319, 0.0, 1.0, 41.59252635298601], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 609000.0000, 
sim time next is 609600.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.36253232507755, 5.678768176975474, 0.0, 1.0, 41.57725875510079], 
processed observation next is [0.0, 0.043478260869565216, 0.3545706371191136, 0.86, 0.0, 0.0, 1.0, 0.7660760464396498, 0.05678768176975474, 0.0, 1.0, 0.2969804196792914], 
reward next is 0.7030, 
noisyNet noise sample is [array([0.14120929], dtype=float32), -0.2642069]. 
=============================================
[2019-04-01 15:41:27,158] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16563: loss 0.0638
[2019-04-01 15:41:27,170] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16568: learning rate 0.0010
[2019-04-01 15:41:28,020] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16905: loss 0.0194
[2019-04-01 15:41:28,021] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16905: learning rate 0.0010
[2019-04-01 15:41:40,611] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.27425121e-12 1.41581520e-08 8.88827315e-14 5.02914007e-16
 9.99999881e-01 1.21515758e-11 1.10486795e-07], sum to 1.0000
[2019-04-01 15:41:40,612] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8077
[2019-04-01 15:41:40,734] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.35, 73.0, 87.0, 0.0, 26.0, 25.63759089015314, 7.246462920654653, 1.0, 1.0, 28.49722983249459], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 815400.0000, 
sim time next is 816000.0000, 
raw observation next is [-5.066666666666666, 72.33333333333333, 90.83333333333333, 0.0, 26.0, 25.5833590382714, 7.293412322870343, 1.0, 1.0, 31.13452530457721], 
processed observation next is [1.0, 0.43478260869565216, 0.32225300092336107, 0.7233333333333333, 0.30277777777777776, 0.0, 1.0, 0.9404798626102, 0.07293412322870343, 1.0, 1.0, 0.22238946646126578], 
reward next is 0.7776, 
noisyNet noise sample is [array([0.2840637], dtype=float32), -1.4209414]. 
=============================================
[2019-04-01 15:41:40,746] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.526276]
 [70.54297 ]
 [70.542755]
 [70.58333 ]
 [70.61729 ]], R is [[70.67955017]
 [70.76920319]
 [70.91283417]
 [71.04942322]
 [71.15673828]].
[2019-04-01 15:41:45,675] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23218: loss 0.3228
[2019-04-01 15:41:45,677] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23219: learning rate 0.0010
[2019-04-01 15:41:46,080] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23438: loss 0.2052
[2019-04-01 15:41:46,081] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23439: learning rate 0.0010
[2019-04-01 15:41:46,724] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 23719: loss 0.6660
[2019-04-01 15:41:46,724] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 23719: learning rate 0.0010
[2019-04-01 15:41:46,936] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23797: loss 0.4063
[2019-04-01 15:41:46,937] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23797: learning rate 0.0010
[2019-04-01 15:41:47,014] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 23828: loss 0.3857
[2019-04-01 15:41:47,015] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 23828: learning rate 0.0010
[2019-04-01 15:41:47,130] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23878: loss 0.3447
[2019-04-01 15:41:47,132] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23879: learning rate 0.0010
[2019-04-01 15:41:47,271] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23948: loss 0.3197
[2019-04-01 15:41:47,275] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23950: learning rate 0.0010
[2019-04-01 15:41:47,284] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23956: loss 0.3406
[2019-04-01 15:41:47,286] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23956: learning rate 0.0010
[2019-04-01 15:41:47,428] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24026: loss 0.4521
[2019-04-01 15:41:47,430] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24027: learning rate 0.0010
[2019-04-01 15:41:47,434] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 24028: loss 0.5987
[2019-04-01 15:41:47,437] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 24029: learning rate 0.0010
[2019-04-01 15:41:47,634] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 24117: loss 0.4469
[2019-04-01 15:41:47,635] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 24117: learning rate 0.0010
[2019-04-01 15:41:47,894] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24230: loss 0.6153
[2019-04-01 15:41:47,898] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24231: learning rate 0.0010
[2019-04-01 15:41:48,138] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 24301: loss 0.5050
[2019-04-01 15:41:48,138] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 24301: learning rate 0.0010
[2019-04-01 15:41:48,540] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 24451: loss 0.4707
[2019-04-01 15:41:48,541] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 24451: learning rate 0.0010
[2019-04-01 15:41:48,647] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 24493: loss 0.2384
[2019-04-01 15:41:48,647] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 24493: learning rate 0.0010
[2019-04-01 15:41:48,761] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.6521231e-14 9.9129971e-10 3.2052883e-16 1.8329747e-18 1.0000000e+00
 1.3075556e-12 1.1191346e-09], sum to 1.0000
[2019-04-01 15:41:48,762] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5344
[2019-04-01 15:41:48,769] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.4, 96.0, 0.0, 0.0, 26.0, 25.22387085704581, 8.025815671788415, 1.0, 1.0, 8.374753960045814], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 928800.0000, 
sim time next is 929400.0000, 
raw observation next is [4.4, 96.66666666666666, 0.0, 0.0, 26.0, 25.16335535148695, 7.675195216174852, 1.0, 1.0, 8.804328219467159], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.9666666666666666, 0.0, 0.0, 1.0, 0.880479335926707, 0.07675195216174852, 1.0, 1.0, 0.06288805871047971], 
reward next is 0.9371, 
noisyNet noise sample is [array([-0.47747862], dtype=float32), -0.057949092]. 
=============================================
[2019-04-01 15:41:49,310] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24777: loss 0.0290
[2019-04-01 15:41:49,310] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24777: learning rate 0.0010
[2019-04-01 15:41:53,857] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7166249e-08 7.8336830e-05 1.2020384e-06 6.9360195e-12 1.9749975e-01
 7.1523857e-08 8.0242056e-01], sum to 1.0000
[2019-04-01 15:41:53,857] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8890
[2019-04-01 15:41:53,955] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.76666666666667, 79.0, 63.16666666666666, 0.0, 26.0, 26.24423742196208, 14.88999219164827, 1.0, 1.0, 9.16407217447811], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1005600.0000, 
sim time next is 1006200.0000, 
raw observation next is [14.95, 78.0, 57.0, 0.0, 26.0, 26.57624284175324, 15.93423530177663, 1.0, 1.0, 8.626437281806501], 
processed observation next is [1.0, 0.6521739130434783, 0.8767313019390582, 0.78, 0.19, 0.0, 1.0, 1.0823204059647484, 0.1593423530177663, 1.0, 1.0, 0.06161740915576072], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.89047176], dtype=float32), 0.7575836]. 
=============================================
[2019-04-01 15:41:55,275] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1398992e-10 4.1639078e-06 1.8700212e-09 1.5122185e-13 9.9907410e-01
 1.0926075e-10 9.2168525e-04], sum to 1.0000
[2019-04-01 15:41:55,278] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2077
[2019-04-01 15:41:55,364] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.80779012168245, 13.47185996727588, 0.0, 1.0, 26.66652552661298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1025400.0000, 
sim time next is 1026000.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 25.96292436863757, 13.69960819919552, 0.0, 1.0, 23.70285890221716], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.77, 0.0, 0.0, 1.0, 0.9947034812339385, 0.1369960819919552, 0.0, 1.0, 0.16930613501583686], 
reward next is 0.8307, 
noisyNet noise sample is [array([2.1543937], dtype=float32), -0.5547752]. 
=============================================
[2019-04-01 15:41:55,384] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[70.73081]
 [69.68325]
 [68.52476]
 [67.83257]
 [66.89756]], R is [[71.80841064]
 [71.89984894]
 [71.96134949]
 [71.87215424]
 [71.92406464]].
[2019-04-01 15:41:57,219] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8381673e-13 4.6221958e-05 5.5416050e-11 5.4184007e-18 9.9989128e-01
 2.2811026e-12 6.2414008e-05], sum to 1.0000
[2019-04-01 15:41:57,221] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9473
[2019-04-01 15:41:57,230] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.7, 81.33333333333334, 0.0, 0.0, 26.0, 25.72632681975497, 13.48153211420409, 0.0, 1.0, 18.09075154605459], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1149600.0000, 
sim time next is 1150200.0000, 
raw observation next is [12.7, 82.0, 0.0, 0.0, 26.0, 25.72148668352767, 13.45068406361906, 0.0, 1.0, 17.14813405082964], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.82, 0.0, 0.0, 1.0, 0.9602123833610955, 0.1345068406361906, 0.0, 1.0, 0.12248667179164029], 
reward next is 0.8775, 
noisyNet noise sample is [array([1.34052], dtype=float32), 1.0206467]. 
=============================================
[2019-04-01 15:41:58,959] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 30788: loss 0.6515
[2019-04-01 15:41:58,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 30788: learning rate 0.0010
[2019-04-01 15:41:59,219] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.95857348e-13 1.50332016e-05 6.32613796e-12 7.81808234e-17
 9.99879837e-01 8.71579881e-13 1.05148974e-04], sum to 1.0000
[2019-04-01 15:41:59,227] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1694
[2019-04-01 15:41:59,326] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.03333333333333, 76.5, 48.33333333333333, 0.0, 26.0, 25.7743925488469, 13.23961985185485, 0.0, 1.0, 14.35735228369103], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1155000.0000, 
sim time next is 1155600.0000, 
raw observation next is [15.5, 75.0, 57.0, 0.0, 26.0, 25.76456335241775, 13.17573938749899, 0.0, 1.0, 13.59089203343883], 
processed observation next is [0.0, 0.391304347826087, 0.8919667590027703, 0.75, 0.19, 0.0, 1.0, 0.9663661932025356, 0.1317573938749899, 0.0, 1.0, 0.0970778002388488], 
reward next is 0.9029, 
noisyNet noise sample is [array([0.32759038], dtype=float32), 0.7715018]. 
=============================================
[2019-04-01 15:41:59,337] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 30985: loss 0.6173
[2019-04-01 15:41:59,341] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 30987: learning rate 0.0010
[2019-04-01 15:41:59,579] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 31146: loss 0.4994
[2019-04-01 15:41:59,581] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 31147: learning rate 0.0010
[2019-04-01 15:41:59,921] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0265208e-13 8.3154344e-05 9.6850852e-12 2.0018204e-19 9.9909401e-01
 4.2182403e-12 8.2281604e-04], sum to 1.0000
[2019-04-01 15:41:59,927] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4563
[2019-04-01 15:41:59,935] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 64.33333333333334, 0.0, 0.0, 26.0, 24.90965515280709, 9.548477207526615, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1185600.0000, 
sim time next is 1186200.0000, 
raw observation next is [18.3, 64.0, 0.0, 0.0, 26.0, 24.88909573236675, 9.468479854635033, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.64, 0.0, 0.0, 1.0, 0.8412993903381073, 0.09468479854635033, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7350217], dtype=float32), -0.99831945]. 
=============================================
[2019-04-01 15:42:00,328] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31577: loss 0.0193
[2019-04-01 15:42:00,330] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31577: learning rate 0.0010
[2019-04-01 15:42:00,358] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 31599: loss 0.0057
[2019-04-01 15:42:00,361] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 31599: learning rate 0.0010
[2019-04-01 15:42:00,518] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 31694: loss 0.0010
[2019-04-01 15:42:00,521] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 31696: learning rate 0.0010
[2019-04-01 15:42:00,534] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 31701: loss 0.0031
[2019-04-01 15:42:00,537] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 31704: learning rate 0.0010
[2019-04-01 15:42:00,669] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 31776: loss 0.0122
[2019-04-01 15:42:00,672] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 31777: learning rate 0.0010
[2019-04-01 15:42:00,706] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4788613e-12 1.3186295e-04 5.0048212e-12 3.9596881e-17 9.9973577e-01
 1.3703058e-11 1.3241870e-04], sum to 1.0000
[2019-04-01 15:42:00,712] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1713
[2019-04-01 15:42:00,722] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.56108490465725, 6.05561781289565, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1232400.0000, 
sim time next is 1233000.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.56529090576948, 6.041734249483021, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 1.0, 0.6521844151099258, 0.06041734249483021, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7952556], dtype=float32), -2.2986698]. 
=============================================
[2019-04-01 15:42:00,732] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[93.94993]
 [93.88092]
 [93.78769]
 [93.69159]
 [93.64029]], R is [[94.06240845]
 [94.12178802]
 [94.18057251]
 [94.23876953]
 [94.29637909]].
[2019-04-01 15:42:00,823] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 31859: loss 0.0291
[2019-04-01 15:42:00,825] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 31859: learning rate 0.0010
[2019-04-01 15:42:01,220] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 32089: loss 0.0108
[2019-04-01 15:42:01,227] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 32092: learning rate 0.0010
[2019-04-01 15:42:01,674] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 32364: loss 0.0017
[2019-04-01 15:42:01,675] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 32365: learning rate 0.0010
[2019-04-01 15:42:01,845] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 32471: loss 0.0340
[2019-04-01 15:42:01,847] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32472: loss 0.0391
[2019-04-01 15:42:01,847] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 32471: learning rate 0.0010
[2019-04-01 15:42:01,849] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32472: learning rate 0.0010
[2019-04-01 15:42:02,426] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 32819: loss 0.0512
[2019-04-01 15:42:02,436] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 32823: learning rate 0.0010
[2019-04-01 15:42:02,664] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 32976: loss 0.0253
[2019-04-01 15:42:02,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 32976: learning rate 0.0010
[2019-04-01 15:42:03,121] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 33269: loss 0.0637
[2019-04-01 15:42:03,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 33270: learning rate 0.0010
[2019-04-01 15:42:03,505] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6060869e-15 6.5692781e-08 3.9144469e-14 3.7443797e-21 9.9999988e-01
 3.1015829e-14 1.4970940e-08], sum to 1.0000
[2019-04-01 15:42:03,510] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0436
[2019-04-01 15:42:03,616] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.1, 80.5, 0.0, 0.0, 26.0, 24.08964106041925, 6.908475571833733, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1213800.0000, 
sim time next is 1214400.0000, 
raw observation next is [16.1, 81.0, 0.0, 0.0, 26.0, 24.06198476680728, 6.852963838849209, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.9085872576177286, 0.81, 0.0, 0.0, 1.0, 0.7231406809724688, 0.06852963838849209, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8335585], dtype=float32), -0.3768632]. 
=============================================
[2019-04-01 15:42:05,000] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2773821e-12 2.1805940e-06 1.2635108e-10 2.2740979e-16 9.9996686e-01
 2.1742974e-12 3.1048032e-05], sum to 1.0000
[2019-04-01 15:42:05,003] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9004
[2019-04-01 15:42:05,011] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 100.0, 51.0, 0.0, 26.0, 24.67785564547299, 9.37694387251421, 0.0, 1.0, 4.980925220003605], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1265400.0000, 
sim time next is 1266000.0000, 
raw observation next is [13.8, 100.0, 45.66666666666666, 0.0, 26.0, 24.66158366580398, 9.322695596551968, 0.0, 1.0, 4.823172285825065], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.1522222222222222, 0.0, 1.0, 0.8087976665434259, 0.09322695596551968, 0.0, 1.0, 0.034451230613036184], 
reward next is 0.9655, 
noisyNet noise sample is [array([1.0643944], dtype=float32), -1.2507693]. 
=============================================
[2019-04-01 15:42:05,015] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.405754]
 [77.19625 ]
 [76.975464]
 [76.77674 ]
 [76.5741  ]], R is [[77.80779266]
 [77.99414062]
 [78.17747498]
 [78.35774994]
 [78.53498077]].
[2019-04-01 15:42:15,027] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39307: loss 7.0333
[2019-04-01 15:42:15,030] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39307: learning rate 0.0010
[2019-04-01 15:42:15,031] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39308: loss 7.1250
[2019-04-01 15:42:15,035] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39308: learning rate 0.0010
[2019-04-01 15:42:15,556] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 39602: loss 6.1565
[2019-04-01 15:42:15,558] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 39602: learning rate 0.0010
[2019-04-01 15:42:15,683] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 39666: loss 5.6292
[2019-04-01 15:42:15,686] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 39669: learning rate 0.0010
[2019-04-01 15:42:15,889] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 39775: loss 5.1303
[2019-04-01 15:42:15,893] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 39776: learning rate 0.0010
[2019-04-01 15:42:15,962] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.9036773e-12 1.6757988e-06 1.7432006e-09 9.1616591e-15 9.9966478e-01
 3.0707156e-11 3.3352611e-04], sum to 1.0000
[2019-04-01 15:42:15,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0029
[2019-04-01 15:42:15,979] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.29851581409653, 9.353522881264354, 0.0, 1.0, 36.0520599408642], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1483800.0000, 
sim time next is 1484400.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.32628382563199, 9.313565171517547, 0.0, 1.0, 36.58811157064694], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 1.0, 0.9037548322331413, 0.09313565171517547, 0.0, 1.0, 0.26134365407604954], 
reward next is 0.7387, 
noisyNet noise sample is [array([0.46002436], dtype=float32), -1.4777284]. 
=============================================
[2019-04-01 15:42:16,009] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 39839: loss 5.9017
[2019-04-01 15:42:16,011] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 39839: learning rate 0.0010
[2019-04-01 15:42:16,050] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39862: loss 7.6541
[2019-04-01 15:42:16,052] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39862: learning rate 0.0010
[2019-04-01 15:42:16,151] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 39916: loss 6.7510
[2019-04-01 15:42:16,151] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 39916: learning rate 0.0010
[2019-04-01 15:42:16,365] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 40039: loss 3.7470
[2019-04-01 15:42:16,365] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 40039: learning rate 0.0010
[2019-04-01 15:42:16,631] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 40201: loss 8.8564
[2019-04-01 15:42:16,632] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 40201: learning rate 0.0010
[2019-04-01 15:42:16,659] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 40209: loss 4.8838
[2019-04-01 15:42:16,660] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 40210: learning rate 0.0010
[2019-04-01 15:42:16,856] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40338: loss 3.2099
[2019-04-01 15:42:16,857] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40340: learning rate 0.0010
[2019-04-01 15:42:16,870] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40349: loss 0.3030
[2019-04-01 15:42:16,871] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40349: learning rate 0.0010
[2019-04-01 15:42:17,356] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 40670: loss -2.6066
[2019-04-01 15:42:17,357] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 40670: learning rate 0.0010
[2019-04-01 15:42:17,450] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 40737: loss -1.5053
[2019-04-01 15:42:17,451] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 40738: learning rate 0.0010
[2019-04-01 15:42:18,034] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 41106: loss 3.4817
[2019-04-01 15:42:18,035] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 41107: learning rate 0.0010
[2019-04-01 15:42:28,436] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7961254e-09 2.1249271e-04 7.1393478e-09 2.6893143e-12 2.2215740e-04
 3.3688641e-10 9.9956542e-01], sum to 1.0000
[2019-04-01 15:42:28,437] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5049
[2019-04-01 15:42:28,565] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8999999999999999, 85.0, 0.0, 0.0, 26.0, 24.87897570368786, 7.846300007047951, 0.0, 1.0, 43.11158932294012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1747800.0000, 
sim time next is 1748400.0000, 
raw observation next is [-1.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.86631583055035, 7.738923435620602, 0.0, 1.0, 43.1822433839036], 
processed observation next is [0.0, 0.21739130434782608, 0.4349030470914128, 0.8566666666666667, 0.0, 0.0, 1.0, 0.8380451186500498, 0.07738923435620601, 0.0, 1.0, 0.3084445955993114], 
reward next is 0.6916, 
noisyNet noise sample is [array([0.18954167], dtype=float32), 0.5910857]. 
=============================================
[2019-04-01 15:42:30,876] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47505: loss 0.9135
[2019-04-01 15:42:30,878] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47507: learning rate 0.0010
[2019-04-01 15:42:30,981] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47537: loss 1.0363
[2019-04-01 15:42:30,981] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47537: learning rate 0.0010
[2019-04-01 15:42:31,555] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 47702: loss 0.4107
[2019-04-01 15:42:31,561] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 47702: learning rate 0.0010
[2019-04-01 15:42:31,921] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47816: loss 0.1360
[2019-04-01 15:42:31,925] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47816: learning rate 0.0010
[2019-04-01 15:42:32,145] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 47898: loss 0.0495
[2019-04-01 15:42:32,145] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 47898: learning rate 0.0010
[2019-04-01 15:42:32,192] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 47914: loss 0.0997
[2019-04-01 15:42:32,197] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 47915: learning rate 0.0010
[2019-04-01 15:42:32,360] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 47980: loss 0.0011
[2019-04-01 15:42:32,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 47982: learning rate 0.0010
[2019-04-01 15:42:32,420] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48007: loss 0.0098
[2019-04-01 15:42:32,420] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48007: learning rate 0.0010
[2019-04-01 15:42:32,696] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48140: loss 0.0174
[2019-04-01 15:42:32,696] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48140: learning rate 0.0010
[2019-04-01 15:42:32,701] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48142: loss 0.0450
[2019-04-01 15:42:32,702] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48142: learning rate 0.0010
[2019-04-01 15:42:32,931] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 48257: loss 0.0902
[2019-04-01 15:42:32,932] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 48258: learning rate 0.0010
[2019-04-01 15:42:33,197] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48383: loss 0.0073
[2019-04-01 15:42:33,198] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48383: learning rate 0.0010
[2019-04-01 15:42:33,513] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 48538: loss 0.0079
[2019-04-01 15:42:33,517] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 48538: learning rate 0.0010
[2019-04-01 15:42:33,691] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.00735484e-07 2.31881835e-03 4.27986197e-06 7.34241501e-10
 7.61239976e-03 3.75369211e-08 9.90064323e-01], sum to 1.0000
[2019-04-01 15:42:33,692] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1515
[2019-04-01 15:42:33,738] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.966666666666667, 78.0, 143.3333333333333, 86.83333333333334, 26.0, 24.96606233405186, 6.440572968735149, 0.0, 1.0, 43.86655869501258], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1849200.0000, 
sim time next is 1849800.0000, 
raw observation next is [-5.783333333333333, 78.0, 138.6666666666667, 79.66666666666667, 26.0, 24.87935446537203, 6.369975165408623, 0.0, 1.0, 49.29202978775749], 
processed observation next is [0.0, 0.391304347826087, 0.3024007386888274, 0.78, 0.46222222222222237, 0.08802946593001842, 1.0, 0.8399077807674331, 0.06369975165408623, 0.0, 1.0, 0.35208592705541064], 
reward next is 0.6479, 
noisyNet noise sample is [array([-0.0176469], dtype=float32), 1.1197848]. 
=============================================
[2019-04-01 15:42:33,802] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 48678: loss 0.0321
[2019-04-01 15:42:33,802] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 48678: learning rate 0.0010
[2019-04-01 15:42:33,875] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 48718: loss 0.0597
[2019-04-01 15:42:33,877] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 48719: learning rate 0.0010
[2019-04-01 15:42:34,476] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 48974: loss 0.0290
[2019-04-01 15:42:34,478] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 48976: learning rate 0.0010
[2019-04-01 15:42:40,728] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5464297e-12 5.9496390e-08 4.1387882e-10 7.5564905e-16 5.7100977e-09
 1.5521113e-13 9.9999988e-01], sum to 1.0000
[2019-04-01 15:42:40,728] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5975
[2019-04-01 15:42:40,798] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 75.0, 201.5, 123.0, 26.0, 25.77493420032639, 7.815570517875924, 1.0, 1.0, 20.93986367297773], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1940400.0000, 
sim time next is 1941000.0000, 
raw observation next is [-5.5, 73.33333333333333, 211.6666666666667, 85.33333333333331, 26.0, 25.78133944379039, 7.81050745597931, 1.0, 1.0, 19.89402297288182], 
processed observation next is [1.0, 0.4782608695652174, 0.3102493074792244, 0.7333333333333333, 0.7055555555555557, 0.09429097605893184, 1.0, 0.9687627776843415, 0.0781050745597931, 1.0, 1.0, 0.142100164092013], 
reward next is 0.8579, 
noisyNet noise sample is [array([0.4729423], dtype=float32), 0.6192581]. 
=============================================
[2019-04-01 15:42:40,802] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[58.737915]
 [57.525364]
 [56.32983 ]
 [55.1438  ]
 [53.57745 ]], R is [[60.26586533]
 [60.51363754]
 [60.75072098]
 [60.9772644 ]
 [61.19503021]].
[2019-04-01 15:42:52,898] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55197: loss 5.2034
[2019-04-01 15:42:52,899] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55197: learning rate 0.0010
[2019-04-01 15:42:53,239] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55343: loss 5.2735
[2019-04-01 15:42:53,240] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55344: learning rate 0.0010
[2019-04-01 15:42:53,266] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55359: loss 4.8332
[2019-04-01 15:42:53,268] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55359: learning rate 0.0010
[2019-04-01 15:42:53,790] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55597: loss 4.8177
[2019-04-01 15:42:53,791] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55597: learning rate 0.0010
[2019-04-01 15:42:53,868] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 55630: loss 5.5089
[2019-04-01 15:42:53,869] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 55630: learning rate 0.0010
[2019-04-01 15:42:53,977] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 55678: loss 4.8076
[2019-04-01 15:42:53,977] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 55678: learning rate 0.0010
[2019-04-01 15:42:54,274] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 55795: loss 4.9506
[2019-04-01 15:42:54,274] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 55795: learning rate 0.0010
[2019-04-01 15:42:54,386] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 55838: loss 5.3191
[2019-04-01 15:42:54,387] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 55838: learning rate 0.0010
[2019-04-01 15:42:54,503] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 55885: loss 5.7920
[2019-04-01 15:42:54,504] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 55885: learning rate 0.0010
[2019-04-01 15:42:54,873] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56024: loss 4.2077
[2019-04-01 15:42:54,874] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56024: learning rate 0.0010
[2019-04-01 15:42:55,122] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 56113: loss 4.5694
[2019-04-01 15:42:55,124] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 56114: learning rate 0.0010
[2019-04-01 15:42:55,180] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56131: loss 4.3568
[2019-04-01 15:42:55,180] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56131: learning rate 0.0010
[2019-04-01 15:42:55,388] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 56189: loss 4.1647
[2019-04-01 15:42:55,394] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 56191: learning rate 0.0010
[2019-04-01 15:42:55,422] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56203: loss 4.4324
[2019-04-01 15:42:55,423] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56203: learning rate 0.0010
[2019-04-01 15:42:56,370] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3234099e-10 5.4813213e-06 9.0899013e-09 9.7021685e-13 4.5294710e-03
 3.8438422e-10 9.9546510e-01], sum to 1.0000
[2019-04-01 15:42:56,371] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8215
[2019-04-01 15:42:56,397] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.533333333333333, 84.33333333333333, 0.0, 0.0, 26.0, 24.28501352726007, 5.521387433749425, 0.0, 1.0, 42.83157237037386], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2094000.0000, 
sim time next is 2094600.0000, 
raw observation next is [-6.616666666666667, 83.66666666666667, 0.0, 0.0, 26.0, 24.23848120385423, 5.45527537579513, 0.0, 1.0, 42.89133902760693], 
processed observation next is [1.0, 0.21739130434782608, 0.2793167128347184, 0.8366666666666667, 0.0, 0.0, 1.0, 0.7483544576934616, 0.0545527537579513, 0.0, 1.0, 0.3063667073400495], 
reward next is 0.6936, 
noisyNet noise sample is [array([-0.182672], dtype=float32), 1.535216]. 
=============================================
[2019-04-01 15:42:56,423] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56518: loss 4.9136
[2019-04-01 15:42:56,424] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56518: learning rate 0.0010
[2019-04-01 15:42:56,905] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 56648: loss 4.8561
[2019-04-01 15:42:56,906] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 56648: learning rate 0.0010
[2019-04-01 15:43:02,599] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6807753e-13 1.6511873e-09 2.2216218e-10 6.9620571e-16 3.1628294e-10
 3.0470398e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 15:43:02,601] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0490
[2019-04-01 15:43:02,636] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 68.0, 116.3333333333333, 190.0, 26.0, 26.59169261587123, 12.57049137827046, 1.0, 1.0, 22.68488499998914], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2217000.0000, 
sim time next is 2217600.0000, 
raw observation next is [-3.9, 68.0, 96.0, 142.5, 26.0, 26.58780394051403, 12.30619940630161, 1.0, 1.0, 21.30309828587525], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.68, 0.32, 0.1574585635359116, 1.0, 1.083971991502004, 0.1230619940630161, 1.0, 1.0, 0.15216498775625178], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7171753], dtype=float32), 0.06149472]. 
=============================================
[2019-04-01 15:43:12,531] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8366340e-10 7.7688384e-08 2.6867431e-09 9.4222204e-14 1.6163697e-03
 7.9249211e-12 9.9838352e-01], sum to 1.0000
[2019-04-01 15:43:12,531] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4763
[2019-04-01 15:43:12,545] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.72180180723525, 6.456929148461209, 0.0, 1.0, 38.73017200708205], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2341200.0000, 
sim time next is 2341800.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.70601275084263, 6.366301948363774, 0.0, 1.0, 38.8394204696388], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 1.0, 0.8151446786918041, 0.06366301948363774, 0.0, 1.0, 0.2774244319259915], 
reward next is 0.7226, 
noisyNet noise sample is [array([-1.72735], dtype=float32), 2.9204602]. 
=============================================
[2019-04-01 15:43:14,595] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2888748e-14 2.9704722e-06 2.9356968e-12 1.3402473e-17 4.9206332e-09
 8.3492385e-15 9.9999702e-01], sum to 1.0000
[2019-04-01 15:43:14,595] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4142
[2019-04-01 15:43:14,638] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9961580e-16 8.9390163e-08 5.6106232e-13 5.7929106e-20 1.2830726e-08
 5.7764577e-17 9.9999988e-01], sum to 1.0000
[2019-04-01 15:43:14,639] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 47.0, 86.0, 341.0, 26.0, 24.9972186796224, 7.935681800623421, 0.0, 1.0, 29.88273185415389], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2388600.0000, 
sim time next is 2389200.0000, 
raw observation next is [0.0, 47.0, 84.83333333333334, 293.8333333333334, 26.0, 25.04216968365575, 7.925763771628138, 0.0, 1.0, 27.5381056856365], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.2827777777777778, 0.3246777163904237, 1.0, 0.8631670976651071, 0.07925763771628139, 0.0, 1.0, 0.19670075489740357], 
reward next is 0.8033, 
noisyNet noise sample is [array([-1.7990953], dtype=float32), 0.81098366]. 
=============================================
[2019-04-01 15:43:14,639] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9355
[2019-04-01 15:43:14,821] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.383333333333333, 62.5, 148.3333333333333, 402.0, 26.0, 24.89757488599036, 7.234149419657956, 0.0, 1.0, 35.57369632576864], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2371800.0000, 
sim time next is 2372400.0000, 
raw observation next is [-2.3, 62.0, 153.0, 378.0, 26.0, 24.85843544824647, 7.493816661101069, 0.0, 1.0, 47.92641972384846], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.62, 0.51, 0.4176795580110497, 1.0, 0.8369193497494959, 0.07493816661101069, 0.0, 1.0, 0.3423315694560604], 
reward next is 0.6577, 
noisyNet noise sample is [array([1.8395017], dtype=float32), 1.4897444]. 
=============================================
[2019-04-01 15:43:15,082] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8793631e-14 6.7317636e-08 1.2458810e-11 1.1469549e-17 5.5985652e-09
 5.7213311e-15 9.9999988e-01], sum to 1.0000
[2019-04-01 15:43:15,084] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0642
[2019-04-01 15:43:15,134] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.1, 48.16666666666667, 223.6666666666667, 384.6666666666666, 26.0, 25.11913071132978, 8.054388784125182, 0.0, 1.0, 22.48226755625167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2376600.0000, 
sim time next is 2377200.0000, 
raw observation next is [-1.0, 49.33333333333334, 237.8333333333333, 404.3333333333334, 26.0, 25.08134205586597, 7.996005381559409, 0.0, 1.0, 21.20557998445423], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.4933333333333334, 0.7927777777777776, 0.44677716390423583, 1.0, 0.8687631508379958, 0.0799600538155941, 0.0, 1.0, 0.15146842846038736], 
reward next is 0.8485, 
noisyNet noise sample is [array([0.18165033], dtype=float32), 0.08811937]. 
=============================================
[2019-04-01 15:43:15,683] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63462: loss 0.1910
[2019-04-01 15:43:15,684] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63463: learning rate 0.0010
[2019-04-01 15:43:15,713] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 63473: loss 0.1705
[2019-04-01 15:43:15,714] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 63473: learning rate 0.0010
[2019-04-01 15:43:16,141] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 63609: loss 0.0448
[2019-04-01 15:43:16,142] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 63609: learning rate 0.0010
[2019-04-01 15:43:16,559] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63742: loss 0.0189
[2019-04-01 15:43:16,562] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63744: learning rate 0.0010
[2019-04-01 15:43:16,857] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 63843: loss 0.1980
[2019-04-01 15:43:16,857] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 63843: learning rate 0.0010
[2019-04-01 15:43:17,196] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 63963: loss 0.0660
[2019-04-01 15:43:17,197] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 63963: learning rate 0.0010
[2019-04-01 15:43:17,233] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 63978: loss 0.0486
[2019-04-01 15:43:17,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 63978: learning rate 0.0010
[2019-04-01 15:43:17,328] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 64014: loss 0.0116
[2019-04-01 15:43:17,331] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 64014: learning rate 0.0010
[2019-04-01 15:43:17,349] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 64023: loss 0.0279
[2019-04-01 15:43:17,350] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 64023: learning rate 0.0010
[2019-04-01 15:43:17,504] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4552087e-12 8.2147358e-07 3.4470784e-11 1.9801502e-16 1.0701593e-07
 7.3665100e-14 9.9999905e-01], sum to 1.0000
[2019-04-01 15:43:17,506] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2172
[2019-04-01 15:43:17,553] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64103: loss 0.0180
[2019-04-01 15:43:17,554] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64103: learning rate 0.0010
[2019-04-01 15:43:17,567] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.383333333333333, 62.5, 148.3333333333333, 402.0, 26.0, 24.87003783885238, 7.251497286605205, 0.0, 1.0, 38.59854275363428], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2371800.0000, 
sim time next is 2372400.0000, 
raw observation next is [-2.3, 62.0, 153.0, 378.0, 26.0, 24.84794594593335, 7.545795913947674, 0.0, 1.0, 48.52767004900852], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.62, 0.51, 0.4176795580110497, 1.0, 0.83542084941905, 0.07545795913947674, 0.0, 1.0, 0.34662621463577514], 
reward next is 0.6534, 
noisyNet noise sample is [array([-0.90513414], dtype=float32), 0.89601105]. 
=============================================
[2019-04-01 15:43:17,780] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 64208: loss 0.0429
[2019-04-01 15:43:17,782] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 64208: learning rate 0.0010
[2019-04-01 15:43:17,901] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64265: loss 0.1577
[2019-04-01 15:43:17,903] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64265: learning rate 0.0010
[2019-04-01 15:43:18,117] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 64368: loss 0.2894
[2019-04-01 15:43:18,119] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 64368: learning rate 0.0010
[2019-04-01 15:43:18,578] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64607: loss 0.1052
[2019-04-01 15:43:18,580] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64608: learning rate 0.0010
[2019-04-01 15:43:19,563] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 65077: loss 0.2492
[2019-04-01 15:43:19,564] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 65077: learning rate 0.0010
[2019-04-01 15:43:19,710] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7815677e-10 3.4917768e-05 1.3592550e-09 3.7391243e-14 2.8247419e-08
 1.2432881e-11 9.9996507e-01], sum to 1.0000
[2019-04-01 15:43:19,710] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7244
[2019-04-01 15:43:19,774] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.033333333333333, 52.66666666666666, 43.5, 457.5, 26.0, 24.76997408550853, 6.336181382025319, 0.0, 1.0, 67.45754684942841], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2450400.0000, 
sim time next is 2451000.0000, 
raw observation next is [-7.666666666666666, 51.33333333333334, 47.0, 499.0, 26.0, 25.04193715198971, 6.54112592200773, 0.0, 1.0, 55.8442335070199], 
processed observation next is [0.0, 0.34782608695652173, 0.25023084025854114, 0.5133333333333334, 0.15666666666666668, 0.5513812154696133, 1.0, 0.8631338788556729, 0.0654112592200773, 0.0, 1.0, 0.3988873821929993], 
reward next is 0.6011, 
noisyNet noise sample is [array([-1.0789524], dtype=float32), 0.5133185]. 
=============================================
[2019-04-01 15:43:19,778] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[63.42168 ]
 [62.696125]
 [61.995907]
 [61.560642]
 [61.21258 ]], R is [[63.99060059]
 [63.86885834]
 [63.64411163]
 [63.33349991]
 [62.90919113]].
[2019-04-01 15:43:19,977] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 65215: loss 0.7100
[2019-04-01 15:43:19,978] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 65215: learning rate 0.0010
[2019-04-01 15:43:20,528] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.4279782e-14 2.1716390e-07 1.1928315e-12 4.3099108e-17 4.8186921e-10
 1.0059881e-14 9.9999976e-01], sum to 1.0000
[2019-04-01 15:43:20,528] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3502
[2019-04-01 15:43:20,552] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.583333333333333, 42.33333333333334, 0.0, 0.0, 26.0, 25.13800409049826, 7.019743027504414, 0.0, 1.0, 42.41252364600413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2409000.0000, 
sim time next is 2409600.0000, 
raw observation next is [-3.766666666666667, 42.66666666666667, 0.0, 0.0, 26.0, 25.113562026099, 6.913875465101666, 0.0, 1.0, 42.42357071267462], 
processed observation next is [0.0, 0.9130434782608695, 0.358264081255771, 0.4266666666666667, 0.0, 0.0, 1.0, 0.8733660037284287, 0.06913875465101665, 0.0, 1.0, 0.303025505090533], 
reward next is 0.6970, 
noisyNet noise sample is [array([-0.08923013], dtype=float32), 0.16697831]. 
=============================================
[2019-04-01 15:43:21,391] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.2973069e-18 1.8034865e-11 6.8054816e-15 1.4403300e-22 4.3131396e-15
 4.9927758e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 15:43:21,394] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0482
[2019-04-01 15:43:21,438] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.8, 27.66666666666667, 87.5, 834.1666666666667, 26.0, 25.05094830477805, 7.445392601738004, 0.0, 1.0, 18.29248053908727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2467200.0000, 
sim time next is 2467800.0000, 
raw observation next is [1.9, 27.5, 87.0, 832.0, 26.0, 25.05475993240489, 7.432778130761044, 0.0, 1.0, 17.37155280155612], 
processed observation next is [0.0, 0.5652173913043478, 0.515235457063712, 0.275, 0.29, 0.9193370165745857, 1.0, 0.8649657046292702, 0.07432778130761043, 0.0, 1.0, 0.12408252001111515], 
reward next is 0.8759, 
noisyNet noise sample is [array([0.27427903], dtype=float32), 0.070428185]. 
=============================================
[2019-04-01 15:43:24,679] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.4047061e-13 1.1468273e-07 2.9450878e-12 6.4213078e-17 9.4517665e-09
 9.9535018e-14 9.9999988e-01], sum to 1.0000
[2019-04-01 15:43:24,680] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8765
[2019-04-01 15:43:24,695] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 39.0, 0.0, 0.0, 26.0, 25.14046754267101, 6.395180953971367, 0.0, 1.0, 38.39543205310709], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2507400.0000, 
sim time next is 2508000.0000, 
raw observation next is [-1.7, 39.33333333333333, 0.0, 0.0, 26.0, 25.19530348259497, 6.317532255167959, 0.0, 1.0, 38.73062233042502], 
processed observation next is [1.0, 0.0, 0.4155124653739613, 0.3933333333333333, 0.0, 0.0, 1.0, 0.8850433546564241, 0.06317532255167958, 0.0, 1.0, 0.2766473023601787], 
reward next is 0.7234, 
noisyNet noise sample is [array([-0.6854529], dtype=float32), 0.58081764]. 
=============================================
[2019-04-01 15:43:24,701] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[70.554184]
 [69.34395 ]
 [68.04932 ]
 [66.470566]
 [64.85484 ]], R is [[71.35088348]
 [71.36312103]
 [71.36769104]
 [71.37554169]
 [71.38299561]].
[2019-04-01 15:43:24,909] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.3261588e-15 6.7531147e-11 3.6260769e-14 3.3844744e-18 1.4068411e-11
 6.7855657e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 15:43:24,911] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6973
[2019-04-01 15:43:24,952] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 54.0, 21.0, 5.999999999999998, 26.0, 25.16312852291336, 6.045674347673291, 1.0, 1.0, 29.34469476061024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2533800.0000, 
sim time next is 2534400.0000, 
raw observation next is [-2.8, 54.0, 28.5, 9.0, 26.0, 25.0767768062115, 5.928994641527676, 1.0, 1.0, 27.46259550915381], 
processed observation next is [1.0, 0.34782608695652173, 0.38504155124653744, 0.54, 0.095, 0.009944751381215469, 1.0, 0.8681109723159288, 0.05928994641527676, 1.0, 1.0, 0.19616139649395578], 
reward next is 0.8038, 
noisyNet noise sample is [array([0.01592734], dtype=float32), 1.9408958]. 
=============================================
[2019-04-01 15:43:33,002] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 71088: loss 0.1042
[2019-04-01 15:43:33,004] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 71088: learning rate 0.0010
[2019-04-01 15:43:33,014] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71097: loss 0.0912
[2019-04-01 15:43:33,016] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71097: learning rate 0.0010
[2019-04-01 15:43:33,199] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 71189: loss 0.1028
[2019-04-01 15:43:33,202] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 71189: learning rate 0.0010
[2019-04-01 15:43:33,987] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71559: loss 0.0070
[2019-04-01 15:43:33,989] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71559: learning rate 0.0010
[2019-04-01 15:43:34,245] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 71687: loss 0.0590
[2019-04-01 15:43:34,245] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 71687: learning rate 0.0010
[2019-04-01 15:43:34,281] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 71696: loss 0.0115
[2019-04-01 15:43:34,282] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 71698: learning rate 0.0010
[2019-04-01 15:43:34,294] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71711: loss 0.0169
[2019-04-01 15:43:34,295] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71711: learning rate 0.0010
[2019-04-01 15:43:34,563] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 71836: loss 0.0372
[2019-04-01 15:43:34,563] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 71836: learning rate 0.0010
[2019-04-01 15:43:34,627] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71864: loss 0.0147
[2019-04-01 15:43:34,627] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71864: learning rate 0.0010
[2019-04-01 15:43:34,645] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 71870: loss 0.0149
[2019-04-01 15:43:34,648] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 71873: learning rate 0.0010
[2019-04-01 15:43:34,927] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 71979: loss 0.0005
[2019-04-01 15:43:34,928] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 71979: learning rate 0.0010
[2019-04-01 15:43:35,297] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 72095: loss 0.0159
[2019-04-01 15:43:35,297] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 72095: learning rate 0.0010
[2019-04-01 15:43:35,450] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 72143: loss 0.0220
[2019-04-01 15:43:35,454] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 72143: learning rate 0.0010
[2019-04-01 15:43:35,720] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72218: loss 0.0449
[2019-04-01 15:43:35,720] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72218: learning rate 0.0010
[2019-04-01 15:43:36,544] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5986202e-14 1.7818220e-08 2.3366678e-12 2.5148578e-19 1.8057824e-11
 5.6907062e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 15:43:36,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8561
[2019-04-01 15:43:36,557] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 37.00000000000001, 320.3333333333334, 26.0, 26.22582402860923, 11.59571101711436, 1.0, 1.0, 23.31243497608319], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2739000.0000, 
sim time next is 2739600.0000, 
raw observation next is [-3.0, 50.0, 28.5, 253.5, 26.0, 25.90610188641387, 10.78455319621406, 1.0, 1.0, 22.32003582866617], 
processed observation next is [1.0, 0.7391304347826086, 0.3795013850415513, 0.5, 0.095, 0.28011049723756903, 1.0, 0.9865859837734102, 0.10784553196214061, 1.0, 1.0, 0.15942882734761551], 
reward next is 0.5267, 
noisyNet noise sample is [array([0.29320952], dtype=float32), 0.42876947]. 
=============================================
[2019-04-01 15:43:36,823] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72586: loss 2.2729
[2019-04-01 15:43:36,844] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72592: learning rate 0.0010
[2019-04-01 15:43:37,651] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 72846: loss 0.0336
[2019-04-01 15:43:37,652] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 72846: learning rate 0.0010
[2019-04-01 15:43:44,264] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0517774e-14 1.1125908e-06 1.5500358e-10 6.6944606e-19 1.9320364e-03
 4.7580797e-15 9.9806696e-01], sum to 1.0000
[2019-04-01 15:43:44,264] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1078
[2019-04-01 15:43:44,280] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 25.08404190651526, 7.038089375114126, 0.0, 1.0, 54.63380163240353], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2866200.0000, 
sim time next is 2866800.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 25.07035232508944, 6.93681819806261, 0.0, 1.0, 54.65151210603265], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 1.0, 0.8671931892984917, 0.0693681819806261, 0.0, 1.0, 0.39036794361451893], 
reward next is 0.6096, 
noisyNet noise sample is [array([0.9313414], dtype=float32), -0.18753995]. 
=============================================
[2019-04-01 15:43:45,993] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.4122647e-10 1.8319300e-04 1.0769485e-03 3.6141785e-14 5.9724212e-01
 6.7649726e-11 4.0149781e-01], sum to 1.0000
[2019-04-01 15:43:45,996] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5807
[2019-04-01 15:43:46,003] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.666666666666667, 100.0, 173.1666666666667, 0.0, 26.0, 25.25771974191532, 7.395392134308724, 1.0, 1.0, 14.2699262327333], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2896800.0000, 
sim time next is 2897400.0000, 
raw observation next is [1.833333333333333, 100.0, 171.3333333333333, 0.0, 26.0, 25.24502849965567, 7.405144436156377, 1.0, 1.0, 13.42083739633881], 
processed observation next is [1.0, 0.5217391304347826, 0.5133887349953832, 1.0, 0.5711111111111109, 0.0, 1.0, 0.8921469285222388, 0.07405144436156377, 1.0, 1.0, 0.09586312425956292], 
reward next is 0.9041, 
noisyNet noise sample is [array([-0.4735358], dtype=float32), 0.19590166]. 
=============================================
[2019-04-01 15:43:49,824] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5741238e-11 4.3504905e-08 4.0448941e-10 1.7869644e-15 3.7985772e-04
 9.8544632e-13 9.9962008e-01], sum to 1.0000
[2019-04-01 15:43:49,825] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3128
[2019-04-01 15:43:49,841] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.33736269188342, 10.41402934118624, 0.0, 1.0, 43.30818341288221], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2930400.0000, 
sim time next is 2931000.0000, 
raw observation next is [-1.166666666666667, 79.16666666666667, 0.0, 0.0, 26.0, 25.34658156467701, 9.331882781027659, 0.0, 1.0, 44.03296664673711], 
processed observation next is [1.0, 0.9565217391304348, 0.43028624192059095, 0.7916666666666667, 0.0, 0.0, 1.0, 0.9066545092395728, 0.09331882781027659, 0.0, 1.0, 0.31452119033383646], 
reward next is 0.6855, 
noisyNet noise sample is [array([-0.32933062], dtype=float32), -2.278829]. 
=============================================
[2019-04-01 15:43:49,870] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[49.032898]
 [49.17437 ]
 [49.200546]
 [49.374866]
 [49.418144]], R is [[49.177948  ]
 [49.37682343]
 [49.57291412]
 [49.76605225]
 [49.95605087]].
[2019-04-01 15:43:50,310] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7690310e-09 1.8916745e-06 3.6193444e-06 7.7266952e-12 5.4887217e-01
 1.3655048e-10 4.5112237e-01], sum to 1.0000
[2019-04-01 15:43:50,311] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9158
[2019-04-01 15:43:50,329] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.75924196094709, 7.308336184548466, 0.0, 1.0, 42.6529504046896], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2943600.0000, 
sim time next is 2944200.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.78136317815858, 7.226847424230843, 0.0, 1.0, 42.59088647122472], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 1.0, 0.8259090254512255, 0.07226847424230842, 0.0, 1.0, 0.30422061765160513], 
reward next is 0.6958, 
noisyNet noise sample is [array([-0.02955167], dtype=float32), 1.9964015]. 
=============================================
[2019-04-01 15:43:50,972] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6194957e-10 7.1569316e-06 4.0717350e-07 1.1090024e-12 7.4455386e-04
 7.6878087e-11 9.9924785e-01], sum to 1.0000
[2019-04-01 15:43:50,973] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8933
[2019-04-01 15:43:51,047] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 75.0, 70.0, 42.33333333333334, 26.0, 23.95157474997421, 7.152322086646069, 0.0, 1.0, 106.925897997429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2967600.0000, 
sim time next is 2968200.0000, 
raw observation next is [-4.0, 74.0, 84.0, 49.0, 26.0, 24.85291841853948, 8.04826991921299, 0.0, 1.0, 70.7244064879609], 
processed observation next is [0.0, 0.34782608695652173, 0.3518005540166205, 0.74, 0.28, 0.05414364640883978, 1.0, 0.8361312026484973, 0.0804826991921299, 0.0, 1.0, 0.5051743320568636], 
reward next is 0.4948, 
noisyNet noise sample is [array([-0.5664664], dtype=float32), -0.42541254]. 
=============================================
[2019-04-01 15:43:52,716] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79341: loss -0.2310
[2019-04-01 15:43:52,717] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79341: learning rate 0.0010
[2019-04-01 15:43:52,822] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 79357: loss 0.1278
[2019-04-01 15:43:52,823] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 79357: learning rate 0.0010
[2019-04-01 15:43:53,001] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 79399: loss 0.2144
[2019-04-01 15:43:53,002] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 79399: learning rate 0.0010
[2019-04-01 15:43:53,681] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79609: loss 1.0021
[2019-04-01 15:43:53,682] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79609: learning rate 0.0010
[2019-04-01 15:43:54,176] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 79798: loss 1.9750
[2019-04-01 15:43:54,178] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 79798: learning rate 0.0010
[2019-04-01 15:43:54,448] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 79879: loss 1.2422
[2019-04-01 15:43:54,449] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 79879: learning rate 0.0010
[2019-04-01 15:43:54,530] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 79893: loss -0.8378
[2019-04-01 15:43:54,533] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 79893: learning rate 0.0010
[2019-04-01 15:43:54,720] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 79948: loss 0.8731
[2019-04-01 15:43:54,722] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 79949: learning rate 0.0010
[2019-04-01 15:43:54,802] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79962: loss 0.1356
[2019-04-01 15:43:54,805] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79962: learning rate 0.0010
[2019-04-01 15:43:54,992] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80011: loss 0.5749
[2019-04-01 15:43:54,993] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80011: learning rate 0.0010
[2019-04-01 15:43:55,114] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80041: loss -1.2137
[2019-04-01 15:43:55,115] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80042: learning rate 0.0010
[2019-04-01 15:43:55,716] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 80282: loss -1.4667
[2019-04-01 15:43:55,718] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 80283: learning rate 0.0010
[2019-04-01 15:43:55,872] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 80331: loss 0.8901
[2019-04-01 15:43:55,874] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 80331: learning rate 0.0010
[2019-04-01 15:43:56,071] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 80409: loss 0.6768
[2019-04-01 15:43:56,073] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 80409: learning rate 0.0010
[2019-04-01 15:43:57,601] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 81136: loss 0.9789
[2019-04-01 15:43:57,601] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 81136: learning rate 0.0010
[2019-04-01 15:43:58,496] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 81454: loss 2.1539
[2019-04-01 15:43:58,499] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 81455: learning rate 0.0010
[2019-04-01 15:44:01,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2245668e-15 6.4923417e-10 2.4793650e-12 8.2196462e-19 1.0000000e+00
 1.7589068e-17 4.9469811e-08], sum to 1.0000
[2019-04-01 15:44:01,685] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2208
[2019-04-01 15:44:01,701] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.5, 100.0, 0.0, 0.0, 26.0, 25.47884719393383, 7.269588614694506, 0.0, 1.0, 48.0675177924134], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3130200.0000, 
sim time next is 3130800.0000, 
raw observation next is [3.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.44929427944482, 7.125374945138847, 0.0, 1.0, 46.9889587644201], 
processed observation next is [1.0, 0.21739130434782608, 0.564173591874423, 1.0, 0.0, 0.0, 1.0, 0.921327754206403, 0.07125374945138847, 0.0, 1.0, 0.3356354197458578], 
reward next is 0.6644, 
noisyNet noise sample is [array([0.69151586], dtype=float32), -0.6945546]. 
=============================================
[2019-04-01 15:44:07,144] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9155161e-05 6.2495502e-07 1.9196950e-06 1.2141653e-09 9.9854016e-01
 7.6124793e-06 1.4205840e-03], sum to 1.0000
[2019-04-01 15:44:07,148] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3300
[2019-04-01 15:44:07,182] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.4653916456658, 11.42559541564925, 0.0, 1.0, 28.94377009666231], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3268800.0000, 
sim time next is 3269400.0000, 
raw observation next is [-4.166666666666667, 72.66666666666667, 0.0, 0.0, 26.0, 25.40266125257701, 11.12895698524719, 0.0, 1.0, 24.66663483489127], 
processed observation next is [1.0, 0.8695652173913043, 0.3471837488457987, 0.7266666666666667, 0.0, 0.0, 1.0, 0.9146658932252869, 0.11128956985247189, 0.0, 1.0, 0.17619024882065193], 
reward next is 0.8238, 
noisyNet noise sample is [array([-0.45214903], dtype=float32), 0.36071286]. 
=============================================
[2019-04-01 15:44:08,538] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 86870: loss 0.0205
[2019-04-01 15:44:08,539] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 86870: learning rate 0.0010
[2019-04-01 15:44:08,980] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.0518990e-12 3.0690288e-08 1.0301465e-09 3.2023465e-15 9.9950480e-01
 1.2743670e-11 4.9524417e-04], sum to 1.0000
[2019-04-01 15:44:08,991] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1741
[2019-04-01 15:44:09,006] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.600000000000001, 77.0, 0.0, 0.0, 26.0, 24.68818442302186, 6.962825437232051, 0.0, 1.0, 43.30271755763524], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3296400.0000, 
sim time next is 3297000.0000, 
raw observation next is [-8.75, 77.0, 0.0, 0.0, 26.0, 24.67700339266706, 7.000098252786437, 0.0, 1.0, 43.28348165858011], 
processed observation next is [1.0, 0.13043478260869565, 0.22022160664819945, 0.77, 0.0, 0.0, 1.0, 0.8110004846667229, 0.07000098252786437, 0.0, 1.0, 0.30916772613271504], 
reward next is 0.6908, 
noisyNet noise sample is [array([1.2800988], dtype=float32), 0.94419557]. 
=============================================
[2019-04-01 15:44:09,010] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[61.272564]
 [61.316154]
 [61.35755 ]
 [61.392784]
 [61.81482 ]], R is [[61.31148148]
 [61.38906479]
 [61.46567154]
 [61.54123688]
 [61.61614227]].
[2019-04-01 15:44:09,127] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87146: loss 0.0175
[2019-04-01 15:44:09,129] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87146: learning rate 0.0010
[2019-04-01 15:44:09,147] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 87157: loss 0.0107
[2019-04-01 15:44:09,148] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 87157: learning rate 0.0010
[2019-04-01 15:44:09,393] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8217984e-08 6.8750903e-09 9.6248357e-07 6.1133939e-14 2.6241645e-05
 2.1295570e-08 9.9997282e-01], sum to 1.0000
[2019-04-01 15:44:09,393] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3019
[2019-04-01 15:44:09,430] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 71.0, 80.66666666666667, 656.8333333333334, 26.0, 26.87798872293488, 19.46580942884968, 1.0, 1.0, 9.188524765023065], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3253200.0000, 
sim time next is 3253800.0000, 
raw observation next is [-2.833333333333333, 71.0, 76.33333333333333, 627.6666666666666, 26.0, 26.95145531459538, 15.20648139838302, 1.0, 1.0, 9.847815984326507], 
processed observation next is [1.0, 0.6521739130434783, 0.3841181902123731, 0.71, 0.2544444444444444, 0.6935543278084714, 1.0, 1.1359221877993397, 0.1520648139838302, 1.0, 1.0, 0.07034154274518933], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5514403], dtype=float32), 0.77234787]. 
=============================================
[2019-04-01 15:44:09,758] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87450: loss 0.0053
[2019-04-01 15:44:09,759] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87450: learning rate 0.0010
[2019-04-01 15:44:10,178] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 87636: loss 0.1071
[2019-04-01 15:44:10,181] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 87636: learning rate 0.0010
[2019-04-01 15:44:10,371] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 87724: loss 0.3646
[2019-04-01 15:44:10,371] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 87724: learning rate 0.0010
[2019-04-01 15:44:10,404] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 87745: loss 0.2398
[2019-04-01 15:44:10,405] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 87745: learning rate 0.0010
[2019-04-01 15:44:10,465] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 87770: loss 0.1827
[2019-04-01 15:44:10,466] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 87770: learning rate 0.0010
[2019-04-01 15:44:10,556] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87820: loss 0.2528
[2019-04-01 15:44:10,558] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87820: learning rate 0.0010
[2019-04-01 15:44:10,799] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 87931: loss 0.1576
[2019-04-01 15:44:10,799] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 87931: learning rate 0.0010
[2019-04-01 15:44:10,815] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 87937: loss 0.1716
[2019-04-01 15:44:10,816] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 87937: learning rate 0.0010
[2019-04-01 15:44:11,390] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 88161: loss 0.0979
[2019-04-01 15:44:11,396] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 88162: learning rate 0.0010
[2019-04-01 15:44:11,770] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 88303: loss 0.4156
[2019-04-01 15:44:11,770] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 88303: learning rate 0.0010
[2019-04-01 15:44:12,178] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 88465: loss 0.0389
[2019-04-01 15:44:12,179] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 88466: learning rate 0.0010
[2019-04-01 15:44:12,920] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 88825: loss 0.0439
[2019-04-01 15:44:12,923] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 88826: learning rate 0.0010
[2019-04-01 15:44:13,861] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 89305: loss 0.0526
[2019-04-01 15:44:13,867] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 89306: learning rate 0.0010
[2019-04-01 15:44:18,254] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.30448402e-10 4.93785092e-06 2.05316454e-07 1.23836217e-15
 9.99724209e-01 1.27903035e-11 2.70600023e-04], sum to 1.0000
[2019-04-01 15:44:18,256] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1170
[2019-04-01 15:44:18,271] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 79.00000000000001, 0.0, 0.0, 26.0, 25.48768188356532, 9.673747244692064, 0.0, 1.0, 36.87791847123424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3460200.0000, 
sim time next is 3460800.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.48264924353243, 9.75851395290318, 0.0, 1.0, 32.52937903231497], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.79, 0.0, 0.0, 1.0, 0.9260927490760612, 0.09758513952903179, 0.0, 1.0, 0.23235270737367836], 
reward next is 0.7676, 
noisyNet noise sample is [array([2.1262686], dtype=float32), -0.78572774]. 
=============================================
[2019-04-01 15:44:18,357] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.1713109e-13 4.1302560e-08 1.2111727e-10 1.0450303e-18 9.9259937e-01
 1.7408180e-14 7.4005891e-03], sum to 1.0000
[2019-04-01 15:44:18,358] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2190
[2019-04-01 15:44:18,372] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.36411008802141, 9.539037712938196, 0.0, 1.0, 35.74790434482757], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3450600.0000, 
sim time next is 3451200.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.27181684082818, 9.527803544964426, 0.0, 1.0, 41.87365201557704], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 1.0, 0.8959738344040257, 0.09527803544964426, 0.0, 1.0, 0.2990975143969789], 
reward next is 0.7009, 
noisyNet noise sample is [array([0.2859403], dtype=float32), -1.6534636]. 
=============================================
[2019-04-01 15:44:23,406] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2505977e-12 3.5794895e-10 4.0496655e-08 8.0784581e-16 9.9999869e-01
 4.2099171e-12 1.3008899e-06], sum to 1.0000
[2019-04-01 15:44:23,410] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3774
[2019-04-01 15:44:23,420] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.64250121355965, 6.5677352978301, 0.0, 1.0, 40.3636069639142], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3563400.0000, 
sim time next is 3564000.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.59343485738201, 6.492454700508863, 0.0, 1.0, 40.37093663392864], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 1.0, 0.7990621224831445, 0.06492454700508862, 0.0, 1.0, 0.2883638330994903], 
reward next is 0.7116, 
noisyNet noise sample is [array([-0.4978833], dtype=float32), -0.6628967]. 
=============================================
[2019-04-01 15:44:23,425] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[60.96627 ]
 [61.151936]
 [61.33295 ]
 [61.50795 ]
 [61.655758]], R is [[60.93629456]
 [61.03862   ]
 [61.1400528 ]
 [61.24027252]
 [61.33943176]].
[2019-04-01 15:44:23,915] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 94853: loss 10.4573
[2019-04-01 15:44:23,918] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 94853: learning rate 0.0010
[2019-04-01 15:44:25,023] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 95298: loss 7.8627
[2019-04-01 15:44:25,023] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 95298: learning rate 0.0010
[2019-04-01 15:44:25,111] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 95343: loss 6.4872
[2019-04-01 15:44:25,112] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 95343: learning rate 0.0010
[2019-04-01 15:44:25,693] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95635: loss 3.0657
[2019-04-01 15:44:25,696] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95636: learning rate 0.0010
[2019-04-01 15:44:25,855] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 95727: loss 2.0025
[2019-04-01 15:44:25,857] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 95727: learning rate 0.0010
[2019-04-01 15:44:25,876] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 95738: loss 1.7956
[2019-04-01 15:44:25,883] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 95740: learning rate 0.0010
[2019-04-01 15:44:25,932] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 95770: loss 2.1584
[2019-04-01 15:44:25,933] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 95770: learning rate 0.0010
[2019-04-01 15:44:26,011] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 95818: loss 1.6759
[2019-04-01 15:44:26,012] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 95818: learning rate 0.0010
[2019-04-01 15:44:26,177] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 95902: loss 1.0827
[2019-04-01 15:44:26,179] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 95903: learning rate 0.0010
[2019-04-01 15:44:26,484] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 96057: loss 0.8727
[2019-04-01 15:44:26,485] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 96057: learning rate 0.0010
[2019-04-01 15:44:26,624] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96129: loss 0.8399
[2019-04-01 15:44:26,626] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96129: learning rate 0.0010
[2019-04-01 15:44:27,267] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 96491: loss 0.5305
[2019-04-01 15:44:27,268] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 96491: learning rate 0.0010
[2019-04-01 15:44:27,621] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 96715: loss 0.5569
[2019-04-01 15:44:27,622] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 96715: learning rate 0.0010
[2019-04-01 15:44:27,849] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 96850: loss 0.1067
[2019-04-01 15:44:27,851] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 96851: learning rate 0.0010
[2019-04-01 15:44:28,282] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 97116: loss 0.2152
[2019-04-01 15:44:28,283] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 97116: learning rate 0.0010
[2019-04-01 15:44:28,612] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0912280e-14 7.7521989e-13 1.1689625e-11 2.4696699e-20 1.0000000e+00
 3.0023268e-16 3.1958556e-13], sum to 1.0000
[2019-04-01 15:44:28,617] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8815
[2019-04-01 15:44:28,628] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 30.83333333333334, 0.0, 0.0, 26.0, 25.45580571098145, 8.502538957123088, 0.0, 1.0, 36.13873028963842], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3628200.0000, 
sim time next is 3628800.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.54387879486818, 8.674257447520782, 0.0, 1.0, 29.14724737228046], 
processed observation next is [0.0, 0.0, 0.7119113573407203, 0.25, 0.0, 0.0, 1.0, 0.9348398278383117, 0.08674257447520782, 0.0, 1.0, 0.20819462408771758], 
reward next is 0.7918, 
noisyNet noise sample is [array([-0.29327422], dtype=float32), -0.93350387]. 
=============================================
[2019-04-01 15:44:29,303] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 97748: loss 0.0047
[2019-04-01 15:44:29,305] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 97749: learning rate 0.0010
[2019-04-01 15:44:33,167] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-01 15:44:33,168] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 15:44:33,169] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 15:44:33,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:44:33,170] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 15:44:33,171] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:44:33,172] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:44:33,178] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run2
[2019-04-01 15:44:33,178] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run2
[2019-04-01 15:44:33,203] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run2
[2019-04-01 15:45:39,121] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.10548096], dtype=float32), 0.23139702]
[2019-04-01 15:45:39,121] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.4, 69.0, 0.0, 0.0, 26.0, 24.09616196725724, 5.37028616587838, 0.0, 1.0, 40.94096018852105]
[2019-04-01 15:45:39,122] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 15:45:39,122] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.8063970e-09 2.1074468e-08 4.6131690e-04 5.3931620e-13 9.9952686e-01
 1.5641854e-10 1.1758017e-05], sampled 0.2608438137857042
[2019-04-01 15:46:07,152] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.10548096], dtype=float32), 0.23139702]
[2019-04-01 15:46:07,153] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.03736804333333343, 77.62229018666666, 0.0, 0.0, 26.0, 25.40701853428959, 8.614276290570343, 0.0, 1.0, 38.42268807854928]
[2019-04-01 15:46:07,153] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 15:46:07,155] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.0370968e-10 2.2564965e-09 3.3410270e-05 1.0469407e-14 9.9996567e-01
 7.3865731e-12 9.9125589e-07], sampled 0.650017241570819
[2019-04-01 15:46:14,919] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5512.6725 232612.9723 39656.1388
[2019-04-01 15:46:15,107] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.10548096], dtype=float32), 0.23139702]
[2019-04-01 15:46:15,107] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-7.0, 45.0, 0.0, 0.0, 26.0, 25.54091915998734, 11.29920777389205, 0.0, 1.0, 37.09273492746912]
[2019-04-01 15:46:15,108] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 15:46:15,110] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.4142389e-08 1.3569309e-07 2.7736696e-05 3.4353846e-11 9.9995017e-01
 9.8567354e-10 2.1968166e-05], sampled 0.746537792835632
[2019-04-01 15:46:29,221] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.10548096], dtype=float32), 0.23139702]
[2019-04-01 15:46:29,221] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [6.849762487666666, 39.506981835, 178.3716641666667, 142.2333475666667, 26.0, 27.38314411403221, 22.01485625831166, 1.0, 1.0, 1.629857002634125]
[2019-04-01 15:46:29,221] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 15:46:29,223] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.3957467e-10 5.7255414e-07 4.2431016e-05 1.5393176e-13 9.9408841e-01
 7.2011327e-11 5.8685564e-03], sampled 0.1619516092015314
[2019-04-01 15:46:31,658] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.10548096], dtype=float32), 0.23139702]
[2019-04-01 15:46:31,658] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.558478361666667, 78.03312336666667, 0.0, 0.0, 26.0, 24.13089871113876, 5.660891504156429, 0.0, 1.0, 40.82014647493272]
[2019-04-01 15:46:31,658] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 15:46:31,659] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.45615581e-09 1.80694908e-08 6.26400171e-04 9.20882402e-13
 9.99358952e-01 3.44679923e-10 1.46868415e-05], sampled 0.4757075805227531
[2019-04-01 15:46:34,830] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5400.1451 256309.4234 36327.1289
[2019-04-01 15:46:37,617] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5769.5756 266750.6002 29074.8724
[2019-04-01 15:46:38,640] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 100000, evaluation results [100000.0, 5400.145051897153, 256309.42342959458, 36327.12890316057, 5512.672521851795, 232612.9723011768, 39656.138836446575, 5769.575613439007, 266750.60024520045, 29074.872374167022]
[2019-04-01 15:46:41,210] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1461784e-08 3.2329325e-07 1.5065730e-06 6.2871187e-11 9.8902661e-01
 9.8973967e-09 1.0971524e-02], sum to 1.0000
[2019-04-01 15:46:41,211] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6970
[2019-04-01 15:46:41,240] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.00000000000001, 67.83333333333333, 567.6666666666666, 26.0, 27.15464900845586, 14.19541845861197, 1.0, 1.0, 9.318765247703293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3774000.0000, 
sim time next is 3774600.0000, 
raw observation next is [0.0, 60.0, 64.0, 539.0, 26.0, 27.05553312775933, 16.3647271546499, 1.0, 1.0, 9.169339132815098], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6, 0.21333333333333335, 0.5955801104972376, 1.0, 1.1507904468227612, 0.163647271546499, 1.0, 1.0, 0.06549527952010785], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.0006835], dtype=float32), -0.6506166]. 
=============================================
[2019-04-01 15:46:43,401] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 102517: loss 0.3043
[2019-04-01 15:46:43,402] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 102517: learning rate 0.0010
[2019-04-01 15:46:43,947] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4966575e-09 9.2482549e-10 5.3601445e-07 2.2483204e-14 9.9999845e-01
 9.9731631e-07 3.3688359e-08], sum to 1.0000
[2019-04-01 15:46:43,947] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9955
[2019-04-01 15:46:43,991] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 77.0, 5.0, 149.0, 26.0, 25.36271958864499, 8.061797227448041, 1.0, 1.0, 42.25854597825294], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3828600.0000, 
sim time next is 3829200.0000, 
raw observation next is [-5.0, 77.0, 19.33333333333333, 198.6666666666667, 26.0, 25.36360229271921, 8.068932710763113, 1.0, 1.0, 37.46498825235572], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.77, 0.06444444444444443, 0.21952117863720078, 1.0, 0.9090860418170301, 0.08068932710763113, 1.0, 1.0, 0.267607058945398], 
reward next is 0.7324, 
noisyNet noise sample is [array([0.45602658], dtype=float32), 1.1108091]. 
=============================================
[2019-04-01 15:46:44,283] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 102996: loss 1.4175
[2019-04-01 15:46:44,288] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 102996: learning rate 0.0010
[2019-04-01 15:46:44,403] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 103061: loss 1.6746
[2019-04-01 15:46:44,406] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 103064: learning rate 0.0010
[2019-04-01 15:46:45,159] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103484: loss 1.2595
[2019-04-01 15:46:45,159] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103484: learning rate 0.0010
[2019-04-01 15:46:45,202] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 103505: loss 1.2095
[2019-04-01 15:46:45,205] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 103505: learning rate 0.0010
[2019-04-01 15:46:45,287] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 103551: loss 0.7990
[2019-04-01 15:46:45,293] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 103553: learning rate 0.0010
[2019-04-01 15:46:45,310] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 103561: loss 1.0195
[2019-04-01 15:46:45,314] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 103562: learning rate 0.0010
[2019-04-01 15:46:45,614] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 103720: loss 0.4121
[2019-04-01 15:46:45,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 103721: learning rate 0.0010
[2019-04-01 15:46:45,995] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 103906: loss 0.2958
[2019-04-01 15:46:46,001] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 103906: learning rate 0.0010
[2019-04-01 15:46:46,144] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 103978: loss 0.1289
[2019-04-01 15:46:46,146] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 103979: learning rate 0.0010
[2019-04-01 15:46:46,269] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104037: loss 0.0884
[2019-04-01 15:46:46,271] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104037: learning rate 0.0010
[2019-04-01 15:46:46,961] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 104343: loss 0.0419
[2019-04-01 15:46:46,962] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 104343: learning rate 0.0010
[2019-04-01 15:46:47,159] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 104430: loss 0.0251
[2019-04-01 15:46:47,160] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 104430: learning rate 0.0010
[2019-04-01 15:46:47,397] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 104550: loss 0.0947
[2019-04-01 15:46:47,399] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 104550: learning rate 0.0010
[2019-04-01 15:46:47,628] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9017374e-09 2.9819229e-09 1.2346139e-05 4.3761796e-13 9.9997497e-01
 1.4375281e-06 1.1199395e-05], sum to 1.0000
[2019-04-01 15:46:47,628] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7015
[2019-04-01 15:46:47,642] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 73.0, 0.0, 0.0, 26.0, 25.27472257639913, 8.028094974438746, 0.0, 1.0, 40.08395207982012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3903600.0000, 
sim time next is 3904200.0000, 
raw observation next is [-3.5, 74.0, 0.0, 0.0, 26.0, 25.23678221741062, 8.021774287576049, 0.0, 1.0, 40.20349360376201], 
processed observation next is [1.0, 0.17391304347826086, 0.36565096952908593, 0.74, 0.0, 0.0, 1.0, 0.8909688882015173, 0.08021774287576049, 0.0, 1.0, 0.28716781145544296], 
reward next is 0.7128, 
noisyNet noise sample is [array([1.0556717], dtype=float32), 0.27926257]. 
=============================================
[2019-04-01 15:46:48,198] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 104889: loss 0.0984
[2019-04-01 15:46:48,200] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 104889: learning rate 0.0010
[2019-04-01 15:46:48,804] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 105172: loss 0.2277
[2019-04-01 15:46:48,806] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 105172: learning rate 0.0010
[2019-04-01 15:46:56,167] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5960597e-07 1.0373075e-06 4.9337859e-07 5.6261560e-13 9.6179777e-01
 1.0240254e-06 3.8199469e-02], sum to 1.0000
[2019-04-01 15:46:56,170] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8790
[2019-04-01 15:46:56,192] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 30.0, 121.0, 816.0, 26.0, 26.84987536418101, 11.77189130374126, 1.0, 1.0, 8.339150538576659], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4102200.0000, 
sim time next is 4102800.0000, 
raw observation next is [0.3333333333333333, 29.33333333333333, 120.8333333333333, 820.1666666666666, 26.0, 26.7506008446048, 13.737970640676, 1.0, 1.0, 7.748576887650178], 
processed observation next is [1.0, 0.4782608695652174, 0.4718374884579871, 0.2933333333333333, 0.4027777777777777, 0.9062615101289134, 1.0, 1.1072286920864, 0.13737970640676, 1.0, 1.0, 0.05534697776892984], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2527014], dtype=float32), 0.4405854]. 
=============================================
[2019-04-01 15:47:00,156] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 110825: loss 30.4070
[2019-04-01 15:47:00,158] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 110826: learning rate 0.0010
[2019-04-01 15:47:00,422] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7707416e-08 4.7982658e-09 4.9648435e-07 1.2173539e-14 9.7793204e-01
 9.1105512e-06 2.2058433e-02], sum to 1.0000
[2019-04-01 15:47:00,422] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9264
[2019-04-01 15:47:00,437] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.833333333333333, 53.33333333333333, 0.0, 0.0, 26.0, 25.04441245719698, 7.156128418910463, 0.0, 1.0, 39.02120418794843], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4164600.0000, 
sim time next is 4165200.0000, 
raw observation next is [-4.0, 54.0, 0.0, 0.0, 26.0, 25.01795965817556, 7.059154874909372, 0.0, 1.0, 39.00156419456255], 
processed observation next is [0.0, 0.21739130434782608, 0.3518005540166205, 0.54, 0.0, 0.0, 1.0, 0.8597085225965085, 0.07059154874909372, 0.0, 1.0, 0.2785826013897325], 
reward next is 0.7214, 
noisyNet noise sample is [array([2.2409356], dtype=float32), -0.84534264]. 
=============================================
[2019-04-01 15:47:00,613] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 111091: loss 24.4546
[2019-04-01 15:47:00,614] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 111092: learning rate 0.0010
[2019-04-01 15:47:00,718] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 111148: loss 23.6380
[2019-04-01 15:47:00,720] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 111148: learning rate 0.0010
[2019-04-01 15:47:01,287] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 111469: loss 18.6888
[2019-04-01 15:47:01,288] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 111469: learning rate 0.0010
[2019-04-01 15:47:01,344] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 111502: loss 20.2351
[2019-04-01 15:47:01,344] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 111502: learning rate 0.0010
[2019-04-01 15:47:01,587] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111639: loss 16.1594
[2019-04-01 15:47:01,589] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111640: learning rate 0.0010
[2019-04-01 15:47:01,810] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 111774: loss 14.5247
[2019-04-01 15:47:01,813] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 111775: learning rate 0.0010
[2019-04-01 15:47:02,299] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 112059: loss 10.2294
[2019-04-01 15:47:02,301] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 112061: learning rate 0.0010
[2019-04-01 15:47:02,355] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 112093: loss 9.5278
[2019-04-01 15:47:02,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 112094: learning rate 0.0010
[2019-04-01 15:47:02,414] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112127: loss 9.0406
[2019-04-01 15:47:02,415] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112127: learning rate 0.0010
[2019-04-01 15:47:02,691] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112295: loss 8.4189
[2019-04-01 15:47:02,692] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112295: learning rate 0.0010
[2019-04-01 15:47:03,222] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 112603: loss 6.1685
[2019-04-01 15:47:03,224] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 112604: learning rate 0.0010
[2019-04-01 15:47:03,254] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 112620: loss 5.2475
[2019-04-01 15:47:03,257] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 112621: learning rate 0.0010
[2019-04-01 15:47:03,418] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4701807e-10 4.3956241e-11 2.6895435e-08 9.2751663e-18 5.9693075e-06
 2.1601932e-07 9.9999380e-01], sum to 1.0000
[2019-04-01 15:47:03,421] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9881
[2019-04-01 15:47:03,433] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.34838559028857, 7.27454103790231, 0.0, 1.0, 39.60282260807769], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4249800.0000, 
sim time next is 4250400.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.31190411167161, 7.279133368653227, 0.0, 1.0, 40.53590899625001], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 1.0, 0.9017005873816585, 0.07279133368653228, 0.0, 1.0, 0.2895422071160715], 
reward next is 0.7105, 
noisyNet noise sample is [array([-0.765674], dtype=float32), 0.26493856]. 
=============================================
[2019-04-01 15:47:03,933] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 113015: loss 3.1881
[2019-04-01 15:47:03,935] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 113015: learning rate 0.0010
[2019-04-01 15:47:04,026] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 113066: loss 3.1970
[2019-04-01 15:47:04,027] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 113066: learning rate 0.0010
[2019-04-01 15:47:04,268] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.8662412e-22 2.9826582e-16 2.2830979e-18 1.5922650e-28 4.0398460e-10
 9.9568243e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 15:47:04,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3820
[2019-04-01 15:47:04,280] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 52.0, 154.0, 782.0, 26.0, 25.29265941853144, 9.115307610707916, 0.0, 1.0, 5.25184952259229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4280400.0000, 
sim time next is 4281000.0000, 
raw observation next is [7.0, 52.83333333333334, 165.3333333333333, 760.3333333333333, 26.0, 25.29803655454025, 9.143686338647555, 0.0, 1.0, 4.507829486234789], 
processed observation next is [0.0, 0.5652173913043478, 0.6565096952908588, 0.5283333333333334, 0.551111111111111, 0.840147329650092, 1.0, 0.8997195077914643, 0.09143686338647555, 0.0, 1.0, 0.03219878204453421], 
reward next is 0.9678, 
noisyNet noise sample is [array([-0.12586616], dtype=float32), -1.1640204]. 
=============================================
[2019-04-01 15:47:04,288] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[90.87515 ]
 [90.092476]
 [89.23011 ]
 [88.29872 ]
 [87.253815]], R is [[91.56852722]
 [91.61532593]
 [91.66080475]
 [91.70470428]
 [91.74688721]].
[2019-04-01 15:47:04,966] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5083066e-09 1.4278218e-06 1.6159193e-09 1.7662230e-17 3.6020932e-01
 3.0943353e-10 6.3978916e-01], sum to 1.0000
[2019-04-01 15:47:04,968] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4890
[2019-04-01 15:47:04,982] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 42.0, 187.0, 89.0, 26.0, 25.11607282873212, 8.269854199397313, 0.0, 1.0, 11.02454526531703], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4199400.0000, 
sim time next is 4200000.0000, 
raw observation next is [2.0, 42.66666666666667, 182.5, 163.8333333333333, 26.0, 25.05406625125975, 8.24587390485479, 0.0, 1.0, 11.59458284808834], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.4266666666666667, 0.6083333333333333, 0.1810313075506445, 1.0, 0.8648666073228216, 0.08245873904854789, 0.0, 1.0, 0.08281844891491671], 
reward next is 0.9172, 
noisyNet noise sample is [array([-1.3948972], dtype=float32), 0.124936864]. 
=============================================
[2019-04-01 15:47:05,000] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[90.431435]
 [90.14269 ]
 [89.7634  ]
 [89.377815]
 [89.01176 ]], R is [[90.70817566]
 [90.72235107]
 [90.76184082]
 [90.80045319]
 [90.8367157 ]].
[2019-04-01 15:47:05,002] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 113661: loss 2.9438
[2019-04-01 15:47:05,004] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 113661: learning rate 0.0010
[2019-04-01 15:47:08,345] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2535244e-07 9.3414112e-09 8.6000837e-05 5.6936517e-16 2.1395371e-04
 5.9110353e-06 9.9969399e-01], sum to 1.0000
[2019-04-01 15:47:08,347] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2669
[2019-04-01 15:47:08,370] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.916666666666666, 54.5, 102.0, 615.0, 26.0, 26.50944528415752, 13.01204535446599, 1.0, 1.0, 13.970479307155], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4353000.0000, 
sim time next is 4353600.0000, 
raw observation next is [7.533333333333333, 52.00000000000001, 104.5, 646.0, 26.0, 26.57348410945138, 14.02249461804105, 1.0, 1.0, 13.83666015779265], 
processed observation next is [1.0, 0.391304347826087, 0.6712834718374886, 0.52, 0.34833333333333333, 0.7138121546961326, 1.0, 1.081926301350197, 0.14022494618041048, 1.0, 1.0, 0.09883328684137607], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6069226], dtype=float32), 0.026191864]. 
=============================================
[2019-04-01 15:47:10,212] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4776704e-05 6.3804197e-09 3.3007909e-05 2.2618918e-15 6.6026747e-01
 3.9530937e-06 3.3967081e-01], sum to 1.0000
[2019-04-01 15:47:10,213] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8266
[2019-04-01 15:47:10,220] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.05, 62.5, 0.0, 0.0, 26.0, 25.78348562318465, 12.74690744736088, 0.0, 1.0, 6.767076805795195], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4404600.0000, 
sim time next is 4405200.0000, 
raw observation next is [7.9, 62.66666666666667, 0.0, 0.0, 26.0, 25.71235810852899, 12.37510233192603, 0.0, 1.0, 6.822897335540709], 
processed observation next is [1.0, 1.0, 0.6814404432132966, 0.6266666666666667, 0.0, 0.0, 1.0, 0.958908301218427, 0.1237510233192603, 0.0, 1.0, 0.04873498096814792], 
reward next is 0.9513, 
noisyNet noise sample is [array([1.9622829], dtype=float32), 2.13762]. 
=============================================
[2019-04-01 15:47:13,417] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3319875e-11 4.1807412e-15 1.6147274e-11 6.9173443e-16 2.2281335e-12
 4.1131457e-08 1.0000000e+00], sum to 1.0000
[2019-04-01 15:47:13,421] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5492
[2019-04-01 15:47:13,428] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 135.3333333333333, 0.0, 26.0, 26.61678593420359, 14.56237954534344, 1.0, 1.0, 9.318450079969361], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4448400.0000, 
sim time next is 4449000.0000, 
raw observation next is [1.0, 86.0, 128.6666666666667, 0.0, 26.0, 26.61881385329389, 14.47066818615417, 1.0, 1.0, 9.582491722530163], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.42888888888888904, 0.0, 1.0, 1.0884019790419843, 0.1447066818615417, 1.0, 1.0, 0.06844636944664402], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.29443586], dtype=float32), -1.2134013]. 
=============================================
[2019-04-01 15:47:13,435] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[25.03685 ]
 [24.940147]
 [25.000244]
 [25.139435]
 [25.428408]], R is [[24.99201202]
 [24.74209213]
 [24.49467087]
 [24.24972534]
 [24.00722885]].
[2019-04-01 15:47:13,653] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 118782: loss 0.9296
[2019-04-01 15:47:13,655] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 118782: learning rate 0.0010
[2019-04-01 15:47:13,978] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 118940: loss 0.6714
[2019-04-01 15:47:13,979] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 118941: learning rate 0.0010
[2019-04-01 15:47:14,401] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.04093901e-12 1.84592789e-14 1.08348206e-13 6.30251406e-21
 9.36746801e-05 1.79257123e-10 9.99906301e-01], sum to 1.0000
[2019-04-01 15:47:14,405] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5262
[2019-04-01 15:47:14,420] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5333333333333333, 73.0, 0.0, 0.0, 26.0, 25.44890379282128, 9.866225040840549, 0.0, 1.0, 36.63252888387488], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4494000.0000, 
sim time next is 4494600.0000, 
raw observation next is [-0.55, 73.0, 0.0, 0.0, 26.0, 25.43847763689268, 10.03376086569562, 0.0, 1.0, 38.01060371414271], 
processed observation next is [1.0, 0.0, 0.44736842105263164, 0.73, 0.0, 0.0, 1.0, 0.9197825195560974, 0.10033760865695619, 0.0, 1.0, 0.2715043122438765], 
reward next is 0.7285, 
noisyNet noise sample is [array([-0.9497941], dtype=float32), -0.76759845]. 
=============================================
[2019-04-01 15:47:14,679] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 119302: loss 0.3054
[2019-04-01 15:47:14,680] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 119302: learning rate 0.0010
[2019-04-01 15:47:14,754] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 119334: loss 0.1948
[2019-04-01 15:47:14,754] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 119334: learning rate 0.0010
[2019-04-01 15:47:14,913] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119404: loss 0.2139
[2019-04-01 15:47:14,920] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119404: learning rate 0.0010
[2019-04-01 15:47:15,340] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 119591: loss 0.1201
[2019-04-01 15:47:15,341] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 119591: learning rate 0.0010
[2019-04-01 15:47:16,029] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 119904: loss 3.1383
[2019-04-01 15:47:16,031] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 119904: learning rate 0.0010
[2019-04-01 15:47:16,270] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 120011: loss 3.1949
[2019-04-01 15:47:16,273] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 120011: learning rate 0.0010
[2019-04-01 15:47:16,635] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120193: loss 3.5933
[2019-04-01 15:47:16,636] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120193: learning rate 0.0010
[2019-04-01 15:47:16,711] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120236: loss 2.2818
[2019-04-01 15:47:16,714] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120238: learning rate 0.0010
[2019-04-01 15:47:16,925] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 120361: loss 2.7323
[2019-04-01 15:47:16,927] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 120361: learning rate 0.0010
[2019-04-01 15:47:16,977] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 120392: loss 2.5300
[2019-04-01 15:47:16,986] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 120396: learning rate 0.0010
[2019-04-01 15:47:17,439] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 120623: loss 2.7891
[2019-04-01 15:47:17,439] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 120623: learning rate 0.0010
[2019-04-01 15:47:17,650] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 120737: loss 2.9053
[2019-04-01 15:47:17,652] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 120738: learning rate 0.0010
[2019-04-01 15:47:18,199] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 121038: loss 5.5104
[2019-04-01 15:47:18,201] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 121039: learning rate 0.0010
[2019-04-01 15:47:18,531] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9612762e-15 1.4657766e-15 3.0858475e-13 1.9212470e-20 2.6514249e-16
 3.5601245e-13 1.0000000e+00], sum to 1.0000
[2019-04-01 15:47:18,532] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3463
[2019-04-01 15:47:18,570] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 48.33333333333334, 258.3333333333334, 91.33333333333334, 26.0, 24.83146387710965, 9.105092129583218, 1.0, 1.0, 43.29371180084733], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4543800.0000, 
sim time next is 4544400.0000, 
raw observation next is [3.0, 47.66666666666667, 261.1666666666666, 102.1666666666667, 26.0, 25.20457777484882, 10.81461684351592, 1.0, 1.0, 22.0472562602935], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.47666666666666674, 0.8705555555555552, 0.11289134438305713, 1.0, 0.8863682535498312, 0.10814616843515919, 1.0, 1.0, 0.15748040185923928], 
reward next is 0.5167, 
noisyNet noise sample is [array([0.1887749], dtype=float32), -0.38960227]. 
=============================================
[2019-04-01 15:47:18,586] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 121254: loss 4.8434
[2019-04-01 15:47:18,588] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 121256: learning rate 0.0010
[2019-04-01 15:47:23,028] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.4328868e-07 6.2775143e-13 2.0238959e-08 2.2474177e-15 4.1824987e-06
 1.1856743e-06 9.9999416e-01], sum to 1.0000
[2019-04-01 15:47:23,032] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3443
[2019-04-01 15:47:23,081] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666666, 93.33333333333334, 21.0, 0.0, 26.0, 25.68406732364851, 9.36512863775174, 1.0, 1.0, 21.87554219431284], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4693800.0000, 
sim time next is 4694400.0000, 
raw observation next is [0.0, 92.0, 31.5, 0.0, 26.0, 25.62582382615773, 9.11965369088904, 1.0, 1.0, 20.63441349003898], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.105, 0.0, 1.0, 0.9465462608796759, 0.0911965369088904, 1.0, 1.0, 0.14738866778599272], 
reward next is 0.8526, 
noisyNet noise sample is [array([1.6765089], dtype=float32), -0.6901648]. 
=============================================
[2019-04-01 15:47:26,486] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1873185e-09 8.5657028e-12 4.4006637e-10 1.9375413e-18 1.4867101e-06
 1.1840844e-09 9.9999857e-01], sum to 1.0000
[2019-04-01 15:47:26,486] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8107
[2019-04-01 15:47:26,500] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 56.16666666666666, 0.0, 0.0, 26.0, 25.38467314108185, 11.23498748108816, 0.0, 1.0, 42.94940160213042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4662600.0000, 
sim time next is 4663200.0000, 
raw observation next is [2.0, 55.33333333333334, 0.0, 0.0, 26.0, 25.51936463121024, 11.65053975102663, 0.0, 1.0, 39.01513292660941], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.5533333333333335, 0.0, 0.0, 1.0, 0.9313378044586058, 0.1165053975102663, 0.0, 1.0, 0.27867952090435294], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.73666835], dtype=float32), -0.71878564]. 
=============================================
[2019-04-01 15:47:27,912] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.6392270e-19 2.8225454e-16 1.0240747e-13 1.1605152e-24 2.4992634e-16
 6.7431704e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 15:47:27,912] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0935
[2019-04-01 15:47:27,946] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666666, 88.0, 195.5, 5.0, 26.0, 26.20314712801498, 11.11267944451076, 1.0, 1.0, 17.80490693956496], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4707600.0000, 
sim time next is 4708200.0000, 
raw observation next is [0.8333333333333334, 87.0, 178.0, 4.0, 26.0, 26.18786487168208, 10.97799239752256, 1.0, 1.0, 17.8654638359761], 
processed observation next is [1.0, 0.4782608695652174, 0.4856879039704525, 0.87, 0.5933333333333334, 0.004419889502762431, 1.0, 1.0268378388117259, 0.1097799239752256, 1.0, 1.0, 0.12761045597125786], 
reward next is 0.4812, 
noisyNet noise sample is [array([-1.4203231], dtype=float32), -1.3308475]. 
=============================================
[2019-04-01 15:47:28,675] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5917640e-17 7.9504937e-16 9.5869899e-14 3.6267520e-23 3.9983846e-16
 1.5208147e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 15:47:28,675] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8218
[2019-04-01 15:47:28,693] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 72.0, 147.0, 0.0, 26.0, 26.18194557262398, 11.59721605518062, 1.0, 1.0, 14.58658279776655], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4721400.0000, 
sim time next is 4722000.0000, 
raw observation next is [1.0, 72.0, 139.1666666666667, 1.833333333333333, 26.0, 26.20110172907717, 11.66004615636937, 1.0, 1.0, 13.75099555534412], 
processed observation next is [1.0, 0.6521739130434783, 0.4903047091412743, 0.72, 0.4638888888888891, 0.002025782688766114, 1.0, 1.0287288184395957, 0.1166004615636937, 1.0, 1.0, 0.09822139682388657], 
reward next is 0.2378, 
noisyNet noise sample is [array([0.33066604], dtype=float32), 0.22984244]. 
=============================================
[2019-04-01 15:47:28,713] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[66.99947 ]
 [66.6671  ]
 [66.40132 ]
 [66.14481 ]
 [65.981705]], R is [[66.87182617]
 [66.46003723]
 [66.02876282]
 [65.52689362]
 [65.08678436]].
[2019-04-01 15:47:29,389] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.8467124e-17 1.3459283e-13 5.2260488e-16 6.5803394e-23 1.2277948e-14
 2.2169607e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 15:47:29,391] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4127
[2019-04-01 15:47:29,408] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 39.5, 203.3333333333333, 641.0, 26.0, 25.22257155684831, 9.758731574248763, 0.0, 1.0, 10.92866890301798], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4799400.0000, 
sim time next is 4800000.0000, 
raw observation next is [2.333333333333333, 39.0, 211.6666666666667, 604.5, 26.0, 25.22236531378842, 9.71245094981677, 0.0, 1.0, 10.19801687556705], 
processed observation next is [0.0, 0.5652173913043478, 0.5272391505078486, 0.39, 0.7055555555555557, 0.6679558011049723, 1.0, 0.8889093305412027, 0.0971245094981677, 0.0, 1.0, 0.0728429776826218], 
reward next is 0.9272, 
noisyNet noise sample is [array([0.3294039], dtype=float32), -1.4612857]. 
=============================================
[2019-04-01 15:47:29,427] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[65.62031 ]
 [65.47645 ]
 [65.372375]
 [65.342964]
 [65.31469 ]], R is [[66.07687378]
 [66.33804321]
 [66.59103394]
 [66.83841705]
 [67.06867218]].
[2019-04-01 15:47:29,427] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 126984: loss 1.9372
[2019-04-01 15:47:29,428] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 126984: learning rate 0.0010
[2019-04-01 15:47:29,482] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127006: loss 1.8542
[2019-04-01 15:47:29,483] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127006: learning rate 0.0010
[2019-04-01 15:47:30,215] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 127372: loss 0.8083
[2019-04-01 15:47:30,226] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 127372: learning rate 0.0010
[2019-04-01 15:47:30,433] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 127481: loss 1.7782
[2019-04-01 15:47:30,435] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 127482: learning rate 0.0010
[2019-04-01 15:47:30,465] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 127499: loss 1.6007
[2019-04-01 15:47:30,466] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 127499: learning rate 0.0010
[2019-04-01 15:47:30,997] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 127794: loss 1.0313
[2019-04-01 15:47:30,999] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 127795: learning rate 0.0010
[2019-04-01 15:47:31,344] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 127954: loss 1.6531
[2019-04-01 15:47:31,344] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 127954: learning rate 0.0010
[2019-04-01 15:47:31,600] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 128093: loss 1.4593
[2019-04-01 15:47:31,603] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 128093: learning rate 0.0010
[2019-04-01 15:47:32,002] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 128299: loss 0.8724
[2019-04-01 15:47:32,004] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 128299: learning rate 0.0010
[2019-04-01 15:47:32,228] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 128428: loss 1.0502
[2019-04-01 15:47:32,233] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 128429: learning rate 0.0010
[2019-04-01 15:47:32,300] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 128470: loss 0.8796
[2019-04-01 15:47:32,302] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 128470: learning rate 0.0010
[2019-04-01 15:47:32,374] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.7706369e-27 2.3164939e-20 7.0071132e-23 2.0604456e-32 2.5897536e-19
 1.2469617e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 15:47:32,374] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3537
[2019-04-01 15:47:32,388] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 148.0, 742.0, 26.0, 25.17073693021709, 9.673133549556999, 0.0, 1.0, 5.772446598510465], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4804200.0000, 
sim time next is 4804800.0000, 
raw observation next is [3.0, 37.0, 139.5, 739.5, 26.0, 25.16956043079558, 9.647044048300634, 0.0, 1.0, 5.78820622054134], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.465, 0.8171270718232044, 1.0, 0.8813657758279402, 0.09647044048300633, 0.0, 1.0, 0.041344330146723854], 
reward next is 0.9587, 
noisyNet noise sample is [array([0.25035018], dtype=float32), -0.43813977]. 
=============================================
[2019-04-01 15:47:32,395] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128522: loss 0.5983
[2019-04-01 15:47:32,395] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128522: learning rate 0.0010
[2019-04-01 15:47:32,976] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 128836: loss 0.6263
[2019-04-01 15:47:32,977] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 128836: learning rate 0.0010
[2019-04-01 15:47:33,216] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 128959: loss 0.4171
[2019-04-01 15:47:33,222] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 128959: learning rate 0.0010
[2019-04-01 15:47:33,352] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 129035: loss 0.2074
[2019-04-01 15:47:33,353] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 129035: learning rate 0.0010
[2019-04-01 15:47:34,005] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 129393: loss 0.4223
[2019-04-01 15:47:34,008] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 129393: learning rate 0.0010
[2019-04-01 15:47:44,207] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.9088334e-14 1.9785643e-18 8.3735934e-09 1.6058993e-27 3.3134773e-11
 7.2959894e-13 1.0000000e+00], sum to 1.0000
[2019-04-01 15:47:44,208] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3924
[2019-04-01 15:47:44,222] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.799999999999999, 19.0, 0.0, 0.0, 26.0, 26.75946262591282, 17.54906153698972, 0.0, 1.0, 5.94165209122562], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5089200.0000, 
sim time next is 5089800.0000, 
raw observation next is [8.75, 19.0, 0.0, 0.0, 26.0, 26.71334806102828, 17.22223040124084, 0.0, 1.0, 6.030964083424176], 
processed observation next is [1.0, 0.9130434782608695, 0.7049861495844876, 0.19, 0.0, 0.0, 1.0, 1.101906865861183, 0.1722223040124084, 0.0, 1.0, 0.04307831488160126], 
reward next is 0.9569, 
noisyNet noise sample is [array([1.9340539], dtype=float32), -1.3654674]. 
=============================================
[2019-04-01 15:47:44,823] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:44,823] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:44,826] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run2
[2019-04-01 15:47:45,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:45,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:45,078] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run2
[2019-04-01 15:47:45,474] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:45,474] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:45,494] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run2
[2019-04-01 15:47:45,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:45,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:45,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run2
[2019-04-01 15:47:45,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:45,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:45,567] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run2
[2019-04-01 15:47:45,935] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:45,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:45,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run2
[2019-04-01 15:47:45,958] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:45,959] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:45,962] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run2
[2019-04-01 15:47:46,302] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.95502735e-16 3.96153900e-12 1.17298835e-17 1.35220020e-23
 5.47382660e-06 2.03300952e-14 9.99994516e-01], sum to 1.0000
[2019-04-01 15:47:46,302] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4559
[2019-04-01 15:47:46,322] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:46,323] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:46,324] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.0, 17.0, 18.0, 146.0, 26.0, 28.64688090886668, 31.96389581242339, 1.0, 1.0, 0.861052602521556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5076000.0000, 
sim time next is 5076600.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 26.0, 28.61937477726867, 26.7653446403407, 1.0, 1.0, 1.020215384956107], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.0, 0.0, 1.0, 1.3741963967526674, 0.267653446403407, 1.0, 1.0, 0.007287252749686479], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.23028696], dtype=float32), -1.3957448]. 
=============================================
[2019-04-01 15:47:46,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run2
[2019-04-01 15:47:46,735] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:46,735] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:46,738] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run2
[2019-04-01 15:47:46,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:46,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:46,903] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run2
[2019-04-01 15:47:46,926] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:46,927] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:46,930] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run2
[2019-04-01 15:47:47,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:47,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:47,123] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run2
[2019-04-01 15:47:47,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:47,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:47,247] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run2
[2019-04-01 15:47:47,474] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:47,475] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:47,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run2
[2019-04-01 15:47:47,611] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:47,611] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:47,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run2
[2019-04-01 15:47:48,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:47:48,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:47:48,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run2
[2019-04-01 15:48:16,335] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1041874e-15 1.0413513e-16 1.3159096e-12 1.0844346e-22 6.4095382e-13
 4.2716968e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 15:48:16,335] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8701
[2019-04-01 15:48:16,390] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.66332403045619, 8.042032892540801, 1.0, 1.0, 31.46418064025173], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 239400.0000, 
sim time next is 240000.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.34260646178463, 7.443610694907015, 1.0, 1.0, 29.50952482788474], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 1.0, 0.9060866373978043, 0.07443610694907016, 1.0, 1.0, 0.2107823201991767], 
reward next is 0.7892, 
noisyNet noise sample is [array([1.417543], dtype=float32), -1.1496592]. 
=============================================
[2019-04-01 15:48:16,397] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[51.421013]
 [51.479168]
 [51.537235]
 [51.41223 ]
 [51.31288 ]], R is [[51.58230591]
 [51.84173965]
 [52.0837822 ]
 [52.30407333]
 [52.52416611]].
[2019-04-01 15:48:20,748] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.10585975e-17 1.32741253e-19 2.64103735e-16 1.96102045e-24
 2.97656322e-14 6.50272219e-19 1.00000000e+00], sum to 1.0000
[2019-04-01 15:48:20,748] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3265
[2019-04-01 15:48:20,798] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.6, 60.0, 96.5, 585.0, 26.0, 25.93124512956868, 8.801070486595478, 1.0, 1.0, 26.25140995373532], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 298800.0000, 
sim time next is 299400.0000, 
raw observation next is [-10.6, 58.16666666666666, 99.66666666666666, 609.3333333333334, 26.0, 25.90326495114843, 8.751915424451887, 1.0, 1.0, 24.86233470321279], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.5816666666666666, 0.3322222222222222, 0.6732965009208104, 1.0, 0.9861807073069185, 0.08751915424451888, 1.0, 1.0, 0.1775881050229485], 
reward next is 0.8224, 
noisyNet noise sample is [array([2.0344958], dtype=float32), -0.3131475]. 
=============================================
[2019-04-01 15:48:21,146] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9651752e-18 3.1821390e-17 2.1802050e-15 1.9275122e-24 2.4538621e-15
 1.1649325e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 15:48:21,149] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3157
[2019-04-01 15:48:21,216] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.6, 49.0, 94.5, 708.0, 26.0, 25.54993494081528, 8.347852158180801, 1.0, 1.0, 45.53589246058259], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 302400.0000, 
sim time next is 303000.0000, 
raw observation next is [-10.41666666666667, 48.16666666666667, 90.66666666666667, 724.6666666666666, 26.0, 25.56881608952365, 9.341019976844365, 1.0, 1.0, 61.27296542285788], 
processed observation next is [1.0, 0.5217391304347826, 0.17405355493998145, 0.4816666666666667, 0.3022222222222222, 0.8007366482504603, 1.0, 0.9384022985033786, 0.09341019976844364, 1.0, 1.0, 0.4376640387346991], 
reward next is 0.5623, 
noisyNet noise sample is [array([-2.3497505], dtype=float32), 1.0434479]. 
=============================================
[2019-04-01 15:48:21,225] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[49.45704 ]
 [49.684322]
 [49.910725]
 [50.142925]
 [50.33962 ]], R is [[49.47509766]
 [49.65509033]
 [50.01284409]
 [50.36008072]
 [50.69620895]].
[2019-04-01 15:48:25,327] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0889961e-13 2.4656850e-15 2.8885647e-09 2.3494587e-19 1.2082787e-09
 2.8644418e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 15:48:25,327] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1797
[2019-04-01 15:48:25,370] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.88187131832281, 9.710776902133675, 0.0, 1.0, 48.08490053211118], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 367200.0000, 
sim time next is 367800.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.77844653223652, 9.807365415518666, 0.0, 1.0, 48.18533919715979], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 1.0, 0.3969209331766455, 0.09807365415518665, 0.0, 1.0, 0.3441809942654271], 
reward next is 0.6558, 
noisyNet noise sample is [array([-2.1833894], dtype=float32), 0.050683696]. 
=============================================
[2019-04-01 15:48:29,087] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2485976e-19 6.6459772e-24 4.1057611e-16 1.1868422e-28 2.8815302e-14
 6.7171817e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 15:48:29,088] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1176
[2019-04-01 15:48:29,141] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 42.0, 0.0, 0.0, 26.0, 25.1801466171002, 8.36357600085706, 0.0, 1.0, 53.97720279235784], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 417600.0000, 
sim time next is 418200.0000, 
raw observation next is [-10.1, 42.83333333333334, 0.0, 0.0, 26.0, 25.19712767991038, 8.117580659627713, 0.0, 1.0, 45.41751871926526], 
processed observation next is [1.0, 0.8695652173913043, 0.18282548476454297, 0.42833333333333345, 0.0, 0.0, 1.0, 0.8853039542729115, 0.08117580659627713, 0.0, 1.0, 0.32441084799475184], 
reward next is 0.6756, 
noisyNet noise sample is [array([1.485259], dtype=float32), -0.7384246]. 
=============================================
[2019-04-01 15:48:32,341] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.7957654e-19 2.0144329e-20 1.0050266e-12 1.1012960e-25 2.1916268e-17
 3.2767464e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 15:48:32,342] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3744
[2019-04-01 15:48:32,366] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.7, 49.5, 0.0, 0.0, 26.0, 23.14885024784437, 5.946287776604964, 0.0, 1.0, 45.33523684182401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 443400.0000, 
sim time next is 444000.0000, 
raw observation next is [-10.8, 50.0, 0.0, 0.0, 26.0, 23.19349576313753, 6.025268557119539, 0.0, 1.0, 45.34810424441579], 
processed observation next is [1.0, 0.13043478260869565, 0.1634349030470914, 0.5, 0.0, 0.0, 1.0, 0.5990708233053615, 0.06025268557119538, 0.0, 1.0, 0.3239150303172556], 
reward next is 0.6761, 
noisyNet noise sample is [array([0.0145291], dtype=float32), -1.2332813]. 
=============================================
[2019-04-01 15:48:32,369] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[62.85916 ]
 [62.67347 ]
 [62.740726]
 [62.78202 ]
 [62.869537]], R is [[62.90594864]
 [62.95306778]
 [62.99994278]
 [63.04633713]
 [63.09266281]].
[2019-04-01 15:48:38,782] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.1946837e-14 1.1666236e-15 7.1482126e-10 4.4654122e-21 1.1988186e-09
 4.1006078e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 15:48:38,785] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0284
[2019-04-01 15:48:38,828] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.09999999999999999, 90.33333333333334, 107.3333333333333, 103.3333333333333, 26.0, 25.07731614527082, 7.091801939091283, 0.0, 1.0, 30.23175382682633], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 551400.0000, 
sim time next is 552000.0000, 
raw observation next is [-0.2, 89.66666666666667, 125.6666666666667, 103.1666666666667, 26.0, 25.00662294409385, 6.97173172811402, 0.0, 1.0, 28.4812807146213], 
processed observation next is [0.0, 0.391304347826087, 0.4570637119113574, 0.8966666666666667, 0.418888888888889, 0.11399631675874773, 1.0, 0.8580889920134069, 0.0697173172811402, 0.0, 1.0, 0.20343771939015212], 
reward next is 0.7966, 
noisyNet noise sample is [array([1.3275933], dtype=float32), -0.30472028]. 
=============================================
[2019-04-01 15:48:38,858] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[65.3292  ]
 [65.187675]
 [65.101006]
 [64.94105 ]
 [64.83135 ]], R is [[65.62288666]
 [65.75071716]
 [65.86364746]
 [65.96059418]
 [66.03993225]].
[2019-04-01 15:48:39,634] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.12108498e-12 9.84960202e-09 8.47991455e-09 1.26503395e-17
 8.42016712e-02 1.64770725e-10 9.15798306e-01], sum to 1.0000
[2019-04-01 15:48:39,634] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7049
[2019-04-01 15:48:39,657] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.8, 90.0, 0.0, 0.0, 26.0, 24.64702815091312, 6.051515638684077, 0.0, 1.0, 39.89405669230624], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 541800.0000, 
sim time next is 542400.0000, 
raw observation next is [0.7000000000000001, 90.66666666666666, 0.0, 0.0, 26.0, 24.67304363584293, 6.087561210371916, 0.0, 1.0, 40.2110315100679], 
processed observation next is [0.0, 0.2608695652173913, 0.4819944598337951, 0.9066666666666666, 0.0, 0.0, 1.0, 0.8104348051204185, 0.06087561210371916, 0.0, 1.0, 0.2872216536433421], 
reward next is 0.7128, 
noisyNet noise sample is [array([0.2720155], dtype=float32), -0.69214255]. 
=============================================
[2019-04-01 15:48:39,887] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.23518474e-14 1.98182085e-17 3.01883452e-09 2.95281951e-21
 1.08777385e-07 1.40385809e-14 9.99999881e-01], sum to 1.0000
[2019-04-01 15:48:39,888] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7569
[2019-04-01 15:48:39,997] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 17.5, 54.5, 26.0, 24.40900932115886, 5.760763279969868, 0.0, 1.0, 40.41802389141514], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 547200.0000, 
sim time next is 547800.0000, 
raw observation next is [0.4166666666666667, 91.83333333333334, 23.0, 71.00000000000001, 26.0, 24.39144597930817, 6.412928589098084, 0.0, 1.0, 91.15019032411575], 
processed observation next is [0.0, 0.34782608695652173, 0.47414589104339805, 0.9183333333333334, 0.07666666666666666, 0.07845303867403317, 1.0, 0.7702065684725957, 0.06412928589098084, 0.0, 1.0, 0.6510727880293983], 
reward next is 0.3489, 
noisyNet noise sample is [array([-1.2370876], dtype=float32), -1.1731341]. 
=============================================
[2019-04-01 15:48:55,660] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7031261e-20 1.2855366e-20 3.9335322e-14 2.4247976e-26 7.5215469e-13
 2.4213411e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 15:48:55,661] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7273
[2019-04-01 15:48:55,675] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 74.33333333333333, 0.0, 0.0, 26.0, 23.82831061410312, 5.259741803355934, 0.0, 1.0, 40.87773705503298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 792600.0000, 
sim time next is 793200.0000, 
raw observation next is [-7.300000000000001, 73.66666666666667, 0.0, 0.0, 26.0, 23.8348095204077, 5.250050229443604, 0.0, 1.0, 40.93603407682217], 
processed observation next is [1.0, 0.17391304347826086, 0.26038781163434904, 0.7366666666666667, 0.0, 0.0, 1.0, 0.690687074343957, 0.05250050229443604, 0.0, 1.0, 0.2924002434058726], 
reward next is 0.7076, 
noisyNet noise sample is [array([-0.15844353], dtype=float32), 0.29011127]. 
=============================================
[2019-04-01 15:48:57,948] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2205498e-22 5.0458245e-22 6.2177119e-19 1.8836666e-31 1.7176038e-18
 4.2779969e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 15:48:57,948] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2548
[2019-04-01 15:48:58,004] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 72.33333333333334, 0.0, 0.0, 26.0, 25.0289955981718, 6.753361578464848, 1.0, 1.0, 61.29320839916301], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 805200.0000, 
sim time next is 805800.0000, 
raw observation next is [-6.7, 73.66666666666666, 10.66666666666666, 0.0, 26.0, 25.26268818479034, 7.026568243400479, 1.0, 1.0, 49.16647911963887], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.7366666666666666, 0.035555555555555535, 0.0, 1.0, 0.894669740684334, 0.07026568243400479, 1.0, 1.0, 0.3511891365688491], 
reward next is 0.6488, 
noisyNet noise sample is [array([1.0349824], dtype=float32), -0.79739237]. 
=============================================
[2019-04-01 15:48:59,701] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4713688e-18 1.2828651e-15 1.8058063e-15 1.1981025e-25 3.1053115e-11
 2.0321658e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 15:48:59,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3287
[2019-04-01 15:48:59,726] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.87380647018962, 7.169795247602043, 0.0, 1.0, 41.29201063329602], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 855600.0000, 
sim time next is 856200.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.87421745999787, 7.109317734678004, 0.0, 1.0, 41.20251896640954], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 1.0, 0.8391739228568385, 0.07109317734678004, 0.0, 1.0, 0.2943037069029253], 
reward next is 0.7057, 
noisyNet noise sample is [array([-0.01509427], dtype=float32), -0.39439505]. 
=============================================
[2019-04-01 15:49:01,679] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8217927e-19 6.4099218e-19 9.7726893e-09 2.2983073e-29 1.3125875e-13
 1.8701478e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 15:49:01,679] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5589
[2019-04-01 15:49:01,733] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 84.66666666666667, 0.0, 0.0, 26.0, 26.39356590335213, 9.863495826896282, 1.0, 1.0, 29.38500018078838], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 840000.0000, 
sim time next is 840600.0000, 
raw observation next is [-3.9, 84.0, 0.0, 0.0, 26.0, 26.14720750125685, 9.204131452363724, 1.0, 1.0, 27.61046300686975], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.84, 0.0, 0.0, 1.0, 1.021029643036693, 0.09204131452363723, 1.0, 1.0, 0.1972175929062125], 
reward next is 0.8028, 
noisyNet noise sample is [array([0.98058736], dtype=float32), -0.66331583]. 
=============================================
[2019-04-01 15:49:04,644] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4034744e-23 3.5783630e-24 8.7409697e-17 6.5260378e-32 6.4127737e-22
 3.1235144e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 15:49:04,644] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0274
[2019-04-01 15:49:04,669] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4, 72.0, 0.0, 0.0, 26.0, 24.593343326568, 5.839252132402369, 0.0, 1.0, 38.48824655041777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 883200.0000, 
sim time next is 883800.0000, 
raw observation next is [-0.3, 72.0, 0.0, 0.0, 26.0, 24.62066423118453, 5.771085079450074, 0.0, 1.0, 38.46418860935103], 
processed observation next is [1.0, 0.21739130434782608, 0.4542936288088643, 0.72, 0.0, 0.0, 1.0, 0.8029520330263615, 0.057710850794500744, 0.0, 1.0, 0.27474420435250735], 
reward next is 0.7253, 
noisyNet noise sample is [array([-1.1788546], dtype=float32), -0.7329434]. 
=============================================
[2019-04-01 15:49:06,986] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.55317253e-21 1.25263105e-23 6.15816312e-17 9.77456996e-32
 9.69565208e-14 5.54945973e-20 1.00000000e+00], sum to 1.0000
[2019-04-01 15:49:06,986] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8250
[2019-04-01 15:49:07,003] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.5, 89.0, 0.0, 0.0, 26.0, 25.35418132308087, 9.255442620392229, 0.0, 1.0, 36.47702368114663], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 954000.0000, 
sim time next is 954600.0000, 
raw observation next is [5.683333333333334, 87.83333333333334, 0.0, 0.0, 26.0, 25.35514518045317, 9.3049589516723, 0.0, 1.0, 37.41547564986951], 
processed observation next is [1.0, 0.043478260869565216, 0.6200369344413666, 0.8783333333333334, 0.0, 0.0, 1.0, 0.9078778829218815, 0.09304958951672299, 0.0, 1.0, 0.2672533974990679], 
reward next is 0.7327, 
noisyNet noise sample is [array([-1.3561658], dtype=float32), -0.34954306]. 
=============================================
[2019-04-01 15:49:17,210] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5897510e-21 8.9353439e-21 1.0802642e-15 2.0373062e-29 3.7894257e-11
 1.5340739e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 15:49:17,214] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6351
[2019-04-01 15:49:17,221] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 9.333333333333332, 0.0, 26.0, 23.47447491554952, 5.914874646858865, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1237800.0000, 
sim time next is 1238400.0000, 
raw observation next is [15.0, 96.0, 14.0, 0.0, 26.0, 23.45944498983525, 5.906097499015705, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.96, 0.04666666666666667, 0.0, 1.0, 0.6370635699764645, 0.05906097499015705, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5270673], dtype=float32), -1.2819613]. 
=============================================
[2019-04-01 15:49:20,728] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0317309e-24 5.6740734e-25 7.2901707e-14 3.1053939e-30 1.3584300e-19
 3.7994609e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 15:49:20,728] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5439
[2019-04-01 15:49:20,735] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.51792318952888, 5.941358314993178, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1233600.0000, 
sim time next is 1234200.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.49679877177203, 5.918434127100599, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 1.0, 0.6423998245388616, 0.05918434127100599, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([3.0618167], dtype=float32), -0.5057854]. 
=============================================
[2019-04-01 15:49:22,521] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6932594e-10 6.1293303e-14 1.4032835e-12 1.5091386e-18 9.2059621e-05
 8.6217783e-10 9.9990797e-01], sum to 1.0000
[2019-04-01 15:49:22,521] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2544
[2019-04-01 15:49:22,548] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 92.0, 0.0, 0.0, 26.0, 25.37046494789811, 11.04110210106588, 0.0, 1.0, 35.49988945463649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1310400.0000, 
sim time next is 1311000.0000, 
raw observation next is [2.1, 92.0, 0.0, 0.0, 26.0, 25.33301613792288, 11.21294499143531, 0.0, 1.0, 40.65427034030917], 
processed observation next is [1.0, 0.17391304347826086, 0.5207756232686982, 0.92, 0.0, 0.0, 1.0, 0.9047165911318399, 0.11212944991435309, 0.0, 1.0, 0.2903876452879226], 
reward next is 0.7096, 
noisyNet noise sample is [array([-1.226075], dtype=float32), 0.017453652]. 
=============================================
[2019-04-01 15:49:22,568] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[58.039574]
 [58.204105]
 [58.384804]
 [58.558197]
 [58.731884]], R is [[58.02613068]
 [58.19229889]
 [58.3820076 ]
 [58.58511353]
 [58.81693268]].
[2019-04-01 15:49:24,459] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9509566e-11 2.6740645e-17 2.0920165e-08 2.6467014e-19 3.9343226e-09
 1.9913852e-13 1.0000000e+00], sum to 1.0000
[2019-04-01 15:49:24,460] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8857
[2019-04-01 15:49:24,475] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.55636830745284, 11.06189871831916, 0.0, 1.0, 25.99213671103443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1315800.0000, 
sim time next is 1316400.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.48175610858922, 11.01415586383457, 0.0, 1.0, 29.75084963005577], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 1.0, 0.9259651583698886, 0.11014155863834571, 0.0, 1.0, 0.21250606878611267], 
reward next is 0.7875, 
noisyNet noise sample is [array([-1.0203886], dtype=float32), 1.3773754]. 
=============================================
[2019-04-01 15:49:25,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7902984e-12 1.6002065e-19 2.3456534e-11 2.5358660e-21 4.5274429e-10
 3.3226441e-13 1.0000000e+00], sum to 1.0000
[2019-04-01 15:49:25,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9354
[2019-04-01 15:49:26,011] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.34347094536225, 10.70180948012691, 0.0, 1.0, 40.20449058089913], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1382400.0000, 
sim time next is 1383000.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.33800872449854, 10.60591004749954, 0.0, 1.0, 40.0409545653518], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.9054298177855057, 0.1060591004749954, 0.0, 1.0, 0.2860068183239414], 
reward next is 0.7140, 
noisyNet noise sample is [array([-0.010241], dtype=float32), 0.25952545]. 
=============================================
[2019-04-01 15:49:26,019] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[59.69378]
 [60.19387]
 [60.38743]
 [60.54927]
 [60.92141]], R is [[59.26930618]
 [59.38943863]
 [59.50683212]
 [59.6215477 ]
 [59.73341751]].
[2019-04-01 15:49:29,129] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0771568e-14 2.8111007e-22 1.4526051e-15 7.8844521e-26 1.1152376e-06
 4.6737982e-15 9.9999893e-01], sum to 1.0000
[2019-04-01 15:49:29,132] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6779
[2019-04-01 15:49:29,164] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.08155749590008, 8.749124815182817, 0.0, 1.0, 38.21513447283884], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1407000.0000, 
sim time next is 1407600.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.09105323256389, 8.898726693264203, 0.0, 1.0, 38.22417880604171], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 1.0, 0.8701504617948415, 0.08898726693264203, 0.0, 1.0, 0.2730298486145836], 
reward next is 0.7270, 
noisyNet noise sample is [array([-1.2429011], dtype=float32), -0.56412387]. 
=============================================
[2019-04-01 15:49:29,503] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0721213e-21 2.3087800e-23 4.1205238e-17 2.6780498e-29 6.6004718e-17
 9.3817615e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 15:49:29,505] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0259
[2019-04-01 15:49:29,518] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3, 97.5, 0.0, 0.0, 26.0, 25.53397534772411, 9.912371269646933, 0.0, 1.0, 31.34089582804438], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1395000.0000, 
sim time next is 1395600.0000, 
raw observation next is [-0.4, 98.33333333333333, 0.0, 0.0, 26.0, 25.40908795429077, 9.676185755529922, 0.0, 1.0, 27.55850533875291], 
processed observation next is [1.0, 0.13043478260869565, 0.45152354570637127, 0.9833333333333333, 0.0, 0.0, 1.0, 0.9155839934701098, 0.09676185755529922, 0.0, 1.0, 0.19684646670537792], 
reward next is 0.8032, 
noisyNet noise sample is [array([-0.03874112], dtype=float32), -0.85978174]. 
=============================================
[2019-04-01 15:49:29,576] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8737721e-13 4.6721644e-22 1.8960775e-12 6.8757235e-24 1.7432431e-09
 2.7284331e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 15:49:29,576] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0190
[2019-04-01 15:49:29,621] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 9.0, 0.0, 26.0, 25.40560017817396, 9.195153138427843, 1.0, 1.0, 24.37822839935649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1411200.0000, 
sim time next is 1411800.0000, 
raw observation next is [-0.6, 100.0, 12.0, 0.0, 26.0, 25.33487052226586, 8.945282741460204, 1.0, 1.0, 23.0602843336919], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.04, 0.0, 1.0, 0.904981503180837, 0.08945282741460205, 1.0, 1.0, 0.16471631666922787], 
reward next is 0.8353, 
noisyNet noise sample is [array([0.79032177], dtype=float32), -0.5175429]. 
=============================================
[2019-04-01 15:49:31,154] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.27074048e-16 3.89785670e-17 1.23492341e-17 2.71007815e-25
 1.55109129e-12 1.12811405e-17 1.00000000e+00], sum to 1.0000
[2019-04-01 15:49:31,155] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2917
[2019-04-01 15:49:31,180] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.266666666666667, 91.0, 0.0, 0.0, 26.0, 24.76738721178971, 8.97929222111643, 0.0, 1.0, 41.96571749314558], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1455600.0000, 
sim time next is 1456200.0000, 
raw observation next is [1.35, 90.5, 0.0, 0.0, 26.0, 24.78378825000871, 9.853798070289885, 0.0, 1.0, 51.19682635422866], 
processed observation next is [1.0, 0.8695652173913043, 0.5000000000000001, 0.905, 0.0, 0.0, 1.0, 0.8262554642869583, 0.09853798070289885, 0.0, 1.0, 0.365691616815919], 
reward next is 0.6343, 
noisyNet noise sample is [array([-0.3645936], dtype=float32), -0.33321697]. 
=============================================
[2019-04-01 15:49:35,945] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8026179e-21 8.3919856e-23 8.3845424e-20 2.2951433e-30 9.8584288e-14
 9.6337719e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 15:49:35,950] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7034
[2019-04-01 15:49:35,956] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.11666666666667, 51.5, 59.33333333333333, 24.66666666666667, 26.0, 27.28322162103179, 20.04820632765774, 1.0, 1.0, 6.108590808475785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1613400.0000, 
sim time next is 1614000.0000, 
raw observation next is [12.93333333333334, 52.00000000000001, 54.66666666666667, 30.83333333333334, 26.0, 27.34814570387388, 20.40929790530626, 1.0, 1.0, 6.197633314003916], 
processed observation next is [1.0, 0.6956521739130435, 0.8208679593721148, 0.52, 0.18222222222222223, 0.03406998158379374, 1.0, 1.192592243410554, 0.20409297905306262, 1.0, 1.0, 0.04426880938574226], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4000202], dtype=float32), 1.1176262]. 
=============================================
[2019-04-01 15:49:35,963] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.021515]
 [76.97745 ]
 [76.90022 ]
 [76.7901  ]
 [76.805305]], R is [[76.25730133]
 [75.49472809]
 [74.73978424]
 [73.99238586]
 [73.25246429]].
[2019-04-01 15:49:42,699] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4080884e-20 4.2985594e-20 6.4061251e-14 9.4054807e-27 7.5068525e-14
 2.0380483e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 15:49:42,699] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0530
[2019-04-01 15:49:42,740] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.15832657596884, 10.11290334345738, 0.0, 1.0, 23.52965511657481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1713000.0000, 
sim time next is 1713600.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.12379061441877, 9.866358408567834, 0.0, 1.0, 22.26902781045458], 
processed observation next is [1.0, 0.8695652173913043, 0.49307479224376743, 0.88, 0.0, 0.0, 1.0, 0.8748272306312528, 0.09866358408567834, 0.0, 1.0, 0.15906448436038986], 
reward next is 0.8409, 
noisyNet noise sample is [array([0.7563895], dtype=float32), -1.9767104]. 
=============================================
[2019-04-01 15:49:43,622] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6845579e-12 1.1752360e-14 1.3001814e-09 2.6351651e-20 1.4684129e-11
 1.9801071e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 15:49:43,623] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3350
[2019-04-01 15:49:43,641] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 84.33333333333333, 0.0, 0.0, 26.0, 24.90652720607745, 8.018425531215629, 0.0, 1.0, 42.9138001338893], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1744800.0000, 
sim time next is 1745400.0000, 
raw observation next is [-0.6, 83.66666666666667, 0.0, 0.0, 26.0, 24.88860903597416, 7.952124856823498, 0.0, 1.0, 42.96119684571148], 
processed observation next is [0.0, 0.17391304347826086, 0.44598337950138506, 0.8366666666666667, 0.0, 0.0, 1.0, 0.8412298622820228, 0.07952124856823498, 0.0, 1.0, 0.306865691755082], 
reward next is 0.6931, 
noisyNet noise sample is [array([-0.7190755], dtype=float32), 0.014684608]. 
=============================================
[2019-04-01 15:50:03,721] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.5089503e-17 7.8440116e-21 3.0663043e-13 4.4494051e-24 3.4634076e-10
 7.8006446e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 15:50:03,721] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8740
[2019-04-01 15:50:03,735] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.35585277473367, 5.680336664644469, 0.0, 1.0, 41.01500710184897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1996800.0000, 
sim time next is 1997400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.38100621139853, 5.665139002819281, 0.0, 1.0, 40.95466572351903], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 1.0, 0.7687151730569332, 0.056651390028192805, 0.0, 1.0, 0.2925333265965645], 
reward next is 0.7075, 
noisyNet noise sample is [array([1.4657913], dtype=float32), -0.08028269]. 
=============================================
[2019-04-01 15:50:14,478] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3579360e-14 1.4658926e-17 2.0327484e-15 5.3940027e-22 4.0512715e-11
 9.1490477e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 15:50:14,478] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6732
[2019-04-01 15:50:14,514] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 67.5, 18.0, 0.0, 26.0, 26.10120556137914, 10.10800005979812, 1.0, 1.0, 22.14677849818412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2134200.0000, 
sim time next is 2134800.0000, 
raw observation next is [-4.5, 68.0, 14.0, 0.0, 26.0, 26.12030045800871, 10.05585628524567, 1.0, 1.0, 19.63564533309422], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.68, 0.04666666666666667, 0.0, 1.0, 1.0171857797155301, 0.10055856285245669, 1.0, 1.0, 0.1402546095221016], 
reward next is 0.8374, 
noisyNet noise sample is [array([-0.06733833], dtype=float32), -0.79796]. 
=============================================
[2019-04-01 15:50:18,722] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4902968e-18 2.0843125e-24 4.1812637e-17 1.4052133e-26 2.2008257e-17
 4.4881789e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 15:50:18,726] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2990
[2019-04-01 15:50:18,782] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 75.0, 0.0, 0.0, 26.0, 24.77756334068109, 6.795839661113781, 1.0, 1.0, 60.19060535787451], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2187000.0000, 
sim time next is 2187600.0000, 
raw observation next is [-5.6, 75.0, 8.499999999999998, 43.66666666666666, 26.0, 25.16087666673801, 7.222438637191381, 1.0, 1.0, 50.44686976109291], 
processed observation next is [1.0, 0.30434782608695654, 0.30747922437673136, 0.75, 0.02833333333333333, 0.04825046040515653, 1.0, 0.8801252381054299, 0.0722243863719138, 1.0, 1.0, 0.3603347840078065], 
reward next is 0.6397, 
noisyNet noise sample is [array([-0.5543943], dtype=float32), -0.0737766]. 
=============================================
[2019-04-01 15:50:21,678] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7455872e-15 5.4483452e-17 7.1263762e-15 3.6393218e-24 2.8555086e-04
 1.2990087e-14 9.9971443e-01], sum to 1.0000
[2019-04-01 15:50:21,678] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2825
[2019-04-01 15:50:21,722] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 70.5, 13.66666666666666, 0.0, 26.0, 25.19314644543888, 8.1258143118166, 1.0, 1.0, 40.31436498569866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2221800.0000, 
sim time next is 2222400.0000, 
raw observation next is [-4.5, 70.0, 8.333333333333332, 0.0, 26.0, 25.41480168315728, 8.070861604689183, 1.0, 1.0, 33.38367652917241], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.7, 0.027777777777777773, 0.0, 1.0, 0.9164002404510398, 0.08070861604689183, 1.0, 1.0, 0.23845483235123152], 
reward next is 0.7615, 
noisyNet noise sample is [array([0.3771497], dtype=float32), 0.7043433]. 
=============================================
[2019-04-01 15:50:34,450] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-01 15:50:34,453] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 15:50:34,453] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 15:50:34,454] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 15:50:34,455] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:50:34,455] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:50:34,455] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:50:34,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run3
[2019-04-01 15:50:34,475] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run3
[2019-04-01 15:50:34,492] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run3
[2019-04-01 15:50:56,132] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.20563942], dtype=float32), 0.010103554]
[2019-04-01 15:50:56,133] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-12.9, 68.0, 0.0, 0.0, 26.0, 22.53517144247474, 7.907637300583997, 0.0, 1.0, 47.57770205047225]
[2019-04-01 15:50:56,133] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 15:50:56,134] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.3079858e-14 5.0341531e-15 8.3572826e-10 6.2249503e-19 1.8358033e-09
 8.3540305e-14 1.0000000e+00], sampled 0.3923991206677635
[2019-04-01 15:52:16,398] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 15:52:24,924] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.20563942], dtype=float32), 0.010103554]
[2019-04-01 15:52:24,924] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.512355240666666, 41.96092342333333, 0.0, 0.0, 26.0, 25.63862724247237, 8.972207833109808, 0.0, 1.0, 28.77150751402524]
[2019-04-01 15:52:24,925] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 15:52:24,926] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.6092929e-16 1.9659240e-19 2.8187691e-12 1.7795252e-24 3.5493341e-11
 1.3863144e-15 1.0000000e+00], sampled 0.44493239581348065
[2019-04-01 15:52:36,536] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 15:52:40,257] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 15:52:41,280] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 200000, evaluation results [200000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 15:52:45,281] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.8889754e-18 1.6899173e-18 1.2228554e-12 7.8399608e-24 1.1819515e-09
 3.5661930e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 15:52:45,284] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2049
[2019-04-01 15:52:45,299] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.42845031833407, 5.470743754410414, 0.0, 1.0, 43.79813867193882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2436000.0000, 
sim time next is 2436600.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.39252220895266, 5.504066627400296, 0.0, 1.0, 43.7783510571984], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 1.0, 0.6275031727075228, 0.05504066627400296, 0.0, 1.0, 0.31270250755141715], 
reward next is 0.6873, 
noisyNet noise sample is [array([-1.223528], dtype=float32), 0.40306532]. 
=============================================
[2019-04-01 15:52:57,111] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.5372491e-19 1.0488591e-22 2.9629328e-19 1.1605045e-27 2.6400269e-11
 3.9931069e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 15:52:57,113] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8399
[2019-04-01 15:52:57,183] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 62.5, 0.0, 0.0, 26.0, 24.9728802336283, 8.130943845056956, 0.0, 1.0, 37.17053081224953], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2663400.0000, 
sim time next is 2664000.0000, 
raw observation next is [-1.2, 63.0, 0.0, 0.0, 26.0, 24.94118439803374, 8.403819633545451, 0.0, 1.0, 44.58215877033587], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.63, 0.0, 0.0, 1.0, 0.8487406282905344, 0.08403819633545451, 0.0, 1.0, 0.3184439912166848], 
reward next is 0.6816, 
noisyNet noise sample is [array([1.7682812], dtype=float32), -0.4978551]. 
=============================================
[2019-04-01 15:52:57,191] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[57.459732]
 [57.37426 ]
 [57.414352]
 [57.293762]
 [56.869255]], R is [[57.64691162]
 [57.80493927]
 [58.00707626]
 [58.22571182]
 [58.4382019 ]].
[2019-04-01 15:53:01,693] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.7558744e-23 1.7041392e-26 8.3042285e-21 2.7319832e-30 4.5360600e-14
 7.9153056e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 15:53:01,694] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7885
[2019-04-01 15:53:01,754] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.24300781249266, 8.981850069121093, 1.0, 1.0, 47.94662988236941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2747400.0000, 
sim time next is 2748000.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.18637137846456, 8.79807364789989, 0.0, 1.0, 47.98388581355833], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 1.0, 0.8837673397806515, 0.0879807364789989, 0.0, 1.0, 0.34274204152541665], 
reward next is 0.6573, 
noisyNet noise sample is [array([-0.34526813], dtype=float32), 0.16346885]. 
=============================================
[2019-04-01 15:53:01,762] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[65.87207 ]
 [65.6909  ]
 [65.35591 ]
 [64.947556]
 [64.50233 ]], R is [[65.58141327]
 [65.58312225]
 [65.58817291]
 [65.5953064 ]
 [65.60170746]].
[2019-04-01 15:53:05,960] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6077997e-19 8.2582688e-20 1.0004417e-13 3.2027208e-24 1.6792092e-13
 1.8898547e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 15:53:05,963] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5475
[2019-04-01 15:53:05,976] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.69086695661031, 6.148332973030681, 0.0, 1.0, 40.54259565467979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2775000.0000, 
sim time next is 2775600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.63986283654081, 6.087849546124763, 0.0, 1.0, 40.46679788650104], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 1.0, 0.8056946909344015, 0.06087849546124763, 0.0, 1.0, 0.2890485563321503], 
reward next is 0.7110, 
noisyNet noise sample is [array([0.2460332], dtype=float32), -3.1620274]. 
=============================================
[2019-04-01 15:53:14,270] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0586106e-15 3.0827521e-25 1.5554774e-14 2.2607560e-25 1.5852635e-11
 3.6680044e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 15:53:14,271] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8347
[2019-04-01 15:53:14,288] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.00000000000001, 0.0, 0.0, 26.0, 25.0174961236763, 8.29173713051586, 0.0, 1.0, 43.02944142593687], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2934600.0000, 
sim time next is 2935200.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 25.00427949920193, 8.20239540294174, 0.0, 1.0, 42.9585388442202], 
processed observation next is [1.0, 1.0, 0.40720221606648205, 0.85, 0.0, 0.0, 1.0, 0.8577542141717045, 0.0820239540294174, 0.0, 1.0, 0.3068467060301443], 
reward next is 0.6932, 
noisyNet noise sample is [array([0.35809064], dtype=float32), 0.0101735545]. 
=============================================
[2019-04-01 15:53:36,296] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6408407e-15 5.8051418e-21 1.4546972e-15 2.1239518e-27 1.1725237e-08
 1.6557451e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 15:53:36,296] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6566
[2019-04-01 15:53:36,338] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.97685403882014, 11.5146020834643, 1.0, 1.0, 16.63395142001539], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3349800.0000, 
sim time next is 3350400.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.80667709856096, 11.11802271003768, 1.0, 1.0, 14.6925635393672], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 1.0, 0.9723824426515658, 0.1111802271003768, 1.0, 1.0, 0.10494688242405142], 
reward next is 0.4478, 
noisyNet noise sample is [array([-1.3321215], dtype=float32), -0.34387326]. 
=============================================
[2019-04-01 15:53:39,162] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.8790548e-13 2.8249184e-18 2.7932337e-17 1.0769031e-26 1.7066260e-06
 6.9227024e-13 9.9999833e-01], sum to 1.0000
[2019-04-01 15:53:39,162] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1349
[2019-04-01 15:53:39,173] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 61.0, 87.16666666666666, 715.8333333333334, 26.0, 26.62878872147653, 15.6935947159486, 1.0, 1.0, 11.24166159813934], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3424800.0000, 
sim time next is 3425400.0000, 
raw observation next is [2.5, 62.5, 84.0, 704.0, 26.0, 26.7431167983519, 16.18140748919924, 1.0, 1.0, 11.19537319631709], 
processed observation next is [1.0, 0.6521739130434783, 0.5318559556786704, 0.625, 0.28, 0.7779005524861878, 1.0, 1.1061595426217, 0.1618140748919924, 1.0, 1.0, 0.07996695140226494], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7976207], dtype=float32), 1.7245361]. 
=============================================
[2019-04-01 15:54:00,359] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3003524e-09 6.3949453e-13 2.4595908e-09 1.1024239e-15 1.4129927e-09
 1.3665605e-09 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:00,359] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1551
[2019-04-01 15:54:00,390] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 107.1666666666667, 789.0, 26.0, 26.98990333310158, 15.04579376113525, 1.0, 1.0, 8.5591670352233], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3853200.0000, 
sim time next is 3853800.0000, 
raw observation next is [2.0, 48.0, 106.0, 782.0, 26.0, 26.43537397226923, 15.49941614516471, 1.0, 1.0, 8.118041385897], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.35333333333333333, 0.8640883977900552, 1.0, 1.062196281752747, 0.1549941614516471, 1.0, 1.0, 0.05798600989926428], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0153497], dtype=float32), 0.24780038]. 
=============================================
[2019-04-01 15:54:01,820] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3198689e-19 3.1108645e-27 5.0900896e-17 2.1274272e-30 2.3401829e-19
 3.0099351e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:01,821] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4184
[2019-04-01 15:54:01,830] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.83333333333334, 0.0, 0.0, 26.0, 25.81053395630748, 12.19083524600503, 0.0, 1.0, 24.71284189877626], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3880200.0000, 
sim time next is 3880800.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.75852984167291, 11.82664600399126, 0.0, 1.0, 23.40072918289443], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 1.0, 0.96550426309613, 0.1182664600399126, 0.0, 1.0, 0.16714806559210307], 
reward next is 0.8329, 
noisyNet noise sample is [array([-0.1169868], dtype=float32), 1.50555]. 
=============================================
[2019-04-01 15:54:14,958] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9590275e-17 1.1081989e-25 2.9961488e-16 7.3845793e-29 1.9847520e-14
 2.0629250e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:14,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9686
[2019-04-01 15:54:14,971] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 38.0, 0.0, 0.0, 26.0, 25.52867159859136, 11.43291893250016, 1.0, 1.0, 10.95196335530743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4129800.0000, 
sim time next is 4130400.0000, 
raw observation next is [2.333333333333333, 39.0, 0.0, 0.0, 26.0, 25.65483346013281, 11.41320750951408, 0.0, 1.0, 10.43848543335251], 
processed observation next is [1.0, 0.8260869565217391, 0.5272391505078486, 0.39, 0.0, 0.0, 1.0, 0.9506904943046871, 0.11413207509514081, 0.0, 1.0, 0.07456061023823221], 
reward next is 0.9254, 
noisyNet noise sample is [array([1.3087212], dtype=float32), 0.9885781]. 
=============================================
[2019-04-01 15:54:15,837] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.9420747e-18 3.0735608e-23 1.3124910e-15 2.9858062e-27 3.5394413e-16
 3.0153740e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:15,841] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8002
[2019-04-01 15:54:15,867] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 36.66666666666666, 0.0, 0.0, 26.0, 24.79946398668972, 6.102893548101332, 0.0, 1.0, 39.68329094841276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4081200.0000, 
sim time next is 4081800.0000, 
raw observation next is [-4.0, 37.33333333333334, 0.0, 0.0, 26.0, 24.75342580579925, 6.001683910501979, 0.0, 1.0, 39.68610529953863], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.3733333333333334, 0.0, 0.0, 1.0, 0.8219179722570357, 0.06001683910501979, 0.0, 1.0, 0.28347218071099023], 
reward next is 0.7165, 
noisyNet noise sample is [array([0.9843109], dtype=float32), 2.181307]. 
=============================================
[2019-04-01 15:54:16,435] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.7829670e-15 1.3999634e-20 4.5165315e-14 8.1799360e-23 1.7399027e-15
 2.7739762e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:16,436] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8624
[2019-04-01 15:54:16,459] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 49.0, 0.0, 0.0, 26.0, 24.63937113005012, 6.128524709755657, 0.0, 1.0, 39.75746178085465], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4172400.0000, 
sim time next is 4173000.0000, 
raw observation next is [-5.0, 49.83333333333334, 0.0, 0.0, 26.0, 24.59835842297193, 6.298627234990046, 0.0, 1.0, 40.42310313527737], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.4983333333333334, 0.0, 0.0, 1.0, 0.7997654889959901, 0.06298627234990045, 0.0, 1.0, 0.28873645096626693], 
reward next is 0.7113, 
noisyNet noise sample is [array([-0.09129882], dtype=float32), 1.0028303]. 
=============================================
[2019-04-01 15:54:16,467] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[51.596264]
 [51.78891 ]
 [51.94963 ]
 [52.108097]
 [52.26048 ]], R is [[51.60418701]
 [51.80416489]
 [52.00294113]
 [52.20049667]
 [52.3968277 ]].
[2019-04-01 15:54:24,194] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.0549303e-20 2.6278932e-23 1.6356063e-12 5.4438011e-29 1.9537016e-18
 1.6203597e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:24,195] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9592
[2019-04-01 15:54:24,208] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.43430589945664, 7.651014687828742, 0.0, 1.0, 37.7654248908101], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4245000.0000, 
sim time next is 4245600.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.45067800068306, 7.59439262008704, 0.0, 1.0, 36.14292308933373], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 1.0, 0.9215254286690084, 0.0759439262008704, 0.0, 1.0, 0.25816373635238377], 
reward next is 0.7418, 
noisyNet noise sample is [array([-0.6419127], dtype=float32), 0.33880013]. 
=============================================
[2019-04-01 15:54:26,727] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4307861e-28 0.0000000e+00 1.3616158e-26 0.0000000e+00 5.3435453e-35
 4.2504605e-26 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:26,728] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0647
[2019-04-01 15:54:26,749] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.6, 29.0, 116.5, 847.5, 26.0, 26.86396558796802, 22.43639265604389, 1.0, 1.0, 1.569930816956743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4366800.0000, 
sim time next is 4367400.0000, 
raw observation next is [14.58333333333333, 29.33333333333334, 116.0, 845.6666666666666, 26.0, 27.27821674776133, 23.97642114473986, 1.0, 1.0, 1.542080210651041], 
processed observation next is [1.0, 0.5652173913043478, 0.8665743305632503, 0.2933333333333334, 0.38666666666666666, 0.9344383057090239, 1.0, 1.182602392537333, 0.2397642114473986, 1.0, 1.0, 0.011014858647507435], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.0606515], dtype=float32), 1.2367995]. 
=============================================
[2019-04-01 15:54:32,090] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2661262e-16 2.6282266e-21 4.4145807e-15 2.0708148e-25 1.6149127e-18
 3.9985241e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:32,095] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1549
[2019-04-01 15:54:32,129] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 72.16666666666667, 0.0, 0.0, 26.0, 25.26730023054164, 10.08650650727474, 0.0, 1.0, 43.64920853601389], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4489800.0000, 
sim time next is 4490400.0000, 
raw observation next is [-0.3666666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.32020074123933, 10.14158103745491, 0.0, 1.0, 43.31667303033835], 
processed observation next is [1.0, 1.0, 0.4524469067405356, 0.7233333333333334, 0.0, 0.0, 1.0, 0.902885820177047, 0.10141581037454911, 0.0, 1.0, 0.30940480735955966], 
reward next is 0.6906, 
noisyNet noise sample is [array([0.922658], dtype=float32), -1.1660496]. 
=============================================
[2019-04-01 15:54:35,491] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.8150699e-21 6.8932823e-27 7.3584970e-18 1.2380264e-33 8.6469134e-25
 3.1911297e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:35,496] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1292
[2019-04-01 15:54:35,510] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 72.16666666666667, 0.0, 0.0, 26.0, 25.26703345547453, 10.08530826071154, 0.0, 1.0, 43.64899780445426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4489800.0000, 
sim time next is 4490400.0000, 
raw observation next is [-0.3666666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.31991804588139, 10.1402230781304, 0.0, 1.0, 43.31642809801846], 
processed observation next is [1.0, 1.0, 0.4524469067405356, 0.7233333333333334, 0.0, 0.0, 1.0, 0.902845435125913, 0.101402230781304, 0.0, 1.0, 0.309403057842989], 
reward next is 0.6906, 
noisyNet noise sample is [array([0.3239054], dtype=float32), 0.21194763]. 
=============================================
[2019-04-01 15:54:35,970] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.4058724e-24 2.2467326e-26 3.5750803e-21 2.5321879e-32 4.5165693e-20
 6.9244514e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:35,973] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4632
[2019-04-01 15:54:35,993] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7333333333333334, 73.0, 0.0, 0.0, 26.0, 25.34224252421711, 9.274423550719371, 0.0, 1.0, 42.5030991279822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4501200.0000, 
sim time next is 4501800.0000, 
raw observation next is [-0.8, 73.0, 0.0, 0.0, 26.0, 25.34517237233283, 9.156044390139577, 0.0, 1.0, 42.31452532756831], 
processed observation next is [1.0, 0.08695652173913043, 0.4404432132963989, 0.73, 0.0, 0.0, 1.0, 0.906453196047547, 0.09156044390139577, 0.0, 1.0, 0.3022466094826308], 
reward next is 0.6978, 
noisyNet noise sample is [array([-1.9771588], dtype=float32), 0.61289567]. 
=============================================
[2019-04-01 15:54:41,219] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0594418e-15 2.6084070e-16 5.9939866e-14 3.0613695e-22 3.0757784e-15
 1.4163350e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:41,221] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2020
[2019-04-01 15:54:41,232] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.5387777901628, 9.491436178009169, 0.0, 1.0, 37.28989929681656], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4686600.0000, 
sim time next is 4687200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.58742615625623, 9.483970451360095, 0.0, 1.0, 30.28466909456641], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 1.0, 0.9410608794651759, 0.09483970451360095, 0.0, 1.0, 0.21631906496118866], 
reward next is 0.7837, 
noisyNet noise sample is [array([-0.35352013], dtype=float32), -0.13850164]. 
=============================================
[2019-04-01 15:54:42,608] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.4279043e-16 1.7186808e-23 1.7674235e-15 9.5288869e-26 4.5748697e-19
 1.8565742e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:42,609] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9402
[2019-04-01 15:54:42,652] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 83.83333333333334, 126.0, 0.0, 26.0, 25.60604250210823, 8.89958395076433, 1.0, 1.0, 23.81695165161327], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4713000.0000, 
sim time next is 4713600.0000, 
raw observation next is [1.333333333333333, 81.66666666666667, 130.5, 0.0, 26.0, 25.49283076574002, 9.133984347154353, 1.0, 1.0, 25.82153706647554], 
processed observation next is [1.0, 0.5652173913043478, 0.4995383194829178, 0.8166666666666668, 0.435, 0.0, 1.0, 0.9275472522485744, 0.09133984347154353, 1.0, 1.0, 0.18443955047482527], 
reward next is 0.8156, 
noisyNet noise sample is [array([-1.71542], dtype=float32), -0.3606082]. 
=============================================
[2019-04-01 15:54:44,680] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2032984e-18 3.7652703e-25 6.8541846e-20 2.1962393e-31 1.1404546e-24
 3.1994522e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:44,681] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6029
[2019-04-01 15:54:44,701] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 72.0, 100.0, 11.0, 26.0, 25.70098454163568, 10.70611522581598, 1.0, 1.0, 16.84811337058714], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4725000.0000, 
sim time next is 4725600.0000, 
raw observation next is [1.0, 72.0, 88.16666666666667, 13.83333333333333, 26.0, 25.88678137646994, 11.0542375214282, 1.0, 1.0, 15.55437352645359], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.72, 0.2938888888888889, 0.015285451197053403, 1.0, 0.9838259109242772, 0.11054237521428201, 1.0, 1.0, 0.11110266804609707], 
reward next is 0.4672, 
noisyNet noise sample is [array([-0.9383303], dtype=float32), -0.27398005]. 
=============================================
[2019-04-01 15:54:46,331] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7514781e-25 9.8264616e-32 4.3309846e-20 5.7683873e-36 3.6254396e-22
 6.0236746e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:46,331] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4387
[2019-04-01 15:54:46,408] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8333333333333334, 78.0, 0.0, 0.0, 26.0, 24.90465188616207, 9.004763538201473, 1.0, 1.0, 51.46883965058264], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4733400.0000, 
sim time next is 4734000.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 24.96530900016997, 9.705361554415774, 1.0, 1.0, 44.28922059557296], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 1.0, 0.8521870000242815, 0.09705361554415774, 1.0, 1.0, 0.316351575682664], 
reward next is 0.6836, 
noisyNet noise sample is [array([-0.9688806], dtype=float32), 0.7162287]. 
=============================================
[2019-04-01 15:54:46,414] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[68.75599 ]
 [69.26058 ]
 [69.64055 ]
 [69.927826]
 [70.200836]], R is [[68.26542664]
 [68.21513367]
 [68.33300781]
 [68.50948334]
 [68.68380737]].
[2019-04-01 15:54:47,453] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.3143458e-23 2.1756926e-31 5.0921492e-24 1.8649486e-38 5.0956979e-27
 2.0499213e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:47,454] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0427
[2019-04-01 15:54:47,467] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666667, 78.0, 0.0, 0.0, 26.0, 25.24601461189742, 9.225331801364435, 1.0, 1.0, 17.84996132305736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4731000.0000, 
sim time next is 4731600.0000, 
raw observation next is [-0.3333333333333333, 78.0, 0.0, 0.0, 26.0, 25.25117011240968, 8.792375470453992, 1.0, 1.0, 19.68151563460174], 
processed observation next is [1.0, 0.782608695652174, 0.4533702677747, 0.78, 0.0, 0.0, 1.0, 0.8930243017728117, 0.08792375470453992, 1.0, 1.0, 0.14058225453286957], 
reward next is 0.8594, 
noisyNet noise sample is [array([-0.27045336], dtype=float32), -1.3514618]. 
=============================================
[2019-04-01 15:54:48,984] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9986105e-25 7.1923824e-30 5.5375583e-26 4.6315499e-35 2.3597522e-19
 4.0099987e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 15:54:48,995] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0230
[2019-04-01 15:54:49,009] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 81.66666666666667, 0.0, 0.0, 26.0, 25.35395930753482, 9.34189129615239, 0.0, 1.0, 43.04346121984801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4746000.0000, 
sim time next is 4746600.0000, 
raw observation next is [-3.0, 80.5, 0.0, 0.0, 26.0, 25.28993392254386, 9.184136180286936, 0.0, 1.0, 42.27356579096575], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.805, 0.0, 0.0, 1.0, 0.8985619889348371, 0.09184136180286936, 0.0, 1.0, 0.3019540413640411], 
reward next is 0.6980, 
noisyNet noise sample is [array([-0.20967732], dtype=float32), -0.92957115]. 
=============================================
[2019-04-01 15:55:02,058] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.6398698e-24 1.6919306e-32 5.7635894e-22 1.7377607e-37 4.9649424e-30
 4.4584890e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 15:55:02,061] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9663
[2019-04-01 15:55:02,070] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.66666666666667, 17.66666666666667, 0.0, 0.0, 26.0, 27.35529933859481, 22.46550974391646, 0.0, 1.0, 4.031387957895413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5080800.0000, 
sim time next is 5081400.0000, 
raw observation next is [10.5, 18.0, 0.0, 0.0, 26.0, 27.32325891063724, 22.09874520964847, 1.0, 1.0, 3.978654551911632], 
processed observation next is [1.0, 0.8260869565217391, 0.7534626038781165, 0.18, 0.0, 0.0, 1.0, 1.1890369872338913, 0.2209874520964847, 1.0, 1.0, 0.028418961085083086], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.855355], dtype=float32), 1.4442835]. 
=============================================
[2019-04-01 15:55:02,192] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:02,192] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:02,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run3
[2019-04-01 15:55:02,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:02,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:02,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:02,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:02,439] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run3
[2019-04-01 15:55:02,478] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run3
[2019-04-01 15:55:03,143] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:03,143] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:03,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run3
[2019-04-01 15:55:03,398] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:03,399] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:03,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run3
[2019-04-01 15:55:03,418] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:03,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:03,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run3
[2019-04-01 15:55:03,468] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:03,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:03,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run3
[2019-04-01 15:55:03,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:03,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:03,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run3
[2019-04-01 15:55:03,703] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:03,703] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:03,706] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run3
[2019-04-01 15:55:03,750] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:03,750] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:03,752] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run3
[2019-04-01 15:55:04,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:04,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:04,119] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run3
[2019-04-01 15:55:04,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:04,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:04,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run3
[2019-04-01 15:55:04,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:04,496] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:04,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run3
[2019-04-01 15:55:04,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:04,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:04,727] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run3
[2019-04-01 15:55:05,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:05,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:05,302] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run3
[2019-04-01 15:55:05,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 15:55:05,771] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:55:05,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run3
[2019-04-01 15:55:15,920] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.2147054e-25 3.4240535e-27 3.2685900e-26 2.3221400e-34 6.1003524e-21
 8.1890299e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 15:55:15,920] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0869
[2019-04-01 15:55:16,006] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 67.5, 0.0, 26.0, 24.13613238379153, 5.751857649080598, 0.0, 1.0, 54.91931181584702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 39600.0000, 
sim time next is 40200.0000, 
raw observation next is [7.7, 93.0, 70.0, 0.0, 26.0, 24.25195069958828, 5.857838025207296, 0.0, 1.0, 54.71622402106045], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.23333333333333334, 0.0, 1.0, 0.7502786713697545, 0.05857838025207296, 0.0, 1.0, 0.3908301715790032], 
reward next is 0.6092, 
noisyNet noise sample is [array([-0.34530044], dtype=float32), -0.46029487]. 
=============================================
[2019-04-01 15:55:21,739] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.7548518e-17 1.5885326e-22 2.9630896e-20 4.2886492e-28 8.1923311e-16
 2.0309447e-09 1.0000000e+00], sum to 1.0000
[2019-04-01 15:55:21,739] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6324
[2019-04-01 15:55:21,795] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.2, 69.33333333333334, 175.0, 111.3333333333333, 26.0, 25.41917513025707, 8.07235931847257, 1.0, 1.0, 42.09657221904245], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 128400.0000, 
sim time next is 129000.0000, 
raw observation next is [-8.3, 65.16666666666667, 166.0, 209.6666666666666, 26.0, 25.48994391394566, 8.435979118974188, 1.0, 1.0, 43.44422976443731], 
processed observation next is [1.0, 0.4782608695652174, 0.23268698060941828, 0.6516666666666667, 0.5533333333333333, 0.23167587476979734, 1.0, 0.9271348448493798, 0.08435979118974188, 1.0, 1.0, 0.3103159268888379], 
reward next is 0.6897, 
noisyNet noise sample is [array([-1.7095203], dtype=float32), -2.0459561]. 
=============================================
[2019-04-01 15:55:21,803] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[55.989063]
 [55.815342]
 [55.500237]
 [55.16845 ]
 [54.816284]], R is [[56.13077927]
 [56.26878357]
 [56.40766144]
 [56.54816055]
 [56.68893814]].
[2019-04-01 15:55:25,120] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5828198e-19 7.3669158e-20 1.6488248e-19 3.6252958e-25 2.9446293e-11
 7.3762597e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 15:55:25,121] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1744
[2019-04-01 15:55:25,161] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 23.05148194855317, 5.946908700591962, 0.0, 1.0, 43.74834862128924], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 188400.0000, 
sim time next is 189000.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.98745579091843, 6.042144900023698, 0.0, 1.0, 43.84308570383897], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 1.0, 0.5696365415597755, 0.06042144900023698, 0.0, 1.0, 0.3131648978845641], 
reward next is 0.6868, 
noisyNet noise sample is [array([-1.0525719], dtype=float32), 0.100684166]. 
=============================================
[2019-04-01 15:55:25,177] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[51.275818]
 [51.391006]
 [51.522854]
 [51.670624]
 [51.80278 ]], R is [[51.33686447]
 [51.51100922]
 [51.68393326]
 [51.85519028]
 [52.02481842]].
[2019-04-01 15:55:25,779] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1493188e-17 1.8023383e-22 4.2105642e-20 1.3824989e-26 7.6719766e-16
 2.3997506e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 15:55:25,789] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6427
[2019-04-01 15:55:25,803] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.18168351170392, 5.909367016529257, 0.0, 1.0, 44.36441362494914], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 168000.0000, 
sim time next is 168600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.13764824714735, 5.845054229989512, 0.0, 1.0, 44.3180854447818], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 1.0, 0.7339497495924786, 0.058450542299895124, 0.0, 1.0, 0.31655775317701285], 
reward next is 0.6834, 
noisyNet noise sample is [array([0.5373849], dtype=float32), 0.16302477]. 
=============================================
[2019-04-01 15:55:28,942] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6619042e-32 0.0000000e+00 1.4945892e-38 0.0000000e+00 2.5560819e-32
 5.5956351e-26 1.0000000e+00], sum to 1.0000
[2019-04-01 15:55:28,942] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7859
[2019-04-01 15:55:29,004] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 65.0, 139.0, 0.0, 26.0, 25.11741863695639, 6.778289934187636, 1.0, 1.0, 37.76739760142264], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 219600.0000, 
sim time next is 220200.0000, 
raw observation next is [-4.316666666666666, 64.5, 142.3333333333333, 0.0, 26.0, 25.15026447327796, 6.889671606030302, 1.0, 1.0, 39.26266001736965], 
processed observation next is [1.0, 0.5652173913043478, 0.34302862419205915, 0.645, 0.4744444444444443, 0.0, 1.0, 0.8786092104682801, 0.06889671606030302, 1.0, 1.0, 0.28044757155264033], 
reward next is 0.7196, 
noisyNet noise sample is [array([-0.31118825], dtype=float32), -0.8651485]. 
=============================================
[2019-04-01 15:55:31,323] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7064171e-25 3.6111180e-34 2.6895368e-29 0.0000000e+00 1.6333873e-26
 7.6671371e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 15:55:31,323] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4256
[2019-04-01 15:55:31,365] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 67.33333333333333, 149.0, 0.0, 26.0, 25.38714779172564, 6.949649594606861, 1.0, 1.0, 32.88279207751035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 214800.0000, 
sim time next is 215400.0000, 
raw observation next is [-5.2, 66.16666666666667, 145.0, 0.0, 26.0, 25.38157125360935, 6.90897975149116, 1.0, 1.0, 32.4218004727167], 
processed observation next is [1.0, 0.4782608695652174, 0.31855955678670367, 0.6616666666666667, 0.48333333333333334, 0.0, 1.0, 0.9116530362299073, 0.06908979751491161, 1.0, 1.0, 0.23158428909083356], 
reward next is 0.7684, 
noisyNet noise sample is [array([-0.81247723], dtype=float32), 1.1144309]. 
=============================================
[2019-04-01 15:55:40,802] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.3386781e-18 5.2315353e-23 4.6976694e-21 2.5051672e-27 4.0111604e-14
 2.5368687e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 15:55:40,803] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0822
[2019-04-01 15:55:40,825] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.4, 68.5, 0.0, 0.0, 26.0, 23.29149013381418, 5.734665853045975, 0.0, 1.0, 46.96105926911677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 348600.0000, 
sim time next is 349200.0000, 
raw observation next is [-14.5, 69.0, 0.0, 0.0, 26.0, 23.25662747813463, 5.734317671340911, 0.0, 1.0, 47.00733609176093], 
processed observation next is [1.0, 0.043478260869565216, 0.06094182825484763, 0.69, 0.0, 0.0, 1.0, 0.6080896397335184, 0.057343176713409105, 0.0, 1.0, 0.33576668636972096], 
reward next is 0.6642, 
noisyNet noise sample is [array([1.1324488], dtype=float32), -0.32419685]. 
=============================================
[2019-04-01 15:55:42,279] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.43037975e-20 9.50005221e-22 1.07581434e-23 2.15919880e-30
 7.35137830e-16 4.78169413e-18 1.00000000e+00], sum to 1.0000
[2019-04-01 15:55:42,279] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5095
[2019-04-01 15:55:42,348] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.96666666666667, 87.0, 27.33333333333334, 520.5, 26.0, 24.8964931818083, 6.490771132709706, 1.0, 1.0, 79.67198611185052], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 376800.0000, 
sim time next is 377400.0000, 
raw observation next is [-15.78333333333333, 88.5, 29.66666666666666, 564.0, 26.0, 25.28799671175429, 6.727602476858053, 1.0, 1.0, 64.73716850006483], 
processed observation next is [1.0, 0.34782608695652173, 0.02539242843951994, 0.885, 0.09888888888888887, 0.6232044198895028, 1.0, 0.898285244536327, 0.06727602476858052, 1.0, 1.0, 0.46240834642903444], 
reward next is 0.5376, 
noisyNet noise sample is [array([-1.3729471], dtype=float32), -1.0156785]. 
=============================================
[2019-04-01 15:55:45,320] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.5991021e-23 5.6958044e-32 6.0907132e-25 2.3629950e-34 2.5051069e-22
 5.7457379e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 15:55:45,320] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3457
[2019-04-01 15:55:45,409] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.9, 52.5, 58.0, 811.0, 26.0, 26.14791657579588, 9.977910988149775, 1.0, 1.0, 33.33168420736868], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 388200.0000, 
sim time next is 388800.0000, 
raw observation next is [-12.8, 51.0, 58.0, 834.5, 26.0, 26.25862579682494, 9.12644995980542, 1.0, 1.0, 34.74950002232195], 
processed observation next is [1.0, 0.5217391304347826, 0.1080332409972299, 0.51, 0.19333333333333333, 0.9220994475138121, 1.0, 1.0369465424035629, 0.0912644995980542, 1.0, 1.0, 0.2482107144451568], 
reward next is 0.7518, 
noisyNet noise sample is [array([1.0765597], dtype=float32), -2.0096679]. 
=============================================
[2019-04-01 15:55:49,580] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5848445e-21 5.2405780e-25 5.1338109e-24 3.9434424e-32 2.7163765e-21
 4.8201721e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 15:55:49,581] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2198
[2019-04-01 15:55:49,651] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.1, 41.5, 0.0, 0.0, 26.0, 23.88675944879895, 5.574066368829736, 1.0, 1.0, 84.6862737087567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 459000.0000, 
sim time next is 459600.0000, 
raw observation next is [-8.0, 41.0, 0.0, 0.0, 26.0, 24.22004732750866, 5.621567803017181, 1.0, 1.0, 80.6577958072269], 
processed observation next is [1.0, 0.30434782608695654, 0.24099722991689754, 0.41, 0.0, 0.0, 1.0, 0.7457210467869514, 0.056215678030171815, 1.0, 1.0, 0.5761271129087636], 
reward next is 0.4239, 
noisyNet noise sample is [array([1.8385268], dtype=float32), -0.0199871]. 
=============================================
[2019-04-01 15:55:55,532] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.4485123e-21 3.4075251e-23 3.9849384e-24 2.9135686e-36 4.6191918e-23
 3.2186282e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 15:55:55,533] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3033
[2019-04-01 15:55:55,565] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.266666666666667, 96.0, 0.0, 0.0, 26.0, 25.07901690819118, 7.327377578968428, 0.0, 1.0, 45.43670798676929], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 505200.0000, 
sim time next is 505800.0000, 
raw observation next is [1.35, 96.0, 0.0, 0.0, 26.0, 25.05055088199725, 7.303590351930585, 0.0, 1.0, 43.03749071522444], 
processed observation next is [1.0, 0.8695652173913043, 0.5000000000000001, 0.96, 0.0, 0.0, 1.0, 0.8643644117138928, 0.07303590351930585, 0.0, 1.0, 0.3074106479658889], 
reward next is 0.6926, 
noisyNet noise sample is [array([0.6993366], dtype=float32), -0.62304413]. 
=============================================
[2019-04-01 15:55:58,062] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.9405183e-17 5.0784423e-21 5.9837273e-22 1.8548286e-26 1.2038013e-18
 6.4590869e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 15:55:58,062] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4751
[2019-04-01 15:55:58,080] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.8, 90.33333333333334, 0.0, 0.0, 26.0, 24.9274734536117, 6.71347444936962, 0.0, 1.0, 39.07448471453062], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 521400.0000, 
sim time next is 522000.0000, 
raw observation next is [5.0, 89.0, 0.0, 0.0, 26.0, 24.92490041909697, 6.893461438343297, 0.0, 1.0, 39.61157406000461], 
processed observation next is [0.0, 0.043478260869565216, 0.6011080332409973, 0.89, 0.0, 0.0, 1.0, 0.8464143455852816, 0.06893461438343297, 0.0, 1.0, 0.2829398147143186], 
reward next is 0.7171, 
noisyNet noise sample is [array([-0.16636814], dtype=float32), -0.99452645]. 
=============================================
[2019-04-01 15:55:58,117] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[66.93096 ]
 [67.281364]
 [67.386406]
 [67.51255 ]
 [67.52518 ]], R is [[66.86365509]
 [66.91591644]
 [66.96757507]
 [67.01859283]
 [67.06893158]].
[2019-04-01 15:55:58,580] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1813145e-19 3.9321061e-22 1.4093458e-20 1.0583250e-28 2.6599839e-17
 5.1854676e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 15:55:58,581] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0986
[2019-04-01 15:55:58,598] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.066666666666667, 83.33333333333334, 0.0, 0.0, 26.0, 24.78157898863218, 6.343574633808825, 0.0, 1.0, 39.52522903053877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 531600.0000, 
sim time next is 532200.0000, 
raw observation next is [2.883333333333334, 82.66666666666667, 0.0, 0.0, 26.0, 24.76062617296093, 6.304959545570725, 0.0, 1.0, 39.57506717989494], 
processed observation next is [0.0, 0.13043478260869565, 0.5424746075715605, 0.8266666666666667, 0.0, 0.0, 1.0, 0.8229465961372756, 0.06304959545570725, 0.0, 1.0, 0.28267905128496384], 
reward next is 0.7173, 
noisyNet noise sample is [array([0.853227], dtype=float32), 0.4874148]. 
=============================================
[2019-04-01 15:56:14,151] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7180531e-23 1.0994687e-20 1.0768167e-22 1.4340084e-30 2.7379181e-16
 7.0547350e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 15:56:14,152] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1163
[2019-04-01 15:56:14,168] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.716666666666667, 74.16666666666667, 0.0, 0.0, 26.0, 23.90814076361379, 5.300257498331701, 0.0, 1.0, 40.69467868999033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 789000.0000, 
sim time next is 789600.0000, 
raw observation next is [-7.633333333333333, 74.33333333333334, 0.0, 0.0, 26.0, 23.91443314311915, 5.297374134373707, 0.0, 1.0, 40.68402591397992], 
processed observation next is [1.0, 0.13043478260869565, 0.2511542012927055, 0.7433333333333334, 0.0, 0.0, 1.0, 0.7020618775884502, 0.05297374134373707, 0.0, 1.0, 0.2906001850998566], 
reward next is 0.7094, 
noisyNet noise sample is [array([-1.0017941], dtype=float32), 1.6539559]. 
=============================================
[2019-04-01 15:56:20,890] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1095854e-23 1.2319294e-27 1.5143632e-19 2.2700984e-32 1.5614222e-18
 4.5213686e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 15:56:20,893] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8374
[2019-04-01 15:56:20,939] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.19374787413183, 7.654435635961588, 1.0, 1.0, 27.66651821794536], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 841800.0000, 
sim time next is 842400.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.20133934874409, 7.575827093175036, 1.0, 1.0, 29.12377553125531], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 1.0, 0.8859056212491555, 0.07575827093175036, 1.0, 1.0, 0.20802696808039506], 
reward next is 0.7920, 
noisyNet noise sample is [array([-2.092745], dtype=float32), -0.8641251]. 
=============================================
[2019-04-01 15:56:21,782] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3459172e-18 2.9809769e-22 8.9935852e-20 3.1654932e-31 8.5894948e-14
 2.0769871e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 15:56:21,783] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7410
[2019-04-01 15:56:21,810] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3, 72.0, 0.0, 0.0, 26.0, 24.62066423118453, 5.771085079450074, 0.0, 1.0, 38.46418860935103], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 883800.0000, 
sim time next is 884400.0000, 
raw observation next is [-0.2, 72.0, 0.0, 0.0, 26.0, 24.56510715834358, 5.698206422437015, 0.0, 1.0, 38.46266202888978], 
processed observation next is [1.0, 0.21739130434782608, 0.4570637119113574, 0.72, 0.0, 0.0, 1.0, 0.795015308334797, 0.05698206422437015, 0.0, 1.0, 0.27473330020635556], 
reward next is 0.7253, 
noisyNet noise sample is [array([1.0799708], dtype=float32), -0.73638153]. 
=============================================
[2019-04-01 15:56:24,155] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.7781894e-27 1.0469772e-28 4.3897965e-29 1.8852135e-37 2.8452067e-14
 1.1707210e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 15:56:24,158] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0981
[2019-04-01 15:56:24,233] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.8, 93.0, 94.0, 0.0, 26.0, 25.30341718020019, 10.70564646089471, 1.0, 1.0, 48.37809930637062], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 913800.0000, 
sim time next is 914400.0000, 
raw observation next is [3.8, 93.0, 93.0, 0.0, 26.0, 26.64536170847598, 13.00222633539223, 1.0, 1.0, 35.03930186728908], 
processed observation next is [1.0, 0.6086956521739131, 0.5678670360110805, 0.93, 0.31, 0.0, 1.0, 1.092194529782283, 0.1300222633539223, 1.0, 1.0, 0.25028072762349346], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7651402], dtype=float32), -0.016762538]. 
=============================================
[2019-04-01 15:56:24,530] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2873415e-27 6.8082082e-30 3.1531282e-31 0.0000000e+00 1.7477531e-29
 5.2212918e-29 1.0000000e+00], sum to 1.0000
[2019-04-01 15:56:24,530] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3010
[2019-04-01 15:56:24,570] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 84.0, 72.16666666666667, 0.0, 26.0, 25.52793302758882, 7.333201831582701, 1.0, 1.0, 14.17530556040976], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 901200.0000, 
sim time next is 901800.0000, 
raw observation next is [1.1, 84.0, 77.0, 0.0, 26.0, 25.54731424073674, 7.249907769614325, 1.0, 1.0, 13.55956766890328], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.25666666666666665, 0.0, 1.0, 0.9353306058195342, 0.07249907769614325, 1.0, 1.0, 0.09685405477788057], 
reward next is 0.9031, 
noisyNet noise sample is [array([-1.1034274], dtype=float32), -1.6000323]. 
=============================================
[2019-04-01 15:56:27,472] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.8274536e-24 5.7621488e-26 2.7686170e-24 3.0142341e-34 2.6755610e-27
 2.5808331e-23 1.0000000e+00], sum to 1.0000
[2019-04-01 15:56:27,487] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5337
[2019-04-01 15:56:27,529] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.5, 92.83333333333333, 18.0, 0.0, 26.0, 25.70544335001741, 10.18102349551107, 1.0, 1.0, 19.99829185827922], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 979800.0000, 
sim time next is 980400.0000, 
raw observation next is [9.600000000000001, 92.66666666666667, 22.5, 0.0, 26.0, 25.63571071987014, 10.68356078647806, 1.0, 1.0, 20.34454506769276], 
processed observation next is [1.0, 0.34782608695652173, 0.7285318559556788, 0.9266666666666667, 0.075, 0.0, 1.0, 0.9479586742671628, 0.1068356078647806, 1.0, 1.0, 0.1453181790549483], 
reward next is 0.5813, 
noisyNet noise sample is [array([0.8423709], dtype=float32), -0.9200941]. 
=============================================
[2019-04-01 15:56:28,207] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.4145482e-18 1.3456306e-24 6.6823757e-18 2.8600060e-29 1.6778817e-23
 1.5747744e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 15:56:28,211] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3757
[2019-04-01 15:56:28,219] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.8, 86.0, 116.0, 0.0, 26.0, 26.71354261645361, 15.19481741607786, 1.0, 1.0, 6.985697529177862], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 991200.0000, 
sim time next is 991800.0000, 
raw observation next is [11.9, 86.0, 120.0, 0.0, 26.0, 26.73736504632905, 15.27892345466016, 1.0, 1.0, 6.710369867471941], 
processed observation next is [1.0, 0.4782608695652174, 0.7922437673130196, 0.86, 0.4, 0.0, 1.0, 1.1053378637612927, 0.15278923454660162, 1.0, 1.0, 0.047931213339085295], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7522817], dtype=float32), 0.17988902]. 
=============================================
[2019-04-01 15:56:30,600] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-01 15:56:30,601] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 15:56:30,601] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:56:30,602] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 15:56:30,602] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:56:30,603] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 15:56:30,603] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 15:56:30,612] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run4
[2019-04-01 15:56:30,625] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run4
[2019-04-01 15:56:30,639] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run4
[2019-04-01 15:58:09,908] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00814143], dtype=float32), -0.04066509]
[2019-04-01 15:58:09,908] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [14.03333333333333, 73.33333333333333, 0.0, 0.0, 26.0, 26.50391925611804, 21.88369488443057, 0.0, 1.0, 5.084436566877536]
[2019-04-01 15:58:09,908] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 15:58:09,909] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.7398427e-22 4.1933724e-24 1.9319349e-18 2.1581519e-31 1.3573739e-18
 1.0385127e-20 1.0000000e+00], sampled 0.42097353065984977
[2019-04-01 15:58:14,645] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00814143], dtype=float32), -0.04066509]
[2019-04-01 15:58:14,646] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [11.8, 56.0, 240.0, 386.5, 26.0, 27.15051091409422, 20.92913649438593, 1.0, 1.0, 5.336671999979554]
[2019-04-01 15:58:14,646] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 15:58:14,647] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.8405805e-14 1.3257666e-21 3.2743808e-11 2.3583190e-21 2.8161112e-18
 4.4274133e-12 1.0000000e+00], sampled 0.08735948841286645
[2019-04-01 15:58:14,831] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00814143], dtype=float32), -0.04066509]
[2019-04-01 15:58:14,831] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.116369781333334, 50.52160926666667, 0.0, 0.0, 26.0, 25.31299923440877, 9.787350100507574, 0.0, 1.0, 12.03960793373395]
[2019-04-01 15:58:14,832] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 15:58:14,832] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.4609578e-13 1.6675226e-19 3.0000577e-10 7.6316795e-20 1.4378538e-15
 5.3980415e-13 1.0000000e+00], sampled 0.5398268940768051
[2019-04-01 15:58:15,058] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 15:58:34,333] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 15:58:37,929] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 15:58:38,952] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 300000, evaluation results [300000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 15:58:39,544] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0276239e-24 2.0839924e-26 4.8712637e-18 2.5906443e-33 2.8598723e-16
 1.1384260e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 15:58:39,552] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6474
[2019-04-01 15:58:39,561] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 76.0, 0.0, 0.0, 26.0, 25.99000745687566, 14.04846978336218, 0.0, 1.0, 25.27090274952618], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1027800.0000, 
sim time next is 1028400.0000, 
raw observation next is [14.4, 75.66666666666667, 0.0, 0.0, 26.0, 26.01558866885666, 14.0105185357939, 0.0, 1.0, 23.79851890548899], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7566666666666667, 0.0, 0.0, 1.0, 1.0022269526938088, 0.140105185357939, 0.0, 1.0, 0.1699894207534928], 
reward next is 0.8300, 
noisyNet noise sample is [array([-1.0162691], dtype=float32), -2.0070767]. 
=============================================
[2019-04-01 15:58:40,877] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.9001732e-20 2.7364737e-26 5.7931003e-16 8.2492094e-33 3.4769485e-22
 3.2505134e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 15:58:40,881] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2520
[2019-04-01 15:58:40,886] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.83333333333333, 49.33333333333334, 44.5, 0.0, 26.0, 27.82018562383774, 26.62764030290524, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1095600.0000, 
sim time next is 1096200.0000, 
raw observation next is [18.55, 49.5, 35.0, 0.0, 26.0, 27.86425867687166, 26.90442733117889, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.976454293628809, 0.495, 0.11666666666666667, 0.0, 1.0, 1.266322668124523, 0.26904427331178893, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5709556], dtype=float32), -0.6234362]. 
=============================================
[2019-04-01 15:58:41,476] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0954749e-27 2.8020485e-28 1.1038322e-25 3.6653478e-38 1.3681327e-21
 6.5954295e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 15:58:41,480] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2333
[2019-04-01 15:58:41,490] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.65095074154138, 13.1673685629365, 0.0, 1.0, 28.52416496352796], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1048200.0000, 
sim time next is 1048800.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 25.84455891859834, 13.5969650733623, 0.0, 1.0, 22.77641592433755], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 1.0, 0.9777941312283345, 0.135969650733623, 0.0, 1.0, 0.16268868517383964], 
reward next is 0.8373, 
noisyNet noise sample is [array([-0.06642089], dtype=float32), 1.3618175]. 
=============================================
[2019-04-01 15:58:42,736] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2913961e-25 7.1719384e-24 1.7696472e-18 5.7803010e-37 3.4557028e-20
 2.8265853e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 15:58:42,744] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1388
[2019-04-01 15:58:42,750] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.51666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 24.54148286964565, 8.1606801965441, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1199400.0000, 
sim time next is 1200000.0000, 
raw observation next is [17.33333333333334, 69.66666666666667, 0.0, 0.0, 26.0, 24.51643886539405, 8.077494228776876, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.9427516158818101, 0.6966666666666668, 0.0, 0.0, 1.0, 0.7880626950562929, 0.08077494228776877, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0177777], dtype=float32), -0.38563156]. 
=============================================
[2019-04-01 15:58:42,759] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[98.29502 ]
 [98.282   ]
 [98.255486]
 [98.25045 ]
 [98.25378 ]], R is [[98.29060364]
 [98.30770111]
 [98.32462311]
 [98.34137726]
 [98.35796356]].
[2019-04-01 15:58:42,814] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7899178e-22 4.0458352e-26 2.8106763e-21 6.0715243e-34 5.5821443e-17
 1.0365328e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 15:58:42,818] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5522
[2019-04-01 15:58:42,833] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.51666666666667, 80.5, 0.0, 0.0, 26.0, 25.65509020324179, 13.26014365817834, 0.0, 1.0, 24.38424808048342], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1147800.0000, 
sim time next is 1148400.0000, 
raw observation next is [12.7, 80.0, 0.0, 0.0, 26.0, 25.67399648711202, 13.23223063521148, 0.0, 1.0, 22.85611834392923], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8, 0.0, 0.0, 1.0, 0.9534280695874315, 0.13232230635211478, 0.0, 1.0, 0.16325798817092307], 
reward next is 0.8367, 
noisyNet noise sample is [array([-0.17788039], dtype=float32), -0.40513283]. 
=============================================
[2019-04-01 15:58:44,308] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.9209863e-29 5.1769120e-34 1.0743388e-29 0.0000000e+00 1.6828785e-19
 1.1672492e-26 1.0000000e+00], sum to 1.0000
[2019-04-01 15:58:44,314] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9771
[2019-04-01 15:58:44,325] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.8, 63.66666666666666, 0.0, 0.0, 26.0, 25.99780224763148, 16.38617274277044, 0.0, 1.0, 19.221590080481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1115400.0000, 
sim time next is 1116000.0000, 
raw observation next is [12.7, 64.0, 0.0, 0.0, 26.0, 26.01362420262148, 16.24249950379929, 0.0, 1.0, 18.162757050933], 
processed observation next is [1.0, 0.9565217391304348, 0.8144044321329641, 0.64, 0.0, 0.0, 1.0, 1.0019463146602112, 0.1624249950379929, 0.0, 1.0, 0.12973397893523572], 
reward next is 0.8703, 
noisyNet noise sample is [array([0.79029876], dtype=float32), 1.7002133]. 
=============================================
[2019-04-01 15:58:44,337] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[102.221054]
 [102.53169 ]
 [102.47988 ]
 [102.93367 ]
 [102.96426 ]], R is [[101.72673035]
 [101.57216644]
 [101.41127777]
 [101.24865723]
 [101.06351471]].
[2019-04-01 15:58:47,628] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4763830e-23 5.8059778e-23 7.3928889e-23 7.0636014e-32 4.6042129e-19
 7.2113239e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 15:58:47,631] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2804
[2019-04-01 15:58:47,641] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 77.0, 0.0, 26.0, 24.71299248977617, 9.590726134462424, 0.0, 1.0, 6.079668533171194], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1261800.0000, 
sim time next is 1262400.0000, 
raw observation next is [13.8, 100.0, 72.66666666666667, 0.0, 26.0, 24.73341611482556, 9.585081492903825, 0.0, 1.0, 5.860003506230999], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.24222222222222223, 0.0, 1.0, 0.8190594449750799, 0.09585081492903826, 0.0, 1.0, 0.04185716790164999], 
reward next is 0.9581, 
noisyNet noise sample is [array([-0.09906181], dtype=float32), -0.48324952]. 
=============================================
[2019-04-01 15:58:48,868] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.6151274e-14 2.3187572e-20 1.4714608e-18 6.4107369e-24 3.0144832e-14
 1.1108039e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 15:58:48,872] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3700
[2019-04-01 15:58:48,896] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.266666666666667, 92.0, 0.0, 0.0, 26.0, 25.31008795843212, 10.87636406011452, 0.0, 1.0, 41.22932275290091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1320000.0000, 
sim time next is 1320600.0000, 
raw observation next is [1.183333333333333, 92.0, 0.0, 0.0, 26.0, 25.36887603585297, 10.88735603955711, 0.0, 1.0, 40.72273064923849], 
processed observation next is [1.0, 0.2608695652173913, 0.49538319482917825, 0.92, 0.0, 0.0, 1.0, 0.9098394336932815, 0.1088735603955711, 0.0, 1.0, 0.29087664749456066], 
reward next is 0.7091, 
noisyNet noise sample is [array([0.56262434], dtype=float32), -0.69090104]. 
=============================================
[2019-04-01 15:59:03,880] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.1027192e-19 1.4530741e-20 1.9846485e-19 1.1451131e-27 1.9430697e-14
 8.3860907e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 15:59:03,883] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8419
[2019-04-01 15:59:03,926] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1564465e-14 2.7898342e-22 6.5674731e-16 9.8290411e-26 3.2750336e-09
 7.1349405e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 15:59:03,930] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9314
[2019-04-01 15:59:03,934] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 82.0, 19.0, 20.0, 26.0, 25.52593574989751, 9.804786637718605, 1.0, 1.0, 18.64993971713634], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1584000.0000, 
sim time next is 1584600.0000, 
raw observation next is [5.266666666666667, 81.00000000000001, 25.0, 25.0, 26.0, 25.43466437301608, 9.704293061306638, 1.0, 1.0, 17.72047013314772], 
processed observation next is [1.0, 0.34782608695652173, 0.6084949215143122, 0.8100000000000002, 0.08333333333333333, 0.027624309392265192, 1.0, 0.9192377675737257, 0.09704293061306637, 1.0, 1.0, 0.12657478666534086], 
reward next is 0.8734, 
noisyNet noise sample is [array([-0.8799465], dtype=float32), -1.1055713]. 
=============================================
[2019-04-01 15:59:03,942] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.683333333333334, 81.0, 0.0, 0.0, 26.0, 25.52019956351774, 11.11721206547464, 0.0, 1.0, 24.74394124126033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1551000.0000, 
sim time next is 1551600.0000, 
raw observation next is [5.5, 82.0, 0.0, 0.0, 26.0, 25.41658830675365, 11.40639239443982, 0.0, 1.0, 33.34517974076387], 
processed observation next is [1.0, 1.0, 0.6149584487534627, 0.82, 0.0, 0.0, 1.0, 0.9166554723933784, 0.1140639239443982, 0.0, 1.0, 0.23817985529117047], 
reward next is 0.7618, 
noisyNet noise sample is [array([1.0954854], dtype=float32), -0.7505665]. 
=============================================
[2019-04-01 15:59:27,910] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7346897e-27 8.7541663e-36 1.4784113e-23 0.0000000e+00 3.8933044e-27
 1.8103748e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 15:59:27,913] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8284
[2019-04-01 15:59:27,966] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.65, 63.5, 137.0, 0.0, 26.0, 25.53047559411738, 7.759991609314622, 1.0, 1.0, 26.47466508029026], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1949400.0000, 
sim time next is 1950000.0000, 
raw observation next is [-3.566666666666666, 63.0, 132.8333333333333, 0.0, 26.0, 25.65018692792779, 7.907468107004138, 1.0, 1.0, 26.70649124116035], 
processed observation next is [1.0, 0.5652173913043478, 0.3638042474607572, 0.63, 0.4427777777777776, 0.0, 1.0, 0.9500267039896845, 0.07907468107004138, 1.0, 1.0, 0.19076065172257392], 
reward next is 0.8092, 
noisyNet noise sample is [array([-1.4860653], dtype=float32), -0.40315524]. 
=============================================
[2019-04-01 15:59:27,978] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.18553 ]
 [75.58014 ]
 [75.99419 ]
 [76.353836]
 [76.61792 ]], R is [[74.92999268]
 [74.99159241]
 [75.05110168]
 [75.10514069]
 [75.15120697]].
[2019-04-01 15:59:36,553] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3964911e-20 4.0384411e-30 1.2009249e-19 1.9315232e-32 4.7090084e-21
 6.7216812e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 15:59:36,554] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2430
[2019-04-01 15:59:36,602] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.0, 32.0, 0.0, 26.0, 25.7879136068595, 8.730366154219597, 1.0, 1.0, 18.56926341477751], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2046600.0000, 
sim time next is 2047200.0000, 
raw observation next is [-3.899999999999999, 82.0, 27.0, 0.0, 26.0, 25.69302126566965, 8.971677659384165, 1.0, 1.0, 31.71671173503327], 
processed observation next is [1.0, 0.6956521739130435, 0.35457063711911363, 0.82, 0.09, 0.0, 1.0, 0.9561458950956643, 0.08971677659384164, 1.0, 1.0, 0.22654794096452338], 
reward next is 0.7735, 
noisyNet noise sample is [array([1.0422869], dtype=float32), -0.5985559]. 
=============================================
[2019-04-01 15:59:42,625] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.6087879e-24 4.9142774e-36 8.2104746e-24 0.0000000e+00 4.4204835e-28
 2.9853171e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 15:59:42,626] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9301
[2019-04-01 15:59:42,674] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.800000000000001, 65.83333333333333, 245.6666666666667, 112.0, 26.0, 25.74433771136282, 8.79034177595009, 1.0, 1.0, 13.61021304625253], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2116200.0000, 
sim time next is 2116800.0000, 
raw observation next is [-6.7, 64.0, 222.0, 117.5, 26.0, 25.72644052025053, 8.693800023590251, 1.0, 1.0, 12.86776472091243], 
processed observation next is [1.0, 0.5217391304347826, 0.2770083102493075, 0.64, 0.74, 0.1298342541436464, 1.0, 0.9609200743215044, 0.08693800023590251, 1.0, 1.0, 0.0919126051493745], 
reward next is 0.9081, 
noisyNet noise sample is [array([0.99348664], dtype=float32), 0.87301815]. 
=============================================
[2019-04-01 15:59:43,458] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0607874e-21 1.4965971e-31 3.4791845e-18 3.9334965e-32 5.2271467e-23
 1.8500055e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 15:59:43,458] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9125
[2019-04-01 15:59:43,511] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 67.33333333333334, 141.0, 0.0, 26.0, 25.91806433141447, 9.611671914119107, 1.0, 1.0, 18.85464261312437], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2123400.0000, 
sim time next is 2124000.0000, 
raw observation next is [-5.6, 68.0, 137.0, 0.0, 26.0, 25.93404009002907, 8.632351992495096, 1.0, 1.0, 19.40250670882961], 
processed observation next is [1.0, 0.6086956521739131, 0.30747922437673136, 0.68, 0.45666666666666667, 0.0, 1.0, 0.9905771557184389, 0.08632351992495096, 1.0, 1.0, 0.1385893336344972], 
reward next is 0.8614, 
noisyNet noise sample is [array([-0.93153346], dtype=float32), -0.15178932]. 
=============================================
[2019-04-01 15:59:43,521] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[67.55264]
 [67.98095]
 [68.42958]
 [68.76859]
 [69.06752]], R is [[67.4203949 ]
 [67.61151886]
 [67.7937088 ]
 [67.96637726]
 [68.12936401]].
[2019-04-01 15:59:44,017] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4913633e-23 3.1327391e-28 7.5691551e-24 7.9173634e-36 1.4587034e-23
 7.7943897e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 15:59:44,017] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6507
[2019-04-01 15:59:44,060] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.55, 78.5, 208.0, 60.0, 26.0, 25.96108696375084, 9.07433182313452, 1.0, 1.0, 19.91276179576075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2111400.0000, 
sim time next is 2112000.0000, 
raw observation next is [-7.466666666666667, 77.33333333333334, 222.1666666666667, 66.83333333333333, 26.0, 25.91384540762128, 9.053082287117457, 1.0, 1.0, 18.97785020380845], 
processed observation next is [1.0, 0.43478260869565216, 0.25577100646352724, 0.7733333333333334, 0.7405555555555557, 0.07384898710865562, 1.0, 0.9876922010887544, 0.09053082287117457, 1.0, 1.0, 0.13555607288434607], 
reward next is 0.8644, 
noisyNet noise sample is [array([0.6803571], dtype=float32), -0.98850536]. 
=============================================
[2019-04-01 15:59:44,065] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.67399]
 [73.09978]
 [73.04251]
 [72.75566]
 [71.92155]], R is [[72.55812836]
 [72.69031525]
 [72.81388092]
 [72.92827606]
 [73.03326416]].
[2019-04-01 15:59:47,935] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.4837785e-21 2.4345386e-26 9.1333533e-23 7.2981583e-36 3.5363804e-22
 5.1155842e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 15:59:47,940] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7947
[2019-04-01 15:59:47,976] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.05358689659843, 8.874709509587944, 0.0, 1.0, 43.45183152427639], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2147400.0000, 
sim time next is 2148000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.12687345207167, 8.93262817796676, 0.0, 1.0, 42.92829808433977], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 1.0, 0.8752676360102386, 0.0893262817796676, 0.0, 1.0, 0.3066307006024269], 
reward next is 0.6934, 
noisyNet noise sample is [array([-0.11983708], dtype=float32), 0.4413658]. 
=============================================
[2019-04-01 15:59:47,983] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.07523 ]
 [78.549614]
 [78.97516 ]
 [79.365204]
 [79.01996 ]], R is [[77.60835266]
 [77.52189636]
 [77.42237854]
 [77.31255341]
 [77.10678101]].
[2019-04-01 15:59:48,256] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.6424338e-24 9.2376821e-27 8.7607628e-25 1.5937916e-34 2.3346625e-20
 1.0810313e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 15:59:48,269] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9640
[2019-04-01 15:59:48,293] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.72970788988303, 5.268376794497381, 0.0, 1.0, 41.38798788427427], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2181600.0000, 
sim time next is 2182200.0000, 
raw observation next is [-6.100000000000001, 78.33333333333334, 0.0, 0.0, 26.0, 23.71577287822182, 5.258598294269452, 0.0, 1.0, 41.36566061625957], 
processed observation next is [1.0, 0.2608695652173913, 0.2936288088642659, 0.7833333333333334, 0.0, 0.0, 1.0, 0.6736818397459743, 0.052585982942694516, 0.0, 1.0, 0.29546900440185403], 
reward next is 0.7045, 
noisyNet noise sample is [array([0.6438244], dtype=float32), 0.60615784]. 
=============================================
[2019-04-01 15:59:53,432] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8600539e-15 2.2664361e-19 1.2960519e-16 7.7330015e-24 8.3725993e-10
 2.6484454e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 15:59:53,432] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6744
[2019-04-01 15:59:53,458] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.9, 73.0, 0.0, 0.0, 26.0, 25.01670765772297, 7.358365805135072, 0.0, 1.0, 43.45312582824356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2241000.0000, 
sim time next is 2241600.0000, 
raw observation next is [-6.0, 73.66666666666667, 0.0, 0.0, 26.0, 24.90705633081876, 7.146164914518043, 0.0, 1.0, 43.65302110616139], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.7366666666666667, 0.0, 0.0, 1.0, 0.8438651901169659, 0.07146164914518044, 0.0, 1.0, 0.3118072936154385], 
reward next is 0.6882, 
noisyNet noise sample is [array([0.37233168], dtype=float32), 2.2750247]. 
=============================================
[2019-04-01 16:00:09,247] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2695435e-18 1.5183572e-23 1.4549329e-19 5.1992972e-26 1.5051179e-14
 2.7371503e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:00:09,248] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9963
[2019-04-01 16:00:09,268] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.766666666666667, 42.66666666666667, 0.0, 0.0, 26.0, 25.11861123349813, 6.928053262628836, 0.0, 1.0, 42.42209594345244], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2409600.0000, 
sim time next is 2410200.0000, 
raw observation next is [-3.95, 43.0, 0.0, 0.0, 26.0, 25.07620591414325, 6.819924808169617, 0.0, 1.0, 42.42564920525133], 
processed observation next is [0.0, 0.9130434782608695, 0.3531855955678671, 0.43, 0.0, 0.0, 1.0, 0.8680294163061788, 0.06819924808169617, 0.0, 1.0, 0.30304035146608094], 
reward next is 0.6970, 
noisyNet noise sample is [array([0.574283], dtype=float32), -1.1398652]. 
=============================================
[2019-04-01 16:00:14,947] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4136802e-31 1.0087224e-33 4.8194694e-27 0.0000000e+00 2.4224985e-31
 4.8759339e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 16:00:14,953] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2621
[2019-04-01 16:00:15,001] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.95, 39.5, 76.0, 777.0, 26.0, 24.94683726661256, 6.862632381052713, 0.0, 1.0, 39.85373263459556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2457000.0000, 
sim time next is 2457600.0000, 
raw observation next is [-3.4, 38.33333333333334, 77.66666666666667, 785.6666666666666, 26.0, 25.01315037711959, 6.979077139492854, 0.0, 1.0, 38.18761139669407], 
processed observation next is [0.0, 0.43478260869565216, 0.368421052631579, 0.3833333333333334, 0.2588888888888889, 0.8681399631675875, 1.0, 0.8590214824456558, 0.06979077139492854, 0.0, 1.0, 0.272768652833529], 
reward next is 0.7272, 
noisyNet noise sample is [array([-0.18688639], dtype=float32), 0.52852815]. 
=============================================
[2019-04-01 16:00:30,012] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6183036e-19 9.5695543e-23 3.8954981e-22 2.9738094e-30 3.8960915e-15
 8.9703432e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:00:30,012] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4092
[2019-04-01 16:00:30,071] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.0, 83.5, 97.0, 612.0, 26.0, 25.87758580989753, 9.25794359022427, 1.0, 1.0, 29.94801071556725], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2712600.0000, 
sim time next is 2713200.0000, 
raw observation next is [-12.66666666666667, 81.0, 100.3333333333333, 622.3333333333333, 26.0, 25.92165692267166, 9.33016839659671, 1.0, 1.0, 28.01404932810123], 
processed observation next is [1.0, 0.391304347826087, 0.11172668513388727, 0.81, 0.3344444444444443, 0.6876611418047881, 1.0, 0.988808131810237, 0.0933016839659671, 1.0, 1.0, 0.2001003523435802], 
reward next is 0.7999, 
noisyNet noise sample is [array([0.04102909], dtype=float32), -0.34442902]. 
=============================================
[2019-04-01 16:00:43,423] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2101986e-12 4.8055290e-20 8.1505745e-18 4.7629639e-22 2.9844482e-04
 2.5773807e-07 9.9970132e-01], sum to 1.0000
[2019-04-01 16:00:43,423] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4083
[2019-04-01 16:00:43,450] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 80.33333333333334, 0.0, 0.0, 26.0, 24.79983517482594, 8.317853183326323, 0.0, 1.0, 44.19399503723189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2924400.0000, 
sim time next is 2925000.0000, 
raw observation next is [-1.0, 81.5, 0.0, 0.0, 26.0, 24.76282106880648, 8.929565939699232, 0.0, 1.0, 54.67285804867594], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.815, 0.0, 0.0, 1.0, 0.8232601526866402, 0.08929565939699233, 0.0, 1.0, 0.3905204146333996], 
reward next is 0.6095, 
noisyNet noise sample is [array([-1.323055], dtype=float32), 1.209973]. 
=============================================
[2019-04-01 16:00:43,481] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[56.545135]
 [57.44406 ]
 [57.60632 ]
 [58.786964]
 [58.949104]], R is [[55.67749405]
 [55.8050499 ]
 [56.06068802]
 [56.34694672]
 [56.62182999]].
[2019-04-01 16:00:48,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9732303e-23 3.8459131e-31 4.5404432e-20 2.6199856e-38 4.5839001e-27
 5.5039008e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:00:48,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9651
[2019-04-01 16:00:48,130] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 181.0, 691.0, 26.0, 25.02964478432724, 9.189500816259597, 0.0, 1.0, 34.92812267368812], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2982600.0000, 
sim time next is 2983200.0000, 
raw observation next is [-3.0, 65.0, 169.1666666666667, 709.1666666666667, 26.0, 25.12425396911808, 9.357213455027363, 0.0, 1.0, 32.57361501685646], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.563888888888889, 0.783609576427256, 1.0, 0.8748934241597256, 0.09357213455027363, 0.0, 1.0, 0.23266867869183183], 
reward next is 0.7673, 
noisyNet noise sample is [array([-0.36622313], dtype=float32), 0.47820643]. 
=============================================
[2019-04-01 16:00:49,801] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.7003424e-19 2.0071439e-22 8.5891410e-20 2.6194134e-25 8.5903053e-16
 2.3818905e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:00:49,802] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1708
[2019-04-01 16:00:49,826] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.25, 65.0, 0.0, 0.0, 26.0, 25.35278439848542, 8.323150773535957, 0.0, 1.0, 39.81780179229219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3011400.0000, 
sim time next is 3012000.0000, 
raw observation next is [-3.333333333333333, 65.0, 0.0, 0.0, 26.0, 25.33746864900144, 8.246111555087088, 0.0, 1.0, 39.70964934368372], 
processed observation next is [0.0, 0.8695652173913043, 0.37026777469990774, 0.65, 0.0, 0.0, 1.0, 0.9053526641430629, 0.08246111555087088, 0.0, 1.0, 0.2836403524548837], 
reward next is 0.7164, 
noisyNet noise sample is [array([-1.2149017], dtype=float32), 2.1253686]. 
=============================================
[2019-04-01 16:00:49,847] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[59.667576]
 [59.58981 ]
 [59.41924 ]
 [59.248055]
 [59.08424 ]], R is [[59.86084747]
 [59.97782898]
 [60.09367752]
 [60.20776749]
 [60.31471252]].
[2019-04-01 16:00:50,254] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0127510e-19 5.4346181e-26 9.5277697e-24 1.0032903e-29 1.6093438e-22
 9.6291837e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:00:50,257] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5957
[2019-04-01 16:00:50,269] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 65.0, 0.0, 0.0, 26.0, 25.33746864900144, 8.246111555087088, 0.0, 1.0, 39.70964934368372], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3012000.0000, 
sim time next is 3012600.0000, 
raw observation next is [-3.416666666666667, 65.0, 0.0, 0.0, 26.0, 25.30167063665698, 8.150979180553646, 0.0, 1.0, 39.60132452714607], 
processed observation next is [0.0, 0.8695652173913043, 0.36795937211449675, 0.65, 0.0, 0.0, 1.0, 0.9002386623795685, 0.08150979180553647, 0.0, 1.0, 0.28286660376532907], 
reward next is 0.7171, 
noisyNet noise sample is [array([1.2585444], dtype=float32), 0.73351216]. 
=============================================
[2019-04-01 16:00:50,438] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9130477e-20 2.3456033e-23 2.5346704e-18 6.8941759e-27 1.1859238e-20
 7.2006178e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:00:50,438] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2732
[2019-04-01 16:00:50,511] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 26.0, 25.08024976972499, 8.145875457366786, 0.0, 1.0, 31.54596268189084], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3001800.0000, 
sim time next is 3002400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.06677791620012, 8.060436520174298, 0.0, 1.0, 29.80989144982171], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6, 0.0, 0.0, 1.0, 0.86668255945716, 0.08060436520174298, 0.0, 1.0, 0.21292779607015508], 
reward next is 0.7871, 
noisyNet noise sample is [array([-0.07393842], dtype=float32), -0.6495441]. 
=============================================
[2019-04-01 16:00:55,901] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.7145005e-24 8.8751481e-36 2.9287778e-22 0.0000000e+00 7.0623693e-30
 3.6878938e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 16:00:55,902] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5612
[2019-04-01 16:00:55,920] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666666, 39.66666666666666, 79.5, 641.8333333333334, 26.0, 25.15160429903096, 8.613537349927947, 0.0, 1.0, 13.08556252666028], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3080400.0000, 
sim time next is 3081000.0000, 
raw observation next is [0.8333333333333334, 39.83333333333334, 75.0, 610.6666666666667, 26.0, 25.21616347070894, 8.63258006139516, 0.0, 1.0, 11.75544721819337], 
processed observation next is [0.0, 0.6521739130434783, 0.4856879039704525, 0.39833333333333343, 0.25, 0.6747697974217312, 1.0, 0.8880233529584197, 0.0863258006139516, 0.0, 1.0, 0.08396748012995264], 
reward next is 0.9160, 
noisyNet noise sample is [array([0.2559963], dtype=float32), -0.3073523]. 
=============================================
[2019-04-01 16:00:55,927] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.77826 ]
 [82.8844  ]
 [82.946014]
 [83.01913 ]
 [83.09172 ]], R is [[82.74917603]
 [82.82821655]
 [82.88301849]
 [82.9291687 ]
 [82.9713974 ]].
[2019-04-01 16:00:55,969] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2432830e-23 2.6160441e-28 9.3071470e-28 2.5077552e-37 4.5949237e-22
 1.2719223e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:00:55,969] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0124
[2019-04-01 16:00:55,988] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 96.0, 0.0, 0.0, 26.0, 25.49144812085687, 8.059682419172253, 0.0, 1.0, 32.74786802384513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3097800.0000, 
sim time next is 3098400.0000, 
raw observation next is [-1.0, 97.33333333333333, 0.0, 0.0, 26.0, 25.44096090166595, 7.909156760756002, 0.0, 1.0, 34.38502267153337], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9733333333333333, 0.0, 0.0, 1.0, 0.9201372716665642, 0.07909156760756002, 0.0, 1.0, 0.24560730479666693], 
reward next is 0.7544, 
noisyNet noise sample is [array([0.37685397], dtype=float32), -0.86153316]. 
=============================================
[2019-04-01 16:01:01,371] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8742902e-17 5.5442012e-24 1.2800119e-20 1.2406642e-26 7.1147972e-16
 1.0451450e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 16:01:01,374] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5331
[2019-04-01 16:01:01,454] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 0.0, 0.0, 26.0, 25.32164375522205, 10.53260177455357, 1.0, 1.0, 37.98369572092799], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3223200.0000, 
sim time next is 3223800.0000, 
raw observation next is [-3.0, 92.0, 1.0, 82.0, 26.0, 25.63455177533934, 10.41453877809171, 1.0, 1.0, 33.59490974576318], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.0033333333333333335, 0.09060773480662983, 1.0, 0.9477931107627627, 0.1041453877809171, 1.0, 1.0, 0.23996364104116558], 
reward next is 0.5942, 
noisyNet noise sample is [array([-0.12463745], dtype=float32), 1.6314375]. 
=============================================
[2019-04-01 16:01:07,124] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.9581641e-10 2.0507736e-20 2.0997974e-10 6.9293499e-19 3.2583928e-06
 5.4986140e-04 9.9944693e-01], sum to 1.0000
[2019-04-01 16:01:07,127] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2225
[2019-04-01 16:01:07,175] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 52.5, 11.0, 133.0, 26.0, 26.09093289533924, 10.94870230173436, 1.0, 1.0, 8.770160273955623], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3346200.0000, 
sim time next is 3346800.0000, 
raw observation next is [-2.666666666666667, 53.33333333333333, 9.166666666666668, 110.8333333333333, 26.0, 25.55042110392515, 10.49191062755787, 1.0, 1.0, 28.20276692928041], 
processed observation next is [1.0, 0.7391304347826086, 0.38873499538319484, 0.5333333333333333, 0.030555555555555558, 0.12246777163904232, 1.0, 0.9357744434178785, 0.1049191062755787, 1.0, 1.0, 0.2014483352091458], 
reward next is 0.6018, 
noisyNet noise sample is [array([-0.28762102], dtype=float32), 0.17377557]. 
=============================================
[2019-04-01 16:01:12,426] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.6353306e-13 1.3236446e-20 8.1699855e-12 3.6594932e-23 3.3259785e-13
 3.5605899e-03 9.9643940e-01], sum to 1.0000
[2019-04-01 16:01:12,427] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2647
[2019-04-01 16:01:12,451] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 48.5, 109.0, 764.0, 26.0, 26.5880680847129, 13.69970705052138, 1.0, 1.0, 12.34806095158579], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3407400.0000, 
sim time next is 3408000.0000, 
raw observation next is [2.666666666666667, 48.66666666666666, 110.0, 770.6666666666667, 26.0, 26.60741478606275, 14.07745503110488, 1.0, 1.0, 11.61723884986105], 
processed observation next is [1.0, 0.43478260869565216, 0.5364727608494922, 0.4866666666666666, 0.36666666666666664, 0.8515653775322285, 1.0, 1.0867735408661072, 0.1407745503110488, 1.0, 1.0, 0.0829802774990075], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.99368364], dtype=float32), 0.14523278]. 
=============================================
[2019-04-01 16:01:12,467] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[44.75655 ]
 [42.971474]
 [41.869705]
 [41.00621 ]
 [40.27759 ]], R is [[45.02167511]
 [44.57145691]
 [44.12574387]
 [43.68448639]
 [43.24764252]].
[2019-04-01 16:01:15,043] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.7151556e-14 3.8267332e-28 2.0055780e-19 1.7335775e-28 9.1370189e-14
 8.0577806e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 16:01:15,044] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3373
[2019-04-01 16:01:15,052] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.40985608016356, 10.09098006572433, 1.0, 1.0, 15.43186844476357], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3438000.0000, 
sim time next is 3438600.0000, 
raw observation next is [1.0, 79.00000000000001, 0.0, 0.0, 26.0, 25.34869025706208, 9.720121558491032, 1.0, 1.0, 16.15004672255531], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.7900000000000001, 0.0, 0.0, 1.0, 0.9069557510088687, 0.09720121558491032, 1.0, 1.0, 0.11535747658968078], 
reward next is 0.8846, 
noisyNet noise sample is [array([0.790587], dtype=float32), -0.012554147]. 
=============================================
[2019-04-01 16:01:19,506] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7995200e-18 1.1818761e-22 1.0744169e-21 1.7109732e-25 2.0660042e-15
 5.3630590e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 16:01:19,510] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5846
[2019-04-01 16:01:19,554] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.27594903403448, 9.467210823776865, 0.0, 1.0, 42.24588209125368], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3542400.0000, 
sim time next is 3543000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.29604479003466, 9.40897675423871, 0.0, 1.0, 42.13231484472682], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 1.0, 0.8994349700049514, 0.0940897675423871, 0.0, 1.0, 0.300945106033763], 
reward next is 0.6991, 
noisyNet noise sample is [array([-1.516192], dtype=float32), 0.13953608]. 
=============================================
[2019-04-01 16:01:19,574] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[44.527733]
 [44.762558]
 [44.781982]
 [44.86389 ]
 [45.07081 ]], R is [[44.47954941]
 [44.73299789]
 [44.9832077 ]
 [45.23008347]
 [45.47332001]].
[2019-04-01 16:01:21,029] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.1270708e-20 1.9802312e-27 9.5334820e-15 5.9938707e-29 1.8478995e-19
 2.2026421e-07 9.9999976e-01], sum to 1.0000
[2019-04-01 16:01:21,029] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0128
[2019-04-01 16:01:21,042] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 41.0, 63.0, 515.0, 26.0, 25.3199823404998, 9.708392410653504, 0.0, 1.0, 9.282428802203848], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3601800.0000, 
sim time next is 3602400.0000, 
raw observation next is [0.0, 40.33333333333334, 54.83333333333334, 452.8333333333334, 26.0, 25.29564724044039, 9.490061548630052, 0.0, 1.0, 9.667569124961283], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.40333333333333343, 0.18277777777777782, 0.5003683241252304, 1.0, 0.8993781772057697, 0.09490061548630052, 0.0, 1.0, 0.06905406517829488], 
reward next is 0.9309, 
noisyNet noise sample is [array([-0.87324107], dtype=float32), -0.14935897]. 
=============================================
[2019-04-01 16:01:27,968] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7575675e-22 4.1621021e-25 1.2181410e-24 3.0149202e-29 4.8346315e-24
 6.8908875e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:01:27,975] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4424
[2019-04-01 16:01:27,990] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.47125749245748, 8.505112143122195, 0.0, 1.0, 33.12896803549967], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3711600.0000, 
sim time next is 3712200.0000, 
raw observation next is [-3.0, 66.0, 0.0, 0.0, 26.0, 25.43038922614032, 8.271737920063686, 0.0, 1.0, 34.42747123205819], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.66, 0.0, 0.0, 1.0, 0.91862703230576, 0.08271737920063686, 0.0, 1.0, 0.24591050880041562], 
reward next is 0.7541, 
noisyNet noise sample is [array([1.1703277], dtype=float32), 0.87869835]. 
=============================================
[2019-04-01 16:01:29,257] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.5235569e-16 2.1377300e-19 3.5211864e-20 3.5940834e-23 5.2289482e-13
 1.8717595e-11 1.0000000e+00], sum to 1.0000
[2019-04-01 16:01:29,259] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6785
[2019-04-01 16:01:29,277] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 75.0, 0.0, 0.0, 26.0, 25.65514558290634, 11.26253305940093, 0.0, 1.0, 27.46127558876408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3793200.0000, 
sim time next is 3793800.0000, 
raw observation next is [-3.0, 76.0, 0.0, 0.0, 26.0, 25.59926362689395, 10.91606408475485, 0.0, 1.0, 26.04584353078909], 
processed observation next is [1.0, 0.9130434782608695, 0.3795013850415513, 0.76, 0.0, 0.0, 1.0, 0.9427519466991358, 0.1091606408475485, 0.0, 1.0, 0.18604173950563635], 
reward next is 0.8140, 
noisyNet noise sample is [array([-2.0886414], dtype=float32), -0.07402513]. 
=============================================
[2019-04-01 16:01:32,246] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2491107e-13 7.0461561e-18 1.2943708e-17 5.2852338e-22 3.6882420e-07
 3.7297123e-12 9.9999964e-01], sum to 1.0000
[2019-04-01 16:01:32,247] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4949
[2019-04-01 16:01:32,259] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 72.0, 0.0, 0.0, 26.0, 25.70005791909919, 12.10005920527085, 0.0, 1.0, 31.65162803164722], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3791400.0000, 
sim time next is 3792000.0000, 
raw observation next is [-3.0, 73.0, 0.0, 0.0, 26.0, 25.7251120350309, 11.90642603411426, 0.0, 1.0, 30.62386487788188], 
processed observation next is [1.0, 0.9130434782608695, 0.3795013850415513, 0.73, 0.0, 0.0, 1.0, 0.9607302907187, 0.11906426034114259, 0.0, 1.0, 0.21874189198487057], 
reward next is 0.7813, 
noisyNet noise sample is [array([0.26755533], dtype=float32), -0.7576724]. 
=============================================
[2019-04-01 16:01:32,264] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[49.098053]
 [48.71204 ]
 [48.21097 ]
 [47.27436 ]
 [46.603302]], R is [[49.58306885]
 [49.86115646]
 [50.10375595]
 [50.283638  ]
 [50.43859482]].
[2019-04-01 16:01:33,884] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7983894e-17 9.4022809e-21 1.3728040e-17 2.0814796e-28 2.9012237e-14
 1.2634930e-13 1.0000000e+00], sum to 1.0000
[2019-04-01 16:01:33,884] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4098
[2019-04-01 16:01:33,895] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 76.0, 0.0, 0.0, 26.0, 25.45158439214319, 8.613486201479084, 0.0, 1.0, 34.77426301083902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3809400.0000, 
sim time next is 3810000.0000, 
raw observation next is [-4.0, 75.0, 0.0, 0.0, 26.0, 25.45190694157909, 8.422592345291442, 0.0, 1.0, 37.47778138683554], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.75, 0.0, 0.0, 1.0, 0.9217009916541556, 0.08422592345291441, 0.0, 1.0, 0.2676984384773967], 
reward next is 0.7323, 
noisyNet noise sample is [array([0.53908336], dtype=float32), 0.3939296]. 
=============================================
[2019-04-01 16:01:33,902] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[58.073185]
 [58.273727]
 [58.34669 ]
 [58.43382 ]
 [58.479023]], R is [[58.00647736]
 [58.17802811]
 [58.29709244]
 [58.40472794]
 [58.51038742]].
[2019-04-01 16:01:34,843] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0768016e-12 2.3739080e-20 4.2247449e-17 1.5731740e-21 4.0162395e-16
 4.4875745e-11 1.0000000e+00], sum to 1.0000
[2019-04-01 16:01:34,843] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8950
[2019-04-01 16:01:34,856] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 50.0, 0.0, 0.0, 26.0, 25.96776560846798, 12.4288090488493, 1.0, 1.0, 8.131812304697933], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3868800.0000, 
sim time next is 3869400.0000, 
raw observation next is [1.166666666666667, 50.5, 0.0, 0.0, 26.0, 25.90670657002989, 12.04807116327061, 1.0, 1.0, 8.043604602916254], 
processed observation next is [1.0, 0.782608695652174, 0.49492151431209613, 0.505, 0.0, 0.0, 1.0, 0.9866723671471271, 0.1204807116327061, 1.0, 1.0, 0.057454318592258954], 
reward next is 0.1233, 
noisyNet noise sample is [array([0.4851305], dtype=float32), -0.244102]. 
=============================================
[2019-04-01 16:01:36,910] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.94272996e-24 5.49360269e-27 1.02468140e-26 1.01248936e-32
 1.65462153e-19 1.44967611e-18 1.00000000e+00], sum to 1.0000
[2019-04-01 16:01:36,910] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1358
[2019-04-01 16:01:36,934] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 58.33333333333334, 0.0, 0.0, 26.0, 25.23146619214723, 9.144529618855024, 0.0, 1.0, 39.15333711235604], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3883200.0000, 
sim time next is 3883800.0000, 
raw observation next is [-1.0, 59.16666666666666, 0.0, 0.0, 26.0, 25.16573310189649, 9.294126601049483, 0.0, 1.0, 44.03622607116579], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.5916666666666666, 0.0, 0.0, 1.0, 0.8808190145566412, 0.09294126601049482, 0.0, 1.0, 0.3145444719368985], 
reward next is 0.6855, 
noisyNet noise sample is [array([0.9848521], dtype=float32), 1.4168814]. 
=============================================
[2019-04-01 16:01:40,506] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.04989026e-09 1.08724236e-16 4.76871355e-06 4.02796274e-16
 6.98361202e-10 5.98738843e-05 9.99935389e-01], sum to 1.0000
[2019-04-01 16:01:40,507] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0919
[2019-04-01 16:01:40,550] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 49.0, 119.6666666666667, 818.3333333333334, 26.0, 26.44991595597097, 13.89594510660524, 1.0, 1.0, 8.012340330908698], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3930600.0000, 
sim time next is 3931200.0000, 
raw observation next is [-6.0, 49.0, 119.5, 822.5, 26.0, 26.46062951326773, 11.64683819140729, 1.0, 1.0, 8.495857446697794], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.49, 0.3983333333333333, 0.9088397790055248, 1.0, 1.0658042161811043, 0.11646838191407291, 1.0, 1.0, 0.06068469604784139], 
reward next is 0.2806, 
noisyNet noise sample is [array([-0.01176941], dtype=float32), 0.27370897]. 
=============================================
[2019-04-01 16:01:53,427] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7771215e-24 1.4404775e-23 3.8297990e-21 5.0702339e-29 5.9451360e-23
 8.9683408e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 16:01:53,428] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5228
[2019-04-01 16:01:53,446] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.21193825174253, 7.887310573160984, 0.0, 1.0, 39.09770539993469], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4159800.0000, 
sim time next is 4160400.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.21769008847227, 7.785471202042483, 0.0, 1.0, 39.09882453789605], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 1.0, 0.8882414412103243, 0.07785471202042483, 0.0, 1.0, 0.2792773181278289], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.08900768], dtype=float32), 1.8740603]. 
=============================================
[2019-04-01 16:01:56,166] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7331594e-21 2.3359238e-23 4.3341621e-18 7.9528296e-31 4.2340175e-19
 9.6074729e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 16:01:56,169] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2892
[2019-04-01 16:01:56,182] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 44.83333333333334, 0.0, 0.0, 26.0, 25.38297468183067, 8.549147184842974, 0.0, 1.0, 36.08426103800627], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4153800.0000, 
sim time next is 4154400.0000, 
raw observation next is [-2.0, 46.0, 0.0, 0.0, 26.0, 25.3366914423451, 8.394602839625263, 0.0, 1.0, 37.69130001416249], 
processed observation next is [0.0, 0.08695652173913043, 0.40720221606648205, 0.46, 0.0, 0.0, 1.0, 0.9052416346207285, 0.08394602839625262, 0.0, 1.0, 0.2692235715297321], 
reward next is 0.7308, 
noisyNet noise sample is [array([-0.65212214], dtype=float32), -0.6979733]. 
=============================================
[2019-04-01 16:01:59,539] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.9711460e-23 5.5706628e-30 8.9125197e-17 1.1728415e-34 2.3635866e-22
 3.3905685e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 16:01:59,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0669
[2019-04-01 16:01:59,579] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 54.0, 193.0, 367.5, 26.0, 25.53233414735212, 8.487803500407997, 0.0, 1.0, 16.76360238907861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4269600.0000, 
sim time next is 4270200.0000, 
raw observation next is [4.166666666666667, 54.16666666666666, 196.6666666666667, 446.3333333333334, 26.0, 25.47228823903872, 8.457096428490575, 0.0, 1.0, 16.929844040841], 
processed observation next is [0.0, 0.43478260869565216, 0.5780240073868884, 0.5416666666666665, 0.6555555555555557, 0.49318600368324134, 1.0, 0.9246126055769598, 0.08457096428490575, 0.0, 1.0, 0.12092745743457858], 
reward next is 0.8791, 
noisyNet noise sample is [array([-0.468926], dtype=float32), 0.053137165]. 
=============================================
[2019-04-01 16:02:03,584] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.4042648e-15 1.1150434e-32 7.5835712e-18 2.9290578e-29 1.0817518e-17
 3.3506016e-13 1.0000000e+00], sum to 1.0000
[2019-04-01 16:02:03,584] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4598
[2019-04-01 16:02:03,609] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7361108e-22 3.9042169e-29 3.7483130e-27 6.7482295e-35 1.0959835e-22
 3.9930033e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 16:02:03,609] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.2, 31.5, 195.0, 629.0, 26.0, 28.79161891398876, 35.44655861698647, 1.0, 1.0, 1.080372850440519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4372200.0000, 
sim time next is 4372800.0000, 
raw observation next is [14.1, 31.66666666666666, 179.6666666666667, 524.1666666666667, 26.0, 28.89363545151189, 30.61010230745886, 1.0, 1.0, 1.160561351587439], 
processed observation next is [1.0, 0.6086956521739131, 0.8531855955678671, 0.3166666666666666, 0.598888888888889, 0.5791896869244937, 1.0, 1.4133764930731272, 0.3061010230745886, 1.0, 1.0, 0.008289723939910279], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.04267811], dtype=float32), -2.1215537]. 
=============================================
[2019-04-01 16:02:03,609] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3557
[2019-04-01 16:02:03,624] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.4, 73.0, 0.0, 0.0, 26.0, 25.59684621440098, 9.90551521585238, 0.0, 1.0, 41.79530925876402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4305600.0000, 
sim time next is 4306200.0000, 
raw observation next is [5.350000000000001, 73.0, 0.0, 0.0, 26.0, 25.73069974502774, 10.19016759351247, 0.0, 1.0, 34.01220064451218], 
processed observation next is [0.0, 0.8695652173913043, 0.6108033240997232, 0.73, 0.0, 0.0, 1.0, 0.9615285350039626, 0.1019016759351247, 0.0, 1.0, 0.24294429031794412], 
reward next is 0.7571, 
noisyNet noise sample is [array([-0.21760081], dtype=float32), -0.5108149]. 
=============================================
[2019-04-01 16:02:09,287] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3166763e-18 1.3076837e-22 2.3932686e-18 9.2391175e-29 1.2215981e-17
 1.3945370e-13 1.0000000e+00], sum to 1.0000
[2019-04-01 16:02:09,288] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5512
[2019-04-01 16:02:09,334] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9, 72.0, 0.0, 0.0, 26.0, 25.72417998634416, 9.396281855460328, 1.0, 1.0, 33.00418552617452], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4519800.0000, 
sim time next is 4520400.0000, 
raw observation next is [-0.8666666666666667, 72.33333333333333, 18.5, 11.0, 26.0, 25.71268849292759, 9.19609603201664, 1.0, 1.0, 28.79725693899819], 
processed observation next is [1.0, 0.30434782608695654, 0.4385964912280702, 0.7233333333333333, 0.06166666666666667, 0.012154696132596685, 1.0, 0.9589554989896557, 0.0919609603201664, 1.0, 1.0, 0.20569469242141564], 
reward next is 0.7943, 
noisyNet noise sample is [array([-1.171692], dtype=float32), 0.857814]. 
=============================================
[2019-04-01 16:02:10,079] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7890965e-17 6.6080163e-21 7.7978697e-18 1.0724271e-28 1.6316103e-10
 2.7572323e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 16:02:10,080] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1738
[2019-04-01 16:02:10,100] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.63576414675797, 12.63822684495678, 0.0, 1.0, 31.12572289424999], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4483200.0000, 
sim time next is 4483800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.69126368939733, 12.55367326491395, 0.0, 1.0, 29.15575951548432], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 1.0, 0.9558948127710474, 0.1255367326491395, 0.0, 1.0, 0.20825542511060227], 
reward next is 0.7917, 
noisyNet noise sample is [array([-1.1394407], dtype=float32), 0.7382548]. 
=============================================
[2019-04-01 16:02:10,770] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8294885e-24 3.2330954e-28 4.2619838e-28 0.0000000e+00 3.0740379e-18
 4.4197595e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:02:10,770] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5891
[2019-04-01 16:02:10,789] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3666666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.32020074123933, 10.14158103745491, 0.0, 1.0, 43.31667303033835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4490400.0000, 
sim time next is 4491000.0000, 
raw observation next is [-0.4, 72.5, 0.0, 0.0, 26.0, 25.35353145651382, 10.14369409977797, 0.0, 1.0, 43.16341528741187], 
processed observation next is [1.0, 1.0, 0.45152354570637127, 0.725, 0.0, 0.0, 1.0, 0.9076473509305458, 0.10143694099777971, 0.0, 1.0, 0.3083101091957991], 
reward next is 0.6917, 
noisyNet noise sample is [array([-0.39817718], dtype=float32), 0.094649084]. 
=============================================
[2019-04-01 16:02:10,800] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[85.14509 ]
 [84.846794]
 [84.70431 ]
 [84.50317 ]
 [84.24628 ]], R is [[85.01480865]
 [84.85525513]
 [84.6949234 ]
 [84.530159  ]
 [84.35735321]].
[2019-04-01 16:02:13,746] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.5980508e-25 2.3693085e-23 8.0254107e-19 1.6231516e-33 3.8727751e-19
 4.0469056e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 16:02:13,749] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3568
[2019-04-01 16:02:13,758] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6000000000000001, 62.33333333333334, 0.0, 0.0, 26.0, 25.52284314623039, 9.470671020311896, 0.0, 1.0, 27.21869066734357], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4581600.0000, 
sim time next is 4582200.0000, 
raw observation next is [0.5, 62.66666666666666, 0.0, 0.0, 26.0, 25.50216049810087, 9.198715106832509, 0.0, 1.0, 28.38710387080982], 
processed observation next is [1.0, 0.0, 0.4764542936288089, 0.6266666666666666, 0.0, 0.0, 1.0, 0.9288800711572672, 0.09198715106832508, 0.0, 1.0, 0.20276502764864157], 
reward next is 0.7972, 
noisyNet noise sample is [array([-0.46981966], dtype=float32), -1.067401]. 
=============================================
[2019-04-01 16:02:23,455] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0657741e-23 5.8312056e-26 2.0493886e-21 4.4078577e-34 4.6276001e-25
 1.3801787e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 16:02:23,457] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3580
[2019-04-01 16:02:23,491] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.33593695363343, 6.084260115940228, 0.0, 1.0, 40.61586921366099], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4767600.0000, 
sim time next is 4768200.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.30495520837487, 6.141949965773189, 0.0, 1.0, 40.67126227849835], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 1.0, 0.757850744053553, 0.061419499657731885, 0.0, 1.0, 0.2905090162749882], 
reward next is 0.7095, 
noisyNet noise sample is [array([0.12446864], dtype=float32), 0.6355849]. 
=============================================
[2019-04-01 16:02:23,620] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0922378e-25 5.5541872e-26 1.4937308e-20 9.7398925e-37 2.4280408e-23
 1.1797905e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 16:02:23,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6902
[2019-04-01 16:02:23,640] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.87314207136227, 7.433488456482124, 0.0, 1.0, 40.26622019755977], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4758000.0000, 
sim time next is 4758600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.83364584796708, 7.335317759633341, 0.0, 1.0, 40.22023579391793], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 1.0, 0.8333779782810115, 0.0733531775963334, 0.0, 1.0, 0.2872873985279852], 
reward next is 0.7127, 
noisyNet noise sample is [array([-0.69618714], dtype=float32), 0.67535007]. 
=============================================
[2019-04-01 16:02:24,897] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6739095e-26 6.4716535e-30 1.2698422e-28 1.1959308e-37 9.5351411e-26
 6.2200211e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 16:02:24,897] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6398
[2019-04-01 16:02:24,912] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 79.66666666666667, 0.0, 0.0, 26.0, 25.03267912816219, 8.130135271384662, 0.0, 1.0, 40.84248382674668], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4753200.0000, 
sim time next is 4753800.0000, 
raw observation next is [-4.0, 77.5, 0.0, 0.0, 26.0, 25.002420560694, 7.980742556736101, 0.0, 1.0, 40.79895212409206], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.775, 0.0, 0.0, 1.0, 0.8574886515277141, 0.079807425567361, 0.0, 1.0, 0.2914210866006576], 
reward next is 0.7086, 
noisyNet noise sample is [array([-1.4769607], dtype=float32), -0.4165]. 
=============================================
[2019-04-01 16:02:26,064] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-01 16:02:26,064] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 16:02:26,065] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 16:02:26,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:02:26,065] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 16:02:26,066] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:02:26,066] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:02:26,076] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run5
[2019-04-01 16:02:26,088] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run5
[2019-04-01 16:02:26,099] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run5
[2019-04-01 16:03:28,281] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.26415208], dtype=float32), -0.12939449]
[2019-04-01 16:03:28,282] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.790766916333332, 90.94920422999999, 161.7885402066667, 249.4554572166667, 26.0, 25.38665984277714, 7.450763902276537, 1.0, 1.0, 27.46188605734844]
[2019-04-01 16:03:28,282] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 16:03:28,283] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.7541657e-21 1.7315616e-28 1.5453679e-20 6.4080699e-32 2.2578193e-23
 3.0168107e-19 1.0000000e+00], sampled 0.7795073342413615
[2019-04-01 16:03:29,976] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.26415208], dtype=float32), -0.12939449]
[2019-04-01 16:03:29,977] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-7.0, 78.5, 0.0, 0.0, 26.0, 24.25392483051254, 5.761356152843947, 0.0, 1.0, 42.06495045104694]
[2019-04-01 16:03:29,977] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:03:29,978] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.9383809e-24 3.3943646e-26 3.3972585e-23 7.6445038e-34 4.3780220e-22
 2.7507669e-21 1.0000000e+00], sampled 0.3174842421445302
[2019-04-01 16:03:32,382] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.26415208], dtype=float32), -0.12939449]
[2019-04-01 16:03:32,382] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.240409709833333, 74.009494525, 111.47301784, 0.0, 26.0, 25.19959528762194, 7.857455060711703, 1.0, 1.0, 31.68109354137781]
[2019-04-01 16:03:32,383] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 16:03:32,383] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.3938294e-21 1.6070061e-30 4.3558829e-22 2.3743259e-34 6.6550628e-23
 3.7805628e-21 1.0000000e+00], sampled 0.3147094444883016
[2019-04-01 16:04:14,484] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 16:04:34,732] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 16:04:37,225] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 16:04:38,249] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 400000, evaluation results [400000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 16:04:40,491] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.7904201e-26 2.4701650e-31 6.2111531e-24 1.6287375e-36 9.6321816e-23
 1.2649616e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 16:04:40,492] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9958
[2019-04-01 16:04:40,531] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 67.0, 0.0, 0.0, 26.0, 24.43511463624611, 5.604336011899115, 0.0, 1.0, 39.12506492584335], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4864800.0000, 
sim time next is 4865400.0000, 
raw observation next is [-4.0, 68.0, 0.0, 0.0, 26.0, 24.40774102768674, 5.569305141764507, 0.0, 1.0, 39.1405246405614], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.68, 0.0, 0.0, 1.0, 0.7725344325266771, 0.055693051417645065, 0.0, 1.0, 0.27957517600401], 
reward next is 0.7204, 
noisyNet noise sample is [array([0.05496309], dtype=float32), -0.6356686]. 
=============================================
[2019-04-01 16:04:41,583] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.1979431e-25 2.2833390e-28 1.6791871e-24 5.1642834e-36 1.8051622e-19
 1.7009150e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 16:04:41,583] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0040
[2019-04-01 16:04:41,601] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.14128940415317, 7.364413022486747, 0.0, 1.0, 38.89668842833755], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4843200.0000, 
sim time next is 4843800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.13166992233248, 7.314449736646973, 0.0, 1.0, 38.85669474045094], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 1.0, 0.8759528460474973, 0.07314449736646973, 0.0, 1.0, 0.2775478195746496], 
reward next is 0.7225, 
noisyNet noise sample is [array([2.2363565], dtype=float32), -1.6073713]. 
=============================================
[2019-04-01 16:04:42,407] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0345519e-18 2.1199141e-33 1.1597039e-19 2.4899300e-37 3.8448148e-30
 2.4684267e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:04:42,408] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0036
[2019-04-01 16:04:42,424] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 44.5, 236.0, 374.0, 26.0, 25.06389869387879, 8.353168539259483, 0.0, 1.0, 12.8647671283747], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4890600.0000, 
sim time next is 4891200.0000, 
raw observation next is [2.666666666666667, 44.66666666666667, 223.8333333333333, 382.0, 26.0, 25.06586503534263, 8.382345416335555, 0.0, 1.0, 12.57762806572377], 
processed observation next is [0.0, 0.6086956521739131, 0.5364727608494922, 0.4466666666666667, 0.746111111111111, 0.4220994475138122, 1.0, 0.8665521479060898, 0.08382345416335556, 0.0, 1.0, 0.0898402004694555], 
reward next is 0.9102, 
noisyNet noise sample is [array([2.4782403], dtype=float32), 1.0560589]. 
=============================================
[2019-04-01 16:04:47,721] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.9900234e-13 2.1083630e-18 1.6540257e-20 1.9667440e-22 4.6814144e-05
 1.1475169e-12 9.9995315e-01], sum to 1.0000
[2019-04-01 16:04:47,725] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2122
[2019-04-01 16:04:47,741] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.0, 26.0, 53.0, 472.5, 26.0, 27.48557497963372, 21.85995703989524, 1.0, 1.0, 0.9313952246403496], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4986000.0000, 
sim time next is 4986600.0000, 
raw observation next is [7.666666666666667, 25.83333333333334, 46.66666666666666, 416.3333333333333, 26.0, 27.65363855510709, 17.71954425510521, 1.0, 1.0, 0.913098506647945], 
processed observation next is [1.0, 0.7391304347826086, 0.674976915974146, 0.2583333333333334, 0.15555555555555553, 0.460036832412523, 1.0, 1.236234079301013, 0.1771954425510521, 1.0, 1.0, 0.006522132190342464], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5371637], dtype=float32), 0.37007776]. 
=============================================
[2019-04-01 16:04:48,881] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4505661e-21 3.7793584e-25 2.6788332e-24 1.5020618e-35 1.2294153e-16
 4.5227544e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 16:04:48,884] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4847
[2019-04-01 16:04:48,896] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 40.0, 0.0, 0.0, 26.0, 25.53545512563784, 9.645437646727949, 0.0, 1.0, 28.6163365853017], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5012400.0000, 
sim time next is 5013000.0000, 
raw observation next is [1.5, 40.0, 0.0, 0.0, 26.0, 25.56398450930364, 9.532128480252963, 0.0, 1.0, 30.86276972343485], 
processed observation next is [1.0, 0.0, 0.5041551246537397, 0.4, 0.0, 0.0, 1.0, 0.937712072757663, 0.09532128480252963, 0.0, 1.0, 0.2204483551673918], 
reward next is 0.7796, 
noisyNet noise sample is [array([-0.8147104], dtype=float32), -0.7523869]. 
=============================================
[2019-04-01 16:04:48,899] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[65.79154]
 [66.01535]
 [66.13442]
 [65.97039]
 [65.95611]], R is [[65.63041687]
 [65.76971436]
 [65.92459106]
 [66.08027649]
 [66.22106934]].
[2019-04-01 16:04:51,183] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5787369e-06 5.8231788e-17 5.7770871e-11 1.0011713e-14 2.1379049e-06
 1.5567976e-07 9.9999511e-01], sum to 1.0000
[2019-04-01 16:04:51,184] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7692
[2019-04-01 16:04:51,205] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.666666666666667, 36.83333333333333, 118.3333333333333, 821.0, 26.0, 27.33743118568696, 18.96553370095571, 1.0, 1.0, 5.67661744377977], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5050200.0000, 
sim time next is 5050800.0000, 
raw observation next is [5.0, 36.0, 119.5, 827.0, 26.0, 27.37134571453512, 14.55885281547825, 1.0, 1.0, 5.496423239363098], 
processed observation next is [1.0, 0.4782608695652174, 0.6011080332409973, 0.36, 0.3983333333333333, 0.9138121546961326, 1.0, 1.1959065306478744, 0.14558852815478251, 1.0, 1.0, 0.0392601659954507], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.57221377], dtype=float32), -0.7560329]. 
=============================================
[2019-04-01 16:04:51,247] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:51,247] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:51,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run4
[2019-04-01 16:04:52,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:52,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:52,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run4
[2019-04-01 16:04:52,250] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:52,251] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:52,255] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run4
[2019-04-01 16:04:53,022] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:53,023] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run4
[2019-04-01 16:04:53,151] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:53,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:53,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:53,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:53,155] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run4
[2019-04-01 16:04:53,170] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run4
[2019-04-01 16:04:53,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:53,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:53,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run4
[2019-04-01 16:04:53,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:53,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:53,591] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run4
[2019-04-01 16:04:53,732] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:53,732] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:53,734] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run4
[2019-04-01 16:04:53,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:53,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:53,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run4
[2019-04-01 16:04:53,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:53,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:53,943] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run4
[2019-04-01 16:04:54,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:54,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:54,323] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run4
[2019-04-01 16:04:54,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:54,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:54,471] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run4
[2019-04-01 16:04:54,556] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:54,559] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:54,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run4
[2019-04-01 16:04:54,814] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:54,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:54,816] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run4
[2019-04-01 16:04:56,315] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:04:56,315] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:04:56,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run4
[2019-04-01 16:05:05,185] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1488778e-23 4.6287639e-33 5.0057910e-23 3.8567271e-38 2.7590109e-25
 2.2838227e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 16:05:05,185] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7745
[2019-04-01 16:05:05,299] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.8, 86.0, 84.33333333333334, 0.0, 26.0, 24.37229006592142, 6.309986271370327, 0.0, 1.0, 36.94034258568596], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 49800.0000, 
sim time next is 50400.0000, 
raw observation next is [7.7, 86.0, 83.0, 0.0, 26.0, 24.3526196870984, 6.360412227192216, 0.0, 1.0, 40.98246538283797], 
processed observation next is [0.0, 0.6086956521739131, 0.6759002770083103, 0.86, 0.27666666666666667, 0.0, 1.0, 0.7646599552997715, 0.06360412227192216, 0.0, 1.0, 0.2927318955916998], 
reward next is 0.7073, 
noisyNet noise sample is [array([1.3522636], dtype=float32), -0.29049718]. 
=============================================
[2019-04-01 16:05:09,418] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1172796e-23 3.3194296e-26 6.3206445e-25 4.6005877e-30 1.4709807e-22
 2.0472669e-25 1.0000000e+00], sum to 1.0000
[2019-04-01 16:05:09,418] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9636
[2019-04-01 16:05:09,443] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 24.29889063619081, 5.898019593113379, 0.0, 1.0, 39.60299653754293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 86400.0000, 
sim time next is 87000.0000, 
raw observation next is [-0.09999999999999999, 94.33333333333334, 0.0, 0.0, 26.0, 24.29575581819556, 5.865834616976014, 0.0, 1.0, 39.64467681055822], 
processed observation next is [1.0, 0.0, 0.4598337950138504, 0.9433333333333335, 0.0, 0.0, 1.0, 0.7565365454565084, 0.05865834616976014, 0.0, 1.0, 0.2831762629325587], 
reward next is 0.7168, 
noisyNet noise sample is [array([0.55754226], dtype=float32), -1.4554926]. 
=============================================
[2019-04-01 16:05:09,474] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[40.03908 ]
 [40.632977]
 [40.791943]
 [40.946674]
 [41.09432 ]], R is [[39.57279205]
 [39.89418411]
 [40.21245956]
 [40.52766418]
 [40.83980942]].
[2019-04-01 16:05:19,763] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.8872272e-17 4.7644656e-27 6.6720086e-18 5.7171084e-27 8.8968602e-17
 4.7453660e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 16:05:19,763] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3286
[2019-04-01 16:05:19,783] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.85912592939003, 6.365888675926965, 0.0, 1.0, 44.29775346444827], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 193200.0000, 
sim time next is 193800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.79255876389044, 6.470579500187881, 0.0, 1.0, 44.33528331511202], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.78, 0.0, 0.0, 1.0, 0.5417941091272057, 0.06470579500187881, 0.0, 1.0, 0.316680595107943], 
reward next is 0.6833, 
noisyNet noise sample is [array([0.51973855], dtype=float32), 1.4253591]. 
=============================================
[2019-04-01 16:05:20,728] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.5002217e-20 7.7775573e-31 5.7435318e-21 9.9769096e-31 2.8655419e-22
 1.4930046e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 16:05:20,729] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0964
[2019-04-01 16:05:20,848] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.816666666666666, 78.0, 22.66666666666667, 203.3333333333333, 26.0, 24.97919037025819, 6.355872273139927, 1.0, 1.0, 69.20609124021642], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 202200.0000, 
sim time next is 202800.0000, 
raw observation next is [-8.733333333333334, 78.0, 28.33333333333334, 249.6666666666667, 26.0, 25.04089896121245, 6.625492777606098, 1.0, 1.0, 59.56167759332418], 
processed observation next is [1.0, 0.34782608695652173, 0.22068328716528163, 0.78, 0.09444444444444447, 0.27587476979742176, 1.0, 0.8629855658874926, 0.06625492777606098, 1.0, 1.0, 0.42544055423802984], 
reward next is 0.5746, 
noisyNet noise sample is [array([0.3226335], dtype=float32), 0.20830168]. 
=============================================
[2019-04-01 16:05:24,415] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7002883e-16 4.3644092e-22 4.3110681e-19 7.4666250e-30 1.5496388e-06
 2.6268163e-12 9.9999845e-01], sum to 1.0000
[2019-04-01 16:05:24,415] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1283
[2019-04-01 16:05:24,499] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 64.0, 15.0, 0.0, 26.0, 25.2306998525842, 7.33309746148436, 1.0, 1.0, 29.66654681849714], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 232800.0000, 
sim time next is 233400.0000, 
raw observation next is [-3.4, 64.5, 12.0, 0.0, 26.0, 25.40577381195857, 7.440984737326076, 1.0, 1.0, 23.2035787728164], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.645, 0.04, 0.0, 1.0, 0.9151105445655102, 0.07440984737326076, 1.0, 1.0, 0.16573984837726], 
reward next is 0.8343, 
noisyNet noise sample is [array([0.22305715], dtype=float32), -0.08342235]. 
=============================================
[2019-04-01 16:05:35,438] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8018135e-18 1.4743091e-26 1.4295036e-21 2.2551710e-28 2.2682235e-14
 6.8668460e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 16:05:35,439] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7345
[2019-04-01 16:05:35,454] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.21235129416974, 8.206124990156148, 0.0, 1.0, 47.69200509281462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 363600.0000, 
sim time next is 364200.0000, 
raw observation next is [-15.7, 73.83333333333334, 0.0, 0.0, 26.0, 22.18409448710596, 8.317456447659568, 0.0, 1.0, 47.90594622803255], 
processed observation next is [1.0, 0.21739130434782608, 0.02770083102493075, 0.7383333333333334, 0.0, 0.0, 1.0, 0.454870641015137, 0.08317456447659569, 0.0, 1.0, 0.3421853302002325], 
reward next is 0.6578, 
noisyNet noise sample is [array([0.83270997], dtype=float32), -1.0924052]. 
=============================================
[2019-04-01 16:05:37,482] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1869584e-19 1.5155344e-29 1.2371635e-24 1.0169230e-29 3.9739183e-16
 4.2639382e-08 1.0000000e+00], sum to 1.0000
[2019-04-01 16:05:37,482] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4986
[2019-04-01 16:05:37,520] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 37.0, 21.0, 403.0, 26.0, 26.00155729343197, 10.09332554610547, 1.0, 1.0, 21.2487081710379], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 405000.0000, 
sim time next is 405600.0000, 
raw observation next is [-8.9, 36.66666666666667, 17.5, 338.6666666666667, 26.0, 26.06656538947232, 9.480443527527983, 1.0, 1.0, 20.34535050721596], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.3666666666666667, 0.058333333333333334, 0.37421731123388585, 1.0, 1.0095093413531886, 0.09480443527527983, 1.0, 1.0, 0.14532393219439974], 
reward next is 0.8547, 
noisyNet noise sample is [array([-0.26571795], dtype=float32), -1.054145]. 
=============================================
[2019-04-01 16:05:41,642] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6858697e-19 3.7691141e-29 2.8352700e-30 2.0755858e-38 4.1378351e-13
 1.7461795e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:05:41,643] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5666
[2019-04-01 16:05:41,706] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.5, 46.16666666666667, 0.0, 0.0, 26.0, 24.81965291927406, 6.988150938188251, 0.0, 1.0, 45.49185863350731], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 420600.0000, 
sim time next is 421200.0000, 
raw observation next is [-10.6, 47.0, 0.0, 0.0, 26.0, 24.71507395478848, 6.786262811360747, 0.0, 1.0, 45.34314724183009], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.47, 0.0, 0.0, 1.0, 0.8164391363983542, 0.06786262811360748, 0.0, 1.0, 0.3238796231559292], 
reward next is 0.6761, 
noisyNet noise sample is [array([-1.116657], dtype=float32), 1.4160521]. 
=============================================
[2019-04-01 16:05:47,682] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0606879e-29 0.0000000e+00 6.0023522e-32 0.0000000e+00 8.2584741e-31
 1.3213201e-26 1.0000000e+00], sum to 1.0000
[2019-04-01 16:05:47,682] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2849
[2019-04-01 16:05:47,718] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 89.0, 0.0, 0.0, 26.0, 24.92490041909697, 6.893461438343297, 0.0, 1.0, 39.61157406000461], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 522000.0000, 
sim time next is 522600.0000, 
raw observation next is [4.883333333333334, 88.83333333333334, 0.0, 0.0, 26.0, 25.04863112171326, 6.937317871490509, 0.0, 1.0, 38.71255858728424], 
processed observation next is [0.0, 0.043478260869565216, 0.597876269621422, 0.8883333333333334, 0.0, 0.0, 1.0, 0.8640901602447515, 0.06937317871490509, 0.0, 1.0, 0.2765182756234589], 
reward next is 0.7235, 
noisyNet noise sample is [array([0.5852762], dtype=float32), -0.3497678]. 
=============================================
[2019-04-01 16:05:54,540] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.3178534e-25 2.7258919e-31 5.0849892e-23 7.8194871e-35 3.9472642e-25
 1.5844243e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 16:05:54,541] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9243
[2019-04-01 16:05:54,582] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 83.0, 70.5, 59.0, 26.0, 25.00272615321628, 7.894194304494452, 0.0, 1.0, 30.10881876852105], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 576000.0000, 
sim time next is 576600.0000, 
raw observation next is [-1.283333333333333, 83.66666666666667, 60.66666666666666, 54.33333333333333, 26.0, 25.0044205947527, 7.825592016364983, 0.0, 1.0, 31.23906320004865], 
processed observation next is [0.0, 0.6956521739130435, 0.4270544783010157, 0.8366666666666667, 0.2022222222222222, 0.060036832412523014, 1.0, 0.8577743706789569, 0.07825592016364982, 0.0, 1.0, 0.2231361657146332], 
reward next is 0.7769, 
noisyNet noise sample is [array([0.8606964], dtype=float32), 0.642666]. 
=============================================
[2019-04-01 16:05:57,983] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9573713e-26 1.0537878e-30 1.1465923e-24 8.7141465e-37 2.2744551e-20
 5.9550610e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 16:05:57,983] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4235
[2019-04-01 16:05:58,043] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.7, 61.0, 100.5, 69.0, 26.0, 24.85425995965222, 6.90668825456473, 0.0, 1.0, 48.03567376077893], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 648000.0000, 
sim time next is 648600.0000, 
raw observation next is [-2.633333333333334, 60.66666666666667, 104.3333333333333, 79.33333333333334, 26.0, 24.95718134347314, 7.021968884413421, 0.0, 1.0, 44.96921024713716], 
processed observation next is [0.0, 0.5217391304347826, 0.3896583564173592, 0.6066666666666667, 0.3477777777777777, 0.08766114180478822, 1.0, 0.8510259062104488, 0.07021968884413421, 0.0, 1.0, 0.3212086446224083], 
reward next is 0.6788, 
noisyNet noise sample is [array([-0.1675605], dtype=float32), -1.3942223]. 
=============================================
[2019-04-01 16:06:01,158] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.3595447e-22 1.0980768e-28 2.6490463e-18 8.0944085e-33 1.6172528e-21
 7.2136701e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 16:06:01,158] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4904
[2019-04-01 16:06:01,256] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.933333333333334, 62.33333333333333, 92.83333333333334, 48.33333333333333, 26.0, 24.7861962290119, 6.539234772070887, 0.0, 1.0, 51.18936036465197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 646800.0000, 
sim time next is 647400.0000, 
raw observation next is [-2.816666666666667, 61.66666666666666, 96.66666666666667, 58.66666666666666, 26.0, 24.78480408408101, 6.728769026263566, 0.0, 1.0, 56.61962407258758], 
processed observation next is [0.0, 0.4782608695652174, 0.38457987072945526, 0.6166666666666666, 0.32222222222222224, 0.06482504604051564, 1.0, 0.8264005834401443, 0.06728769026263566, 0.0, 1.0, 0.40442588623276843], 
reward next is 0.5956, 
noisyNet noise sample is [array([0.5785027], dtype=float32), 0.9575627]. 
=============================================
[2019-04-01 16:06:08,863] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5362048e-20 1.9213071e-26 1.6782840e-24 1.4363520e-37 3.3879078e-12
 3.2985150e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 16:06:08,865] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5089
[2019-04-01 16:06:08,912] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 56.0, 0.0, 0.0, 26.0, 25.4757328345349, 9.212415297442305, 1.0, 1.0, 29.75669113952523], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 756000.0000, 
sim time next is 756600.0000, 
raw observation next is [-3.9, 55.5, 0.0, 0.0, 26.0, 25.65117592208924, 9.004179868342803, 1.0, 1.0, 31.0451235778715], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.555, 0.0, 0.0, 1.0, 0.9501679888698915, 0.09004179868342803, 1.0, 1.0, 0.22175088269908214], 
reward next is 0.7782, 
noisyNet noise sample is [array([0.74296683], dtype=float32), 0.4155393]. 
=============================================
[2019-04-01 16:06:09,651] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.9660096e-18 2.0251756e-20 4.9932791e-18 3.3577496e-29 4.6854143e-10
 1.4175430e-05 9.9998581e-01], sum to 1.0000
[2019-04-01 16:06:09,651] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1168
[2019-04-01 16:06:09,699] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 55.0, 0.0, 0.0, 26.0, 25.52173452158665, 8.470699061476607, 1.0, 1.0, 29.0605945480767], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 757200.0000, 
sim time next is 757800.0000, 
raw observation next is [-3.9, 54.5, 0.0, 0.0, 26.0, 25.28147710465402, 7.979689345753471, 1.0, 1.0, 25.61746850246612], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.545, 0.0, 0.0, 1.0, 0.8973538720934313, 0.07979689345753471, 1.0, 1.0, 0.182981917874758], 
reward next is 0.8170, 
noisyNet noise sample is [array([-1.6048051], dtype=float32), 0.1561937]. 
=============================================
[2019-04-01 16:06:19,682] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0681122e-24 6.2027775e-37 5.3005122e-25 7.2734150e-38 2.9939197e-13
 1.1375247e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 16:06:19,682] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1160
[2019-04-01 16:06:19,697] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.46625926682733, 5.688321928546415, 0.0, 1.0, 38.02756608286612], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 888600.0000, 
sim time next is 889200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.50625358302045, 5.701957523950955, 0.0, 1.0, 38.00712609963331], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 1.0, 0.786607654717207, 0.05701957523950955, 0.0, 1.0, 0.27147947214023793], 
reward next is 0.7285, 
noisyNet noise sample is [array([0.8779779], dtype=float32), 0.4095538]. 
=============================================
[2019-04-01 16:06:20,676] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0433187e-24 1.0616073e-37 1.0693388e-28 0.0000000e+00 1.2691584e-22
 1.8942726e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 16:06:20,678] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1132
[2019-04-01 16:06:20,753] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.55, 76.0, 29.0, 0.0, 26.0, 25.08471133475434, 6.834865241725583, 1.0, 1.0, 29.96272547382501], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 894600.0000, 
sim time next is 895200.0000, 
raw observation next is [0.7333333333333334, 77.33333333333333, 32.16666666666666, 0.0, 26.0, 25.27292906343324, 7.075606398295716, 1.0, 1.0, 25.15207727868229], 
processed observation next is [1.0, 0.34782608695652173, 0.4829178208679595, 0.7733333333333333, 0.10722222222222219, 0.0, 1.0, 0.8961327233476055, 0.07075606398295715, 1.0, 1.0, 0.17965769484773064], 
reward next is 0.8203, 
noisyNet noise sample is [array([0.15611269], dtype=float32), 1.6439775]. 
=============================================
[2019-04-01 16:06:25,664] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0061814e-26 5.3385847e-31 5.0034643e-22 0.0000000e+00 4.3806427e-23
 5.0340107e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 16:06:25,667] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8876
[2019-04-01 16:06:25,780] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.0, 83.0, 0.0, 0.0, 26.0, 25.40330344893153, 9.304191431633457, 0.0, 1.0, 35.81901891741513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 975600.0000, 
sim time next is 976200.0000, 
raw observation next is [9.9, 84.66666666666667, 0.0, 0.0, 26.0, 25.39279577524377, 10.40687259869332, 0.0, 1.0, 47.73069406966497], 
processed observation next is [1.0, 0.30434782608695654, 0.7368421052631581, 0.8466666666666667, 0.0, 0.0, 1.0, 0.9132565393205384, 0.1040687259869332, 0.0, 1.0, 0.3409335290690355], 
reward next is 0.6591, 
noisyNet noise sample is [array([0.93680435], dtype=float32), 0.8883504]. 
=============================================
[2019-04-01 16:06:26,704] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.6587898e-09 1.5878824e-08 4.0003617e-10 8.0297443e-21 9.9998391e-01
 9.1994243e-06 6.9084408e-06], sum to 1.0000
[2019-04-01 16:06:26,705] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6531
[2019-04-01 16:06:26,720] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 81.0, 98.16666666666667, 0.0, 26.0, 26.79325197521719, 16.39854547348704, 1.0, 1.0, 5.869678876795775], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1002000.0000, 
sim time next is 1002600.0000, 
raw observation next is [14.4, 81.0, 94.0, 0.0, 26.0, 26.84019246401486, 16.5108474487778, 1.0, 1.0, 5.931500642065396], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.31333333333333335, 0.0, 1.0, 1.1200274948592654, 0.165108474487778, 1.0, 1.0, 0.042367861729038545], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.93897384], dtype=float32), 1.3632569]. 
=============================================
[2019-04-01 16:06:27,412] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.49894975e-15 3.20365364e-17 1.01268025e-17 1.10995232e-25
 9.50386584e-01 3.53404452e-08 4.96134833e-02], sum to 1.0000
[2019-04-01 16:06:27,417] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2943
[2019-04-01 16:06:27,423] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.58333333333333, 80.5, 0.0, 0.0, 26.0, 26.04721464692044, 12.60413435078197, 1.0, 1.0, 4.967714421503383], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1014600.0000, 
sim time next is 1015200.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 26.14052912203598, 12.56225011621722, 1.0, 1.0, 4.558544911937554], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 1.0, 1.0200755888622826, 0.1256225011621722, 1.0, 1.0, 0.032561035085268246], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3387661], dtype=float32), 0.67014]. 
=============================================
[2019-04-01 16:06:28,583] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.7354282e-22 5.0296319e-19 4.7166336e-20 1.8785186e-34 1.1612378e-15
 4.8332965e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 16:06:28,588] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1775
[2019-04-01 16:06:28,593] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.18333333333333, 56.33333333333333, 0.0, 0.0, 26.0, 26.51375321662696, 17.50818541208112, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1104600.0000, 
sim time next is 1105200.0000, 
raw observation next is [15.0, 57.0, 0.0, 0.0, 26.0, 26.39244570501097, 17.28165931929359, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8781163434903049, 0.57, 0.0, 0.0, 1.0, 1.0560636721444243, 0.17281659319293588, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.36396736], dtype=float32), -0.08216231]. 
=============================================
[2019-04-01 16:06:30,567] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4797786e-28 1.2358329e-32 8.8691260e-28 0.0000000e+00 2.6948975e-29
 4.0069698e-29 1.0000000e+00], sum to 1.0000
[2019-04-01 16:06:30,572] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1851
[2019-04-01 16:06:30,585] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.56666666666667, 78.0, 39.66666666666666, 0.0, 26.0, 25.72815240780995, 13.27709740911019, 0.0, 1.0, 15.02862266634533], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1154400.0000, 
sim time next is 1155000.0000, 
raw observation next is [15.03333333333333, 76.5, 48.33333333333333, 0.0, 26.0, 25.73360817926043, 13.16517359662319, 0.0, 1.0, 14.2699584500194], 
processed observation next is [0.0, 0.34782608695652173, 0.879039704524469, 0.765, 0.1611111111111111, 0.0, 1.0, 0.9619440256086327, 0.1316517359662319, 0.0, 1.0, 0.10192827464299571], 
reward next is 0.8981, 
noisyNet noise sample is [array([1.3100314], dtype=float32), 0.8641088]. 
=============================================
[2019-04-01 16:06:30,599] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.83489 ]
 [80.853294]
 [80.89344 ]
 [80.890144]
 [80.82371 ]], R is [[80.90685272]
 [80.99044037]
 [81.06999969]
 [81.13021851]
 [81.16553497]].
[2019-04-01 16:06:33,987] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4256930e-34 0.0000000e+00 3.2511789e-27 0.0000000e+00 1.7838478e-24
 7.1922176e-30 1.0000000e+00], sum to 1.0000
[2019-04-01 16:06:33,990] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5153
[2019-04-01 16:06:33,998] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.55, 64.0, 80.0, 0.0, 26.0, 25.02969252574574, 10.28785339958446, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1179000.0000, 
sim time next is 1179600.0000, 
raw observation next is [18.63333333333333, 63.66666666666667, 71.5, 0.0, 26.0, 25.01583100293387, 10.32596563138879, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.9787626962142197, 0.6366666666666667, 0.23833333333333334, 0.0, 1.0, 0.8594044289905527, 0.10325965631388791, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0807889], dtype=float32), -2.636448]. 
=============================================
[2019-04-01 16:06:40,375] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.7509567e-16 2.5848215e-17 1.8255035e-12 1.5414688e-22 1.3460933e-12
 4.8246838e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 16:06:40,375] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5037
[2019-04-01 16:06:40,403] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.48892940386551, 10.98266381198607, 0.0, 1.0, 34.07244291944291], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1317000.0000, 
sim time next is 1317600.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.4598273785883, 10.72402164263442, 0.0, 1.0, 35.58302437116088], 
processed observation next is [1.0, 0.2608695652173913, 0.5069252077562327, 0.92, 0.0, 0.0, 1.0, 0.9228324826554716, 0.10724021642634421, 0.0, 1.0, 0.2541644597940063], 
reward next is 0.7458, 
noisyNet noise sample is [array([-0.40399486], dtype=float32), 0.7420884]. 
=============================================
[2019-04-01 16:06:44,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9006661e-18 8.8541794e-29 3.7381648e-17 2.2599817e-31 3.0554359e-19
 2.2947773e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 16:06:44,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6560
[2019-04-01 16:06:44,766] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.17919998619124, 9.169668212267682, 0.0, 1.0, 38.11486133031943], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1404000.0000, 
sim time next is 1404600.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.21081825210331, 9.051830130999017, 0.0, 1.0, 38.0908628513726], 
processed observation next is [1.0, 0.2608695652173913, 0.44598337950138506, 1.0, 0.0, 0.0, 1.0, 0.8872597503004727, 0.09051830130999017, 0.0, 1.0, 0.27207759179551855], 
reward next is 0.7279, 
noisyNet noise sample is [array([-0.67346853], dtype=float32), -0.33537176]. 
=============================================
[2019-04-01 16:06:46,447] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0234305e-22 3.4424569e-33 1.3915222e-23 7.2729546e-35 2.2998872e-13
 3.7971630e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 16:06:46,447] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3251
[2019-04-01 16:06:46,461] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 92.66666666666667, 0.0, 0.0, 26.0, 25.3587235961479, 9.947571431925278, 0.0, 1.0, 36.34887308939864], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1477200.0000, 
sim time next is 1477800.0000, 
raw observation next is [2.2, 93.0, 0.0, 0.0, 26.0, 25.43555974159145, 9.882769428457856, 0.0, 1.0, 35.97609621380085], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.93, 0.0, 0.0, 1.0, 0.9193656773702072, 0.09882769428457855, 0.0, 1.0, 0.2569721158128632], 
reward next is 0.7430, 
noisyNet noise sample is [array([-1.9080812], dtype=float32), -1.0743685]. 
=============================================
[2019-04-01 16:06:50,611] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8577525e-19 1.3441107e-26 3.1632899e-20 1.2050724e-28 8.1580166e-11
 2.0871317e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 16:06:50,612] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9335
[2019-04-01 16:06:50,633] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 95.33333333333334, 0.0, 0.0, 26.0, 25.39507829901318, 9.599307594331192, 0.0, 1.0, 36.05086799618813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1482000.0000, 
sim time next is 1482600.0000, 
raw observation next is [2.2, 95.66666666666666, 0.0, 0.0, 26.0, 25.39075449316588, 9.42037332915037, 0.0, 1.0, 35.07063676081918], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9566666666666666, 0.0, 0.0, 1.0, 0.9129649275951256, 0.09420373329150371, 0.0, 1.0, 0.25050454829156554], 
reward next is 0.7495, 
noisyNet noise sample is [array([-0.6771099], dtype=float32), -0.7731433]. 
=============================================
[2019-04-01 16:06:50,884] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.1418195e-15 1.9023172e-25 1.3419241e-17 8.1739610e-27 1.4216267e-17
 3.0588342e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 16:06:50,885] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5953
[2019-04-01 16:06:50,920] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.35, 100.0, 18.0, 0.0, 26.0, 25.38044543206191, 9.227035424816448, 1.0, 1.0, 17.79449441492591], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1499400.0000, 
sim time next is 1500000.0000, 
raw observation next is [1.433333333333333, 100.0, 22.83333333333333, 0.0, 26.0, 25.53002598127041, 9.325928806177279, 1.0, 1.0, 16.80532033887659], 
processed observation next is [1.0, 0.34782608695652173, 0.502308402585411, 1.0, 0.0761111111111111, 0.0, 1.0, 0.9328608544672014, 0.09325928806177279, 1.0, 1.0, 0.12003800242054707], 
reward next is 0.8800, 
noisyNet noise sample is [array([-2.0996454], dtype=float32), 0.21397175]. 
=============================================
[2019-04-01 16:06:50,925] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[55.16598 ]
 [56.28996 ]
 [57.489567]
 [59.14845 ]
 [60.938564]], R is [[54.58048248]
 [54.90757751]
 [55.21326828]
 [55.51782608]
 [55.81084061]].
[2019-04-01 16:06:52,309] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1418469e-09 7.2343901e-18 8.4845209e-10 4.3505241e-20 7.7552491e-01
 2.5636118e-07 2.2447480e-01], sum to 1.0000
[2019-04-01 16:06:52,312] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5298
[2019-04-01 16:06:52,326] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.383333333333333, 99.33333333333334, 64.33333333333333, 0.0, 26.0, 25.8415804005808, 9.973203692741775, 1.0, 1.0, 10.90190042272143], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1505400.0000, 
sim time next is 1506000.0000, 
raw observation next is [2.566666666666667, 98.66666666666667, 68.66666666666667, 0.0, 26.0, 25.82729745552839, 10.05588471032869, 1.0, 1.0, 10.2173731966053], 
processed observation next is [1.0, 0.43478260869565216, 0.5337026777469991, 0.9866666666666667, 0.2288888888888889, 0.0, 1.0, 0.9753282079326274, 0.10055884710328691, 1.0, 1.0, 0.07298123711860928], 
reward next is 0.9047, 
noisyNet noise sample is [array([0.50460017], dtype=float32), -1.285624]. 
=============================================
[2019-04-01 16:06:52,362] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[48.985363]
 [48.531128]
 [48.105087]
 [47.799778]
 [47.52874 ]], R is [[49.9684906 ]
 [50.39093781]
 [50.79946518]
 [51.10115814]
 [51.43435669]].
[2019-04-01 16:06:56,092] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7057391e-10 2.3792528e-12 5.5609385e-11 8.5647321e-20 4.9159434e-07
 3.4875270e-06 9.9999607e-01], sum to 1.0000
[2019-04-01 16:06:56,092] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6788
[2019-04-01 16:06:56,096] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2193410e-12 1.3225838e-09 3.2827234e-09 3.8675830e-19 1.3867019e-06
 4.7896024e-02 9.5210254e-01], sum to 1.0000
[2019-04-01 16:06:56,096] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9916
[2019-04-01 16:06:56,115] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 49.0, 111.5, 0.0, 26.0, 26.80937533251269, 17.29389978414146, 1.0, 1.0, 5.045714104353869], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1609200.0000, 
sim time next is 1609800.0000, 
raw observation next is [13.71666666666667, 49.33333333333334, 100.3333333333333, 0.0, 26.0, 25.99170820643415, 15.79566074012679, 1.0, 1.0, 4.549985275604617], 
processed observation next is [1.0, 0.6521739130434783, 0.8425669436749772, 0.4933333333333334, 0.3344444444444443, 0.0, 1.0, 0.9988154580620215, 0.1579566074012679, 1.0, 1.0, 0.032499894825747264], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.055105], dtype=float32), -0.17829648]. 
=============================================
[2019-04-01 16:06:56,117] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.43333333333334, 50.33333333333334, 158.0, 82.66666666666667, 26.0, 26.47186304444934, 15.8616835938325, 1.0, 1.0, 4.59902989585485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1601400.0000, 
sim time next is 1602000.0000, 
raw observation next is [13.8, 49.0, 162.5, 62.0, 26.0, 25.71909161564803, 14.66139933846083, 1.0, 1.0, 4.178259125408271], 
processed observation next is [1.0, 0.5652173913043478, 0.844875346260388, 0.49, 0.5416666666666666, 0.06850828729281767, 1.0, 0.9598702308068613, 0.14661399338460832, 1.0, 1.0, 0.029844708038630506], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.07467436], dtype=float32), 1.0322495]. 
=============================================
[2019-04-01 16:06:56,129] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[31.133104]
 [30.299892]
 [29.351763]
 [28.51009 ]
 [27.767622]], R is [[31.48825836]
 [31.17337608]
 [30.86164284]
 [30.5530262 ]
 [30.24749565]].
[2019-04-01 16:07:01,805] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.2789851e-18 1.5377655e-26 3.4115606e-21 7.1290403e-29 1.5114734e-17
 9.0943724e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 16:07:01,805] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5969
[2019-04-01 16:07:01,820] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.25, 93.5, 0.0, 0.0, 26.0, 25.26946261425346, 9.988315273582954, 0.0, 1.0, 42.7955780624674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1726200.0000, 
sim time next is 1726800.0000, 
raw observation next is [0.3333333333333333, 93.0, 0.0, 0.0, 26.0, 25.26494454298869, 9.930118141392933, 0.0, 1.0, 42.73881502801507], 
processed observation next is [1.0, 1.0, 0.4718374884579871, 0.93, 0.0, 0.0, 1.0, 0.8949920775698129, 0.09930118141392934, 0.0, 1.0, 0.30527725020010765], 
reward next is 0.6947, 
noisyNet noise sample is [array([0.18198651], dtype=float32), 0.9340122]. 
=============================================
[2019-04-01 16:07:06,779] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.2634745e-23 1.9442910e-31 8.0003260e-24 4.4133910e-35 7.1702568e-20
 5.6157990e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 16:07:06,779] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7779
[2019-04-01 16:07:06,804] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.0411335137843, 7.7566623974072, 0.0, 1.0, 45.99774937097657], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1800000.0000, 
sim time next is 1800600.0000, 
raw observation next is [-4.583333333333333, 83.5, 0.0, 0.0, 26.0, 25.03182018233533, 7.673235556337471, 0.0, 1.0, 45.94852667348997], 
processed observation next is [0.0, 0.8695652173913043, 0.3356417359187443, 0.835, 0.0, 0.0, 1.0, 0.8616885974764757, 0.07673235556337471, 0.0, 1.0, 0.3282037619534998], 
reward next is 0.6718, 
noisyNet noise sample is [array([1.5970235], dtype=float32), 0.37393686]. 
=============================================
[2019-04-01 16:07:08,332] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0178498e-18 9.5047031e-27 1.4692135e-23 5.8587079e-30 6.1575352e-15
 1.0673560e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 16:07:08,333] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9375
[2019-04-01 16:07:08,398] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 24.96435589146027, 7.891889547578863, 0.0, 1.0, 43.78923220988295], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1791000.0000, 
sim time next is 1791600.0000, 
raw observation next is [-3.899999999999999, 82.0, 0.0, 0.0, 26.0, 24.93526315209904, 7.940445963439731, 0.0, 1.0, 47.78340361690574], 
processed observation next is [0.0, 0.7391304347826086, 0.35457063711911363, 0.82, 0.0, 0.0, 1.0, 0.8478947360141486, 0.07940445963439731, 0.0, 1.0, 0.341310025835041], 
reward next is 0.6587, 
noisyNet noise sample is [array([-0.64964503], dtype=float32), -1.7551186]. 
=============================================
[2019-04-01 16:07:14,736] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.5115597e-16 4.7486956e-28 3.2876250e-17 5.9005350e-31 6.0579478e-14
 1.9960730e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 16:07:14,754] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8401
[2019-04-01 16:07:14,842] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 76.33333333333333, 43.33333333333333, 0.0, 26.0, 24.91901422167275, 7.275095836980086, 0.0, 1.0, 45.6662525820541], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1872600.0000, 
sim time next is 1873200.0000, 
raw observation next is [-4.5, 77.66666666666667, 36.16666666666666, 0.0, 26.0, 24.91395241265338, 7.477768695381741, 0.0, 1.0, 55.20404184689747], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.7766666666666667, 0.12055555555555553, 0.0, 1.0, 0.8448503446647686, 0.07477768695381741, 0.0, 1.0, 0.3943145846206962], 
reward next is 0.6057, 
noisyNet noise sample is [array([0.7800943], dtype=float32), 0.00502619]. 
=============================================
[2019-04-01 16:07:16,526] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2311864e-20 6.3250553e-26 4.8993868e-25 4.2878269e-30 1.1481271e-19
 1.0917868e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:07:16,526] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6754
[2019-04-01 16:07:16,542] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.566666666666666, 76.33333333333334, 0.0, 0.0, 26.0, 24.51592981779948, 5.878573824531112, 0.0, 1.0, 44.31066374239322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1894800.0000, 
sim time next is 1895400.0000, 
raw observation next is [-6.75, 77.0, 0.0, 0.0, 26.0, 24.47697315237349, 5.822925876580908, 0.0, 1.0, 44.32314806116393], 
processed observation next is [0.0, 0.9565217391304348, 0.275623268698061, 0.77, 0.0, 0.0, 1.0, 0.7824247360533556, 0.05822925876580908, 0.0, 1.0, 0.31659391472259946], 
reward next is 0.6834, 
noisyNet noise sample is [array([0.11441907], dtype=float32), -0.95840764]. 
=============================================
[2019-04-01 16:07:16,734] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.3664085e-17 1.5621725e-26 8.3357761e-21 2.0150021e-27 1.4404985e-17
 1.1688312e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 16:07:16,734] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4937
[2019-04-01 16:07:16,751] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.36079875131144, 5.675571906706746, 0.0, 1.0, 44.32750701585842], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1897200.0000, 
sim time next is 1897800.0000, 
raw observation next is [-7.3, 79.50000000000001, 0.0, 0.0, 26.0, 24.31957068318577, 5.632506703568805, 0.0, 1.0, 44.33215439696885], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.7950000000000002, 0.0, 0.0, 1.0, 0.7599386690265385, 0.05632506703568805, 0.0, 1.0, 0.3166582456926347], 
reward next is 0.6833, 
noisyNet noise sample is [array([-1.2151784], dtype=float32), 1.0279963]. 
=============================================
[2019-04-01 16:07:46,407] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.0553937e-17 5.3200241e-31 1.7332325e-22 8.1095849e-33 3.6443553e-06
 1.1427475e-13 9.9999630e-01], sum to 1.0000
[2019-04-01 16:07:46,407] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2585
[2019-04-01 16:07:46,424] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.283333333333333, 75.5, 0.0, 0.0, 26.0, 24.70610955185098, 6.713268673940171, 0.0, 1.0, 43.51891528270796], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2243400.0000, 
sim time next is 2244000.0000, 
raw observation next is [-6.366666666666667, 76.0, 0.0, 0.0, 26.0, 24.64850809940794, 6.59062926288576, 0.0, 1.0, 43.50582906222441], 
processed observation next is [1.0, 1.0, 0.28624192059095105, 0.76, 0.0, 0.0, 1.0, 0.8069297284868487, 0.0659062926288576, 0.0, 1.0, 0.31075592187303147], 
reward next is 0.6892, 
noisyNet noise sample is [array([0.4587683], dtype=float32), 1.4488385]. 
=============================================
[2019-04-01 16:07:46,428] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[62.777496]
 [62.962563]
 [62.419884]
 [62.015682]
 [61.588165]], R is [[63.33501434]
 [63.39081573]
 [63.44577789]
 [63.4997673 ]
 [63.55296326]].
[2019-04-01 16:07:52,289] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.64689410e-24 1.18812356e-29 7.47674725e-17 2.68798112e-34
 1.00935360e-18 3.07696735e-09 1.00000000e+00], sum to 1.0000
[2019-04-01 16:07:52,292] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9102
[2019-04-01 16:07:52,353] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.366666666666667, 54.66666666666667, 0.0, 0.0, 26.0, 25.01154881708338, 8.008164878325253, 0.0, 1.0, 38.06308287867973], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2316000.0000, 
sim time next is 2316600.0000, 
raw observation next is [-1.45, 55.0, 0.0, 0.0, 26.0, 24.96407549262064, 8.123370341640403, 0.0, 1.0, 38.97142014864324], 
processed observation next is [1.0, 0.8260869565217391, 0.422437673130194, 0.55, 0.0, 0.0, 1.0, 0.8520107846600915, 0.08123370341640403, 0.0, 1.0, 0.2783672867760231], 
reward next is 0.7216, 
noisyNet noise sample is [array([-0.7399589], dtype=float32), 0.8181154]. 
=============================================
[2019-04-01 16:07:53,011] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4691884e-24 2.6371662e-35 8.8530832e-25 3.5315524e-38 5.5575863e-23
 6.2168010e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 16:07:53,011] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1750
[2019-04-01 16:07:53,032] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 64.5, 0.0, 0.0, 26.0, 24.99997271933319, 7.331841273821856, 0.0, 1.0, 37.99063292490095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2333400.0000, 
sim time next is 2334000.0000, 
raw observation next is [-2.3, 64.0, 0.0, 0.0, 26.0, 24.98352979292443, 7.221425444117767, 0.0, 1.0, 37.93719918859738], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.64, 0.0, 0.0, 1.0, 0.8547899704177756, 0.07221425444117767, 0.0, 1.0, 0.270979994204267], 
reward next is 0.7290, 
noisyNet noise sample is [array([-0.09468352], dtype=float32), -0.12623176]. 
=============================================
[2019-04-01 16:07:53,046] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.46689 ]
 [76.821014]
 [76.90454 ]
 [76.98073 ]
 [77.06889 ]], R is [[76.10688782]
 [76.07445526]
 [76.04199982]
 [76.00920868]
 [75.97710419]].
[2019-04-01 16:07:53,707] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7220563e-28 1.5487579e-32 6.3442665e-26 6.8577574e-38 8.4051848e-23
 2.6682231e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 16:07:53,709] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9828
[2019-04-01 16:07:53,767] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 37.0, 0.0, 26.0, 25.11283251231447, 7.788825041816497, 0.0, 1.0, 60.38849584486335], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2363400.0000, 
sim time next is 2364000.0000, 
raw observation next is [-3.4, 69.0, 51.0, 59.99999999999999, 26.0, 25.45986400610083, 8.148619157049792, 0.0, 1.0, 47.72827317887899], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.17, 0.06629834254143646, 1.0, 0.9228377151572614, 0.08148619157049791, 0.0, 1.0, 0.3409162369919928], 
reward next is 0.6591, 
noisyNet noise sample is [array([1.196869], dtype=float32), 1.7797514]. 
=============================================
[2019-04-01 16:07:53,777] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[67.028656]
 [66.69489 ]
 [66.30068 ]
 [66.20415 ]
 [66.15478 ]], R is [[67.33208466]
 [67.22742462]
 [66.89605713]
 [66.45928955]
 [66.49500275]].
[2019-04-01 16:08:00,509] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7176819e-26 1.0115695e-34 1.0427129e-27 1.0968284e-37 8.5521004e-27
 6.5062753e-25 1.0000000e+00], sum to 1.0000
[2019-04-01 16:08:00,509] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2738
[2019-04-01 16:08:00,563] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.066666666666666, 42.33333333333334, 0.0, 0.0, 26.0, 25.07585306249716, 7.376145841717386, 0.0, 1.0, 43.41984369413767], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2403600.0000, 
sim time next is 2404200.0000, 
raw observation next is [-3.233333333333333, 42.16666666666666, 0.0, 0.0, 26.0, 25.11786205432624, 7.358024016105593, 0.0, 1.0, 43.10558600424341], 
processed observation next is [0.0, 0.8260869565217391, 0.3730378578024008, 0.4216666666666666, 0.0, 0.0, 1.0, 0.8739802934751769, 0.07358024016105592, 0.0, 1.0, 0.30789704288745295], 
reward next is 0.6921, 
noisyNet noise sample is [array([-0.641727], dtype=float32), -0.61159796]. 
=============================================
[2019-04-01 16:08:13,278] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4701947e-23 2.8061660e-36 1.5455696e-26 1.6767124e-36 5.6704594e-25
 3.7979917e-23 1.0000000e+00], sum to 1.0000
[2019-04-01 16:08:13,280] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8180
[2019-04-01 16:08:13,343] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.97065271777662, 7.158627378359206, 1.0, 1.0, 62.61701452389532], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2619000.0000, 
sim time next is 2619600.0000, 
raw observation next is [-7.3, 79.0, 18.66666666666666, 6.666666666666667, 26.0, 25.36450282490601, 7.421777042139674, 0.0, 1.0, 52.72868966157552], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.79, 0.0622222222222222, 0.007366482504604052, 1.0, 0.9092146892722869, 0.07421777042139674, 0.0, 1.0, 0.37663349758268233], 
reward next is 0.6234, 
noisyNet noise sample is [array([0.31775695], dtype=float32), -0.061482877]. 
=============================================
[2019-04-01 16:08:13,717] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.12198374e-20 4.06209801e-29 3.01049637e-21 1.07384026e-34
 1.75275800e-17 4.67041195e-17 1.00000000e+00], sum to 1.0000
[2019-04-01 16:08:13,717] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5570
[2019-04-01 16:08:13,748] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.266666666666667, 63.0, 164.0, 257.6666666666667, 26.0, 25.76291906743346, 8.26232959052841, 1.0, 1.0, 19.23692324363826], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2630400.0000, 
sim time next is 2631000.0000, 
raw observation next is [-4.083333333333333, 62.5, 176.0, 240.3333333333333, 26.0, 25.7580084685897, 8.293360515918083, 1.0, 1.0, 18.24854724579078], 
processed observation next is [1.0, 0.43478260869565216, 0.34949215143120965, 0.625, 0.5866666666666667, 0.265561694290976, 1.0, 0.9654297812271001, 0.08293360515918083, 1.0, 1.0, 0.1303467660413627], 
reward next is 0.8697, 
noisyNet noise sample is [array([-0.37566942], dtype=float32), -0.93873]. 
=============================================
[2019-04-01 16:08:13,756] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[62.82959 ]
 [63.197605]
 [63.5203  ]
 [63.82369 ]
 [64.16899 ]], R is [[62.80575562]
 [63.04029465]
 [63.26487732]
 [63.47872925]
 [63.68185425]].
[2019-04-01 16:08:14,470] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.9374599e-22 2.6665334e-27 2.5713003e-16 1.3259652e-34 6.6693081e-19
 3.1508452e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:08:14,472] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1616
[2019-04-01 16:08:14,488] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.366666666666667, 59.33333333333334, 212.0, 188.3333333333333, 26.0, 25.68397471983064, 8.162103275152637, 1.0, 1.0, 15.51840373041091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2632800.0000, 
sim time next is 2633400.0000, 
raw observation next is [-3.1, 58.0, 224.0, 171.0, 26.0, 25.67673820451834, 8.173186926066604, 1.0, 1.0, 25.84703005094257], 
processed observation next is [1.0, 0.4782608695652174, 0.37673130193905824, 0.58, 0.7466666666666667, 0.18895027624309393, 1.0, 0.95381974350262, 0.08173186926066604, 1.0, 1.0, 0.18462164322101837], 
reward next is 0.8154, 
noisyNet noise sample is [array([-0.46796754], dtype=float32), 0.10005811]. 
=============================================
[2019-04-01 16:08:21,035] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.86376577e-15 2.94553493e-22 7.86020608e-13 1.33903259e-25
 3.12285232e-11 1.30144745e-05 9.99987006e-01], sum to 1.0000
[2019-04-01 16:08:21,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2891
[2019-04-01 16:08:21,088] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 57.5, 109.0, 788.0, 26.0, 26.87564007066281, 16.36995674555833, 1.0, 1.0, 28.01131270400099], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2727000.0000, 
sim time next is 2727600.0000, 
raw observation next is [-5.199999999999999, 57.0, 107.8333333333333, 778.8333333333334, 26.0, 27.02365543068769, 16.91022815133042, 1.0, 1.0, 26.13426734552643], 
processed observation next is [1.0, 0.5652173913043478, 0.31855955678670367, 0.57, 0.35944444444444434, 0.8605893186003684, 1.0, 1.1462364900982414, 0.1691022815133042, 1.0, 1.0, 0.18667333818233164], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3414345], dtype=float32), 1.5035998]. 
=============================================
[2019-04-01 16:08:21,222] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.92904991e-13 5.27647979e-20 3.92383114e-02 2.88768193e-18
 5.89370730e-09 1.05135426e-01 8.55626225e-01], sum to 1.0000
[2019-04-01 16:08:21,225] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7054
[2019-04-01 16:08:21,277] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 76.0, 107.0, 643.0, 26.0, 25.96768313643442, 9.666282083299047, 1.0, 1.0, 24.63642420678385], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2714400.0000, 
sim time next is 2715000.0000, 
raw observation next is [-11.5, 74.0, 110.3333333333333, 653.3333333333333, 26.0, 26.0024075949468, 9.818772394108429, 1.0, 1.0, 23.13242029860263], 
processed observation next is [1.0, 0.43478260869565216, 0.1440443213296399, 0.74, 0.36777777777777765, 0.721915285451197, 1.0, 1.0003439421352571, 0.09818772394108428, 1.0, 1.0, 0.16523157356144735], 
reward next is 0.8348, 
noisyNet noise sample is [array([0.61781883], dtype=float32), -0.53383684]. 
=============================================
[2019-04-01 16:08:21,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[58.608402]
 [59.22138 ]
 [59.83557 ]
 [60.551113]
 [61.10685 ]], R is [[58.32508469]
 [58.56585693]
 [58.79256439]
 [59.00453949]
 [59.2005806 ]].
[2019-04-01 16:08:25,357] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6596155e-19 2.0608820e-28 5.6626821e-16 1.0163617e-27 3.4765160e-19
 5.1881323e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:08:25,357] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3054
[2019-04-01 16:08:25,477] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.833333333333334, 64.0, 0.0, 0.0, 26.0, 23.59927343626448, 5.403999826011282, 0.0, 1.0, 129.3091199915095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2790600.0000, 
sim time next is 2791200.0000, 
raw observation next is [-6.666666666666667, 64.0, 0.0, 0.0, 26.0, 23.85904630638703, 5.973730141300625, 1.0, 1.0, 113.5164130321547], 
processed observation next is [1.0, 0.30434782608695654, 0.27793167128347185, 0.64, 0.0, 0.0, 1.0, 0.6941494723410041, 0.059737301413006254, 1.0, 1.0, 0.8108315216582479], 
reward next is 0.1892, 
noisyNet noise sample is [array([-0.44724208], dtype=float32), 1.088647]. 
=============================================
[2019-04-01 16:08:25,764] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0600965e-20 2.8499674e-27 1.6216052e-17 1.7599546e-31 7.6367775e-12
 4.9961041e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 16:08:25,764] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1909
[2019-04-01 16:08:25,802] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.3, 26.5, 71.0, 76.0, 26.0, 24.87251904316527, 7.396783489885796, 1.0, 1.0, 31.9923376491806], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2824200.0000, 
sim time next is 2824800.0000, 
raw observation next is [6.2, 27.0, 60.00000000000001, 71.0, 26.0, 25.16041951282109, 8.017607823536705, 1.0, 1.0, 19.72619673610721], 
processed observation next is [1.0, 0.6956521739130435, 0.6343490304709142, 0.27, 0.2, 0.07845303867403315, 1.0, 0.8800599304030128, 0.08017607823536706, 1.0, 1.0, 0.14090140525790865], 
reward next is 0.8591, 
noisyNet noise sample is [array([-1.6119432], dtype=float32), 0.76623654]. 
=============================================
[2019-04-01 16:08:25,853] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.3929972e-12 1.3601008e-23 2.4991584e-07 3.2058685e-26 1.3540020e-13
 6.7560188e-04 9.9932420e-01], sum to 1.0000
[2019-04-01 16:08:25,855] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3782
[2019-04-01 16:08:25,890] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 56.5, 159.3333333333333, 324.6666666666666, 26.0, 25.98982658032936, 9.173933369923233, 1.0, 1.0, 22.56010906415238], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2800200.0000, 
sim time next is 2800800.0000, 
raw observation next is [-3.0, 55.0, 163.0, 370.5, 26.0, 25.99520738370196, 9.339301577321985, 1.0, 1.0, 20.74158422182177], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.55, 0.5433333333333333, 0.4093922651933702, 1.0, 0.9993153405288514, 0.09339301577321985, 1.0, 1.0, 0.14815417301301265], 
reward next is 0.8518, 
noisyNet noise sample is [array([-0.21131557], dtype=float32), 0.59373605]. 
=============================================
[2019-04-01 16:08:28,663] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.1297618e-24 1.1224532e-30 1.5172583e-29 0.0000000e+00 4.2410128e-21
 5.8514086e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 16:08:28,664] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1412
[2019-04-01 16:08:28,712] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.35537736174039, 9.519191363967996, 0.0, 1.0, 59.57731659653212], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2838600.0000, 
sim time next is 2839200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.38815970012584, 9.93164453132127, 0.0, 1.0, 53.83204034532005], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 1.0, 0.91259424287512, 0.0993164453132127, 0.0, 1.0, 0.3845145738951432], 
reward next is 0.6155, 
noisyNet noise sample is [array([1.1761478], dtype=float32), 0.5043953]. 
=============================================
[2019-04-01 16:08:29,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7131178e-31 3.0274269e-35 4.5616917e-28 0.0000000e+00 4.8741970e-27
 1.3842206e-13 1.0000000e+00], sum to 1.0000
[2019-04-01 16:08:29,752] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9340
[2019-04-01 16:08:29,771] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666666, 24.83333333333334, 87.0, 25.33333333333333, 26.0, 25.83304753493938, 9.211136953374115, 1.0, 1.0, 17.42112372575385], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2821800.0000, 
sim time next is 2822400.0000, 
raw observation next is [6.6, 25.0, 83.0, 38.0, 26.0, 25.9491922644996, 9.46479671672438, 1.0, 1.0, 16.98881909805598], 
processed observation next is [1.0, 0.6956521739130435, 0.6454293628808865, 0.25, 0.27666666666666667, 0.041988950276243095, 1.0, 0.9927417520713716, 0.09464796716724379, 1.0, 1.0, 0.121348707843257], 
reward next is 0.8787, 
noisyNet noise sample is [array([0.10356376], dtype=float32), -0.5688957]. 
=============================================
[2019-04-01 16:08:45,867] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.8499967e-17 5.5748304e-31 8.7022206e-16 1.5020702e-33 4.7915787e-17
 8.6992157e-05 9.9991298e-01], sum to 1.0000
[2019-04-01 16:08:45,871] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6588
[2019-04-01 16:08:45,880] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 108.0, 746.0, 26.0, 26.68441843972845, 14.59818564309167, 1.0, 1.0, 9.092269610730694], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3148200.0000, 
sim time next is 3148800.0000, 
raw observation next is [7.0, 100.0, 109.0, 755.8333333333334, 26.0, 26.73883823362483, 15.02845199536853, 1.0, 1.0, 8.530385942077366], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.36333333333333334, 0.8351749539594844, 1.0, 1.1055483190892612, 0.1502845199536853, 1.0, 1.0, 0.060931328157695475], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.99501574], dtype=float32), -0.27980837]. 
=============================================
[2019-04-01 16:08:52,967] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0787173e-20 1.4450206e-27 2.5300131e-19 3.0883719e-29 1.8196822e-18
 1.0000000e+00 1.5957115e-22], sum to 1.0000
[2019-04-01 16:08:52,968] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7321
[2019-04-01 16:08:52,997] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 75.66666666666667, 114.6666666666667, 821.0, 26.0, 25.15217363840639, 12.37440245140672, 1.0, 1.0, 34.11065855118264], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3241200.0000, 
sim time next is 3241800.0000, 
raw observation next is [-2.0, 78.0, 115.0, 823.0, 26.0, 25.52172581988674, 13.93390660643509, 1.0, 1.0, 25.81617849206248], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.78, 0.38333333333333336, 0.9093922651933701, 1.0, 0.9316751171266772, 0.1393390660643509, 1.0, 1.0, 0.18440127494330344], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.42050204], dtype=float32), -0.6261675]. 
=============================================
[2019-04-01 16:08:55,844] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6966126e-15 6.4332661e-26 2.5839332e-17 5.0272114e-27 2.0463911e-18
 3.6670335e-07 9.9999964e-01], sum to 1.0000
[2019-04-01 16:08:55,847] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5428
[2019-04-01 16:08:55,866] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.75, 77.0, 0.0, 0.0, 26.0, 24.65705491193307, 6.91508376623316, 0.0, 1.0, 43.29947145738861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3297000.0000, 
sim time next is 3297600.0000, 
raw observation next is [-8.9, 77.0, 0.0, 0.0, 26.0, 24.70008637674698, 6.624455229245783, 0.0, 1.0, 43.96804688088656], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.77, 0.0, 0.0, 1.0, 0.8142980538209973, 0.06624455229245782, 0.0, 1.0, 0.3140574777206183], 
reward next is 0.6859, 
noisyNet noise sample is [array([1.2875873], dtype=float32), -1.7861043]. 
=============================================
[2019-04-01 16:08:57,431] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.8754234e-16 3.2839826e-29 6.3140533e-14 7.0438315e-30 1.0754783e-11
 6.4888547e-05 9.9993515e-01], sum to 1.0000
[2019-04-01 16:08:57,432] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3127
[2019-04-01 16:08:57,449] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.91844360969431, 7.829772561933758, 0.0, 1.0, 41.44721236766615], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3372000.0000, 
sim time next is 3372600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.02084871874892, 7.790626104527031, 0.0, 1.0, 40.44398402815818], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.8601212455355599, 0.0779062610452703, 0.0, 1.0, 0.28888560020112986], 
reward next is 0.7111, 
noisyNet noise sample is [array([-0.20114906], dtype=float32), -0.3048806]. 
=============================================
[2019-04-01 16:08:59,323] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.67215151e-21 1.43608572e-34 4.01066159e-26 1.35760175e-33
 9.83454175e-21 1.51030991e-16 1.00000000e+00], sum to 1.0000
[2019-04-01 16:08:59,326] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8612
[2019-04-01 16:08:59,344] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.80905585164189, 6.872519076549087, 0.0, 1.0, 40.61037824735177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3378600.0000, 
sim time next is 3379200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.78139918576583, 6.894218740675441, 0.0, 1.0, 40.61200386561924], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.8259141693951187, 0.06894218740675441, 0.0, 1.0, 0.2900857418972803], 
reward next is 0.7099, 
noisyNet noise sample is [array([-1.2328069], dtype=float32), 0.33215034]. 
=============================================
[2019-04-01 16:09:05,171] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-01 16:09:05,173] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 16:09:05,174] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:09:05,174] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 16:09:05,175] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 16:09:05,175] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:09:05,175] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:09:05,178] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run6
[2019-04-01 16:09:05,194] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run6
[2019-04-01 16:09:05,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run6
[2019-04-01 16:09:45,421] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.3118969], dtype=float32), -0.12251946]
[2019-04-01 16:09:45,421] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [13.8, 100.0, 92.0, 0.0, 26.0, 24.8007063659971, 9.87007413911496, 0.0, 1.0, 7.590751542659266]
[2019-04-01 16:09:45,421] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:09:45,422] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.2511083e-18 1.1302189e-27 5.8733201e-15 2.9132093e-31 2.0936515e-17
 1.0850252e-09 1.0000000e+00], sampled 0.5288063282568418
[2019-04-01 16:10:23,037] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.3118969], dtype=float32), -0.12251946]
[2019-04-01 16:10:23,038] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [6.9, 44.5, 14.0, 153.0, 26.0, 24.81521356317121, 6.582150505310736, 0.0, 1.0, 17.11071499369081]
[2019-04-01 16:10:23,038] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 16:10:23,039] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.1770728e-18 2.6986744e-28 2.6340020e-18 2.8410608e-31 3.3226643e-16
 4.2601638e-09 1.0000000e+00], sampled 0.45798735461663254
[2019-04-01 16:10:36,109] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.3118969], dtype=float32), -0.12251946]
[2019-04-01 16:10:36,110] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [6.0, 100.0, 28.33333333333333, 185.3333333333333, 26.0, 25.53101867758647, 7.685793797530049, 1.0, 1.0, 29.63420918788542]
[2019-04-01 16:10:36,110] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:10:36,111] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.56196721e-14 5.86413520e-24 2.00761230e-09 6.14954177e-26
 1.05659254e-16 2.52405101e-07 9.99999762e-01], sampled 0.703155702078886
[2019-04-01 16:10:43,441] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.3118969], dtype=float32), -0.12251946]
[2019-04-01 16:10:43,441] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.452794911166666, 88.01930185333333, 0.0, 0.0, 26.0, 25.38781362840861, 9.714911112243945, 0.0, 1.0, 41.83847850222686]
[2019-04-01 16:10:43,441] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 16:10:43,442] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.8369227e-18 9.2664068e-28 5.4259189e-18 6.9158668e-31 2.6984322e-17
 1.5547137e-12 1.0000000e+00], sampled 0.20466302947994353
[2019-04-01 16:10:44,776] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.3118969], dtype=float32), -0.12251946]
[2019-04-01 16:10:44,776] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [10.43333333333333, 68.66666666666667, 0.0, 0.0, 26.0, 25.90882452145945, 12.92292814212444, 0.0, 1.0, 18.37618968648481]
[2019-04-01 16:10:44,776] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 16:10:44,777] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.8456749e-21 5.5290831e-30 6.1167575e-21 4.2458710e-35 7.1644131e-18
 1.4702077e-13 1.0000000e+00], sampled 0.05724852984446838
[2019-04-01 16:10:48,416] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.3118969], dtype=float32), -0.12251946]
[2019-04-01 16:10:48,417] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [22.35, 37.0, 0.0, 0.0, 26.0, 29.96684925141261, 60.95709315461688, 1.0, 0.0, 0.0]
[2019-04-01 16:10:48,417] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 16:10:48,417] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.9747923e-29 0.0000000e+00 1.3366843e-32 0.0000000e+00 2.2831197e-17
 6.2221275e-06 9.9999380e-01], sampled 0.3304993298846737
[2019-04-01 16:10:53,620] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5513.3332 233559.4456 39826.0640
[2019-04-01 16:11:14,410] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5370.2675 256644.4313 36645.7067
[2019-04-01 16:11:16,046] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5742.7793 267589.2952 29529.9393
[2019-04-01 16:11:17,068] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 500000, evaluation results [500000.0, 5370.26752585393, 256644.43132655267, 36645.706724125565, 5513.333204398207, 233559.44559394405, 39826.06395794262, 5742.7792616167935, 267589.2951953736, 29529.93926066535]
[2019-04-01 16:11:20,055] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2932309e-09 8.3246124e-21 3.2208256e-05 2.7425908e-21 1.3760775e-12
 1.4749952e-05 9.9995303e-01], sum to 1.0000
[2019-04-01 16:11:20,056] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2821
[2019-04-01 16:11:20,078] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 66.0, 0.0, 0.0, 26.0, 24.87890903220439, 7.175758015759662, 0.0, 1.0, 40.35419405837252], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3559800.0000, 
sim time next is 3560400.0000, 
raw observation next is [-5.0, 65.0, 0.0, 0.0, 26.0, 24.83177516293782, 7.034486234601796, 0.0, 1.0, 40.36696585669159], 
processed observation next is [0.0, 0.21739130434782608, 0.32409972299168976, 0.65, 0.0, 0.0, 1.0, 0.8331107375625457, 0.07034486234601796, 0.0, 1.0, 0.28833547040493995], 
reward next is 0.7117, 
noisyNet noise sample is [array([2.6288817], dtype=float32), -0.21549678]. 
=============================================
[2019-04-01 16:11:25,494] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.8090019e-20 1.1632115e-37 2.0219158e-21 2.0230556e-38 1.3535819e-14
 1.4897367e-05 9.9998510e-01], sum to 1.0000
[2019-04-01 16:11:25,501] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0806
[2019-04-01 16:11:25,537] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.13059120665818, 8.214948281113278, 0.0, 1.0, 22.98586374921699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3608400.0000, 
sim time next is 3609000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.11801385552249, 8.085150618515469, 0.0, 1.0, 21.31406299128195], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 1.0, 0.874001979360356, 0.08085150618515469, 0.0, 1.0, 0.15224330708058537], 
reward next is 0.8478, 
noisyNet noise sample is [array([0.8735137], dtype=float32), 0.6343439]. 
=============================================
[2019-04-01 16:11:25,550] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[101.16897 ]
 [101.725   ]
 [102.07053 ]
 [102.26892 ]
 [102.449615]], R is [[100.29343414]
 [100.12631989]
 [ 99.94385529]
 [ 99.76678467]
 [ 99.595047  ]].
[2019-04-01 16:11:27,010] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.8641527e-17 1.9141868e-34 4.3542314e-16 3.2444652e-32 6.5736771e-18
 4.6904311e-06 9.9999535e-01], sum to 1.0000
[2019-04-01 16:11:27,012] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0502
[2019-04-01 16:11:27,051] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 54.16666666666666, 113.0, 796.6666666666667, 26.0, 25.34471538335731, 9.731856298930737, 0.0, 1.0, 18.8124253017845], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3582600.0000, 
sim time next is 3583200.0000, 
raw observation next is [-3.666666666666667, 54.33333333333334, 113.5, 806.3333333333333, 26.0, 25.29018076345773, 9.571217594494799, 0.0, 1.0, 20.198098313608], 
processed observation next is [0.0, 0.4782608695652174, 0.3610341643582641, 0.5433333333333334, 0.37833333333333335, 0.8909760589318599, 1.0, 0.8985972519225329, 0.09571217594494799, 0.0, 1.0, 0.1442721308114857], 
reward next is 0.8557, 
noisyNet noise sample is [array([-0.64770573], dtype=float32), -1.2735381]. 
=============================================
[2019-04-01 16:11:34,901] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.2619068e-13 3.0405880e-23 5.6546633e-16 1.7789605e-22 3.3558464e-13
 9.9999821e-01 1.7465708e-06], sum to 1.0000
[2019-04-01 16:11:34,901] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4021
[2019-04-01 16:11:34,923] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 60.0, 117.0, 824.1666666666666, 26.0, 26.38527907895106, 13.28977989059783, 1.0, 1.0, 8.132117559427035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3843600.0000, 
sim time next is 3844200.0000, 
raw observation next is [-1.0, 60.0, 117.0, 826.3333333333334, 26.0, 26.43897610121623, 13.65423830262961, 1.0, 1.0, 7.274344998470152], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.39, 0.9130755064456723, 1.0, 1.0627108716023186, 0.1365423830262961, 1.0, 1.0, 0.05195960713192966], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.01395807], dtype=float32), -0.34801346]. 
=============================================
[2019-04-01 16:11:38,963] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1948547e-10 1.5269572e-17 5.5305243e-11 1.6149254e-16 1.7501684e-03
 4.0501966e-10 9.9824983e-01], sum to 1.0000
[2019-04-01 16:11:38,967] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6017
[2019-04-01 16:11:38,979] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.38842915647792, 9.243135210857602, 0.0, 1.0, 37.36199959445147], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3889800.0000, 
sim time next is 3890400.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.34373087717966, 9.144621647921042, 0.0, 1.0, 38.43540764115892], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 1.0, 0.9062472681685227, 0.09144621647921042, 0.0, 1.0, 0.274538626008278], 
reward next is 0.7255, 
noisyNet noise sample is [array([1.6510214], dtype=float32), 2.030019]. 
=============================================
[2019-04-01 16:11:54,502] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0898047e-05 3.9139062e-20 1.2937992e-13 4.6579959e-20 1.8792027e-05
 9.9954069e-01 4.1963762e-04], sum to 1.0000
[2019-04-01 16:11:54,503] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2484
[2019-04-01 16:11:54,519] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1666666666666666, 42.5, 0.0, 0.0, 26.0, 25.27635602975841, 9.154249239044965, 0.0, 1.0, 41.92294412381999], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4143000.0000, 
sim time next is 4143600.0000, 
raw observation next is [0.0, 43.0, 0.0, 0.0, 26.0, 25.2681629831209, 9.388483204028056, 0.0, 1.0, 43.01518565956094], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.43, 0.0, 0.0, 1.0, 0.895451854731557, 0.09388483204028056, 0.0, 1.0, 0.307251326139721], 
reward next is 0.6927, 
noisyNet noise sample is [array([0.21052134], dtype=float32), 0.44149652]. 
=============================================
[2019-04-01 16:11:54,757] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1265254e-15 1.5091465e-26 1.0883332e-19 3.4047712e-25 1.0548492e-03
 9.9894518e-01 9.2145103e-10], sum to 1.0000
[2019-04-01 16:11:54,763] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8207
[2019-04-01 16:11:54,781] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 37.33333333333334, 0.0, 0.0, 26.0, 25.94862876303791, 12.69108061827019, 0.0, 1.0, 22.88251266914515], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4137600.0000, 
sim time next is 4138200.0000, 
raw observation next is [1.0, 38.0, 0.0, 0.0, 26.0, 25.95035390330796, 12.49927504242096, 0.0, 1.0, 21.6717121828561], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.38, 0.0, 0.0, 1.0, 0.9929077004725657, 0.1249927504242096, 0.0, 1.0, 0.15479794416325787], 
reward next is 0.8452, 
noisyNet noise sample is [array([1.0156047], dtype=float32), 1.7059163]. 
=============================================
[2019-04-01 16:11:54,824] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1473709e-13 1.0576203e-25 1.6999636e-12 3.3595722e-25 4.2090450e-17
 1.0000000e+00 8.0689866e-15], sum to 1.0000
[2019-04-01 16:11:54,824] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8139
[2019-04-01 16:11:54,860] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.166666666666667, 37.33333333333334, 96.0, 539.0, 26.0, 26.22157822015604, 10.04380772967382, 1.0, 1.0, 24.4541639095823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4092600.0000, 
sim time next is 4093200.0000, 
raw observation next is [-3.0, 38.0, 98.0, 574.0, 26.0, 26.16467460586715, 10.20462431893864, 1.0, 1.0, 21.29867279540665], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.38, 0.32666666666666666, 0.6342541436464089, 1.0, 1.0235249436953073, 0.1020462431893864, 1.0, 1.0, 0.15213337711004749], 
reward next is 0.7660, 
noisyNet noise sample is [array([-2.3717914], dtype=float32), 0.5750593]. 
=============================================
[2019-04-01 16:11:57,625] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0634361e-10 1.0740519e-22 9.2917026e-16 3.1701303e-26 1.9396709e-14
 5.3207346e-05 9.9994683e-01], sum to 1.0000
[2019-04-01 16:11:57,628] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7694
[2019-04-01 16:11:57,643] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.066666666666667, 42.83333333333334, 0.0, 0.0, 26.0, 25.44853072341476, 8.95063476937223, 0.0, 1.0, 41.36138493687766], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4218600.0000, 
sim time next is 4219200.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.54360870952559, 9.141065697792278, 0.0, 1.0, 34.50556237559841], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 1.0, 0.9348012442179413, 0.09141065697792278, 0.0, 1.0, 0.24646830268284578], 
reward next is 0.7535, 
noisyNet noise sample is [array([-0.48105147], dtype=float32), 1.0002364]. 
=============================================
[2019-04-01 16:12:00,852] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6885164e-25 0.0000000e+00 1.3149583e-35 0.0000000e+00 1.1011395e-27
 1.0000000e+00 6.5172462e-20], sum to 1.0000
[2019-04-01 16:12:00,853] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4319
[2019-04-01 16:12:00,859] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.933333333333334, 58.00000000000001, 222.1666666666667, 440.3333333333334, 26.0, 25.37286784493733, 9.409207315528567, 0.0, 1.0, 5.165430771638176], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4285200.0000, 
sim time next is 4285800.0000, 
raw observation next is [6.9, 58.5, 229.0, 385.0, 26.0, 25.37620113977554, 9.384510569627325, 0.0, 1.0, 4.057195086348999], 
processed observation next is [0.0, 0.6086956521739131, 0.6537396121883658, 0.585, 0.7633333333333333, 0.425414364640884, 1.0, 0.9108858771107913, 0.09384510569627326, 0.0, 1.0, 0.02897996490249285], 
reward next is 0.9710, 
noisyNet noise sample is [array([0.34386855], dtype=float32), 0.58313996]. 
=============================================
[2019-04-01 16:12:02,619] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3812962e-23 0.0000000e+00 2.1332229e-21 6.5009229e-38 4.5209270e-29
 1.0000000e+00 1.3187416e-19], sum to 1.0000
[2019-04-01 16:12:02,622] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5265
[2019-04-01 16:12:02,641] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 54.5, 204.0, 604.0, 26.0, 25.31820022037928, 8.42414738440806, 0.0, 1.0, 14.42026357968132], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4271400.0000, 
sim time next is 4272000.0000, 
raw observation next is [4.666666666666666, 54.66666666666667, 190.1666666666667, 640.3333333333334, 26.0, 25.25594996086896, 8.369667712933738, 0.0, 1.0, 13.14963100455721], 
processed observation next is [0.0, 0.43478260869565216, 0.5918744228993538, 0.5466666666666667, 0.6338888888888891, 0.7075506445672192, 1.0, 0.8937071372669944, 0.08369667712933739, 0.0, 1.0, 0.0939259357468372], 
reward next is 0.9061, 
noisyNet noise sample is [array([-0.4149459], dtype=float32), 0.99712616]. 
=============================================
[2019-04-01 16:12:02,654] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.2552  ]
 [79.52819 ]
 [78.870186]
 [78.29002 ]
 [77.815834]], R is [[80.95022583]
 [81.03771973]
 [81.11598969]
 [81.1838913 ]
 [81.25231934]].
[2019-04-01 16:12:06,149] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9036197e-05 4.4479012e-21 1.8868548e-06 1.9908542e-16 1.4303287e-04
 9.9983597e-01 1.3151369e-08], sum to 1.0000
[2019-04-01 16:12:06,153] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9039
[2019-04-01 16:12:06,192] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 74.0, 46.0, 245.5, 26.0, 25.70475914875, 9.005068577388862, 1.0, 1.0, 23.3457706050594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4348800.0000, 
sim time next is 4349400.0000, 
raw observation next is [3.55, 71.16666666666667, 61.33333333333334, 327.3333333333334, 26.0, 25.67770391815068, 9.101025839603585, 1.0, 1.0, 21.86411350112428], 
processed observation next is [1.0, 0.34782608695652173, 0.5609418282548477, 0.7116666666666667, 0.20444444444444448, 0.36169429097605904, 1.0, 0.9539577025929543, 0.09101025839603585, 1.0, 1.0, 0.15617223929374485], 
reward next is 0.8438, 
noisyNet noise sample is [array([0.8789032], dtype=float32), 1.2035693]. 
=============================================
[2019-04-01 16:12:08,360] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4101563e-11 2.7751749e-25 9.4910228e-19 2.6022180e-24 1.8243769e-02
 9.8175621e-01 4.0757048e-08], sum to 1.0000
[2019-04-01 16:12:08,361] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5708
[2019-04-01 16:12:08,367] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.05, 62.5, 0.0, 0.0, 26.0, 25.82738922699725, 12.83062838835651, 0.0, 1.0, 6.806506672505674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4404600.0000, 
sim time next is 4405200.0000, 
raw observation next is [7.9, 62.66666666666667, 0.0, 0.0, 26.0, 25.75659037619373, 12.45903713679485, 0.0, 1.0, 6.86132198132765], 
processed observation next is [1.0, 1.0, 0.6814404432132966, 0.6266666666666667, 0.0, 0.0, 1.0, 0.9652271965991045, 0.1245903713679485, 0.0, 1.0, 0.04900944272376893], 
reward next is 0.9510, 
noisyNet noise sample is [array([0.6705498], dtype=float32), -1.3697582]. 
=============================================
[2019-04-01 16:12:22,668] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.6587421e-16 4.7297712e-33 1.8466434e-28 3.3558553e-33 9.9990427e-01
 9.5740565e-05 6.3947413e-15], sum to 1.0000
[2019-04-01 16:12:22,668] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4354
[2019-04-01 16:12:22,681] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.166666666666667, 52.33333333333334, 0.0, 0.0, 26.0, 25.91776988120137, 13.32127028123106, 1.0, 1.0, 4.752035167679477], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4647000.0000, 
sim time next is 4647600.0000, 
raw observation next is [3.0, 53.0, 0.0, 0.0, 26.0, 25.90159333235581, 12.93133434733584, 1.0, 1.0, 4.854208581784196], 
processed observation next is [1.0, 0.8260869565217391, 0.5457063711911359, 0.53, 0.0, 0.0, 1.0, 0.9859419046222584, 0.1293133434733584, 1.0, 1.0, 0.03467291844131568], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0342106], dtype=float32), 0.17022799]. 
=============================================
[2019-04-01 16:12:26,684] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4599242e-11 7.7059075e-30 1.0373496e-16 1.0156419e-30 4.2166801e-14
 9.2114723e-01 7.8852698e-02], sum to 1.0000
[2019-04-01 16:12:26,684] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1595
[2019-04-01 16:12:26,721] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 42.0, 0.0, 0.0, 26.0, 24.99675743707355, 8.054114155474386, 0.0, 1.0, 25.88271031840758], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4819200.0000, 
sim time next is 4819800.0000, 
raw observation next is [1.166666666666667, 42.5, 0.0, 0.0, 26.0, 25.01713212202735, 8.014817134442852, 0.0, 1.0, 24.58659362365474], 
processed observation next is [0.0, 0.782608695652174, 0.49492151431209613, 0.425, 0.0, 0.0, 1.0, 0.8595903031467641, 0.08014817134442852, 0.0, 1.0, 0.17561852588324814], 
reward next is 0.8244, 
noisyNet noise sample is [array([0.73523325], dtype=float32), -0.73904955]. 
=============================================
[2019-04-01 16:12:27,690] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5177375e-15 1.9644898e-28 6.2924216e-18 2.4251548e-28 1.8893078e-16
 3.0487993e-07 9.9999964e-01], sum to 1.0000
[2019-04-01 16:12:27,691] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5079
[2019-04-01 16:12:27,702] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 48.33333333333334, 0.0, 0.0, 26.0, 25.55804863304837, 9.452709031956362, 0.0, 1.0, 31.34848406551857], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4825200.0000, 
sim time next is 4825800.0000, 
raw observation next is [0.5, 49.0, 0.0, 0.0, 26.0, 25.57271669848295, 9.286937654628199, 0.0, 1.0, 28.75261411196653], 
processed observation next is [0.0, 0.8695652173913043, 0.4764542936288089, 0.49, 0.0, 0.0, 1.0, 0.9389595283547071, 0.09286937654628198, 0.0, 1.0, 0.2053758150854752], 
reward next is 0.7946, 
noisyNet noise sample is [array([0.07165925], dtype=float32), -0.4771339]. 
=============================================
[2019-04-01 16:12:40,461] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8613291e-25 4.7736981e-31 0.0000000e+00 4.0377244e-34 2.5955091e-12
 1.0000000e+00 2.3090161e-17], sum to 1.0000
[2019-04-01 16:12:40,462] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4089
[2019-04-01 16:12:40,484] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.666666666666667, 25.83333333333334, 46.66666666666666, 416.3333333333333, 26.0, 27.6537211621461, 17.72014563097639, 1.0, 1.0, 0.9131353098525121], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4986600.0000, 
sim time next is 4987200.0000, 
raw observation next is [7.333333333333334, 25.66666666666667, 40.33333333333333, 360.1666666666666, 26.0, 27.75864926626353, 19.90664232124326, 1.0, 1.0, 0.9612694151133351], 
processed observation next is [1.0, 0.7391304347826086, 0.6657433056325024, 0.2566666666666667, 0.13444444444444442, 0.3979742173112338, 1.0, 1.2512356094662185, 0.19906642321243262, 1.0, 1.0, 0.006866210107952394], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2936447], dtype=float32), -0.63323075]. 
=============================================
[2019-04-01 16:12:40,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:40,486] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:40,499] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run5
[2019-04-01 16:12:41,923] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:41,923] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:41,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run5
[2019-04-01 16:12:42,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:42,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:42,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run5
[2019-04-01 16:12:42,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:42,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:42,392] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run5
[2019-04-01 16:12:43,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:43,046] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:43,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run5
[2019-04-01 16:12:43,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:43,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:43,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run5
[2019-04-01 16:12:43,144] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:43,144] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:43,146] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run5
[2019-04-01 16:12:43,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:43,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:43,531] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run5
[2019-04-01 16:12:43,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:43,800] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:43,802] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run5
[2019-04-01 16:12:44,236] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1916055e-14 3.1598388e-31 8.2732619e-26 1.9780102e-31 8.7775200e-09
 9.9567324e-01 4.3267086e-03], sum to 1.0000
[2019-04-01 16:12:44,237] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2120
[2019-04-01 16:12:44,246] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.85, 19.0, 0.0, 0.0, 26.0, 26.80557125563571, 17.86984739167056, 0.0, 1.0, 5.850029557212075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5088600.0000, 
sim time next is 5089200.0000, 
raw observation next is [8.799999999999999, 19.0, 0.0, 0.0, 26.0, 26.75771115722721, 17.53507931920827, 0.0, 1.0, 5.940897215483914], 
processed observation next is [1.0, 0.9130434782608695, 0.7063711911357342, 0.19, 0.0, 0.0, 1.0, 1.1082444510324585, 0.1753507931920827, 0.0, 1.0, 0.04243498011059939], 
reward next is 0.9576, 
noisyNet noise sample is [array([-0.5701118], dtype=float32), -0.17111956]. 
=============================================
[2019-04-01 16:12:44,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:44,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:44,731] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run5
[2019-04-01 16:12:45,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:45,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:45,043] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run5
[2019-04-01 16:12:45,090] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:45,091] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:45,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run5
[2019-04-01 16:12:45,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:45,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:45,132] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run5
[2019-04-01 16:12:45,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:45,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:45,478] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run5
[2019-04-01 16:12:46,043] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:46,043] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:46,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run5
[2019-04-01 16:12:46,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:12:46,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:12:46,539] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run5
[2019-04-01 16:12:59,443] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5693176e-12 5.2968372e-22 7.0329220e-12 7.2808214e-23 2.0416033e-10
 8.3213224e-04 9.9916792e-01], sum to 1.0000
[2019-04-01 16:12:59,444] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1693
[2019-04-01 16:12:59,506] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.9, 86.5, 0.0, 0.0, 26.0, 24.56722223089221, 7.020649027994793, 0.0, 1.0, 43.78164214602168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 67800.0000, 
sim time next is 68400.0000, 
raw observation next is [3.8, 86.0, 0.0, 0.0, 26.0, 24.59179742232404, 7.099464720252397, 0.0, 1.0, 43.75534467885407], 
processed observation next is [0.0, 0.8260869565217391, 0.5678670360110805, 0.86, 0.0, 0.0, 1.0, 0.7988282031891484, 0.07099464720252396, 0.0, 1.0, 0.3125381762775291], 
reward next is 0.6875, 
noisyNet noise sample is [array([-0.3301763], dtype=float32), -1.0181661]. 
=============================================
[2019-04-01 16:12:59,658] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0412443e-18 3.7886142e-26 2.7556507e-16 5.8331182e-29 2.2570239e-21
 9.5867321e-02 9.0413266e-01], sum to 1.0000
[2019-04-01 16:12:59,659] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4600
[2019-04-01 16:12:59,730] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.366666666666667, 86.0, 74.16666666666667, 0.0, 26.0, 24.58602853800519, 6.762851624473463, 0.0, 1.0, 28.65342482565066], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 52800.0000, 
sim time next is 53400.0000, 
raw observation next is [7.283333333333333, 86.0, 69.33333333333334, 0.0, 26.0, 24.60687727106302, 6.749492866947293, 0.0, 1.0, 26.74227948821038], 
processed observation next is [0.0, 0.6086956521739131, 0.6643582640812559, 0.86, 0.23111111111111116, 0.0, 1.0, 0.8009824672947171, 0.06749492866947293, 0.0, 1.0, 0.19101628205864557], 
reward next is 0.8090, 
noisyNet noise sample is [array([1.6072788], dtype=float32), -0.67978925]. 
=============================================
[2019-04-01 16:13:01,245] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.7681239e-15 2.1925721e-22 4.3332262e-13 7.7994733e-24 5.0075656e-14
 3.9198701e-05 9.9996078e-01], sum to 1.0000
[2019-04-01 16:13:01,245] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2060
[2019-04-01 16:13:01,265] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 84.33333333333334, 0.0, 0.0, 26.0, 24.09076125637732, 5.438546767044222, 0.0, 1.0, 42.68680840714487], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 98400.0000, 
sim time next is 99000.0000, 
raw observation next is [-3.1, 83.0, 0.0, 0.0, 26.0, 24.07109118882764, 5.402989112409135, 0.0, 1.0, 42.81285681473335], 
processed observation next is [1.0, 0.13043478260869565, 0.37673130193905824, 0.83, 0.0, 0.0, 1.0, 0.7244415984039486, 0.05402989112409135, 0.0, 1.0, 0.3058061201052382], 
reward next is 0.6942, 
noisyNet noise sample is [array([-1.159404], dtype=float32), 1.1550668]. 
=============================================
[2019-04-01 16:13:01,294] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[48.20552 ]
 [48.385303]
 [48.222908]
 [48.04985 ]
 [47.816734]], R is [[48.24169922]
 [48.45437622]
 [48.66552734]
 [48.87570572]
 [49.08467865]].
[2019-04-01 16:13:09,946] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1691535e-16 1.2379845e-32 1.3250772e-16 1.3329766e-30 1.0278190e-15
 1.0000000e+00 7.4918483e-10], sum to 1.0000
[2019-04-01 16:13:09,948] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4501
[2019-04-01 16:13:09,976] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.900000000000002, 74.0, 0.0, 0.0, 26.0, 23.65071702428297, 5.436285399474168, 0.0, 1.0, 43.80436547332231], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 177600.0000, 
sim time next is 178200.0000, 
raw observation next is [-8.9, 74.0, 0.0, 0.0, 26.0, 23.62679828905739, 5.452460436598375, 0.0, 1.0, 43.78511353077744], 
processed observation next is [1.0, 0.043478260869565216, 0.21606648199445982, 0.74, 0.0, 0.0, 1.0, 0.6609711841510558, 0.05452460436598375, 0.0, 1.0, 0.31275081093412455], 
reward next is 0.6872, 
noisyNet noise sample is [array([0.38894564], dtype=float32), 0.7217339]. 
=============================================
[2019-04-01 16:13:14,362] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8953927e-15 2.6539483e-28 4.7984082e-20 1.3335794e-26 1.2492504e-11
 9.9999988e-01 1.3065815e-07], sum to 1.0000
[2019-04-01 16:13:14,364] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3813
[2019-04-01 16:13:14,391] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 77.33333333333334, 0.0, 0.0, 26.0, 24.43137970177805, 6.077554281318991, 0.0, 1.0, 43.56025680851187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 253200.0000, 
sim time next is 253800.0000, 
raw observation next is [-3.9, 78.5, 0.0, 0.0, 26.0, 24.39854778952397, 6.019534284301706, 0.0, 1.0, 43.56912498028985], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.785, 0.0, 0.0, 1.0, 0.7712211127891386, 0.06019534284301706, 0.0, 1.0, 0.3112080355734989], 
reward next is 0.6888, 
noisyNet noise sample is [array([0.5160563], dtype=float32), 0.7387098]. 
=============================================
[2019-04-01 16:13:20,327] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3911806e-31 0.0000000e+00 6.2654233e-35 0.0000000e+00 2.6109098e-28
 1.0000000e+00 4.1769741e-25], sum to 1.0000
[2019-04-01 16:13:20,327] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9775
[2019-04-01 16:13:20,372] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.866666666666667, 44.33333333333334, 31.66666666666666, 279.5, 26.0, 26.35697659246833, 10.73264610705495, 1.0, 1.0, 22.36779516781477], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 318000.0000, 
sim time next is 318600.0000, 
raw observation next is [-10.05, 45.5, 24.0, 240.0, 26.0, 25.86685519949923, 9.69003818681071, 1.0, 1.0, 20.96202907832184], 
processed observation next is [1.0, 0.6956521739130435, 0.18421052631578946, 0.455, 0.08, 0.26519337016574585, 1.0, 0.9809793142141758, 0.09690038186810711, 1.0, 1.0, 0.1497287791308703], 
reward next is 0.8503, 
noisyNet noise sample is [array([-0.72648686], dtype=float32), -0.4447302]. 
=============================================
[2019-04-01 16:13:20,386] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0910723e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.2041624e-31
 1.0000000e+00 1.9363790e-23], sum to 1.0000
[2019-04-01 16:13:20,387] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5276
[2019-04-01 16:13:20,469] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 44.0, 92.83333333333334, 629.6666666666667, 26.0, 26.58798247878021, 14.17501771775594, 1.0, 1.0, 45.53407434335024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 308400.0000, 
sim time next is 309000.0000, 
raw observation next is [-9.5, 44.0, 90.66666666666667, 628.3333333333333, 26.0, 26.92583802275778, 14.97799047749347, 1.0, 1.0, 41.39850935549142], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.3022222222222222, 0.6942909760589318, 1.0, 1.1322625746796828, 0.14977990477493472, 1.0, 1.0, 0.29570363825351015], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.73832417], dtype=float32), 2.0462391]. 
=============================================
[2019-04-01 16:13:20,503] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[50.973785]
 [50.96338 ]
 [50.97289 ]
 [51.42835 ]
 [51.725517]], R is [[50.64985657]
 [50.14336014]
 [49.64192581]
 [49.43690109]
 [49.50649643]].
[2019-04-01 16:13:28,367] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.52685780e-19 2.13570264e-32 5.51856524e-16 1.20330335e-29
 1.89208233e-19 1.00000000e+00 1.05070609e-13], sum to 1.0000
[2019-04-01 16:13:28,368] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2928
[2019-04-01 16:13:28,441] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.225037253115, 8.086044311694232, 0.0, 1.0, 48.6075916624246], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 361200.0000, 
sim time next is 361800.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.14778256866947, 8.131571056811994, 0.0, 1.0, 48.52651629463193], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 1.0, 0.4496832240956386, 0.08131571056811994, 0.0, 1.0, 0.3466179735330852], 
reward next is 0.6534, 
noisyNet noise sample is [array([-0.8114727], dtype=float32), -0.53011984]. 
=============================================
[2019-04-01 16:13:34,567] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.6724687e-22 1.3414768e-35 2.0881073e-14 6.3213455e-32 1.5623291e-24
 1.0000000e+00 2.0460565e-14], sum to 1.0000
[2019-04-01 16:13:34,567] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9007
[2019-04-01 16:13:34,584] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.0, 53.0, 0.0, 0.0, 26.0, 23.27840755284528, 5.799291791826152, 0.0, 1.0, 45.20756203029917], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 440400.0000, 
sim time next is 441000.0000, 
raw observation next is [-10.9, 52.0, 0.0, 0.0, 26.0, 23.27647029433815, 5.808004216385943, 0.0, 1.0, 45.21447083737074], 
processed observation next is [1.0, 0.08695652173913043, 0.16066481994459833, 0.52, 0.0, 0.0, 1.0, 0.6109243277625929, 0.05808004216385943, 0.0, 1.0, 0.32296050598121956], 
reward next is 0.6770, 
noisyNet noise sample is [array([0.65675503], dtype=float32), 0.07649333]. 
=============================================
[2019-04-01 16:13:34,603] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.830986]
 [77.6225  ]
 [77.428444]
 [77.24382 ]
 [77.08086 ]], R is [[77.90719604]
 [77.80521393]
 [77.70464325]
 [77.60528564]
 [77.50737   ]].
[2019-04-01 16:13:43,367] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1862987e-15 9.7117572e-26 6.1739926e-11 1.2248744e-25 1.3684638e-19
 1.7487645e-03 9.9825126e-01], sum to 1.0000
[2019-04-01 16:13:43,367] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9547
[2019-04-01 16:13:43,399] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 88.66666666666667, 0.0, 0.0, 26.0, 24.52682896701161, 5.874024315938828, 0.0, 1.0, 40.26460540271493], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 540600.0000, 
sim time next is 541200.0000, 
raw observation next is [0.9000000000000001, 89.33333333333334, 0.0, 0.0, 26.0, 24.49495150158666, 6.082118532343912, 0.0, 1.0, 40.84966223233969], 
processed observation next is [0.0, 0.2608695652173913, 0.48753462603878117, 0.8933333333333334, 0.0, 0.0, 1.0, 0.784993071655237, 0.06082118532343912, 0.0, 1.0, 0.2917833016595692], 
reward next is 0.7082, 
noisyNet noise sample is [array([0.7074621], dtype=float32), 0.07305254]. 
=============================================
[2019-04-01 16:13:44,902] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0557986e-22 7.2445430e-35 3.0191685e-20 1.0832345e-31 2.0324037e-18
 9.9983191e-01 1.6803187e-04], sum to 1.0000
[2019-04-01 16:13:44,904] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1419
[2019-04-01 16:13:44,932] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 83.66666666666667, 0.0, 0.0, 26.0, 25.08273213594263, 7.279856861252429, 0.0, 1.0, 42.39350323891951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 593400.0000, 
sim time next is 594000.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 26.0, 25.05141991621732, 7.189402322285379, 0.0, 1.0, 42.41816399754929], 
processed observation next is [0.0, 0.9130434782608695, 0.38504155124653744, 0.83, 0.0, 0.0, 1.0, 0.8644885594596171, 0.07189402322285379, 0.0, 1.0, 0.3029868856967807], 
reward next is 0.6970, 
noisyNet noise sample is [array([-1.0799938], dtype=float32), -2.2614179]. 
=============================================
[2019-04-01 16:13:44,936] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[69.37384]
 [69.36629]
 [69.34977]
 [69.33606]
 [69.31824]], R is [[69.37854767]
 [69.38195038]
 [69.38543701]
 [69.38863373]
 [69.39176178]].
[2019-04-01 16:13:46,220] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.9207729e-10 2.4103128e-19 1.6104282e-04 2.9185567e-21 1.1866353e-14
 6.2153363e-01 3.7830529e-01], sum to 1.0000
[2019-04-01 16:13:46,220] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4341
[2019-04-01 16:13:46,246] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 75.0, 0.0, 0.0, 26.0, 24.13966825287105, 5.414583248080912, 0.0, 1.0, 42.51386482990416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 616200.0000, 
sim time next is 616800.0000, 
raw observation next is [-4.1, 75.0, 0.0, 0.0, 26.0, 24.17504866250048, 5.403318494087967, 0.0, 1.0, 42.66024159182259], 
processed observation next is [0.0, 0.13043478260869565, 0.3490304709141275, 0.75, 0.0, 0.0, 1.0, 0.7392926660714971, 0.054033184940879664, 0.0, 1.0, 0.30471601137016135], 
reward next is 0.6953, 
noisyNet noise sample is [array([1.3627805], dtype=float32), 0.38794208]. 
=============================================
[2019-04-01 16:13:46,527] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.3805948e-22 2.0352412e-28 1.1761716e-17 5.4248429e-33 2.4995932e-19
 1.0000000e+00 2.6410518e-09], sum to 1.0000
[2019-04-01 16:13:46,528] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4487
[2019-04-01 16:13:46,558] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 84.33333333333333, 0.0, 0.0, 26.0, 25.10534568831725, 7.372837785452393, 0.0, 1.0, 42.37752098831263], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 592800.0000, 
sim time next is 593400.0000, 
raw observation next is [-2.8, 83.66666666666667, 0.0, 0.0, 26.0, 25.08273213594263, 7.279856861252429, 0.0, 1.0, 42.39350323891951], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8366666666666667, 0.0, 0.0, 1.0, 0.8689617337060902, 0.07279856861252429, 0.0, 1.0, 0.3028107374208536], 
reward next is 0.6972, 
noisyNet noise sample is [array([1.256813], dtype=float32), 0.11358401]. 
=============================================
[2019-04-01 16:13:46,840] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5944865e-20 4.1531328e-29 1.9059505e-16 1.2999662e-27 9.1979667e-23
 1.0000000e+00 8.5806739e-09], sum to 1.0000
[2019-04-01 16:13:46,842] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1201
[2019-04-01 16:13:46,859] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 78.66666666666667, 0.0, 0.0, 26.0, 24.20424091965869, 5.450684307435734, 0.0, 1.0, 42.05719129703903], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 614400.0000, 
sim time next is 615000.0000, 
raw observation next is [-3.9, 76.83333333333333, 0.0, 0.0, 26.0, 24.1753892869934, 5.423242593543684, 0.0, 1.0, 42.21398698082274], 
processed observation next is [0.0, 0.08695652173913043, 0.3545706371191136, 0.7683333333333333, 0.0, 0.0, 1.0, 0.739341326713343, 0.05423242593543684, 0.0, 1.0, 0.30152847843444813], 
reward next is 0.6985, 
noisyNet noise sample is [array([0.12820874], dtype=float32), -0.761297]. 
=============================================
[2019-04-01 16:13:46,892] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[67.76864 ]
 [67.68556 ]
 [67.626656]
 [67.4903  ]
 [67.41734 ]], R is [[67.81016541]
 [67.83165741]
 [67.85378265]
 [67.87639618]
 [67.89942169]].
[2019-04-01 16:13:46,942] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1281310e-13 1.2985476e-32 1.8604333e-15 5.6497893e-31 3.5278782e-18
 9.9277240e-01 7.2275610e-03], sum to 1.0000
[2019-04-01 16:13:46,942] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1943
[2019-04-01 16:13:46,984] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 80.0, 138.0, 595.0, 26.0, 24.94671892488563, 8.287909738809068, 0.0, 1.0, 36.19286451136958], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 567000.0000, 
sim time next is 567600.0000, 
raw observation next is [-1.2, 80.0, 136.1666666666667, 573.6666666666667, 26.0, 25.02533381008277, 8.429724168676119, 0.0, 1.0, 32.99831224233935], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.45388888888888906, 0.6338858195211787, 1.0, 0.8607619728689672, 0.08429724168676118, 0.0, 1.0, 0.23570223030242396], 
reward next is 0.7643, 
noisyNet noise sample is [array([0.9756149], dtype=float32), -0.81147635]. 
=============================================
[2019-04-01 16:13:47,114] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7411215e-16 8.1519431e-33 9.6254045e-17 6.2989818e-30 2.6484544e-20
 9.8309547e-01 1.6904512e-02], sum to 1.0000
[2019-04-01 16:13:47,115] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7667
[2019-04-01 16:13:47,170] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 80.0, 136.6666666666667, 561.8333333333334, 26.0, 24.8911859351301, 8.028806114792046, 0.0, 1.0, 41.60999702073529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 566400.0000, 
sim time next is 567000.0000, 
raw observation next is [-1.2, 80.0, 138.0, 595.0, 26.0, 24.94671892488563, 8.287909738809068, 0.0, 1.0, 36.19286451136958], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.46, 0.6574585635359116, 1.0, 0.8495312749836614, 0.08287909738809068, 0.0, 1.0, 0.258520460795497], 
reward next is 0.7415, 
noisyNet noise sample is [array([-0.79303354], dtype=float32), -0.00055595953]. 
=============================================
[2019-04-01 16:13:47,174] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[65.50145 ]
 [65.797264]
 [66.16066 ]
 [66.54693 ]
 [66.89707 ]], R is [[65.29264832]
 [65.34251404]
 [65.45663452]
 [65.5898819 ]
 [65.72869873]].
[2019-04-01 16:13:47,417] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8967957e-14 1.7945265e-26 8.4028509e-13 3.4237528e-26 2.9218494e-20
 1.1347478e-01 8.8652515e-01], sum to 1.0000
[2019-04-01 16:13:47,418] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6794
[2019-04-01 16:13:47,440] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.3, 83.0, 0.0, 0.0, 26.0, 24.810241895122, 6.479386165596686, 0.0, 1.0, 42.08736827387791], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 600600.0000, 
sim time next is 601200.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.77836798231034, 6.420205951290228, 0.0, 1.0, 42.01402368967926], 
processed observation next is [0.0, 1.0, 0.368421052631579, 0.83, 0.0, 0.0, 1.0, 0.8254811403300485, 0.06420205951290228, 0.0, 1.0, 0.3001001692119947], 
reward next is 0.6999, 
noisyNet noise sample is [array([-0.35721496], dtype=float32), -0.95555]. 
=============================================
[2019-04-01 16:13:51,226] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6288308e-11 3.4373095e-25 2.6311600e-08 5.0542504e-24 3.9314511e-17
 7.8288984e-04 9.9921703e-01], sum to 1.0000
[2019-04-01 16:13:51,227] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7751
[2019-04-01 16:13:51,241] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.3, 75.0, 0.0, 0.0, 26.0, 24.12567548247791, 5.342223265255235, 0.0, 1.0, 43.07929516411479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 618000.0000, 
sim time next is 618600.0000, 
raw observation next is [-4.399999999999999, 75.0, 0.0, 0.0, 26.0, 24.07436876314983, 5.313153607725636, 0.0, 1.0, 43.28791139100056], 
processed observation next is [0.0, 0.13043478260869565, 0.3407202216066483, 0.75, 0.0, 0.0, 1.0, 0.7249098233071187, 0.05313153607725636, 0.0, 1.0, 0.3091993670785754], 
reward next is 0.6908, 
noisyNet noise sample is [array([0.07653578], dtype=float32), 0.8874917]. 
=============================================
[2019-04-01 16:13:53,430] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7727836e-19 1.4981943e-29 2.0544127e-17 1.2038840e-28 1.6990724e-18
 9.9998987e-01 1.0119794e-05], sum to 1.0000
[2019-04-01 16:13:53,431] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5195
[2019-04-01 16:13:53,475] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8999999999999999, 57.0, 81.0, 56.0, 26.0, 24.94186166515018, 6.870411961939524, 0.0, 1.0, 30.32123559757923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 657000.0000, 
sim time next is 657600.0000, 
raw observation next is [-0.8, 56.0, 81.33333333333333, 53.0, 26.0, 24.87195723310129, 6.790535163118065, 0.0, 1.0, 35.08892514318624], 
processed observation next is [0.0, 0.6086956521739131, 0.4404432132963989, 0.56, 0.2711111111111111, 0.05856353591160221, 1.0, 0.8388510333001845, 0.06790535163118065, 0.0, 1.0, 0.2506351795941874], 
reward next is 0.7494, 
noisyNet noise sample is [array([-0.05202474], dtype=float32), 0.17765188]. 
=============================================
[2019-04-01 16:13:55,070] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.7723457e-13 1.6499899e-20 3.0394469e-13 1.6214615e-21 3.4877198e-18
 1.6286416e-04 9.9983716e-01], sum to 1.0000
[2019-04-01 16:13:55,077] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5015
[2019-04-01 16:13:55,117] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.466666666666667, 63.0, 0.0, 0.0, 26.0, 24.92400213104834, 6.878917397378046, 0.0, 1.0, 47.52499160782565], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 674400.0000, 
sim time next is 675000.0000, 
raw observation next is [-2.55, 63.5, 0.0, 0.0, 26.0, 24.93734291777145, 6.849787883427747, 0.0, 1.0, 44.55977334658879], 
processed observation next is [0.0, 0.8260869565217391, 0.3919667590027701, 0.635, 0.0, 0.0, 1.0, 0.8481918453959213, 0.06849787883427746, 0.0, 1.0, 0.3182840953327771], 
reward next is 0.6817, 
noisyNet noise sample is [array([-0.2626292], dtype=float32), 0.26122865]. 
=============================================
[2019-04-01 16:13:55,121] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[61.50109 ]
 [61.566666]
 [61.66184 ]
 [61.77663 ]
 [61.79017 ]], R is [[61.49264908]
 [61.5382576 ]
 [61.60043716]
 [61.71028137]
 [61.79868317]].
[2019-04-01 16:14:03,661] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.2275947e-28 0.0000000e+00 1.3272718e-25 0.0000000e+00 9.8965601e-29
 1.0000000e+00 2.8360388e-22], sum to 1.0000
[2019-04-01 16:14:03,661] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3475
[2019-04-01 16:14:03,730] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 75.0, 46.5, 0.0, 26.0, 25.78248264974966, 7.429881265188314, 1.0, 1.0, 31.32956776224599], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 810000.0000, 
sim time next is 810600.0000, 
raw observation next is [-6.2, 75.0, 51.33333333333334, 0.0, 26.0, 25.70805188422817, 7.360753707018882, 1.0, 1.0, 29.34679464852608], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.75, 0.17111111111111113, 0.0, 1.0, 0.9582931263183099, 0.07360753707018883, 1.0, 1.0, 0.2096199617751863], 
reward next is 0.7904, 
noisyNet noise sample is [array([0.5411007], dtype=float32), -0.47929496]. 
=============================================
[2019-04-01 16:14:11,391] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4567087e-23 4.8927128e-35 2.1898746e-22 1.2668842e-38 2.5577516e-23
 1.0000000e+00 7.9328176e-12], sum to 1.0000
[2019-04-01 16:14:11,391] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5591
[2019-04-01 16:14:11,404] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.34041911707536, 9.233692684264929, 0.0, 1.0, 37.84365796425489], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 950400.0000, 
sim time next is 951000.0000, 
raw observation next is [5.083333333333334, 94.83333333333333, 0.0, 0.0, 26.0, 25.34440318792012, 9.227441444665196, 0.0, 1.0, 37.75591213662255], 
processed observation next is [1.0, 0.0, 0.6034164358264081, 0.9483333333333333, 0.0, 0.0, 1.0, 0.9063433125600169, 0.09227441444665196, 0.0, 1.0, 0.26968508669016106], 
reward next is 0.7303, 
noisyNet noise sample is [array([-0.28668708], dtype=float32), 0.8730027]. 
=============================================
[2019-04-01 16:14:11,409] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.474174]
 [82.19908 ]
 [82.13262 ]
 [82.05678 ]
 [81.95895 ]], R is [[82.37759399]
 [82.2835083 ]
 [82.19062042]
 [82.09738922]
 [82.00436401]].
[2019-04-01 16:14:14,587] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3539268e-27 2.6496675e-37 5.7427446e-28 0.0000000e+00 2.2389940e-22
 1.0000000e+00 4.8739007e-19], sum to 1.0000
[2019-04-01 16:14:14,589] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6005
[2019-04-01 16:14:14,607] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 89.0, 0.0, 0.0, 26.0, 25.35418132308087, 9.255442620392229, 0.0, 1.0, 36.47702368114663], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 954000.0000, 
sim time next is 954600.0000, 
raw observation next is [5.683333333333334, 87.83333333333334, 0.0, 0.0, 26.0, 25.35514518045317, 9.3049589516723, 0.0, 1.0, 37.41547564986951], 
processed observation next is [1.0, 0.043478260869565216, 0.6200369344413666, 0.8783333333333334, 0.0, 0.0, 1.0, 0.9078778829218815, 0.09304958951672299, 0.0, 1.0, 0.2672533974990679], 
reward next is 0.7327, 
noisyNet noise sample is [array([-1.4168139], dtype=float32), -1.0671659]. 
=============================================
[2019-04-01 16:14:14,981] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.4828301e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.4157970e-26
 1.0000000e+00 2.8153528e-30], sum to 1.0000
[2019-04-01 16:14:14,985] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5684
[2019-04-01 16:14:15,010] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.600000000000001, 92.66666666666667, 24.0, 0.0, 26.0, 26.44095825355136, 11.6835610141906, 1.0, 1.0, 14.28432931814712], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 922800.0000, 
sim time next is 923400.0000, 
raw observation next is [4.7, 92.5, 18.0, 0.0, 26.0, 26.3868959733747, 11.54017989553345, 1.0, 1.0, 13.58776620097712], 
processed observation next is [1.0, 0.6956521739130435, 0.592797783933518, 0.925, 0.06, 0.0, 1.0, 1.0552708533392428, 0.1154017989553345, 1.0, 1.0, 0.09705547286412228], 
reward next is 0.2869, 
noisyNet noise sample is [array([0.3433428], dtype=float32), -0.9782672]. 
=============================================
[2019-04-01 16:14:16,828] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1029854e-19 5.1611600e-33 1.5186418e-22 3.7515571e-36 2.6312262e-21
 1.0000000e+00 3.1023257e-16], sum to 1.0000
[2019-04-01 16:14:16,832] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0845
[2019-04-01 16:14:16,878] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.8, 86.33333333333334, 0.0, 0.0, 26.0, 25.72884464392503, 10.54540193347701, 1.0, 1.0, 30.18060886470738], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 976800.0000, 
sim time next is 977400.0000, 
raw observation next is [9.7, 88.0, 0.0, 0.0, 26.0, 25.80453620570699, 10.64592974652408, 1.0, 1.0, 27.14922423864761], 
processed observation next is [1.0, 0.30434782608695654, 0.7313019390581719, 0.88, 0.0, 0.0, 1.0, 0.9720766008152845, 0.1064592974652408, 1.0, 1.0, 0.19392303027605434], 
reward next is 0.5477, 
noisyNet noise sample is [array([-0.73128676], dtype=float32), 0.51526535]. 
=============================================
[2019-04-01 16:14:22,903] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.1702056e-29 0.0000000e+00 5.6159624e-25 0.0000000e+00 1.6822386e-35
 3.9895069e-02 9.6010494e-01], sum to 1.0000
[2019-04-01 16:14:22,903] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2264
[2019-04-01 16:14:22,923] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1184990e-27 2.0374051e-35 2.6160005e-27 0.0000000e+00 2.0437491e-20
 9.9999046e-01 9.5672467e-06], sum to 1.0000
[2019-04-01 16:14:22,925] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6493
[2019-04-01 16:14:22,939] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 98.0, 95.0, 0.0, 26.0, 25.01376523039048, 10.16711700503034, 0.0, 1.0, 18.01722078380234], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1251000.0000, 
sim time next is 1251600.0000, 
raw observation next is [14.4, 97.33333333333334, 96.0, 0.0, 26.0, 25.04007340369938, 10.21659933093689, 0.0, 1.0, 16.83872888145156], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 0.9733333333333334, 0.32, 0.0, 1.0, 0.8628676290999115, 0.1021659933093689, 0.0, 1.0, 0.12027663486751113], 
reward next is 0.8797, 
noisyNet noise sample is [array([1.1717281], dtype=float32), 0.6589421]. 
=============================================
[2019-04-01 16:14:22,940] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.61666666666667, 64.33333333333334, 0.0, 0.0, 26.0, 26.0111482137288, 16.04815690415468, 0.0, 1.0, 17.16956999596344], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1116600.0000, 
sim time next is 1117200.0000, 
raw observation next is [12.53333333333333, 64.66666666666667, 0.0, 0.0, 26.0, 25.99565255117079, 15.81933833896702, 0.0, 1.0, 16.2879792061066], 
processed observation next is [1.0, 0.9565217391304348, 0.8097876269621421, 0.6466666666666667, 0.0, 0.0, 1.0, 0.9993789358815413, 0.1581933833896702, 0.0, 1.0, 0.11634270861504713], 
reward next is 0.8837, 
noisyNet noise sample is [array([-0.38973108], dtype=float32), 1.7096467]. 
=============================================
[2019-04-01 16:14:23,638] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4436283e-25 6.0874136e-33 2.7367368e-17 0.0000000e+00 1.1035981e-22
 1.5232130e-08 1.0000000e+00], sum to 1.0000
[2019-04-01 16:14:23,643] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5975
[2019-04-01 16:14:23,671] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 98.66666666666667, 92.16666666666667, 0.0, 26.0, 24.98990539322109, 10.06564844309033, 0.0, 1.0, 19.31087519708893], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1250400.0000, 
sim time next is 1251000.0000, 
raw observation next is [14.4, 98.0, 95.0, 0.0, 26.0, 25.01376523039048, 10.16711700503034, 0.0, 1.0, 18.01722078380234], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 0.98, 0.31666666666666665, 0.0, 1.0, 0.8591093186272113, 0.1016711700503034, 0.0, 1.0, 0.12869443417001672], 
reward next is 0.8713, 
noisyNet noise sample is [array([-0.30433843], dtype=float32), 0.59704745]. 
=============================================
[2019-04-01 16:14:23,680] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[102.28664]
 [103.31466]
 [103.83379]
 [104.53621]
 [104.61423]], R is [[101.09811401]
 [100.94920349]
 [100.79177094]
 [100.62595367]
 [100.45330811]].
[2019-04-01 16:14:28,552] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.24742596e-13 1.94872560e-19 1.02938996e-17 1.10977280e-22
 1.28438405e-05 9.99987125e-01 1.95421249e-11], sum to 1.0000
[2019-04-01 16:14:28,553] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9605
[2019-04-01 16:14:28,561] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.66666666666667, 39.66666666666667, 0.0, 26.0, 25.71740966308528, 10.82760849108869, 1.0, 1.0, 10.74122183201486], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1352400.0000, 
sim time next is 1353000.0000, 
raw observation next is [1.1, 92.83333333333333, 35.33333333333334, 0.0, 26.0, 25.71780134908285, 10.80053884220075, 1.0, 1.0, 10.84239828961677], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.9283333333333332, 0.11777777777777781, 0.0, 1.0, 0.9596859070118359, 0.10800538842200749, 1.0, 1.0, 0.07744570206869122], 
reward next is 0.6023, 
noisyNet noise sample is [array([0.5069698], dtype=float32), 0.22802888]. 
=============================================
[2019-04-01 16:14:28,585] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[26.21208 ]
 [26.135042]
 [26.077482]
 [26.01471 ]
 [25.958582]], R is [[26.6275959 ]
 [26.95355415]
 [27.26382256]
 [27.55339432]
 [27.83438683]].
[2019-04-01 16:14:31,170] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.9470159e-15 1.8292805e-20 9.1380366e-23 9.3395047e-25 1.8951516e-06
 9.9999774e-01 4.0126386e-07], sum to 1.0000
[2019-04-01 16:14:31,172] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9570
[2019-04-01 16:14:31,182] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8, 94.5, 18.0, 0.0, 26.0, 25.67541201455117, 10.20929914288503, 1.0, 1.0, 18.8860772791944], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1355400.0000, 
sim time next is 1356000.0000, 
raw observation next is [0.7000000000000001, 95.0, 15.0, 0.0, 26.0, 25.53139935810816, 10.22646964794792, 1.0, 1.0, 19.33700067614254], 
processed observation next is [1.0, 0.6956521739130435, 0.4819944598337951, 0.95, 0.05, 0.0, 1.0, 0.9330570511583086, 0.1022646964794792, 1.0, 1.0, 0.13812143340101815], 
reward next is 0.7713, 
noisyNet noise sample is [array([0.03115872], dtype=float32), -1.3092191]. 
=============================================
[2019-04-01 16:14:31,192] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[37.26921 ]
 [37.254044]
 [37.2678  ]
 [37.26149 ]
 [37.245445]], R is [[37.61634064]
 [38.02155685]
 [38.32463837]
 [38.5939064 ]
 [38.82411575]].
[2019-04-01 16:14:36,373] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.21104651e-20 9.94575050e-31 7.82511737e-15 2.10022778e-31
 1.20839914e-23 1.00000000e+00 4.49602196e-11], sum to 1.0000
[2019-04-01 16:14:36,375] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5455
[2019-04-01 16:14:36,425] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.60864030596714, 9.707002492017347, 1.0, 1.0, 29.72110812095028], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1410000.0000, 
sim time next is 1410600.0000, 
raw observation next is [-0.6, 100.0, 5.999999999999998, 0.0, 26.0, 25.49785088449347, 9.47888905653693, 1.0, 1.0, 26.05127710002892], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.019999999999999993, 0.0, 1.0, 0.9282644120704957, 0.09478889056536931, 1.0, 1.0, 0.18608055071449228], 
reward next is 0.8139, 
noisyNet noise sample is [array([-1.3793405], dtype=float32), 0.09740218]. 
=============================================
[2019-04-01 16:14:37,725] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6237721e-24 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.9441201e-14
 1.0000000e+00 3.5980652e-21], sum to 1.0000
[2019-04-01 16:14:37,733] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9786
[2019-04-01 16:14:37,772] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.6, 52.0, 76.0, 570.5, 26.0, 26.54733604926466, 16.56735966904071, 1.0, 1.0, 1.520141238220304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1522800.0000, 
sim time next is 1523400.0000, 
raw observation next is [11.7, 51.66666666666667, 76.33333333333333, 539.6666666666666, 26.0, 25.94895066543424, 14.80889837727048, 1.0, 1.0, 1.430784380196429], 
processed observation next is [1.0, 0.6521739130434783, 0.7867036011080333, 0.5166666666666667, 0.2544444444444444, 0.596316758747698, 1.0, 0.992707237919177, 0.1480889837727048, 1.0, 1.0, 0.010219888429974493], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9207259], dtype=float32), 1.100588]. 
=============================================
[2019-04-01 16:14:40,248] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6840998e-07 1.2217703e-18 5.4947048e-08 8.3591827e-21 1.1385836e-09
 9.8031062e-01 1.9689133e-02], sum to 1.0000
[2019-04-01 16:14:40,250] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4364
[2019-04-01 16:14:40,277] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.6, 100.0, 32.5, 0.0, 26.0, 25.62452539963503, 9.499832340899781, 1.0, 1.0, 15.22739012958236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1501200.0000, 
sim time next is 1501800.0000, 
raw observation next is [1.7, 100.0, 37.33333333333334, 0.0, 26.0, 25.65313099010712, 9.483264267207764, 1.0, 1.0, 14.56596229135286], 
processed observation next is [1.0, 0.391304347826087, 0.5096952908587258, 1.0, 0.12444444444444448, 0.0, 1.0, 0.950447284301017, 0.09483264267207764, 1.0, 1.0, 0.10404258779537758], 
reward next is 0.8960, 
noisyNet noise sample is [array([-0.46149743], dtype=float32), -0.16419299]. 
=============================================
[2019-04-01 16:14:40,666] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9018650e-19 3.7028807e-32 7.4404312e-19 8.7506316e-34 2.5153574e-25
 9.9999952e-01 4.5484194e-07], sum to 1.0000
[2019-04-01 16:14:40,666] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8751
[2019-04-01 16:14:40,679] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 93.66666666666667, 0.0, 0.0, 26.0, 25.41537782619445, 9.54958751671817, 0.0, 1.0, 31.68489995743247], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1479000.0000, 
sim time next is 1479600.0000, 
raw observation next is [2.2, 94.0, 0.0, 0.0, 26.0, 25.37970714253436, 9.345172349318196, 0.0, 1.0, 33.11361463915236], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.94, 0.0, 0.0, 1.0, 0.9113867346477659, 0.09345172349318195, 0.0, 1.0, 0.23652581885108828], 
reward next is 0.7635, 
noisyNet noise sample is [array([-0.88948125], dtype=float32), -0.66952693]. 
=============================================
[2019-04-01 16:14:42,980] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5459841e-15 3.6050697e-23 9.0436556e-14 2.0393462e-27 7.3596790e-18
 6.0667775e-05 9.9993932e-01], sum to 1.0000
[2019-04-01 16:14:42,983] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8168
[2019-04-01 16:14:42,997] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.25, 82.0, 0.0, 0.0, 26.0, 25.53461655474867, 12.16171339501, 0.0, 1.0, 33.14886667469611], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1553400.0000, 
sim time next is 1554000.0000, 
raw observation next is [5.166666666666666, 82.0, 0.0, 0.0, 26.0, 25.55709702043171, 12.03651861214699, 0.0, 1.0, 26.09135968856309], 
processed observation next is [1.0, 1.0, 0.6057248384118191, 0.82, 0.0, 0.0, 1.0, 0.9367281457759583, 0.1203651861214699, 0.0, 1.0, 0.1863668549183078], 
reward next is 0.8136, 
noisyNet noise sample is [array([0.49825945], dtype=float32), -0.4199094]. 
=============================================
[2019-04-01 16:14:43,014] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[60.291687]
 [60.44964 ]
 [60.451313]
 [60.607918]
 [60.60529 ]], R is [[60.25854492]
 [60.41918182]
 [60.53769302]
 [60.65404892]
 [60.80932999]].
[2019-04-01 16:14:43,219] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8061111e-28 1.5564540e-36 4.6996874e-24 0.0000000e+00 2.5182302e-24
 9.9999988e-01 7.0317206e-08], sum to 1.0000
[2019-04-01 16:14:43,219] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8116
[2019-04-01 16:14:43,234] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.74093643861291, 11.41720074511125, 0.0, 1.0, 16.67533263111195], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1652400.0000, 
sim time next is 1653000.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.60412667614114, 10.88045314987819, 0.0, 1.0, 15.741983798934], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 1.0, 0.9434466680201626, 0.10880453149878189, 0.0, 1.0, 0.11244274142095714], 
reward next is 0.8876, 
noisyNet noise sample is [array([1.0101125], dtype=float32), -0.60429275]. 
=============================================
[2019-04-01 16:14:43,248] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.35007 ]
 [76.619675]
 [76.84475 ]
 [76.97589 ]
 [77.124146]], R is [[76.09425354]
 [76.21420288]
 [76.31813049]
 [76.42506409]
 [76.52452087]].
[2019-04-01 16:14:46,179] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0272985e-15 2.4136494e-20 1.8196479e-19 1.1016220e-22 2.7825706e-06
 9.7278994e-01 2.7207244e-02], sum to 1.0000
[2019-04-01 16:14:46,180] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5475
[2019-04-01 16:14:46,208] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.70356044244372, 11.15548860235705, 1.0, 1.0, 17.45389829959805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1705200.0000, 
sim time next is 1705800.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.57389818059508, 10.68794563481, 1.0, 1.0, 15.39117176596157], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.0, 0.0, 1.0, 0.9391283115135829, 0.10687945634809999, 1.0, 1.0, 0.10993694118543978], 
reward next is 0.6149, 
noisyNet noise sample is [array([-0.3413118], dtype=float32), 0.26337823]. 
=============================================
[2019-04-01 16:14:51,620] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.4417791e-14 2.7064546e-23 7.6788848e-07 4.6002507e-24 1.6545366e-15
 9.8555607e-01 1.4443060e-02], sum to 1.0000
[2019-04-01 16:14:51,628] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4540
[2019-04-01 16:14:51,650] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.45, 91.83333333333334, 0.0, 0.0, 26.0, 25.24693548290458, 9.508626685901126, 0.0, 1.0, 42.38139691080078], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1732200.0000, 
sim time next is 1732800.0000, 
raw observation next is [0.4, 91.66666666666667, 0.0, 0.0, 26.0, 25.23061440088924, 9.43022362391013, 0.0, 1.0, 42.37111183649547], 
processed observation next is [0.0, 0.043478260869565216, 0.4736842105263158, 0.9166666666666667, 0.0, 0.0, 1.0, 0.8900877715556058, 0.0943022362391013, 0.0, 1.0, 0.30265079883211055], 
reward next is 0.6973, 
noisyNet noise sample is [array([-0.6907796], dtype=float32), -1.4947065]. 
=============================================
[2019-04-01 16:14:57,236] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3312614e-14 8.8385128e-26 7.7961824e-11 2.2773844e-27 5.5722585e-17
 5.9714166e-05 9.9994028e-01], sum to 1.0000
[2019-04-01 16:14:57,236] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5976
[2019-04-01 16:14:57,302] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 87.0, 66.0, 0.0, 26.0, 25.05868017598551, 8.243567876240563, 0.0, 1.0, 34.74503018626133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1783800.0000, 
sim time next is 1784400.0000, 
raw observation next is [-3.2, 87.0, 59.66666666666666, 0.0, 26.0, 25.04731855299494, 8.110981042307685, 0.0, 1.0, 36.92734953506486], 
processed observation next is [0.0, 0.6521739130434783, 0.37396121883656513, 0.87, 0.19888888888888887, 0.0, 1.0, 0.8639026504278488, 0.08110981042307686, 0.0, 1.0, 0.26376678239332046], 
reward next is 0.7362, 
noisyNet noise sample is [array([0.941899], dtype=float32), 0.16028923]. 
=============================================
[2019-04-01 16:15:14,357] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.8720940e-28 0.0000000e+00 4.2768406e-34 0.0000000e+00 3.4370962e-22
 1.0000000e+00 5.1559700e-16], sum to 1.0000
[2019-04-01 16:15:14,357] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3123
[2019-04-01 16:15:14,430] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.39280323892879, 8.02251176087824, 1.0, 1.0, 29.09684072099521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2050200.0000, 
sim time next is 2050800.0000, 
raw observation next is [-3.899999999999999, 82.0, 0.0, 0.0, 26.0, 25.23849392244941, 8.184224029275255, 1.0, 1.0, 29.61625904711314], 
processed observation next is [1.0, 0.7391304347826086, 0.35457063711911363, 0.82, 0.0, 0.0, 1.0, 0.891213417492773, 0.08184224029275254, 1.0, 1.0, 0.21154470747937956], 
reward next is 0.7885, 
noisyNet noise sample is [array([0.47479776], dtype=float32), -0.30929506]. 
=============================================
[2019-04-01 16:15:21,042] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.6041951e-17 2.7856028e-26 3.6415139e-14 1.4451565e-27 1.2180768e-20
 4.0708953e-03 9.9592906e-01], sum to 1.0000
[2019-04-01 16:15:21,060] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7540
[2019-04-01 16:15:21,078] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.86897339829391, 7.119806480087568, 0.0, 1.0, 42.09427010839778], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2073000.0000, 
sim time next is 2073600.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 26.0, 24.83425270793063, 7.022424633974221, 0.0, 1.0, 42.10507514411889], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 1.0, 0.8334646725615187, 0.07022424633974221, 0.0, 1.0, 0.3007505367437064], 
reward next is 0.6992, 
noisyNet noise sample is [array([-1.2440869], dtype=float32), -0.6457311]. 
=============================================
[2019-04-01 16:15:22,905] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-01 16:15:22,907] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 16:15:22,907] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:15:22,914] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 16:15:22,915] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run7
[2019-04-01 16:15:22,915] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:15:22,915] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 16:15:22,931] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:15:22,935] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run7
[2019-04-01 16:15:22,935] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run7
[2019-04-01 16:16:02,281] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.19777137], dtype=float32), -0.2756718]
[2019-04-01 16:16:02,282] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [14.4, 98.0, 95.0, 0.0, 26.0, 25.01376523039048, 10.16711700503034, 0.0, 1.0, 18.01722078380234]
[2019-04-01 16:16:02,282] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:16:02,283] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.1098125e-27 1.8838577e-34 8.0291035e-22 0.0000000e+00 2.8101346e-25
 9.0412816e-12 1.0000000e+00], sampled 0.7694230927510471
[2019-04-01 16:16:46,486] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.19777137], dtype=float32), -0.2756718]
[2019-04-01 16:16:46,486] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [6.292039231, 15.729502465, 97.83539697666669, 147.0460194666667, 26.0, 25.05787887447558, 7.71339993322825, 1.0, 1.0, 33.07729450437716]
[2019-04-01 16:16:46,486] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 16:16:46,487] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.4823457e-23 1.2380921e-32 4.6395394e-27 5.3119723e-37 1.1774871e-15
 9.9999523e-01 4.7911221e-06], sampled 0.939165043176887
[2019-04-01 16:17:10,867] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 16:17:14,656] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.19777137], dtype=float32), -0.2756718]
[2019-04-01 16:17:14,656] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.0, 37.0, 114.0, 544.0, 26.0, 25.35201496910447, 9.488872187727452, 0.0, 1.0, 9.136974268406872]
[2019-04-01 16:17:14,656] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:17:14,656] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.7025544e-19 5.7425668e-30 1.0148002e-21 4.7261733e-31 1.1946715e-17
 3.1603381e-04 9.9968398e-01], sampled 0.09779481250392263
[2019-04-01 16:17:30,499] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 16:17:33,874] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 16:17:34,897] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 600000, evaluation results [600000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 16:17:50,196] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4203626e-25 1.8606885e-34 7.3797350e-29 0.0000000e+00 9.9969779e-25
 9.9999225e-01 7.7094783e-06], sum to 1.0000
[2019-04-01 16:17:50,197] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9295
[2019-04-01 16:17:50,247] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 54.0, 0.0, 0.0, 26.0, 25.06631508362022, 7.845158342850908, 1.0, 1.0, 24.82187252492979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2314800.0000, 
sim time next is 2315400.0000, 
raw observation next is [-1.283333333333333, 54.33333333333333, 0.0, 0.0, 26.0, 25.02525269112568, 7.988534390538024, 1.0, 1.0, 37.75328493126445], 
processed observation next is [1.0, 0.8260869565217391, 0.4270544783010157, 0.5433333333333333, 0.0, 0.0, 1.0, 0.8607503844465256, 0.07988534390538024, 1.0, 1.0, 0.26966632093760323], 
reward next is 0.7303, 
noisyNet noise sample is [array([-0.54699546], dtype=float32), 0.82921547]. 
=============================================
[2019-04-01 16:17:50,634] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0297541e-22 1.2935617e-32 2.5795240e-23 1.8973070e-35 1.1846945e-19
 1.0000000e+00 6.9067956e-09], sum to 1.0000
[2019-04-01 16:17:50,634] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3136
[2019-04-01 16:17:50,658] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 54.33333333333333, 0.0, 0.0, 26.0, 25.46458675055569, 9.67360408750826, 0.0, 1.0, 37.55104155360561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2322600.0000, 
sim time next is 2323200.0000, 
raw observation next is [-1.7, 54.66666666666667, 0.0, 0.0, 26.0, 25.47968383314248, 9.596235493503881, 0.0, 1.0, 31.0609232745766], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5466666666666667, 0.0, 0.0, 1.0, 0.9256691190203544, 0.09596235493503881, 0.0, 1.0, 0.22186373767554712], 
reward next is 0.7781, 
noisyNet noise sample is [array([-0.0106618], dtype=float32), 0.10280176]. 
=============================================
[2019-04-01 16:18:06,257] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.1783248e-33 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7180874e-27
 1.0000000e+00 3.9673800e-21], sum to 1.0000
[2019-04-01 16:18:06,257] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9517
[2019-04-01 16:18:06,265] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.3, 29.0, 114.0, 351.0, 26.0, 25.85026459035964, 8.929224202234082, 1.0, 1.0, 6.094639758825614], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2561400.0000, 
sim time next is 2562000.0000, 
raw observation next is [3.3, 29.0, 106.6666666666667, 319.5, 26.0, 25.90484196370834, 9.08157326484644, 1.0, 1.0, 6.142706642447497], 
processed observation next is [1.0, 0.6521739130434783, 0.554016620498615, 0.29, 0.3555555555555557, 0.35303867403314915, 1.0, 0.986405994815477, 0.0908157326484644, 1.0, 1.0, 0.043876476017482124], 
reward next is 0.9561, 
noisyNet noise sample is [array([0.0257696], dtype=float32), 0.007370298]. 
=============================================
[2019-04-01 16:18:06,271] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.35095]
 [73.25497]
 [73.24045]
 [73.32263]
 [73.5819 ]], R is [[73.66912842]
 [73.88890076]
 [74.10726929]
 [74.32345581]
 [74.53712463]].
[2019-04-01 16:18:06,811] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6856894e-32 0.0000000e+00 4.8861478e-35 0.0000000e+00 2.9649379e-24
 1.0000000e+00 2.1300852e-20], sum to 1.0000
[2019-04-01 16:18:06,812] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0699
[2019-04-01 16:18:06,845] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 49.0, 134.5, 39.0, 26.0, 25.91989454753789, 7.577035864616557, 1.0, 1.0, 16.09896168222033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2541600.0000, 
sim time next is 2542200.0000, 
raw observation next is [-1.1, 48.66666666666667, 134.0, 41.0, 26.0, 25.93789101582574, 7.537417165296207, 1.0, 1.0, 15.35136021922354], 
processed observation next is [1.0, 0.43478260869565216, 0.4321329639889197, 0.4866666666666667, 0.44666666666666666, 0.045303867403314914, 1.0, 0.9911272879751059, 0.07537417165296208, 1.0, 1.0, 0.10965257299445386], 
reward next is 0.8903, 
noisyNet noise sample is [array([-0.8139741], dtype=float32), 0.57318836]. 
=============================================
[2019-04-01 16:18:09,691] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.82225656e-20 2.96781423e-28 6.31126417e-14 1.00377005e-32
 5.15678249e-19 4.01547819e-04 9.99598444e-01], sum to 1.0000
[2019-04-01 16:18:09,695] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6379
[2019-04-01 16:18:09,723] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.80778909245603, 6.257979494783949, 0.0, 1.0, 41.22268282237501], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2601600.0000, 
sim time next is 2602200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.81771570346349, 6.152059641225994, 0.0, 1.0, 41.25581282532288], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 1.0, 0.8311022433519274, 0.06152059641225994, 0.0, 1.0, 0.2946843773237348], 
reward next is 0.7053, 
noisyNet noise sample is [array([0.88983595], dtype=float32), -0.17043044]. 
=============================================
[2019-04-01 16:18:09,741] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4384166e-33 0.0000000e+00 3.2322116e-33 0.0000000e+00 1.9914958e-20
 1.0000000e+00 3.3415495e-18], sum to 1.0000
[2019-04-01 16:18:09,741] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9328
[2019-04-01 16:18:09,842] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.166666666666667, 49.33333333333334, 231.5, 157.6666666666667, 26.0, 25.19762816591456, 7.465250527246954, 1.0, 1.0, 24.10292713644326], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2637600.0000, 
sim time next is 2638200.0000, 
raw observation next is [-0.8833333333333332, 48.16666666666667, 218.0, 168.3333333333333, 26.0, 24.4792299669102, 7.799430159690182, 1.0, 1.0, 90.19132431098613], 
processed observation next is [1.0, 0.5217391304347826, 0.43813481071098803, 0.4816666666666667, 0.7266666666666667, 0.18600368324125224, 1.0, 0.7827471381300286, 0.07799430159690182, 1.0, 1.0, 0.6442237450784724], 
reward next is 0.3558, 
noisyNet noise sample is [array([0.34835136], dtype=float32), 0.43304837]. 
=============================================
[2019-04-01 16:18:17,410] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.8361874e-14 3.0524293e-27 1.3675601e-19 6.6375614e-25 2.1839012e-11
 9.5565647e-01 4.4343572e-02], sum to 1.0000
[2019-04-01 16:18:17,411] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8494
[2019-04-01 16:18:17,444] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.0, 72.0, 113.6666666666667, 663.6666666666667, 26.0, 26.03224556160438, 9.814387587938901, 1.0, 1.0, 21.75896028367019], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2715600.0000, 
sim time next is 2716200.0000, 
raw observation next is [-10.5, 70.0, 117.0, 674.0, 26.0, 26.00589978192853, 9.895174764764596, 1.0, 1.0, 20.49338536169468], 
processed observation next is [1.0, 0.43478260869565216, 0.17174515235457063, 0.7, 0.39, 0.7447513812154696, 1.0, 1.0008428259897901, 0.09895174764764596, 1.0, 1.0, 0.14638132401210488], 
reward next is 0.8536, 
noisyNet noise sample is [array([-0.27233556], dtype=float32), -2.5925848]. 
=============================================
[2019-04-01 16:18:24,629] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.2092079e-20 6.6193005e-34 2.0076978e-27 4.3412232e-35 5.6736750e-13
 1.0000000e+00 5.6019309e-09], sum to 1.0000
[2019-04-01 16:18:24,629] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9914
[2019-04-01 16:18:24,700] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 100.0, 88.33333333333334, 0.0, 26.0, 26.22714966954098, 11.64285303348835, 1.0, 1.0, 28.65945865080654], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2904600.0000, 
sim time next is 2905200.0000, 
raw observation next is [2.0, 100.0, 87.5, 0.0, 26.0, 26.37618672030404, 12.06993284651143, 1.0, 1.0, 27.01154801250476], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.2916666666666667, 0.0, 1.0, 1.0537409600434344, 0.12069932846511429, 1.0, 1.0, 0.1929396286607483], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8587139], dtype=float32), -0.5036149]. 
=============================================
[2019-04-01 16:18:31,110] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2193102e-17 5.4209196e-26 1.7594203e-16 1.1013328e-30 4.9396034e-13
 1.2184138e-03 9.9878162e-01], sum to 1.0000
[2019-04-01 16:18:31,111] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9643
[2019-04-01 16:18:31,130] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 79.16666666666667, 0.0, 0.0, 26.0, 25.34622777675145, 9.330469818952436, 0.0, 1.0, 44.03337537821017], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2931000.0000, 
sim time next is 2931600.0000, 
raw observation next is [-1.333333333333333, 80.33333333333334, 0.0, 0.0, 26.0, 25.33937275985513, 9.082031655977753, 0.0, 1.0, 43.7770044372644], 
processed observation next is [1.0, 0.9565217391304348, 0.42566943674976926, 0.8033333333333335, 0.0, 0.0, 1.0, 0.9056246799793044, 0.09082031655977753, 0.0, 1.0, 0.3126928888376029], 
reward next is 0.6873, 
noisyNet noise sample is [array([-0.976434], dtype=float32), 0.5479096]. 
=============================================
[2019-04-01 16:18:40,899] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6383925e-13 3.8757699e-24 1.2287240e-14 1.9093998e-26 1.1512314e-15
 9.8763847e-01 1.2361573e-02], sum to 1.0000
[2019-04-01 16:18:40,900] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3691
[2019-04-01 16:18:40,919] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.866666666666667, 100.0, 0.0, 0.0, 26.0, 25.24812235713507, 6.993243011097111, 0.0, 1.0, 52.87903241266756], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3127200.0000, 
sim time next is 3127800.0000, 
raw observation next is [2.933333333333333, 100.0, 0.0, 0.0, 26.0, 25.25304839559928, 7.036416926088362, 0.0, 1.0, 52.83410446459414], 
processed observation next is [1.0, 0.17391304347826086, 0.543859649122807, 1.0, 0.0, 0.0, 1.0, 0.8932926279427542, 0.07036416926088361, 0.0, 1.0, 0.37738646046138674], 
reward next is 0.6226, 
noisyNet noise sample is [array([-0.368508], dtype=float32), 0.419613]. 
=============================================
[2019-04-01 16:18:54,631] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3055864e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.9192853e-23
 1.0000000e+00 2.6390630e-24], sum to 1.0000
[2019-04-01 16:18:54,631] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0787
[2019-04-01 16:18:54,657] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 67.0, 36.5, 317.0, 26.0, 26.71097974452807, 12.53535733076424, 1.0, 1.0, 10.5907699383537], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3430800.0000, 
sim time next is 3431400.0000, 
raw observation next is [2.0, 67.0, 28.33333333333333, 251.6666666666666, 26.0, 26.64818423688915, 14.49122326166706, 1.0, 1.0, 10.43986633699805], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.09444444444444443, 0.27808471454880285, 1.0, 1.0925977481270215, 0.1449122326166706, 1.0, 1.0, 0.07457047383570035], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.00881846], dtype=float32), -1.1019244]. 
=============================================
[2019-04-01 16:18:54,901] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8302932e-15 1.7215146e-24 2.8809932e-13 2.5707394e-25 2.2205436e-12
 1.0000000e+00 6.9699933e-11], sum to 1.0000
[2019-04-01 16:18:54,903] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3354
[2019-04-01 16:18:54,954] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.666666666666667, 60.00000000000001, 72.83333333333333, 369.5, 26.0, 25.58232971517146, 9.307817889443555, 1.0, 1.0, 31.93965648672155], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3399600.0000, 
sim time next is 3400200.0000, 
raw observation next is [-1.5, 60.0, 87.0, 422.0, 26.0, 25.80800338911918, 9.708404531891013, 1.0, 1.0, 27.53513886249578], 
processed observation next is [1.0, 0.34782608695652173, 0.4210526315789474, 0.6, 0.29, 0.4662983425414365, 1.0, 0.9725719127313113, 0.09708404531891013, 1.0, 1.0, 0.1966795633035413], 
reward next is 0.8033, 
noisyNet noise sample is [array([0.5780072], dtype=float32), 0.6004112]. 
=============================================
[2019-04-01 16:18:57,026] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2045361e-20 1.7524571e-32 1.2849267e-23 5.6833407e-34 5.0610662e-17
 1.0000000e+00 2.1250487e-13], sum to 1.0000
[2019-04-01 16:18:57,027] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5620
[2019-04-01 16:18:57,038] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 83.66666666666666, 0.0, 0.0, 26.0, 25.78474129344687, 12.02158705843525, 0.0, 1.0, 25.55630598868182], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3447600.0000, 
sim time next is 3448200.0000, 
raw observation next is [1.0, 84.83333333333334, 0.0, 0.0, 26.0, 25.73263433577857, 11.92567034523457, 0.0, 1.0, 24.23757383501526], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.8483333333333334, 0.0, 0.0, 1.0, 0.9618049051112243, 0.1192567034523457, 0.0, 1.0, 0.17312552739296616], 
reward next is 0.8269, 
noisyNet noise sample is [array([-0.49849027], dtype=float32), -1.0299592]. 
=============================================
[2019-04-01 16:19:05,112] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2098749e-16 3.2171616e-29 1.1954397e-12 8.4314234e-30 8.4898000e-17
 8.1432439e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 16:19:05,112] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3907
[2019-04-01 16:19:05,129] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 50.0, 0.0, 0.0, 26.0, 25.35758748426671, 8.376260837620523, 0.0, 1.0, 38.65775594167233], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3621600.0000, 
sim time next is 3622200.0000, 
raw observation next is [-2.166666666666667, 51.66666666666667, 0.0, 0.0, 26.0, 25.36728943883279, 8.352113571675721, 0.0, 1.0, 38.12732277726371], 
processed observation next is [0.0, 0.9565217391304348, 0.4025854108956602, 0.5166666666666667, 0.0, 0.0, 1.0, 0.9096127769761126, 0.0835211357167572, 0.0, 1.0, 0.2723380198375979], 
reward next is 0.7277, 
noisyNet noise sample is [array([-1.2393786], dtype=float32), -0.48458406]. 
=============================================
[2019-04-01 16:19:07,588] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0455815e-14 2.3176812e-25 5.6077729e-11 3.3256257e-27 1.1375921e-15
 2.2156308e-09 1.0000000e+00], sum to 1.0000
[2019-04-01 16:19:07,591] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7730
[2019-04-01 16:19:07,608] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.5, 29.5, 4.0, 121.0, 26.0, 25.4437252643594, 7.787893273429472, 0.0, 1.0, 31.46934635840402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3655800.0000, 
sim time next is 3656400.0000, 
raw observation next is [8.333333333333334, 30.33333333333334, 18.16666666666666, 168.0, 26.0, 25.43701703720649, 7.987268989756143, 0.0, 1.0, 32.49052205952363], 
processed observation next is [0.0, 0.30434782608695654, 0.6934441366574331, 0.3033333333333334, 0.060555555555555536, 0.1856353591160221, 1.0, 0.9195738624580702, 0.07987268989756142, 0.0, 1.0, 0.2320751575680259], 
reward next is 0.7679, 
noisyNet noise sample is [array([0.49929115], dtype=float32), -1.3105413]. 
=============================================
[2019-04-01 16:19:19,254] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.7325892e-08 1.9336182e-13 1.2759443e-07 2.3358319e-19 4.3437076e-10
 3.2727394e-04 9.9967253e-01], sum to 1.0000
[2019-04-01 16:19:19,255] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8508
[2019-04-01 16:19:19,280] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 68.0, 0.0, 0.0, 26.0, 25.12565063144937, 7.402788182001477, 0.0, 1.0, 40.88915220534231], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3907800.0000, 
sim time next is 3908400.0000, 
raw observation next is [-5.333333333333334, 65.0, 0.0, 0.0, 26.0, 25.03916207048706, 7.259136479208618, 0.0, 1.0, 41.05782367417579], 
processed observation next is [1.0, 0.21739130434782608, 0.31486611265004616, 0.65, 0.0, 0.0, 1.0, 0.8627374386410084, 0.07259136479208618, 0.0, 1.0, 0.29327016910125564], 
reward next is 0.7067, 
noisyNet noise sample is [array([-0.3933376], dtype=float32), -0.7544008]. 
=============================================
[2019-04-01 16:19:22,268] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9409638e-11 1.9995888e-19 7.3163555e-15 1.9674849e-26 3.4572735e-19
 3.2833909e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 16:19:22,269] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0611
[2019-04-01 16:19:22,284] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.833333333333334, 48.33333333333334, 0.0, 0.0, 26.0, 25.3662140921563, 10.49447501340313, 0.0, 1.0, 43.35772046720687], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3966600.0000, 
sim time next is 3967200.0000, 
raw observation next is [-8.0, 49.0, 0.0, 0.0, 26.0, 25.3881596038037, 10.63552687996673, 0.0, 1.0, 44.01668327351786], 
processed observation next is [1.0, 0.9565217391304348, 0.24099722991689754, 0.49, 0.0, 0.0, 1.0, 0.9125942291148144, 0.1063552687996673, 0.0, 1.0, 0.3144048805251276], 
reward next is 0.6856, 
noisyNet noise sample is [array([0.10114928], dtype=float32), 0.40217873]. 
=============================================
[2019-04-01 16:19:25,162] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6606957e-31 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8639316e-23
 1.0000000e+00 1.1836553e-27], sum to 1.0000
[2019-04-01 16:19:25,162] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9042
[2019-04-01 16:19:25,207] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 29.0, 116.0, 835.5, 26.0, 26.22223375125945, 10.76692340095096, 1.0, 1.0, 8.249500939549156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4021200.0000, 
sim time next is 4021800.0000, 
raw observation next is [-3.833333333333333, 28.5, 115.3333333333333, 833.6666666666666, 26.0, 25.68491434994853, 10.95105608803075, 1.0, 1.0, 27.08870966438759], 
processed observation next is [1.0, 0.5652173913043478, 0.3564173591874424, 0.285, 0.3844444444444443, 0.9211786372007366, 1.0, 0.9549877642783616, 0.1095105608803075, 1.0, 1.0, 0.19349078331705422], 
reward next is 0.4261, 
noisyNet noise sample is [array([0.8189052], dtype=float32), -0.082667105]. 
=============================================
[2019-04-01 16:19:25,753] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1230346e-08 8.9804923e-18 6.5855369e-07 1.2442592e-21 2.0197264e-04
 2.2054158e-07 9.9979717e-01], sum to 1.0000
[2019-04-01 16:19:25,754] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6475
[2019-04-01 16:19:25,786] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 38.33333333333334, 0.0, 0.0, 26.0, 25.04364653762708, 7.516492754659995, 0.0, 1.0, 40.1998787577084], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4062000.0000, 
sim time next is 4062600.0000, 
raw observation next is [-6.0, 39.0, 0.0, 0.0, 26.0, 25.01760080342722, 7.505024331009845, 0.0, 1.0, 40.20884868373107], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.39, 0.0, 0.0, 1.0, 0.8596572576324603, 0.07505024331009845, 0.0, 1.0, 0.2872060620266505], 
reward next is 0.7128, 
noisyNet noise sample is [array([0.470757], dtype=float32), 1.0587313]. 
=============================================
[2019-04-01 16:19:32,059] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5713612e-22 4.6870748e-35 2.9903140e-31 5.3351659e-38 1.0711542e-23
 1.0000000e+00 4.8025985e-14], sum to 1.0000
[2019-04-01 16:19:32,070] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3029
[2019-04-01 16:19:32,089] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.5, 36.0, 93.0, 437.0, 26.0, 27.4331868552168, 15.7218388332553, 1.0, 1.0, 1.382231865888288], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4120200.0000, 
sim time next is 4120800.0000, 
raw observation next is [3.333333333333333, 36.33333333333333, 81.33333333333333, 373.6666666666667, 26.0, 27.38587444465803, 18.32380383381727, 1.0, 1.0, 1.383028964116201], 
processed observation next is [1.0, 0.6956521739130435, 0.5549399815327793, 0.3633333333333333, 0.2711111111111111, 0.41289134438305714, 1.0, 1.1979820635225755, 0.1832380383381727, 1.0, 1.0, 0.009878778315115721], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3143654], dtype=float32), -0.27937412]. 
=============================================
[2019-04-01 16:19:36,132] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9683688e-25 2.6207642e-29 7.8448695e-21 0.0000000e+00 1.9371392e-29
 1.2440947e-06 9.9999881e-01], sum to 1.0000
[2019-04-01 16:19:36,134] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0983
[2019-04-01 16:19:36,151] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 26.0, 25.49443528199814, 8.468228876523378, 0.0, 1.0, 36.79806259350726], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4226400.0000, 
sim time next is 4227000.0000, 
raw observation next is [1.0, 47.00000000000001, 0.0, 0.0, 26.0, 25.53789867101126, 8.40528441121655, 0.0, 1.0, 29.38240530117969], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.4700000000000001, 0.0, 0.0, 1.0, 0.9339855244301799, 0.0840528441121655, 0.0, 1.0, 0.20987432357985494], 
reward next is 0.7901, 
noisyNet noise sample is [array([-0.8779633], dtype=float32), 1.3196636]. 
=============================================
[2019-04-01 16:19:36,175] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[84.31124]
 [84.28835]
 [84.23563]
 [84.1132 ]
 [84.04666]], R is [[84.30807495]
 [84.20215607]
 [84.07443237]
 [83.94239807]
 [83.81734467]].
[2019-04-01 16:19:45,976] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6269341e-15 3.4915600e-18 6.2298358e-20 5.0663871e-27 7.7894487e-07
 9.9999917e-01 2.1668443e-09], sum to 1.0000
[2019-04-01 16:19:45,977] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4067
[2019-04-01 16:19:46,007] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 180.3333333333333, 5.0, 26.0, 26.04081736448553, 10.87869746850606, 1.0, 1.0, 10.47490933282682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4454400.0000, 
sim time next is 4455000.0000, 
raw observation next is [0.0, 92.0, 196.0, 6.0, 26.0, 25.63766030379637, 11.07615108439808, 1.0, 1.0, 22.23837653437167], 
processed observation next is [1.0, 0.5652173913043478, 0.46260387811634357, 0.92, 0.6533333333333333, 0.0066298342541436465, 1.0, 0.9482371862566242, 0.11076151084398081, 1.0, 1.0, 0.15884554667408338], 
reward next is 0.4107, 
noisyNet noise sample is [array([1.1097938], dtype=float32), 1.1689957]. 
=============================================
[2019-04-01 16:19:46,051] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[33.106384]
 [32.939472]
 [32.69453 ]
 [32.30968 ]
 [31.88293 ]], R is [[33.12740707]
 [33.3698349 ]
 [33.03613663]
 [32.70577621]
 [32.37871933]].
[2019-04-01 16:19:48,036] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.15039595e-22 6.05257452e-30 1.55235715e-13 1.17667754e-34
 3.88788537e-28 1.08705495e-17 1.00000000e+00], sum to 1.0000
[2019-04-01 16:19:48,057] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7905
[2019-04-01 16:19:48,073] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 73.0, 0.0, 0.0, 26.0, 25.3294869313652, 9.46967366382102, 0.0, 1.0, 42.8210725832171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4502400.0000, 
sim time next is 4503000.0000, 
raw observation next is [-0.9333333333333333, 73.0, 0.0, 0.0, 26.0, 25.43210422970944, 9.589242173596121, 0.0, 1.0, 41.70698528468911], 
processed observation next is [1.0, 0.08695652173913043, 0.4367497691597415, 0.73, 0.0, 0.0, 1.0, 0.9188720328156345, 0.09589242173596121, 0.0, 1.0, 0.2979070377477794], 
reward next is 0.7021, 
noisyNet noise sample is [array([-0.44341445], dtype=float32), 0.11408752]. 
=============================================
[2019-04-01 16:19:48,091] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[58.445934]
 [58.744137]
 [59.04079 ]
 [59.43085 ]
 [59.74807 ]], R is [[58.34156418]
 [58.45228195]
 [58.5655098 ]
 [58.67625809]
 [58.78197479]].
[2019-04-01 16:19:51,151] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.6472575e-19 1.4341480e-25 2.1181544e-22 1.3810726e-37 1.4298573e-21
 6.1026151e-07 9.9999940e-01], sum to 1.0000
[2019-04-01 16:19:51,153] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8921
[2019-04-01 16:19:51,160] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.5193353999358, 9.617523796725672, 1.0, 1.0, 9.428729308309286], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4559400.0000, 
sim time next is 4560000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.40261415915725, 9.212285584603784, 1.0, 1.0, 10.05561179154119], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.9146591655938927, 0.09212285584603784, 1.0, 1.0, 0.07182579851100851], 
reward next is 0.9282, 
noisyNet noise sample is [array([-0.16525586], dtype=float32), -0.36401644]. 
=============================================
[2019-04-01 16:19:51,174] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[58.237267]
 [58.12069 ]
 [58.075203]
 [57.95024 ]
 [57.837868]], R is [[58.61857605]
 [58.96504211]
 [59.31128311]
 [59.58566666]
 [59.86434555]].
[2019-04-01 16:19:53,334] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.7865149e-27 1.0677746e-26 1.2381968e-25 0.0000000e+00 2.0891741e-26
 2.5360231e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 16:19:53,338] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9227
[2019-04-01 16:19:53,355] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.35, 65.33333333333334, 0.0, 0.0, 26.0, 25.52594059117134, 9.41250385702048, 0.0, 1.0, 26.08445198365784], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4587000.0000, 
sim time next is 4587600.0000, 
raw observation next is [-0.5, 65.66666666666667, 0.0, 0.0, 26.0, 25.52027155583313, 9.372583337865764, 0.0, 1.0, 25.33031403915485], 
processed observation next is [1.0, 0.08695652173913043, 0.44875346260387816, 0.6566666666666667, 0.0, 0.0, 1.0, 0.9314673651190185, 0.09372583337865764, 0.0, 1.0, 0.18093081456539178], 
reward next is 0.8191, 
noisyNet noise sample is [array([-0.34561896], dtype=float32), 0.6553599]. 
=============================================
[2019-04-01 16:19:54,504] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6073108e-13 8.6006377e-18 1.3403326e-13 3.5072443e-23 3.2817451e-07
 1.0534301e-01 8.9465672e-01], sum to 1.0000
[2019-04-01 16:19:54,505] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8138
[2019-04-01 16:19:54,527] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666666, 57.33333333333334, 134.8333333333333, 724.0, 26.0, 26.49380543969247, 14.1545958057726, 1.0, 1.0, 9.843202676831002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4616400.0000, 
sim time next is 4617000.0000, 
raw observation next is [1.0, 56.0, 129.0, 767.0, 26.0, 26.59381103943834, 14.39875786783603, 1.0, 1.0, 9.071658995441133], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.56, 0.43, 0.8475138121546961, 1.0, 1.0848301484911913, 0.14398757867836032, 1.0, 1.0, 0.06479756425315095], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.49521908], dtype=float32), 1.940575]. 
=============================================
[2019-04-01 16:19:54,537] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[34.347935]
 [34.4897  ]
 [34.6026  ]
 [34.61246 ]
 [34.446472]], R is [[33.80319977]
 [33.465168  ]
 [33.13051605]
 [32.79920959]
 [32.47121811]].
[2019-04-01 16:19:55,114] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1589254e-19 1.3221420e-26 2.4771073e-18 5.0019426e-35 5.5330428e-25
 5.3894642e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 16:19:55,114] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6430
[2019-04-01 16:19:55,126] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.866666666666667, 76.0, 0.0, 0.0, 26.0, 25.08164667832158, 7.580712277308566, 0.0, 1.0, 35.87485548458794], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4603200.0000, 
sim time next is 4603800.0000, 
raw observation next is [-2.933333333333333, 76.5, 0.0, 0.0, 26.0, 25.04915894983865, 7.577816234146515, 0.0, 1.0, 35.88019995459451], 
processed observation next is [1.0, 0.2608695652173913, 0.38134810710988, 0.765, 0.0, 0.0, 1.0, 0.8641655642626641, 0.07577816234146514, 0.0, 1.0, 0.2562871425328179], 
reward next is 0.7437, 
noisyNet noise sample is [array([-1.6970766], dtype=float32), 0.30717108]. 
=============================================
[2019-04-01 16:19:56,166] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.7723821e-18 2.0816586e-24 1.0138485e-12 2.6160110e-38 1.3090535e-26
 5.8990358e-28 1.0000000e+00], sum to 1.0000
[2019-04-01 16:19:56,166] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9276
[2019-04-01 16:19:56,189] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.72874194466536, 10.88346912325135, 0.0, 1.0, 23.9491090502043], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4666200.0000, 
sim time next is 4666800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.67817414459905, 10.59623693597889, 0.0, 1.0, 22.71707355908245], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.9540248777998644, 0.10596236935978891, 0.0, 1.0, 0.16226481113630323], 
reward next is 0.8377, 
noisyNet noise sample is [array([0.01622579], dtype=float32), -0.5415797]. 
=============================================
[2019-04-01 16:19:56,533] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.8159934e-28 8.6779401e-33 3.2833096e-27 0.0000000e+00 7.1604530e-25
 2.5871311e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 16:19:56,535] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8378
[2019-04-01 16:19:56,547] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.666666666666667, 50.33333333333334, 0.0, 0.0, 26.0, 25.15783307433458, 12.22988077499748, 1.0, 1.0, 6.861958798852934], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4645200.0000, 
sim time next is 4645800.0000, 
raw observation next is [3.5, 51.0, 0.0, 0.0, 26.0, 25.59682507573693, 12.98696976836498, 1.0, 1.0, 5.412047235914653], 
processed observation next is [1.0, 0.782608695652174, 0.5595567867036012, 0.51, 0.0, 0.0, 1.0, 0.9424035822481329, 0.1298696976836498, 1.0, 1.0, 0.03865748025653324], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.07560761], dtype=float32), 0.36585143]. 
=============================================
[2019-04-01 16:20:06,399] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7945530e-28 1.5234059e-29 4.5069714e-25 0.0000000e+00 2.5507803e-34
 3.6413569e-34 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:06,399] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2623
[2019-04-01 16:20:06,415] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8333333333333334, 54.33333333333334, 0.0, 0.0, 26.0, 25.35848178809591, 8.601515859383918, 0.0, 1.0, 40.35380279082181], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4830600.0000, 
sim time next is 4831200.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.39100560706583, 8.669871003531497, 0.0, 1.0, 39.91499794552722], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 1.0, 0.9130008010094041, 0.08669871003531497, 0.0, 1.0, 0.2851071281823373], 
reward next is 0.7149, 
noisyNet noise sample is [array([-1.7301543], dtype=float32), -1.0333563]. 
=============================================
[2019-04-01 16:20:09,499] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.9824310e-32 3.8323772e-33 7.9822627e-28 0.0000000e+00 1.5282902e-28
 6.2206428e-30 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:09,501] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7697
[2019-04-01 16:20:09,522] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 39.33333333333334, 0.0, 0.0, 26.0, 25.45020434075141, 8.70069105474073, 0.0, 1.0, 39.5952694997845], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4911000.0000, 
sim time next is 4911600.0000, 
raw observation next is [1.0, 38.66666666666667, 0.0, 0.0, 26.0, 25.52327586591574, 8.739177511822186, 0.0, 1.0, 32.50140544174755], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.3866666666666667, 0.0, 0.0, 1.0, 0.931896552273677, 0.08739177511822187, 0.0, 1.0, 0.2321528960124825], 
reward next is 0.7678, 
noisyNet noise sample is [array([-1.721091], dtype=float32), 2.5129418]. 
=============================================
[2019-04-01 16:20:10,271] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7363601e-29 2.8053770e-34 2.3253287e-21 0.0000000e+00 2.8034477e-29
 5.5669409e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:10,276] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8167
[2019-04-01 16:20:10,291] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 48.66666666666666, 0.0, 0.0, 26.0, 25.10080419884893, 6.309692478095063, 0.0, 1.0, 38.22052132463081], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4945200.0000, 
sim time next is 4945800.0000, 
raw observation next is [-2.833333333333333, 49.33333333333334, 0.0, 0.0, 26.0, 25.02797370241458, 6.254648944714755, 0.0, 1.0, 38.25351107675277], 
processed observation next is [1.0, 0.21739130434782608, 0.3841181902123731, 0.4933333333333334, 0.0, 0.0, 1.0, 0.8611391003449401, 0.06254648944714755, 0.0, 1.0, 0.2732393648339484], 
reward next is 0.7268, 
noisyNet noise sample is [array([-0.4057573], dtype=float32), -0.056900334]. 
=============================================
[2019-04-01 16:20:10,528] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.16573778e-24 3.48455060e-33 9.96622554e-30 0.00000000e+00
 1.08353096e-26 4.30197570e-18 1.00000000e+00], sum to 1.0000
[2019-04-01 16:20:10,531] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2804
[2019-04-01 16:20:10,540] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 44.5, 236.0, 374.0, 26.0, 25.06389869387879, 8.353168539259483, 0.0, 1.0, 12.8647671283747], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4890600.0000, 
sim time next is 4891200.0000, 
raw observation next is [2.666666666666667, 44.66666666666667, 223.8333333333333, 382.0, 26.0, 25.06586503534263, 8.382345416335555, 0.0, 1.0, 12.57762806572377], 
processed observation next is [0.0, 0.6086956521739131, 0.5364727608494922, 0.4466666666666667, 0.746111111111111, 0.4220994475138122, 1.0, 0.8665521479060898, 0.08382345416335556, 0.0, 1.0, 0.0898402004694555], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.2187214], dtype=float32), -1.0772474]. 
=============================================
[2019-04-01 16:20:10,785] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8548625e-28 2.4624233e-30 6.4289475e-27 0.0000000e+00 2.1486536e-33
 6.0841744e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:10,788] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4256
[2019-04-01 16:20:10,795] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 79.00000000000001, 273.6666666666667, 26.0, 25.19695972629253, 8.19946522867123, 0.0, 1.0, 5.730959086047829], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4899000.0000, 
sim time next is 4899600.0000, 
raw observation next is [3.0, 45.0, 67.5, 252.0, 26.0, 25.16047067106108, 8.034962739539019, 0.0, 1.0, 5.61028169135106], 
processed observation next is [0.0, 0.7391304347826086, 0.5457063711911359, 0.45, 0.225, 0.27845303867403315, 1.0, 0.8800672387230115, 0.08034962739539019, 0.0, 1.0, 0.04007344065250757], 
reward next is 0.9599, 
noisyNet noise sample is [array([0.13530587], dtype=float32), -0.08453719]. 
=============================================
[2019-04-01 16:20:11,056] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0029463e-28 1.5072406e-33 6.2350588e-24 0.0000000e+00 2.3605570e-31
 2.3323901e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:11,059] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5055
[2019-04-01 16:20:11,074] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.00629852466891, 6.284192224582609, 0.0, 1.0, 38.23274051297874], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4946400.0000, 
sim time next is 4947000.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.02397509962162, 6.319413933680234, 0.0, 1.0, 38.2068133813582], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 1.0, 0.8605678713745171, 0.06319413933680235, 0.0, 1.0, 0.2729058098668443], 
reward next is 0.7271, 
noisyNet noise sample is [array([-0.46844235], dtype=float32), -0.36298797]. 
=============================================
[2019-04-01 16:20:11,083] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[68.35761]
 [68.4882 ]
 [68.58175]
 [68.66051]
 [68.76338]], R is [[68.28917694]
 [68.33319092]
 [68.37664032]
 [68.41989136]
 [68.4631424 ]].
[2019-04-01 16:20:13,126] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7900203e-19 1.3869647e-23 1.1734241e-10 3.9775482e-36 7.9947455e-12
 2.3229006e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:13,131] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7335
[2019-04-01 16:20:13,179] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 25.83855012480947, 12.49757376757167, 0.0, 1.0, 21.1743108446148], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5004000.0000, 
sim time next is 5004600.0000, 
raw observation next is [3.0, 36.5, 0.0, 0.0, 26.0, 25.85933858173377, 11.08577455031652, 0.0, 1.0, 21.96923669534098], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.365, 0.0, 0.0, 1.0, 0.9799055116762528, 0.1108577455031652, 0.0, 1.0, 0.15692311925243557], 
reward next is 0.8431, 
noisyNet noise sample is [array([0.02024194], dtype=float32), -0.048441347]. 
=============================================
[2019-04-01 16:20:13,217] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6150075e-15 1.3166409e-26 1.4238711e-28 0.0000000e+00 3.0307888e-13
 1.7328754e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:13,217] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5192
[2019-04-01 16:20:13,239] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 24.0, 0.0, 0.0, 26.0, 26.29073926130991, 14.09683252519216, 1.0, 1.0, 6.21097074233681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4991400.0000, 
sim time next is 4992000.0000, 
raw observation next is [6.0, 23.66666666666666, 0.0, 0.0, 26.0, 25.28031752189467, 12.28878573508313, 1.0, 1.0, 4.363736785772038], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2366666666666666, 0.0, 0.0, 1.0, 0.8971882174135243, 0.1228878573508313, 1.0, 1.0, 0.03116954846980027], 
reward next is 0.0533, 
noisyNet noise sample is [array([-0.42736313], dtype=float32), -1.6859596]. 
=============================================
[2019-04-01 16:20:13,255] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[33.2784  ]
 [32.289944]
 [30.844246]
 [29.671177]
 [28.884386]], R is [[33.72115326]
 [33.38394165]
 [33.05010223]
 [32.71960068]
 [32.39240646]].
[2019-04-01 16:20:14,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9242074e-18 4.1400272e-28 1.9831123e-29 6.9780011e-36 5.6997185e-19
 6.9175348e-02 9.3082458e-01], sum to 1.0000
[2019-04-01 16:20:14,177] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8740
[2019-04-01 16:20:14,206] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.333333333333334, 25.66666666666667, 123.8333333333333, 861.6666666666667, 26.0, 27.27788200851332, 16.38183920128678, 1.0, 1.0, 1.992891396192438], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5055600.0000, 
sim time next is 5056200.0000, 
raw observation next is [8.5, 25.5, 124.0, 865.0, 26.0, 26.99926895861942, 19.57857710939142, 1.0, 1.0, 1.923916905254617], 
processed observation next is [1.0, 0.5217391304347826, 0.698060941828255, 0.255, 0.41333333333333333, 0.9558011049723757, 1.0, 1.142752708374203, 0.1957857710939142, 1.0, 1.0, 0.01374226360896155], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.01352], dtype=float32), 0.9913543]. 
=============================================
[2019-04-01 16:20:15,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:15,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:16,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run6
[2019-04-01 16:20:16,432] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:16,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:16,483] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run6
[2019-04-01 16:20:16,800] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:16,800] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:16,846] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run6
[2019-04-01 16:20:18,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:18,088] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:18,091] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run6
[2019-04-01 16:20:18,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:18,195] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:18,204] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run6
[2019-04-01 16:20:18,324] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:18,324] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:18,335] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run6
[2019-04-01 16:20:18,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:18,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:18,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run6
[2019-04-01 16:20:18,674] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:18,675] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:18,684] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run6
[2019-04-01 16:20:18,844] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:18,844] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:18,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run6
[2019-04-01 16:20:18,902] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:18,902] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:18,908] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run6
[2019-04-01 16:20:19,190] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:19,190] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:19,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run6
[2019-04-01 16:20:19,295] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:19,295] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:19,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run6
[2019-04-01 16:20:19,679] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:19,679] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:19,683] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run6
[2019-04-01 16:20:19,930] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:19,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:19,934] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run6
[2019-04-01 16:20:20,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:20,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:20,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run6
[2019-04-01 16:20:20,145] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:20:20,146] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:20:20,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run6
[2019-04-01 16:20:22,768] A3C_AGENT_WORKER-Thread-4 INFO:Local step 42500, global step 679700: loss 1.2961
[2019-04-01 16:20:22,769] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 42500, global step 679700: learning rate 0.0010
[2019-04-01 16:20:23,588] A3C_AGENT_WORKER-Thread-16 INFO:Local step 42500, global step 679749: loss 0.9865
[2019-04-01 16:20:23,589] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 42500, global step 679749: learning rate 0.0010
[2019-04-01 16:20:24,376] A3C_AGENT_WORKER-Thread-17 INFO:Local step 42500, global step 679803: loss 0.3686
[2019-04-01 16:20:24,376] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 42500, global step 679803: learning rate 0.0010
[2019-04-01 16:20:26,161] A3C_AGENT_WORKER-Thread-11 INFO:Local step 42500, global step 679907: loss 0.2391
[2019-04-01 16:20:26,170] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 42500, global step 679909: learning rate 0.0010
[2019-04-01 16:20:26,577] A3C_AGENT_WORKER-Thread-15 INFO:Local step 42500, global step 679962: loss 0.2724
[2019-04-01 16:20:26,578] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 42500, global step 679962: learning rate 0.0010
[2019-04-01 16:20:26,581] A3C_AGENT_WORKER-Thread-18 INFO:Local step 42500, global step 679963: loss 0.2828
[2019-04-01 16:20:26,581] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 42500, global step 679963: learning rate 0.0010
[2019-04-01 16:20:27,332] A3C_AGENT_WORKER-Thread-2 INFO:Local step 42500, global step 680091: loss 0.4449
[2019-04-01 16:20:27,341] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 42500, global step 680091: learning rate 0.0010
[2019-04-01 16:20:27,712] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0583924e-21 4.7116486e-29 3.5274381e-20 3.1639418e-35 7.6511280e-26
 1.7101235e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:27,713] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7485
[2019-04-01 16:20:27,761] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.199999999999999, 96.0, 0.0, 0.0, 26.0, 20.27296541626721, 18.61549166692402, 0.0, 1.0, 43.07675356051003], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 6600.0000, 
sim time next is 7200.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 26.0, 20.33809210750304, 18.25237401206585, 0.0, 1.0, 42.81790721096496], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 1.0, 0.19115601535757726, 0.1825237401206585, 0.0, 1.0, 0.3058421943640354], 
reward next is 0.6942, 
noisyNet noise sample is [array([-0.16105348], dtype=float32), 2.6250806]. 
=============================================
[2019-04-01 16:20:27,771] A3C_AGENT_WORKER-Thread-3 INFO:Local step 42500, global step 680197: loss 0.2469
[2019-04-01 16:20:27,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 42500, global step 680197: learning rate 0.0010
[2019-04-01 16:20:27,824] A3C_AGENT_WORKER-Thread-10 INFO:Local step 42500, global step 680214: loss 0.8030
[2019-04-01 16:20:27,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 42500, global step 680214: learning rate 0.0010
[2019-04-01 16:20:28,021] A3C_AGENT_WORKER-Thread-13 INFO:Local step 42500, global step 680282: loss 0.4518
[2019-04-01 16:20:28,023] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 42500, global step 680282: learning rate 0.0010
[2019-04-01 16:20:28,077] A3C_AGENT_WORKER-Thread-9 INFO:Local step 42500, global step 680300: loss 1.6199
[2019-04-01 16:20:28,078] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 42500, global step 680300: learning rate 0.0010
[2019-04-01 16:20:28,153] A3C_AGENT_WORKER-Thread-12 INFO:Local step 42500, global step 680329: loss 1.7027
[2019-04-01 16:20:28,178] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 42500, global step 680334: learning rate 0.0010
[2019-04-01 16:20:28,377] A3C_AGENT_WORKER-Thread-5 INFO:Local step 42500, global step 680407: loss 3.6290
[2019-04-01 16:20:28,378] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 42500, global step 680407: learning rate 0.0010
[2019-04-01 16:20:28,751] A3C_AGENT_WORKER-Thread-19 INFO:Local step 42500, global step 680558: loss 4.0822
[2019-04-01 16:20:28,756] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 42500, global step 680560: learning rate 0.0010
[2019-04-01 16:20:29,011] A3C_AGENT_WORKER-Thread-20 INFO:Local step 42500, global step 680644: loss 1.6087
[2019-04-01 16:20:29,013] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 42500, global step 680644: learning rate 0.0010
[2019-04-01 16:20:29,013] A3C_AGENT_WORKER-Thread-14 INFO:Local step 42500, global step 680645: loss 1.2258
[2019-04-01 16:20:29,014] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 42500, global step 680646: learning rate 0.0010
[2019-04-01 16:20:33,877] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5223369e-29 1.0153061e-30 2.4846925e-22 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:33,877] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7302
[2019-04-01 16:20:33,910] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.09999999999999999, 94.33333333333334, 0.0, 0.0, 26.0, 24.29575581819556, 5.865834616976014, 0.0, 1.0, 39.64467681055822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 87000.0000, 
sim time next is 87600.0000, 
raw observation next is [-0.2, 93.66666666666667, 0.0, 0.0, 26.0, 24.27391394270554, 5.849745059077553, 0.0, 1.0, 39.73243542242935], 
processed observation next is [1.0, 0.0, 0.4570637119113574, 0.9366666666666668, 0.0, 0.0, 1.0, 0.7534162775293629, 0.05849745059077553, 0.0, 1.0, 0.28380311016020965], 
reward next is 0.7162, 
noisyNet noise sample is [array([-0.36377293], dtype=float32), 0.49715823]. 
=============================================
[2019-04-01 16:20:40,348] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.1046462e-35 0.0000000e+00 5.8622609e-38 0.0000000e+00 3.6984307e-27
 2.3887568e-23 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:40,349] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5459
[2019-04-01 16:20:40,412] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 65.0, 133.0, 0.0, 26.0, 25.30490626281637, 6.825354550986863, 1.0, 1.0, 31.41949934517089], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 217200.0000, 
sim time next is 217800.0000, 
raw observation next is [-4.75, 65.0, 129.0, 0.0, 26.0, 25.25290181838715, 6.73783208951635, 1.0, 1.0, 31.55482834632256], 
processed observation next is [1.0, 0.5217391304347826, 0.3310249307479225, 0.65, 0.43, 0.0, 1.0, 0.8932716883410217, 0.0673783208951635, 1.0, 1.0, 0.22539163104516113], 
reward next is 0.7746, 
noisyNet noise sample is [array([-0.25607705], dtype=float32), 1.2611729]. 
=============================================
[2019-04-01 16:20:42,476] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4175374e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 2.2938770e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:42,476] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9890
[2019-04-01 16:20:42,524] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.2, 66.16666666666667, 145.0, 0.0, 26.0, 25.38157125360935, 6.90897975149116, 1.0, 1.0, 32.4218004727167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 215400.0000, 
sim time next is 216000.0000, 
raw observation next is [-5.0, 65.0, 141.0, 0.0, 26.0, 25.33397026061224, 6.849343766709308, 1.0, 1.0, 31.98745592084951], 
processed observation next is [1.0, 0.5217391304347826, 0.32409972299168976, 0.65, 0.47, 0.0, 1.0, 0.9048528943731772, 0.06849343766709308, 1.0, 1.0, 0.22848182800606792], 
reward next is 0.7715, 
noisyNet noise sample is [array([0.7982663], dtype=float32), 0.13645627]. 
=============================================
[2019-04-01 16:20:42,530] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.689415]
 [73.54885 ]
 [73.36789 ]
 [73.14791 ]
 [72.94962 ]], R is [[73.79598236]
 [73.8264389 ]
 [73.85329437]
 [73.87613678]
 [73.89504242]].
[2019-04-01 16:20:48,246] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6683866e-28 1.9173192e-31 7.8743657e-25 3.2457329e-38 2.4223457e-35
 4.6788277e-25 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:48,247] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8045
[2019-04-01 16:20:48,275] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.9, 69.0, 0.0, 0.0, 26.0, 22.63201348962993, 6.694037007064327, 0.0, 1.0, 47.12925224348212], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 282000.0000, 
sim time next is 282600.0000, 
raw observation next is [-12.0, 68.5, 0.0, 0.0, 26.0, 22.57159228139683, 6.802286788807386, 0.0, 1.0, 47.15324936270441], 
processed observation next is [1.0, 0.2608695652173913, 0.13019390581717452, 0.685, 0.0, 0.0, 1.0, 0.5102274687709755, 0.06802286788807387, 0.0, 1.0, 0.3368089240193172], 
reward next is 0.6632, 
noisyNet noise sample is [array([-0.37389126], dtype=float32), 1.0396403]. 
=============================================
[2019-04-01 16:20:48,973] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43000, global step 687078: loss 1.1506
[2019-04-01 16:20:48,973] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43000, global step 687079: learning rate 0.0010
[2019-04-01 16:20:49,569] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43000, global step 687310: loss 0.7385
[2019-04-01 16:20:49,571] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43000, global step 687310: learning rate 0.0010
[2019-04-01 16:20:49,678] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43000, global step 687359: loss 0.3909
[2019-04-01 16:20:49,679] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43000, global step 687359: learning rate 0.0010
[2019-04-01 16:20:50,932] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43000, global step 687707: loss -1.4119
[2019-04-01 16:20:50,934] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43000, global step 687707: learning rate 0.0010
[2019-04-01 16:20:51,659] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43000, global step 687869: loss 0.2949
[2019-04-01 16:20:51,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43000, global step 687869: learning rate 0.0010
[2019-04-01 16:20:51,828] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43000, global step 687912: loss 0.0959
[2019-04-01 16:20:51,828] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43000, global step 687912: learning rate 0.0010
[2019-04-01 16:20:52,384] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43000, global step 688067: loss 0.1210
[2019-04-01 16:20:52,385] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43000, global step 688067: learning rate 0.0010
[2019-04-01 16:20:52,522] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43000, global step 688106: loss 0.0646
[2019-04-01 16:20:52,523] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43000, global step 688106: learning rate 0.0010
[2019-04-01 16:20:52,746] A3C_AGENT_WORKER-Thread-9 INFO:Local step 43000, global step 688170: loss -0.6939
[2019-04-01 16:20:52,747] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 43000, global step 688170: learning rate 0.0010
[2019-04-01 16:20:53,069] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43000, global step 688260: loss 0.0077
[2019-04-01 16:20:53,070] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43000, global step 688260: learning rate 0.0010
[2019-04-01 16:20:53,150] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43000, global step 688283: loss -0.2863
[2019-04-01 16:20:53,152] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43000, global step 688283: learning rate 0.0010
[2019-04-01 16:20:53,536] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43000, global step 688399: loss -0.4452
[2019-04-01 16:20:53,536] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43000, global step 688399: learning rate 0.0010
[2019-04-01 16:20:53,539] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43000, global step 688399: loss 0.0864
[2019-04-01 16:20:53,539] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43000, global step 688399: learning rate 0.0010
[2019-04-01 16:20:53,544] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43000, global step 688400: loss 0.0366
[2019-04-01 16:20:53,545] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43000, global step 688400: learning rate 0.0010
[2019-04-01 16:20:53,697] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43000, global step 688441: loss -0.0270
[2019-04-01 16:20:53,697] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43000, global step 688441: learning rate 0.0010
[2019-04-01 16:20:54,183] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43000, global step 688584: loss -0.0310
[2019-04-01 16:20:54,184] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43000, global step 688584: learning rate 0.0010
[2019-04-01 16:20:56,769] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3169575e-17 1.0157565e-28 1.0729528e-17 7.8015926e-32 8.8095997e-23
 5.0536273e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:56,769] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1202
[2019-04-01 16:20:56,786] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.91666666666667, 69.0, 0.0, 0.0, 26.0, 23.27218394869478, 5.796129226154785, 0.0, 1.0, 47.57658006214069], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 352200.0000, 
sim time next is 352800.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.26153076239245, 5.875157682968205, 0.0, 1.0, 47.78357124070722], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 1.0, 0.6087901089132072, 0.05875157682968205, 0.0, 1.0, 0.3413112231479087], 
reward next is 0.6587, 
noisyNet noise sample is [array([0.45951042], dtype=float32), -0.5996389]. 
=============================================
[2019-04-01 16:20:59,667] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2514421e-30 0.0000000e+00 4.9611510e-32 0.0000000e+00 6.9160569e-22
 6.6373212e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 16:20:59,667] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4377
[2019-04-01 16:20:59,715] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.76666666666667, 62.0, 68.83333333333334, 734.8333333333333, 26.0, 25.83825042801426, 8.138405858622564, 1.0, 1.0, 31.43140556349002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 384000.0000, 
sim time next is 384600.0000, 
raw observation next is [-13.58333333333333, 61.0, 66.66666666666666, 740.6666666666666, 26.0, 25.81858157021459, 7.974773873476273, 1.0, 1.0, 29.63465609524459], 
processed observation next is [1.0, 0.43478260869565216, 0.08633425669436758, 0.61, 0.22222222222222218, 0.8184162062615101, 1.0, 0.9740830814592272, 0.07974773873476274, 1.0, 1.0, 0.21167611496603278], 
reward next is 0.7883, 
noisyNet noise sample is [array([0.59619707], dtype=float32), -0.71500874]. 
=============================================
[2019-04-01 16:21:00,865] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0735402e-28 0.0000000e+00 1.4695535e-27 0.0000000e+00 1.6457616e-24
 4.7723517e-02 9.5227647e-01], sum to 1.0000
[2019-04-01 16:21:00,865] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1412
[2019-04-01 16:21:00,904] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.7, 46.83333333333334, 52.66666666666667, 868.6666666666666, 26.0, 26.23087552241138, 10.87330615515995, 1.0, 1.0, 25.5796603302681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 395400.0000, 
sim time next is 396000.0000, 
raw observation next is [-10.5, 46.0, 51.5, 859.5, 26.0, 26.25335812370781, 10.82637994287096, 1.0, 1.0, 24.25179708705355], 
processed observation next is [1.0, 0.6086956521739131, 0.17174515235457063, 0.46, 0.17166666666666666, 0.9497237569060774, 1.0, 1.0361940176725442, 0.10826379942870959, 1.0, 1.0, 0.1732271220503825], 
reward next is 0.4962, 
noisyNet noise sample is [array([-0.04771733], dtype=float32), -0.60275126]. 
=============================================
[2019-04-01 16:21:00,916] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[72.559525]
 [72.30464 ]
 [72.13554 ]
 [71.95869 ]
 [71.76463 ]], R is [[72.58444977]
 [72.3265686 ]
 [72.08402252]
 [71.85372162]
 [71.62879944]].
[2019-04-01 16:21:04,049] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.06605955e-29 9.57292279e-36 4.36886663e-28 0.00000000e+00
 1.26786737e-28 8.46318787e-32 1.00000000e+00], sum to 1.0000
[2019-04-01 16:21:04,050] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0540
[2019-04-01 16:21:04,068] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.61666666666667, 54.16666666666666, 0.0, 0.0, 26.0, 23.75618234665477, 5.563901843388339, 0.0, 1.0, 44.54614239025142], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 432600.0000, 
sim time next is 433200.0000, 
raw observation next is [-11.53333333333333, 54.33333333333334, 0.0, 0.0, 26.0, 23.71291032031112, 5.55567965482363, 0.0, 1.0, 44.62179101913307], 
processed observation next is [1.0, 0.0, 0.14312096029547564, 0.5433333333333334, 0.0, 0.0, 1.0, 0.6732729029015887, 0.055556796548236295, 0.0, 1.0, 0.31872707870809336], 
reward next is 0.6813, 
noisyNet noise sample is [array([0.64265907], dtype=float32), -1.6157473]. 
=============================================
[2019-04-01 16:21:10,033] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.6824255e-28 1.7851449e-35 1.5690713e-25 0.0000000e+00 3.2405651e-36
 3.7611281e-29 1.0000000e+00], sum to 1.0000
[2019-04-01 16:21:10,033] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1453
[2019-04-01 16:21:10,050] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.766666666666667, 88.66666666666667, 0.0, 0.0, 26.0, 25.09020338806689, 6.961455363905439, 0.0, 1.0, 38.96707347127686], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 523200.0000, 
sim time next is 523800.0000, 
raw observation next is [4.65, 88.5, 0.0, 0.0, 26.0, 25.10664747219361, 6.920946932397901, 0.0, 1.0, 39.00640044605828], 
processed observation next is [0.0, 0.043478260869565216, 0.5914127423822716, 0.885, 0.0, 0.0, 1.0, 0.872378210313373, 0.069209469323979, 0.0, 1.0, 0.2786171460432734], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.4660834], dtype=float32), -0.053126454]. 
=============================================
[2019-04-01 16:21:12,842] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43500, global step 694786: loss 0.2209
[2019-04-01 16:21:12,844] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43500, global step 694786: learning rate 0.0010
[2019-04-01 16:21:14,018] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43500, global step 695168: loss 0.0865
[2019-04-01 16:21:14,020] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43500, global step 695168: learning rate 0.0010
[2019-04-01 16:21:14,163] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43500, global step 695213: loss 0.0916
[2019-04-01 16:21:14,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43500, global step 695218: learning rate 0.0010
[2019-04-01 16:21:14,958] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43500, global step 695494: loss 0.5451
[2019-04-01 16:21:14,959] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43500, global step 695494: learning rate 0.0010
[2019-04-01 16:21:15,792] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43500, global step 695819: loss 1.3837
[2019-04-01 16:21:15,792] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43500, global step 695819: learning rate 0.0010
[2019-04-01 16:21:15,863] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43500, global step 695852: loss 0.9897
[2019-04-01 16:21:15,864] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43500, global step 695853: learning rate 0.0010
[2019-04-01 16:21:15,950] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43500, global step 695886: loss 0.5635
[2019-04-01 16:21:15,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43500, global step 695886: learning rate 0.0010
[2019-04-01 16:21:16,050] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43500, global step 695928: loss 0.1960
[2019-04-01 16:21:16,051] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43500, global step 695928: learning rate 0.0010
[2019-04-01 16:21:16,327] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43500, global step 696046: loss 0.0227
[2019-04-01 16:21:16,327] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43500, global step 696047: learning rate 0.0010
[2019-04-01 16:21:16,745] A3C_AGENT_WORKER-Thread-9 INFO:Local step 43500, global step 696259: loss 0.0265
[2019-04-01 16:21:16,746] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 43500, global step 696260: learning rate 0.0010
[2019-04-01 16:21:17,069] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43500, global step 696411: loss 0.1505
[2019-04-01 16:21:17,071] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43500, global step 696413: learning rate 0.0010
[2019-04-01 16:21:17,158] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43500, global step 696449: loss 0.0191
[2019-04-01 16:21:17,159] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43500, global step 696449: learning rate 0.0010
[2019-04-01 16:21:17,269] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43500, global step 696497: loss 0.0024
[2019-04-01 16:21:17,270] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43500, global step 696497: learning rate 0.0010
[2019-04-01 16:21:17,367] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43500, global step 696538: loss 0.0079
[2019-04-01 16:21:17,368] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43500, global step 696538: learning rate 0.0010
[2019-04-01 16:21:17,376] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43500, global step 696542: loss 0.0153
[2019-04-01 16:21:17,383] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43500, global step 696544: learning rate 0.0010
[2019-04-01 16:21:17,477] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43500, global step 696583: loss 0.0102
[2019-04-01 16:21:17,478] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43500, global step 696583: learning rate 0.0010
[2019-04-01 16:21:17,950] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8345197e-32 0.0000000e+00 1.2141268e-26 0.0000000e+00 3.6642610e-36
 2.0285463e-26 1.0000000e+00], sum to 1.0000
[2019-04-01 16:21:17,950] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5024
[2019-04-01 16:21:18,003] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.65, 65.0, 100.0, 0.0, 26.0, 24.77445126493222, 6.517126439242148, 0.0, 1.0, 60.77731271668171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 642600.0000, 
sim time next is 643200.0000, 
raw observation next is [-3.566666666666666, 65.0, 98.16666666666667, 6.333333333333332, 26.0, 24.8719340410238, 6.668905479133822, 0.0, 1.0, 48.60002103404198], 
processed observation next is [0.0, 0.43478260869565216, 0.3638042474607572, 0.65, 0.32722222222222225, 0.006998158379373847, 1.0, 0.838847720146257, 0.06668905479133821, 0.0, 1.0, 0.34714300738601417], 
reward next is 0.6529, 
noisyNet noise sample is [array([1.3506455], dtype=float32), 0.31657448]. 
=============================================
[2019-04-01 16:21:19,786] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.5509860e-33 0.0000000e+00 1.7303348e-34 0.0000000e+00 4.5354036e-36
 6.0477755e-37 1.0000000e+00], sum to 1.0000
[2019-04-01 16:21:19,789] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3372
[2019-04-01 16:21:19,840] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 59.0, 147.0, 96.5, 26.0, 24.88704437782914, 6.8114875904093, 0.0, 1.0, 39.41669626974661], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 651600.0000, 
sim time next is 652200.0000, 
raw observation next is [-2.116666666666667, 59.16666666666667, 158.6666666666667, 95.33333333333334, 26.0, 24.84792837690813, 6.869584402100062, 0.0, 1.0, 44.05137025392632], 
processed observation next is [0.0, 0.5652173913043478, 0.40397045244690677, 0.5916666666666667, 0.5288888888888891, 0.10534069981583795, 1.0, 0.8354183395583041, 0.06869584402100062, 0.0, 1.0, 0.3146526446709023], 
reward next is 0.6853, 
noisyNet noise sample is [array([1.6188637], dtype=float32), 0.07631189]. 
=============================================
[2019-04-01 16:21:25,907] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6870236e-18 5.2236862e-35 1.0609057e-32 3.5092199e-38 2.0348046e-11
 6.5132021e-03 9.9348676e-01], sum to 1.0000
[2019-04-01 16:21:25,907] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4742
[2019-04-01 16:21:25,922] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2333333333333334, 54.66666666666667, 123.1666666666667, 504.0, 26.0, 25.79292943210215, 8.39369958801567, 1.0, 1.0, 8.638854239413227], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 735600.0000, 
sim time next is 736200.0000, 
raw observation next is [-0.04999999999999999, 53.5, 131.0, 449.0, 26.0, 25.751416347769, 8.223317344636534, 1.0, 1.0, 8.487976858911361], 
processed observation next is [1.0, 0.5217391304347826, 0.461218836565097, 0.535, 0.43666666666666665, 0.49613259668508286, 1.0, 0.9644880496812854, 0.08223317344636534, 1.0, 1.0, 0.06062840613508115], 
reward next is 0.9394, 
noisyNet noise sample is [array([-0.07079047], dtype=float32), 0.3573877]. 
=============================================
[2019-04-01 16:21:26,335] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-01 16:21:26,336] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 16:21:26,336] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:21:26,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run8
[2019-04-01 16:21:26,353] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 16:21:26,357] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:21:26,358] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 16:21:26,359] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:21:26,362] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run8
[2019-04-01 16:21:26,362] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run8
[2019-04-01 16:22:18,849] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.4696403], dtype=float32), -0.3940882]
[2019-04-01 16:22:18,850] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.3, 76.0, 0.0, 0.0, 26.0, 25.06222277246726, 9.56465051754052, 1.0, 1.0, 17.66119315839964]
[2019-04-01 16:22:18,851] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 16:22:18,852] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.8374000e-31 0.0000000e+00 1.5512684e-35 0.0000000e+00 7.4237263e-29
 4.1958023e-25 1.0000000e+00], sampled 0.7933600867004221
[2019-04-01 16:22:34,215] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.4696403], dtype=float32), -0.3940882]
[2019-04-01 16:22:34,215] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.4, 61.0, 0.0, 0.0, 26.0, 23.35807621463404, 5.525529353837752, 0.0, 1.0, 43.74158108848707]
[2019-04-01 16:22:34,215] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:22:34,216] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.0326407e-27 2.6702977e-36 2.2249252e-25 0.0000000e+00 1.6834213e-30
 3.5767915e-25 1.0000000e+00], sampled 0.9269398378437493
[2019-04-01 16:23:04,329] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.4696403], dtype=float32), -0.3940882]
[2019-04-01 16:23:04,330] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [8.016666666666666, 70.5, 202.6666666666667, 108.6666666666667, 26.0, 26.42641192045763, 22.20016676233046, 0.0, 1.0, 7.479763328388787]
[2019-04-01 16:23:04,330] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 16:23:04,330] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.6351051e-34 0.0000000e+00 3.4595710e-33 0.0000000e+00 6.2551544e-35
 2.5843534e-28 1.0000000e+00], sampled 0.3855159078416599
[2019-04-01 16:23:06,256] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.4696403], dtype=float32), -0.3940882]
[2019-04-01 16:23:06,257] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.718374461333333, 55.59295916666667, 34.94192585, 710.2738761666667, 26.0, 26.47668042753149, 13.86122992677599, 1.0, 1.0, 5.478626375118146]
[2019-04-01 16:23:06,257] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 16:23:06,257] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.3909974e-31 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6547175e-25
 1.4065072e-09 1.0000000e+00], sampled 0.5298036088422584
[2019-04-01 16:23:08,013] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.4696403], dtype=float32), -0.3940882]
[2019-04-01 16:23:08,013] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.924874085, 30.356905055, 234.0431851, 592.8112638, 26.0, 26.18625943977802, 13.53900582410684, 1.0, 1.0, 5.161395054894866]
[2019-04-01 16:23:08,013] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 16:23:08,014] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.5371302e-30 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.5119633e-23
 2.7222157e-04 9.9972779e-01], sampled 0.4604245659416867
[2019-04-01 16:23:08,667] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 16:23:27,405] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 16:23:31,412] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 16:23:32,436] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 700000, evaluation results [700000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 16:23:39,934] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2040457e-24 0.0000000e+00 1.5214197e-29 0.0000000e+00 7.6195165e-36
 4.3224546e-26 1.0000000e+00], sum to 1.0000
[2019-04-01 16:23:39,967] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4018
[2019-04-01 16:23:39,982] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.58609093833697, 6.204894776248914, 0.0, 1.0, 39.54788306605298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 864600.0000, 
sim time next is 865200.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.56268616290413, 6.188434915197597, 0.0, 1.0, 39.42944627666317], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 1.0, 0.794669451843447, 0.061884349151975965, 0.0, 1.0, 0.28163890197616553], 
reward next is 0.7184, 
noisyNet noise sample is [array([0.8263749], dtype=float32), 0.79869235]. 
=============================================
[2019-04-01 16:23:40,041] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44000, global step 702542: loss 0.3153
[2019-04-01 16:23:40,041] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44000, global step 702542: learning rate 0.0010
[2019-04-01 16:23:40,931] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44000, global step 702907: loss 0.7211
[2019-04-01 16:23:40,932] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44000, global step 702907: learning rate 0.0010
[2019-04-01 16:23:41,340] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44000, global step 703110: loss 0.3740
[2019-04-01 16:23:41,341] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44000, global step 703110: learning rate 0.0010
[2019-04-01 16:23:42,391] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44000, global step 703612: loss 0.2962
[2019-04-01 16:23:42,391] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44000, global step 703612: learning rate 0.0010
[2019-04-01 16:23:42,473] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2981456e-28 3.4096574e-38 3.4540896e-23 0.0000000e+00 4.3818616e-31
 4.1673026e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:23:42,475] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1020
[2019-04-01 16:23:42,488] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7, 72.66666666666666, 0.0, 0.0, 26.0, 24.48898525589966, 5.71645868260602, 0.0, 1.0, 38.62001715708099], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 881400.0000, 
sim time next is 882000.0000, 
raw observation next is [-0.6, 72.0, 0.0, 0.0, 26.0, 24.4835826638712, 5.776368208939242, 0.0, 1.0, 38.58810548215821], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 0.72, 0.0, 0.0, 1.0, 0.7833689519816, 0.057763682089392414, 0.0, 1.0, 0.27562932487255865], 
reward next is 0.7244, 
noisyNet noise sample is [array([0.00216809], dtype=float32), 0.49213323]. 
=============================================
[2019-04-01 16:23:42,494] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.71502 ]
 [77.706566]
 [77.679985]
 [77.64641 ]
 [77.610466]], R is [[77.66459656]
 [77.61209869]
 [77.56016541]
 [77.50868988]
 [77.457901  ]].
[2019-04-01 16:23:42,542] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44000, global step 703679: loss 0.3663
[2019-04-01 16:23:42,543] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44000, global step 703680: learning rate 0.0010
[2019-04-01 16:23:42,981] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44000, global step 703889: loss 0.1900
[2019-04-01 16:23:42,981] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44000, global step 703889: learning rate 0.0010
[2019-04-01 16:23:43,023] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44000, global step 703906: loss 0.1521
[2019-04-01 16:23:43,023] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44000, global step 703906: learning rate 0.0010
[2019-04-01 16:23:43,140] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44000, global step 703958: loss 0.1365
[2019-04-01 16:23:43,146] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44000, global step 703958: learning rate 0.0010
[2019-04-01 16:23:43,286] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44000, global step 704010: loss 0.1106
[2019-04-01 16:23:43,288] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44000, global step 704011: learning rate 0.0010
[2019-04-01 16:23:43,689] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44000, global step 704162: loss 0.1132
[2019-04-01 16:23:43,693] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44000, global step 704164: learning rate 0.0010
[2019-04-01 16:23:43,702] A3C_AGENT_WORKER-Thread-9 INFO:Local step 44000, global step 704167: loss 0.0860
[2019-04-01 16:23:43,703] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 44000, global step 704167: learning rate 0.0010
[2019-04-01 16:23:44,216] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44000, global step 704386: loss 0.1072
[2019-04-01 16:23:44,218] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44000, global step 704386: loss 0.2403
[2019-04-01 16:23:44,219] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44000, global step 704387: learning rate 0.0010
[2019-04-01 16:23:44,219] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44000, global step 704386: learning rate 0.0010
[2019-04-01 16:23:44,258] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44000, global step 704401: loss 0.1087
[2019-04-01 16:23:44,258] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44000, global step 704401: learning rate 0.0010
[2019-04-01 16:23:44,688] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44000, global step 704594: loss 0.2600
[2019-04-01 16:23:44,689] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44000, global step 704594: learning rate 0.0010
[2019-04-01 16:23:44,929] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44000, global step 704699: loss 0.2477
[2019-04-01 16:23:44,930] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44000, global step 704699: learning rate 0.0010
[2019-04-01 16:23:46,431] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3721216e-30 0.0000000e+00 5.0646897e-35 0.0000000e+00 1.5805098e-32
 8.2338138e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 16:23:46,433] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0539
[2019-04-01 16:23:46,471] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.7, 100.0, 0.0, 0.0, 26.0, 25.24603693013207, 8.640866906867624, 0.0, 1.0, 23.72924776125898], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 934200.0000, 
sim time next is 934800.0000, 
raw observation next is [4.8, 100.0, 0.0, 0.0, 26.0, 25.15641690132223, 8.358079671270131, 0.0, 1.0, 22.43467608445161], 
processed observation next is [1.0, 0.8260869565217391, 0.5955678670360112, 1.0, 0.0, 0.0, 1.0, 0.8794881287603188, 0.08358079671270131, 0.0, 1.0, 0.1602476863175115], 
reward next is 0.8398, 
noisyNet noise sample is [array([1.4325368], dtype=float32), -1.1040909]. 
=============================================
[2019-04-01 16:23:47,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4398613e-31 0.0000000e+00 1.6050822e-30 0.0000000e+00 4.4381174e-38
 2.8144021e-34 1.0000000e+00], sum to 1.0000
[2019-04-01 16:23:47,043] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2509
[2019-04-01 16:23:47,051] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 26.00211501145331, 14.19497801284579, 0.0, 1.0, 18.26087185078256], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1038600.0000, 
sim time next is 1039200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 26.12973572073928, 14.1441625084142, 0.0, 1.0, 17.67412759961321], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 1.0, 1.0185336743913258, 0.141441625084142, 0.0, 1.0, 0.1262437685686658], 
reward next is 0.8738, 
noisyNet noise sample is [array([-1.7052819], dtype=float32), -1.6395942]. 
=============================================
[2019-04-01 16:23:47,658] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8494960e-24 1.0983907e-36 2.4196877e-26 0.0000000e+00 4.6612800e-25
 1.0374986e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 16:23:47,661] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6003
[2019-04-01 16:23:47,694] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.8, 83.0, 0.0, 0.0, 26.0, 25.41522007992589, 9.139808473974098, 0.0, 1.0, 30.11502859714143], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 975000.0000, 
sim time next is 975600.0000, 
raw observation next is [10.0, 83.0, 0.0, 0.0, 26.0, 25.40330344893153, 9.304191431633457, 0.0, 1.0, 35.81901891741513], 
processed observation next is [1.0, 0.30434782608695654, 0.739612188365651, 0.83, 0.0, 0.0, 1.0, 0.9147576355616474, 0.09304191431633457, 0.0, 1.0, 0.25585013512439375], 
reward next is 0.7441, 
noisyNet noise sample is [array([-0.6377477], dtype=float32), -0.76151174]. 
=============================================
[2019-04-01 16:23:50,793] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0287764e-23 4.9550554e-34 5.3416562e-25 1.3496675e-37 7.2572291e-08
 1.8373993e-14 9.9999988e-01], sum to 1.0000
[2019-04-01 16:23:50,798] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3074
[2019-04-01 16:23:50,806] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.68333333333333, 69.16666666666667, 207.3333333333333, 143.3333333333333, 26.0, 27.17956363199521, 22.52277849154467, 1.0, 1.0, 4.885063193911367], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1077000.0000, 
sim time next is 1077600.0000, 
raw observation next is [15.86666666666667, 68.33333333333334, 230.6666666666667, 179.1666666666667, 26.0, 27.26178843565102, 23.69034280898824, 1.0, 1.0, 4.509058135351736], 
processed observation next is [1.0, 0.4782608695652174, 0.9021237303785783, 0.6833333333333335, 0.7688888888888891, 0.19797421731123394, 1.0, 1.1802554908072886, 0.2369034280898824, 1.0, 1.0, 0.032207558109655254], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.02288816], dtype=float32), 0.23409538]. 
=============================================
[2019-04-01 16:23:52,117] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44500, global step 709266: loss 7.6091
[2019-04-01 16:23:52,119] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44500, global step 709269: learning rate 0.0010
[2019-04-01 16:23:53,213] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44500, global step 709989: loss 9.5575
[2019-04-01 16:23:53,217] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44500, global step 709992: learning rate 0.0010
[2019-04-01 16:23:53,442] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.7219091e-34 0.0000000e+00 2.3831823e-34 0.0000000e+00 7.0991129e-35
 2.0692881e-35 1.0000000e+00], sum to 1.0000
[2019-04-01 16:23:53,446] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2004
[2019-04-01 16:23:53,456] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.6, 77.0, 0.0, 0.0, 26.0, 25.66184389887181, 13.28379167064145, 0.0, 1.0, 23.11763057066659], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1141200.0000, 
sim time next is 1141800.0000, 
raw observation next is [11.6, 78.0, 0.0, 0.0, 26.0, 25.67043264614163, 13.35373915164219, 0.0, 1.0, 23.0979962162546], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.78, 0.0, 0.0, 1.0, 0.9529189494488044, 0.13353739151642188, 0.0, 1.0, 0.16498568725896143], 
reward next is 0.8350, 
noisyNet noise sample is [array([0.37442544], dtype=float32), 0.06477031]. 
=============================================
[2019-04-01 16:23:53,902] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4942595e-30 0.0000000e+00 4.2068927e-25 0.0000000e+00 1.3294339e-35
 1.4846431e-33 1.0000000e+00], sum to 1.0000
[2019-04-01 16:23:53,904] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6482
[2019-04-01 16:23:53,918] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.41666666666667, 72.0, 0.0, 0.0, 26.0, 25.64503551985477, 14.50606636591713, 0.0, 1.0, 27.05563456362996], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1123800.0000, 
sim time next is 1124400.0000, 
raw observation next is [11.23333333333333, 73.0, 0.0, 0.0, 26.0, 25.68635308414146, 14.60972448820991, 0.0, 1.0, 24.25624072528175], 
processed observation next is [0.0, 0.0, 0.7737765466297323, 0.73, 0.0, 0.0, 1.0, 0.9551932977344941, 0.1460972448820991, 0.0, 1.0, 0.17325886232344107], 
reward next is 0.8267, 
noisyNet noise sample is [array([1.5661974], dtype=float32), 2.9761903]. 
=============================================
[2019-04-01 16:23:53,929] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44500, global step 710462: loss 14.5257
[2019-04-01 16:23:53,933] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44500, global step 710462: learning rate 0.0010
[2019-04-01 16:23:55,049] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44500, global step 711222: loss 10.4064
[2019-04-01 16:23:55,051] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44500, global step 711223: learning rate 0.0010
[2019-04-01 16:23:55,411] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44500, global step 711477: loss 13.2801
[2019-04-01 16:23:55,412] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44500, global step 711477: learning rate 0.0010
[2019-04-01 16:23:56,062] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44500, global step 711881: loss 11.5790
[2019-04-01 16:23:56,063] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44500, global step 711882: learning rate 0.0010
[2019-04-01 16:23:56,126] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44500, global step 711917: loss 10.8245
[2019-04-01 16:23:56,128] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44500, global step 711919: learning rate 0.0010
[2019-04-01 16:23:56,269] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44500, global step 712000: loss 9.8387
[2019-04-01 16:23:56,271] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44500, global step 712000: learning rate 0.0010
[2019-04-01 16:23:56,590] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44500, global step 712201: loss 10.3941
[2019-04-01 16:23:56,592] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44500, global step 712201: learning rate 0.0010
[2019-04-01 16:23:56,949] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44500, global step 712445: loss 8.2108
[2019-04-01 16:23:56,951] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44500, global step 712447: learning rate 0.0010
[2019-04-01 16:23:57,325] A3C_AGENT_WORKER-Thread-9 INFO:Local step 44500, global step 712685: loss 6.9884
[2019-04-01 16:23:57,326] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 44500, global step 712685: learning rate 0.0010
[2019-04-01 16:23:57,476] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44500, global step 712788: loss 5.6428
[2019-04-01 16:23:57,478] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44500, global step 712789: learning rate 0.0010
[2019-04-01 16:23:57,512] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44500, global step 712813: loss 5.9067
[2019-04-01 16:23:57,513] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44500, global step 712813: learning rate 0.0010
[2019-04-01 16:23:57,635] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44500, global step 712886: loss 4.2825
[2019-04-01 16:23:57,637] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44500, global step 712886: learning rate 0.0010
[2019-04-01 16:23:58,023] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44500, global step 713154: loss 3.7254
[2019-04-01 16:23:58,024] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44500, global step 713155: learning rate 0.0010
[2019-04-01 16:23:58,059] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44500, global step 713177: loss 3.5988
[2019-04-01 16:23:58,062] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44500, global step 713178: learning rate 0.0010
[2019-04-01 16:24:00,252] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.7867528e-36 0.0000000e+00 3.6185487e-35 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:24:00,254] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2564
[2019-04-01 16:24:00,280] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.25, 98.0, 0.0, 0.0, 26.0, 24.56732436389722, 9.276818357201629, 0.0, 1.0, 22.42189635417521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1272600.0000, 
sim time next is 1273200.0000, 
raw observation next is [9.600000000000001, 97.33333333333334, 0.0, 0.0, 26.0, 24.57328765786059, 9.494307274115444, 0.0, 1.0, 24.6284972932173], 
processed observation next is [0.0, 0.7391304347826086, 0.7285318559556788, 0.9733333333333334, 0.0, 0.0, 1.0, 0.7961839511229414, 0.09494307274115445, 0.0, 1.0, 0.175917837808695], 
reward next is 0.8241, 
noisyNet noise sample is [array([-0.17148727], dtype=float32), 0.23350485]. 
=============================================
[2019-04-01 16:24:07,891] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2255450e-25 6.8867514e-38 1.4144866e-24 0.0000000e+00 1.2787381e-35
 2.5942222e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 16:24:07,891] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7155
[2019-04-01 16:24:07,904] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.29050370757653, 9.17396584513152, 0.0, 1.0, 36.42787132478483], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1486800.0000, 
sim time next is 1487400.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.26785954694419, 9.04649324613559, 0.0, 1.0, 36.51935740679494], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 1.0, 0.895408506706313, 0.0904649324613559, 0.0, 1.0, 0.26085255290567816], 
reward next is 0.7391, 
noisyNet noise sample is [array([1.1719706], dtype=float32), -1.3813081]. 
=============================================
[2019-04-01 16:24:08,377] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45000, global step 718408: loss 0.1560
[2019-04-01 16:24:08,378] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45000, global step 718408: learning rate 0.0010
[2019-04-01 16:24:09,258] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45000, global step 718800: loss 0.4629
[2019-04-01 16:24:09,262] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45000, global step 718801: learning rate 0.0010
[2019-04-01 16:24:09,493] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45000, global step 718929: loss 0.2503
[2019-04-01 16:24:09,494] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45000, global step 718929: learning rate 0.0010
[2019-04-01 16:24:09,563] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.2646881e-27 0.0000000e+00 9.9585185e-36 0.0000000e+00 1.1079287e-32
 3.1878551e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 16:24:09,563] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3650
[2019-04-01 16:24:09,576] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.433333333333333, 90.0, 0.0, 0.0, 26.0, 24.87679097795346, 10.29634538856063, 0.0, 1.0, 43.42303034086166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1456800.0000, 
sim time next is 1457400.0000, 
raw observation next is [1.516666666666667, 89.5, 0.0, 0.0, 26.0, 25.0495088419492, 10.82607137438986, 0.0, 1.0, 41.77581590188062], 
processed observation next is [1.0, 0.8695652173913043, 0.5046168051708219, 0.895, 0.0, 0.0, 1.0, 0.8642155488498859, 0.1082607137438986, 0.0, 1.0, 0.298398685013433], 
reward next is 0.7016, 
noisyNet noise sample is [array([0.2680466], dtype=float32), 1.4897836]. 
=============================================
[2019-04-01 16:24:10,602] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45000, global step 719585: loss 0.1840
[2019-04-01 16:24:10,603] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45000, global step 719585: learning rate 0.0010
[2019-04-01 16:24:10,868] A3C_AGENT_WORKER-Thread-11 INFO:Local step 45000, global step 719748: loss 0.4640
[2019-04-01 16:24:10,868] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 45000, global step 719748: learning rate 0.0010
[2019-04-01 16:24:11,621] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45000, global step 720171: loss 0.2020
[2019-04-01 16:24:11,623] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45000, global step 720174: learning rate 0.0010
[2019-04-01 16:24:11,636] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45000, global step 720180: loss 0.1635
[2019-04-01 16:24:11,639] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45000, global step 720180: learning rate 0.0010
[2019-04-01 16:24:11,709] A3C_AGENT_WORKER-Thread-13 INFO:Local step 45000, global step 720222: loss 0.1836
[2019-04-01 16:24:11,712] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 45000, global step 720223: learning rate 0.0010
[2019-04-01 16:24:12,026] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45000, global step 720405: loss 0.2811
[2019-04-01 16:24:12,027] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45000, global step 720405: learning rate 0.0010
[2019-04-01 16:24:12,055] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45000, global step 720421: loss 0.5554
[2019-04-01 16:24:12,058] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45000, global step 720422: learning rate 0.0010
[2019-04-01 16:24:12,219] A3C_AGENT_WORKER-Thread-9 INFO:Local step 45000, global step 720501: loss 0.2666
[2019-04-01 16:24:12,220] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 45000, global step 720501: learning rate 0.0010
[2019-04-01 16:24:12,487] A3C_AGENT_WORKER-Thread-12 INFO:Local step 45000, global step 720667: loss 0.2382
[2019-04-01 16:24:12,489] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 45000, global step 720668: learning rate 0.0010
[2019-04-01 16:24:12,752] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45000, global step 720856: loss 0.4365
[2019-04-01 16:24:12,755] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45000, global step 720857: learning rate 0.0010
[2019-04-01 16:24:12,767] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45000, global step 720868: loss 0.4860
[2019-04-01 16:24:12,767] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45000, global step 720868: learning rate 0.0010
[2019-04-01 16:24:12,977] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45000, global step 721010: loss 0.8259
[2019-04-01 16:24:12,978] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45000, global step 721011: learning rate 0.0010
[2019-04-01 16:24:13,205] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45000, global step 721158: loss 1.2229
[2019-04-01 16:24:13,208] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45000, global step 721158: learning rate 0.0010
[2019-04-01 16:24:21,471] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4869870e-14 2.9792885e-26 1.6457889e-20 5.2808129e-33 7.1879114e-15
 2.2658995e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 16:24:21,472] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6388
[2019-04-01 16:24:21,517] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.183333333333333, 86.83333333333334, 20.33333333333334, 0.0, 26.0, 25.5899424178434, 10.12449722479105, 1.0, 1.0, 12.60956135647447], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1702200.0000, 
sim time next is 1702800.0000, 
raw observation next is [1.1, 88.0, 15.5, 0.0, 26.0, 24.89963159189466, 9.393554998259605, 1.0, 1.0, 45.08273408217272], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.051666666666666666, 0.0, 1.0, 0.8428045131278087, 0.09393554998259605, 1.0, 1.0, 0.32201952915837656], 
reward next is 0.6780, 
noisyNet noise sample is [array([-1.8706356], dtype=float32), -0.2554845]. 
=============================================
[2019-04-01 16:24:21,813] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1277888e-29 1.1748143e-37 3.2258722e-23 0.0000000e+00 3.3847282e-28
 6.2611560e-32 1.0000000e+00], sum to 1.0000
[2019-04-01 16:24:21,813] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9060
[2019-04-01 16:24:21,834] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 87.0, 0.0, 0.0, 26.0, 24.65352930171908, 7.10492370676193, 0.0, 1.0, 43.47978136029199], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1753800.0000, 
sim time next is 1754400.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 26.0, 24.63407029612841, 7.036013866298544, 0.0, 1.0, 43.50209758495027], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 1.0, 0.8048671851612015, 0.07036013866298543, 0.0, 1.0, 0.3107292684639305], 
reward next is 0.6893, 
noisyNet noise sample is [array([0.24735238], dtype=float32), -1.1351061]. 
=============================================
[2019-04-01 16:24:24,553] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45500, global step 726981: loss 0.1755
[2019-04-01 16:24:24,554] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45500, global step 726981: learning rate 0.0010
[2019-04-01 16:24:24,922] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45500, global step 727095: loss 0.0309
[2019-04-01 16:24:24,923] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45500, global step 727095: learning rate 0.0010
[2019-04-01 16:24:25,341] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45500, global step 727225: loss 0.0102
[2019-04-01 16:24:25,342] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45500, global step 727225: learning rate 0.0010
[2019-04-01 16:24:27,090] A3C_AGENT_WORKER-Thread-11 INFO:Local step 45500, global step 727752: loss 0.0011
[2019-04-01 16:24:27,091] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 45500, global step 727752: learning rate 0.0010
[2019-04-01 16:24:27,159] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45500, global step 727772: loss 0.0003
[2019-04-01 16:24:27,162] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45500, global step 727772: learning rate 0.0010
[2019-04-01 16:24:27,877] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45500, global step 728017: loss 0.0005
[2019-04-01 16:24:27,878] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45500, global step 728017: learning rate 0.0010
[2019-04-01 16:24:27,888] A3C_AGENT_WORKER-Thread-13 INFO:Local step 45500, global step 728024: loss 0.0053
[2019-04-01 16:24:27,890] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 45500, global step 728024: learning rate 0.0010
[2019-04-01 16:24:27,991] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7657298e-25 1.0838926e-30 1.6152498e-20 5.2650603e-34 5.3358918e-33
 3.4027398e-26 1.0000000e+00], sum to 1.0000
[2019-04-01 16:24:27,991] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5746
[2019-04-01 16:24:28,005] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 81.66666666666667, 0.0, 0.0, 26.0, 23.77858844280366, 5.253060125409867, 0.0, 1.0, 46.27752518010517], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1830000.0000, 
sim time next is 1830600.0000, 
raw observation next is [-6.2, 81.0, 0.0, 0.0, 26.0, 23.74170546858648, 5.245828021731467, 0.0, 1.0, 46.31066516322054], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.81, 0.0, 0.0, 1.0, 0.6773864955123544, 0.05245828021731467, 0.0, 1.0, 0.3307904654515753], 
reward next is 0.6692, 
noisyNet noise sample is [array([0.70021665], dtype=float32), 0.6488619]. 
=============================================
[2019-04-01 16:24:28,211] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45500, global step 728164: loss 0.0228
[2019-04-01 16:24:28,212] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45500, global step 728164: learning rate 0.0010
[2019-04-01 16:24:28,325] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45500, global step 728215: loss 0.0381
[2019-04-01 16:24:28,329] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45500, global step 728216: learning rate 0.0010
[2019-04-01 16:24:28,428] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45500, global step 728266: loss 0.0137
[2019-04-01 16:24:28,429] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45500, global step 728266: learning rate 0.0010
[2019-04-01 16:24:28,522] A3C_AGENT_WORKER-Thread-9 INFO:Local step 45500, global step 728314: loss 0.0144
[2019-04-01 16:24:28,525] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 45500, global step 728316: learning rate 0.0010
[2019-04-01 16:24:28,928] A3C_AGENT_WORKER-Thread-12 INFO:Local step 45500, global step 728519: loss 0.0376
[2019-04-01 16:24:28,930] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 45500, global step 728520: learning rate 0.0010
[2019-04-01 16:24:28,931] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45500, global step 728520: loss 0.0003
[2019-04-01 16:24:28,932] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45500, global step 728520: learning rate 0.0010
[2019-04-01 16:24:29,047] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45500, global step 728582: loss 0.0003
[2019-04-01 16:24:29,049] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45500, global step 728582: learning rate 0.0010
[2019-04-01 16:24:29,105] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45500, global step 728613: loss 0.0005
[2019-04-01 16:24:29,106] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45500, global step 728613: learning rate 0.0010
[2019-04-01 16:24:29,107] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45500, global step 728613: loss 0.0016
[2019-04-01 16:24:29,107] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45500, global step 728613: learning rate 0.0010
[2019-04-01 16:24:35,816] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.04247714e-29 3.74034552e-33 7.80976517e-21 3.70395216e-34
 7.79172650e-30 1.75237915e-22 1.00000000e+00], sum to 1.0000
[2019-04-01 16:24:35,817] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8573
[2019-04-01 16:24:35,832] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 77.33333333333334, 0.0, 0.0, 26.0, 24.08974954021519, 5.402839673213802, 0.0, 1.0, 44.52181683992236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1903200.0000, 
sim time next is 1903800.0000, 
raw observation next is [-7.299999999999999, 76.16666666666666, 0.0, 0.0, 26.0, 24.07387204848369, 5.388232680772198, 0.0, 1.0, 44.55537249992177], 
processed observation next is [1.0, 0.0, 0.2603878116343491, 0.7616666666666666, 0.0, 0.0, 1.0, 0.7248388640690985, 0.05388232680772198, 0.0, 1.0, 0.31825266071372693], 
reward next is 0.6817, 
noisyNet noise sample is [array([-1.1239377], dtype=float32), -1.2814145]. 
=============================================
[2019-04-01 16:24:40,836] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3358606e-26 0.0000000e+00 1.0837886e-31 0.0000000e+00 2.9108031e-28
 5.7761765e-23 1.0000000e+00], sum to 1.0000
[2019-04-01 16:24:40,836] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6533
[2019-04-01 16:24:40,908] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.583333333333333, 72.33333333333334, 0.0, 0.0, 26.0, 25.28042724808866, 7.959690098377069, 1.0, 1.0, 29.97338678328023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1968600.0000, 
sim time next is 1969200.0000, 
raw observation next is [-4.5, 71.0, 0.0, 0.0, 26.0, 25.22482901839293, 7.661023433654847, 1.0, 1.0, 31.24885658441154], 
processed observation next is [1.0, 0.8260869565217391, 0.3379501385041552, 0.71, 0.0, 0.0, 1.0, 0.8892612883418474, 0.07661023433654847, 1.0, 1.0, 0.22320611846008243], 
reward next is 0.7768, 
noisyNet noise sample is [array([0.7161062], dtype=float32), 1.5269724]. 
=============================================
[2019-04-01 16:24:41,339] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0159229e-21 5.6953580e-36 1.4282118e-21 1.7451779e-33 1.4530157e-24
 2.8889784e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:24:41,343] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6888
[2019-04-01 16:24:41,429] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.37222364046249, 5.821513751990442, 0.0, 1.0, 41.2051471045114], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1992000.0000, 
sim time next is 1992600.0000, 
raw observation next is [-5.9, 85.0, 0.0, 0.0, 26.0, 24.36590400125533, 5.753283514003845, 0.0, 1.0, 41.19015694021714], 
processed observation next is [1.0, 0.043478260869565216, 0.2991689750692521, 0.85, 0.0, 0.0, 1.0, 0.7665577144650472, 0.05753283514003846, 0.0, 1.0, 0.29421540671583674], 
reward next is 0.7058, 
noisyNet noise sample is [array([0.32001558], dtype=float32), -1.0059893]. 
=============================================
[2019-04-01 16:24:47,437] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46000, global step 734791: loss 2.1546
[2019-04-01 16:24:47,439] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46000, global step 734791: learning rate 0.0010
[2019-04-01 16:24:47,844] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46000, global step 734943: loss 1.9562
[2019-04-01 16:24:47,844] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46000, global step 734943: learning rate 0.0010
[2019-04-01 16:24:48,752] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46000, global step 735310: loss 2.5508
[2019-04-01 16:24:48,752] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46000, global step 735310: learning rate 0.0010
[2019-04-01 16:24:49,716] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46000, global step 735741: loss 3.0477
[2019-04-01 16:24:49,717] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46000, global step 735741: learning rate 0.0010
[2019-04-01 16:24:50,154] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46000, global step 735905: loss 1.6752
[2019-04-01 16:24:50,161] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46000, global step 735910: learning rate 0.0010
[2019-04-01 16:24:50,287] A3C_AGENT_WORKER-Thread-11 INFO:Local step 46000, global step 735953: loss 3.2302
[2019-04-01 16:24:50,287] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 46000, global step 735953: learning rate 0.0010
[2019-04-01 16:24:50,422] A3C_AGENT_WORKER-Thread-13 INFO:Local step 46000, global step 735997: loss 1.2809
[2019-04-01 16:24:50,422] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 46000, global step 735997: learning rate 0.0010
[2019-04-01 16:24:50,690] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46000, global step 736076: loss 2.0050
[2019-04-01 16:24:50,690] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46000, global step 736076: learning rate 0.0010
[2019-04-01 16:24:50,996] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46000, global step 736156: loss 2.6218
[2019-04-01 16:24:50,998] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46000, global step 736156: learning rate 0.0010
[2019-04-01 16:24:51,082] A3C_AGENT_WORKER-Thread-9 INFO:Local step 46000, global step 736184: loss 3.6369
[2019-04-01 16:24:51,096] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 46000, global step 736184: learning rate 0.0010
[2019-04-01 16:24:51,329] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46000, global step 736258: loss 4.6007
[2019-04-01 16:24:51,329] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46000, global step 736258: learning rate 0.0010
[2019-04-01 16:24:51,612] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46000, global step 736351: loss 2.7475
[2019-04-01 16:24:51,612] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46000, global step 736351: learning rate 0.0010
[2019-04-01 16:24:51,643] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46000, global step 736362: loss 2.5411
[2019-04-01 16:24:51,643] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46000, global step 736362: learning rate 0.0010
[2019-04-01 16:24:51,948] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46000, global step 736467: loss 4.8215
[2019-04-01 16:24:51,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46000, global step 736467: learning rate 0.0010
[2019-04-01 16:24:52,031] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46000, global step 736495: loss 5.4362
[2019-04-01 16:24:52,033] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46000, global step 736495: learning rate 0.0010
[2019-04-01 16:24:52,079] A3C_AGENT_WORKER-Thread-12 INFO:Local step 46000, global step 736511: loss 5.6542
[2019-04-01 16:24:52,080] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 46000, global step 736511: learning rate 0.0010
[2019-04-01 16:24:55,241] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7725477e-23 9.2369749e-35 1.5824173e-23 1.4831585e-35 1.9060762e-29
 2.1571818e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:24:55,242] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5757
[2019-04-01 16:24:55,257] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.44748305398537, 5.766991496813868, 0.0, 1.0, 41.9810736062753], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2169000.0000, 
sim time next is 2169600.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.39505537652474, 5.792396797327283, 0.0, 1.0, 41.96622599628738], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 1.0, 0.7707221966463916, 0.05792396797327284, 0.0, 1.0, 0.2997587571163384], 
reward next is 0.7002, 
noisyNet noise sample is [array([0.3529271], dtype=float32), -1.2322035]. 
=============================================
[2019-04-01 16:25:00,157] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6143565e-17 6.6663093e-30 5.0347198e-23 1.2604646e-33 6.2228103e-17
 7.9095399e-04 9.9920911e-01], sum to 1.0000
[2019-04-01 16:25:00,157] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6044
[2019-04-01 16:25:00,212] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 70.5, 13.66666666666666, 0.0, 26.0, 25.19314644543888, 8.1258143118166, 1.0, 1.0, 40.31436498569866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2221800.0000, 
sim time next is 2222400.0000, 
raw observation next is [-4.5, 70.0, 8.333333333333332, 0.0, 26.0, 25.41480168315728, 8.070861604689183, 1.0, 1.0, 33.38367652917241], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.7, 0.027777777777777773, 0.0, 1.0, 0.9164002404510398, 0.08070861604689183, 1.0, 1.0, 0.23845483235123152], 
reward next is 0.7615, 
noisyNet noise sample is [array([1.1246637], dtype=float32), 0.8561338]. 
=============================================
[2019-04-01 16:25:02,068] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3198526e-23 9.1231042e-33 3.4644467e-24 5.8639861e-37 2.9493332e-19
 8.5393797e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:02,070] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3091
[2019-04-01 16:25:02,098] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 68.0, 0.0, 0.0, 26.0, 25.26107984745403, 9.163062244473652, 0.0, 1.0, 45.04701011598927], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2235600.0000, 
sim time next is 2236200.0000, 
raw observation next is [-5.100000000000001, 68.5, 0.0, 0.0, 26.0, 25.28538541274789, 9.09142614982139, 0.0, 1.0, 44.86996209823576], 
processed observation next is [1.0, 0.9130434782608695, 0.32132963988919666, 0.685, 0.0, 0.0, 1.0, 0.897912201821127, 0.0909142614982139, 0.0, 1.0, 0.3204997292731126], 
reward next is 0.6795, 
noisyNet noise sample is [array([-0.28264934], dtype=float32), 0.14353205]. 
=============================================
[2019-04-01 16:25:04,616] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.7200601e-21 0.0000000e+00 3.0671133e-31 4.3081657e-38 1.7857131e-21
 9.8383409e-01 1.6165998e-02], sum to 1.0000
[2019-04-01 16:25:04,617] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2538
[2019-04-01 16:25:04,642] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.95, 56.83333333333334, 228.3333333333333, 86.0, 26.0, 25.61249563697685, 8.022346123299386, 1.0, 1.0, 21.86748499057692], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2290200.0000, 
sim time next is 2290800.0000, 
raw observation next is [-2.7, 55.66666666666667, 245.1666666666667, 80.0, 26.0, 25.60009058551955, 8.169475573749537, 1.0, 1.0, 24.43194852039445], 
processed observation next is [1.0, 0.5217391304347826, 0.38781163434903054, 0.5566666666666668, 0.8172222222222224, 0.08839779005524862, 1.0, 0.9428700836456498, 0.08169475573749536, 1.0, 1.0, 0.1745139180028175], 
reward next is 0.8255, 
noisyNet noise sample is [array([0.28335586], dtype=float32), 2.1152508]. 
=============================================
[2019-04-01 16:25:08,350] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1334254e-22 0.0000000e+00 2.4272075e-23 6.8142600e-38 1.6620348e-35
 9.3456154e-26 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:08,351] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9507
[2019-04-01 16:25:08,362] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 61.0, 0.0, 0.0, 26.0, 25.05165025082536, 7.559423431303412, 0.0, 1.0, 38.10344253449422], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2330400.0000, 
sim time next is 2331000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.99899025442432, 7.456002584905046, 0.0, 1.0, 38.10034724206442], 
processed observation next is [1.0, 1.0, 0.3988919667590028, 0.62, 0.0, 0.0, 1.0, 0.856998607774903, 0.07456002584905046, 0.0, 1.0, 0.2721453374433173], 
reward next is 0.7279, 
noisyNet noise sample is [array([0.42592308], dtype=float32), -0.742152]. 
=============================================
[2019-04-01 16:25:08,365] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[78.09795]
 [78.04452]
 [77.54007]
 [77.4368 ]
 [77.20197]], R is [[78.31027222]
 [78.25499725]
 [78.19933319]
 [78.14459229]
 [78.09015656]].
[2019-04-01 16:25:09,323] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1009564e-24 9.7811892e-36 8.8529773e-26 6.8122918e-36 1.5823545e-26
 3.4927059e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:09,325] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9021
[2019-04-01 16:25:09,342] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 66.33333333333333, 0.0, 0.0, 26.0, 24.20751965535261, 5.540308869313058, 0.0, 1.0, 40.62329285254349], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2353200.0000, 
sim time next is 2353800.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.18487788453443, 5.515006003277169, 0.0, 1.0, 40.65087724710835], 
processed observation next is [0.0, 0.21739130434782608, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 1.0, 0.7406968406477757, 0.055150060032771696, 0.0, 1.0, 0.2903634089079168], 
reward next is 0.7096, 
noisyNet noise sample is [array([-1.2352744], dtype=float32), 0.40955323]. 
=============================================
[2019-04-01 16:25:09,428] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46500, global step 742883: loss 0.0576
[2019-04-01 16:25:09,432] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46500, global step 742883: learning rate 0.0010
[2019-04-01 16:25:10,219] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46500, global step 743146: loss 0.0800
[2019-04-01 16:25:10,220] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46500, global step 743146: learning rate 0.0010
[2019-04-01 16:25:11,086] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46500, global step 743429: loss 0.1451
[2019-04-01 16:25:11,086] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46500, global step 743429: learning rate 0.0010
[2019-04-01 16:25:11,727] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2011917e-20 3.1586491e-34 8.0991532e-21 2.6064112e-36 1.9472918e-29
 4.1438871e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:11,727] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5692
[2019-04-01 16:25:11,774] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 48.66666666666667, 147.6666666666667, 56.83333333333332, 26.0, 24.95432323565761, 7.535500356720107, 0.0, 1.0, 29.52543719100468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2385600.0000, 
sim time next is 2386200.0000, 
raw observation next is [0.0, 47.83333333333334, 135.3333333333333, 113.6666666666666, 26.0, 24.93932241336984, 7.570184524073123, 0.0, 1.0, 31.23178041946205], 
processed observation next is [0.0, 0.6086956521739131, 0.46260387811634357, 0.47833333333333344, 0.45111111111111096, 0.125598526703499, 1.0, 0.8484746304814058, 0.07570184524073123, 0.0, 1.0, 0.22308414585330036], 
reward next is 0.7769, 
noisyNet noise sample is [array([0.0515977], dtype=float32), -0.6896073]. 
=============================================
[2019-04-01 16:25:12,248] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46500, global step 743808: loss 0.0052
[2019-04-01 16:25:12,249] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46500, global step 743808: learning rate 0.0010
[2019-04-01 16:25:12,319] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46500, global step 743834: loss 0.0161
[2019-04-01 16:25:12,321] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46500, global step 743834: learning rate 0.0010
[2019-04-01 16:25:12,498] A3C_AGENT_WORKER-Thread-11 INFO:Local step 46500, global step 743901: loss 0.0338
[2019-04-01 16:25:12,498] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 46500, global step 743901: learning rate 0.0010
[2019-04-01 16:25:13,075] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46500, global step 744125: loss 0.0038
[2019-04-01 16:25:13,075] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46500, global step 744125: learning rate 0.0010
[2019-04-01 16:25:13,103] A3C_AGENT_WORKER-Thread-13 INFO:Local step 46500, global step 744136: loss 0.0061
[2019-04-01 16:25:13,104] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 46500, global step 744136: learning rate 0.0010
[2019-04-01 16:25:13,118] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46500, global step 744141: loss 0.0245
[2019-04-01 16:25:13,118] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46500, global step 744141: learning rate 0.0010
[2019-04-01 16:25:13,210] A3C_AGENT_WORKER-Thread-9 INFO:Local step 46500, global step 744179: loss 0.0864
[2019-04-01 16:25:13,211] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 46500, global step 744181: learning rate 0.0010
[2019-04-01 16:25:13,773] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46500, global step 744445: loss 0.0614
[2019-04-01 16:25:13,774] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46500, global step 744445: learning rate 0.0010
[2019-04-01 16:25:13,998] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46500, global step 744556: loss 0.0145
[2019-04-01 16:25:13,998] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46500, global step 744556: learning rate 0.0010
[2019-04-01 16:25:14,035] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46500, global step 744576: loss 0.0200
[2019-04-01 16:25:14,040] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46500, global step 744579: learning rate 0.0010
[2019-04-01 16:25:14,349] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46500, global step 744735: loss 0.0307
[2019-04-01 16:25:14,351] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46500, global step 744736: learning rate 0.0010
[2019-04-01 16:25:14,356] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46500, global step 744741: loss 0.0820
[2019-04-01 16:25:14,357] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46500, global step 744741: learning rate 0.0010
[2019-04-01 16:25:14,450] A3C_AGENT_WORKER-Thread-12 INFO:Local step 46500, global step 744788: loss 0.1209
[2019-04-01 16:25:14,451] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 46500, global step 744788: learning rate 0.0010
[2019-04-01 16:25:18,551] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2298415e-17 7.6308256e-30 1.4479691e-17 1.9699903e-29 9.6723111e-24
 7.3051249e-09 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:18,559] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1662
[2019-04-01 16:25:18,570] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.90523848381428, 5.821695539600441, 0.0, 1.0, 38.17605788733135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2516400.0000, 
sim time next is 2517000.0000, 
raw observation next is [-1.7, 44.83333333333334, 0.0, 0.0, 26.0, 24.90173869950313, 5.869166925382373, 0.0, 1.0, 38.1205157719671], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4483333333333334, 0.0, 0.0, 1.0, 0.8431055285004473, 0.058691669253823735, 0.0, 1.0, 0.2722893983711936], 
reward next is 0.7277, 
noisyNet noise sample is [array([-0.9109558], dtype=float32), 0.7768147]. 
=============================================
[2019-04-01 16:25:18,575] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[68.349205]
 [68.70067 ]
 [68.97043 ]
 [69.220955]
 [69.27444 ]], R is [[68.08314514]
 [68.12962341]
 [68.17546844]
 [68.2204895 ]
 [68.26457214]].
[2019-04-01 16:25:20,711] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.7009231e-21 0.0000000e+00 1.9154740e-34 0.0000000e+00 4.1031515e-21
 9.9754250e-01 2.4575773e-03], sum to 1.0000
[2019-04-01 16:25:20,713] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4795
[2019-04-01 16:25:20,735] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.633333333333334, 27.0, 161.0, 309.5, 26.0, 25.83978189659118, 8.627871850246635, 1.0, 1.0, 5.740327517954559], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2557200.0000, 
sim time next is 2557800.0000, 
raw observation next is [3.55, 27.5, 159.0, 275.0, 26.0, 25.89478374065758, 8.64574402000732, 1.0, 1.0, 5.87928426549574], 
processed observation next is [1.0, 0.6086956521739131, 0.5609418282548477, 0.275, 0.53, 0.30386740331491713, 1.0, 0.9849691058082257, 0.0864574402000732, 1.0, 1.0, 0.04199488761068386], 
reward next is 0.9580, 
noisyNet noise sample is [array([0.0014625], dtype=float32), 0.010510789]. 
=============================================
[2019-04-01 16:25:21,776] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1452619e-19 0.0000000e+00 1.2503048e-25 0.0000000e+00 9.1250114e-27
 4.6653377e-13 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:21,778] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0199
[2019-04-01 16:25:21,821] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9666666666666667, 38.66666666666667, 0.0, 0.0, 26.0, 25.02043487218051, 7.824534305853757, 0.0, 1.0, 35.27713076339673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2575200.0000, 
sim time next is 2575800.0000, 
raw observation next is [-1.15, 40.0, 0.0, 0.0, 26.0, 25.02140125347888, 7.792779319741835, 0.0, 1.0, 31.36765763650358], 
processed observation next is [1.0, 0.8260869565217391, 0.4307479224376732, 0.4, 0.0, 0.0, 1.0, 0.8602001790684116, 0.07792779319741835, 0.0, 1.0, 0.224054697403597], 
reward next is 0.7759, 
noisyNet noise sample is [array([0.50284404], dtype=float32), -1.2207167]. 
=============================================
[2019-04-01 16:25:23,457] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.6042758e-15 2.2236779e-25 4.7546797e-12 1.1044629e-27 2.8312514e-21
 1.9179274e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:23,457] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7333
[2019-04-01 16:25:23,488] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.40106989275523, 9.207490491897547, 0.0, 1.0, 40.84437765790851], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2583000.0000, 
sim time next is 2583600.0000, 
raw observation next is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.42345689827154, 9.157744108459733, 0.0, 1.0, 39.84404504547452], 
processed observation next is [1.0, 0.9130434782608695, 0.38504155124653744, 0.56, 0.0, 0.0, 1.0, 0.9176366997530769, 0.09157744108459732, 0.0, 1.0, 0.28460032175338945], 
reward next is 0.7154, 
noisyNet noise sample is [array([0.77691716], dtype=float32), -0.41975686]. 
=============================================
[2019-04-01 16:25:27,348] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47000, global step 750637: loss 8.6608
[2019-04-01 16:25:27,365] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47000, global step 750637: learning rate 0.0010
[2019-04-01 16:25:27,775] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47000, global step 750795: loss 10.1381
[2019-04-01 16:25:27,776] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47000, global step 750795: learning rate 0.0010
[2019-04-01 16:25:28,799] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47000, global step 751293: loss 9.1998
[2019-04-01 16:25:28,801] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47000, global step 751294: learning rate 0.0010
[2019-04-01 16:25:29,729] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47000, global step 751708: loss 11.3311
[2019-04-01 16:25:29,730] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47000, global step 751709: learning rate 0.0010
[2019-04-01 16:25:30,063] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47000, global step 751848: loss 7.3547
[2019-04-01 16:25:30,064] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47000, global step 751848: learning rate 0.0010
[2019-04-01 16:25:30,224] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47000, global step 751910: loss 12.9078
[2019-04-01 16:25:30,231] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47000, global step 751910: learning rate 0.0010
[2019-04-01 16:25:30,349] A3C_AGENT_WORKER-Thread-13 INFO:Local step 47000, global step 751957: loss 9.1550
[2019-04-01 16:25:30,351] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 47000, global step 751958: learning rate 0.0010
[2019-04-01 16:25:30,461] A3C_AGENT_WORKER-Thread-11 INFO:Local step 47000, global step 752001: loss 10.7870
[2019-04-01 16:25:30,462] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 47000, global step 752001: learning rate 0.0010
[2019-04-01 16:25:30,960] A3C_AGENT_WORKER-Thread-9 INFO:Local step 47000, global step 752181: loss 21.7047
[2019-04-01 16:25:30,962] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 47000, global step 752182: learning rate 0.0010
[2019-04-01 16:25:30,967] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47000, global step 752185: loss 17.9826
[2019-04-01 16:25:30,970] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47000, global step 752185: learning rate 0.0010
[2019-04-01 16:25:31,441] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47000, global step 752310: loss 17.0665
[2019-04-01 16:25:31,442] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47000, global step 752310: learning rate 0.0010
[2019-04-01 16:25:31,519] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9186206e-19 1.2064932e-29 1.8429350e-21 1.3897070e-32 1.7216617e-21
 1.9553802e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:31,519] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9715
[2019-04-01 16:25:31,554] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.83730143990424, 6.710576709437923, 0.0, 1.0, 41.56757866173229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2769600.0000, 
sim time next is 2770200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.83451829510985, 6.674287932132525, 0.0, 1.0, 41.44838309748309], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 1.0, 0.8335026135871212, 0.06674287932132525, 0.0, 1.0, 0.29605987926773636], 
reward next is 0.7039, 
noisyNet noise sample is [array([1.264541], dtype=float32), -0.40769264]. 
=============================================
[2019-04-01 16:25:31,736] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47000, global step 752401: loss 18.1191
[2019-04-01 16:25:31,737] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47000, global step 752401: learning rate 0.0010
[2019-04-01 16:25:31,775] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47000, global step 752414: loss 19.1869
[2019-04-01 16:25:31,776] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47000, global step 752414: learning rate 0.0010
[2019-04-01 16:25:32,127] A3C_AGENT_WORKER-Thread-12 INFO:Local step 47000, global step 752535: loss 29.0212
[2019-04-01 16:25:32,129] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 47000, global step 752535: learning rate 0.0010
[2019-04-01 16:25:32,176] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47000, global step 752549: loss 26.2311
[2019-04-01 16:25:32,177] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47000, global step 752549: learning rate 0.0010
[2019-04-01 16:25:32,387] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47000, global step 752629: loss 29.9992
[2019-04-01 16:25:32,390] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47000, global step 752630: learning rate 0.0010
[2019-04-01 16:25:35,223] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0530532e-26 2.2216802e-37 1.4715970e-31 0.0000000e+00 1.9443084e-21
 3.5128833e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:35,225] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6684
[2019-04-01 16:25:35,247] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.5, 29.0, 5.0, 46.0, 26.0, 25.76627080888155, 8.347929871377358, 1.0, 1.0, 11.12518312621601], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2827800.0000, 
sim time next is 2828400.0000, 
raw observation next is [5.333333333333333, 29.33333333333334, 0.0, 0.0, 26.0, 25.66886746917667, 8.360194896511798, 1.0, 1.0, 11.76634452647609], 
processed observation next is [1.0, 0.7391304347826086, 0.6103416435826409, 0.2933333333333334, 0.0, 0.0, 1.0, 0.9526953527395244, 0.08360194896511798, 1.0, 1.0, 0.08404531804625778], 
reward next is 0.9160, 
noisyNet noise sample is [array([-0.98158085], dtype=float32), -0.13002281]. 
=============================================
[2019-04-01 16:25:35,301] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.01727344e-19 3.31438361e-32 7.66550342e-18 4.11592059e-32
 2.64707988e-23 1.54977632e-22 1.00000000e+00], sum to 1.0000
[2019-04-01 16:25:35,303] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5778
[2019-04-01 16:25:35,316] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.83451829510985, 6.674287932132525, 0.0, 1.0, 41.44838309748309], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2770200.0000, 
sim time next is 2770800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.84317882036191, 6.663348054231339, 0.0, 1.0, 41.31499498539034], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 1.0, 0.8347398314802729, 0.06663348054231338, 0.0, 1.0, 0.29510710703850246], 
reward next is 0.7049, 
noisyNet noise sample is [array([2.3293335], dtype=float32), -1.0882785]. 
=============================================
[2019-04-01 16:25:37,329] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1775726e-20 0.0000000e+00 4.3285071e-27 0.0000000e+00 1.2411904e-17
 9.9969471e-01 3.0531400e-04], sum to 1.0000
[2019-04-01 16:25:37,331] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1909
[2019-04-01 16:25:37,343] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 30.0, 183.5, 86.5, 26.0, 25.79554012375166, 9.689553767790677, 1.0, 1.0, 21.33810316498015], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2815200.0000, 
sim time next is 2815800.0000, 
raw observation next is [6.166666666666666, 29.0, 161.6666666666667, 57.66666666666666, 26.0, 25.92419359077102, 9.792717358323719, 1.0, 1.0, 19.22615924951389], 
processed observation next is [1.0, 0.6086956521739131, 0.6334256694367498, 0.29, 0.5388888888888891, 0.06372007366482503, 1.0, 0.9891705129672884, 0.09792717358323719, 1.0, 1.0, 0.13732970892509921], 
reward next is 0.8627, 
noisyNet noise sample is [array([0.6424915], dtype=float32), -0.0091540245]. 
=============================================
[2019-04-01 16:25:41,919] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7649678e-26 1.1793969e-33 1.1571952e-24 6.4434744e-37 5.9737940e-26
 7.4397819e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:41,919] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9898
[2019-04-01 16:25:41,942] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 81.33333333333334, 0.0, 0.0, 26.0, 25.28934745567737, 7.590752398942617, 0.0, 1.0, 55.09593425938873], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2859600.0000, 
sim time next is 2860200.0000, 
raw observation next is [1.0, 82.5, 0.0, 0.0, 26.0, 25.27659182483593, 7.56691710041301, 0.0, 1.0, 55.04824970224431], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.825, 0.0, 0.0, 1.0, 0.8966559749765614, 0.07566917100413009, 0.0, 1.0, 0.3932017835874594], 
reward next is 0.6068, 
noisyNet noise sample is [array([0.03899503], dtype=float32), 1.8812877]. 
=============================================
[2019-04-01 16:25:46,606] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47500, global step 758872: loss 0.0012
[2019-04-01 16:25:46,607] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47500, global step 758872: learning rate 0.0010
[2019-04-01 16:25:47,375] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47500, global step 759156: loss 0.0443
[2019-04-01 16:25:47,380] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47500, global step 759156: learning rate 0.0010
[2019-04-01 16:25:48,508] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47500, global step 759543: loss 0.0039
[2019-04-01 16:25:48,511] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47500, global step 759544: learning rate 0.0010
[2019-04-01 16:25:49,215] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47500, global step 759786: loss 0.2881
[2019-04-01 16:25:49,216] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47500, global step 759787: learning rate 0.0010
[2019-04-01 16:25:49,481] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47500, global step 759895: loss 0.4927
[2019-04-01 16:25:49,484] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47500, global step 759898: learning rate 0.0010
[2019-04-01 16:25:49,530] A3C_AGENT_WORKER-Thread-13 INFO:Local step 47500, global step 759915: loss 0.3913
[2019-04-01 16:25:49,530] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 47500, global step 759915: learning rate 0.0010
[2019-04-01 16:25:49,571] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47500, global step 759935: loss 0.3201
[2019-04-01 16:25:49,572] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47500, global step 759935: learning rate 0.0010
[2019-04-01 16:25:49,731] A3C_AGENT_WORKER-Thread-11 INFO:Local step 47500, global step 760008: loss 0.5625
[2019-04-01 16:25:49,745] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 47500, global step 760008: learning rate 0.0010
[2019-04-01 16:25:50,273] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5075149e-23 1.1916904e-38 6.1319274e-26 0.0000000e+00 6.9446969e-27
 2.7953644e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:50,274] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9794
[2019-04-01 16:25:50,311] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 55.83333333333334, 74.0, 602.6666666666667, 26.0, 25.11716891761077, 9.258093288286405, 0.0, 1.0, 23.98006569332322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2994600.0000, 
sim time next is 2995200.0000, 
raw observation next is [-1.0, 55.0, 69.5, 570.5, 26.0, 25.136510663912, 9.265739259536431, 0.0, 1.0, 19.71130037197166], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.55, 0.23166666666666666, 0.6303867403314917, 1.0, 0.876644380558857, 0.09265739259536432, 0.0, 1.0, 0.14079500265694042], 
reward next is 0.8592, 
noisyNet noise sample is [array([-0.7149227], dtype=float32), 0.7686048]. 
=============================================
[2019-04-01 16:25:50,316] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47500, global step 760285: loss 0.6759
[2019-04-01 16:25:50,319] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47500, global step 760286: learning rate 0.0010
[2019-04-01 16:25:50,469] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47500, global step 760357: loss 1.5696
[2019-04-01 16:25:50,472] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47500, global step 760357: learning rate 0.0010
[2019-04-01 16:25:50,536] A3C_AGENT_WORKER-Thread-9 INFO:Local step 47500, global step 760392: loss 1.1747
[2019-04-01 16:25:50,540] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 47500, global step 760394: learning rate 0.0010
[2019-04-01 16:25:50,699] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1134676e-25 6.2421125e-36 1.5317921e-28 0.0000000e+00 1.4290405e-28
 2.4879165e-30 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:50,701] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7920
[2019-04-01 16:25:50,715] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.22395888626292, 5.484179146352538, 0.0, 1.0, 39.10060519476693], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3035400.0000, 
sim time next is 3036000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.22090458759745, 5.454897779550305, 0.0, 1.0, 39.21225260414961], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.7458435125139212, 0.05454897779550305, 0.0, 1.0, 0.2800875186010687], 
reward next is 0.7199, 
noisyNet noise sample is [array([-0.51059115], dtype=float32), 1.0772859]. 
=============================================
[2019-04-01 16:25:50,722] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[70.332184]
 [70.3623  ]
 [70.5359  ]
 [70.71917 ]
 [70.96221 ]], R is [[70.34777832]
 [70.36501312]
 [70.3830719 ]
 [70.40207672]
 [70.42195892]].
[2019-04-01 16:25:50,725] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47500, global step 760492: loss 0.9308
[2019-04-01 16:25:50,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47500, global step 760492: learning rate 0.0010
[2019-04-01 16:25:51,183] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47500, global step 760736: loss 0.5845
[2019-04-01 16:25:51,186] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47500, global step 760738: learning rate 0.0010
[2019-04-01 16:25:51,257] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47500, global step 760766: loss 0.3021
[2019-04-01 16:25:51,258] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47500, global step 760766: learning rate 0.0010
[2019-04-01 16:25:51,370] A3C_AGENT_WORKER-Thread-12 INFO:Local step 47500, global step 760818: loss 0.4206
[2019-04-01 16:25:51,373] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 47500, global step 760818: learning rate 0.0010
[2019-04-01 16:25:51,484] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7122464e-25 0.0000000e+00 1.1504000e-23 0.0000000e+00 2.4627752e-34
 2.6336350e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:51,485] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8273
[2019-04-01 16:25:51,536] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 60.66666666666666, 85.66666666666667, 405.0, 26.0, 25.55632326801321, 8.295581378521575, 0.0, 1.0, 45.74581335096222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3055200.0000, 
sim time next is 3055800.0000, 
raw observation next is [-6.0, 59.83333333333334, 88.33333333333334, 451.0, 26.0, 25.75373048789648, 8.388627460941692, 0.0, 1.0, 42.73423795777119], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.5983333333333334, 0.29444444444444445, 0.4983425414364641, 1.0, 0.9648186411280685, 0.08388627460941692, 0.0, 1.0, 0.3052445568412228], 
reward next is 0.6948, 
noisyNet noise sample is [array([1.0028852], dtype=float32), -2.1517875]. 
=============================================
[2019-04-01 16:25:51,536] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47500, global step 760894: loss 0.3046
[2019-04-01 16:25:51,538] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47500, global step 760894: learning rate 0.0010
[2019-04-01 16:25:55,276] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3103647e-24 1.4503399e-33 4.6696826e-24 5.1635347e-36 5.2486113e-36
 9.2035262e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:25:55,277] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1864
[2019-04-01 16:25:55,290] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 100.0, 0.0, 0.0, 26.0, 25.36610396003596, 7.651086353350536, 0.0, 1.0, 37.57038812824328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3119400.0000, 
sim time next is 3120000.0000, 
raw observation next is [1.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.461983928405, 7.634598939618638, 0.0, 1.0, 37.26500576510615], 
processed observation next is [1.0, 0.08695652173913043, 0.5087719298245615, 1.0, 0.0, 0.0, 1.0, 0.9231405612007144, 0.07634598939618638, 0.0, 1.0, 0.2661786126079011], 
reward next is 0.7338, 
noisyNet noise sample is [array([2.5043054], dtype=float32), -0.16745177]. 
=============================================
[2019-04-01 16:25:55,306] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[65.8874  ]
 [66.35147 ]
 [67.055336]
 [67.74159 ]
 [68.31868 ]], R is [[65.43218231]
 [65.50950623]
 [65.58097076]
 [65.64540863]
 [65.71297455]].
[2019-04-01 16:26:02,185] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48000, global step 766585: loss 43.1433
[2019-04-01 16:26:02,185] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48000, global step 766585: learning rate 0.0010
[2019-04-01 16:26:02,592] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48000, global step 766766: loss 44.2809
[2019-04-01 16:26:02,594] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48000, global step 766767: learning rate 0.0010
[2019-04-01 16:26:02,870] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.16877120e-18 1.01539026e-32 2.24289746e-25 4.06019447e-28
 2.66300464e-13 1.00000000e+00 3.09784143e-10], sum to 1.0000
[2019-04-01 16:26:02,872] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6839
[2019-04-01 16:26:02,910] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.166666666666666, 71.16666666666667, 106.3333333333333, 656.6666666666666, 26.0, 26.25393855636298, 12.08342285956952, 1.0, 1.0, 21.37698486360875], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3318600.0000, 
sim time next is 3319200.0000, 
raw observation next is [-8.0, 70.0, 107.5, 677.5, 26.0, 26.30588624213341, 12.32916494014448, 1.0, 1.0, 20.08166678744235], 
processed observation next is [1.0, 0.43478260869565216, 0.24099722991689754, 0.7, 0.35833333333333334, 0.7486187845303868, 1.0, 1.0436980345904874, 0.1232916494014448, 1.0, 1.0, 0.14344047705315965], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2923148], dtype=float32), -1.2454363]. 
=============================================
[2019-04-01 16:26:03,035] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1502542e-24 1.7672496e-36 3.5865504e-23 1.3009511e-38 6.0164931e-33
 2.2707070e-30 1.0000000e+00], sum to 1.0000
[2019-04-01 16:26:03,037] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3680
[2019-04-01 16:26:03,050] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666666, 86.66666666666667, 0.0, 0.0, 26.0, 25.14827663078241, 9.147001410596564, 0.0, 1.0, 42.69226593766588], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3282000.0000, 
sim time next is 3282600.0000, 
raw observation next is [-6.833333333333334, 85.33333333333333, 0.0, 0.0, 26.0, 25.11064170692729, 8.96258570625981, 0.0, 1.0, 42.77277434673055], 
processed observation next is [1.0, 1.0, 0.27331486611265005, 0.8533333333333333, 0.0, 0.0, 1.0, 0.8729488152753271, 0.08962585706259811, 0.0, 1.0, 0.30551981676236106], 
reward next is 0.6945, 
noisyNet noise sample is [array([2.2080846], dtype=float32), -0.45316532]. 
=============================================
[2019-04-01 16:26:03,754] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48000, global step 767340: loss 43.3307
[2019-04-01 16:26:03,754] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48000, global step 767340: learning rate 0.0010
[2019-04-01 16:26:04,668] A3C_AGENT_WORKER-Thread-13 INFO:Local step 48000, global step 767781: loss 40.1535
[2019-04-01 16:26:04,672] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 48000, global step 767782: learning rate 0.0010
[2019-04-01 16:26:04,715] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48000, global step 767805: loss 39.7136
[2019-04-01 16:26:04,715] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48000, global step 767805: learning rate 0.0010
[2019-04-01 16:26:04,801] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48000, global step 767848: loss 45.0796
[2019-04-01 16:26:04,802] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48000, global step 767848: learning rate 0.0010
[2019-04-01 16:26:04,868] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48000, global step 767879: loss 43.0816
[2019-04-01 16:26:04,869] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48000, global step 767879: learning rate 0.0010
[2019-04-01 16:26:04,999] A3C_AGENT_WORKER-Thread-11 INFO:Local step 48000, global step 767941: loss 49.8511
[2019-04-01 16:26:05,000] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 48000, global step 767942: learning rate 0.0010
[2019-04-01 16:26:05,005] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4338138e-19 4.7086661e-29 8.0266884e-21 2.1509499e-33 6.5561689e-28
 4.5073762e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 16:26:05,006] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0187
[2019-04-01 16:26:05,018] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 79.33333333333334, 0.0, 0.0, 26.0, 25.01978359371928, 8.543532754075045, 0.0, 1.0, 43.01285719111781], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3284400.0000, 
sim time next is 3285000.0000, 
raw observation next is [-7.0, 77.0, 0.0, 0.0, 26.0, 24.97915951970675, 8.483981971969351, 0.0, 1.0, 43.0765319948858], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.77, 0.0, 0.0, 1.0, 0.8541656456723931, 0.08483981971969351, 0.0, 1.0, 0.3076895142491843], 
reward next is 0.6923, 
noisyNet noise sample is [array([-0.3355141], dtype=float32), -0.42999524]. 
=============================================
[2019-04-01 16:26:05,032] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[67.79731 ]
 [69.47859 ]
 [71.199005]
 [72.82893 ]
 [72.87402 ]], R is [[66.3801651 ]
 [66.40912628]
 [66.43833923]
 [66.4677887 ]
 [66.49758911]].
[2019-04-01 16:26:05,580] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48000, global step 768182: loss 36.1518
[2019-04-01 16:26:05,584] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48000, global step 768182: learning rate 0.0010
[2019-04-01 16:26:05,590] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48000, global step 768188: loss 37.8560
[2019-04-01 16:26:05,592] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48000, global step 768188: learning rate 0.0010
[2019-04-01 16:26:05,725] A3C_AGENT_WORKER-Thread-9 INFO:Local step 48000, global step 768249: loss 48.1607
[2019-04-01 16:26:05,726] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 48000, global step 768249: learning rate 0.0010
[2019-04-01 16:26:05,890] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48000, global step 768337: loss 47.3568
[2019-04-01 16:26:05,891] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48000, global step 768337: learning rate 0.0010
[2019-04-01 16:26:06,470] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48000, global step 768559: loss 37.4347
[2019-04-01 16:26:06,471] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48000, global step 768559: learning rate 0.0010
[2019-04-01 16:26:06,538] A3C_AGENT_WORKER-Thread-12 INFO:Local step 48000, global step 768591: loss 40.8211
[2019-04-01 16:26:06,539] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 48000, global step 768593: learning rate 0.0010
[2019-04-01 16:26:06,549] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48000, global step 768596: loss 41.4585
[2019-04-01 16:26:06,549] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48000, global step 768596: learning rate 0.0010
[2019-04-01 16:26:06,676] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48000, global step 768660: loss 40.3935
[2019-04-01 16:26:06,677] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48000, global step 768660: learning rate 0.0010
[2019-04-01 16:26:07,597] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.6084672e-23 1.2565946e-37 3.9895023e-31 0.0000000e+00 3.1731763e-20
 5.7093272e-05 9.9994290e-01], sum to 1.0000
[2019-04-01 16:26:07,600] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9038
[2019-04-01 16:26:07,607] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 54.0, 118.0, 811.0, 26.0, 26.23440943175891, 12.48346795358965, 1.0, 1.0, 8.259122972554257], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3328200.0000, 
sim time next is 3328800.0000, 
raw observation next is [-5.333333333333333, 54.0, 117.3333333333333, 809.1666666666667, 26.0, 26.10096148473817, 12.14751939117207, 1.0, 1.0, 8.04882028969165], 
processed observation next is [1.0, 0.5217391304347826, 0.3148661126500462, 0.54, 0.391111111111111, 0.8941068139963169, 1.0, 1.0144230692483103, 0.1214751939117207, 1.0, 1.0, 0.057491573497797495], 
reward next is 0.0835, 
noisyNet noise sample is [array([-0.8627642], dtype=float32), -0.4052077]. 
=============================================
[2019-04-01 16:26:13,368] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7119595e-17 5.4247419e-32 3.5055798e-16 2.0839272e-35 5.2011064e-32
 2.0549127e-31 1.0000000e+00], sum to 1.0000
[2019-04-01 16:26:13,369] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9214
[2019-04-01 16:26:13,386] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.47103849020101, 9.091448267251584, 0.0, 1.0, 33.73390328724978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3463200.0000, 
sim time next is 3463800.0000, 
raw observation next is [1.0, 77.83333333333334, 0.0, 0.0, 26.0, 25.40067395578461, 9.023800295113896, 0.0, 1.0, 35.6802644574889], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.7783333333333334, 0.0, 0.0, 1.0, 0.9143819936835159, 0.09023800295113896, 0.0, 1.0, 0.25485903183920644], 
reward next is 0.7451, 
noisyNet noise sample is [array([-0.81965715], dtype=float32), 1.6417781]. 
=============================================
[2019-04-01 16:26:13,401] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6166451e-25 0.0000000e+00 1.8343850e-36 0.0000000e+00 9.3918182e-35
 2.1173398e-37 1.0000000e+00], sum to 1.0000
[2019-04-01 16:26:13,409] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1718
[2019-04-01 16:26:13,432] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.57919544815903, 12.63265729541867, 0.0, 1.0, 44.46438983517574], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3444000.0000, 
sim time next is 3444600.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.70456393838916, 12.85978598110044, 0.0, 1.0, 35.90329372284837], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 1.0, 0.9577948483413087, 0.1285978598110044, 0.0, 1.0, 0.25645209802034546], 
reward next is 0.7435, 
noisyNet noise sample is [array([0.06413501], dtype=float32), -0.47408307]. 
=============================================
[2019-04-01 16:26:15,763] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1799953e-13 2.1197189e-19 5.9101178e-13 7.4120775e-19 1.4463069e-14
 4.9226331e-11 1.0000000e+00], sum to 1.0000
[2019-04-01 16:26:15,767] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2365
[2019-04-01 16:26:15,799] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 71.0, 91.66666666666666, 489.3333333333334, 26.0, 25.46495886442913, 9.365581325860964, 1.0, 1.0, 20.84768988102392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3487800.0000, 
sim time next is 3488400.0000, 
raw observation next is [-1.0, 71.0, 93.5, 534.5, 26.0, 25.57154575854205, 9.425341034162193, 1.0, 1.0, 18.62784989693173], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.71, 0.31166666666666665, 0.5906077348066299, 1.0, 0.938792251220293, 0.09425341034162192, 1.0, 1.0, 0.1330560706923695], 
reward next is 0.8669, 
noisyNet noise sample is [array([-1.0318843], dtype=float32), -0.13730258]. 
=============================================
[2019-04-01 16:26:15,830] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8768577e-25 3.0468508e-31 1.9470636e-27 1.4344355e-37 3.1850237e-28
 2.8654509e-32 1.0000000e+00], sum to 1.0000
[2019-04-01 16:26:15,838] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5610
[2019-04-01 16:26:15,854] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.36028915793197, 12.6082205158716, 0.0, 1.0, 45.23578055148981], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3531000.0000, 
sim time next is 3531600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.64721732262629, 13.09070205071477, 0.0, 1.0, 43.54772433111163], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 1.0, 0.9496024746608985, 0.1309070205071477, 0.0, 1.0, 0.3110551737936545], 
reward next is 0.6889, 
noisyNet noise sample is [array([-1.9965818], dtype=float32), -0.48764098]. 
=============================================
[2019-04-01 16:26:16,985] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3957572e-26 3.7475274e-34 3.4565642e-25 8.8802189e-38 2.6440213e-31
 3.3814631e-28 1.0000000e+00], sum to 1.0000
[2019-04-01 16:26:16,989] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7536
[2019-04-01 16:26:17,004] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.3599195515397, 9.4974634380261, 0.0, 1.0, 41.83713825200345], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3544200.0000, 
sim time next is 3544800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.35989768008789, 9.371110886406361, 0.0, 1.0, 41.71370636510016], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 1.0, 0.9085568114411272, 0.09371110886406361, 0.0, 1.0, 0.2979550454650011], 
reward next is 0.7020, 
noisyNet noise sample is [array([1.7491652], dtype=float32), 0.7203292]. 
=============================================
[2019-04-01 16:26:17,686] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48500, global step 774705: loss 0.0171
[2019-04-01 16:26:17,688] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48500, global step 774706: learning rate 0.0010
[2019-04-01 16:26:17,730] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0952444e-26 1.8696965e-34 4.3590824e-26 3.6675296e-38 8.2734478e-32
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:26:17,733] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3216
[2019-04-01 16:26:17,754] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 76.0, 0.0, 0.0, 26.0, 25.73978181087396, 10.70441323249972, 0.0, 1.0, 26.8292061043265], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3535800.0000, 
sim time next is 3536400.0000, 
raw observation next is [-1.0, 74.0, 0.0, 0.0, 26.0, 25.71624852429571, 9.813366826534027, 0.0, 1.0, 25.53664331758393], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.74, 0.0, 0.0, 1.0, 0.9594640748993873, 0.09813366826534027, 0.0, 1.0, 0.1824045951255995], 
reward next is 0.8176, 
noisyNet noise sample is [array([0.99754316], dtype=float32), -0.6413199]. 
=============================================
[2019-04-01 16:26:18,145] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48500, global step 774921: loss 0.0176
[2019-04-01 16:26:18,147] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48500, global step 774922: learning rate 0.0010
[2019-04-01 16:26:19,170] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48500, global step 775368: loss 1.5239
[2019-04-01 16:26:19,171] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48500, global step 775369: learning rate 0.0010
[2019-04-01 16:26:19,751] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.67997251e-27 0.00000000e+00 6.85805560e-35 0.00000000e+00
 1.04782785e-36 6.06810985e-30 1.00000000e+00], sum to 1.0000
[2019-04-01 16:26:19,758] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2546
[2019-04-01 16:26:19,773] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 42.33333333333334, 88.66666666666667, 713.8333333333333, 26.0, 25.21622396704379, 10.05962361164507, 0.0, 1.0, 15.84831926370285], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3597600.0000, 
sim time next is 3598200.0000, 
raw observation next is [-0.5, 42.5, 86.0, 699.0, 26.0, 25.28629599346809, 10.28575296007846, 0.0, 1.0, 12.58146175231368], 
processed observation next is [0.0, 0.6521739130434783, 0.44875346260387816, 0.425, 0.2866666666666667, 0.7723756906077348, 1.0, 0.8980422847811559, 0.10285752960078459, 0.0, 1.0, 0.08986758394509772], 
reward next is 0.9101, 
noisyNet noise sample is [array([-0.9204452], dtype=float32), 1.0323517]. 
=============================================
[2019-04-01 16:26:19,887] A3C_AGENT_WORKER-Thread-13 INFO:Local step 48500, global step 775708: loss 9.8387
[2019-04-01 16:26:19,895] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 48500, global step 775710: learning rate 0.0010
[2019-04-01 16:26:20,065] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48500, global step 775786: loss 12.2693
[2019-04-01 16:26:20,069] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48500, global step 775787: learning rate 0.0010
[2019-04-01 16:26:20,298] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 16:26:20,298] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4386
[2019-04-01 16:26:20,309] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.133333333333333, 27.66666666666667, 0.0, 0.0, 26.0, 25.53350101923102, 7.953856569154354, 0.0, 1.0, 31.79672072483081], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3640800.0000, 
sim time next is 3641400.0000, 
raw observation next is [8.1, 28.0, 0.0, 0.0, 26.0, 25.53094921790507, 7.966642816327467, 0.0, 1.0, 29.45536666143638], 
processed observation next is [0.0, 0.13043478260869565, 0.6869806094182825, 0.28, 0.0, 0.0, 1.0, 0.9329927454150102, 0.07966642816327467, 0.0, 1.0, 0.210395476153117], 
reward next is 0.7896, 
noisyNet noise sample is [array([-0.451959], dtype=float32), -1.7975342]. 
=============================================
[2019-04-01 16:26:20,318] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48500, global step 775928: loss 16.9456
[2019-04-01 16:26:20,323] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48500, global step 775929: learning rate 0.0010
[2019-04-01 16:26:20,436] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48500, global step 775996: loss 15.5826
[2019-04-01 16:26:20,437] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48500, global step 775996: learning rate 0.0010
[2019-04-01 16:26:20,636] A3C_AGENT_WORKER-Thread-11 INFO:Local step 48500, global step 776121: loss 15.1354
[2019-04-01 16:26:20,638] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 48500, global step 776121: learning rate 0.0010
[2019-04-01 16:26:20,950] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48500, global step 776311: loss 12.3714
[2019-04-01 16:26:20,950] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48500, global step 776311: learning rate 0.0010
[2019-04-01 16:26:20,977] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48500, global step 776328: loss 10.5831
[2019-04-01 16:26:20,977] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48500, global step 776328: learning rate 0.0010
[2019-04-01 16:26:21,144] A3C_AGENT_WORKER-Thread-9 INFO:Local step 48500, global step 776437: loss 14.2816
[2019-04-01 16:26:21,146] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 48500, global step 776438: learning rate 0.0010
[2019-04-01 16:26:21,155] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48500, global step 776446: loss 13.9558
[2019-04-01 16:26:21,157] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48500, global step 776447: learning rate 0.0010
[2019-04-01 16:26:21,806] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48500, global step 776857: loss 5.4134
[2019-04-01 16:26:21,808] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48500, global step 776858: learning rate 0.0010
[2019-04-01 16:26:21,919] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48500, global step 776927: loss 5.8627
[2019-04-01 16:26:21,923] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48500, global step 776928: learning rate 0.0010
[2019-04-01 16:26:21,926] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48500, global step 776930: loss 5.9654
[2019-04-01 16:26:21,928] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48500, global step 776931: learning rate 0.0010
[2019-04-01 16:26:22,197] A3C_AGENT_WORKER-Thread-12 INFO:Local step 48500, global step 777098: loss 7.5426
[2019-04-01 16:26:22,198] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 48500, global step 777099: learning rate 0.0010
[2019-04-01 16:26:31,521] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49000, global step 782328: loss 0.0193
[2019-04-01 16:26:31,523] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49000, global step 782330: learning rate 0.0010
[2019-04-01 16:26:32,012] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49000, global step 782557: loss 0.0866
[2019-04-01 16:26:32,014] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49000, global step 782557: learning rate 0.0010
[2019-04-01 16:26:32,175] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4853627e-09 1.3027313e-27 5.2143702e-27 1.1305777e-33 2.3886573e-15
 7.5448073e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 16:26:32,176] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1220
[2019-04-01 16:26:32,187] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 42.33333333333334, 56.33333333333334, 489.0, 26.0, 26.80143716864058, 17.46061308325626, 1.0, 1.0, 7.392298367462963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3861600.0000, 
sim time next is 3862200.0000, 
raw observation next is [3.0, 41.66666666666667, 48.66666666666667, 427.0, 26.0, 26.96407506189558, 18.00195874727525, 1.0, 1.0, 7.26020801417256], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.41666666666666674, 0.16222222222222224, 0.4718232044198895, 1.0, 1.137725008842226, 0.18001958747275248, 1.0, 1.0, 0.05185862867266114], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.13968356], dtype=float32), 0.4269185]. 
=============================================
[2019-04-01 16:26:32,616] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49000, global step 782847: loss 0.1727
[2019-04-01 16:26:32,617] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49000, global step 782847: learning rate 0.0010
[2019-04-01 16:26:32,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4033070e-25 0.0000000e+00 1.8262717e-36 0.0000000e+00 1.8612463e-30
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:26:32,649] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2576
[2019-04-01 16:26:32,663] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 45.66666666666667, 15.0, 149.1666666666667, 26.0, 26.13944898971361, 14.07473224225608, 1.0, 1.0, 6.484053273612422], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3865200.0000, 
sim time next is 3865800.0000, 
raw observation next is [2.166666666666667, 46.83333333333333, 0.0, 0.0, 26.0, 26.10867871384126, 13.58359368853939, 1.0, 1.0, 6.56192403451989], 
processed observation next is [1.0, 0.7391304347826086, 0.5226223453370269, 0.46833333333333327, 0.0, 0.0, 1.0, 1.0155255305487516, 0.1358359368853939, 1.0, 1.0, 0.04687088596085636], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.48749742], dtype=float32), 1.3039395]. 
=============================================
[2019-04-01 16:26:33,796] A3C_AGENT_WORKER-Thread-13 INFO:Local step 49000, global step 783494: loss 0.0762
[2019-04-01 16:26:33,798] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 49000, global step 783495: learning rate 0.0010
[2019-04-01 16:26:33,980] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49000, global step 783609: loss 0.0121
[2019-04-01 16:26:33,981] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49000, global step 783609: learning rate 0.0010
[2019-04-01 16:26:34,049] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49000, global step 783642: loss 0.0106
[2019-04-01 16:26:34,050] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49000, global step 783642: learning rate 0.0010
[2019-04-01 16:26:34,368] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49000, global step 783799: loss 0.0491
[2019-04-01 16:26:34,368] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49000, global step 783799: learning rate 0.0010
[2019-04-01 16:26:34,693] A3C_AGENT_WORKER-Thread-11 INFO:Local step 49000, global step 783958: loss 0.1118
[2019-04-01 16:26:34,696] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 49000, global step 783961: learning rate 0.0010
[2019-04-01 16:26:35,095] A3C_AGENT_WORKER-Thread-9 INFO:Local step 49000, global step 784149: loss 0.0234
[2019-04-01 16:26:35,096] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 49000, global step 784149: learning rate 0.0010
[2019-04-01 16:26:35,102] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49000, global step 784152: loss 0.1129
[2019-04-01 16:26:35,104] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49000, global step 784152: learning rate 0.0010
[2019-04-01 16:26:35,119] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49000, global step 784161: loss 0.1407
[2019-04-01 16:26:35,122] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49000, global step 784162: learning rate 0.0010
[2019-04-01 16:26:35,143] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49000, global step 784173: loss 0.0608
[2019-04-01 16:26:35,148] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49000, global step 784173: learning rate 0.0010
[2019-04-01 16:26:35,645] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49000, global step 784373: loss 0.0084
[2019-04-01 16:26:35,647] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49000, global step 784373: learning rate 0.0010
[2019-04-01 16:26:35,678] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49000, global step 784383: loss 0.0362
[2019-04-01 16:26:35,682] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49000, global step 784383: learning rate 0.0010
[2019-04-01 16:26:36,183] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49000, global step 784582: loss 0.0151
[2019-04-01 16:26:36,185] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49000, global step 784582: learning rate 0.0010
[2019-04-01 16:26:36,327] A3C_AGENT_WORKER-Thread-12 INFO:Local step 49000, global step 784643: loss 0.0141
[2019-04-01 16:26:36,327] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 49000, global step 784643: learning rate 0.0010
[2019-04-01 16:26:38,530] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.3300039e-32 1.0384370e-36 5.7004255e-35 0.0000000e+00 7.0767237e-28
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:26:38,531] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7532
[2019-04-01 16:26:38,580] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.833333333333334, 44.33333333333333, 0.0, 0.0, 26.0, 25.47533682589047, 11.072445941772, 0.0, 1.0, 39.23861028653719], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3959400.0000, 
sim time next is 3960000.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.48514797541447, 11.10159749964154, 0.0, 1.0, 37.29303836683179], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 1.0, 0.9264497107734958, 0.11101597499641541, 0.0, 1.0, 0.2663788454773699], 
reward next is 0.7336, 
noisyNet noise sample is [array([0.8339248], dtype=float32), 0.08861822]. 
=============================================
[2019-04-01 16:26:38,591] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[47.03208 ]
 [44.31749 ]
 [44.801006]
 [45.083218]
 [45.498165]], R is [[49.82302475]
 [50.04451752]
 [49.82506561]
 [49.65524292]
 [49.51678848]].
[2019-04-01 16:26:41,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.49726951e-07 1.54164740e-19 1.95930681e-14 2.84742224e-21
 2.67797877e-04 1.00549705e-05 9.99721467e-01], sum to 1.0000
[2019-04-01 16:26:41,779] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1587
[2019-04-01 16:26:41,810] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 40.0, 115.5, 798.5, 26.0, 26.53929876271523, 12.37754584507694, 1.0, 1.0, 15.22385827329781], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4014000.0000, 
sim time next is 4014600.0000, 
raw observation next is [-7.666666666666667, 39.5, 116.6666666666667, 804.3333333333334, 26.0, 26.46921100690034, 12.36238546176251, 1.0, 1.0, 14.4538266445492], 
processed observation next is [1.0, 0.4782608695652174, 0.2502308402585411, 0.395, 0.388888888888889, 0.8887661141804789, 1.0, 1.067030143842906, 0.12362385461762511, 1.0, 1.0, 0.10324161888963714], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.26092225], dtype=float32), 0.8433073]. 
=============================================
[2019-04-01 16:26:42,873] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.374058e-35 0.000000e+00 0.000000e+00 0.000000e+00 4.581264e-33
 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-04-01 16:26:42,876] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8850
[2019-04-01 16:26:42,910] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 24.33333333333333, 35.66666666666666, 311.3333333333333, 26.0, 26.88506910108721, 15.23496348044864, 1.0, 1.0, 6.832141250422818], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4036200.0000, 
sim time next is 4036800.0000, 
raw observation next is [-2.333333333333333, 24.66666666666666, 27.83333333333333, 252.1666666666667, 26.0, 26.48684634786361, 13.0218752080735, 1.0, 1.0, 6.644202951193061], 
processed observation next is [1.0, 0.7391304347826086, 0.3979686057248385, 0.24666666666666662, 0.09277777777777776, 0.2786372007366483, 1.0, 1.0695494782662303, 0.130218752080735, 1.0, 1.0, 0.047458592508521864], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3699008], dtype=float32), -0.7576078]. 
=============================================
[2019-04-01 16:26:48,073] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49500, global step 790496: loss 19.9088
[2019-04-01 16:26:48,073] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49500, global step 790496: learning rate 0.0010
[2019-04-01 16:26:48,418] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49500, global step 790716: loss 13.3667
[2019-04-01 16:26:48,420] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49500, global step 790716: learning rate 0.0010
[2019-04-01 16:26:48,939] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49500, global step 791024: loss 17.0362
[2019-04-01 16:26:48,939] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49500, global step 791024: learning rate 0.0010
[2019-04-01 16:26:49,645] A3C_AGENT_WORKER-Thread-13 INFO:Local step 49500, global step 791411: loss 9.4705
[2019-04-01 16:26:49,649] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 49500, global step 791412: learning rate 0.0010
[2019-04-01 16:26:50,330] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49500, global step 791750: loss 3.3657
[2019-04-01 16:26:50,331] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49500, global step 791751: learning rate 0.0010
[2019-04-01 16:26:50,394] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49500, global step 791789: loss 2.5232
[2019-04-01 16:26:50,395] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49500, global step 791789: learning rate 0.0010
[2019-04-01 16:26:50,798] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49500, global step 792000: loss 0.6310
[2019-04-01 16:26:50,800] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49500, global step 792000: learning rate 0.0010
[2019-04-01 16:26:50,966] A3C_AGENT_WORKER-Thread-11 INFO:Local step 49500, global step 792101: loss 0.2984
[2019-04-01 16:26:50,967] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 49500, global step 792101: learning rate 0.0010
[2019-04-01 16:26:51,079] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49500, global step 792166: loss 0.1352
[2019-04-01 16:26:51,080] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49500, global step 792166: learning rate 0.0010
[2019-04-01 16:26:51,306] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49500, global step 792308: loss 0.5277
[2019-04-01 16:26:51,308] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49500, global step 792308: learning rate 0.0010
[2019-04-01 16:26:51,420] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49500, global step 792377: loss 0.5740
[2019-04-01 16:26:51,422] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49500, global step 792378: learning rate 0.0010
[2019-04-01 16:26:51,717] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49500, global step 792562: loss 1.1529
[2019-04-01 16:26:51,718] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49500, global step 792562: learning rate 0.0010
[2019-04-01 16:26:51,742] A3C_AGENT_WORKER-Thread-9 INFO:Local step 49500, global step 792578: loss 0.0761
[2019-04-01 16:26:51,743] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 49500, global step 792578: learning rate 0.0010
[2019-04-01 16:26:51,765] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.7451053e-35 0.0000000e+00 1.7834414e-37 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:26:51,767] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7835
[2019-04-01 16:26:51,778] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.61155223573422, 8.560644301563512, 0.0, 1.0, 25.44750500774572], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4221600.0000, 
sim time next is 4222200.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.55872402765629, 8.328360566082303, 0.0, 1.0, 24.14353911369018], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 1.0, 0.93696057537947, 0.08328360566082303, 0.0, 1.0, 0.17245385081207273], 
reward next is 0.8275, 
noisyNet noise sample is [array([-1.5814964], dtype=float32), -0.98994476]. 
=============================================
[2019-04-01 16:26:51,822] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49500, global step 792627: loss 0.6060
[2019-04-01 16:26:51,823] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49500, global step 792628: learning rate 0.0010
[2019-04-01 16:26:52,052] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49500, global step 792763: loss 1.4671
[2019-04-01 16:26:52,053] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49500, global step 792764: learning rate 0.0010
[2019-04-01 16:26:52,745] A3C_AGENT_WORKER-Thread-12 INFO:Local step 49500, global step 793184: loss 0.5816
[2019-04-01 16:26:52,746] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 49500, global step 793186: learning rate 0.0010
[2019-04-01 16:26:59,166] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.26657770e-15 5.05971638e-24 5.51308930e-18 8.86669695e-34
 5.05768115e-20 1.02441644e-32 1.00000000e+00], sum to 1.0000
[2019-04-01 16:26:59,167] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4219
[2019-04-01 16:26:59,186] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 26.0, 27.72194546096762, 24.81431716457412, 1.0, 1.0, 1.654359741494354], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4366200.0000, 
sim time next is 4366800.0000, 
raw observation next is [14.6, 29.0, 116.5, 847.5, 26.0, 26.86396558796802, 22.43639265604389, 1.0, 1.0, 1.569930816956743], 
processed observation next is [1.0, 0.5652173913043478, 0.8670360110803325, 0.29, 0.3883333333333333, 0.93646408839779, 1.0, 1.1234236554240031, 0.22436392656043888, 1.0, 1.0, 0.01121379154969102], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.93051773], dtype=float32), -0.6741386]. 
=============================================
[2019-04-01 16:27:01,904] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50000, global step 798582: loss 0.7821
[2019-04-01 16:27:01,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50000, global step 798583: learning rate 0.0010
[2019-04-01 16:27:02,923] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50000, global step 799057: loss 0.4428
[2019-04-01 16:27:02,926] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50000, global step 799058: learning rate 0.0010
[2019-04-01 16:27:03,087] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50000, global step 799130: loss 0.4017
[2019-04-01 16:27:03,088] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50000, global step 799130: learning rate 0.0010
[2019-04-01 16:27:03,623] A3C_AGENT_WORKER-Thread-13 INFO:Local step 50000, global step 799353: loss 0.0905
[2019-04-01 16:27:03,624] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 50000, global step 799353: learning rate 0.0010
[2019-04-01 16:27:04,186] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50000, global step 799600: loss 0.1577
[2019-04-01 16:27:04,187] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50000, global step 799600: learning rate 0.0010
[2019-04-01 16:27:04,253] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50000, global step 799634: loss 0.1260
[2019-04-01 16:27:04,255] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50000, global step 799634: learning rate 0.0010
[2019-04-01 16:27:04,875] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50000, global step 799978: loss 0.0972
[2019-04-01 16:27:04,876] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50000, global step 799978: learning rate 0.0010
[2019-04-01 16:27:04,923] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-01 16:27:04,924] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 16:27:04,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:27:04,925] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 16:27:04,926] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:27:04,926] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 16:27:04,927] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:27:04,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run9
[2019-04-01 16:27:04,943] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run9
[2019-04-01 16:27:04,970] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run9
[2019-04-01 16:28:25,825] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.586916], dtype=float32), -0.3750159]
[2019-04-01 16:28:25,826] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.2968776223333333, 91.54387775333333, 0.0, 0.0, 26.0, 24.43880264180728, 5.671907646999074, 0.0, 1.0, 55.82792711608865]
[2019-04-01 16:28:25,826] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 16:28:25,827] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.5828519e-30 3.8426292e-38 2.5544885e-28 0.0000000e+00 2.6142578e-36
 1.3926395e-35 1.0000000e+00], sampled 0.21531830724869683
[2019-04-01 16:28:47,021] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 16:29:06,598] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.586916], dtype=float32), -0.3750159]
[2019-04-01 16:29:06,598] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [6.0, 23.0, 0.0, 0.0, 26.0, 26.08009040436971, 13.3974578373198, 1.0, 1.0, 4.417274001056122]
[2019-04-01 16:29:06,598] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:29:06,598] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.4191275e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sampled 0.7850055176093569
[2019-04-01 16:29:08,077] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 16:29:10,880] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 16:29:11,903] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 800000, evaluation results [800000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 16:29:11,970] A3C_AGENT_WORKER-Thread-11 INFO:Local step 50000, global step 800035: loss 0.0662
[2019-04-01 16:29:11,971] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 50000, global step 800035: learning rate 0.0010
[2019-04-01 16:29:12,236] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50000, global step 800185: loss 0.3774
[2019-04-01 16:29:12,238] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50000, global step 800185: learning rate 0.0010
[2019-04-01 16:29:12,515] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50000, global step 800329: loss 0.3202
[2019-04-01 16:29:12,516] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50000, global step 800330: learning rate 0.0010
[2019-04-01 16:29:12,615] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50000, global step 800384: loss 0.4024
[2019-04-01 16:29:12,617] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50000, global step 800384: learning rate 0.0010
[2019-04-01 16:29:12,773] A3C_AGENT_WORKER-Thread-9 INFO:Local step 50000, global step 800472: loss 0.0833
[2019-04-01 16:29:12,775] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 50000, global step 800473: learning rate 0.0010
[2019-04-01 16:29:12,880] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50000, global step 800520: loss 0.1305
[2019-04-01 16:29:12,880] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50000, global step 800520: learning rate 0.0010
[2019-04-01 16:29:12,948] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.45469316e-28 0.00000000e+00 1.39278655e-30 0.00000000e+00
 0.00000000e+00 0.00000000e+00 1.00000000e+00], sum to 1.0000
[2019-04-01 16:29:12,951] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7359
[2019-04-01 16:29:12,962] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 25.82041704458178, 11.87119649996746, 0.0, 1.0, 23.94290433791848], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4570800.0000, 
sim time next is 4571400.0000, 
raw observation next is [1.166666666666667, 60.33333333333334, 0.0, 0.0, 26.0, 25.77787378342209, 11.75637532173629, 0.0, 1.0, 22.59841498040131], 
processed observation next is [1.0, 0.9130434782608695, 0.49492151431209613, 0.6033333333333334, 0.0, 0.0, 1.0, 0.968267683346013, 0.1175637532173629, 0.0, 1.0, 0.16141724986000935], 
reward next is 0.8386, 
noisyNet noise sample is [array([-0.11458311], dtype=float32), -0.7512107]. 
=============================================
[2019-04-01 16:29:13,089] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50000, global step 800620: loss 0.0333
[2019-04-01 16:29:13,092] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50000, global step 800622: learning rate 0.0010
[2019-04-01 16:29:13,136] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50000, global step 800643: loss 0.0431
[2019-04-01 16:29:13,147] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50000, global step 800645: learning rate 0.0010
[2019-04-01 16:29:13,860] A3C_AGENT_WORKER-Thread-12 INFO:Local step 50000, global step 801003: loss 0.1625
[2019-04-01 16:29:13,861] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 50000, global step 801003: learning rate 0.0010
[2019-04-01 16:29:13,885] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2302109e-25 7.0479294e-34 2.2592744e-24 6.8394172e-38 2.2364116e-26
 8.6400508e-26 1.0000000e+00], sum to 1.0000
[2019-04-01 16:29:13,886] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4684
[2019-04-01 16:29:13,901] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 59.0, 221.0, 18.0, 26.0, 26.22240379497376, 11.42813576451195, 1.0, 1.0, 10.19925712298076], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4530600.0000, 
sim time next is 4531200.0000, 
raw observation next is [1.666666666666667, 58.33333333333334, 203.8333333333333, 15.0, 26.0, 26.24545671140863, 11.4193317872643, 1.0, 1.0, 9.528690620271542], 
processed observation next is [1.0, 0.43478260869565216, 0.5087719298245615, 0.5833333333333335, 0.6794444444444443, 0.016574585635359115, 1.0, 1.035065244486947, 0.114193317872643, 1.0, 1.0, 0.06806207585908244], 
reward next is 0.3642, 
noisyNet noise sample is [array([0.16653346], dtype=float32), 0.06551496]. 
=============================================
[2019-04-01 16:29:16,026] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0561750e-21 6.9983960e-33 1.4468659e-23 3.2631275e-37 6.4988036e-34
 1.6969560e-36 1.0000000e+00], sum to 1.0000
[2019-04-01 16:29:16,031] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2511
[2019-04-01 16:29:16,052] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.44679991398511, 10.178394399197, 0.0, 1.0, 36.89518036822106], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4578600.0000, 
sim time next is 4579200.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.55952843236771, 10.16864465452284, 0.0, 1.0, 30.18459680551442], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.61, 0.0, 0.0, 1.0, 0.9370754903382442, 0.1016864465452284, 0.0, 1.0, 0.21560426289653156], 
reward next is 0.7844, 
noisyNet noise sample is [array([0.9760497], dtype=float32), -0.24463351]. 
=============================================
[2019-04-01 16:29:24,462] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50500, global step 806597: loss 1.2725
[2019-04-01 16:29:24,462] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50500, global step 806597: learning rate 0.0010
[2019-04-01 16:29:25,084] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7643137e-22 3.0434295e-35 2.1467251e-21 0.0000000e+00 0.0000000e+00
 9.2858710e-35 1.0000000e+00], sum to 1.0000
[2019-04-01 16:29:25,084] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9418
[2019-04-01 16:29:25,103] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 81.66666666666667, 0.0, 0.0, 26.0, 25.35395930753482, 9.34189129615239, 0.0, 1.0, 43.04346121984801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4746000.0000, 
sim time next is 4746600.0000, 
raw observation next is [-3.0, 80.5, 0.0, 0.0, 26.0, 25.28993392254386, 9.184136180286936, 0.0, 1.0, 42.27356579096575], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.805, 0.0, 0.0, 1.0, 0.8985619889348371, 0.09184136180286936, 0.0, 1.0, 0.3019540413640411], 
reward next is 0.6980, 
noisyNet noise sample is [array([0.42976037], dtype=float32), -1.3858105]. 
=============================================
[2019-04-01 16:29:25,496] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50500, global step 807123: loss 0.0030
[2019-04-01 16:29:25,497] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50500, global step 807124: learning rate 0.0010
[2019-04-01 16:29:25,560] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3307439e-26 0.0000000e+00 5.5003519e-28 0.0000000e+00 3.7586699e-32
 1.7609163e-37 1.0000000e+00], sum to 1.0000
[2019-04-01 16:29:25,560] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1201
[2019-04-01 16:29:25,579] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 74.5, 0.0, 0.0, 26.0, 24.80194042618295, 7.252643348723119, 0.0, 1.0, 40.09816575285966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4759800.0000, 
sim time next is 4760400.0000, 
raw observation next is [-4.666666666666667, 78.0, 0.0, 0.0, 26.0, 24.78935959896184, 7.156129131768435, 0.0, 1.0, 40.07112997225609], 
processed observation next is [0.0, 0.08695652173913043, 0.3333333333333333, 0.78, 0.0, 0.0, 1.0, 0.8270513712802631, 0.07156129131768435, 0.0, 1.0, 0.28622235694468634], 
reward next is 0.7138, 
noisyNet noise sample is [array([0.12926757], dtype=float32), 0.09957085]. 
=============================================
[2019-04-01 16:29:25,758] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50500, global step 807275: loss 0.0084
[2019-04-01 16:29:25,759] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50500, global step 807275: learning rate 0.0010
[2019-04-01 16:29:26,115] A3C_AGENT_WORKER-Thread-13 INFO:Local step 50500, global step 807486: loss 0.2160
[2019-04-01 16:29:26,116] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 50500, global step 807486: learning rate 0.0010
[2019-04-01 16:29:26,243] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50500, global step 807550: loss 0.0049
[2019-04-01 16:29:26,245] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50500, global step 807550: learning rate 0.0010
[2019-04-01 16:29:26,428] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50500, global step 807636: loss 0.1162
[2019-04-01 16:29:26,430] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50500, global step 807636: learning rate 0.0010
[2019-04-01 16:29:27,187] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50500, global step 807959: loss 0.3989
[2019-04-01 16:29:27,193] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50500, global step 807962: learning rate 0.0010
[2019-04-01 16:29:27,403] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.4066836e-25 0.0000000e+00 7.7227566e-28 0.0000000e+00 1.5192379e-38
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:29:27,403] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4707
[2019-04-01 16:29:27,416] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.39100560706583, 8.669871003531497, 0.0, 1.0, 39.91499794552722], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4831200.0000, 
sim time next is 4831800.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.43320386919621, 8.669993319503881, 0.0, 1.0, 38.89370612713858], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 1.0, 0.919029124170887, 0.08669993319503881, 0.0, 1.0, 0.2778121866224184], 
reward next is 0.7222, 
noisyNet noise sample is [array([0.05076635], dtype=float32), -0.48362994]. 
=============================================
[2019-04-01 16:29:27,465] A3C_AGENT_WORKER-Thread-11 INFO:Local step 50500, global step 808096: loss 0.5641
[2019-04-01 16:29:27,468] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 50500, global step 808097: learning rate 0.0010
[2019-04-01 16:29:27,777] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50500, global step 808269: loss 2.3494
[2019-04-01 16:29:27,777] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50500, global step 808269: learning rate 0.0010
[2019-04-01 16:29:27,945] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50500, global step 808374: loss 2.8194
[2019-04-01 16:29:27,948] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50500, global step 808376: learning rate 0.0010
[2019-04-01 16:29:28,125] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50500, global step 808479: loss 1.2645
[2019-04-01 16:29:28,127] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50500, global step 808480: learning rate 0.0010
[2019-04-01 16:29:28,208] A3C_AGENT_WORKER-Thread-9 INFO:Local step 50500, global step 808535: loss 1.0110
[2019-04-01 16:29:28,213] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 50500, global step 808537: learning rate 0.0010
[2019-04-01 16:29:28,348] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50500, global step 808618: loss 1.0463
[2019-04-01 16:29:28,350] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50500, global step 808618: learning rate 0.0010
[2019-04-01 16:29:28,425] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50500, global step 808664: loss 2.3539
[2019-04-01 16:29:28,426] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50500, global step 808664: learning rate 0.0010
[2019-04-01 16:29:28,510] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50500, global step 808716: loss 1.2620
[2019-04-01 16:29:28,512] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50500, global step 808717: learning rate 0.0010
[2019-04-01 16:29:29,012] A3C_AGENT_WORKER-Thread-12 INFO:Local step 50500, global step 808989: loss 1.9037
[2019-04-01 16:29:29,013] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 50500, global step 808989: learning rate 0.0010
[2019-04-01 16:29:31,039] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3021622e-31 0.0000000e+00 3.9929899e-38 0.0000000e+00 0.0000000e+00
 3.8422519e-36 1.0000000e+00], sum to 1.0000
[2019-04-01 16:29:31,041] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5861
[2019-04-01 16:29:31,094] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5333333333333332, 48.66666666666667, 282.6666666666667, 321.6666666666667, 26.0, 24.97950670708583, 7.90925323719387, 0.0, 1.0, 25.40363510378995], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4880400.0000, 
sim time next is 4881000.0000, 
raw observation next is [0.7666666666666667, 47.83333333333334, 282.3333333333333, 335.3333333333333, 26.0, 25.00505203570173, 8.044529041623333, 0.0, 1.0, 21.15196540544941], 
processed observation next is [0.0, 0.4782608695652174, 0.4838411819021238, 0.47833333333333344, 0.941111111111111, 0.3705340699815838, 1.0, 0.8578645765288186, 0.08044529041623333, 0.0, 1.0, 0.1510854671817815], 
reward next is 0.8489, 
noisyNet noise sample is [array([0.59056735], dtype=float32), -0.7571388]. 
=============================================
[2019-04-01 16:29:31,101] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[84.920296]
 [85.18398 ]
 [85.71933 ]
 [85.95503 ]
 [86.29386 ]], R is [[84.69303894]
 [84.66464996]
 [84.65100861]
 [84.66194153]
 [84.69229126]].
[2019-04-01 16:29:36,363] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.6607317e-28 1.2519893e-33 1.9350542e-26 0.0000000e+00 2.5687049e-29
 1.1531379e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 16:29:36,365] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3940
[2019-04-01 16:29:36,379] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 50.83333333333334, 0.0, 0.0, 26.0, 25.43384861903007, 8.384978544494727, 0.0, 1.0, 35.67886612787588], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5028600.0000, 
sim time next is 5029200.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.41415642792969, 8.431851834278573, 0.0, 1.0, 34.06087148474157], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 0.5, 0.0, 0.0, 1.0, 0.9163080611328128, 0.08431851834278574, 0.0, 1.0, 0.24329193917672548], 
reward next is 0.7567, 
noisyNet noise sample is [array([-0.4842481], dtype=float32), -2.3293808]. 
=============================================
[2019-04-01 16:29:38,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:38,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:38,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run7
[2019-04-01 16:29:39,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:39,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:39,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run7
[2019-04-01 16:29:40,168] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:40,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:40,183] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run7
[2019-04-01 16:29:40,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:40,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:40,458] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run7
[2019-04-01 16:29:40,470] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:40,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:40,490] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run7
[2019-04-01 16:29:40,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:40,627] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:40,650] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run7
[2019-04-01 16:29:41,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:41,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:41,268] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run7
[2019-04-01 16:29:41,456] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:41,456] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:41,460] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run7
[2019-04-01 16:29:41,480] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:41,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:41,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run7
[2019-04-01 16:29:41,518] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:41,519] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:41,522] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run7
[2019-04-01 16:29:41,676] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:41,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:41,680] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run7
[2019-04-01 16:29:41,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:41,910] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:41,914] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run7
[2019-04-01 16:29:41,935] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:41,935] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:41,939] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run7
[2019-04-01 16:29:42,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:42,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:42,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run7
[2019-04-01 16:29:42,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:42,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:42,197] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run7
[2019-04-01 16:29:42,269] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:29:42,269] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:29:42,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run7
[2019-04-01 16:29:53,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.7099665e-17 4.6605890e-24 5.4286075e-19 1.3128555e-24 2.6275137e-22
 2.6138901e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:29:53,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6335
[2019-04-01 16:29:53,629] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 23.73530379846254, 5.272150311256188, 0.0, 1.0, 43.54400011028127], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 104400.0000, 
sim time next is 105000.0000, 
raw observation next is [-5.283333333333333, 74.16666666666667, 0.0, 0.0, 26.0, 23.652567773155, 5.280075809372593, 0.0, 1.0, 43.6826714896532], 
processed observation next is [1.0, 0.21739130434782608, 0.31625115420129274, 0.7416666666666667, 0.0, 0.0, 1.0, 0.6646525390221427, 0.05280075809372593, 0.0, 1.0, 0.31201908206895146], 
reward next is 0.6880, 
noisyNet noise sample is [array([1.1144894], dtype=float32), -0.65725154]. 
=============================================
[2019-04-01 16:29:53,632] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[40.006847]
 [40.129677]
 [40.25837 ]
 [40.361458]
 [40.443993]], R is [[40.14825821]
 [40.43574905]
 [40.72105789]
 [41.00432587]
 [41.28553391]].
[2019-04-01 16:31:03,706] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5769518e-32 0.0000000e+00 8.6977693e-38 0.0000000e+00 0.0000000e+00
 7.5735331e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 16:31:03,711] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2017
[2019-04-01 16:31:03,722] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.52556745232907, 9.672190301928907, 0.0, 1.0, 25.53240995230691], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 970800.0000, 
sim time next is 971400.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 26.0, 25.54040670948098, 9.828873131616454, 0.0, 1.0, 28.17813247744289], 
processed observation next is [1.0, 0.21739130434782608, 0.7063711911357342, 0.83, 0.0, 0.0, 1.0, 0.9343438156401399, 0.09828873131616454, 0.0, 1.0, 0.20127237483887778], 
reward next is 0.7987, 
noisyNet noise sample is [array([-0.03679793], dtype=float32), -0.029272052]. 
=============================================
[2019-04-01 16:31:05,390] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1553936e-18 9.3637905e-31 2.5182350e-28 1.7779451e-38 3.1286834e-10
 4.1034104e-32 1.0000000e+00], sum to 1.0000
[2019-04-01 16:31:05,391] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7860
[2019-04-01 16:31:05,406] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.76666666666667, 79.0, 63.16666666666666, 0.0, 26.0, 26.26335343781658, 14.95326055678132, 1.0, 1.0, 9.122045104996193], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1005600.0000, 
sim time next is 1006200.0000, 
raw observation next is [14.95, 78.0, 57.0, 0.0, 26.0, 26.59101030721638, 16.12189224754377, 1.0, 1.0, 9.116039281663378], 
processed observation next is [1.0, 0.6521739130434783, 0.8767313019390582, 0.78, 0.19, 0.0, 1.0, 1.084430043888054, 0.1612189224754377, 1.0, 1.0, 0.06511456629759556], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1832772], dtype=float32), -1.1553153]. 
=============================================
[2019-04-01 16:31:06,575] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 16:31:06,578] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0714
[2019-04-01 16:31:06,592] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.10966033707084, 11.95829109423195, 0.0, 1.0, 45.75959537903682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1024800.0000, 
sim time next is 1025400.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 25.43535733030578, 12.96351271588227, 0.0, 1.0, 40.25977741771784], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 1.0, 0.9193367614722544, 0.1296351271588227, 0.0, 1.0, 0.2875698386979846], 
reward next is 0.7124, 
noisyNet noise sample is [array([-0.501824], dtype=float32), -1.5187733]. 
=============================================
[2019-04-01 16:31:14,568] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.9489803e-35 1.0000000e+00], sum to 1.0000
[2019-04-01 16:31:14,570] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2137
[2019-04-01 16:31:14,574] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 98.0, 28.0, 0.0, 26.0, 23.37614321180921, 5.795288352970256, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1240200.0000, 
sim time next is 1240800.0000, 
raw observation next is [15.0, 98.66666666666666, 35.66666666666666, 0.0, 26.0, 23.36390760841438, 5.787037206314385, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.9866666666666666, 0.11888888888888886, 0.0, 1.0, 0.6234153726306256, 0.05787037206314385, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4210288], dtype=float32), 0.5663131]. 
=============================================
[2019-04-01 16:31:16,626] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6443734e-26 0.0000000e+00 4.1367246e-27 0.0000000e+00 0.0000000e+00
 6.7751063e-35 1.0000000e+00], sum to 1.0000
[2019-04-01 16:31:16,627] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6156
[2019-04-01 16:31:16,653] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 55.33333333333333, 0.0, 26.0, 24.67290609260106, 9.368806309742185, 0.0, 1.0, 5.14344735437885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1264800.0000, 
sim time next is 1265400.0000, 
raw observation next is [13.8, 100.0, 51.0, 0.0, 26.0, 24.65599970240774, 9.313715049373357, 0.0, 1.0, 4.981907479063783], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.17, 0.0, 1.0, 0.8079999574868201, 0.09313715049373357, 0.0, 1.0, 0.035585053421884165], 
reward next is 0.9644, 
noisyNet noise sample is [array([-0.6513277], dtype=float32), -2.6402187]. 
=============================================
[2019-04-01 16:31:22,111] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.9124363e-28 0.0000000e+00 1.8571267e-29 0.0000000e+00 4.9000982e-32
 3.8952752e-37 1.0000000e+00], sum to 1.0000
[2019-04-01 16:31:22,112] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2347
[2019-04-01 16:31:22,128] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.08333333333333331, 95.16666666666667, 0.0, 0.0, 26.0, 25.35725314275003, 10.8328177175618, 0.0, 1.0, 40.12916397072178], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1378200.0000, 
sim time next is 1378800.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.32694993737415, 10.90601970480508, 0.0, 1.0, 41.59967997313503], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.9038499910534499, 0.10906019704805081, 0.0, 1.0, 0.2971405712366788], 
reward next is 0.7029, 
noisyNet noise sample is [array([0.8930942], dtype=float32), -1.5528885]. 
=============================================
[2019-04-01 16:31:23,013] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4147766e-23 1.2476387e-37 8.8388112e-32 0.0000000e+00 1.0655001e-26
 3.1943384e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 16:31:23,018] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6051
[2019-04-01 16:31:23,035] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.34347094536225, 10.70180948012691, 0.0, 1.0, 40.20449058089913], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1382400.0000, 
sim time next is 1383000.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.33800872449854, 10.60591004749954, 0.0, 1.0, 40.0409545653518], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.9054298177855057, 0.1060591004749954, 0.0, 1.0, 0.2860068183239414], 
reward next is 0.7140, 
noisyNet noise sample is [array([-0.4760562], dtype=float32), 0.9235288]. 
=============================================
[2019-04-01 16:31:23,057] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.74092 ]
 [79.339386]
 [79.46811 ]
 [79.57132 ]
 [80.66522 ]], R is [[76.32953644]
 [76.27906036]
 [76.22756195]
 [76.17507172]
 [76.12140656]].
[2019-04-01 16:31:25,282] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.0527961e-18 0.0000000e+00 1.7107330e-31 0.0000000e+00 7.3454689e-22
 2.3849168e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 16:31:25,283] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5729
[2019-04-01 16:31:25,351] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 63.33333333333334, 0.0, 26.0, 24.82267955953295, 9.20945023673925, 1.0, 1.0, 47.40137955599388], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1435800.0000, 
sim time next is 1436400.0000, 
raw observation next is [1.1, 92.0, 59.0, 0.0, 26.0, 25.34469935442954, 10.5656812818, 1.0, 1.0, 27.84358728332735], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.19666666666666666, 0.0, 1.0, 0.9063856220613629, 0.105656812818, 1.0, 1.0, 0.19888276630948107], 
reward next is 0.5748, 
noisyNet noise sample is [array([1.0405847], dtype=float32), 0.75157535]. 
=============================================
[2019-04-01 16:31:39,067] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.3960005e-35 0.0000000e+00 9.9238019e-32 0.0000000e+00 3.9712672e-38
 5.6084646e-31 1.0000000e+00], sum to 1.0000
[2019-04-01 16:31:39,071] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0722
[2019-04-01 16:31:39,104] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.84044612764454, 7.643561741516291, 0.0, 1.0, 43.24834226147474], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1749000.0000, 
sim time next is 1749600.0000, 
raw observation next is [-1.2, 87.0, 0.0, 0.0, 26.0, 24.81208373559778, 7.55797189108745, 0.0, 1.0, 43.30476237768519], 
processed observation next is [0.0, 0.2608695652173913, 0.42936288088642666, 0.87, 0.0, 0.0, 1.0, 0.8302976765139688, 0.0755797189108745, 0.0, 1.0, 0.30931973126917994], 
reward next is 0.6907, 
noisyNet noise sample is [array([-2.1239967], dtype=float32), -0.09917893]. 
=============================================
[2019-04-01 16:31:44,582] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8585643e-30 4.2250427e-36 1.1682101e-28 2.0370492e-37 8.3657288e-34
 4.0123122e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 16:31:44,583] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9004
[2019-04-01 16:31:44,637] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.1, 71.66666666666667, 162.6666666666667, 54.00000000000001, 26.0, 24.90835571599052, 7.336971244528468, 0.0, 1.0, 54.64000532727753], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1857000.0000, 
sim time next is 1857600.0000, 
raw observation next is [-5.0, 71.0, 152.0, 40.5, 26.0, 25.02837768556335, 7.520896081502182, 0.0, 1.0, 50.58713411894147], 
processed observation next is [0.0, 0.5217391304347826, 0.32409972299168976, 0.71, 0.5066666666666667, 0.044751381215469614, 1.0, 0.8611968122233356, 0.07520896081502182, 0.0, 1.0, 0.36133667227815336], 
reward next is 0.6387, 
noisyNet noise sample is [array([0.17858629], dtype=float32), -1.201663]. 
=============================================
[2019-04-01 16:31:58,686] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2170470e-24 1.0196302e-37 2.7221386e-25 2.4739615e-34 3.8242983e-27
 2.3365526e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 16:31:58,686] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5890
[2019-04-01 16:31:58,709] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.25664758887367, 5.675457237457356, 0.0, 1.0, 41.13737016345753], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1995600.0000, 
sim time next is 1996200.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.32991358789015, 5.680015907257719, 0.0, 1.0, 41.05504582077731], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 1.0, 0.76141622684145, 0.05680015907257719, 0.0, 1.0, 0.2932503272912665], 
reward next is 0.7067, 
noisyNet noise sample is [array([-0.8942486], dtype=float32), 1.2928557]. 
=============================================
[2019-04-01 16:32:05,537] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.2342357e-21 1.3539756e-34 3.8821135e-24 7.0112757e-33 1.4207228e-30
 1.1582914e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:32:05,537] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7967
[2019-04-01 16:32:05,557] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 91.0, 0.0, 0.0, 26.0, 24.76844208739948, 7.054726576612739, 0.0, 1.0, 42.73534470290401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2074800.0000, 
sim time next is 2075400.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 26.0, 24.85716888253618, 7.028032936489677, 0.0, 1.0, 41.74150133122776], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 1.0, 0.8367384117908827, 0.07028032936489677, 0.0, 1.0, 0.2981535809373411], 
reward next is 0.7018, 
noisyNet noise sample is [array([0.62505496], dtype=float32), -1.2330823]. 
=============================================
[2019-04-01 16:32:11,252] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 3.2539433e-36 1.0000000e+00], sum to 1.0000
[2019-04-01 16:32:11,253] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5940
[2019-04-01 16:32:11,298] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.2, 77.0, 0.0, 0.0, 26.0, 25.38236214254214, 8.980507983814872, 0.0, 1.0, 27.26904845793065], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2143200.0000, 
sim time next is 2143800.0000, 
raw observation next is [-5.3, 78.5, 0.0, 0.0, 26.0, 25.34200348503941, 8.516665866195801, 0.0, 1.0, 29.87218798049474], 
processed observation next is [1.0, 0.8260869565217391, 0.31578947368421056, 0.785, 0.0, 0.0, 1.0, 0.9060004978627728, 0.08516665866195801, 0.0, 1.0, 0.21337277128924814], 
reward next is 0.7866, 
noisyNet noise sample is [array([-0.06537613], dtype=float32), 0.5192683]. 
=============================================
[2019-04-01 16:32:11,531] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0256330e-31 0.0000000e+00 5.7337248e-36 0.0000000e+00 3.8554279e-33
 1.4814221e-13 1.0000000e+00], sum to 1.0000
[2019-04-01 16:32:11,531] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6497
[2019-04-01 16:32:11,550] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 26.0, 23.95543194737105, 5.313451512674543, 0.0, 1.0, 41.44721087605692], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2178000.0000, 
sim time next is 2178600.0000, 
raw observation next is [-6.2, 75.66666666666667, 0.0, 0.0, 26.0, 23.9294550904313, 5.288054691456293, 0.0, 1.0, 41.43361243151635], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7566666666666667, 0.0, 0.0, 1.0, 0.7042078700616143, 0.052880546914562926, 0.0, 1.0, 0.29595437451083106], 
reward next is 0.7040, 
noisyNet noise sample is [array([-1.5350208], dtype=float32), -0.52056086]. 
=============================================
[2019-04-01 16:32:12,720] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.2723404e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6119743e-36
 1.0961856e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 16:32:12,720] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4679
[2019-04-01 16:32:12,810] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 68.0, 138.5, 142.5, 26.0, 24.82459602470372, 8.239179312670679, 1.0, 1.0, 70.15667331726291], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2214000.0000, 
sim time next is 2214600.0000, 
raw observation next is [-3.9, 68.0, 144.6666666666667, 190.0, 26.0, 25.3472329328687, 9.762304431223567, 1.0, 1.0, 40.89441391365693], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.68, 0.4822222222222224, 0.20994475138121546, 1.0, 0.9067475618383857, 0.09762304431223567, 1.0, 1.0, 0.29210295652612095], 
reward next is 0.7079, 
noisyNet noise sample is [array([-0.25920746], dtype=float32), 1.3447964]. 
=============================================
[2019-04-01 16:32:13,542] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3806751e-19 4.9405764e-35 1.9811503e-30 0.0000000e+00 1.3944176e-24
 6.4691326e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 16:32:13,542] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5822
[2019-04-01 16:32:13,585] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.816666666666666, 67.5, 133.0, 0.0, 26.0, 25.5463599331814, 7.943425618899433, 1.0, 1.0, 28.35488899659002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2203800.0000, 
sim time next is 2204400.0000, 
raw observation next is [-3.733333333333333, 67.0, 130.5, 0.0, 26.0, 25.58289405272129, 7.856106045680036, 1.0, 1.0, 28.19191613373187], 
processed observation next is [1.0, 0.5217391304347826, 0.35918744228993543, 0.67, 0.435, 0.0, 1.0, 0.9404134361030414, 0.07856106045680036, 1.0, 1.0, 0.20137082952665622], 
reward next is 0.7986, 
noisyNet noise sample is [array([-0.9199549], dtype=float32), -0.6235663]. 
=============================================
[2019-04-01 16:32:29,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.6255042e-29 4.8736118e-38 4.2485432e-28 1.3444511e-38 3.6933261e-35
 6.6127641e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 16:32:29,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5317
[2019-04-01 16:32:29,847] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333333, 42.0, 0.0, 0.0, 26.0, 24.83837251414937, 6.259549737714686, 0.0, 1.0, 42.43789139804912], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2414400.0000, 
sim time next is 2415000.0000, 
raw observation next is [-4.916666666666667, 41.5, 0.0, 0.0, 26.0, 24.78822056256034, 6.173139382819429, 0.0, 1.0, 42.46113876903308], 
processed observation next is [0.0, 0.9565217391304348, 0.32640812557710064, 0.415, 0.0, 0.0, 1.0, 0.8268886517943344, 0.06173139382819429, 0.0, 1.0, 0.30329384835023626], 
reward next is 0.6967, 
noisyNet noise sample is [array([-0.65035826], dtype=float32), 1.1405439]. 
=============================================
[2019-04-01 16:32:29,855] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[65.863525]
 [65.96062 ]
 [66.04163 ]
 [66.11442 ]
 [66.17197 ]], R is [[65.79555511]
 [65.83447266]
 [65.87316895]
 [65.91170502]
 [65.94997406]].
[2019-04-01 16:32:30,244] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7159398e-26 0.0000000e+00 9.8707311e-37 0.0000000e+00 2.0113604e-36
 4.4624709e-34 1.0000000e+00], sum to 1.0000
[2019-04-01 16:32:30,244] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8257
[2019-04-01 16:32:30,262] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 25.83333333333334, 52.66666666666666, 412.6666666666666, 26.0, 24.95634822424543, 7.121932686098589, 0.0, 1.0, 18.29539871586774], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2477400.0000, 
sim time next is 2478000.0000, 
raw observation next is [3.3, 25.66666666666667, 50.33333333333333, 345.8333333333333, 26.0, 24.93889644244733, 7.067242285668025, 0.0, 1.0, 20.18380518861052], 
processed observation next is [0.0, 0.6956521739130435, 0.554016620498615, 0.2566666666666667, 0.16777777777777778, 0.38213627992633514, 1.0, 0.8484137774924757, 0.07067242285668024, 0.0, 1.0, 0.14417003706150372], 
reward next is 0.8558, 
noisyNet noise sample is [array([-1.0559788], dtype=float32), -0.9169457]. 
=============================================
[2019-04-01 16:32:30,277] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.79742]
 [73.08732]
 [73.33176]
 [73.52319]
 [73.71708]], R is [[72.5927124 ]
 [72.73609924]
 [72.89291382]
 [73.05973053]
 [73.23156738]].
[2019-04-01 16:32:33,294] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1109770e-30 0.0000000e+00 1.2844957e-30 0.0000000e+00 2.4910048e-36
 2.2123829e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 16:32:33,295] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2403
[2019-04-01 16:32:33,338] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 32.66666666666666, 86.66666666666667, 831.6666666666667, 26.0, 25.02529174923292, 7.075650798315152, 0.0, 1.0, 21.80840137571056], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2461200.0000, 
sim time next is 2461800.0000, 
raw observation next is [-0.8833333333333332, 31.83333333333334, 87.33333333333334, 834.3333333333334, 26.0, 25.06250190836294, 7.039579781486931, 0.0, 1.0, 24.54529527712098], 
processed observation next is [0.0, 0.4782608695652174, 0.43813481071098803, 0.3183333333333334, 0.29111111111111115, 0.921915285451197, 1.0, 0.8660717011947057, 0.07039579781486931, 0.0, 1.0, 0.1753235376937213], 
reward next is 0.8247, 
noisyNet noise sample is [array([-0.72130346], dtype=float32), -0.07456697]. 
=============================================
[2019-04-01 16:32:36,094] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 3.8798944e-38 0.0000000e+00 0.0000000e+00
 2.2329467e-37 1.0000000e+00], sum to 1.0000
[2019-04-01 16:32:36,095] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4343
[2019-04-01 16:32:36,122] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 25.33333333333333, 41.0, 239.6666666666667, 26.0, 25.04403469014535, 7.160442817191639, 0.0, 1.0, 19.70300563179645], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2479200.0000, 
sim time next is 2479800.0000, 
raw observation next is [3.3, 25.16666666666667, 34.0, 200.3333333333334, 26.0, 25.07544328365601, 7.125702886263793, 0.0, 1.0, 15.87388561302973], 
processed observation next is [0.0, 0.6956521739130435, 0.554016620498615, 0.2516666666666667, 0.11333333333333333, 0.2213627992633518, 1.0, 0.8679204690937158, 0.07125702886263792, 0.0, 1.0, 0.11338489723592664], 
reward next is 0.8866, 
noisyNet noise sample is [array([-1.649924], dtype=float32), 1.8516774]. 
=============================================
[2019-04-01 16:33:02,679] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1537753e-22 1.1314670e-31 2.4465497e-24 1.6186473e-28 1.7528662e-22
 2.1819916e-01 7.8180081e-01], sum to 1.0000
[2019-04-01 16:33:02,684] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4933
[2019-04-01 16:33:02,699] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 84.5, 0.0, 0.0, 26.0, 24.67742003859953, 6.883314973708711, 0.0, 1.0, 42.39518313837165], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2946600.0000, 
sim time next is 2947200.0000, 
raw observation next is [-2.666666666666667, 84.33333333333334, 0.0, 0.0, 26.0, 24.6374081049131, 6.891135620091229, 0.0, 1.0, 42.32265955786653], 
processed observation next is [0.0, 0.08695652173913043, 0.38873499538319484, 0.8433333333333334, 0.0, 0.0, 1.0, 0.8053440149875856, 0.0689113562009123, 0.0, 1.0, 0.3023047111276181], 
reward next is 0.6977, 
noisyNet noise sample is [array([1.0403521], dtype=float32), 0.5181804]. 
=============================================
[2019-04-01 16:33:06,697] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.2628271e-37 0.0000000e+00 0.0000000e+00
 2.2033340e-35 1.0000000e+00], sum to 1.0000
[2019-04-01 16:33:06,698] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1646
[2019-04-01 16:33:06,770] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 24.9935064818291, 7.895698411074197, 0.0, 1.0, 34.19354089892491], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3004200.0000, 
sim time next is 3004800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 24.98692731846055, 7.917093084704464, 0.0, 1.0, 35.49466613577644], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6, 0.0, 0.0, 1.0, 0.8552753312086497, 0.07917093084704464, 0.0, 1.0, 0.2535333295412603], 
reward next is 0.7465, 
noisyNet noise sample is [array([0.33966735], dtype=float32), -0.25899532]. 
=============================================
[2019-04-01 16:33:08,168] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.5550037e-29 0.0000000e+00 2.9301248e-33 0.0000000e+00 1.1463372e-34
 6.8896526e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:33:08,172] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8308
[2019-04-01 16:33:08,189] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.25891430566864, 5.501487851690196, 0.0, 1.0, 38.96065428152622], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3034800.0000, 
sim time next is 3035400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.22395888626292, 5.484179146352538, 0.0, 1.0, 39.10060519476693], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.7462798408947029, 0.054841791463525386, 0.0, 1.0, 0.27929003710547806], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.6177453], dtype=float32), -2.3101058]. 
=============================================
[2019-04-01 16:33:12,820] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2753350e-32 0.0000000e+00 1.4300879e-35 0.0000000e+00 9.0184905e-28
 4.3358039e-31 1.0000000e+00], sum to 1.0000
[2019-04-01 16:33:12,821] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3331
[2019-04-01 16:33:12,838] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 26.0, 25.25493715859504, 15.01207295071629, 0.0, 1.0, 60.59725531510066], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3184200.0000, 
sim time next is 3184800.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 26.0, 25.42164928226555, 16.42773693348199, 0.0, 1.0, 43.64622558103427], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 1.0, 0.9173784688950788, 0.16427736933481993, 0.0, 1.0, 0.3117587541502448], 
reward next is 0.6882, 
noisyNet noise sample is [array([0.5823995], dtype=float32), -2.0549777]. 
=============================================
[2019-04-01 16:33:14,081] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-01 16:33:14,082] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 16:33:14,083] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 16:33:14,083] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:33:14,083] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 16:33:14,083] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:33:14,083] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:33:14,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run10
[2019-04-01 16:33:14,103] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run10
[2019-04-01 16:33:14,119] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run10
[2019-04-01 16:33:34,258] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.7302999], dtype=float32), -0.48839]
[2019-04-01 16:33:34,259] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.133333333333333, 31.33333333333334, 86.0, 0.0, 26.0, 25.31046370053168, 6.078963701832777, 1.0, 1.0, 24.6230944334144]
[2019-04-01 16:33:34,259] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:33:34,259] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.5309969e-23 2.2700292e-32 7.3429607e-26 2.0946549e-35 3.9131514e-20
 6.3797687e-12 1.0000000e+00], sampled 0.3979580320641525
[2019-04-01 16:34:18,161] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.7302999], dtype=float32), -0.48839]
[2019-04-01 16:34:18,161] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.074725962, 85.323724555, 0.0, 0.0, 26.0, 24.09702608955579, 5.626703577344796, 0.0, 1.0, 44.29699868832668]
[2019-04-01 16:34:18,161] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 16:34:18,162] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [9.6039526e-26 1.9224127e-35 2.9790481e-27 2.2438957e-35 1.5352913e-27
 7.2953694e-15 1.0000000e+00], sampled 0.32866405842827795
[2019-04-01 16:34:50,085] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.7302999], dtype=float32), -0.48839]
[2019-04-01 16:34:50,085] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [8.466666666666667, 26.33333333333334, 0.0, 0.0, 26.0, 25.54211136764308, 7.94541790109398, 0.0, 1.0, 27.12358076551863]
[2019-04-01 16:34:50,086] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:34:50,086] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.2157320e-31 0.0000000e+00 8.9096594e-33 0.0000000e+00 1.8979460e-33
 5.9597988e-25 1.0000000e+00], sampled 0.861213403951305
[2019-04-01 16:34:55,454] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 16:35:16,025] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 16:35:18,650] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 16:35:19,674] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 900000, evaluation results [900000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 16:35:30,678] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.908121e-33 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-04-01 16:35:30,678] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2102
[2019-04-01 16:35:30,697] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 72.5, 608.5, 26.0, 26.33282900960314, 14.29323037560203, 1.0, 1.0, 9.954266262531311], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3427200.0000, 
sim time next is 3427800.0000, 
raw observation next is [2.0, 67.0, 68.66666666666666, 576.6666666666667, 26.0, 25.69971192881965, 12.75282932442795, 1.0, 1.0, 12.17100077377975], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.22888888888888886, 0.6372007366482505, 1.0, 0.9571017041170927, 0.1275282932442795, 1.0, 1.0, 0.0869357198127125], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.06007062], dtype=float32), 0.03850795]. 
=============================================
[2019-04-01 16:35:31,367] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.4258828e-28 0.0000000e+00 1.1512602e-31 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:35:31,367] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1777
[2019-04-01 16:35:31,384] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 12.0, 121.0, 26.0, 26.22976420775728, 11.89018030224785, 1.0, 1.0, 9.985666634130947], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3432600.0000, 
sim time next is 3433200.0000, 
raw observation next is [2.0, 67.0, 10.0, 100.8333333333333, 26.0, 26.00154523070689, 12.83623465625308, 1.0, 1.0, 9.654451322235847], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.03333333333333333, 0.11141804788213625, 1.0, 1.0002207472438411, 0.1283623465625308, 1.0, 1.0, 0.0689603665873989], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.22672951], dtype=float32), 0.227913]. 
=============================================
[2019-04-01 16:35:52,866] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0121025e-12 8.0795457e-24 1.9871895e-16 7.6651172e-27 6.8076672e-10
 4.6543350e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 16:35:52,866] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4901
[2019-04-01 16:35:52,878] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.0, 110.5, 797.0, 26.0, 26.63330025972543, 14.97119432935576, 1.0, 1.0, 8.15300345438253], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3765600.0000, 
sim time next is 3766200.0000, 
raw observation next is [0.0, 60.0, 109.0, 790.0, 26.0, 26.71555981438826, 15.03285793620284, 1.0, 1.0, 8.329314837996137], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.6, 0.36333333333333334, 0.8729281767955801, 1.0, 1.1022228306268944, 0.1503285793620284, 1.0, 1.0, 0.05949510598568669], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4872817], dtype=float32), -0.2664141]. 
=============================================
[2019-04-01 16:35:54,024] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0232252e-15 2.6174789e-30 7.0155690e-22 6.6071680e-26 3.7347158e-17
 1.0000000e+00 2.6278150e-14], sum to 1.0000
[2019-04-01 16:35:54,025] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0998
[2019-04-01 16:35:54,043] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 75.0, 0.0, 0.0, 26.0, 25.28805099910114, 7.999679632037766, 0.0, 1.0, 40.31556381459887], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3904800.0000, 
sim time next is 3905400.0000, 
raw observation next is [-3.833333333333333, 76.0, 0.0, 0.0, 26.0, 25.23286690678863, 7.876273448919473, 0.0, 1.0, 40.43832442167262], 
processed observation next is [1.0, 0.17391304347826086, 0.3564173591874424, 0.76, 0.0, 0.0, 1.0, 0.8904095581126613, 0.07876273448919473, 0.0, 1.0, 0.2888451744405187], 
reward next is 0.7112, 
noisyNet noise sample is [array([-0.26552877], dtype=float32), -0.35008377]. 
=============================================
[2019-04-01 16:35:58,323] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1470635e-26 2.9699035e-34 7.6060396e-29 0.0000000e+00 4.6631808e-26
 2.3003573e-31 1.0000000e+00], sum to 1.0000
[2019-04-01 16:35:58,323] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0117
[2019-04-01 16:35:58,355] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.166666666666667, 38.0, 99.33333333333334, 766.6666666666666, 26.0, 25.84243400446664, 15.45365367726527, 1.0, 1.0, 30.04862234544819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3941400.0000, 
sim time next is 3942000.0000, 
raw observation next is [-4.0, 38.0, 96.5, 756.0, 26.0, 26.51915036506743, 17.19863732171244, 1.0, 1.0, 21.12558368076223], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.38, 0.32166666666666666, 0.8353591160220994, 1.0, 1.0741643378667756, 0.1719863732171244, 1.0, 1.0, 0.15089702629115878], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.04922734], dtype=float32), 0.44948015]. 
=============================================
[2019-04-01 16:35:58,362] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[29.148262]
 [28.235039]
 [27.560192]
 [26.744987]
 [25.6207  ]], R is [[29.6892643 ]
 [29.39237213]
 [29.0984478 ]
 [28.80746269]
 [28.94402695]].
[2019-04-01 16:35:58,450] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7324690e-04 1.9714231e-20 9.6912265e-18 5.0760290e-21 2.7695183e-02
 1.6086036e-05 9.7211546e-01], sum to 1.0000
[2019-04-01 16:35:58,452] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6844
[2019-04-01 16:35:58,505] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.48823513033029, 11.07483169168219, 0.0, 1.0, 40.14895709906683], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3961200.0000, 
sim time next is 3961800.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.46296744162429, 11.19591270604464, 0.0, 1.0, 42.30406920008222], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 1.0, 0.9232810630891842, 0.11195912706044639, 0.0, 1.0, 0.30217192285773015], 
reward next is 0.6978, 
noisyNet noise sample is [array([-1.3028607], dtype=float32), 0.14636761]. 
=============================================
[2019-04-01 16:36:08,877] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7519797e-13 3.3092960e-22 1.7930446e-16 1.4256395e-25 7.8021513e-11
 9.2541711e-08 9.9999988e-01], sum to 1.0000
[2019-04-01 16:36:08,877] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1609
[2019-04-01 16:36:08,897] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 34.5, 110.6666666666667, 739.0, 26.0, 26.61271911576961, 12.89821477513149, 1.0, 1.0, 13.02029875239174], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4097400.0000, 
sim time next is 4098000.0000, 
raw observation next is [-1.666666666666667, 34.0, 112.3333333333333, 754.0, 26.0, 26.67805986623258, 13.02387380617739, 1.0, 1.0, 12.157659712547], 
processed observation next is [1.0, 0.43478260869565216, 0.4164358264081256, 0.34, 0.37444444444444436, 0.8331491712707182, 1.0, 1.096865695176083, 0.13023873806177388, 1.0, 1.0, 0.08684042651819286], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.05378187], dtype=float32), 1.2246919]. 
=============================================
[2019-04-01 16:36:08,904] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[35.784447]
 [35.332886]
 [34.735367]
 [34.189205]
 [33.695904]], R is [[35.86830521]
 [35.50962067]
 [35.15452576]
 [34.91865921]
 [34.81549072]].
[2019-04-01 16:36:10,944] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.1872035e-22 1.6720549e-30 1.9739004e-23 1.3775869e-34 8.2735829e-30
 4.9864438e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:10,948] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7373
[2019-04-01 16:36:10,963] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 46.0, 0.0, 0.0, 26.0, 25.33563723609276, 8.406230927324733, 0.0, 1.0, 37.76306808306854], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4154400.0000, 
sim time next is 4155000.0000, 
raw observation next is [-2.166666666666667, 46.66666666666667, 0.0, 0.0, 26.0, 25.29206578992797, 8.310103810306023, 0.0, 1.0, 39.2756380811003], 
processed observation next is [0.0, 0.08695652173913043, 0.4025854108956602, 0.46666666666666673, 0.0, 0.0, 1.0, 0.8988665414182816, 0.08310103810306023, 0.0, 1.0, 0.28054027200785925], 
reward next is 0.7195, 
noisyNet noise sample is [array([-0.2949914], dtype=float32), -0.24457546]. 
=============================================
[2019-04-01 16:36:10,971] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[64.73243 ]
 [64.737526]
 [64.52236 ]
 [64.2497  ]
 [63.786106]], R is [[64.71374512]
 [64.79686737]
 [64.89045715]
 [64.97962189]
 [65.0646286 ]].
[2019-04-01 16:36:14,384] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.266839e-37 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-04-01 16:36:14,384] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6521
[2019-04-01 16:36:14,394] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 54.5, 188.0, 717.0, 26.0, 25.30513645549389, 9.247104638744268, 0.0, 1.0, 4.346935780260324], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4282200.0000, 
sim time next is 4282800.0000, 
raw observation next is [7.0, 55.33333333333334, 194.8333333333333, 661.6666666666666, 26.0, 25.31060498021708, 9.262767829262131, 0.0, 1.0, 3.587980320010681], 
processed observation next is [0.0, 0.5652173913043478, 0.6565096952908588, 0.5533333333333335, 0.6494444444444443, 0.731123388581952, 1.0, 0.9015149971738684, 0.09262767829262131, 0.0, 1.0, 0.02562843085721915], 
reward next is 0.9744, 
noisyNet noise sample is [array([-1.6051772], dtype=float32), 0.62824]. 
=============================================
[2019-04-01 16:36:26,747] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.4606246e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4671865e-33
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:26,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0762
[2019-04-01 16:36:26,809] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 255.5, 80.5, 26.0, 25.57368461629969, 9.578442185220874, 1.0, 1.0, 7.621887378905676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4543200.0000, 
sim time next is 4543800.0000, 
raw observation next is [3.0, 48.33333333333334, 258.3333333333334, 91.33333333333334, 26.0, 24.83258449825621, 9.107261091527066, 1.0, 1.0, 43.1805900351749], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.48333333333333345, 0.8611111111111114, 0.10092081031307552, 1.0, 0.8332263568937444, 0.09107261091527066, 1.0, 1.0, 0.30843278596553503], 
reward next is 0.6916, 
noisyNet noise sample is [array([0.5458993], dtype=float32), 0.55005103]. 
=============================================
[2019-04-01 16:36:27,114] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7442151e-25 2.1082249e-29 1.9957591e-23 9.6297957e-30 1.0226235e-26
 9.1223802e-09 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:27,117] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9351
[2019-04-01 16:36:27,129] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.31807127021447, 8.494286143149568, 0.0, 1.0, 40.03306813050544], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4517400.0000, 
sim time next is 4518000.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.33017551424446, 8.31632974187884, 0.0, 1.0, 39.99009418658081], 
processed observation next is [1.0, 0.30434782608695654, 0.4349030470914128, 0.71, 0.0, 0.0, 1.0, 0.9043107877492085, 0.0831632974187884, 0.0, 1.0, 0.28564352990414865], 
reward next is 0.7144, 
noisyNet noise sample is [array([0.38452896], dtype=float32), 0.25923494]. 
=============================================
[2019-04-01 16:36:27,135] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[67.60544 ]
 [67.61745 ]
 [67.67029 ]
 [67.745636]
 [67.809   ]], R is [[67.67175293]
 [67.70908356]
 [67.74531555]
 [67.78046417]
 [67.81476593]].
[2019-04-01 16:36:31,265] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1293742e-24 2.0306294e-33 7.3794617e-27 4.5396167e-33 5.1313021e-25
 8.8256380e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:31,266] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3565
[2019-04-01 16:36:31,277] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 25.29742844929308, 8.415273245293214, 0.0, 1.0, 36.49719831439355], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4596000.0000, 
sim time next is 4596600.0000, 
raw observation next is [-1.916666666666667, 70.66666666666667, 0.0, 0.0, 26.0, 25.28291960623471, 8.417019611811943, 0.0, 1.0, 36.13188355972068], 
processed observation next is [1.0, 0.17391304347826086, 0.4095106186518929, 0.7066666666666667, 0.0, 0.0, 1.0, 0.8975599437478158, 0.08417019611811943, 0.0, 1.0, 0.2580848825694334], 
reward next is 0.7419, 
noisyNet noise sample is [array([-1.6178463], dtype=float32), -0.47559202]. 
=============================================
[2019-04-01 16:36:34,786] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2253939e-30 0.0000000e+00 9.9274070e-38 0.0000000e+00 1.6251361e-38
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:34,786] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8373
[2019-04-01 16:36:34,802] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 125.5, 0.9999999999999998, 26.0, 26.12970978056912, 10.5949565946815, 1.0, 1.0, 19.74835591672559], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4710000.0000, 
sim time next is 4710600.0000, 
raw observation next is [1.0, 86.0, 108.0, 0.0, 26.0, 26.06179688251414, 10.18987200943266, 1.0, 1.0, 19.95904435342564], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.36, 0.0, 1.0, 1.0088281260734486, 0.1018987200943266, 1.0, 1.0, 0.14256460252446884], 
reward next is 0.7815, 
noisyNet noise sample is [array([0.9781049], dtype=float32), -1.1258329]. 
=============================================
[2019-04-01 16:36:35,024] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.8521061e-32 0.0000000e+00 5.3047409e-32 0.0000000e+00 2.4242414e-27
 3.5956924e-28 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:35,024] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7016
[2019-04-01 16:36:35,054] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 79.16666666666667, 0.0, 0.0, 26.0, 25.06818279265031, 8.64635539777847, 0.0, 1.0, 19.6251508829157], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4738200.0000, 
sim time next is 4738800.0000, 
raw observation next is [-1.333333333333333, 80.33333333333334, 0.0, 0.0, 26.0, 24.94462248964647, 8.791875979305908, 0.0, 1.0, 40.58461816460258], 
processed observation next is [1.0, 0.8695652173913043, 0.42566943674976926, 0.8033333333333335, 0.0, 0.0, 1.0, 0.84923178423521, 0.08791875979305909, 0.0, 1.0, 0.2898901297471613], 
reward next is 0.7101, 
noisyNet noise sample is [array([-0.75673044], dtype=float32), -1.8614485]. 
=============================================
[2019-04-01 16:36:35,500] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9425345e-23 0.0000000e+00 4.3288294e-31 0.0000000e+00 1.0868336e-35
 2.8022844e-35 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:35,502] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5237
[2019-04-01 16:36:35,530] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 72.16666666666666, 179.6666666666667, 4.0, 26.0, 26.01876156004554, 11.35941688605954, 1.0, 1.0, 19.42550173318981], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4719000.0000, 
sim time next is 4719600.0000, 
raw observation next is [1.0, 72.0, 171.5, 3.0, 26.0, 26.11958393738594, 11.62867920602663, 1.0, 1.0, 18.01828953027734], 
processed observation next is [1.0, 0.6521739130434783, 0.4903047091412743, 0.72, 0.5716666666666667, 0.0033149171270718232, 1.0, 1.0170834196265628, 0.1162867920602663, 1.0, 1.0, 0.12870206807340956], 
reward next is 0.2198, 
noisyNet noise sample is [array([-0.40294608], dtype=float32), -0.49452737]. 
=============================================
[2019-04-01 16:36:37,072] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2230383e-28 0.0000000e+00 1.3999182e-31 0.0000000e+00 3.5612281e-26
 1.0972637e-37 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:37,073] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3641
[2019-04-01 16:36:37,103] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.14789327725634, 10.37318249029573, 1.0, 1.0, 29.46499465253671], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4734600.0000, 
sim time next is 4735200.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.40020807043004, 10.34045271856985, 1.0, 1.0, 23.95858752449513], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 1.0, 0.9143154386328628, 0.1034045271856985, 1.0, 1.0, 0.17113276803210806], 
reward next is 0.6927, 
noisyNet noise sample is [array([-0.26964447], dtype=float32), -0.20090023]. 
=============================================
[2019-04-01 16:36:39,649] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.6368146e-28 0.0000000e+00 6.5958962e-38 0.0000000e+00 7.6484626e-36
 3.1152530e-38 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:39,651] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1149
[2019-04-01 16:36:39,666] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 25.29971238992487, 10.81681885102911, 0.0, 1.0, 43.13585704130449], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4741200.0000, 
sim time next is 4741800.0000, 
raw observation next is [-2.166666666666667, 84.83333333333334, 0.0, 0.0, 26.0, 25.37582746951998, 10.93810881402526, 0.0, 1.0, 42.8015244263968], 
processed observation next is [1.0, 0.9130434782608695, 0.4025854108956602, 0.8483333333333334, 0.0, 0.0, 1.0, 0.9108324956457113, 0.10938108814025259, 0.0, 1.0, 0.3057251744742629], 
reward next is 0.6943, 
noisyNet noise sample is [array([-0.8614034], dtype=float32), 0.0957995]. 
=============================================
[2019-04-01 16:36:40,157] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3240963e-19 5.7846985e-27 1.2596049e-20 7.4185570e-26 4.7494313e-16
 2.6677037e-08 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:40,158] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3000
[2019-04-01 16:36:40,215] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 67.0, 167.0, 524.0, 26.0, 25.70199209103711, 10.20263363100962, 0.0, 1.0, 28.8076535873746], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4787400.0000, 
sim time next is 4788000.0000, 
raw observation next is [-3.0, 65.0, 163.5, 575.5, 26.0, 25.65879192417839, 10.09394708456038, 0.0, 1.0, 26.96941796594264], 
processed observation next is [0.0, 0.43478260869565216, 0.3795013850415513, 0.65, 0.545, 0.6359116022099448, 1.0, 0.9512559891683416, 0.1009394708456038, 0.0, 1.0, 0.19263869975673314], 
reward next is 0.8074, 
noisyNet noise sample is [array([-0.67773193], dtype=float32), -0.48770666]. 
=============================================
[2019-04-01 16:36:40,234] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[62.074486]
 [61.441765]
 [60.811996]
 [60.092857]
 [59.347004]], R is [[63.2273407 ]
 [63.38929749]
 [63.53506851]
 [63.66381454]
 [63.77455902]].
[2019-04-01 16:36:43,084] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.98134843e-32 0.00000000e+00 1.04682884e-29 0.00000000e+00
 0.00000000e+00 0.00000000e+00 1.00000000e+00], sum to 1.0000
[2019-04-01 16:36:43,086] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7957
[2019-04-01 16:36:43,095] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.9, 44.16666666666667, 260.0, 383.3333333333333, 26.0, 25.09466742568253, 8.294645734679982, 0.0, 1.0, 12.13085439177112], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4888200.0000, 
sim time next is 4888800.0000, 
raw observation next is [2.0, 44.0, 254.0, 381.0, 26.0, 25.07978674260371, 8.300776242212274, 0.0, 1.0, 12.44553418700141], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.44, 0.8466666666666667, 0.42099447513812155, 1.0, 0.8685409632291012, 0.08300776242212274, 0.0, 1.0, 0.08889667276429579], 
reward next is 0.9111, 
noisyNet noise sample is [array([-0.7827765], dtype=float32), 1.8892168]. 
=============================================
[2019-04-01 16:36:47,795] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.2573092e-28 2.1173258e-35 8.7606297e-25 2.0706372e-36 1.4051886e-23
 1.0994043e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:47,799] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1068
[2019-04-01 16:36:47,814] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 46.66666666666667, 0.0, 0.0, 26.0, 25.18401261588588, 6.579800157898513, 0.0, 1.0, 38.1183795472137], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4943400.0000, 
sim time next is 4944000.0000, 
raw observation next is [-2.333333333333333, 47.33333333333334, 0.0, 0.0, 26.0, 25.17082007148334, 6.553920298705035, 0.0, 1.0, 38.12004025953181], 
processed observation next is [1.0, 0.21739130434782608, 0.3979686057248385, 0.47333333333333344, 0.0, 0.0, 1.0, 0.8815457244976201, 0.06553920298705035, 0.0, 1.0, 0.27228600185379864], 
reward next is 0.7277, 
noisyNet noise sample is [array([-0.5571347], dtype=float32), -0.655583]. 
=============================================
[2019-04-01 16:36:47,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[67.298546]
 [67.36667 ]
 [67.38351 ]
 [67.371025]
 [67.34713 ]], R is [[67.27258301]
 [67.32758331]
 [67.38309479]
 [67.43772125]
 [67.49200439]].
[2019-04-01 16:36:49,396] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0821197e-31 0.0000000e+00 3.8911375e-34 0.0000000e+00 0.0000000e+00
 1.1921048e-37 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:49,396] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1455
[2019-04-01 16:36:49,435] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 38.66666666666667, 0.0, 0.0, 26.0, 25.52327586591574, 8.739177511822186, 0.0, 1.0, 32.50140544174755], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4911600.0000, 
sim time next is 4912200.0000, 
raw observation next is [1.0, 38.0, 0.0, 0.0, 26.0, 25.54952829943213, 8.631705113609257, 0.0, 1.0, 28.66200320485481], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.38, 0.0, 0.0, 1.0, 0.9356468999188756, 0.08631705113609257, 0.0, 1.0, 0.2047285943203915], 
reward next is 0.7953, 
noisyNet noise sample is [array([-2.4581268], dtype=float32), -0.45466554]. 
=============================================
[2019-04-01 16:36:50,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9848343e-22 2.2301907e-35 4.7128781e-26 1.6506758e-33 3.7087460e-26
 5.9436183e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:50,839] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2513
[2019-04-01 16:36:50,849] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.43199039753186, 8.417976007674227, 0.0, 1.0, 34.24505095953156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5023800.0000, 
sim time next is 5024400.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.39564478424237, 8.467617070775928, 0.0, 1.0, 36.14502763731977], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.55, 0.0, 0.0, 1.0, 0.913663540606053, 0.08467617070775928, 0.0, 1.0, 0.25817876883799834], 
reward next is 0.7418, 
noisyNet noise sample is [array([-0.30889314], dtype=float32), 0.0020399177]. 
=============================================
[2019-04-01 16:36:53,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:53,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:53,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:53,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:53,398] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run8
[2019-04-01 16:36:53,446] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run8
[2019-04-01 16:36:53,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:53,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:53,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run8
[2019-04-01 16:36:54,171] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:54,172] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:54,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run8
[2019-04-01 16:36:54,298] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2421035e-19 1.7812978e-24 1.3904062e-18 2.7270713e-28 2.0328073e-18
 2.1432429e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 16:36:54,298] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7876
[2019-04-01 16:36:54,326] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 46.00000000000001, 107.0, 643.0, 26.0, 26.40400482532423, 12.55517109506354, 1.0, 1.0, 13.81017577203654], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5044200.0000, 
sim time next is 5044800.0000, 
raw observation next is [1.666666666666667, 45.0, 109.5, 670.5, 26.0, 26.45070727472026, 13.05981365293111, 1.0, 1.0, 12.8060966609711], 
processed observation next is [1.0, 0.391304347826087, 0.5087719298245615, 0.45, 0.365, 0.7408839779005525, 1.0, 1.0643867535314655, 0.1305981365293111, 1.0, 1.0, 0.09147211900693643], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.57405126], dtype=float32), 0.13175443]. 
=============================================
[2019-04-01 16:36:55,116] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 16:36:55,118] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5416
[2019-04-01 16:36:55,134] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 17.33333333333334, 62.66666666666667, 487.3333333333334, 26.0, 29.0384127643015, 35.26749513238244, 1.0, 1.0, 0.6237501127923023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5071800.0000, 
sim time next is 5072400.0000, 
raw observation next is [12.0, 17.0, 56.0, 438.5, 26.0, 29.17556096373134, 29.00635964285925, 1.0, 1.0, 0.6278682316008353], 
processed observation next is [1.0, 0.7391304347826086, 0.7950138504155125, 0.17, 0.18666666666666668, 0.4845303867403315, 1.0, 1.4536515662473342, 0.2900635964285925, 1.0, 1.0, 0.004484773082863109], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.73196536], dtype=float32), -0.052636012]. 
=============================================
[2019-04-01 16:36:55,175] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:55,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:55,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:55,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:55,202] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run8
[2019-04-01 16:36:55,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run8
[2019-04-01 16:36:55,594] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:55,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:55,599] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run8
[2019-04-01 16:36:55,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:55,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:55,622] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run8
[2019-04-01 16:36:55,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:55,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:55,863] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run8
[2019-04-01 16:36:56,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:56,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:56,496] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run8
[2019-04-01 16:36:56,516] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:56,517] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:56,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run8
[2019-04-01 16:36:56,558] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:56,558] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:56,562] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run8
[2019-04-01 16:36:56,766] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:56,766] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:56,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run8
[2019-04-01 16:36:56,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:56,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:56,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run8
[2019-04-01 16:36:56,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:56,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:57,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run8
[2019-04-01 16:36:57,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:36:57,439] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:36:57,442] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run8
[2019-04-01 16:37:00,639] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0432166e-26 2.8938705e-31 9.3587080e-26 5.1786505e-34 1.6286210e-19
 1.2011335e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:37:00,639] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0253
[2019-04-01 16:37:00,662] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 26.0, 20.39157905586699, 17.90486735058782, 0.0, 1.0, 42.57656599808955], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 7800.0000, 
sim time next is 8400.0000, 
raw observation next is [7.200000000000001, 96.0, 0.0, 0.0, 26.0, 20.43903413221822, 17.46183864057448, 0.0, 1.0, 42.33972548336991], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 1.0, 0.2055763046026031, 0.1746183864057448, 0.0, 1.0, 0.30242661059549936], 
reward next is 0.6976, 
noisyNet noise sample is [array([0.961348], dtype=float32), -0.0983529]. 
=============================================
[2019-04-01 16:37:06,147] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.0039059e-21 9.1215358e-26 4.3166400e-25 1.6641263e-30 5.0305465e-29
 1.2433758e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 16:37:06,148] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7762
[2019-04-01 16:37:06,164] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.33845001820355, 11.47308540624874, 0.0, 1.0, 39.76508233179062], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 25200.0000, 
sim time next is 25800.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.35777915755586, 11.3440405199272, 0.0, 1.0, 39.76508498850342], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 1.0, 0.33682559393655154, 0.113440405199272, 0.0, 1.0, 0.28403632134645296], 
reward next is 0.7160, 
noisyNet noise sample is [array([-0.04093574], dtype=float32), 0.09874289]. 
=============================================
[2019-04-01 16:37:11,135] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.06822305e-33 0.00000000e+00 1.41074104e-32 0.00000000e+00
 8.63345867e-34 0.00000000e+00 1.00000000e+00], sum to 1.0000
[2019-04-01 16:37:11,135] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5771
[2019-04-01 16:37:11,182] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.1, 87.5, 0.0, 0.0, 26.0, 24.58227879003451, 6.926754580185862, 0.0, 1.0, 38.73473406442054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 66600.0000, 
sim time next is 67200.0000, 
raw observation next is [4.0, 87.0, 0.0, 0.0, 26.0, 24.56374650134915, 6.954250317297334, 0.0, 1.0, 41.85207300978838], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.87, 0.0, 0.0, 1.0, 0.7948209287641644, 0.06954250317297334, 0.0, 1.0, 0.29894337864134557], 
reward next is 0.7011, 
noisyNet noise sample is [array([-1.0883017], dtype=float32), 0.037675384]. 
=============================================
[2019-04-01 16:37:13,472] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3583177e-22 2.7528761e-29 2.8915568e-21 3.9277129e-30 8.5284116e-28
 3.2524161e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 16:37:13,473] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9747
[2019-04-01 16:37:13,597] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 26.0, 23.17074811527826, 5.536767175946541, 0.0, 1.0, 45.1733171474331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 111600.0000, 
sim time next is 112200.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 26.0, 23.11199233293407, 5.398801823577618, 0.0, 1.0, 112.0343255767053], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 1.0, 0.5874274761334384, 0.05398801823577618, 0.0, 1.0, 0.8002451826907522], 
reward next is 0.1998, 
noisyNet noise sample is [array([0.61495775], dtype=float32), -1.5875725]. 
=============================================
[2019-04-01 16:37:46,042] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9698507e-24 0.0000000e+00 2.1748463e-31 2.5809020e-38 2.7628416e-27
 1.2121162e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 16:37:46,043] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4986
[2019-04-01 16:37:46,054] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.8, 90.0, 0.0, 0.0, 26.0, 24.64702815091312, 6.051515638684077, 0.0, 1.0, 39.89405669230624], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 541800.0000, 
sim time next is 542400.0000, 
raw observation next is [0.7000000000000001, 90.66666666666666, 0.0, 0.0, 26.0, 24.67304363584293, 6.087561210371916, 0.0, 1.0, 40.2110315100679], 
processed observation next is [0.0, 0.2608695652173913, 0.4819944598337951, 0.9066666666666666, 0.0, 0.0, 1.0, 0.8104348051204185, 0.06087561210371916, 0.0, 1.0, 0.2872216536433421], 
reward next is 0.7128, 
noisyNet noise sample is [array([1.7168872], dtype=float32), -0.63818055]. 
=============================================
[2019-04-01 16:37:47,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.08940667e-25 1.08730985e-36 4.00835900e-30 3.05571392e-35
 1.62899804e-29 3.77227888e-10 1.00000000e+00], sum to 1.0000
[2019-04-01 16:37:47,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9949
[2019-04-01 16:37:47,264] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.2, 94.33333333333334, 0.0, 0.0, 26.0, 24.94781233741558, 6.757537478476308, 0.0, 1.0, 39.12582795584517], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 519600.0000, 
sim time next is 520200.0000, 
raw observation next is [4.4, 93.0, 0.0, 0.0, 26.0, 24.93840141189388, 6.739193024170317, 0.0, 1.0, 39.10304444922073], 
processed observation next is [0.0, 0.0, 0.5844875346260389, 0.93, 0.0, 0.0, 1.0, 0.848343058841983, 0.06739193024170316, 0.0, 1.0, 0.27930746035157666], 
reward next is 0.7207, 
noisyNet noise sample is [array([-1.0754662], dtype=float32), 0.55978763]. 
=============================================
[2019-04-01 16:37:49,316] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5849982e-32 0.0000000e+00 4.1269083e-33 1.2789070e-38 1.4949781e-29
 2.1342127e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 16:37:49,317] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0756
[2019-04-01 16:37:49,358] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.066666666666667, 83.33333333333334, 0.0, 0.0, 26.0, 24.78157898863218, 6.343574633808825, 0.0, 1.0, 39.52522903053877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 531600.0000, 
sim time next is 532200.0000, 
raw observation next is [2.883333333333334, 82.66666666666667, 0.0, 0.0, 26.0, 24.76062617296093, 6.304959545570725, 0.0, 1.0, 39.57506717989494], 
processed observation next is [0.0, 0.13043478260869565, 0.5424746075715605, 0.8266666666666667, 0.0, 0.0, 1.0, 0.8229465961372756, 0.06304959545570725, 0.0, 1.0, 0.28267905128496384], 
reward next is 0.7173, 
noisyNet noise sample is [array([1.0621358], dtype=float32), 1.0533417]. 
=============================================
[2019-04-01 16:37:50,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4486288e-29 0.0000000e+00 2.4458373e-35 0.0000000e+00 4.4681113e-33
 1.1795559e-23 1.0000000e+00], sum to 1.0000
[2019-04-01 16:37:50,283] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1076
[2019-04-01 16:37:50,349] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.25, 91.5, 34.0, 104.0, 26.0, 25.15808258016253, 7.429636639033887, 0.0, 1.0, 41.04973958894443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 549000.0000, 
sim time next is 549600.0000, 
raw observation next is [0.1666666666666667, 91.33333333333334, 52.33333333333333, 103.8333333333333, 26.0, 25.2110715090326, 7.391618695250062, 0.0, 1.0, 36.54739868988946], 
processed observation next is [0.0, 0.34782608695652173, 0.4672206832871654, 0.9133333333333334, 0.17444444444444443, 0.11473296500920807, 1.0, 0.8872959298618, 0.07391618695250063, 0.0, 1.0, 0.2610528477849247], 
reward next is 0.7389, 
noisyNet noise sample is [array([0.90791374], dtype=float32), 1.3263729]. 
=============================================
[2019-04-01 16:37:54,194] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7748574e-28 5.6621710e-36 1.9494174e-29 1.0258569e-37 7.3148947e-25
 3.8794270e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:37:54,196] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5353
[2019-04-01 16:37:54,211] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 26.0, 25.05141991621732, 7.189402322285379, 0.0, 1.0, 42.41816399754929], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 594000.0000, 
sim time next is 594600.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 26.0, 25.0187617883969, 7.103617608294226, 0.0, 1.0, 42.43402001221027], 
processed observation next is [0.0, 0.9130434782608695, 0.38504155124653744, 0.83, 0.0, 0.0, 1.0, 0.8598231126281287, 0.07103617608294227, 0.0, 1.0, 0.30310014294435905], 
reward next is 0.6969, 
noisyNet noise sample is [array([-0.51597553], dtype=float32), 0.7680971]. 
=============================================
[2019-04-01 16:37:57,788] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6475467e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:37:57,793] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6266
[2019-04-01 16:37:57,865] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8, 55.00000000000001, 36.33333333333333, 18.83333333333333, 26.0, 24.83031417880632, 6.821887516491387, 0.0, 1.0, 42.38764056415376], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 663600.0000, 
sim time next is 664200.0000, 
raw observation next is [-0.8999999999999999, 55.5, 27.0, 15.0, 26.0, 24.8209114304747, 6.873855188809898, 0.0, 1.0, 44.98568733445935], 
processed observation next is [0.0, 0.6956521739130435, 0.43767313019390586, 0.555, 0.09, 0.016574585635359115, 1.0, 0.8315587757820998, 0.06873855188809898, 0.0, 1.0, 0.3213263381032811], 
reward next is 0.6787, 
noisyNet noise sample is [array([0.9979966], dtype=float32), -0.6291046]. 
=============================================
[2019-04-01 16:38:03,869] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7608925e-26 2.1935772e-38 9.9808686e-31 3.8478091e-37 4.4392531e-30
 1.1254674e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:38:03,869] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4804
[2019-04-01 16:38:03,899] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 26.0, 23.53748737045436, 5.334221145265642, 0.0, 1.0, 41.57673958779502], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 799800.0000, 
sim time next is 800400.0000, 
raw observation next is [-7.100000000000001, 69.66666666666667, 0.0, 0.0, 26.0, 23.48864366926812, 5.352308062945316, 0.0, 1.0, 41.59075091409775], 
processed observation next is [1.0, 0.2608695652173913, 0.26592797783933514, 0.6966666666666668, 0.0, 0.0, 1.0, 0.6412348098954459, 0.053523080629453164, 0.0, 1.0, 0.2970767922435553], 
reward next is 0.7029, 
noisyNet noise sample is [array([-1.5783776], dtype=float32), 1.8063232]. 
=============================================
[2019-04-01 16:38:15,653] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 16:38:15,653] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4716
[2019-04-01 16:38:15,697] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 81.66666666666667, 0.0, 26.0, 25.81687808913828, 12.48076435106658, 1.0, 1.0, 5.477148991284597], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1003800.0000, 
sim time next is 1004400.0000, 
raw observation next is [14.4, 81.0, 75.5, 0.0, 26.0, 24.38779645029432, 10.67113665094526, 1.0, 1.0, 29.74160172081139], 
processed observation next is [1.0, 0.6521739130434783, 0.8614958448753465, 0.81, 0.25166666666666665, 0.0, 1.0, 0.769685207184903, 0.1067113665094526, 1.0, 1.0, 0.2124400122915099], 
reward next is 0.5191, 
noisyNet noise sample is [array([-0.2607777], dtype=float32), -0.54230434]. 
=============================================
[2019-04-01 16:38:17,307] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6671673e-33 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.4097513e-37
 1.0685371e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 16:38:17,311] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7896
[2019-04-01 16:38:17,322] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 82.5, 0.0, 0.0, 26.0, 25.5559825779531, 9.952139002318999, 0.0, 1.0, 26.97551544588774], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 964200.0000, 
sim time next is 964800.0000, 
raw observation next is [7.7, 83.0, 0.0, 0.0, 26.0, 25.62185171702175, 9.759264278709518, 0.0, 1.0, 26.26145377659721], 
processed observation next is [1.0, 0.17391304347826086, 0.6759002770083103, 0.83, 0.0, 0.0, 1.0, 0.9459788167173931, 0.09759264278709517, 0.0, 1.0, 0.1875818126899801], 
reward next is 0.8124, 
noisyNet noise sample is [array([0.6408196], dtype=float32), -0.7974572]. 
=============================================
[2019-04-01 16:38:20,104] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1032664e-28 0.0000000e+00 3.1914722e-34 0.0000000e+00 2.4860108e-29
 6.6292535e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 16:38:20,108] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4391
[2019-04-01 16:38:20,163] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.9, 84.66666666666667, 0.0, 0.0, 26.0, 25.39279577524377, 10.40687259869332, 0.0, 1.0, 47.73069406966497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 976200.0000, 
sim time next is 976800.0000, 
raw observation next is [9.8, 86.33333333333334, 0.0, 0.0, 26.0, 25.72884464392503, 10.54540193347701, 1.0, 1.0, 30.18060886470738], 
processed observation next is [1.0, 0.30434782608695654, 0.7340720221606649, 0.8633333333333334, 0.0, 0.0, 1.0, 0.9612635205607185, 0.1054540193347701, 1.0, 1.0, 0.21557577760505273], 
reward next is 0.5663, 
noisyNet noise sample is [array([0.18841635], dtype=float32), 0.104381025]. 
=============================================
[2019-04-01 16:38:21,417] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4434201e-31 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7120658e-35
 8.4107977e-23 1.0000000e+00], sum to 1.0000
[2019-04-01 16:38:21,418] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3654
[2019-04-01 16:38:21,428] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.1, 77.5, 0.0, 0.0, 26.0, 25.8398021582544, 12.12933960021952, 0.0, 1.0, 11.48347973642323], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1045800.0000, 
sim time next is 1046400.0000, 
raw observation next is [14.2, 77.33333333333333, 0.0, 0.0, 26.0, 25.72044068648351, 11.81277963908466, 0.0, 1.0, 10.34626267199364], 
processed observation next is [1.0, 0.08695652173913043, 0.8559556786703602, 0.7733333333333333, 0.0, 0.0, 1.0, 0.96006295521193, 0.1181277963908466, 0.0, 1.0, 0.07390187622852601], 
reward next is 0.9261, 
noisyNet noise sample is [array([0.04255185], dtype=float32), 0.27626017]. 
=============================================
[2019-04-01 16:38:25,095] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.9247627e-33 1.0000000e+00], sum to 1.0000
[2019-04-01 16:38:25,099] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9454
[2019-04-01 16:38:25,113] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 26.0, 25.64328321395885, 13.30874847271424, 0.0, 1.0, 23.44098500404233], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1135800.0000, 
sim time next is 1136400.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.6279119614004, 13.28299915656363, 0.0, 1.0, 24.35071191550526], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 1.0, 0.9468445659143429, 0.1328299915656363, 0.0, 1.0, 0.1739336565393233], 
reward next is 0.8261, 
noisyNet noise sample is [array([-0.0571984], dtype=float32), 0.5188352]. 
=============================================
[2019-04-01 16:38:25,649] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 16:38:25,651] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7381
[2019-04-01 16:38:25,659] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.78333333333333, 73.66666666666666, 0.0, 0.0, 26.0, 24.49909083043413, 7.964908124317446, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1201800.0000, 
sim time next is 1202400.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 26.0, 24.46551727924974, 7.897012353924548, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 1.0, 0.7807881827499631, 0.07897012353924547, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5614301], dtype=float32), 1.0771118]. 
=============================================
[2019-04-01 16:38:27,344] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7528013e-30 0.0000000e+00 4.6381825e-34 0.0000000e+00 2.4712776e-38
 2.6049373e-38 1.0000000e+00], sum to 1.0000
[2019-04-01 16:38:27,345] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6867
[2019-04-01 16:38:27,353] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 55.33333333333333, 0.0, 26.0, 24.67290609260106, 9.368806309742185, 0.0, 1.0, 5.14344735437885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1264800.0000, 
sim time next is 1265400.0000, 
raw observation next is [13.8, 100.0, 51.0, 0.0, 26.0, 24.65599970240774, 9.313715049373357, 0.0, 1.0, 4.981907479063783], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.17, 0.0, 1.0, 0.8079999574868201, 0.09313715049373357, 0.0, 1.0, 0.035585053421884165], 
reward next is 0.9644, 
noisyNet noise sample is [array([0.9787526], dtype=float32), 0.06429432]. 
=============================================
[2019-04-01 16:38:27,580] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 16:38:27,585] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7601
[2019-04-01 16:38:27,590] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.78333333333333, 73.66666666666666, 0.0, 0.0, 26.0, 24.49909083043413, 7.964908124317446, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1201800.0000, 
sim time next is 1202400.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 26.0, 24.46551727924974, 7.897012353924548, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 1.0, 0.7807881827499631, 0.07897012353924547, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18576723], dtype=float32), -0.5307891]. 
=============================================
[2019-04-01 16:38:30,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.049581e-35 0.000000e+00 5.457008e-35 0.000000e+00 0.000000e+00
 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-04-01 16:38:30,646] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1528
[2019-04-01 16:38:30,653] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 71.0, 0.0, 26.0, 25.8581913296975, 11.56527868328945, 1.0, 1.0, 9.668649465423265], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1348200.0000, 
sim time next is 1348800.0000, 
raw observation next is [1.1, 92.0, 66.5, 0.0, 26.0, 25.86630411665227, 11.39066257021881, 1.0, 1.0, 9.37550769199307], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.22166666666666668, 0.0, 1.0, 0.9809005880931814, 0.1139066257021881, 1.0, 1.0, 0.06696791208566479], 
reward next is 0.3768, 
noisyNet noise sample is [array([-0.83106136], dtype=float32), -1.036923]. 
=============================================
[2019-04-01 16:38:36,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.0414798e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.8238444e-26 1.0000000e+00], sum to 1.0000
[2019-04-01 16:38:36,462] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8908
[2019-04-01 16:38:36,485] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.05724429492312, 9.501082835677417, 0.0, 1.0, 15.88392976664573], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1366200.0000, 
sim time next is 1366800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 24.85088829768753, 9.151407560694807, 0.0, 1.0, 25.65715755357007], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 1.0, 0.8358411853839327, 0.09151407560694808, 0.0, 1.0, 0.18326541109692907], 
reward next is 0.8167, 
noisyNet noise sample is [array([0.19179828], dtype=float32), -1.3255494]. 
=============================================
[2019-04-01 16:38:38,475] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2187441e-27 0.0000000e+00 9.2191513e-30 9.8257667e-36 8.8236894e-31
 2.5507946e-08 1.0000000e+00], sum to 1.0000
[2019-04-01 16:38:38,475] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4731
[2019-04-01 16:38:38,484] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.34324769283486, 9.257061462498859, 0.0, 1.0, 34.8525117983822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1483200.0000, 
sim time next is 1483800.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.29862016256528, 9.354152895602992, 0.0, 1.0, 36.0507949175365], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 1.0, 0.8998028803664683, 0.09354152895602992, 0.0, 1.0, 0.2575056779824036], 
reward next is 0.7425, 
noisyNet noise sample is [array([-0.09747362], dtype=float32), 0.47903955]. 
=============================================
[2019-04-01 16:38:40,488] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 16:38:40,489] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3830
[2019-04-01 16:38:40,554] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 67.66666666666667, 0.0, 26.0, 25.47540157522831, 9.532264906580073, 1.0, 1.0, 15.18058636818252], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1435200.0000, 
sim time next is 1435800.0000, 
raw observation next is [1.1, 92.0, 63.33333333333334, 0.0, 26.0, 24.82267955953295, 9.20945023673925, 1.0, 1.0, 47.40137955599388], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.21111111111111114, 0.0, 1.0, 0.8318113656475644, 0.09209450236739251, 1.0, 1.0, 0.3385812825428134], 
reward next is 0.6614, 
noisyNet noise sample is [array([-1.9194026], dtype=float32), -1.0344118]. 
=============================================
[2019-04-01 16:38:42,666] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0426430e-15 2.1703495e-27 1.1701464e-19 2.8048216e-25 4.2150914e-14
 3.1804913e-07 9.9999964e-01], sum to 1.0000
[2019-04-01 16:38:42,670] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7254
[2019-04-01 16:38:42,687] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.083333333333334, 81.50000000000001, 0.0, 0.0, 26.0, 25.79112938328959, 10.967788418171, 0.0, 1.0, 21.91421673963049], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1577400.0000, 
sim time next is 1578000.0000, 
raw observation next is [5.166666666666667, 81.0, 0.0, 0.0, 26.0, 25.73941925007621, 10.55006913326795, 0.0, 1.0, 20.89216299793613], 
processed observation next is [1.0, 0.2608695652173913, 0.6057248384118191, 0.81, 0.0, 0.0, 1.0, 0.9627741785823157, 0.1055006913326795, 0.0, 1.0, 0.1492297356995438], 
reward next is 0.8508, 
noisyNet noise sample is [array([-0.9172407], dtype=float32), -0.269048]. 
=============================================
[2019-04-01 16:38:42,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[56.6613  ]
 [56.63649 ]
 [56.666954]
 [56.75903 ]
 [56.85415 ]], R is [[56.96960068]
 [57.24337769]
 [57.50615692]
 [57.75719833]
 [57.99623108]].
[2019-04-01 16:38:47,679] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.8741831e-32 0.0000000e+00 1.2546775e-31 0.0000000e+00 9.1544609e-37
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:38:47,681] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3336
[2019-04-01 16:38:47,692] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 49.0, 171.5, 20.66666666666666, 26.0, 26.55454975794579, 16.20202122187194, 1.0, 1.0, 4.095058595086583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1603200.0000, 
sim time next is 1603800.0000, 
raw observation next is [13.8, 49.0, 176.0, 0.0, 26.0, 26.6736535698218, 17.38367290787011, 1.0, 1.0, 4.947868556320239], 
processed observation next is [1.0, 0.5652173913043478, 0.844875346260388, 0.49, 0.5866666666666667, 0.0, 1.0, 1.096236224260257, 0.1738367290787011, 1.0, 1.0, 0.03534191825943028], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.21546483], dtype=float32), 0.40900025]. 
=============================================
[2019-04-01 16:38:47,742] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.6684288e-25 0.0000000e+00 3.9701757e-31 0.0000000e+00 2.0246478e-28
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:38:47,743] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8989
[2019-04-01 16:38:47,755] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.3, 51.0, 64.0, 18.5, 26.0, 27.29556864109044, 20.13848811588787, 1.0, 1.0, 6.054445321093378], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1612800.0000, 
sim time next is 1613400.0000, 
raw observation next is [13.11666666666667, 51.5, 59.33333333333333, 24.66666666666667, 26.0, 27.35436475903152, 20.46987394818251, 1.0, 1.0, 6.140424582532965], 
processed observation next is [1.0, 0.6956521739130435, 0.8259464450600187, 0.515, 0.19777777777777777, 0.027255985267034995, 1.0, 1.1934806798616455, 0.2046987394818251, 1.0, 1.0, 0.04386017558952118], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6274581], dtype=float32), 0.3108659]. 
=============================================
[2019-04-01 16:38:58,892] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-01 16:38:58,903] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 16:38:58,915] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:38:58,915] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 16:38:58,916] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:38:58,921] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 16:38:58,937] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:39:00,034] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run11
[2019-04-01 16:39:00,147] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run11
[2019-04-01 16:39:00,175] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run11
[2019-04-01 16:39:09,550] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.7545671], dtype=float32), -0.4840935]
[2019-04-01 16:39:09,550] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.4, 71.0, 0.0, 0.0, 26.0, 24.08984196594644, 5.784657445180585, 0.0, 1.0, 44.26839483942695]
[2019-04-01 16:39:09,550] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:39:09,551] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.9896698e-17 2.1451058e-28 5.3687349e-22 5.6572205e-24 1.1006704e-18
 3.9145046e-01 6.0854954e-01], sampled 0.222930617190242
[2019-04-01 16:40:12,800] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.7545671], dtype=float32), -0.4840935]
[2019-04-01 16:40:12,800] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.5, 47.5, 175.3333333333333, 180.6666666666667, 26.0, 26.11086662689449, 11.1547205743707, 1.0, 1.0, 14.28082941949529]
[2019-04-01 16:40:12,800] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:40:12,801] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.0870876e-31 0.0000000e+00 4.0576832e-37 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sampled 0.10256321289179215
[2019-04-01 16:40:22,715] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.7545671], dtype=float32), -0.4840935]
[2019-04-01 16:40:22,715] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.0, 77.0, 0.0, 0.0, 26.0, 23.99777695137447, 5.286697024985419, 0.0, 1.0, 39.71125134539744]
[2019-04-01 16:40:22,716] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:40:22,716] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.4583203e-17 9.4610779e-28 7.9170046e-21 4.7030617e-24 1.0552142e-19
 2.9858243e-01 7.0141757e-01], sampled 0.3066906200484938
[2019-04-01 16:40:42,028] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 16:41:00,297] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 16:41:06,653] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 16:41:07,676] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 1000000, evaluation results [1000000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 16:41:07,915] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0674009e-20 4.6808632e-31 1.6693636e-23 1.5742015e-26 1.7495836e-17
 7.6017824e-07 9.9999928e-01], sum to 1.0000
[2019-04-01 16:41:07,918] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4695
[2019-04-01 16:41:07,941] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 84.0, 0.0, 0.0, 26.0, 25.01323316714876, 7.589100466419812, 0.0, 1.0, 45.21304382458564], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1801200.0000, 
sim time next is 1801800.0000, 
raw observation next is [-4.75, 84.5, 0.0, 0.0, 26.0, 24.99067009403793, 7.49540600006618, 0.0, 1.0, 45.48082731998401], 
processed observation next is [0.0, 0.8695652173913043, 0.3310249307479225, 0.845, 0.0, 0.0, 1.0, 0.8558100134339901, 0.07495406000066179, 0.0, 1.0, 0.3248630522856001], 
reward next is 0.6751, 
noisyNet noise sample is [array([-1.4015088], dtype=float32), 0.010114677]. 
=============================================
[2019-04-01 16:41:17,067] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3467816e-09 1.3503253e-24 7.6587222e-18 6.3613358e-23 8.9714941e-10
 9.9999607e-01 3.9106567e-06], sum to 1.0000
[2019-04-01 16:41:17,068] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8381
[2019-04-01 16:41:17,117] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 79.0, 128.0, 392.5, 26.0, 25.616442814091, 7.610481487667042, 1.0, 1.0, 35.24699257921835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1936800.0000, 
sim time next is 1937400.0000, 
raw observation next is [-7.016666666666667, 78.33333333333334, 142.3333333333333, 340.3333333333333, 26.0, 25.70309392823587, 7.817479856820135, 1.0, 1.0, 34.32886840573779], 
processed observation next is [1.0, 0.43478260869565216, 0.2682363804247461, 0.7833333333333334, 0.4744444444444443, 0.3760589318600368, 1.0, 0.9575848468908384, 0.07817479856820135, 1.0, 1.0, 0.24520620289812708], 
reward next is 0.7548, 
noisyNet noise sample is [array([0.46325028], dtype=float32), -1.0511079]. 
=============================================
[2019-04-01 16:41:31,931] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.7628619e-19 0.0000000e+00 3.1750474e-31 0.0000000e+00 0.0000000e+00
 1.9184923e-36 1.0000000e+00], sum to 1.0000
[2019-04-01 16:41:31,933] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4398
[2019-04-01 16:41:31,982] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.8, 66.66666666666666, 145.0, 0.0, 26.0, 26.3329650275497, 13.24272398703038, 1.0, 1.0, 38.19742230350683], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2122800.0000, 
sim time next is 2123400.0000, 
raw observation next is [-5.7, 67.33333333333334, 141.0, 0.0, 26.0, 26.76288555391773, 14.03741140650886, 1.0, 1.0, 37.15313033790748], 
processed observation next is [1.0, 0.5652173913043478, 0.30470914127423826, 0.6733333333333335, 0.47, 0.0, 1.0, 1.1089836505596757, 0.1403741140650886, 1.0, 1.0, 0.26537950241362485], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.37825337], dtype=float32), -1.0906049]. 
=============================================
[2019-04-01 16:41:35,522] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4240052e-21 5.4953539e-36 9.7174529e-25 2.8252157e-30 5.5341364e-17
 8.8178471e-04 9.9911827e-01], sum to 1.0000
[2019-04-01 16:41:35,522] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5371
[2019-04-01 16:41:35,574] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.199999999999999, 78.83333333333334, 0.0, 0.0, 26.0, 24.34458117499892, 5.826725242043811, 0.0, 1.0, 42.03181206551373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2164200.0000, 
sim time next is 2164800.0000, 
raw observation next is [-7.100000000000001, 78.66666666666667, 0.0, 0.0, 26.0, 24.27480529091885, 5.784131708722292, 0.0, 1.0, 42.06817141567043], 
processed observation next is [1.0, 0.043478260869565216, 0.26592797783933514, 0.7866666666666667, 0.0, 0.0, 1.0, 0.753543612988407, 0.05784131708722292, 0.0, 1.0, 0.3004869386833602], 
reward next is 0.6995, 
noisyNet noise sample is [array([1.3753414], dtype=float32), 0.15780747]. 
=============================================
[2019-04-01 16:41:36,776] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3433161e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:41:36,777] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3374
[2019-04-01 16:41:36,822] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 70.0, 124.0, 0.0, 26.0, 26.04134900851454, 9.5543008170294, 1.0, 1.0, 24.31789824761255], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2211600.0000, 
sim time next is 2212200.0000, 
raw observation next is [-3.9, 69.5, 120.0, 0.0, 26.0, 26.06036467592358, 9.463278810551522, 1.0, 1.0, 22.95533016100926], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.695, 0.4, 0.0, 1.0, 1.00862352513194, 0.09463278810551522, 1.0, 1.0, 0.163966644007209], 
reward next is 0.8360, 
noisyNet noise sample is [array([-0.5253901], dtype=float32), 0.22041239]. 
=============================================
[2019-04-01 16:41:56,896] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 16:41:56,896] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5948
[2019-04-01 16:41:56,904] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.716666666666666, 26.5, 163.0, 344.0, 26.0, 25.7693621254661, 8.483977893712273, 1.0, 1.0, 5.727588156825869], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2556600.0000, 
sim time next is 2557200.0000, 
raw observation next is [3.633333333333334, 27.0, 161.0, 309.5, 26.0, 25.83978189659118, 8.627871850246635, 1.0, 1.0, 5.740327517954559], 
processed observation next is [1.0, 0.6086956521739131, 0.5632502308402586, 0.27, 0.5366666666666666, 0.3419889502762431, 1.0, 0.9771116995130258, 0.08627871850246635, 1.0, 1.0, 0.041002339413961135], 
reward next is 0.9590, 
noisyNet noise sample is [array([1.6449654], dtype=float32), 0.7269312]. 
=============================================
[2019-04-01 16:42:01,406] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2286504e-32 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:42:01,406] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5609
[2019-04-01 16:42:01,439] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333334, 30.0, 28.0, 57.33333333333332, 26.0, 25.90102609870877, 8.903651097787353, 1.0, 1.0, 9.639677067029906], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2567400.0000, 
sim time next is 2568000.0000, 
raw observation next is [1.966666666666667, 31.0, 17.5, 31.16666666666666, 26.0, 25.9318567132682, 8.402815535030635, 1.0, 1.0, 10.30387384479699], 
processed observation next is [1.0, 0.7391304347826086, 0.5170821791320407, 0.31, 0.058333333333333334, 0.03443830570902393, 1.0, 0.9902652447526, 0.08402815535030635, 1.0, 1.0, 0.07359909889140707], 
reward next is 0.9264, 
noisyNet noise sample is [array([1.2099644], dtype=float32), -1.5662285]. 
=============================================
[2019-04-01 16:42:01,447] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[87.3583  ]
 [87.57968 ]
 [87.76285 ]
 [88.02058 ]
 [88.311554]], R is [[87.13465881]
 [87.19445801]
 [87.25106049]
 [87.304245  ]
 [87.3539505 ]].
[2019-04-01 16:42:03,889] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.8375044e-21 4.3993441e-35 1.7574896e-29 5.2296074e-30 1.1786148e-27
 9.9974948e-01 2.5049417e-04], sum to 1.0000
[2019-04-01 16:42:03,889] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8405
[2019-04-01 16:42:03,911] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.366666666666667, 81.33333333333334, 0.0, 0.0, 26.0, 24.1295630994979, 5.332261722696036, 0.0, 1.0, 42.7815680948475], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2611200.0000, 
sim time next is 2611800.0000, 
raw observation next is [-6.45, 80.5, 0.0, 0.0, 26.0, 24.05182252387282, 5.300507025002548, 0.0, 1.0, 42.94557928476689], 
processed observation next is [1.0, 0.21739130434782608, 0.28393351800554023, 0.805, 0.0, 0.0, 1.0, 0.7216889319818313, 0.05300507025002548, 0.0, 1.0, 0.30675413774833493], 
reward next is 0.6932, 
noisyNet noise sample is [array([0.27175805], dtype=float32), -0.17352296]. 
=============================================
[2019-04-01 16:42:05,556] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.94414491e-28 4.54284287e-38 2.90999680e-32 1.11306508e-35
 1.05954795e-29 9.98741567e-01 1.25844334e-03], sum to 1.0000
[2019-04-01 16:42:05,556] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2800
[2019-04-01 16:42:05,585] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.25, 84.33333333333334, 0.0, 0.0, 26.0, 23.83173401930333, 5.358308127308796, 0.0, 1.0, 43.90271068422976], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2689800.0000, 
sim time next is 2690400.0000, 
raw observation next is [-13.6, 85.66666666666667, 0.0, 0.0, 26.0, 23.76648509591661, 5.332387097429589, 0.0, 1.0, 43.89640507005518], 
processed observation next is [1.0, 0.13043478260869565, 0.08587257617728532, 0.8566666666666667, 0.0, 0.0, 1.0, 0.6809264422738016, 0.05332387097429589, 0.0, 1.0, 0.3135457505003941], 
reward next is 0.6865, 
noisyNet noise sample is [array([0.05397541], dtype=float32), -0.4845441]. 
=============================================
[2019-04-01 16:42:36,331] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9240148e-19 3.8016795e-32 6.8685718e-27 1.1631831e-29 1.6033118e-22
 9.9999475e-01 5.1936931e-06], sum to 1.0000
[2019-04-01 16:42:36,333] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7801
[2019-04-01 16:42:36,356] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 100.0, 0.0, 0.0, 26.0, 26.10532270712946, 16.80678894187099, 0.0, 1.0, 21.19687962266588], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3189600.0000, 
sim time next is 3190200.0000, 
raw observation next is [2.0, 98.83333333333334, 0.0, 0.0, 26.0, 26.05906134874664, 14.29638542134962, 0.0, 1.0, 22.04570930693549], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.9883333333333334, 0.0, 0.0, 1.0, 1.008437335535234, 0.14296385421349622, 0.0, 1.0, 0.15746935219239636], 
reward next is 0.8425, 
noisyNet noise sample is [array([0.1000502], dtype=float32), -0.27768078]. 
=============================================
[2019-04-01 16:42:37,107] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2390278e-07 1.9051533e-21 3.8705967e-15 2.6555144e-23 3.8304904e-11
 9.1241748e-27 9.9999988e-01], sum to 1.0000
[2019-04-01 16:42:37,108] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7877
[2019-04-01 16:42:37,116] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.66016351013516, 13.68770663750821, 0.0, 1.0, 6.632120920291569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3181200.0000, 
sim time next is 3181800.0000, 
raw observation next is [3.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.54650515356452, 13.1968278298802, 0.0, 1.0, 5.664220405852321], 
processed observation next is [1.0, 0.8260869565217391, 0.5503231763619576, 1.0, 0.0, 0.0, 1.0, 0.9352150219377887, 0.131968278298802, 0.0, 1.0, 0.04045871718465943], 
reward next is 0.9595, 
noisyNet noise sample is [array([0.40848938], dtype=float32), -0.80219984]. 
=============================================
[2019-04-01 16:42:37,244] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0467705e-26 0.0000000e+00 6.5612431e-37 0.0000000e+00 2.0613643e-33
 2.1399214e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 16:42:37,265] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8203
[2019-04-01 16:42:37,280] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.333333333333333, 100.0, 0.0, 0.0, 26.0, 26.02214858952895, 16.0315495102241, 1.0, 1.0, 5.838813541487626], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3178200.0000, 
sim time next is 3178800.0000, 
raw observation next is [4.0, 100.0, 0.0, 0.0, 26.0, 25.95051361901161, 15.87101720362718, 1.0, 1.0, 6.106176957566017], 
processed observation next is [1.0, 0.8260869565217391, 0.5734072022160666, 1.0, 0.0, 0.0, 1.0, 0.9929305170016585, 0.1587101720362718, 1.0, 1.0, 0.043615549696900124], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6942763], dtype=float32), 1.5071582]. 
=============================================
[2019-04-01 16:42:40,311] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5671266e-30 0.0000000e+00 1.7133567e-33 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:42:40,312] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6660
[2019-04-01 16:42:40,331] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.266666666666667, 77.0, 112.3333333333333, 801.1666666666666, 26.0, 26.40428757525462, 16.09325193353029, 1.0, 1.0, 7.817228745841079], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3237600.0000, 
sim time next is 3238200.0000, 
raw observation next is [-2.2, 75.5, 113.0, 811.0, 26.0, 26.44941285339799, 16.2106422474353, 1.0, 1.0, 7.540294804108392], 
processed observation next is [1.0, 0.4782608695652174, 0.4016620498614959, 0.755, 0.37666666666666665, 0.8961325966850828, 1.0, 1.064201836199713, 0.162106422474353, 1.0, 1.0, 0.05385924860077423], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.87311715], dtype=float32), 2.5943828]. 
=============================================
[2019-04-01 16:42:50,000] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8382456e-14 2.2640762e-30 3.0940340e-23 1.3339128e-25 1.7945016e-12
 8.7095660e-01 1.2904346e-01], sum to 1.0000
[2019-04-01 16:42:50,000] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5200
[2019-04-01 16:42:50,021] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.47317824772529, 10.33975089434571, 0.0, 1.0, 32.71259856981587], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3361800.0000, 
sim time next is 3362400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.41609689114349, 10.37469960312931, 0.0, 1.0, 36.14540646033941], 
processed observation next is [1.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 1.0, 0.9165852701633556, 0.1037469960312931, 0.0, 1.0, 0.25818147471671005], 
reward next is 0.7418, 
noisyNet noise sample is [array([0.12247597], dtype=float32), 0.41527075]. 
=============================================
[2019-04-01 16:42:55,080] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0988831e-15 3.1595403e-34 2.4457778e-15 6.1080136e-28 5.6735897e-27
 2.0002413e-07 9.9999976e-01], sum to 1.0000
[2019-04-01 16:42:55,082] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1826
[2019-04-01 16:42:55,125] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 59.5, 111.0, 758.0, 26.0, 25.62090838596404, 10.45397805927974, 0.0, 1.0, 23.00814977471652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3580200.0000, 
sim time next is 3580800.0000, 
raw observation next is [-4.333333333333334, 57.66666666666667, 111.5, 767.6666666666667, 26.0, 25.54854862430648, 10.25767981976363, 0.0, 1.0, 21.6270898248518], 
processed observation next is [0.0, 0.43478260869565216, 0.3425669436749769, 0.5766666666666667, 0.37166666666666665, 0.8482504604051566, 1.0, 0.9355069463294973, 0.1025767981976363, 0.0, 1.0, 0.15447921303465573], 
reward next is 0.8455, 
noisyNet noise sample is [array([-0.63203853], dtype=float32), -0.5142273]. 
=============================================
[2019-04-01 16:42:57,031] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.8394902e-22 0.0000000e+00 1.5530221e-26 8.8399109e-35 3.1997900e-24
 1.4342125e-09 1.0000000e+00], sum to 1.0000
[2019-04-01 16:42:57,038] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0062
[2019-04-01 16:42:57,053] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.23705103168301, 8.90768564811968, 0.0, 1.0, 40.12736890590459], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3613800.0000, 
sim time next is 3614400.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.35032343673879, 9.124365614502478, 0.0, 1.0, 39.1966843862464], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 1.0, 0.9071890623912557, 0.09124365614502478, 0.0, 1.0, 0.2799763170446172], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.81353354], dtype=float32), -0.006507429]. 
=============================================
[2019-04-01 16:42:59,165] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.4025366e-24 0.0000000e+00 1.5818741e-23 5.8175087e-37 0.0000000e+00
 4.9659046e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 16:42:59,168] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9248
[2019-04-01 16:42:59,205] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 54.33333333333334, 113.5, 806.3333333333333, 26.0, 25.28987306695731, 9.5695808249097, 0.0, 1.0, 20.19066201984876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3583200.0000, 
sim time next is 3583800.0000, 
raw observation next is [-3.5, 54.5, 114.0, 816.0, 26.0, 25.23372440772533, 9.443026902069356, 0.0, 1.0, 20.25679138662138], 
processed observation next is [0.0, 0.4782608695652174, 0.36565096952908593, 0.545, 0.38, 0.901657458563536, 1.0, 0.8905320582464756, 0.09443026902069356, 0.0, 1.0, 0.14469136704729557], 
reward next is 0.8553, 
noisyNet noise sample is [array([1.0267441], dtype=float32), -1.7347591]. 
=============================================
[2019-04-01 16:43:06,539] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.1263248e-28 1.9327408e-38 1.2709959e-35 1.4588217e-36 2.6378813e-27
 2.9084350e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 16:43:06,561] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6995
[2019-04-01 16:43:06,585] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 63.0, 0.0, 0.0, 26.0, 25.59789208259067, 10.03576572438074, 0.0, 1.0, 40.53577711876198], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3700800.0000, 
sim time next is 3701400.0000, 
raw observation next is [2.833333333333333, 62.83333333333333, 0.0, 0.0, 26.0, 25.71359437391195, 10.15928501339326, 0.0, 1.0, 32.75111354751201], 
processed observation next is [0.0, 0.8695652173913043, 0.541089566020314, 0.6283333333333333, 0.0, 0.0, 1.0, 0.9590849105588498, 0.1015928501339326, 0.0, 1.0, 0.2339365253393715], 
reward next is 0.7661, 
noisyNet noise sample is [array([0.05708829], dtype=float32), -0.32848302]. 
=============================================
[2019-04-01 16:43:08,555] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.6939373e-28 2.4465280e-35 7.7356673e-29 9.4231427e-36 2.8173883e-19
 5.9747621e-30 1.0000000e+00], sum to 1.0000
[2019-04-01 16:43:08,560] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9941
[2019-04-01 16:43:08,587] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.32060424014179, 10.39613814506753, 1.0, 1.0, 27.8154967368561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3783000.0000, 
sim time next is 3783600.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.38648878430656, 10.40024171446884, 1.0, 1.0, 25.17097131759197], 
processed observation next is [1.0, 0.8260869565217391, 0.40720221606648205, 0.71, 0.0, 0.0, 1.0, 0.912355540615223, 0.10400241714468841, 1.0, 1.0, 0.17979265226851407], 
reward next is 0.6601, 
noisyNet noise sample is [array([-0.18354647], dtype=float32), -0.5753812]. 
=============================================
[2019-04-01 16:43:09,175] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.2082898e-36 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:43:09,175] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4640
[2019-04-01 16:43:09,193] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 65.5, 17.0, 147.0, 26.0, 25.80453281878551, 11.05691523897021, 1.0, 1.0, 8.596936090565638], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3778200.0000, 
sim time next is 3778800.0000, 
raw observation next is [-1.333333333333333, 67.33333333333333, 14.16666666666667, 122.5, 26.0, 25.68379722126548, 11.8611854507507, 1.0, 1.0, 24.59784379918922], 
processed observation next is [1.0, 0.7391304347826086, 0.42566943674976926, 0.6733333333333333, 0.047222222222222235, 0.13535911602209943, 1.0, 0.954828174466497, 0.118611854507507, 1.0, 1.0, 0.175698884279923], 
reward next is 0.0798, 
noisyNet noise sample is [array([1.1829109], dtype=float32), -0.39180973]. 
=============================================
[2019-04-01 16:43:10,804] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.35617912e-27 1.32083165e-33 1.18980175e-29 1.27100556e-35
 1.84232535e-19 3.27720720e-17 1.00000000e+00], sum to 1.0000
[2019-04-01 16:43:10,805] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3715
[2019-04-01 16:43:10,842] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 66.0, 0.0, 0.0, 26.0, 25.20306201089377, 9.649000142237862, 1.0, 1.0, 24.74253624973425], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3786600.0000, 
sim time next is 3787200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.16510324409939, 9.547756336804872, 1.0, 1.0, 26.77494831561547], 
processed observation next is [1.0, 0.8695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 1.0, 0.8807290348713417, 0.09547756336804872, 1.0, 1.0, 0.1912496308258248], 
reward next is 0.8088, 
noisyNet noise sample is [array([-1.0034572], dtype=float32), -0.07284276]. 
=============================================
[2019-04-01 16:43:13,011] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7172133e-15 1.1798028e-24 3.5361123e-21 1.8548832e-28 2.2588847e-18
 6.1770243e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 16:43:13,012] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1696
[2019-04-01 16:43:13,023] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 50.00000000000001, 113.6666666666667, 825.8333333333334, 26.0, 26.32571653105915, 14.88681249502338, 1.0, 1.0, 9.821722195604956], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3849600.0000, 
sim time next is 3850200.0000, 
raw observation next is [1.5, 49.5, 113.0, 824.0, 26.0, 26.53925809759923, 15.69013656546591, 1.0, 1.0, 9.442769859975373], 
processed observation next is [1.0, 0.5652173913043478, 0.5041551246537397, 0.495, 0.37666666666666665, 0.9104972375690608, 1.0, 1.0770368710856044, 0.1569013656546591, 1.0, 1.0, 0.06744835614268123], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9587638], dtype=float32), -2.510333]. 
=============================================
[2019-04-01 16:43:26,880] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 16:43:26,881] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6803
[2019-04-01 16:43:26,895] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 36.33333333333333, 81.33333333333333, 373.6666666666667, 26.0, 27.38587444465803, 18.32380383381727, 1.0, 1.0, 1.383028964116201], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4120800.0000, 
sim time next is 4121400.0000, 
raw observation next is [3.166666666666667, 36.66666666666667, 69.66666666666667, 310.3333333333334, 26.0, 26.88688886845548, 17.00874643708984, 1.0, 1.0, 1.431594549233412], 
processed observation next is [1.0, 0.6956521739130435, 0.5503231763619576, 0.3666666666666667, 0.23222222222222225, 0.3429097605893187, 1.0, 1.1266984097793542, 0.1700874643708984, 1.0, 1.0, 0.010225675351667228], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.12932491], dtype=float32), -0.59560996]. 
=============================================
[2019-04-01 16:43:29,800] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.89953255e-34 0.00000000e+00 2.01772768e-32 0.00000000e+00
 0.00000000e+00 1.28112085e-20 1.00000000e+00], sum to 1.0000
[2019-04-01 16:43:29,800] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8552
[2019-04-01 16:43:29,811] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.22152025013982, 7.786369946806599, 0.0, 1.0, 39.09960962705225], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4160400.0000, 
sim time next is 4161000.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.1979633733989, 7.725431761374314, 0.0, 1.0, 39.10228423957097], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 1.0, 0.8854233390569856, 0.07725431761374314, 0.0, 1.0, 0.27930203028264977], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.03094728], dtype=float32), -0.84581983]. 
=============================================
[2019-04-01 16:43:29,831] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.77283 ]
 [81.833336]
 [81.927124]
 [82.10121 ]
 [82.228   ]], R is [[81.63206482]
 [81.53646088]
 [81.44181824]
 [81.34790802]
 [81.25494385]].
[2019-04-01 16:43:30,995] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.0361031e-26 1.0821021e-33 4.3824968e-31 1.4768128e-35 5.7662918e-23
 5.5273255e-25 1.0000000e+00], sum to 1.0000
[2019-04-01 16:43:30,999] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0104
[2019-04-01 16:43:31,009] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 69.0, 0.0, 0.0, 26.0, 24.92263086069122, 7.204578595959468, 0.0, 1.0, 14.29552694521914], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4302000.0000, 
sim time next is 4302600.0000, 
raw observation next is [5.9, 69.66666666666667, 0.0, 0.0, 26.0, 24.87857243819548, 7.155738919652975, 0.0, 1.0, 16.74368714115296], 
processed observation next is [0.0, 0.8260869565217391, 0.626038781163435, 0.6966666666666668, 0.0, 0.0, 1.0, 0.8397960625993542, 0.07155738919652975, 0.0, 1.0, 0.11959776529394972], 
reward next is 0.8804, 
noisyNet noise sample is [array([-1.637956], dtype=float32), -0.2384444]. 
=============================================
[2019-04-01 16:43:32,075] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0719387e-26 0.0000000e+00 7.6603916e-32 0.0000000e+00 0.0000000e+00
 6.5774000e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 16:43:32,081] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4889
[2019-04-01 16:43:32,120] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 35.0, 113.0, 755.0, 26.0, 25.38679239466088, 9.05328489420198, 0.0, 1.0, 17.57803992537151], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4184400.0000, 
sim time next is 4185000.0000, 
raw observation next is [-1.5, 35.0, 114.0, 774.0, 26.0, 25.37421453141057, 8.997186366565261, 0.0, 1.0, 16.46143074317652], 
processed observation next is [0.0, 0.43478260869565216, 0.4210526315789474, 0.35, 0.38, 0.8552486187845304, 1.0, 0.9106020759157956, 0.08997186366565262, 0.0, 1.0, 0.11758164816554657], 
reward next is 0.8824, 
noisyNet noise sample is [array([-1.5469652], dtype=float32), 0.8360402]. 
=============================================
[2019-04-01 16:43:32,132] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.791565]
 [75.5085  ]
 [75.1171  ]
 [74.67395 ]
 [74.24483 ]], R is [[76.25086212]
 [76.3628006 ]
 [76.46495819]
 [76.5566864 ]
 [76.63729095]].
[2019-04-01 16:43:32,362] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.5800259e-24 7.1788839e-32 4.5229836e-26 1.5249681e-30 3.6592279e-22
 9.9408120e-01 5.9187845e-03], sum to 1.0000
[2019-04-01 16:43:32,362] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2481
[2019-04-01 16:43:32,377] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.8, 69.66666666666667, 0.0, 0.0, 26.0, 25.65987301942629, 8.261647289342763, 0.0, 1.0, 25.52241262045327], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4335600.0000, 
sim time next is 4336200.0000, 
raw observation next is [3.75, 69.5, 0.0, 0.0, 26.0, 25.59341342337952, 8.07374789174662, 0.0, 1.0, 22.78021712025368], 
processed observation next is [1.0, 0.17391304347826086, 0.5664819944598338, 0.695, 0.0, 0.0, 1.0, 0.9419162033399314, 0.0807374789174662, 0.0, 1.0, 0.16271583657324057], 
reward next is 0.8373, 
noisyNet noise sample is [array([-2.144538], dtype=float32), -0.78834784]. 
=============================================
[2019-04-01 16:43:34,550] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4861106e-27 0.0000000e+00 7.9331081e-32 2.3762000e-37 1.1487684e-33
 2.6809294e-03 9.9731904e-01], sum to 1.0000
[2019-04-01 16:43:34,551] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0118
[2019-04-01 16:43:34,569] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.35703614842416, 7.287231662413561, 0.0, 1.0, 39.12724067687465], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4255800.0000, 
sim time next is 4256400.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.35090386089714, 7.290120754123014, 0.0, 1.0, 39.14749823012799], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 1.0, 0.907271980128163, 0.07290120754123014, 0.0, 1.0, 0.279624987358057], 
reward next is 0.7204, 
noisyNet noise sample is [array([0.2844946], dtype=float32), 0.7009971]. 
=============================================
[2019-04-01 16:43:38,159] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.5297023e-31 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.1365351e-31
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:43:38,160] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9134
[2019-04-01 16:43:38,166] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.0, 38.0, 24.0, 0.0, 26.0, 28.37063494761064, 29.99463428294846, 1.0, 1.0, 6.122068637058105], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4381200.0000, 
sim time next is 4381800.0000, 
raw observation next is [12.9, 39.0, 19.0, 0.0, 26.0, 28.44184589818758, 30.73150547321094, 1.0, 1.0, 5.706956603011707], 
processed observation next is [1.0, 0.7391304347826086, 0.8199445983379503, 0.39, 0.06333333333333334, 0.0, 1.0, 1.3488351283125115, 0.3073150547321094, 1.0, 1.0, 0.04076397573579791], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.47985837], dtype=float32), -0.43454757]. 
=============================================
[2019-04-01 16:43:39,373] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9590073e-22 1.1986982e-30 6.9066064e-25 2.1078155e-28 7.9121811e-18
 1.7085982e-07 9.9999988e-01], sum to 1.0000
[2019-04-01 16:43:39,381] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2543
[2019-04-01 16:43:39,393] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.916666666666666, 66.16666666666667, 0.0, 0.0, 26.0, 25.96781903827211, 13.0089939869964, 0.0, 1.0, 14.29550039063035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4414200.0000, 
sim time next is 4414800.0000, 
raw observation next is [5.733333333333334, 66.33333333333334, 0.0, 0.0, 26.0, 25.92944382715795, 12.62476441828385, 0.0, 1.0, 13.65119828216911], 
processed observation next is [1.0, 0.08695652173913043, 0.6214219759926132, 0.6633333333333334, 0.0, 0.0, 1.0, 0.98992054673685, 0.1262476441828385, 0.0, 1.0, 0.09750855915835079], 
reward next is 0.9025, 
noisyNet noise sample is [array([-0.32195607], dtype=float32), -1.2426063]. 
=============================================
[2019-04-01 16:43:41,791] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.8582633e-22 7.3024842e-26 5.3040382e-15 2.1606769e-27 1.2446002e-16
 5.2373807e-25 1.0000000e+00], sum to 1.0000
[2019-04-01 16:43:41,793] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7350
[2019-04-01 16:43:41,802] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 135.3333333333333, 0.0, 26.0, 26.48275120877264, 13.82040196360821, 1.0, 1.0, 7.847953369878865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4448400.0000, 
sim time next is 4449000.0000, 
raw observation next is [1.0, 86.0, 128.6666666666667, 0.0, 26.0, 26.4786890642002, 13.66926040232417, 1.0, 1.0, 8.928284248568925], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.42888888888888904, 0.0, 1.0, 1.0683841520286, 0.1366926040232417, 1.0, 1.0, 0.06377345891834947], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7255778], dtype=float32), -0.8111456]. 
=============================================
[2019-04-01 16:43:41,828] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[18.22307 ]
 [17.896215]
 [17.650877]
 [17.507969]
 [17.573942]], R is [[18.29503632]
 [18.11208534]
 [17.93096542]
 [17.75165558]
 [17.57413864]].
[2019-04-01 16:43:43,402] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0497614e-21 1.5473372e-34 3.0219403e-29 5.6657133e-34 6.5067242e-22
 6.8244481e-01 3.1755522e-01], sum to 1.0000
[2019-04-01 16:43:43,403] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0664
[2019-04-01 16:43:43,421] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.6442422298247, 10.23369950795758, 1.0, 1.0, 10.20334472371741], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4558800.0000, 
sim time next is 4559400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.58050780409958, 9.845277416733383, 1.0, 1.0, 10.32990557853327], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.940072543442797, 0.09845277416733383, 1.0, 1.0, 0.0737850398466662], 
reward next is 0.9262, 
noisyNet noise sample is [array([-0.681366], dtype=float32), 0.76996094]. 
=============================================
[2019-04-01 16:43:49,010] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.9751694e-29 0.0000000e+00 1.2089404e-35 1.5985323e-37 5.9668873e-28
 1.0000000e+00 4.7555553e-25], sum to 1.0000
[2019-04-01 16:43:49,011] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3439
[2019-04-01 16:43:49,022] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.166666666666667, 67.33333333333334, 0.0, 0.0, 26.0, 25.2905627753327, 8.690394019460953, 0.0, 1.0, 37.70169636838281], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4590600.0000, 
sim time next is 4591200.0000, 
raw observation next is [-1.233333333333333, 67.66666666666667, 0.0, 0.0, 26.0, 25.30938773783019, 8.959422684879348, 0.0, 1.0, 37.47671258479957], 
processed observation next is [1.0, 0.13043478260869565, 0.4284395198522623, 0.6766666666666667, 0.0, 0.0, 1.0, 0.9013411054043127, 0.08959422684879348, 0.0, 1.0, 0.2676908041771398], 
reward next is 0.7323, 
noisyNet noise sample is [array([0.9150781], dtype=float32), 1.6045752]. 
=============================================
[2019-04-01 16:43:53,951] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0664935e-22 7.6398390e-34 7.6333547e-31 1.4132357e-31 1.9370065e-16
 4.5222794e-11 1.0000000e+00], sum to 1.0000
[2019-04-01 16:43:53,952] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2440
[2019-04-01 16:43:53,970] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 52.16666666666667, 0.0, 0.0, 26.0, 25.61187858469572, 11.44151380035168, 0.0, 1.0, 7.686453546656852], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4650600.0000, 
sim time next is 4651200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.53755707468103, 11.15897000296092, 0.0, 1.0, 8.83404518716757], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.9339367249544327, 0.1115897000296092, 0.0, 1.0, 0.06310032276548265], 
reward next is 0.9369, 
noisyNet noise sample is [array([0.88461626], dtype=float32), -0.17267863]. 
=============================================
[2019-04-01 16:43:55,609] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1641402e-08 9.9135462e-28 3.4406018e-16 2.5163384e-20 4.2127721e-20
 9.9982613e-01 1.7389830e-04], sum to 1.0000
[2019-04-01 16:43:55,609] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1485
[2019-04-01 16:43:55,670] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 58.66666666666667, 156.5, 678.5, 26.0, 25.54526784828392, 9.985168054169653, 0.0, 1.0, 23.60817100380858], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4789200.0000, 
sim time next is 4789800.0000, 
raw observation next is [-2.5, 55.5, 153.0, 730.0, 26.0, 25.4895952748068, 9.894094384870883, 0.0, 1.0, 23.79696745196267], 
processed observation next is [0.0, 0.43478260869565216, 0.39335180055401664, 0.555, 0.51, 0.8066298342541437, 1.0, 0.9270850392581141, 0.09894094384870883, 0.0, 1.0, 0.16997833894259048], 
reward next is 0.8300, 
noisyNet noise sample is [array([-0.5050081], dtype=float32), -0.35259154]. 
=============================================
[2019-04-01 16:43:59,049] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.0344425e-26 3.4318295e-37 6.0361408e-28 4.1787461e-31 3.5610787e-26
 1.0000000e+00 1.4073065e-17], sum to 1.0000
[2019-04-01 16:43:59,049] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8672
[2019-04-01 16:43:59,138] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.666666666666666, 87.0, 103.3333333333333, 349.1666666666667, 26.0, 24.27377006741339, 8.29696140633378, 0.0, 1.0, 88.4498537763264], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4782000.0000, 
sim time next is 4782600.0000, 
raw observation next is [-5.5, 84.5, 124.0, 419.0, 26.0, 25.22845510114001, 9.737147062075103, 0.0, 1.0, 59.0957453215544], 
processed observation next is [0.0, 0.34782608695652173, 0.3102493074792244, 0.845, 0.41333333333333333, 0.46298342541436466, 1.0, 0.8897793001628587, 0.09737147062075102, 0.0, 1.0, 0.42211246658253143], 
reward next is 0.5779, 
noisyNet noise sample is [array([-0.01857337], dtype=float32), -0.5653868]. 
=============================================
[2019-04-01 16:44:08,480] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5288847e-26 1.1198030e-35 3.1597852e-32 4.9063739e-28 1.5348185e-25
 1.0000000e+00 1.4950530e-28], sum to 1.0000
[2019-04-01 16:44:08,483] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4232
[2019-04-01 16:44:08,500] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.50476881774068, 7.432585677102352, 0.0, 1.0, 32.90565726985815], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4933200.0000, 
sim time next is 4933800.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.48214125308656, 7.235439528857488, 0.0, 1.0, 27.60311251075502], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 1.0, 0.9260201790123658, 0.07235439528857487, 0.0, 1.0, 0.19716508936253588], 
reward next is 0.8028, 
noisyNet noise sample is [array([-0.8743572], dtype=float32), -1.5907934]. 
=============================================
[2019-04-01 16:44:08,522] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9751036e-25 3.7093405e-26 7.0473648e-19 6.0857571e-28 3.4426953e-17
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 16:44:08,526] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7243
[2019-04-01 16:44:08,551] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.666666666666666, 25.33333333333333, 115.6666666666667, 853.1666666666667, 26.0, 26.60793081862248, 15.69380090997828, 1.0, 1.0, 1.983536906860141], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4974000.0000, 
sim time next is 4974600.0000, 
raw observation next is [7.833333333333334, 25.66666666666667, 114.3333333333333, 846.3333333333333, 26.0, 25.96278998049301, 14.8756331671168, 1.0, 1.0, 1.881730414752068], 
processed observation next is [1.0, 0.5652173913043478, 0.6795937211449677, 0.2566666666666667, 0.381111111111111, 0.9351749539594842, 1.0, 0.9946842829275729, 0.148756331671168, 1.0, 1.0, 0.013440931533943342], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.69136655], dtype=float32), -0.002043089]. 
=============================================
[2019-04-01 16:44:10,454] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3654777e-15 7.1681451e-29 6.1358567e-19 1.1126466e-25 1.5647876e-15
 4.1111618e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 16:44:10,455] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1035
[2019-04-01 16:44:10,465] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 25.0, 122.5, 855.0, 26.0, 27.00607406277053, 15.80511827755985, 1.0, 1.0, 4.448508616632821], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4968000.0000, 
sim time next is 4968600.0000, 
raw observation next is [6.166666666666666, 24.83333333333334, 122.6666666666667, 858.3333333333334, 26.0, 27.05754603097006, 16.29563202139196, 1.0, 1.0, 4.029912083694413], 
processed observation next is [1.0, 0.5217391304347826, 0.6334256694367498, 0.2483333333333334, 0.408888888888889, 0.9484346224677717, 1.0, 1.151078004424294, 0.1629563202139196, 1.0, 1.0, 0.02878508631210295], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.36861554], dtype=float32), 1.4342024]. 
=============================================
[2019-04-01 16:44:10,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:10,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:10,868] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run9
[2019-04-01 16:44:11,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:11,447] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:11,467] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run9
[2019-04-01 16:44:11,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:11,514] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:11,546] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run9
[2019-04-01 16:44:11,671] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 16:44:11,673] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8806
[2019-04-01 16:44:11,682] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.6, 19.33333333333334, 0.0, 0.0, 26.0, 26.61581493402993, 14.61181181308113, 0.0, 1.0, 6.421803795028493], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5091600.0000, 
sim time next is 5092200.0000, 
raw observation next is [8.55, 19.5, 0.0, 0.0, 26.0, 26.46490241397981, 13.91635974575728, 0.0, 1.0, 6.562825557711768], 
processed observation next is [1.0, 0.9565217391304348, 0.6994459833795015, 0.195, 0.0, 0.0, 1.0, 1.0664146305685445, 0.13916359745757279, 0.0, 1.0, 0.04687732541222691], 
reward next is 0.9531, 
noisyNet noise sample is [array([-1.0310421], dtype=float32), -1.0718267]. 
=============================================
[2019-04-01 16:44:11,749] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.8770715e-30 0.0000000e+00 7.0240630e-28 0.0000000e+00 0.0000000e+00
 3.6689608e-36 1.0000000e+00], sum to 1.0000
[2019-04-01 16:44:11,753] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2117
[2019-04-01 16:44:11,781] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 18.0, 76.0, 585.0, 26.0, 28.88764752048184, 33.97124060024692, 1.0, 1.0, 0.6106938831898551], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5070600.0000, 
sim time next is 5071200.0000, 
raw observation next is [12.0, 17.66666666666666, 69.33333333333333, 536.1666666666666, 26.0, 28.93935966594068, 34.5810455924187, 1.0, 1.0, 0.6329245544209606], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.1766666666666666, 0.2311111111111111, 0.5924493554327808, 1.0, 1.4199085237058118, 0.34581045592418697, 1.0, 1.0, 0.004520889674435433], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1053141], dtype=float32), -0.19471045]. 
=============================================
[2019-04-01 16:44:12,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:12,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:12,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run9
[2019-04-01 16:44:12,938] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:12,938] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:12,976] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run9
[2019-04-01 16:44:13,514] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:13,514] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:13,519] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run9
[2019-04-01 16:44:13,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:13,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:13,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run9
[2019-04-01 16:44:13,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:13,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:13,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run9
[2019-04-01 16:44:14,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:14,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:14,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run9
[2019-04-01 16:44:14,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:14,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:14,467] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run9
[2019-04-01 16:44:14,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:14,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:14,518] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run9
[2019-04-01 16:44:14,604] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:14,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:14,608] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run9
[2019-04-01 16:44:15,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:15,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:15,104] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run9
[2019-04-01 16:44:16,231] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:16,232] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:16,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run9
[2019-04-01 16:44:16,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:16,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:16,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:44:16,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:44:16,356] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run9
[2019-04-01 16:44:16,434] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run9
[2019-04-01 16:44:24,954] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5110222e-34 0.0000000e+00 0.0000000e+00 8.0642234e-36 9.9857763e-32
 1.0000000e+00 1.8652669e-25], sum to 1.0000
[2019-04-01 16:44:24,954] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8080
[2019-04-01 16:44:24,970] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3666666666666667, 95.33333333333333, 0.0, 0.0, 26.0, 24.47376356759727, 6.22102441406567, 0.0, 1.0, 39.59748331063272], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 81600.0000, 
sim time next is 82200.0000, 
raw observation next is [0.3333333333333333, 95.16666666666667, 0.0, 0.0, 26.0, 24.44926770127914, 6.173495799851644, 0.0, 1.0, 39.57969885462875], 
processed observation next is [0.0, 0.9565217391304348, 0.4718374884579871, 0.9516666666666667, 0.0, 0.0, 1.0, 0.7784668144684487, 0.06173495799851644, 0.0, 1.0, 0.28271213467591966], 
reward next is 0.7173, 
noisyNet noise sample is [array([1.5016824], dtype=float32), 1.3715048]. 
=============================================
[2019-04-01 16:44:34,396] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2651685e-25 0.0000000e+00 2.8526903e-32 1.1733836e-33 5.4837287e-24
 1.0000000e+00 1.9663259e-22], sum to 1.0000
[2019-04-01 16:44:34,397] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3174
[2019-04-01 16:44:34,456] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.116666666666666, 74.5, 109.0, 0.0, 26.0, 25.37797199220229, 6.708361399664336, 1.0, 1.0, 36.87925334740687], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 209400.0000, 
sim time next is 210000.0000, 
raw observation next is [-6.933333333333334, 74.0, 116.5, 0.0, 26.0, 25.39090028569193, 6.715642779177853, 1.0, 1.0, 36.6531005686849], 
processed observation next is [1.0, 0.43478260869565216, 0.270544783010157, 0.74, 0.3883333333333333, 0.0, 1.0, 0.9129857550988473, 0.06715642779177852, 1.0, 1.0, 0.26180786120489213], 
reward next is 0.7382, 
noisyNet noise sample is [array([2.4172013], dtype=float32), -0.13893506]. 
=============================================
[2019-04-01 16:44:34,488] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[68.90751]
 [68.4474 ]
 [67.9097 ]
 [67.43216]
 [66.71828]], R is [[69.1497345 ]
 [69.19481659]
 [69.24038696]
 [69.29122925]
 [69.36412811]].
[2019-04-01 16:44:40,986] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2749930e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6919469e-28
 1.0000000e+00 5.6137115e-30], sum to 1.0000
[2019-04-01 16:44:40,987] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8348
[2019-04-01 16:44:41,035] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.3, 80.0, 0.0, 0.0, 26.0, 24.20783419972478, 5.70293723015115, 0.0, 1.0, 43.74711426466891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 258000.0000, 
sim time next is 258600.0000, 
raw observation next is [-4.399999999999999, 79.5, 0.0, 0.0, 26.0, 24.16883361922463, 5.661067527569803, 0.0, 1.0, 43.75026506231969], 
processed observation next is [1.0, 1.0, 0.3407202216066483, 0.795, 0.0, 0.0, 1.0, 0.7384048027463754, 0.05661067527569803, 0.0, 1.0, 0.3125018933022835], 
reward next is 0.6875, 
noisyNet noise sample is [array([1.2904886], dtype=float32), -0.32049763]. 
=============================================
[2019-04-01 16:44:41,775] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8048158e-12 7.4659600e-34 2.2801915e-16 4.3979747e-28 1.7196158e-17
 3.5460435e-03 9.9645400e-01], sum to 1.0000
[2019-04-01 16:44:41,776] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1468
[2019-04-01 16:44:41,838] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.51666666666667, 62.5, 89.66666666666667, 469.0, 26.0, 25.87869000365266, 8.686960611576788, 1.0, 1.0, 34.42320632447637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 295800.0000, 
sim time next is 296400.0000, 
raw observation next is [-11.33333333333333, 62.00000000000001, 88.33333333333333, 490.5, 26.0, 25.91954159300919, 8.793688340614791, 1.0, 1.0, 33.01747909062719], 
processed observation next is [1.0, 0.43478260869565216, 0.14866112650046176, 0.6200000000000001, 0.29444444444444445, 0.541988950276243, 1.0, 0.9885059418584555, 0.08793688340614791, 1.0, 1.0, 0.2358391363616228], 
reward next is 0.7642, 
noisyNet noise sample is [array([-0.06372321], dtype=float32), 0.671779]. 
=============================================
[2019-04-01 16:44:44,093] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.2067502e-27 0.0000000e+00 5.2272024e-32 1.8781603e-34 2.1114668e-23
 1.0000000e+00 1.7582099e-31], sum to 1.0000
[2019-04-01 16:44:44,099] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9450
[2019-04-01 16:44:44,126] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.333333333333334, 69.0, 0.0, 0.0, 26.0, 24.12332836932416, 5.528785448913058, 0.0, 1.0, 44.15013571212118], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 262200.0000, 
sim time next is 262800.0000, 
raw observation next is [-6.7, 67.0, 0.0, 0.0, 26.0, 24.12070646607608, 5.506240254478793, 0.0, 1.0, 44.33478500765112], 
processed observation next is [1.0, 0.043478260869565216, 0.2770083102493075, 0.67, 0.0, 0.0, 1.0, 0.7315294951537256, 0.055062402544787926, 0.0, 1.0, 0.3166770357689366], 
reward next is 0.6833, 
noisyNet noise sample is [array([-1.2575839], dtype=float32), -0.2545837]. 
=============================================
[2019-04-01 16:45:00,882] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-01 16:45:00,882] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8958557e-32
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 16:45:00,884] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 16:45:00,884] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4667
[2019-04-01 16:45:00,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:45:00,885] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 16:45:00,886] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:45:00,888] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 16:45:00,897] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:45:00,899] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.61300156451609, 5.552461543709946, 0.0, 1.0, 44.7715554370595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 435600.0000, 
sim time next is 436200.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.58537140386114, 5.552039793710755, 0.0, 1.0, 44.82296961265564], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 1.0, 0.6550530576944483, 0.05552039793710754, 0.0, 1.0, 0.320164068661826], 
reward next is 0.6798, 
noisyNet noise sample is [array([1.1603172], dtype=float32), 0.9271329]. 
=============================================
[2019-04-01 16:45:00,901] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run12
[2019-04-01 16:45:00,917] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run12
[2019-04-01 16:45:00,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run12
[2019-04-01 16:45:29,199] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.8872084], dtype=float32), -0.3892147]
[2019-04-01 16:45:29,199] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.842050506166667, 51.88090196833333, 40.24943595333334, 435.8363096666666, 26.0, 25.92783079261597, 9.535648856885963, 1.0, 1.0, 10.23138786010951]
[2019-04-01 16:45:29,200] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 16:45:29,201] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.6003093e-20 3.7708029e-33 2.6435679e-23 4.1991033e-31 2.5894062e-20
 1.0405856e-07 9.9999988e-01], sampled 0.8686110785515144
[2019-04-01 16:46:27,850] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.8872084], dtype=float32), -0.3892147]
[2019-04-01 16:46:27,850] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.6, 48.0, 120.0, 774.5, 26.0, 26.16202751962819, 12.58939165264355, 1.0, 1.0, 9.543776667944371]
[2019-04-01 16:46:27,850] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 16:46:27,851] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.0314779e-19 1.1532795e-33 6.2200196e-22 3.1815660e-31 2.4786234e-20
 8.9824944e-06 9.9999106e-01], sampled 0.6398983262684743
[2019-04-01 16:46:33,701] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.8872084], dtype=float32), -0.3892147]
[2019-04-01 16:46:33,701] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [11.7, 41.0, 96.83333333333334, 724.0, 26.0, 25.78016708589455, 12.06031042989054, 0.0, 1.0, 13.09479610583093]
[2019-04-01 16:46:33,702] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 16:46:33,702] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.0767117e-26 0.0000000e+00 1.2078344e-27 3.8663097e-34 1.8006638e-27
 1.0000000e+00 1.7316674e-13], sampled 0.1326051351789468
[2019-04-01 16:46:43,227] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 16:47:02,297] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 16:47:06,596] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 16:47:07,619] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1100000, evaluation results [1100000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 16:47:11,812] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 16:47:11,812] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3430
[2019-04-01 16:47:11,859] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 24.79150088995238, 6.516633542695625, 1.0, 1.0, 28.02491464122853], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 501000.0000, 
sim time next is 501600.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 24.66590614740071, 6.44087015885147, 0.0, 1.0, 39.00908289243969], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 1.0, 0.8094151639143874, 0.0644087015885147, 0.0, 1.0, 0.2786363063745692], 
reward next is 0.7214, 
noisyNet noise sample is [array([-0.27596083], dtype=float32), 0.38619474]. 
=============================================
[2019-04-01 16:47:15,275] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9433479e-25 0.0000000e+00 7.7274164e-27 1.1614892e-32 9.4375710e-28
 1.0000000e+00 1.6418041e-16], sum to 1.0000
[2019-04-01 16:47:15,275] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4749
[2019-04-01 16:47:15,329] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 80.5, 130.0, 396.0, 26.0, 25.03117622112433, 7.873507435941522, 0.0, 1.0, 26.12958338768838], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 563400.0000, 
sim time next is 564000.0000, 
raw observation next is [-1.066666666666667, 80.33333333333333, 131.3333333333333, 429.1666666666666, 26.0, 25.00806315311193, 7.883588664418845, 0.0, 1.0, 24.93788638169879], 
processed observation next is [0.0, 0.5217391304347826, 0.43305632502308405, 0.8033333333333332, 0.4377777777777776, 0.4742173112338857, 1.0, 0.8582947361588472, 0.07883588664418845, 0.0, 1.0, 0.17812775986927706], 
reward next is 0.8219, 
noisyNet noise sample is [array([1.246512], dtype=float32), 0.6357017]. 
=============================================
[2019-04-01 16:47:15,337] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[69.38437]
 [69.36651]
 [69.28335]
 [69.20966]
 [69.11979]], R is [[69.43599701]
 [69.55500031]
 [69.64694214]
 [69.69759369]
 [69.73543549]].
[2019-04-01 16:47:43,320] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3729342e-33 0.0000000e+00 2.0180775e-36 0.0000000e+00 3.4219015e-33
 1.0000000e+00 4.2024832e-17], sum to 1.0000
[2019-04-01 16:47:43,321] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7969
[2019-04-01 16:47:43,341] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.61666666666667, 86.0, 125.0, 0.0, 26.0, 25.68095329161103, 11.88770819633515, 1.0, 1.0, 5.291607159993208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 996600.0000, 
sim time next is 997200.0000, 
raw observation next is [12.7, 86.0, 123.5, 0.0, 26.0, 25.98421857087689, 12.52396418393295, 1.0, 1.0, 4.82626506185635], 
processed observation next is [1.0, 0.5652173913043478, 0.8144044321329641, 0.86, 0.4116666666666667, 0.0, 1.0, 0.9977455101252701, 0.1252396418393295, 1.0, 1.0, 0.034473321870402494], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5755431], dtype=float32), 0.2758748]. 
=============================================
[2019-04-01 16:47:43,711] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7080140e-23 0.0000000e+00 8.4111123e-32 5.1554824e-37 1.5234520e-22
 1.0000000e+00 6.2209068e-14], sum to 1.0000
[2019-04-01 16:47:43,711] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7441
[2019-04-01 16:47:43,725] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 114.0, 0.0, 26.0, 27.06061018767629, 19.90807063056261, 1.0, 1.0, 5.954659537557662], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1074600.0000, 
sim time next is 1075200.0000, 
raw observation next is [14.76666666666667, 73.33333333333334, 137.3333333333333, 35.83333333333333, 26.0, 27.1138625999306, 20.57780577924424, 1.0, 1.0, 5.549496751934778], 
processed observation next is [1.0, 0.43478260869565216, 0.8716528162511544, 0.7333333333333334, 0.4577777777777776, 0.03959484346224677, 1.0, 1.159123228561514, 0.2057780577924424, 1.0, 1.0, 0.03963926251381984], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.24750477], dtype=float32), 1.3953289]. 
=============================================
[2019-04-01 16:47:43,884] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8088983e-34 0.0000000e+00 2.1199141e-33 0.0000000e+00 1.2935378e-16
 1.0000000e+00 3.1566656e-09], sum to 1.0000
[2019-04-01 16:47:43,887] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5995
[2019-04-01 16:47:43,901] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.91666666666667, 53.66666666666667, 0.0, 0.0, 26.0, 26.84259328479545, 19.51458704158955, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1102200.0000, 
sim time next is 1102800.0000, 
raw observation next is [15.73333333333334, 54.33333333333334, 0.0, 0.0, 26.0, 26.75603150203466, 19.08203831702833, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8984302862419208, 0.5433333333333334, 0.0, 0.0, 1.0, 1.1080045002906656, 0.19082038317028333, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2496647], dtype=float32), 0.3925937]. 
=============================================
[2019-04-01 16:47:44,257] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.5663540e-28 0.0000000e+00 1.5013023e-32 4.7549527e-34 8.1840288e-22
 1.0000000e+00 2.2594392e-23], sum to 1.0000
[2019-04-01 16:47:44,259] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6214
[2019-04-01 16:47:44,271] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.2, 83.0, 18.5, 58.66666666666666, 26.0, 25.70553888769643, 13.03110008777458, 1.0, 1.0, 12.27100648201], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1066800.0000, 
sim time next is 1067400.0000, 
raw observation next is [12.2, 83.0, 22.0, 69.0, 26.0, 25.87047306237821, 13.85763131329637, 1.0, 1.0, 11.51284199481486], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.07333333333333333, 0.07624309392265194, 1.0, 0.9814961517683157, 0.1385763131329637, 1.0, 1.0, 0.08223458567724899], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.30380824], dtype=float32), 0.673917]. 
=============================================
[2019-04-01 16:47:45,125] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.5918832e-26
 1.0000000e+00 3.4397983e-37], sum to 1.0000
[2019-04-01 16:47:45,128] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5777
[2019-04-01 16:47:45,140] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.1, 62.66666666666667, 0.0, 0.0, 26.0, 25.79263907285289, 16.28596207687146, 0.0, 1.0, 24.17098482918323], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1113600.0000, 
sim time next is 1114200.0000, 
raw observation next is [13.0, 63.0, 0.0, 0.0, 26.0, 25.88439451937815, 16.44170804097175, 0.0, 1.0, 20.79137320160446], 
processed observation next is [1.0, 0.9130434782608695, 0.8227146814404434, 0.63, 0.0, 0.0, 1.0, 0.9834849313397359, 0.1644170804097175, 0.0, 1.0, 0.148509808582889], 
reward next is 0.8515, 
noisyNet noise sample is [array([-1.308091], dtype=float32), 0.63266677]. 
=============================================
[2019-04-01 16:47:47,490] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 16:47:47,491] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3827
[2019-04-01 16:47:47,506] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.0, 77.66666666666667, 0.0, 0.0, 26.0, 25.85191466351545, 12.64289068949347, 0.0, 1.0, 11.07122570134835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1045200.0000, 
sim time next is 1045800.0000, 
raw observation next is [14.1, 77.5, 0.0, 0.0, 26.0, 25.8398021582544, 12.12933960021952, 0.0, 1.0, 11.48347973642323], 
processed observation next is [1.0, 0.08695652173913043, 0.8531855955678671, 0.775, 0.0, 0.0, 1.0, 0.9771145940363429, 0.1212933960021952, 0.0, 1.0, 0.08202485526016594], 
reward next is 0.9180, 
noisyNet noise sample is [array([1.6499782], dtype=float32), 0.20459321]. 
=============================================
[2019-04-01 16:47:53,659] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.9662771e-34 0.0000000e+00 1.7868731e-30 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.1197211e-17], sum to 1.0000
[2019-04-01 16:47:53,666] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8138
[2019-04-01 16:47:53,728] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.3802239220833, 10.74798722003275, 0.0, 1.0, 40.59176575071481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1321200.0000, 
sim time next is 1321800.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.35461894827116, 11.71227139778572, 0.0, 1.0, 56.03947200822995], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.0, 0.0, 1.0, 0.9078027068958799, 0.1171227139778572, 0.0, 1.0, 0.4002819429159282], 
reward next is 0.5997, 
noisyNet noise sample is [array([0.17030363], dtype=float32), -0.13392277]. 
=============================================
[2019-04-01 16:47:55,210] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 3.0310146e-24], sum to 1.0000
[2019-04-01 16:47:55,214] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5981
[2019-04-01 16:47:55,231] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.52693624679293, 12.44677061177075, 0.0, 1.0, 28.0873423144797], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1286400.0000, 
sim time next is 1287000.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.47731875878325, 12.25453457022707, 0.0, 1.0, 30.20182998544846], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 1.0, 0.9253312512547502, 0.1225453457022707, 0.0, 1.0, 0.21572735703891757], 
reward next is 0.7843, 
noisyNet noise sample is [array([-1.0798892], dtype=float32), 0.7912521]. 
=============================================
[2019-04-01 16:47:55,245] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[88.55209 ]
 [88.389145]
 [88.22539 ]
 [88.04283 ]
 [87.8963  ]], R is [[88.64794159]
 [88.56083679]
 [88.48760223]
 [88.40510559]
 [88.31690979]].
[2019-04-01 16:47:57,215] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 3.415091e-35], sum to 1.0000
[2019-04-01 16:47:57,215] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0859
[2019-04-01 16:47:57,232] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.766666666666667, 97.33333333333334, 0.0, 0.0, 26.0, 25.40125650089939, 11.9419821610248, 0.0, 1.0, 35.21311208048014], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1294800.0000, 
sim time next is 1295400.0000, 
raw observation next is [4.583333333333334, 96.66666666666666, 0.0, 0.0, 26.0, 25.35869736286698, 12.00063785736416, 0.0, 1.0, 38.71589104349462], 
processed observation next is [0.0, 1.0, 0.5895660203139428, 0.9666666666666666, 0.0, 0.0, 1.0, 0.9083853375524258, 0.12000637857364159, 0.0, 1.0, 0.27654207888210447], 
reward next is 0.7235, 
noisyNet noise sample is [array([0.6718104], dtype=float32), 0.22396332]. 
=============================================
[2019-04-01 16:47:57,500] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8690875e-37
 1.0000000e+00 1.8505194e-24], sum to 1.0000
[2019-04-01 16:47:57,502] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8111
[2019-04-01 16:47:57,517] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 96.0, 0.0, 0.0, 26.0, 25.3432614092941, 12.12932408571891, 0.0, 1.0, 40.62250677756018], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1296000.0000, 
sim time next is 1296600.0000, 
raw observation next is [4.300000000000001, 95.5, 0.0, 0.0, 26.0, 25.35244545144814, 12.26461085148894, 0.0, 1.0, 40.58748738196588], 
processed observation next is [1.0, 0.0, 0.5817174515235458, 0.955, 0.0, 0.0, 1.0, 0.9074922073497343, 0.12264610851488938, 0.0, 1.0, 0.2899106241568991], 
reward next is 0.7101, 
noisyNet noise sample is [array([-1.4478304], dtype=float32), 0.479662]. 
=============================================
[2019-04-01 16:47:58,566] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2840104e-35 0.0000000e+00 3.0728811e-38 0.0000000e+00 2.9549639e-34
 1.0000000e+00 2.4317709e-35], sum to 1.0000
[2019-04-01 16:47:58,572] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1877
[2019-04-01 16:47:58,656] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.62901739486527, 12.1067356073586, 1.0, 1.0, 37.88600758331033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1322400.0000, 
sim time next is 1323000.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.77494995474412, 12.47709165490572, 1.0, 1.0, 33.3822225562124], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.0, 0.0, 1.0, 0.9678499935348742, 0.1247709165490572, 1.0, 1.0, 0.23844444683008856], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.49520698], dtype=float32), 0.3427343]. 
=============================================
[2019-04-01 16:47:58,669] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[68.33981]
 [71.40375]
 [71.29994]
 [71.30781]
 [71.35263]], R is [[65.10543823]
 [64.45438385]
 [64.40956116]
 [64.4755249 ]
 [64.5398941 ]].
[2019-04-01 16:47:59,948] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5637878e-19 4.6001526e-36 3.4869512e-29 1.6482270e-27 2.5813815e-14
 1.0000000e+00 9.1092009e-24], sum to 1.0000
[2019-04-01 16:47:59,949] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6627
[2019-04-01 16:47:59,979] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.440560973233, 11.22171564501706, 1.0, 1.0, 19.56645215088191], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1363800.0000, 
sim time next is 1364400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.37742548280975, 10.86588829167478, 1.0, 1.0, 18.54141685181637], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 1.0, 0.9110607832585355, 0.1086588829167478, 1.0, 1.0, 0.13243869179868833], 
reward next is 0.5212, 
noisyNet noise sample is [array([-1.3409243], dtype=float32), 1.2659527]. 
=============================================
[2019-04-01 16:48:12,698] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.7715165e-27 0.0000000e+00 1.3736169e-38 0.0000000e+00 7.3128453e-21
 1.0000000e+00 1.2826602e-29], sum to 1.0000
[2019-04-01 16:48:12,699] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2455
[2019-04-01 16:48:12,730] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 26.0, 27.51776556390263, 21.03668731190351, 1.0, 1.0, 6.558505232959135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1615800.0000, 
sim time next is 1616400.0000, 
raw observation next is [12.2, 54.0, 25.5, 18.5, 26.0, 27.4595732970142, 17.7570832050684, 1.0, 1.0, 6.759241493818349], 
processed observation next is [1.0, 0.7391304347826086, 0.8005540166204987, 0.54, 0.085, 0.020441988950276244, 1.0, 1.2085104710020285, 0.17757083205068402, 1.0, 1.0, 0.048280296384416777], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.27776685], dtype=float32), 0.7996493]. 
=============================================
[2019-04-01 16:48:13,158] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5615662e-18 1.2046104e-32 2.0605992e-28 4.1711282e-26 6.0629649e-14
 1.0000000e+00 2.0239672e-29], sum to 1.0000
[2019-04-01 16:48:13,162] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2777
[2019-04-01 16:48:13,212] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 83.33333333333334, 0.0, 26.0, 26.66892611942001, 15.82803140515296, 1.0, 1.0, 28.60091425268326], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1693200.0000, 
sim time next is 1693800.0000, 
raw observation next is [1.1, 88.0, 80.0, 0.0, 26.0, 26.72086897099284, 15.85201557928325, 1.0, 1.0, 26.75209510868822], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.88, 0.26666666666666666, 0.0, 1.0, 1.1029812815704056, 0.15852015579283252, 1.0, 1.0, 0.19108639363348728], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.10798445], dtype=float32), -1.768681]. 
=============================================
[2019-04-01 16:48:13,865] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 16:48:13,866] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6207
[2019-04-01 16:48:13,886] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.08333333333333331, 94.5, 0.0, 0.0, 26.0, 25.27932437204542, 10.16353615090294, 0.0, 1.0, 43.97116354507255], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1723800.0000, 
sim time next is 1724400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.2801751166443, 10.11879701615436, 0.0, 1.0, 43.31528945398984], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.8971678738063288, 0.1011879701615436, 0.0, 1.0, 0.309394924671356], 
reward next is 0.6906, 
noisyNet noise sample is [array([1.2030561], dtype=float32), -0.23692685]. 
=============================================
[2019-04-01 16:48:15,515] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 16:48:15,515] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7342
[2019-04-01 16:48:15,542] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.42125963768663, 11.996270524473, 0.0, 1.0, 39.37676392713183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1655400.0000, 
sim time next is 1656000.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.53985448121167, 12.37873269023436, 0.0, 1.0, 36.92032167735197], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 1.0, 0.9342649258873814, 0.12378732690234359, 0.0, 1.0, 0.2637165834096569], 
reward next is 0.7363, 
noisyNet noise sample is [array([-1.0449778], dtype=float32), -0.4789813]. 
=============================================
[2019-04-01 16:48:15,587] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[75.7404 ]
 [75.73447]
 [75.70987]
 [75.67715]
 [75.76906]], R is [[75.75692749]
 [75.71809387]
 [75.69103241]
 [75.68165588]
 [75.74143219]].
[2019-04-01 16:48:21,500] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6223668e-22 0.0000000e+00 4.0342665e-31 1.2361926e-30 1.6693657e-22
 1.0000000e+00 2.6241850e-13], sum to 1.0000
[2019-04-01 16:48:21,500] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7056
[2019-04-01 16:48:21,518] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.62507774726143, 5.238009293245457, 0.0, 1.0, 46.39477144864696], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1834200.0000, 
sim time next is 1834800.0000, 
raw observation next is [-6.199999999999999, 79.0, 0.0, 0.0, 26.0, 23.62461583609257, 5.245766982994458, 0.0, 1.0, 46.40498716938484], 
processed observation next is [0.0, 0.21739130434782608, 0.2908587257617729, 0.79, 0.0, 0.0, 1.0, 0.6606594051560813, 0.05245766982994458, 0.0, 1.0, 0.3314641940670346], 
reward next is 0.6685, 
noisyNet noise sample is [array([1.3859707], dtype=float32), 0.426323]. 
=============================================
[2019-04-01 16:48:24,728] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0636597e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.4279174e-22
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 16:48:24,729] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7928
[2019-04-01 16:48:24,743] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 80.5, 0.0, 0.0, 26.0, 24.56693119669329, 6.234279389296451, 0.0, 1.0, 44.8689579180953], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1812600.0000, 
sim time next is 1813200.0000, 
raw observation next is [-5.0, 80.0, 0.0, 0.0, 26.0, 24.5367762158216, 6.171715478318222, 0.0, 1.0, 44.88195265747267], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.8, 0.0, 0.0, 1.0, 0.7909680308316572, 0.061717154783182224, 0.0, 1.0, 0.32058537612480476], 
reward next is 0.6794, 
noisyNet noise sample is [array([0.17751244], dtype=float32), 0.30218866]. 
=============================================
[2019-04-01 16:48:25,281] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.6523488e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1897563e-29
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 16:48:25,282] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4031
[2019-04-01 16:48:25,320] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.05, 84.5, 0.0, 0.0, 26.0, 24.89476191485115, 6.96922467524454, 0.0, 1.0, 46.18609391723155], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1884600.0000, 
sim time next is 1885200.0000, 
raw observation next is [-5.233333333333333, 85.0, 0.0, 0.0, 26.0, 24.92421125107159, 6.942424997399705, 0.0, 1.0, 44.57879092098271], 
processed observation next is [0.0, 0.8260869565217391, 0.31763619575253926, 0.85, 0.0, 0.0, 1.0, 0.8463158930102271, 0.06942424997399704, 0.0, 1.0, 0.3184199351498765], 
reward next is 0.6816, 
noisyNet noise sample is [array([-1.4157149], dtype=float32), -0.16342302]. 
=============================================
[2019-04-01 16:48:27,233] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.27943205e-20 2.62069673e-37 6.14370420e-26 1.00529014e-29
 1.38510914e-15 1.00000000e+00 3.34493357e-08], sum to 1.0000
[2019-04-01 16:48:27,235] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9460
[2019-04-01 16:48:27,299] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.516666666666667, 78.0, 107.6666666666667, 62.66666666666667, 26.0, 25.16767271647532, 6.752622282915141, 0.0, 1.0, 46.41843886648363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1847400.0000, 
sim time next is 1848000.0000, 
raw observation next is [-6.333333333333334, 78.0, 127.8333333333333, 78.33333333333334, 26.0, 25.11176206718541, 6.665930370929376, 0.0, 1.0, 43.43060073000839], 
processed observation next is [0.0, 0.391304347826087, 0.28716528162511545, 0.78, 0.426111111111111, 0.08655616942909762, 1.0, 0.8731088667407728, 0.06665930370929375, 0.0, 1.0, 0.3102185766429171], 
reward next is 0.6898, 
noisyNet noise sample is [array([-0.5116278], dtype=float32), -0.1989872]. 
=============================================
[2019-04-01 16:48:27,313] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[60.063915]
 [59.799656]
 [59.452465]
 [59.15515 ]
 [58.766476]], R is [[60.42753601]
 [60.49169922]
 [60.5338707 ]
 [60.55389404]
 [60.48204803]].
[2019-04-01 16:48:32,176] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6220564e-27 1.0715803e-36 7.1086970e-30 1.3523059e-30 4.1414068e-23
 1.0000000e+00 1.6906050e-28], sum to 1.0000
[2019-04-01 16:48:32,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8205
[2019-04-01 16:48:32,194] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.1, 76.33333333333333, 0.0, 0.0, 26.0, 24.62701613698079, 6.059480791989468, 0.0, 1.0, 44.24638557976359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1893000.0000, 
sim time next is 1893600.0000, 
raw observation next is [-6.2, 75.0, 0.0, 0.0, 26.0, 24.59070030115017, 5.99729285184781, 0.0, 1.0, 44.27117318289631], 
processed observation next is [0.0, 0.9565217391304348, 0.2908587257617729, 0.75, 0.0, 0.0, 1.0, 0.7986714715928817, 0.0599729285184781, 0.0, 1.0, 0.31622266559211654], 
reward next is 0.6838, 
noisyNet noise sample is [array([0.9235662], dtype=float32), 1.7992007]. 
=============================================
[2019-04-01 16:48:35,116] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 16:48:35,116] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9169
[2019-04-01 16:48:35,148] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 80.5, 0.0, 0.0, 26.0, 25.1902214430594, 8.558346393824328, 0.0, 1.0, 44.18420710378399], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1974600.0000, 
sim time next is 1975200.0000, 
raw observation next is [-5.6, 79.66666666666667, 0.0, 0.0, 26.0, 25.19806095752753, 8.522146721873776, 0.0, 1.0, 43.53530765113015], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.7966666666666667, 0.0, 0.0, 1.0, 0.8854372796467901, 0.08522146721873776, 0.0, 1.0, 0.3109664832223582], 
reward next is 0.6890, 
noisyNet noise sample is [array([-0.33168766], dtype=float32), -0.0059881825]. 
=============================================
[2019-04-01 16:48:39,680] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3585514e-30 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.6788635e-24
 1.0000000e+00 7.6556360e-38], sum to 1.0000
[2019-04-01 16:48:39,680] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1884
[2019-04-01 16:48:39,730] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.416666666666667, 81.66666666666667, 132.0, 0.0, 26.0, 25.65992828656824, 7.890636946929528, 1.0, 1.0, 32.73836517292794], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2027400.0000, 
sim time next is 2028000.0000, 
raw observation next is [-5.233333333333333, 80.33333333333334, 139.5, 0.0, 26.0, 25.6637502230847, 8.007881041805453, 1.0, 1.0, 32.25205750474279], 
processed observation next is [1.0, 0.4782608695652174, 0.31763619575253926, 0.8033333333333335, 0.465, 0.0, 1.0, 0.9519643175835287, 0.08007881041805452, 1.0, 1.0, 0.23037183931959135], 
reward next is 0.7696, 
noisyNet noise sample is [array([-0.4839197], dtype=float32), -0.6259742]. 
=============================================
[2019-04-01 16:48:39,734] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[70.40023 ]
 [70.329384]
 [70.31979 ]
 [70.279816]
 [70.243004]], R is [[70.53391266]
 [70.59472656]
 [70.6552887 ]
 [70.71566772]
 [70.77627563]].
[2019-04-01 16:48:46,944] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1307404e-35 0.0000000e+00 4.5746336e-38 0.0000000e+00 4.4229930e-37
 1.0000000e+00 3.6205682e-33], sum to 1.0000
[2019-04-01 16:48:46,944] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0253
[2019-04-01 16:48:46,975] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.366666666666667, 85.66666666666667, 0.0, 0.0, 26.0, 24.28205125212267, 5.623805297231558, 0.0, 1.0, 42.47128719926971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2092800.0000, 
sim time next is 2093400.0000, 
raw observation next is [-6.45, 85.0, 0.0, 0.0, 26.0, 24.32225685979789, 5.576419088095882, 0.0, 1.0, 42.73934441902104], 
processed observation next is [1.0, 0.21739130434782608, 0.28393351800554023, 0.85, 0.0, 0.0, 1.0, 0.7603224085425556, 0.055764190880958825, 0.0, 1.0, 0.305281031564436], 
reward next is 0.6947, 
noisyNet noise sample is [array([0.6022123], dtype=float32), 1.1449221]. 
=============================================
[2019-04-01 16:49:07,183] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1497752e-27 0.0000000e+00 4.4089046e-32 7.2681212e-37 2.2234969e-35
 1.0000000e+00 1.5891554e-24], sum to 1.0000
[2019-04-01 16:49:07,183] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1784
[2019-04-01 16:49:07,229] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.466666666666667, 63.0, 143.6666666666667, 426.0, 26.0, 24.9524570980571, 7.22458112525305, 0.0, 1.0, 29.84478982452314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2371200.0000, 
sim time next is 2371800.0000, 
raw observation next is [-2.383333333333333, 62.5, 148.3333333333333, 402.0, 26.0, 24.88275887865181, 7.25972685472811, 0.0, 1.0, 37.20614770600758], 
processed observation next is [0.0, 0.43478260869565216, 0.3965835641735919, 0.625, 0.4944444444444443, 0.44419889502762433, 1.0, 0.8403941255216871, 0.0725972685472811, 0.0, 1.0, 0.2657581979000541], 
reward next is 0.7342, 
noisyNet noise sample is [array([0.24716626], dtype=float32), -0.4561716]. 
=============================================
[2019-04-01 16:49:12,967] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2687622e-35 0.0000000e+00 7.5907295e-38 0.0000000e+00 4.0305305e-30
 1.0000000e+00 1.2341828e-19], sum to 1.0000
[2019-04-01 16:49:12,967] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7190
[2019-04-01 16:49:12,996] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.7, 47.33333333333334, 166.0, 53.66666666666666, 26.0, 25.86578819900121, 7.487900543019497, 1.0, 1.0, 12.7225708537132], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2544600.0000, 
sim time next is 2545200.0000, 
raw observation next is [-0.6, 47.0, 182.5, 58.0, 26.0, 25.81671399427265, 7.511448783223147, 1.0, 1.0, 12.09482936092129], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.47, 0.6083333333333333, 0.06408839779005525, 1.0, 0.9738162848960928, 0.07511448783223147, 1.0, 1.0, 0.08639163829229493], 
reward next is 0.9136, 
noisyNet noise sample is [array([-0.44895524], dtype=float32), -0.7293942]. 
=============================================
[2019-04-01 16:49:24,511] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 16:49:24,513] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1085
[2019-04-01 16:49:24,529] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.1, 69.0, 0.0, 0.0, 26.0, 25.40285157093199, 9.454446721375364, 0.0, 1.0, 41.81537628465924], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2671200.0000, 
sim time next is 2671800.0000, 
raw observation next is [-3.416666666666667, 69.0, 0.0, 0.0, 26.0, 25.34277687279342, 8.473782532750528, 0.0, 1.0, 46.3642370561739], 
processed observation next is [1.0, 0.9565217391304348, 0.36795937211449675, 0.69, 0.0, 0.0, 1.0, 0.9061109818276313, 0.08473782532750528, 0.0, 1.0, 0.33117312182981357], 
reward next is 0.6688, 
noisyNet noise sample is [array([-0.29584277], dtype=float32), -0.1345979]. 
=============================================
[2019-04-01 16:49:26,354] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1415684e-25 0.0000000e+00 1.4981020e-35 0.0000000e+00 2.8241452e-27
 1.0000000e+00 2.9141986e-16], sum to 1.0000
[2019-04-01 16:49:26,355] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4506
[2019-04-01 16:49:26,403] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.133333333333333, 54.33333333333333, 96.66666666666667, 693.3333333333334, 26.0, 26.59064500300708, 15.35650764223798, 1.0, 1.0, 17.9054288577892], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2731800.0000, 
sim time next is 2732400.0000, 
raw observation next is [-4.0, 54.0, 94.0, 673.5, 26.0, 25.85544696267885, 13.7662382995997, 1.0, 1.0, 16.39953141672337], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.54, 0.31333333333333335, 0.7441988950276243, 1.0, 0.9793495660969788, 0.13766238299599698, 1.0, 1.0, 0.11713951011945264], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7731453], dtype=float32), -0.38909712]. 
=============================================
[2019-04-01 16:49:28,634] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0463979e-36 0.0000000e+00 4.9612710e-37 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.7147052e-31], sum to 1.0000
[2019-04-01 16:49:28,634] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8724
[2019-04-01 16:49:28,758] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 23.72553061737345, 5.28004603213773, 0.0, 1.0, 62.0649826505952], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2790000.0000, 
sim time next is 2790600.0000, 
raw observation next is [-6.833333333333334, 64.0, 0.0, 0.0, 26.0, 23.59927343626448, 5.403999826011282, 0.0, 1.0, 129.3091199915095], 
processed observation next is [1.0, 0.30434782608695654, 0.27331486611265005, 0.64, 0.0, 0.0, 1.0, 0.657039062323497, 0.054039998260112826, 0.0, 1.0, 0.923636571367925], 
reward next is 0.0764, 
noisyNet noise sample is [array([-0.08077116], dtype=float32), 1.1375957]. 
=============================================
[2019-04-01 16:49:28,947] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2799618e-28 0.0000000e+00 4.8562501e-27 2.7724817e-34 2.9901337e-22
 1.0000000e+00 2.1272388e-24], sum to 1.0000
[2019-04-01 16:49:28,947] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2651
[2019-04-01 16:49:28,999] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.33333333333333, 78.5, 103.6666666666667, 632.6666666666667, 26.0, 25.92539923632541, 9.496110698350249, 1.0, 1.0, 26.26875408422804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2713800.0000, 
sim time next is 2714400.0000, 
raw observation next is [-12.0, 76.0, 107.0, 643.0, 26.0, 25.96768313643442, 9.666282083299047, 1.0, 1.0, 24.63642420678385], 
processed observation next is [1.0, 0.43478260869565216, 0.13019390581717452, 0.76, 0.3566666666666667, 0.7104972375690608, 1.0, 0.9953833052049171, 0.09666282083299046, 1.0, 1.0, 0.17597445861988464], 
reward next is 0.8240, 
noisyNet noise sample is [array([-0.97537005], dtype=float32), 1.5347909]. 
=============================================
[2019-04-01 16:49:31,727] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 16:49:31,727] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7772
[2019-04-01 16:49:31,745] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.69299699799302, 6.690146461935474, 0.0, 1.0, 42.44942020979188], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2766600.0000, 
sim time next is 2767200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.69750819070294, 6.7891972081685, 0.0, 1.0, 42.22756859451355], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 1.0, 0.8139297415289916, 0.067891972081685, 0.0, 1.0, 0.30162548996081107], 
reward next is 0.6984, 
noisyNet noise sample is [array([-0.16124283], dtype=float32), -0.2801769]. 
=============================================
[2019-04-01 16:49:35,267] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1102987e-31 0.0000000e+00 3.4157908e-37 0.0000000e+00 3.8069200e-34
 1.0000000e+00 4.8220281e-26], sum to 1.0000
[2019-04-01 16:49:35,267] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8541
[2019-04-01 16:49:35,318] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 93.0, 87.5, 134.5, 26.0, 24.82653703773399, 7.280758466853475, 1.0, 1.0, 48.73807885232128], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2881200.0000, 
sim time next is 2881800.0000, 
raw observation next is [1.5, 93.0, 105.0, 156.0, 26.0, 25.0847612250916, 7.525383087859342, 1.0, 1.0, 38.031303063916], 
processed observation next is [1.0, 0.34782608695652173, 0.5041551246537397, 0.93, 0.35, 0.1723756906077348, 1.0, 0.8692516035845144, 0.07525383087859341, 1.0, 1.0, 0.2716521647422571], 
reward next is 0.7283, 
noisyNet noise sample is [array([-0.78568465], dtype=float32), -0.17056721]. 
=============================================
[2019-04-01 16:49:39,177] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5659998e-30 0.0000000e+00 4.7375825e-37 0.0000000e+00 9.5219363e-28
 1.0000000e+00 9.7966541e-20], sum to 1.0000
[2019-04-01 16:49:39,177] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0571
[2019-04-01 16:49:39,190] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 100.0, 160.3333333333333, 0.0, 26.0, 25.46763606654936, 7.447022422820711, 1.0, 1.0, 15.3117461400136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2895600.0000, 
sim time next is 2896200.0000, 
raw observation next is [1.5, 100.0, 175.0, 0.0, 26.0, 25.30219246898787, 7.441397856740515, 1.0, 1.0, 14.76349783572834], 
processed observation next is [1.0, 0.5217391304347826, 0.5041551246537397, 1.0, 0.5833333333333334, 0.0, 1.0, 0.9003132098554099, 0.07441397856740516, 1.0, 1.0, 0.10545355596948815], 
reward next is 0.8945, 
noisyNet noise sample is [array([1.6606108], dtype=float32), 0.102646425]. 
=============================================
[2019-04-01 16:49:41,407] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0422179e-18 3.7617420e-29 9.0798865e-22 7.9386146e-29 1.1877479e-19
 9.9651474e-01 3.4853120e-03], sum to 1.0000
[2019-04-01 16:49:41,409] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2929
[2019-04-01 16:49:41,442] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 60.5, 506.1666666666666, 26.0, 25.17227483444376, 8.973533681614187, 0.0, 1.0, 17.30034879801634], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2996400.0000, 
sim time next is 2997000.0000, 
raw observation next is [-1.0, 55.0, 56.0, 474.0, 26.0, 25.13382515183377, 8.79768203781588, 0.0, 1.0, 21.07102876960287], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.55, 0.18666666666666668, 0.523756906077348, 1.0, 0.8762607359762526, 0.08797682037815881, 0.0, 1.0, 0.1505073483543062], 
reward next is 0.8495, 
noisyNet noise sample is [array([0.2785518], dtype=float32), -1.1561664]. 
=============================================
[2019-04-01 16:49:41,456] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.34324 ]
 [72.37413 ]
 [72.37804 ]
 [72.338844]
 [72.25656 ]], R is [[72.411026  ]
 [72.56334686]
 [72.7123642 ]
 [72.8444519 ]
 [72.94471741]].
[2019-04-01 16:49:42,817] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1134829e-33
 1.0000000e+00 5.9803199e-32], sum to 1.0000
[2019-04-01 16:49:42,819] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5160
[2019-04-01 16:49:42,844] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.833333333333333, 78.16666666666667, 0.0, 0.0, 26.0, 24.1902882669471, 5.803236144729269, 0.0, 1.0, 42.06687931806713], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2958600.0000, 
sim time next is 2959200.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 24.15645963153141, 5.750047910889933, 0.0, 1.0, 42.06162196927032], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 1.0, 0.7366370902187727, 0.05750047910889933, 0.0, 1.0, 0.3004401569233594], 
reward next is 0.6996, 
noisyNet noise sample is [array([0.38197955], dtype=float32), 0.007434146]. 
=============================================
[2019-04-01 16:49:46,141] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.0081694e-27 2.3842883e-37 1.9958717e-25 2.1424141e-36 3.2468456e-32
 9.6626896e-01 3.3731084e-02], sum to 1.0000
[2019-04-01 16:49:46,141] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2008
[2019-04-01 16:49:46,178] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 54.16666666666667, 113.0, 813.0, 26.0, 25.12428039160334, 8.314737610556604, 0.0, 1.0, 21.87750462995711], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3067800.0000, 
sim time next is 3068400.0000, 
raw observation next is [-2.666666666666667, 53.33333333333334, 113.5, 815.0, 26.0, 25.14070755616676, 8.341174362357778, 0.0, 1.0, 18.94409925866785], 
processed observation next is [0.0, 0.5217391304347826, 0.38873499538319484, 0.5333333333333334, 0.37833333333333335, 0.9005524861878453, 1.0, 0.8772439365952514, 0.08341174362357778, 0.0, 1.0, 0.13531499470477037], 
reward next is 0.8647, 
noisyNet noise sample is [array([-0.51391125], dtype=float32), 0.043829482]. 
=============================================
[2019-04-01 16:49:49,882] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0748644e-25 0.0000000e+00 1.2834335e-30 0.0000000e+00 9.3973059e-35
 1.6880560e-03 9.9831200e-01], sum to 1.0000
[2019-04-01 16:49:49,883] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5253
[2019-04-01 16:49:49,894] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 96.33333333333333, 604.5, 26.0, 26.15403940492554, 11.02030557643557, 1.0, 1.0, 16.17437127617407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3144000.0000, 
sim time next is 3144600.0000, 
raw observation next is [7.0, 100.0, 99.0, 647.0, 26.0, 26.26277894603754, 11.56533812037098, 1.0, 1.0, 14.87925712298181], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.33, 0.7149171270718232, 1.0, 1.037539849433934, 0.1156533812037098, 1.0, 1.0, 0.10628040802129865], 
reward next is 0.2676, 
noisyNet noise sample is [array([-0.5864976], dtype=float32), -0.0008449798]. 
=============================================
[2019-04-01 16:49:54,397] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5824438e-35
 1.0000000e+00 1.4555247e-22], sum to 1.0000
[2019-04-01 16:49:54,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7354
[2019-04-01 16:49:54,409] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 97.66666666666667, 0.0, 0.0, 26.0, 25.94601793508056, 13.58668778066298, 0.0, 1.0, 20.79643709807034], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3190800.0000, 
sim time next is 3191400.0000, 
raw observation next is [2.0, 96.5, 0.0, 0.0, 26.0, 25.73676913982618, 12.84574412883153, 0.0, 1.0, 18.4464045080782], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.965, 0.0, 0.0, 1.0, 0.96239559140374, 0.1284574412883153, 0.0, 1.0, 0.13176003220055857], 
reward next is 0.8682, 
noisyNet noise sample is [array([0.08732369], dtype=float32), 0.27512053]. 
=============================================
[2019-04-01 16:49:56,705] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3055221e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0959967e-25
 9.9999833e-01 1.6514642e-06], sum to 1.0000
[2019-04-01 16:49:56,706] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3900
[2019-04-01 16:49:56,713] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.66016351013516, 13.68770663750821, 0.0, 1.0, 6.632120920291569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3181200.0000, 
sim time next is 3181800.0000, 
raw observation next is [3.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.54650515356452, 13.1968278298802, 0.0, 1.0, 5.664220405852321], 
processed observation next is [1.0, 0.8260869565217391, 0.5503231763619576, 1.0, 0.0, 0.0, 1.0, 0.9352150219377887, 0.131968278298802, 0.0, 1.0, 0.04045871718465943], 
reward next is 0.9595, 
noisyNet noise sample is [array([0.37180957], dtype=float32), 0.2778729]. 
=============================================
[2019-04-01 16:49:59,548] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3331047e-21 0.0000000e+00 5.8555511e-32 5.3963455e-38 4.4119002e-24
 9.9998844e-01 1.1590353e-05], sum to 1.0000
[2019-04-01 16:49:59,551] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4051
[2019-04-01 16:49:59,569] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 52.5, 11.0, 133.0, 26.0, 26.18884084830316, 11.35965922681017, 1.0, 1.0, 11.55185427584725], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3346200.0000, 
sim time next is 3346800.0000, 
raw observation next is [-2.666666666666667, 53.33333333333333, 9.166666666666668, 110.8333333333333, 26.0, 25.63479745609762, 10.81310259119115, 1.0, 1.0, 26.12687693158874], 
processed observation next is [1.0, 0.7391304347826086, 0.38873499538319484, 0.5333333333333333, 0.030555555555555558, 0.12246777163904232, 1.0, 0.9478282080139456, 0.1081310259119115, 1.0, 1.0, 0.18662054951134813], 
reward next is 0.4881, 
noisyNet noise sample is [array([-1.077549], dtype=float32), -1.0930003]. 
=============================================
[2019-04-01 16:50:00,109] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1192597e-32 0.0000000e+00 5.0990124e-31 0.0000000e+00 1.7793120e-27
 1.0000000e+00 3.9421607e-26], sum to 1.0000
[2019-04-01 16:50:00,109] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5113
[2019-04-01 16:50:00,128] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.633333333333333, 76.33333333333333, 0.0, 0.0, 26.0, 24.49893952203206, 6.26395953113555, 0.0, 1.0, 43.22238714165579], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3300000.0000, 
sim time next is 3300600.0000, 
raw observation next is [-9.816666666666666, 76.16666666666667, 0.0, 0.0, 26.0, 24.43020479885713, 6.238827862189768, 0.0, 1.0, 43.18311450256954], 
processed observation next is [1.0, 0.17391304347826086, 0.19067405355494, 0.7616666666666667, 0.0, 0.0, 1.0, 0.7757435426938757, 0.062388278621897685, 0.0, 1.0, 0.3084508178754967], 
reward next is 0.6915, 
noisyNet noise sample is [array([0.2541432], dtype=float32), 0.5980002]. 
=============================================
[2019-04-01 16:50:07,482] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.98662026e-20 4.63688514e-33 3.43493704e-24 2.02335066e-32
 1.29359206e-20 9.37322147e-06 9.99990582e-01], sum to 1.0000
[2019-04-01 16:50:07,486] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7867
[2019-04-01 16:50:07,512] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 48.66666666666666, 110.0, 770.6666666666667, 26.0, 26.60810555390613, 14.08303442793292, 1.0, 1.0, 11.62753605588563], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3408000.0000, 
sim time next is 3408600.0000, 
raw observation next is [2.833333333333333, 48.83333333333334, 111.0, 777.3333333333333, 26.0, 26.65965000579153, 13.8304011254414, 1.0, 1.0, 18.69546626997881], 
processed observation next is [1.0, 0.43478260869565216, 0.541089566020314, 0.48833333333333345, 0.37, 0.8589318600368323, 1.0, 1.0942357151130757, 0.138304011254414, 1.0, 1.0, 0.13353904478556292], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0372024], dtype=float32), -0.5477134]. 
=============================================
[2019-04-01 16:50:09,848] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3348466e-30 3.0531242e-37 7.8483892e-34 1.4942160e-34 1.7613575e-32
 1.0000000e+00 7.3653422e-20], sum to 1.0000
[2019-04-01 16:50:09,848] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1793
[2019-04-01 16:50:09,866] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 26.0, 25.34805160654068, 9.154277614007258, 0.0, 1.0, 40.74965193905109], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3466800.0000, 
sim time next is 3467400.0000, 
raw observation next is [1.0, 71.16666666666667, 0.0, 0.0, 26.0, 25.39772559877011, 9.361154358526816, 0.0, 1.0, 40.73477632221709], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.7116666666666667, 0.0, 0.0, 1.0, 0.9139607998243012, 0.09361154358526816, 0.0, 1.0, 0.29096268801583636], 
reward next is 0.7090, 
noisyNet noise sample is [array([-0.10141756], dtype=float32), -1.57631]. 
=============================================
[2019-04-01 16:50:13,060] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.0606661e-30 0.0000000e+00 1.7680952e-33 0.0000000e+00 8.4768593e-31
 1.0000000e+00 1.6664259e-15], sum to 1.0000
[2019-04-01 16:50:13,061] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3753
[2019-04-01 16:50:13,075] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.0, 26.66666666666667, 99.0, 619.6666666666666, 26.0, 25.81341468730151, 10.00407973030689, 0.0, 1.0, 17.49544414762998], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3662400.0000, 
sim time next is 3663000.0000, 
raw observation next is [11.0, 27.0, 101.0, 663.0, 26.0, 25.80078661365391, 10.07166502659561, 0.0, 1.0, 16.35015860237355], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.27, 0.33666666666666667, 0.7325966850828729, 1.0, 0.9715409448077013, 0.10071665026595611, 0.0, 1.0, 0.11678684715981108], 
reward next is 0.8832, 
noisyNet noise sample is [array([-2.0225387], dtype=float32), 0.42985684]. 
=============================================
[2019-04-01 16:50:13,081] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[68.77926 ]
 [68.78243 ]
 [68.805176]
 [68.91286 ]
 [69.26507 ]], R is [[68.95609283]
 [69.14156342]
 [69.31634521]
 [69.47912598]
 [69.64221191]].
[2019-04-01 16:50:13,527] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6059737e-26 8.0333359e-36 5.8292438e-35 1.2886126e-31 2.3605114e-24
 1.0000000e+00 1.8935866e-24], sum to 1.0000
[2019-04-01 16:50:13,528] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2918
[2019-04-01 16:50:13,541] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.833333333333333, 61.0, 0.0, 0.0, 26.0, 25.27474503308381, 9.467437798172258, 0.0, 1.0, 42.34417375070217], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3541800.0000, 
sim time next is 3542400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.27594903403448, 9.467210823776865, 0.0, 1.0, 42.24588209125368], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 1.0, 0.8965641477192113, 0.09467210823776866, 0.0, 1.0, 0.30175630065181197], 
reward next is 0.6982, 
noisyNet noise sample is [array([-1.2930484], dtype=float32), -0.7817587]. 
=============================================
[2019-04-01 16:50:27,005] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.37743324e-20 3.12554115e-28 6.98058726e-25 9.92818119e-27
 1.16558925e-19 1.00000000e+00 1.30648435e-11], sum to 1.0000
[2019-04-01 16:50:27,005] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1400
[2019-04-01 16:50:27,067] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 77.0, 5.0, 149.0, 26.0, 25.35497137590883, 7.97772389880465, 1.0, 1.0, 41.94028034730673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3828600.0000, 
sim time next is 3829200.0000, 
raw observation next is [-5.0, 77.0, 19.33333333333333, 198.6666666666667, 26.0, 25.35263557553325, 7.982497184960167, 1.0, 1.0, 37.17309624772192], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.77, 0.06444444444444443, 0.21952117863720078, 1.0, 0.9075193679333212, 0.07982497184960166, 1.0, 1.0, 0.2655221160551566], 
reward next is 0.7345, 
noisyNet noise sample is [array([-1.6841279], dtype=float32), -0.24747832]. 
=============================================
[2019-04-01 16:50:28,067] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5947297e-25 2.6829145e-31 3.8953714e-26 1.5960338e-30 1.0265531e-23
 1.0000000e+00 2.3926923e-19], sum to 1.0000
[2019-04-01 16:50:28,067] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0074
[2019-04-01 16:50:28,103] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.9456989945723, 7.035114950662996, 0.0, 1.0, 43.33604255770793], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3817200.0000, 
sim time next is 3817800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.90653764139392, 6.910947989933722, 0.0, 1.0, 43.38463540469373], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 1.0, 0.843791091627703, 0.06910947989933722, 0.0, 1.0, 0.3098902528906695], 
reward next is 0.6901, 
noisyNet noise sample is [array([-0.92208624], dtype=float32), -0.81571525]. 
=============================================
[2019-04-01 16:50:28,940] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9963851e-15 6.9149170e-31 4.3953777e-21 4.8715078e-32 2.7723936e-14
 8.9261057e-11 1.0000000e+00], sum to 1.0000
[2019-04-01 16:50:28,946] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3760
[2019-04-01 16:50:28,984] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 45.0, 117.5, 829.5, 26.0, 26.5907946625766, 16.40728241105549, 1.0, 1.0, 19.31997886357837], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3934800.0000, 
sim time next is 3935400.0000, 
raw observation next is [-5.833333333333333, 43.83333333333334, 117.0, 827.6666666666666, 26.0, 26.74910254949208, 16.85507115552095, 1.0, 1.0, 18.23858193841818], 
processed observation next is [1.0, 0.5652173913043478, 0.30101569713758086, 0.4383333333333334, 0.39, 0.9145488029465929, 1.0, 1.1070146499274398, 0.1685507115552095, 1.0, 1.0, 0.13027558527441557], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.91881084], dtype=float32), -0.009087245]. 
=============================================
[2019-04-01 16:50:30,724] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2254733e-10 1.2481245e-16 1.1650607e-09 1.1012985e-15 7.8654449e-13
 2.6600092e-04 9.9973398e-01], sum to 1.0000
[2019-04-01 16:50:30,725] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3684
[2019-04-01 16:50:30,774] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.66666666666667, 56.33333333333333, 94.33333333333333, 486.3333333333333, 26.0, 26.07009133949545, 10.02182708511233, 1.0, 1.0, 37.29350663778276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4005600.0000, 
sim time next is 4006200.0000, 
raw observation next is [-11.33333333333333, 54.66666666666667, 95.66666666666666, 528.6666666666667, 26.0, 26.17840634574288, 10.17098031637478, 1.0, 1.0, 34.5571566596904], 
processed observation next is [1.0, 0.34782608695652173, 0.14866112650046176, 0.5466666666666667, 0.31888888888888883, 0.5841620626151014, 1.0, 1.0254866208204112, 0.10170980316374781, 1.0, 1.0, 0.24683683328350284], 
reward next is 0.6848, 
noisyNet noise sample is [array([0.11162471], dtype=float32), 0.3413914]. 
=============================================
[2019-04-01 16:50:33,138] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7660756e-17 2.3969012e-21 1.5344449e-16 7.1289464e-25 7.4957041e-11
 5.0144052e-25 1.0000000e+00], sum to 1.0000
[2019-04-01 16:50:33,139] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4880
[2019-04-01 16:50:33,146] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 47.66666666666667, 119.1666666666667, 830.8333333333334, 26.0, 26.41242116139824, 13.39473599745395, 1.0, 1.0, 7.278043246236561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3932400.0000, 
sim time next is 3933000.0000, 
raw observation next is [-6.0, 47.0, 119.0, 835.0, 26.0, 26.33062270046875, 13.22460138382742, 1.0, 1.0, 7.138216193122317], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.47, 0.39666666666666667, 0.9226519337016574, 1.0, 1.0472318143526789, 0.1322460138382742, 1.0, 1.0, 0.050987258522302265], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7758693], dtype=float32), -0.0982124]. 
=============================================
[2019-04-01 16:50:33,149] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[20.36382 ]
 [20.27052 ]
 [20.158274]
 [20.011496]
 [19.866465]], R is [[20.20317459]
 [20.0011425 ]
 [19.8011322 ]
 [19.6031208 ]
 [19.40708923]].
[2019-04-01 16:50:38,466] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1553697e-23 3.9022893e-25 1.9996925e-22 4.4401293e-32 5.5371612e-17
 4.7645750e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 16:50:38,468] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1382
[2019-04-01 16:50:38,497] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 28.66666666666666, 119.3333333333333, 839.1666666666667, 26.0, 26.32777660710752, 13.50058335625922, 1.0, 1.0, 4.625696604490122], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4106400.0000, 
sim time next is 4107000.0000, 
raw observation next is [2.666666666666667, 28.83333333333334, 118.6666666666667, 837.3333333333334, 26.0, 26.44649970934551, 14.07224359196531, 1.0, 1.0, 4.47980801180283], 
processed observation next is [1.0, 0.5217391304347826, 0.5364727608494922, 0.2883333333333334, 0.39555555555555566, 0.9252302025782689, 1.0, 1.0637856727636443, 0.1407224359196531, 1.0, 1.0, 0.0319986286557345], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.00477908], dtype=float32), -0.33322263]. 
=============================================
[2019-04-01 16:50:38,528] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[30.472256]
 [30.202915]
 [29.994251]
 [29.901918]
 [29.763466]], R is [[30.42305374]
 [30.11882401]
 [29.81763649]
 [29.65049934]
 [29.41391373]].
[2019-04-01 16:50:39,531] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3885256e-12 2.0726305e-18 5.1923628e-16 2.2335840e-22 2.0239497e-02
 6.0438919e-01 3.7537134e-01], sum to 1.0000
[2019-04-01 16:50:39,532] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3036
[2019-04-01 16:50:39,552] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.64294455581253, 10.36509437953124, 1.0, 1.0, 9.279421478503293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4042800.0000, 
sim time next is 4043400.0000, 
raw observation next is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.59752222937454, 10.21092690524678, 1.0, 1.0, 23.74500946053978], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.31, 0.0, 0.0, 1.0, 0.9425031756249344, 0.1021092690524678, 1.0, 1.0, 0.169607210432427], 
reward next is 0.7460, 
noisyNet noise sample is [array([0.0331191], dtype=float32), -2.2707841]. 
=============================================
[2019-04-01 16:50:46,112] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-01 16:50:46,113] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 16:50:46,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:50:46,115] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 16:50:46,115] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:50:46,116] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 16:50:46,116] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:50:46,122] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run13
[2019-04-01 16:50:46,122] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run13
[2019-04-01 16:50:46,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run13
[2019-04-01 16:50:55,255] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.72335213], dtype=float32), -0.401314]
[2019-04-01 16:50:55,255] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [6.700000000000001, 89.0, 0.0, 0.0, 26.0, 25.04977751221985, 7.421683324983171, 1.0, 1.0, 35.36711631018301]
[2019-04-01 16:50:55,255] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 16:50:55,256] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.7351660e-21 2.1506234e-25 4.6994483e-20 3.1393970e-25 3.4700669e-18
 9.9999666e-01 3.3056722e-06], sampled 0.4123862121736199
[2019-04-01 16:51:43,456] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.72335213], dtype=float32), -0.401314]
[2019-04-01 16:51:43,457] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.713830507666668, 74.84035483000001, 179.5613738333333, 0.0, 26.0, 25.47266580779198, 7.473538277118362, 1.0, 1.0, 45.59005699074268]
[2019-04-01 16:51:43,457] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 16:51:43,458] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.9779419e-19 3.4883667e-26 9.6972629e-21 4.7727610e-27 9.7076097e-17
 1.7582037e-08 1.0000000e+00], sampled 0.9215545778598863
[2019-04-01 16:52:09,942] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.72335213], dtype=float32), -0.401314]
[2019-04-01 16:52:09,943] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.666666666666667, 58.33333333333334, 86.0, 681.0, 26.0, 25.11827551735821, 9.153567860401187, 0.0, 1.0, 21.61656559606749]
[2019-04-01 16:52:09,943] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:52:09,944] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.91097895e-27 3.18889210e-34 5.01582120e-25 1.45936246e-35
 1.52971485e-26 1.07953895e-08 1.00000000e+00], sampled 0.2503255410478312
[2019-04-01 16:52:28,037] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 16:52:48,722] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 16:52:51,635] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 16:52:52,658] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1200000, evaluation results [1200000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 16:53:03,654] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5665802e-29 3.1294091e-34 1.6915653e-29 1.0367711e-35 4.2387335e-28
 1.0000000e+00 1.5367673e-09], sum to 1.0000
[2019-04-01 16:53:03,657] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7492
[2019-04-01 16:53:03,676] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.55510891493781, 9.83369223751027, 0.0, 1.0, 33.85358730737691], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4497000.0000, 
sim time next is 4497600.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.5450664422257, 9.304065528610856, 0.0, 1.0, 33.21772188376184], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 1.0, 0.9350094917465286, 0.09304065528610855, 0.0, 1.0, 0.23726944202687028], 
reward next is 0.7627, 
noisyNet noise sample is [array([0.6211654], dtype=float32), 1.0102457]. 
=============================================
[2019-04-01 16:53:08,481] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1454852e-23 1.1008833e-33 7.4460093e-30 1.7010591e-34 5.3286828e-12
 9.9999547e-01 4.5330739e-06], sum to 1.0000
[2019-04-01 16:53:08,481] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6267
[2019-04-01 16:53:08,509] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 24.99084257254909, 8.36221666967233, 1.0, 1.0, 19.49897304819442], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4563000.0000, 
sim time next is 4563600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 24.90225734140247, 8.370371546643378, 1.0, 1.0, 27.12554137456946], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.8431796202003531, 0.08370371546643378, 1.0, 1.0, 0.1937538669612104], 
reward next is 0.8062, 
noisyNet noise sample is [array([-1.7402811], dtype=float32), -0.282032]. 
=============================================
[2019-04-01 16:53:20,727] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6875156e-30 3.4453496e-36 6.6444639e-26 1.1571560e-35 4.4497195e-32
 9.9999845e-01 1.5263065e-06], sum to 1.0000
[2019-04-01 16:53:20,730] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4880
[2019-04-01 16:53:20,774] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.333333333333334, 73.0, 165.6666666666667, 420.6666666666667, 26.0, 25.7541738262572, 10.17961577348187, 0.0, 1.0, 35.36911003834146], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4785600.0000, 
sim time next is 4786200.0000, 
raw observation next is [-4.0, 71.0, 174.0, 421.0, 26.0, 25.72115582295817, 10.0877093205235, 0.0, 1.0, 33.03037679714303], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.58, 0.46519337016574586, 1.0, 0.960165117565453, 0.100877093205235, 0.0, 1.0, 0.2359312628367359], 
reward next is 0.7641, 
noisyNet noise sample is [array([-0.52435094], dtype=float32), -1.7176585]. 
=============================================
[2019-04-01 16:53:25,306] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.2670453e-38], sum to 1.0000
[2019-04-01 16:53:25,306] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2322
[2019-04-01 16:53:25,329] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 44.33333333333334, 0.0, 0.0, 26.0, 24.98619564145928, 8.015761374777421, 0.0, 1.0, 38.37296352376701], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4821600.0000, 
sim time next is 4822200.0000, 
raw observation next is [1.0, 45.0, 0.0, 0.0, 26.0, 25.02652711358559, 8.521734216067768, 0.0, 1.0, 50.73920698182698], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.45, 0.0, 0.0, 1.0, 0.8609324447979415, 0.08521734216067768, 0.0, 1.0, 0.36242290701304986], 
reward next is 0.6376, 
noisyNet noise sample is [array([1.3454968], dtype=float32), -2.288351]. 
=============================================
[2019-04-01 16:53:29,476] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6061096e-17 1.2204275e-27 5.7402709e-20 6.9737555e-30 5.8384499e-15
 1.2707732e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 16:53:29,481] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1169
[2019-04-01 16:53:29,494] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.5, 27.0, 122.0, 845.0, 26.0, 26.830521870536, 14.55217023780588, 1.0, 1.0, 5.847628639734393], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4966200.0000, 
sim time next is 4966800.0000, 
raw observation next is [5.0, 26.33333333333333, 122.1666666666667, 848.3333333333334, 26.0, 26.91231463902452, 15.05602303589501, 1.0, 1.0, 5.4924613344554], 
processed observation next is [1.0, 0.4782608695652174, 0.6011080332409973, 0.2633333333333333, 0.4072222222222223, 0.9373848987108656, 1.0, 1.1303306627177885, 0.15056023035895008, 1.0, 1.0, 0.039231866674681426], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.54641783], dtype=float32), 3.2337687]. 
=============================================
[2019-04-01 16:53:30,068] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.3170885e-33], sum to 1.0000
[2019-04-01 16:53:30,069] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9886
[2019-04-01 16:53:30,095] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 26.0, 25.39182511854846, 7.732299448087541, 0.0, 1.0, 38.21012387568443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4924800.0000, 
sim time next is 4925400.0000, 
raw observation next is [0.8333333333333334, 40.5, 0.0, 0.0, 26.0, 25.40765385517369, 7.692729345317171, 0.0, 1.0, 37.19785941239769], 
processed observation next is [1.0, 0.0, 0.4856879039704525, 0.405, 0.0, 0.0, 1.0, 0.9153791221676703, 0.07692729345317172, 0.0, 1.0, 0.2656989958028406], 
reward next is 0.7343, 
noisyNet noise sample is [array([0.7963366], dtype=float32), 0.08506344]. 
=============================================
[2019-04-01 16:53:31,390] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:31,390] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:31,399] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run10
[2019-04-01 16:53:32,353] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 3.643215e-36
 1.000000e+00 7.925712e-34], sum to 1.0000
[2019-04-01 16:53:32,356] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9658
[2019-04-01 16:53:32,362] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.75, 19.0, 0.0, 0.0, 26.0, 26.71223888250531, 17.2139224971867, 0.0, 1.0, 6.030459798899811], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5089800.0000, 
sim time next is 5090400.0000, 
raw observation next is [8.7, 19.0, 0.0, 0.0, 26.0, 26.66715912367356, 16.90084207706802, 0.0, 1.0, 6.115968376208786], 
processed observation next is [1.0, 0.9565217391304348, 0.703601108033241, 0.19, 0.0, 0.0, 1.0, 1.09530844623908, 0.1690084207706802, 0.0, 1.0, 0.043685488401491335], 
reward next is 0.9563, 
noisyNet noise sample is [array([-1.8071471], dtype=float32), 2.1429183]. 
=============================================
[2019-04-01 16:53:33,593] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:33,594] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:33,598] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run10
[2019-04-01 16:53:34,111] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:34,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:34,119] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run10
[2019-04-01 16:53:34,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:34,348] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:34,363] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run10
[2019-04-01 16:53:34,734] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:34,735] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:34,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run10
[2019-04-01 16:53:35,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:35,681] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:35,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run10
[2019-04-01 16:53:35,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:35,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:35,998] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run10
[2019-04-01 16:53:36,610] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:36,610] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:36,614] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run10
[2019-04-01 16:53:36,806] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:36,806] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:36,813] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run10
[2019-04-01 16:53:37,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:37,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:37,512] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run10
[2019-04-01 16:53:38,436] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:38,436] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:38,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run10
[2019-04-01 16:53:38,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:38,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:38,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:38,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:38,662] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run10
[2019-04-01 16:53:38,690] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run10
[2019-04-01 16:53:38,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:38,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:38,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run10
[2019-04-01 16:53:38,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:38,978] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:39,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run10
[2019-04-01 16:53:39,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 16:53:39,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:53:39,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run10
[2019-04-01 16:53:43,744] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2512069e-25 9.5754769e-29 8.7935130e-24 6.7782220e-31 1.1438897e-21
 1.0000000e+00 1.5450220e-14], sum to 1.0000
[2019-04-01 16:53:43,744] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9032
[2019-04-01 16:53:43,778] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.283333333333333, 95.5, 0.0, 0.0, 26.0, 20.64815043084408, 15.82453396865496, 0.0, 1.0, 41.34536238277023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 11400.0000, 
sim time next is 12000.0000, 
raw observation next is [7.366666666666667, 95.0, 0.0, 0.0, 26.0, 20.74236716074804, 15.51077494107185, 0.0, 1.0, 41.14779602248005], 
processed observation next is [0.0, 0.13043478260869565, 0.6666666666666667, 0.95, 0.0, 0.0, 1.0, 0.24890959439257695, 0.1551077494107185, 0.0, 1.0, 0.29391282873200036], 
reward next is 0.7061, 
noisyNet noise sample is [array([-0.22591484], dtype=float32), 2.1319964]. 
=============================================
[2019-04-01 16:53:43,822] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[60.24572 ]
 [60.37855 ]
 [60.333317]
 [60.085064]
 [59.787636]], R is [[60.9678154 ]
 [61.06281662]
 [61.15552139]
 [61.24597549]
 [61.33416748]].
[2019-04-01 16:53:45,087] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2247646e-21 1.2806625e-27 9.4415746e-20 3.7331339e-26 4.1334403e-22
 5.3341518e-04 9.9946660e-01], sum to 1.0000
[2019-04-01 16:53:45,087] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7948
[2019-04-01 16:53:45,178] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.200000000000001, 87.16666666666667, 93.0, 0.0, 26.0, 24.37319417706123, 6.260441715843837, 0.0, 1.0, 41.96639119310963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 46200.0000, 
sim time next is 46800.0000, 
raw observation next is [8.3, 86.0, 91.5, 0.0, 26.0, 24.4327609251275, 6.332833409099351, 0.0, 1.0, 35.59181825425814], 
processed observation next is [0.0, 0.5652173913043478, 0.6925207756232689, 0.86, 0.305, 0.0, 1.0, 0.7761087035896429, 0.0633283340909935, 0.0, 1.0, 0.254227273244701], 
reward next is 0.7458, 
noisyNet noise sample is [array([-1.0076379], dtype=float32), 0.23377807]. 
=============================================
[2019-04-01 16:53:55,834] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3828658e-19 1.3616658e-30 6.5371597e-23 4.2763950e-30 5.9868805e-15
 7.0449696e-06 9.9999297e-01], sum to 1.0000
[2019-04-01 16:53:55,834] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7637
[2019-04-01 16:53:55,887] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.1, 73.5, 184.0, 13.0, 26.0, 25.42389933114709, 7.816175757898184, 1.0, 1.0, 41.7807023385962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 127800.0000, 
sim time next is 128400.0000, 
raw observation next is [-8.2, 69.33333333333334, 175.0, 111.3333333333333, 26.0, 25.41917513025707, 8.07235931847257, 1.0, 1.0, 42.09657221904245], 
processed observation next is [1.0, 0.4782608695652174, 0.23545706371191139, 0.6933333333333335, 0.5833333333333334, 0.12302025782688762, 1.0, 0.9170250186081529, 0.0807235931847257, 1.0, 1.0, 0.3006898015645889], 
reward next is 0.6993, 
noisyNet noise sample is [array([-0.01369418], dtype=float32), -0.41278446]. 
=============================================
[2019-04-01 16:53:57,690] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7425587e-31 0.0000000e+00 2.9419012e-37 0.0000000e+00 8.3286540e-29
 1.0000000e+00 2.1885823e-20], sum to 1.0000
[2019-04-01 16:53:57,690] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8418
[2019-04-01 16:53:57,741] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.78342819908658, 7.281588564753856, 0.0, 1.0, 60.40204107116437], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 244800.0000, 
sim time next is 245400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.81285941743472, 7.256189198297672, 0.0, 1.0, 47.33677680424042], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 1.0, 0.8304084882049599, 0.07256189198297672, 0.0, 1.0, 0.338119834316003], 
reward next is 0.6619, 
noisyNet noise sample is [array([-1.4716456], dtype=float32), 0.08313377]. 
=============================================
[2019-04-01 16:54:11,427] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4383480e-19 1.4564815e-27 3.9783372e-28 9.7008937e-27 1.5785098e-16
 8.0370926e-05 9.9991965e-01], sum to 1.0000
[2019-04-01 16:54:11,428] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1481
[2019-04-01 16:54:11,497] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 36.33333333333333, 14.0, 274.3333333333334, 26.0, 25.94934270071529, 9.256259813531926, 1.0, 1.0, 19.23149579580592], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 406200.0000, 
sim time next is 406800.0000, 
raw observation next is [-8.9, 36.0, 10.5, 210.0, 26.0, 25.58578113185132, 8.864718659074276, 1.0, 1.0, 52.61440394712569], 
processed observation next is [1.0, 0.7391304347826086, 0.21606648199445982, 0.36, 0.035, 0.23204419889502761, 1.0, 0.94082587597876, 0.08864718659074276, 1.0, 1.0, 0.3758171710508978], 
reward next is 0.6242, 
noisyNet noise sample is [array([0.570321], dtype=float32), 1.4516383]. 
=============================================
[2019-04-01 16:54:12,628] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9978597e-26 8.1843722e-37 4.5322420e-27 4.1981751e-31 1.1808320e-27
 1.0000000e+00 1.1734572e-22], sum to 1.0000
[2019-04-01 16:54:12,628] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9285
[2019-04-01 16:54:12,644] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.8, 74.66666666666667, 0.0, 0.0, 26.0, 22.18069665643386, 8.550892631283924, 0.0, 1.0, 47.85824342748707], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 364800.0000, 
sim time next is 365400.0000, 
raw observation next is [-15.9, 75.5, 0.0, 0.0, 26.0, 22.09178611334319, 8.68074481608025, 0.0, 1.0, 47.86166726138892], 
processed observation next is [1.0, 0.21739130434782608, 0.02216066481994457, 0.755, 0.0, 0.0, 1.0, 0.44168373047759857, 0.08680744816080249, 0.0, 1.0, 0.3418690518670637], 
reward next is 0.6581, 
noisyNet noise sample is [array([-0.50154364], dtype=float32), 2.7959197]. 
=============================================
[2019-04-01 16:54:15,927] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5089935e-15 2.9369300e-28 8.9503068e-20 2.6052373e-25 9.4279754e-14
 5.1666302e-05 9.9994838e-01], sum to 1.0000
[2019-04-01 16:54:15,928] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5450
[2019-04-01 16:54:16,042] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.61666666666667, 51.0, 58.0, 858.0, 26.0, 25.76779519992379, 8.499374452670743, 1.0, 1.0, 32.44575270067882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 389400.0000, 
sim time next is 390000.0000, 
raw observation next is [-12.43333333333333, 51.00000000000001, 58.0, 881.5, 26.0, 24.99790994164645, 8.063831837785415, 1.0, 1.0, 81.03973482044714], 
processed observation next is [1.0, 0.5217391304347826, 0.11819021237303795, 0.5100000000000001, 0.19333333333333333, 0.9740331491712707, 1.0, 0.8568442773780645, 0.08063831837785415, 1.0, 1.0, 0.5788552487174795], 
reward next is 0.4211, 
noisyNet noise sample is [array([0.6926746], dtype=float32), 0.3746873]. 
=============================================
[2019-04-01 16:54:16,053] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[58.823814]
 [58.47403 ]
 [57.932854]
 [57.440525]
 [56.930088]], R is [[58.68311691]
 [58.86453247]
 [59.02767563]
 [59.19931793]
 [59.30043793]].
[2019-04-01 16:54:19,818] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.14770256e-26 0.00000000e+00 1.39806540e-25 4.46313679e-33
 1.10523049e-26 1.00000000e+00 2.35064083e-20], sum to 1.0000
[2019-04-01 16:54:19,819] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2560
[2019-04-01 16:54:19,862] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.95, 63.0, 71.0, 729.0, 26.0, 25.83774635466998, 8.16656327343796, 1.0, 1.0, 33.43782093334304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 383400.0000, 
sim time next is 384000.0000, 
raw observation next is [-13.76666666666667, 62.0, 68.83333333333334, 734.8333333333333, 26.0, 25.83825042801426, 8.138405858622564, 1.0, 1.0, 31.43140556349002], 
processed observation next is [1.0, 0.43478260869565216, 0.08125577100646345, 0.62, 0.22944444444444448, 0.8119705340699815, 1.0, 0.9768929182877513, 0.08138405858622563, 1.0, 1.0, 0.22451003973921443], 
reward next is 0.7755, 
noisyNet noise sample is [array([0.74856293], dtype=float32), 0.035382863]. 
=============================================
[2019-04-01 16:54:19,869] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[66.15338 ]
 [66.41564 ]
 [66.722145]
 [67.05329 ]
 [67.12335 ]], R is [[66.30541992]
 [66.40352631]
 [66.48475647]
 [66.54908752]
 [66.59635162]].
[2019-04-01 16:54:27,425] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2782179e-30 6.2357748e-38 4.2813396e-29 1.5517520e-37 1.3204486e-33
 9.9999988e-01 7.8811347e-08], sum to 1.0000
[2019-04-01 16:54:27,426] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7773
[2019-04-01 16:54:27,451] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.966666666666667, 84.0, 0.0, 0.0, 26.0, 24.68074448027633, 6.144500478762697, 0.0, 1.0, 39.88545753235483], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 535200.0000, 
sim time next is 535800.0000, 
raw observation next is [1.783333333333333, 84.5, 0.0, 0.0, 26.0, 24.65232414757022, 6.097726208250847, 0.0, 1.0, 39.96667635966526], 
processed observation next is [0.0, 0.17391304347826086, 0.5120036934441367, 0.845, 0.0, 0.0, 1.0, 0.8074748782243171, 0.06097726208250848, 0.0, 1.0, 0.2854762597118947], 
reward next is 0.7145, 
noisyNet noise sample is [array([-1.1788375], dtype=float32), -0.12232555]. 
=============================================
[2019-04-01 16:54:42,318] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.8413534e-35
 1.0000000e+00 8.5487170e-25], sum to 1.0000
[2019-04-01 16:54:42,319] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1408
[2019-04-01 16:54:42,363] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 64.0, 0.0, 0.0, 26.0, 24.73714995254378, 6.703375654764524, 0.0, 1.0, 42.83740696163845], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 770400.0000, 
sim time next is 771000.0000, 
raw observation next is [-6.283333333333333, 64.5, 0.0, 0.0, 26.0, 24.70100086774657, 6.617254771930983, 0.0, 1.0, 42.72218412973891], 
processed observation next is [1.0, 0.9565217391304348, 0.288550323176362, 0.645, 0.0, 0.0, 1.0, 0.814428695392367, 0.06617254771930983, 0.0, 1.0, 0.30515845806956365], 
reward next is 0.6948, 
noisyNet noise sample is [array([0.6151841], dtype=float32), 0.3528122]. 
=============================================
[2019-04-01 16:54:42,368] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[59.58956 ]
 [60.902008]
 [62.832886]
 [65.00273 ]
 [67.129395]], R is [[58.71932983]
 [58.82615662]
 [58.93116379]
 [59.03446198]
 [59.13639069]].
[2019-04-01 16:54:46,845] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4146150e-27 0.0000000e+00 2.4395620e-31 0.0000000e+00 3.3022111e-36
 1.0000000e+00 5.3760516e-21], sum to 1.0000
[2019-04-01 16:54:46,846] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1851
[2019-04-01 16:54:46,860] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 71.0, 0.0, 0.0, 26.0, 24.0628647043058, 5.409204918518188, 0.0, 1.0, 41.0219837423156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 784800.0000, 
sim time next is 785400.0000, 
raw observation next is [-7.8, 71.5, 0.0, 0.0, 26.0, 24.02436299182604, 5.376085890446572, 0.0, 1.0, 40.99314751711192], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.715, 0.0, 0.0, 1.0, 0.7177661416894345, 0.05376085890446572, 0.0, 1.0, 0.2928081965507994], 
reward next is 0.7072, 
noisyNet noise sample is [array([0.4981516], dtype=float32), 0.6795661]. 
=============================================
[2019-04-01 16:54:47,031] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.80417477e-26 1.42805233e-38 2.28380409e-25 0.00000000e+00
 2.36029294e-26 1.09301165e-13 1.00000000e+00], sum to 1.0000
[2019-04-01 16:54:47,035] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1845
[2019-04-01 16:54:47,068] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.35, 73.0, 87.0, 0.0, 26.0, 25.6227869633018, 7.281278737458027, 1.0, 1.0, 32.10076370596352], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 815400.0000, 
sim time next is 816000.0000, 
raw observation next is [-5.066666666666666, 72.33333333333333, 90.83333333333333, 0.0, 26.0, 25.58924040716312, 7.324989450945218, 1.0, 1.0, 30.20873857069995], 
processed observation next is [1.0, 0.43478260869565216, 0.32225300092336107, 0.7233333333333333, 0.30277777777777776, 0.0, 1.0, 0.9413200581661602, 0.07324989450945217, 1.0, 1.0, 0.2157767040764282], 
reward next is 0.7842, 
noisyNet noise sample is [array([-0.5717356], dtype=float32), -0.49048215]. 
=============================================
[2019-04-01 16:54:47,077] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.264595]
 [77.71331 ]
 [78.12764 ]
 [78.69867 ]
 [79.287186]], R is [[77.38182831]
 [77.37871552]
 [77.39620209]
 [77.47189331]
 [77.53878021]].
[2019-04-01 16:54:50,378] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0928389e-31 0.0000000e+00 2.8876955e-38 0.0000000e+00 6.1669478e-34
 1.3011650e-07 9.9999988e-01], sum to 1.0000
[2019-04-01 16:54:50,378] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2983
[2019-04-01 16:54:50,419] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 84.66666666666667, 50.66666666666666, 0.0, 26.0, 25.70107656564441, 8.742746098301323, 1.0, 1.0, 23.52652041851338], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 832800.0000, 
sim time next is 833400.0000, 
raw observation next is [-3.9, 84.0, 49.0, 0.0, 26.0, 25.88675432243168, 8.874610422459625, 1.0, 1.0, 22.25259995787309], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.84, 0.16333333333333333, 0.0, 1.0, 0.9838220460616688, 0.08874610422459625, 1.0, 1.0, 0.15894714255623638], 
reward next is 0.8411, 
noisyNet noise sample is [array([1.462341], dtype=float32), 2.5511997]. 
=============================================
[2019-04-01 16:54:55,514] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7452068e-38 0.0000000e+00 1.6490865e-37 0.0000000e+00 0.0000000e+00
 1.0000000e+00 4.0651795e-08], sum to 1.0000
[2019-04-01 16:54:55,514] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0579
[2019-04-01 16:54:55,534] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.3, 77.16666666666667, 0.0, 0.0, 26.0, 25.60335390038455, 11.51485630205184, 0.0, 1.0, 10.05497468897143], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1047000.0000, 
sim time next is 1047600.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 25.55347420895541, 12.29679656795754, 0.0, 1.0, 32.13945461492612], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 1.0, 0.9362106012793444, 0.1229679656795754, 0.0, 1.0, 0.229567532963758], 
reward next is 0.7704, 
noisyNet noise sample is [array([-0.98928714], dtype=float32), -0.06778392]. 
=============================================
[2019-04-01 16:54:55,829] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 5.452143e-26], sum to 1.0000
[2019-04-01 16:54:55,830] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3499
[2019-04-01 16:54:55,870] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.600000000000001, 89.66666666666667, 0.0, 0.0, 26.0, 25.76807518264165, 10.50569463027878, 1.0, 1.0, 23.87049833576561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 978000.0000, 
sim time next is 978600.0000, 
raw observation next is [9.5, 91.33333333333333, 8.999999999999998, 0.0, 26.0, 25.72613988003954, 10.43308964957449, 1.0, 1.0, 22.32357865471857], 
processed observation next is [1.0, 0.30434782608695654, 0.7257617728531857, 0.9133333333333333, 0.029999999999999995, 0.0, 1.0, 0.9608771257199342, 0.1043308964957449, 1.0, 1.0, 0.1594541332479898], 
reward next is 0.6673, 
noisyNet noise sample is [array([0.0032082], dtype=float32), 0.41063648]. 
=============================================
[2019-04-01 16:55:01,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9828805e-31 3.8282611e-38 3.0465120e-32 0.0000000e+00 0.0000000e+00
 9.9999988e-01 1.5643582e-07], sum to 1.0000
[2019-04-01 16:55:01,774] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3002
[2019-04-01 16:55:01,785] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.2, 67.0, 106.5, 0.0, 26.0, 25.45495014642881, 11.3212728850992, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1159200.0000, 
sim time next is 1159800.0000, 
raw observation next is [17.38333333333333, 66.66666666666667, 114.3333333333333, 0.0, 26.0, 25.38243135279889, 11.10456066004325, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.9441366574330563, 0.6666666666666667, 0.381111111111111, 0.0, 1.0, 0.9117759075426984, 0.1110456066004325, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.79697], dtype=float32), 1.6082213]. 
=============================================
[2019-04-01 16:55:05,325] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.7020958e-37 0.0000000e+00 9.2483370e-33 0.0000000e+00 1.3244671e-29
 1.6396223e-13 1.0000000e+00], sum to 1.0000
[2019-04-01 16:55:05,329] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4199
[2019-04-01 16:55:05,340] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.63333333333333, 54.66666666666667, 165.8333333333333, 0.0, 26.0, 27.20062261227116, 22.76180462301518, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1086000.0000, 
sim time next is 1086600.0000, 
raw observation next is [18.71666666666667, 54.33333333333333, 160.6666666666667, 0.0, 26.0, 27.40051793024986, 23.7361644545693, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.981071098799631, 0.5433333333333333, 0.5355555555555557, 0.0, 1.0, 1.200073990035694, 0.23736164454569297, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9277182], dtype=float32), -0.39114115]. 
=============================================
[2019-04-01 16:55:07,633] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.3490875e-27 2.3449966e-35 2.9725048e-21 1.9293371e-37 1.2453986e-21
 9.9998009e-01 1.9932537e-05], sum to 1.0000
[2019-04-01 16:55:07,637] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3439
[2019-04-01 16:55:07,644] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.71666666666667, 63.33333333333333, 46.0, 0.0, 26.0, 25.02124560653599, 10.14165097571785, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1181400.0000, 
sim time next is 1182000.0000, 
raw observation next is [18.63333333333333, 63.66666666666667, 37.5, 0.0, 26.0, 25.00368455835274, 10.04818961128724, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.9787626962142197, 0.6366666666666667, 0.125, 0.0, 1.0, 0.8576692226218202, 0.10048189611287241, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.82337207], dtype=float32), 0.10353694]. 
=============================================
[2019-04-01 16:55:07,663] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[107.24896 ]
 [106.67196 ]
 [106.06372 ]
 [105.517334]
 [104.95204 ]], R is [[107.71572876]
 [107.63857269]
 [107.56218719]
 [107.48656464]
 [107.41169739]].
[2019-04-01 16:55:08,593] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1904685e-35 0.0000000e+00 4.5528886e-36 0.0000000e+00 5.0165714e-36
 9.9712282e-01 2.8772098e-03], sum to 1.0000
[2019-04-01 16:55:08,597] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1716
[2019-04-01 16:55:08,605] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.46666666666667, 100.0, 12.66666666666667, 0.0, 26.0, 24.57703144774667, 9.192519334531339, 0.0, 1.0, 14.79674579226444], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1270200.0000, 
sim time next is 1270800.0000, 
raw observation next is [12.2, 100.0, 9.5, 0.0, 26.0, 24.57729071476799, 9.212572302445297, 0.0, 1.0, 15.91641560089237], 
processed observation next is [0.0, 0.7391304347826086, 0.8005540166204987, 1.0, 0.03166666666666667, 0.0, 1.0, 0.7967558163954271, 0.09212572302445297, 0.0, 1.0, 0.11368868286351692], 
reward next is 0.8863, 
noisyNet noise sample is [array([-0.68177426], dtype=float32), -1.3733914]. 
=============================================
[2019-04-01 16:55:21,560] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7381777e-25 1.0082768e-34 8.0069956e-23 4.2188837e-34 4.2634474e-23
 1.8571172e-04 9.9981433e-01], sum to 1.0000
[2019-04-01 16:55:21,566] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3310
[2019-04-01 16:55:21,584] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.29862016256528, 9.354152895602992, 0.0, 1.0, 36.0507949175365], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1483800.0000, 
sim time next is 1484400.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.32637726208533, 9.31428462896264, 0.0, 1.0, 36.58970918400021], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 1.0, 0.9037681802979043, 0.09314284628962641, 0.0, 1.0, 0.26135506560000155], 
reward next is 0.7386, 
noisyNet noise sample is [array([0.80878896], dtype=float32), -0.23095462]. 
=============================================
[2019-04-01 16:55:21,602] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0840763e-30 1.5473734e-34 3.0087189e-26 3.7373284e-36 1.0160628e-34
 7.6875709e-05 9.9992311e-01], sum to 1.0000
[2019-04-01 16:55:21,608] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6776
[2019-04-01 16:55:21,619] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.12844916473033, 9.647341542167473, 0.0, 1.0, 39.4306956720702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1465200.0000, 
sim time next is 1465800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.19098541364547, 9.76248978935917, 0.0, 1.0, 38.35076095927654], 
processed observation next is [1.0, 1.0, 0.5069252077562327, 0.92, 0.0, 0.0, 1.0, 0.8844264876636387, 0.0976248978935917, 0.0, 1.0, 0.2739340068519753], 
reward next is 0.7261, 
noisyNet noise sample is [array([-1.1421136], dtype=float32), -1.712517]. 
=============================================
[2019-04-01 16:55:22,261] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3593533e-36 0.0000000e+00 1.1310024e-29 0.0000000e+00 4.0144054e-36
 9.9999976e-01 1.8856159e-07], sum to 1.0000
[2019-04-01 16:55:22,264] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6999
[2019-04-01 16:55:22,280] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 95.66666666666666, 0.0, 0.0, 26.0, 25.39075449316588, 9.42037332915037, 0.0, 1.0, 35.07063676081918], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1482600.0000, 
sim time next is 1483200.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.34324769283486, 9.257061462498859, 0.0, 1.0, 34.8525117983822], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 1.0, 0.9061782418335517, 0.09257061462498858, 0.0, 1.0, 0.24894651284558716], 
reward next is 0.7511, 
noisyNet noise sample is [array([0.35084605], dtype=float32), -1.1598142]. 
=============================================
[2019-04-01 16:55:32,804] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.4527573e-29 6.2463536e-33 2.4095957e-28 6.1808590e-34 1.5914967e-29
 9.8870647e-01 1.1293481e-02], sum to 1.0000
[2019-04-01 16:55:32,804] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0413
[2019-04-01 16:55:32,847] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.8, 83.66666666666667, 52.0, 0.0, 26.0, 25.09203979451331, 7.829406730758104, 0.0, 1.0, 30.40061759114631], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1761000.0000, 
sim time next is 1761600.0000, 
raw observation next is [-1.9, 84.33333333333334, 58.5, 0.0, 26.0, 25.0283865064055, 7.573706655673085, 0.0, 1.0, 28.7081039932387], 
processed observation next is [0.0, 0.391304347826087, 0.4099722991689751, 0.8433333333333334, 0.195, 0.0, 1.0, 0.8611980723436429, 0.07573706655673085, 0.0, 1.0, 0.20505788566599073], 
reward next is 0.7949, 
noisyNet noise sample is [array([0.30739805], dtype=float32), -0.496009]. 
=============================================
[2019-04-01 16:55:39,823] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3301602e-23 3.2766373e-26 1.5024986e-23 5.3485318e-28 1.0580504e-25
 5.2405265e-04 9.9947602e-01], sum to 1.0000
[2019-04-01 16:55:39,823] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9038
[2019-04-01 16:55:39,871] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 79.0, 29.0, 0.0, 26.0, 25.01279789639956, 7.667576232685252, 0.0, 1.0, 44.37778573517517], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1873800.0000, 
sim time next is 1874400.0000, 
raw observation next is [-4.5, 80.33333333333333, 24.33333333333334, 0.0, 26.0, 25.12296798854522, 7.729577017674231, 0.0, 1.0, 40.45642732246834], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.8033333333333332, 0.08111111111111113, 0.0, 1.0, 0.8747097126493172, 0.07729577017674231, 0.0, 1.0, 0.2889744808747739], 
reward next is 0.7110, 
noisyNet noise sample is [array([0.16573259], dtype=float32), -1.5736433]. 
=============================================
[2019-04-01 16:55:40,698] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6993649e-27 1.5804277e-36 8.6345332e-29 1.5448439e-34 0.0000000e+00
 9.9971372e-01 2.8626723e-04], sum to 1.0000
[2019-04-01 16:55:40,698] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5630
[2019-04-01 16:55:40,717] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 83.66666666666667, 0.0, 0.0, 26.0, 23.85330737416584, 5.280310825621046, 0.0, 1.0, 46.2448877090733], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1828200.0000, 
sim time next is 1828800.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 26.0, 23.81423707990624, 5.270432369213803, 0.0, 1.0, 46.25096424274703], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.83, 0.0, 0.0, 1.0, 0.6877481542723197, 0.05270432369213803, 0.0, 1.0, 0.33036403030533595], 
reward next is 0.6696, 
noisyNet noise sample is [array([0.7797704], dtype=float32), 0.73202634]. 
=============================================
[2019-04-01 16:55:45,090] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3855464e-32 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5284822e-36
 1.9145848e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 16:55:45,090] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1163
[2019-04-01 16:55:45,123] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 62.0, 112.0, 0.0, 26.0, 25.79587219266446, 8.093285496114559, 1.0, 1.0, 17.74613626253162], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1953000.0000, 
sim time next is 1953600.0000, 
raw observation next is [-3.0, 62.0, 105.6666666666667, 0.0, 26.0, 25.80121249699891, 8.03873869386495, 1.0, 1.0, 16.92366586651289], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.62, 0.3522222222222223, 0.0, 1.0, 0.9716017852855587, 0.0803873869386495, 1.0, 1.0, 0.12088332761794922], 
reward next is 0.8791, 
noisyNet noise sample is [array([-0.66100085], dtype=float32), -0.46644747]. 
=============================================
[2019-04-01 16:55:47,269] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0393707e-24 1.4756395e-30 1.1397722e-16 6.1592906e-32 1.0233561e-28
 3.4748445e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 16:55:47,269] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5788
[2019-04-01 16:55:47,331] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.566666666666666, 80.16666666666667, 113.6666666666667, 444.6666666666667, 26.0, 25.60347117550656, 7.398411821188465, 1.0, 1.0, 36.17643936063912], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1936200.0000, 
sim time next is 1936800.0000, 
raw observation next is [-7.3, 79.0, 128.0, 392.5, 26.0, 25.616442814091, 7.610481487667042, 1.0, 1.0, 35.24699257921835], 
processed observation next is [1.0, 0.43478260869565216, 0.26038781163434904, 0.79, 0.4266666666666667, 0.43370165745856354, 1.0, 0.9452061162987141, 0.07610481487667042, 1.0, 1.0, 0.2517642327087025], 
reward next is 0.7482, 
noisyNet noise sample is [array([-0.10589845], dtype=float32), 0.25007024]. 
=============================================
[2019-04-01 16:55:50,672] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6222527e-26 3.1263002e-37 5.9368683e-28 3.4596655e-38 1.3422866e-25
 8.5730809e-01 1.4269187e-01], sum to 1.0000
[2019-04-01 16:55:50,673] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4803
[2019-04-01 16:55:50,706] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.25355911318414, 5.625949691331582, 0.0, 1.0, 41.18792112073437], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1995000.0000, 
sim time next is 1995600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.25664758887367, 5.675457237457356, 0.0, 1.0, 41.13737016345753], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 1.0, 0.7509496555533813, 0.056754572374573556, 0.0, 1.0, 0.29383835831041094], 
reward next is 0.7062, 
noisyNet noise sample is [array([-0.9550233], dtype=float32), -1.0872215]. 
=============================================
[2019-04-01 16:55:53,035] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.14827038e-28 3.12167972e-36 1.12807614e-23 4.59980145e-36
 1.26968037e-31 9.90093052e-01 9.90696531e-03], sum to 1.0000
[2019-04-01 16:55:53,036] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9308
[2019-04-01 16:55:53,053] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.38775470733426, 5.593509156752243, 0.0, 1.0, 40.90060838103716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1998000.0000, 
sim time next is 1998600.0000, 
raw observation next is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.32285741066251, 5.594304104927692, 0.0, 1.0, 40.86935091550787], 
processed observation next is [1.0, 0.13043478260869565, 0.3074792243767313, 0.83, 0.0, 0.0, 1.0, 0.7604082015232159, 0.055943041049276915, 0.0, 1.0, 0.2919239351107705], 
reward next is 0.7081, 
noisyNet noise sample is [array([-2.4462323], dtype=float32), -0.47568774]. 
=============================================
[2019-04-01 16:56:19,688] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.5375243e-32
 4.0793893e-05 9.9995923e-01], sum to 1.0000
[2019-04-01 16:56:19,689] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5847
[2019-04-01 16:56:19,749] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.616666666666667, 55.66666666666667, 0.0, 0.0, 26.0, 24.97634172656417, 8.36449476501391, 1.0, 1.0, 36.88328789083551], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2317800.0000, 
sim time next is 2318400.0000, 
raw observation next is [-1.7, 56.0, 0.0, 0.0, 26.0, 25.01665874272833, 8.413456301197634, 0.0, 1.0, 35.22243337870433], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.56, 0.0, 0.0, 1.0, 0.8595226775326184, 0.08413456301197635, 0.0, 1.0, 0.25158880984788806], 
reward next is 0.7484, 
noisyNet noise sample is [array([0.56626576], dtype=float32), 1.0512767]. 
=============================================
[2019-04-01 16:56:21,360] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8827154e-29 1.1245222e-34 4.9760884e-26 1.2363781e-37 2.5232934e-30
 3.4592696e-04 9.9965405e-01], sum to 1.0000
[2019-04-01 16:56:21,360] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2846
[2019-04-01 16:56:21,390] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.1, 47.16666666666666, 0.0, 0.0, 26.0, 24.4585103065626, 5.577946379818147, 0.0, 1.0, 42.70301901666823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2422200.0000, 
sim time next is 2422800.0000, 
raw observation next is [-6.2, 48.0, 0.0, 0.0, 26.0, 24.3996848177861, 5.521431479428554, 0.0, 1.0, 42.77177483452567], 
processed observation next is [0.0, 0.043478260869565216, 0.2908587257617729, 0.48, 0.0, 0.0, 1.0, 0.7713835453980141, 0.05521431479428554, 0.0, 1.0, 0.3055126773894691], 
reward next is 0.6945, 
noisyNet noise sample is [array([-0.7623367], dtype=float32), -1.2021965]. 
=============================================
[2019-04-01 16:56:23,631] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9495131e-37 0.0000000e+00 2.9827202e-35 0.0000000e+00 7.8032076e-30
 1.0000000e+00 5.0641034e-15], sum to 1.0000
[2019-04-01 16:56:23,632] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3494
[2019-04-01 16:56:23,658] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 59.5, 0.0, 0.0, 26.0, 22.88318564486316, 6.367718742128152, 0.0, 1.0, 43.63797915467949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2446200.0000, 
sim time next is 2446800.0000, 
raw observation next is [-9.5, 59.0, 9.166666666666664, 102.6666666666667, 26.0, 22.84470495864452, 6.35126490898779, 0.0, 1.0, 43.60776701423212], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.59, 0.030555555555555548, 0.11344383057090243, 1.0, 0.5492435655206458, 0.0635126490898779, 0.0, 1.0, 0.31148405010165797], 
reward next is 0.6885, 
noisyNet noise sample is [array([-0.39771616], dtype=float32), -0.026950452]. 
=============================================
[2019-04-01 16:56:27,073] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.5962165e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.2434638e-36
 9.2414299e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 16:56:27,073] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8033
[2019-04-01 16:56:27,099] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.566666666666666, 60.66666666666667, 0.0, 0.0, 26.0, 23.31723316717249, 5.592859113639363, 0.0, 1.0, 43.65795314810415], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2438400.0000, 
sim time next is 2439000.0000, 
raw observation next is [-8.65, 60.5, 0.0, 0.0, 26.0, 23.29424212480898, 5.633216451599741, 0.0, 1.0, 43.61524738633517], 
processed observation next is [0.0, 0.21739130434782608, 0.22299168975069253, 0.605, 0.0, 0.0, 1.0, 0.6134631606869974, 0.05633216451599741, 0.0, 1.0, 0.3115374813309655], 
reward next is 0.6885, 
noisyNet noise sample is [array([0.52371377], dtype=float32), -0.026435735]. 
=============================================
[2019-04-01 16:56:27,107] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[68.154015]
 [68.17714 ]
 [68.20095 ]
 [68.22578 ]
 [68.228325]], R is [[68.13446808]
 [68.14128113]
 [68.14778137]
 [68.153862  ]
 [68.15961456]].
[2019-04-01 16:56:28,432] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4159990e-33 6.3612398e-38 2.5552435e-32 0.0000000e+00 3.1858622e-28
 1.0000000e+00 8.7714082e-15], sum to 1.0000
[2019-04-01 16:56:28,432] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0005
[2019-04-01 16:56:28,452] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 39.66666666666667, 0.0, 0.0, 26.0, 25.04007916992484, 6.063403065736381, 0.0, 1.0, 38.69035691904893], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2509800.0000, 
sim time next is 2510400.0000, 
raw observation next is [-1.7, 39.33333333333334, 0.0, 0.0, 26.0, 24.98831415565156, 5.999754481542155, 0.0, 1.0, 38.65900817953622], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.3933333333333334, 0.0, 0.0, 1.0, 0.8554734508073655, 0.05999754481542155, 0.0, 1.0, 0.276135772710973], 
reward next is 0.7239, 
noisyNet noise sample is [array([-0.7686683], dtype=float32), -0.52346325]. 
=============================================
[2019-04-01 16:56:44,984] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8683447e-34 0.0000000e+00 3.0225616e-35 0.0000000e+00 7.4703436e-32
 7.8391671e-01 2.1608324e-01], sum to 1.0000
[2019-04-01 16:56:44,985] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0196
[2019-04-01 16:56:45,004] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 62.33333333333334, 0.0, 0.0, 26.0, 25.39229845934013, 9.53671240277919, 0.0, 1.0, 45.2981102705596], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2755200.0000, 
sim time next is 2755800.0000, 
raw observation next is [-6.0, 61.5, 0.0, 0.0, 26.0, 25.34063420280182, 9.468899989067232, 0.0, 1.0, 47.6089631779458], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.615, 0.0, 0.0, 1.0, 0.9058048861145457, 0.09468899989067232, 0.0, 1.0, 0.3400640226996129], 
reward next is 0.6599, 
noisyNet noise sample is [array([-2.0823414], dtype=float32), -0.6339287]. 
=============================================
[2019-04-01 16:56:48,740] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9742948e-18 6.5194721e-29 2.1388219e-18 1.2885150e-28 9.4722088e-23
 1.3875237e-01 8.6124766e-01], sum to 1.0000
[2019-04-01 16:56:48,740] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2772
[2019-04-01 16:56:48,853] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 23.72553061737345, 5.28004603213773, 0.0, 1.0, 62.0649826505952], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2790000.0000, 
sim time next is 2790600.0000, 
raw observation next is [-6.833333333333334, 64.0, 0.0, 0.0, 26.0, 23.59927343626448, 5.403999826011282, 0.0, 1.0, 129.3091199915095], 
processed observation next is [1.0, 0.30434782608695654, 0.27331486611265005, 0.64, 0.0, 0.0, 1.0, 0.657039062323497, 0.054039998260112826, 0.0, 1.0, 0.923636571367925], 
reward next is 0.0764, 
noisyNet noise sample is [array([0.32798183], dtype=float32), 2.4360316]. 
=============================================
[2019-04-01 16:56:51,384] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-01 16:56:51,384] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 16:56:51,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:56:51,387] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run14
[2019-04-01 16:56:51,402] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 16:56:51,404] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:56:51,404] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 16:56:51,405] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 16:56:51,408] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run14
[2019-04-01 16:56:51,408] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run14
[2019-04-01 16:57:41,602] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.60154533], dtype=float32), -0.27810735]
[2019-04-01 16:57:41,602] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.476531199666668, 82.31478163666667, 0.0, 0.0, 26.0, 23.23544304237629, 5.490999939192068, 0.0, 1.0, 46.74655692087485]
[2019-04-01 16:57:41,602] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 16:57:41,603] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.9007036e-29 2.8137123e-36 1.0144796e-26 1.3735989e-34 6.3966957e-30
 1.0000000e+00 4.6023660e-09], sampled 0.9094907475301671
[2019-04-01 16:57:52,046] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.60154533], dtype=float32), -0.27810735]
[2019-04-01 16:57:52,046] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-7.100000000000001, 78.66666666666667, 0.0, 0.0, 26.0, 24.27480529091885, 5.784131708722292, 0.0, 1.0, 42.06817141567043]
[2019-04-01 16:57:52,046] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 16:57:52,046] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.0937491e-29 3.2975529e-36 4.7534089e-27 2.4435985e-35 1.2172736e-29
 9.9999976e-01 1.9819366e-07], sampled 0.4251876133494681
[2019-04-01 16:58:03,395] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.60154533], dtype=float32), -0.27810735]
[2019-04-01 16:58:03,396] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.149765323333333, 46.79314129666668, 291.19846575, 0.0, 26.0, 24.93930966467681, 6.480214343805454, 1.0, 1.0, 11.10372210759003]
[2019-04-01 16:58:03,396] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 16:58:03,397] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 1.8302592e-38 0.0000000e+00 0.0000000e+00
 3.7505033e-26 1.0000000e+00], sampled 0.13030765868764438
[2019-04-01 16:58:33,151] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 16:58:53,441] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 16:58:55,614] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 16:58:56,637] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1300000, evaluation results [1300000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 16:59:07,742] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 7.3827694e-31 0.0000000e+00 0.0000000e+00
 1.0000000e+00 5.2977723e-29], sum to 1.0000
[2019-04-01 16:59:07,744] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6238
[2019-04-01 16:59:07,764] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.02652190432605, 5.302503768657388, 0.0, 1.0, 39.69040723390784], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3039600.0000, 
sim time next is 3040200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.99777695137447, 5.286697024985419, 0.0, 1.0, 39.71125134539744], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.7139681359106385, 0.05286697024985419, 0.0, 1.0, 0.28365179532426743], 
reward next is 0.7163, 
noisyNet noise sample is [array([0.23206809], dtype=float32), 0.2710268]. 
=============================================
[2019-04-01 16:59:19,076] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0196292e-28 6.7760884e-35 5.9966242e-34 3.6369229e-37 2.3984160e-24
 6.7533348e-33 1.0000000e+00], sum to 1.0000
[2019-04-01 16:59:19,082] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3881
[2019-04-01 16:59:19,098] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 75.0, 25.66666666666666, 239.6666666666666, 26.0, 26.50666887677207, 15.11833401759697, 1.0, 1.0, 8.98946047631659], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3258600.0000, 
sim time next is 3259200.0000, 
raw observation next is [-4.0, 73.0, 17.33333333333333, 171.8333333333333, 26.0, 26.23493765474793, 12.63688087524381, 1.0, 1.0, 8.874795095852935], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.73, 0.05777777777777776, 0.18987108655616938, 1.0, 1.0335625221068472, 0.12636880875243808, 1.0, 1.0, 0.06339139354180667], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4796969], dtype=float32), -0.6636351]. 
=============================================
[2019-04-01 16:59:25,315] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.1662275e-24 3.9576897e-29 7.1472227e-16 2.4195456e-29 6.5698000e-17
 4.1453085e-09 1.0000000e+00], sum to 1.0000
[2019-04-01 16:59:25,316] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5798
[2019-04-01 16:59:25,344] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 50.0, 102.3333333333333, 693.3333333333334, 26.0, 26.53636266158577, 12.72841367720258, 1.0, 1.0, 16.95972677196882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3405000.0000, 
sim time next is 3405600.0000, 
raw observation next is [2.0, 48.0, 104.0, 711.0, 26.0, 26.58570781157299, 13.15909649436604, 1.0, 1.0, 15.85861073349175], 
processed observation next is [1.0, 0.43478260869565216, 0.518005540166205, 0.48, 0.3466666666666667, 0.7856353591160221, 1.0, 1.0836725445104274, 0.1315909649436604, 1.0, 1.0, 0.11327579095351249], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.029197], dtype=float32), -1.3785712]. 
=============================================
[2019-04-01 16:59:31,021] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6420525e-18 1.5766125e-26 3.7195063e-15 2.0256815e-22 1.2164537e-19
 1.0000000e+00 2.1582101e-08], sum to 1.0000
[2019-04-01 16:59:31,024] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8599
[2019-04-01 16:59:31,054] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8333333333333334, 71.16666666666667, 31.33333333333333, 204.3333333333333, 26.0, 25.32772541608146, 8.206021099285472, 1.0, 1.0, 32.95995685973343], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3484200.0000, 
sim time next is 3484800.0000, 
raw observation next is [-1.0, 71.0, 45.5, 253.0, 26.0, 25.24723248367274, 7.998968185672119, 1.0, 1.0, 30.06512095339785], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.15166666666666667, 0.2795580110497238, 1.0, 0.8924617833818199, 0.07998968185672119, 1.0, 1.0, 0.2147508639528418], 
reward next is 0.7852, 
noisyNet noise sample is [array([-0.32120842], dtype=float32), 1.8817587]. 
=============================================
[2019-04-01 16:59:33,028] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9943357e-21 7.1111147e-29 3.8045458e-23 4.4957837e-29 2.9304347e-20
 6.6010074e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 16:59:33,028] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3241
[2019-04-01 16:59:33,040] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 59.66666666666667, 114.0, 803.3333333333333, 26.0, 26.24119468589032, 13.27128358836482, 1.0, 1.0, 7.680842793051067], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3496800.0000, 
sim time next is 3497400.0000, 
raw observation next is [1.5, 59.0, 115.0, 810.0, 26.0, 26.31960988260751, 13.38058398234889, 1.0, 1.0, 7.433098549325047], 
processed observation next is [1.0, 0.4782608695652174, 0.5041551246537397, 0.59, 0.38333333333333336, 0.8950276243093923, 1.0, 1.0456585546582156, 0.1338058398234889, 1.0, 1.0, 0.05309356106660748], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.26366022], dtype=float32), 1.1347866]. 
=============================================
[2019-04-01 16:59:36,122] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9404316e-30 0.0000000e+00 1.0203357e-27 1.1607781e-36 6.0202247e-29
 1.0000000e+00 2.7955060e-19], sum to 1.0000
[2019-04-01 16:59:36,125] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9882
[2019-04-01 16:59:36,145] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.58185828158395, 7.924969288506176, 0.0, 1.0, 23.04365450902641], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3633000.0000, 
sim time next is 3633600.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.52681087591586, 7.811057844068404, 0.0, 1.0, 25.6768358278663], 
processed observation next is [0.0, 0.043478260869565216, 0.7119113573407203, 0.25, 0.0, 0.0, 1.0, 0.9324015537022657, 0.07811057844068404, 0.0, 1.0, 0.183405970199045], 
reward next is 0.8166, 
noisyNet noise sample is [array([-1.5397094], dtype=float32), 1.208312]. 
=============================================
[2019-04-01 16:59:37,655] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.2198268e-35 0.0000000e+00 4.2387733e-32
 1.0000000e+00 9.0242513e-10], sum to 1.0000
[2019-04-01 16:59:37,657] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3010
[2019-04-01 16:59:37,665] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 43.0, 108.5, 797.0, 26.0, 25.35794020536606, 10.06995668615992, 0.0, 1.0, 4.95405145482606], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3679200.0000, 
sim time next is 3679800.0000, 
raw observation next is [6.0, 43.66666666666667, 107.0, 790.0, 26.0, 25.40744414462865, 10.10448813695923, 0.0, 1.0, 4.01148014168549], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.4366666666666667, 0.3566666666666667, 0.8729281767955801, 1.0, 0.9153491635183784, 0.10104488136959229, 0.0, 1.0, 0.028653429583467782], 
reward next is 0.9713, 
noisyNet noise sample is [array([-0.8394079], dtype=float32), 0.0526586]. 
=============================================
[2019-04-01 16:59:42,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.1370389e-11 3.3397800e-19 8.8581296e-13 4.3343364e-16 8.3386039e-05
 9.1556859e-01 8.4348015e-02], sum to 1.0000
[2019-04-01 16:59:42,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2308
[2019-04-01 16:59:42,116] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 69.0, 114.3333333333333, 813.1666666666666, 26.0, 26.47771902575479, 12.76972657191464, 1.0, 1.0, 8.718996636541407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3756000.0000, 
sim time next is 3756600.0000, 
raw observation next is [-2.5, 68.0, 115.0, 822.0, 26.0, 26.48359428601746, 12.96244576404369, 1.0, 1.0, 8.404179772504289], 
processed observation next is [1.0, 0.4782608695652174, 0.39335180055401664, 0.68, 0.38333333333333336, 0.9082872928176795, 1.0, 1.0690848980024943, 0.1296244576404369, 1.0, 1.0, 0.06002985551788778], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6190988], dtype=float32), -1.425323]. 
=============================================
[2019-04-01 16:59:53,877] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.4081868e-08 4.7928465e-16 3.1858637e-15 4.4112734e-14 9.6151966e-01
 3.7738152e-02 7.4210431e-04], sum to 1.0000
[2019-04-01 16:59:53,878] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3857
[2019-04-01 16:59:53,890] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 38.0, 96.5, 756.0, 26.0, 26.86214258427072, 18.25912550579529, 1.0, 1.0, 12.72258087659835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3942000.0000, 
sim time next is 3942600.0000, 
raw observation next is [-4.0, 37.33333333333334, 93.66666666666666, 745.3333333333334, 26.0, 26.97031906392829, 18.81868201230402, 1.0, 1.0, 11.83934455159574], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.3733333333333334, 0.3122222222222222, 0.823572744014733, 1.0, 1.1386170091326129, 0.1881868201230402, 1.0, 1.0, 0.08456674679711243], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7851538], dtype=float32), 0.3030731]. 
=============================================
[2019-04-01 16:59:55,241] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00000000e+00 0.00000000e+00 1.31850885e-36 0.00000000e+00
 3.46143837e-37 1.00000000e+00 8.32252032e-30], sum to 1.0000
[2019-04-01 16:59:55,243] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8387
[2019-04-01 16:59:55,264] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.5, 66.0, 0.0, 0.0, 26.0, 24.03945311732016, 5.38612610617151, 0.0, 1.0, 43.14779679180189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3990600.0000, 
sim time next is 3991200.0000, 
raw observation next is [-12.66666666666667, 67.0, 0.0, 0.0, 26.0, 23.96979242325463, 5.347891510310099, 0.0, 1.0, 43.1945008676518], 
processed observation next is [1.0, 0.17391304347826086, 0.11172668513388727, 0.67, 0.0, 0.0, 1.0, 0.709970346179233, 0.05347891510310099, 0.0, 1.0, 0.3085321490546557], 
reward next is 0.6915, 
noisyNet noise sample is [array([-0.12825012], dtype=float32), -1.9452647]. 
=============================================
[2019-04-01 16:59:55,547] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4523717e-06 1.4766428e-07 3.4646218e-04 9.9920624e-01 4.4277575e-04
 3.7272605e-22 5.8603774e-16], sum to 1.0000
[2019-04-01 16:59:55,548] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0667
[2019-04-01 16:59:55,563] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 48.33333333333334, 119.3333333333333, 826.6666666666667, 20.0, 25.8211439359003, 11.52817112262421, 1.0, 1.0, 8.278535448486315], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3931800.0000, 
sim time next is 3932400.0000, 
raw observation next is [-6.0, 47.66666666666667, 119.1666666666667, 830.8333333333334, 20.0, 25.075445075653, 9.628686226991293, 1.0, 1.0, 7.819589058098848], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.47666666666666674, 0.3972222222222223, 0.91804788213628, 0.14285714285714285, 0.8679207250932858, 0.09628686226991294, 1.0, 1.0, 0.055854207557848916], 
reward next is 0.9441, 
noisyNet noise sample is [array([1.1233752], dtype=float32), 1.3823651]. 
=============================================
[2019-04-01 17:00:04,443] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5930317e-37 0.0000000e+00 6.7325446e-35 0.0000000e+00 6.5226184e-35
 1.0000000e+00 1.0025451e-20], sum to 1.0000
[2019-04-01 17:00:04,446] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1957
[2019-04-01 17:00:04,468] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 51.33333333333333, 0.0, 0.0, 26.0, 24.89988350471402, 6.733921094857187, 0.0, 1.0, 39.07775162853918], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4167600.0000, 
sim time next is 4168200.0000, 
raw observation next is [-4.0, 50.66666666666667, 0.0, 0.0, 26.0, 24.86395487208551, 6.650629486394055, 0.0, 1.0, 39.13103734638917], 
processed observation next is [0.0, 0.21739130434782608, 0.3518005540166205, 0.5066666666666667, 0.0, 0.0, 1.0, 0.8377078388693587, 0.06650629486394055, 0.0, 1.0, 0.2795074096170655], 
reward next is 0.7205, 
noisyNet noise sample is [array([-0.28453407], dtype=float32), 0.20146362]. 
=============================================
[2019-04-01 17:00:06,788] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.288196e-37
 1.000000e+00 8.140802e-09], sum to 1.0000
[2019-04-01 17:00:06,792] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6599
[2019-04-01 17:00:06,808] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.75, 40.83333333333334, 16.0, 108.6666666666667, 26.0, 25.03580957531555, 7.615646184370799, 0.0, 1.0, 14.08327301702082], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4211400.0000, 
sim time next is 4212000.0000, 
raw observation next is [1.7, 41.0, 0.0, 0.0, 26.0, 25.01195257952561, 7.499231023034615, 0.0, 1.0, 20.91098145011403], 
processed observation next is [0.0, 0.782608695652174, 0.5096952908587258, 0.41, 0.0, 0.0, 1.0, 0.8588503685036584, 0.07499231023034615, 0.0, 1.0, 0.14936415321510021], 
reward next is 0.8506, 
noisyNet noise sample is [array([-1.5932696], dtype=float32), -0.53648716]. 
=============================================
[2019-04-01 17:00:06,823] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[74.82764 ]
 [74.90899 ]
 [75.16268 ]
 [75.476036]
 [75.89316 ]], R is [[74.90740204]
 [75.05773163]
 [75.2428894 ]
 [75.4413681 ]
 [75.63389587]].
[2019-04-01 17:00:09,908] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:00:09,909] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4942
[2019-04-01 17:00:09,922] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.166666666666667, 64.83333333333334, 0.0, 0.0, 26.0, 25.22739265625583, 7.887424289513636, 0.0, 1.0, 4.188462148279649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4299000.0000, 
sim time next is 4299600.0000, 
raw observation next is [6.133333333333334, 65.66666666666667, 0.0, 0.0, 26.0, 25.16184913016215, 7.698490316219036, 0.0, 1.0, 4.25084107099188], 
processed observation next is [0.0, 0.782608695652174, 0.6325023084025855, 0.6566666666666667, 0.0, 0.0, 1.0, 0.8802641614517357, 0.07698490316219037, 0.0, 1.0, 0.03036315050708486], 
reward next is 0.9696, 
noisyNet noise sample is [array([-2.2427468], dtype=float32), -0.510027]. 
=============================================
[2019-04-01 17:00:10,556] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6567610e-36 0.0000000e+00 2.8140773e-36 0.0000000e+00 2.5474248e-27
 1.0000000e+00 2.0668225e-09], sum to 1.0000
[2019-04-01 17:00:10,558] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6727
[2019-04-01 17:00:10,594] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.666666666666667, 53.66666666666667, 185.6666666666667, 209.8333333333333, 26.0, 25.60864725728678, 8.390131058767011, 0.0, 1.0, 18.93572836943292], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4268400.0000, 
sim time next is 4269000.0000, 
raw observation next is [3.833333333333333, 53.83333333333333, 189.3333333333333, 288.6666666666666, 26.0, 25.5444526619414, 8.47345876529146, 0.0, 1.0, 17.83233921479094], 
processed observation next is [0.0, 0.391304347826087, 0.5687903970452447, 0.5383333333333333, 0.631111111111111, 0.31896869244935533, 1.0, 0.9349218088487713, 0.0847345876529146, 0.0, 1.0, 0.127373851534221], 
reward next is 0.8726, 
noisyNet noise sample is [array([0.6769075], dtype=float32), -0.39438504]. 
=============================================
[2019-04-01 17:00:10,599] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.22901 ]
 [77.683525]
 [77.001114]
 [76.355835]
 [75.71165 ]], R is [[78.73988342]
 [78.81723022]
 [78.88565063]
 [78.94463348]
 [78.99358368]].
[2019-04-01 17:00:13,205] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.4110289e-32 4.3118729e-32 0.0000000e+00 3.3896152e-36 8.7486355e-13
 2.1264836e-04 9.9978739e-01], sum to 1.0000
[2019-04-01 17:00:13,205] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7975
[2019-04-01 17:00:13,219] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.3, 31.33333333333334, 181.6666666666667, 664.5, 26.0, 28.68101178657729, 34.38231650755628, 1.0, 1.0, 1.141200369752686], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4371600.0000, 
sim time next is 4372200.0000, 
raw observation next is [14.2, 31.5, 195.0, 629.0, 26.0, 28.79161891398876, 35.44655861698647, 1.0, 1.0, 1.080372850440519], 
processed observation next is [1.0, 0.6086956521739131, 0.8559556786703602, 0.315, 0.65, 0.6950276243093922, 1.0, 1.3988027019983942, 0.35446558616986473, 1.0, 1.0, 0.007716948931717993], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6078479], dtype=float32), 2.1948984]. 
=============================================
[2019-04-01 17:00:14,279] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9417819e-20 4.5118659e-21 1.1473296e-20 2.0061115e-21 6.1063860e-05
 9.9993896e-01 7.6449078e-09], sum to 1.0000
[2019-04-01 17:00:14,282] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0883
[2019-04-01 17:00:14,292] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.56666666666667, 29.66666666666667, 115.5, 843.8333333333334, 26.0, 27.54861193578141, 26.64375179631998, 1.0, 1.0, 1.49990423283272], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4368000.0000, 
sim time next is 4368600.0000, 
raw observation next is [14.55, 30.0, 115.0, 842.0, 26.0, 27.95643088996348, 28.9047814870991, 1.0, 1.0, 1.45491070753971], 
processed observation next is [1.0, 0.5652173913043478, 0.865650969529086, 0.3, 0.38333333333333336, 0.9303867403314917, 1.0, 1.2794901271376402, 0.289047814870991, 1.0, 1.0, 0.010392219339569358], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.02002371], dtype=float32), -0.31509727]. 
=============================================
[2019-04-01 17:00:23,898] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.64070865e-24 1.03146451e-29 2.75223420e-30 1.40835517e-30
 2.28003955e-21 1.00000000e+00 1.09139285e-17], sum to 1.0000
[2019-04-01 17:00:23,900] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0771
[2019-04-01 17:00:23,904] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.7247910e-37 0.0000000e+00 2.6159764e-34 0.0000000e+00 9.5827980e-34
 1.0000000e+00 1.5253616e-34], sum to 1.0000
[2019-04-01 17:00:23,912] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8080
[2019-04-01 17:00:23,917] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.26598875663911, 9.042375921588869, 1.0, 1.0, 10.3430805558256], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4560600.0000, 
sim time next is 4561200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.21591584347392, 8.90445990486912, 1.0, 1.0, 10.47368906700552], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.8879879776391313, 0.08904459904869119, 1.0, 1.0, 0.07481206476432514], 
reward next is 0.9252, 
noisyNet noise sample is [array([-1.4024472], dtype=float32), -0.65692264]. 
=============================================
[2019-04-01 17:00:23,932] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.9500000000000001, 66.66666666666667, 0.0, 0.0, 26.0, 25.41430906278531, 8.496731955822932, 0.0, 1.0, 26.17131324848405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4589400.0000, 
sim time next is 4590000.0000, 
raw observation next is [-1.1, 67.0, 0.0, 0.0, 26.0, 25.310484952859, 8.473470802017921, 0.0, 1.0, 32.6978807793888], 
processed observation next is [1.0, 0.13043478260869565, 0.4321329639889197, 0.67, 0.0, 0.0, 1.0, 0.9014978504084284, 0.08473470802017921, 0.0, 1.0, 0.23355629128134855], 
reward next is 0.7664, 
noisyNet noise sample is [array([0.35138935], dtype=float32), -1.6834272]. 
=============================================
[2019-04-01 17:00:23,938] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[59.20384 ]
 [59.173214]
 [59.06934 ]
 [58.998405]
 [58.913204]], R is [[59.37557602]
 [59.59488297]
 [59.81273651]
 [60.02444458]
 [60.24327087]].
[2019-04-01 17:00:26,083] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5810391e-31 1.8316980e-38 4.9295985e-36 1.6031544e-38 1.1087140e-34
 1.0000000e+00 5.2371600e-14], sum to 1.0000
[2019-04-01 17:00:26,084] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4693
[2019-04-01 17:00:26,099] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 73.0, 165.5, 3.0, 26.0, 25.57825219116238, 9.513160511203912, 1.0, 1.0, 18.66871250533526], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4716000.0000, 
sim time next is 4716600.0000, 
raw observation next is [1.833333333333333, 72.83333333333334, 175.6666666666667, 4.0, 26.0, 25.41500710521222, 9.294701169299822, 1.0, 1.0, 19.1556162286993], 
processed observation next is [1.0, 0.6086956521739131, 0.5133887349953832, 0.7283333333333334, 0.5855555555555557, 0.004419889502762431, 1.0, 0.9164295864588884, 0.09294701169299822, 1.0, 1.0, 0.136825830204995], 
reward next is 0.8632, 
noisyNet noise sample is [array([0.02700015], dtype=float32), 0.33181998]. 
=============================================
[2019-04-01 17:00:27,549] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7129177e-34 1.7281855e-38 1.4256160e-33 4.1250990e-38 2.9814780e-27
 9.7121227e-01 2.8787706e-02], sum to 1.0000
[2019-04-01 17:00:27,552] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9275
[2019-04-01 17:00:27,567] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.83333333333334, 0.0, 0.0, 26.0, 25.78054658143155, 11.46537875864004, 0.0, 1.0, 26.65652331139892], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4665000.0000, 
sim time next is 4665600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.76374807038498, 11.18680739362032, 0.0, 1.0, 25.28611613654139], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.9662497243407115, 0.1118680739362032, 0.0, 1.0, 0.18061511526100993], 
reward next is 0.8194, 
noisyNet noise sample is [array([-1.2118387], dtype=float32), 1.158594]. 
=============================================
[2019-04-01 17:00:31,497] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.0493270e-33 0.0000000e+00 7.0722215e-37
 9.9914348e-01 8.5651741e-04], sum to 1.0000
[2019-04-01 17:00:31,501] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7958
[2019-04-01 17:00:31,510] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 122.5, 734.5, 26.0, 25.15191434713942, 9.713906011303637, 0.0, 1.0, 5.190475098625951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4806000.0000, 
sim time next is 4806600.0000, 
raw observation next is [3.0, 37.0, 114.0, 732.0, 26.0, 25.20272094934424, 9.771052964553183, 0.0, 1.0, 5.073583016152503], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.38, 0.8088397790055248, 1.0, 0.8861029927634628, 0.09771052964553183, 0.0, 1.0, 0.03623987868680359], 
reward next is 0.9638, 
noisyNet noise sample is [array([0.05686266], dtype=float32), 1.2766793]. 
=============================================
[2019-04-01 17:00:40,182] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1102909e-37 0.0000000e+00 2.7112314e-36 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.2489308e-12], sum to 1.0000
[2019-04-01 17:00:40,183] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0098
[2019-04-01 17:00:40,218] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 253.5, 171.5, 26.0, 25.48140518151719, 8.31759093128934, 0.0, 1.0, 22.37347257649912], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4874400.0000, 
sim time next is 4875000.0000, 
raw observation next is [-1.733333333333333, 58.66666666666666, 269.0, 169.0, 26.0, 25.41263888328987, 8.203872643605322, 0.0, 1.0, 21.11288091722496], 
processed observation next is [0.0, 0.43478260869565216, 0.41458910433979695, 0.5866666666666666, 0.8966666666666666, 0.1867403314917127, 1.0, 0.9160912690414099, 0.08203872643605323, 0.0, 1.0, 0.15080629226589257], 
reward next is 0.8492, 
noisyNet noise sample is [array([0.66318345], dtype=float32), 0.22841999]. 
=============================================
[2019-04-01 17:00:40,224] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.43669]
 [85.11599]
 [84.95262]
 [84.62473]
 [84.34817]], R is [[85.64064789]
 [85.6244278 ]
 [85.59862518]
 [85.56235504]
 [85.51499176]].
[2019-04-01 17:00:43,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:43,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:43,970] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9195235e-37
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:00:43,974] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8887
[2019-04-01 17:00:43,988] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.833333333333333, 30.33333333333334, 0.0, 0.0, 26.0, 25.57330374456104, 11.46491711376722, 0.0, 1.0, 35.00944730111479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5001000.0000, 
sim time next is 5001600.0000, 
raw observation next is [3.666666666666667, 31.66666666666667, 0.0, 0.0, 26.0, 25.58012902987165, 11.95593998454545, 0.0, 1.0, 34.81336656249974], 
processed observation next is [1.0, 0.9130434782608695, 0.564173591874423, 0.3166666666666667, 0.0, 0.0, 1.0, 0.940018432838807, 0.11955939984545451, 0.0, 1.0, 0.24866690401785527], 
reward next is 0.7513, 
noisyNet noise sample is [array([-0.76539785], dtype=float32), -0.17987566]. 
=============================================
[2019-04-01 17:00:44,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run11
[2019-04-01 17:00:44,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9782090e-34 0.0000000e+00 2.7777580e-33 2.9697306e-38 4.3883924e-30
 9.9957770e-01 4.2234076e-04], sum to 1.0000
[2019-04-01 17:00:44,666] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7969
[2019-04-01 17:00:44,680] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 46.66666666666667, 0.0, 0.0, 26.0, 25.46484399105378, 7.847114550031546, 0.0, 1.0, 29.82410316852757], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5032200.0000, 
sim time next is 5032800.0000, 
raw observation next is [-1.0, 46.0, 0.0, 0.0, 26.0, 25.37032592152173, 7.745122603349017, 0.0, 1.0, 32.44542361019724], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.46, 0.0, 0.0, 1.0, 0.9100465602173899, 0.07745122603349017, 0.0, 1.0, 0.23175302578712312], 
reward next is 0.7682, 
noisyNet noise sample is [array([0.03071789], dtype=float32), 0.12956947]. 
=============================================
[2019-04-01 17:00:45,443] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:45,444] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:45,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run11
[2019-04-01 17:00:45,584] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7984615e-33 4.8346622e-34 3.3111417e-36 2.0933198e-36 3.5958864e-12
 1.0000000e+00 3.8150913e-11], sum to 1.0000
[2019-04-01 17:00:45,595] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3403
[2019-04-01 17:00:45,612] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.333333333333334, 25.66666666666667, 40.33333333333333, 360.1666666666666, 26.0, 27.75864926626353, 19.90664232124326, 1.0, 1.0, 0.9612694151133351], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4987200.0000, 
sim time next is 4987800.0000, 
raw observation next is [7.0, 25.5, 34.0, 304.0, 26.0, 27.00181320251682, 17.63010056007792, 1.0, 1.0, 0.9858514527056361], 
processed observation next is [1.0, 0.7391304347826086, 0.6565096952908588, 0.255, 0.11333333333333333, 0.33591160220994476, 1.0, 1.1431161717881173, 0.17630100560077921, 1.0, 1.0, 0.007041796090754544], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6248668], dtype=float32), 1.5103059]. 
=============================================
[2019-04-01 17:00:48,426] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:48,426] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:48,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run11
[2019-04-01 17:00:48,456] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:48,462] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:48,486] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run11
[2019-04-01 17:00:48,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:48,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:48,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run11
[2019-04-01 17:00:48,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:48,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:48,896] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run11
[2019-04-01 17:00:49,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:49,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:49,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run11
[2019-04-01 17:00:49,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:49,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:49,386] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run11
[2019-04-01 17:00:50,122] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:50,122] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:50,126] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run11
[2019-04-01 17:00:50,257] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:50,257] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:50,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run11
[2019-04-01 17:00:50,301] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85000, global step 1359201: loss 0.0210
[2019-04-01 17:00:50,302] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85000, global step 1359201: learning rate 0.0010
[2019-04-01 17:00:50,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:50,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:50,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run11
[2019-04-01 17:00:50,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:50,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:50,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run11
[2019-04-01 17:00:51,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:51,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:51,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run11
[2019-04-01 17:00:51,638] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.1732842e-26
 1.0000000e+00 3.9051584e-35], sum to 1.0000
[2019-04-01 17:00:51,638] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0961
[2019-04-01 17:00:51,644] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.83333333333333, 17.33333333333334, 0.0, 0.0, 26.0, 27.35419268457086, 22.79319403652186, 1.0, 1.0, 4.760725852270133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5080200.0000, 
sim time next is 5080800.0000, 
raw observation next is [10.66666666666667, 17.66666666666667, 0.0, 0.0, 26.0, 27.35513369862236, 22.4646652598308, 0.0, 1.0, 4.031105323973799], 
processed observation next is [1.0, 0.8260869565217391, 0.7580794090489382, 0.17666666666666672, 0.0, 0.0, 1.0, 1.1935905283746229, 0.224646652598308, 0.0, 1.0, 0.028793609456955707], 
reward next is 0.9712, 
noisyNet noise sample is [array([-0.23434666], dtype=float32), -1.2965043]. 
=============================================
[2019-04-01 17:00:51,723] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.9532243e-36
 1.0000000e+00 4.5042788e-30], sum to 1.0000
[2019-04-01 17:00:51,723] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8919
[2019-04-01 17:00:51,747] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.7, 19.0, 0.0, 0.0, 26.0, 26.66649638551299, 16.89535797070967, 0.0, 1.0, 6.115699963917969], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5090400.0000, 
sim time next is 5091000.0000, 
raw observation next is [8.65, 19.16666666666667, 0.0, 0.0, 26.0, 26.62240403804755, 15.3476812905419, 0.0, 1.0, 6.244170268185368], 
processed observation next is [1.0, 0.9565217391304348, 0.7022160664819946, 0.1916666666666667, 0.0, 0.0, 1.0, 1.0889148625782212, 0.153476812905419, 0.0, 1.0, 0.044601216201324054], 
reward next is 0.9554, 
noisyNet noise sample is [array([-0.85345125], dtype=float32), -0.64674926]. 
=============================================
[2019-04-01 17:00:51,767] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[75.87249 ]
 [76.18816 ]
 [76.586525]
 [77.25096 ]
 [77.824905]], R is [[75.68843842]
 [75.88787079]
 [76.08591461]
 [76.28262329]
 [76.47801208]].
[2019-04-01 17:00:52,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:52,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:52,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run11
[2019-04-01 17:00:52,175] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85000, global step 1359469: loss 0.0039
[2019-04-01 17:00:52,177] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85000, global step 1359469: learning rate 0.0010
[2019-04-01 17:00:53,043] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:53,043] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:53,047] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run11
[2019-04-01 17:00:53,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:00:53,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:00:53,168] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run11
[2019-04-01 17:00:57,100] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85000, global step 1359788: loss 0.1564
[2019-04-01 17:00:57,106] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85000, global step 1359788: learning rate 0.0010
[2019-04-01 17:00:57,116] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85000, global step 1359791: loss 0.1364
[2019-04-01 17:00:57,117] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85000, global step 1359791: learning rate 0.0010
[2019-04-01 17:00:57,165] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85000, global step 1359804: loss 0.1327
[2019-04-01 17:00:57,168] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85000, global step 1359804: learning rate 0.0010
[2019-04-01 17:00:57,184] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85000, global step 1359807: loss 0.1333
[2019-04-01 17:00:57,198] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85000, global step 1359807: learning rate 0.0010
[2019-04-01 17:00:57,983] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 3.4177199e-37 0.0000000e+00 2.6649625e-38
 1.0000000e+00 6.2517034e-25], sum to 1.0000
[2019-04-01 17:00:57,983] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2371
[2019-04-01 17:00:58,007] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85000, global step 1359952: loss 0.0769
[2019-04-01 17:00:58,008] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85000, global step 1359952: learning rate 0.0010
[2019-04-01 17:00:58,047] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.699999999999999, 93.0, 23.83333333333333, 0.0, 26.0, 22.58938655461505, 6.753261531977536, 0.0, 1.0, 58.60546206520611], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 31200.0000, 
sim time next is 31800.0000, 
raw observation next is [7.7, 93.0, 26.66666666666666, 0.0, 26.0, 22.76056745944424, 6.47605374874388, 0.0, 1.0, 57.97020794674206], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.08888888888888886, 0.0, 1.0, 0.5372239227777484, 0.06476053748743879, 0.0, 1.0, 0.41407291390530043], 
reward next is 0.5859, 
noisyNet noise sample is [array([0.06049404], dtype=float32), 0.97585696]. 
=============================================
[2019-04-01 17:00:58,254] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85000, global step 1360016: loss 0.0433
[2019-04-01 17:00:58,257] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85000, global step 1360016: learning rate 0.0010
[2019-04-01 17:00:59,008] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85000, global step 1360177: loss 0.0011
[2019-04-01 17:00:59,032] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85000, global step 1360177: learning rate 0.0010
[2019-04-01 17:00:59,201] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85000, global step 1360232: loss 0.0171
[2019-04-01 17:00:59,201] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85000, global step 1360232: learning rate 0.0010
[2019-04-01 17:00:59,290] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85000, global step 1360255: loss 0.0256
[2019-04-01 17:00:59,301] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85000, global step 1360256: learning rate 0.0010
[2019-04-01 17:00:59,870] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85000, global step 1360397: loss 0.0081
[2019-04-01 17:00:59,871] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85000, global step 1360398: learning rate 0.0010
[2019-04-01 17:01:00,626] A3C_AGENT_WORKER-Thread-9 INFO:Local step 85000, global step 1360620: loss 0.1577
[2019-04-01 17:01:00,637] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 85000, global step 1360620: learning rate 0.0010
[2019-04-01 17:01:00,689] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85000, global step 1360640: loss 0.2190
[2019-04-01 17:01:00,691] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85000, global step 1360640: learning rate 0.0010
[2019-04-01 17:01:01,001] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0355421e-27 3.3743726e-30 1.5600950e-25 6.4213393e-30 5.6406125e-21
 1.0000000e+00 2.3712998e-10], sum to 1.0000
[2019-04-01 17:01:01,003] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2305
[2019-04-01 17:01:01,050] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.200000000000001, 86.0, 90.0, 0.0, 26.0, 24.46472732719951, 6.361351795015189, 0.0, 1.0, 29.35926978568329], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 47400.0000, 
sim time next is 48000.0000, 
raw observation next is [8.100000000000001, 86.0, 88.5, 0.0, 26.0, 24.47021906864107, 6.356683086638308, 0.0, 1.0, 27.5740620756966], 
processed observation next is [0.0, 0.5652173913043478, 0.6869806094182827, 0.86, 0.295, 0.0, 1.0, 0.7814598669487245, 0.06356683086638308, 0.0, 1.0, 0.1969575862549757], 
reward next is 0.8030, 
noisyNet noise sample is [array([0.15321419], dtype=float32), -0.9022585]. 
=============================================
[2019-04-01 17:01:01,054] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[69.46717 ]
 [69.00229 ]
 [68.51078 ]
 [68.090904]
 [67.689766]], R is [[69.99365997]
 [70.08401489]
 [70.12895203]
 [70.1279068 ]
 [70.1076355 ]].
[2019-04-01 17:01:01,194] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85000, global step 1360790: loss 0.0829
[2019-04-01 17:01:01,195] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85000, global step 1360790: learning rate 0.0010
[2019-04-01 17:01:01,496] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 3.7648203e-35 0.0000000e+00 4.8932357e-35
 1.0000000e+00 3.5612111e-13], sum to 1.0000
[2019-04-01 17:01:01,497] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2763
[2019-04-01 17:01:01,534] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85000, global step 1360884: loss 0.1179
[2019-04-01 17:01:01,535] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85000, global step 1360884: learning rate 0.0010
[2019-04-01 17:01:01,564] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 86.0, 0.0, 0.0, 26.0, 24.51796046892883, 6.689604825735214, 0.0, 1.0, 34.60356078995094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 61200.0000, 
sim time next is 61800.0000, 
raw observation next is [5.316666666666667, 86.5, 0.0, 0.0, 26.0, 24.51698015074754, 6.752509448464461, 0.0, 1.0, 41.43706675905602], 
processed observation next is [0.0, 0.7391304347826086, 0.6098799630655587, 0.865, 0.0, 0.0, 1.0, 0.7881400215353628, 0.06752509448464461, 0.0, 1.0, 0.2959790482789716], 
reward next is 0.7040, 
noisyNet noise sample is [array([-1.1666905], dtype=float32), -0.08840072]. 
=============================================
[2019-04-01 17:01:09,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.2778286e-16], sum to 1.0000
[2019-04-01 17:01:09,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3539
[2019-04-01 17:01:10,058] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.1, 68.66666666666666, 22.5, 2.5, 26.0, 24.72302594410133, 8.622289233057236, 1.0, 1.0, 94.10187487973123], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 146400.0000, 
sim time next is 147000.0000, 
raw observation next is [-7.199999999999999, 69.83333333333334, 18.0, 2.0, 26.0, 25.39998909559115, 10.32795879299636, 1.0, 1.0, 54.53565979088965], 
processed observation next is [1.0, 0.6956521739130435, 0.26315789473684215, 0.6983333333333335, 0.06, 0.0022099447513812156, 1.0, 0.9142841565130213, 0.10327958792996361, 1.0, 1.0, 0.38954042707778325], 
reward next is 0.4793, 
noisyNet noise sample is [array([0.53935325], dtype=float32), 1.1560113]. 
=============================================
[2019-04-01 17:01:10,065] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[100.26763 ]
 [102.008156]
 [103.19543 ]
 [104.01037 ]
 [104.84121 ]], R is [[98.65373993]
 [97.99504852]
 [97.72434235]
 [97.51622772]
 [97.26618195]].
[2019-04-01 17:01:17,606] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85500, global step 1365997: loss 8.5733
[2019-04-01 17:01:17,612] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85500, global step 1365998: learning rate 0.0010
[2019-04-01 17:01:19,636] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85500, global step 1366722: loss 6.5290
[2019-04-01 17:01:19,637] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85500, global step 1366722: learning rate 0.0010
[2019-04-01 17:01:22,709] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85500, global step 1367715: loss 0.0676
[2019-04-01 17:01:22,709] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85500, global step 1367715: learning rate 0.0010
[2019-04-01 17:01:22,835] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85500, global step 1367744: loss 0.0598
[2019-04-01 17:01:22,836] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85500, global step 1367744: learning rate 0.0010
[2019-04-01 17:01:23,371] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85500, global step 1367856: loss 0.9156
[2019-04-01 17:01:23,372] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85500, global step 1367856: learning rate 0.0010
[2019-04-01 17:01:23,532] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85500, global step 1367894: loss 1.8831
[2019-04-01 17:01:23,533] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85500, global step 1367894: learning rate 0.0010
[2019-04-01 17:01:23,883] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85500, global step 1367969: loss 3.0960
[2019-04-01 17:01:23,883] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85500, global step 1367969: learning rate 0.0010
[2019-04-01 17:01:24,159] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85500, global step 1368036: loss 6.2444
[2019-04-01 17:01:24,160] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85500, global step 1368036: learning rate 0.0010
[2019-04-01 17:01:24,411] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1468319e-33 3.9496501e-37 1.9775824e-34 0.0000000e+00 1.4520959e-24
 1.0000000e+00 3.6074392e-28], sum to 1.0000
[2019-04-01 17:01:24,413] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8700
[2019-04-01 17:01:24,428] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.2, 80.33333333333333, 0.0, 0.0, 26.0, 24.33203570657348, 6.226981352238556, 0.0, 1.0, 46.53611374702047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 337200.0000, 
sim time next is 337800.0000, 
raw observation next is [-13.3, 81.16666666666667, 0.0, 0.0, 26.0, 24.29143299611339, 6.138372718823613, 0.0, 1.0, 46.54403907220807], 
processed observation next is [1.0, 0.9130434782608695, 0.09418282548476452, 0.8116666666666668, 0.0, 0.0, 1.0, 0.7559189994447699, 0.06138372718823613, 0.0, 1.0, 0.33245742194434336], 
reward next is 0.6675, 
noisyNet noise sample is [array([1.2135003], dtype=float32), -0.47514895]. 
=============================================
[2019-04-01 17:01:24,879] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85500, global step 1368247: loss 7.7141
[2019-04-01 17:01:24,880] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85500, global step 1368248: learning rate 0.0010
[2019-04-01 17:01:25,073] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85500, global step 1368312: loss 6.9982
[2019-04-01 17:01:25,075] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85500, global step 1368314: learning rate 0.0010
[2019-04-01 17:01:25,420] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85500, global step 1368434: loss 6.0917
[2019-04-01 17:01:25,420] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85500, global step 1368434: learning rate 0.0010
[2019-04-01 17:01:26,037] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85500, global step 1368634: loss 4.2885
[2019-04-01 17:01:26,038] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85500, global step 1368634: learning rate 0.0010
[2019-04-01 17:01:26,376] A3C_AGENT_WORKER-Thread-9 INFO:Local step 85500, global step 1368753: loss 4.1920
[2019-04-01 17:01:26,377] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 85500, global step 1368753: learning rate 0.0010
[2019-04-01 17:01:26,953] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85500, global step 1368953: loss 5.5331
[2019-04-01 17:01:26,953] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85500, global step 1368953: learning rate 0.0010
[2019-04-01 17:01:27,156] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85500, global step 1369019: loss 3.1249
[2019-04-01 17:01:27,159] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85500, global step 1369019: learning rate 0.0010
[2019-04-01 17:01:27,447] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85500, global step 1369115: loss 2.5373
[2019-04-01 17:01:27,448] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85500, global step 1369115: learning rate 0.0010
[2019-04-01 17:01:29,996] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.2124023e-16 8.6721278e-24 5.8086131e-13 2.0094346e-23 4.8316807e-18
 6.5868057e-02 9.3413198e-01], sum to 1.0000
[2019-04-01 17:01:29,996] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5521
[2019-04-01 17:01:30,058] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.05, 78.0, 39.0, 738.0, 26.0, 25.52462425307006, 7.336495523677563, 1.0, 1.0, 60.40093431973803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 379800.0000, 
sim time next is 380400.0000, 
raw observation next is [-14.86666666666667, 74.0, 44.33333333333333, 736.5, 26.0, 25.61155639669663, 7.744661432461105, 1.0, 1.0, 60.84758927105428], 
processed observation next is [1.0, 0.391304347826087, 0.05078485687903958, 0.74, 0.14777777777777776, 0.8138121546961326, 1.0, 0.9445080566709473, 0.07744661432461104, 1.0, 1.0, 0.43462563765038775], 
reward next is 0.5654, 
noisyNet noise sample is [array([1.0537384], dtype=float32), -0.31286362]. 
=============================================
[2019-04-01 17:01:32,942] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8818496e-28 2.7224884e-32 3.2374952e-29 0.0000000e+00 3.7869787e-22
 1.4641244e-06 9.9999857e-01], sum to 1.0000
[2019-04-01 17:01:32,943] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0328
[2019-04-01 17:01:32,984] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.100000000000001, 38.66666666666667, 34.33333333333334, 656.3333333333334, 26.0, 26.75844491999074, 13.07309296698982, 1.0, 1.0, 28.58923948416351], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 402000.0000, 
sim time next is 402600.0000, 
raw observation next is [-9.0, 38.33333333333334, 31.66666666666666, 605.6666666666667, 26.0, 26.44615585619578, 12.05605226656711, 1.0, 1.0, 26.60941143944596], 
processed observation next is [1.0, 0.6521739130434783, 0.21329639889196678, 0.3833333333333334, 0.10555555555555554, 0.6692449355432781, 1.0, 1.0637365508851115, 0.1205605226656711, 1.0, 1.0, 0.19006722456747113], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6369589], dtype=float32), -0.35045567]. 
=============================================
[2019-04-01 17:01:41,155] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86000, global step 1373561: loss 0.0351
[2019-04-01 17:01:41,156] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86000, global step 1373562: learning rate 0.0010
[2019-04-01 17:01:41,468] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 1.870008e-29], sum to 1.0000
[2019-04-01 17:01:41,470] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4262
[2019-04-01 17:01:41,486] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.15, 94.0, 0.0, 0.0, 26.0, 24.98718878535976, 7.004707552190268, 0.0, 1.0, 40.67413883087432], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 509400.0000, 
sim time next is 510000.0000, 
raw observation next is [2.333333333333333, 93.33333333333333, 0.0, 0.0, 26.0, 24.97670761490177, 6.986787206534774, 0.0, 1.0, 40.55769851367256], 
processed observation next is [1.0, 0.9130434782608695, 0.5272391505078486, 0.9333333333333332, 0.0, 0.0, 1.0, 0.8538153735573957, 0.06986787206534774, 0.0, 1.0, 0.2896978465262326], 
reward next is 0.7103, 
noisyNet noise sample is [array([-0.01856283], dtype=float32), 0.48612708]. 
=============================================
[2019-04-01 17:01:41,495] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.02153 ]
 [76.711105]
 [77.2973  ]
 [77.51147 ]
 [77.1223  ]], R is [[75.00405121]
 [74.96348572]
 [74.92256927]
 [74.88277435]
 [74.83742523]].
[2019-04-01 17:01:41,599] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5906509e-27 1.5412154e-36 3.2716776e-28 2.3497245e-35 4.7984039e-28
 9.9999535e-01 4.6214313e-06], sum to 1.0000
[2019-04-01 17:01:41,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7377
[2019-04-01 17:01:41,704] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.4166666666666667, 91.83333333333334, 23.0, 71.00000000000001, 26.0, 24.39144597930817, 6.412928589098084, 0.0, 1.0, 91.15019032411575], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 547800.0000, 
sim time next is 548400.0000, 
raw observation next is [0.3333333333333334, 91.66666666666667, 28.5, 87.5, 26.0, 24.67034698432514, 7.053574304556672, 0.0, 1.0, 53.21593345641915], 
processed observation next is [0.0, 0.34782608695652173, 0.4718374884579871, 0.9166666666666667, 0.095, 0.09668508287292818, 1.0, 0.8100495691893056, 0.07053574304556672, 0.0, 1.0, 0.3801138104029939], 
reward next is 0.6199, 
noisyNet noise sample is [array([-1.4423087], dtype=float32), -1.7598404]. 
=============================================
[2019-04-01 17:01:42,995] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86000, global step 1374284: loss 0.0459
[2019-04-01 17:01:42,996] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86000, global step 1374284: learning rate 0.0010
[2019-04-01 17:01:43,959] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.9596349e-35 1.1933538e-38 2.5745678e-29 0.0000000e+00 0.0000000e+00
 9.9999857e-01 1.4459990e-06], sum to 1.0000
[2019-04-01 17:01:43,959] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7148
[2019-04-01 17:01:44,001] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.133333333333333, 80.16666666666667, 132.6666666666667, 462.3333333333334, 26.0, 25.01389106387748, 7.817756867832277, 0.0, 1.0, 28.73997323771699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 564600.0000, 
sim time next is 565200.0000, 
raw observation next is [-1.2, 80.0, 134.0, 495.5, 26.0, 24.97085535805458, 7.751453655349266, 0.0, 1.0, 29.70680093502609], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.44666666666666666, 0.5475138121546961, 1.0, 0.8529793368649398, 0.07751453655349266, 0.0, 1.0, 0.21219143525018636], 
reward next is 0.7878, 
noisyNet noise sample is [array([-0.21549904], dtype=float32), -1.623854]. 
=============================================
[2019-04-01 17:01:45,506] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86000, global step 1375135: loss 0.0211
[2019-04-01 17:01:45,507] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86000, global step 1375135: learning rate 0.0010
[2019-04-01 17:01:45,887] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86000, global step 1375275: loss 0.0215
[2019-04-01 17:01:45,888] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86000, global step 1375275: learning rate 0.0010
[2019-04-01 17:01:46,300] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86000, global step 1375439: loss 0.0172
[2019-04-01 17:01:46,302] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86000, global step 1375439: learning rate 0.0010
[2019-04-01 17:01:46,988] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86000, global step 1375714: loss 0.0075
[2019-04-01 17:01:46,989] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86000, global step 1375714: learning rate 0.0010
[2019-04-01 17:01:47,255] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86000, global step 1375829: loss 0.0083
[2019-04-01 17:01:47,258] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86000, global step 1375830: learning rate 0.0010
[2019-04-01 17:01:47,285] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86000, global step 1375841: loss 0.0041
[2019-04-01 17:01:47,286] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86000, global step 1375841: learning rate 0.0010
[2019-04-01 17:01:47,755] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86000, global step 1376051: loss -0.0072
[2019-04-01 17:01:47,757] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86000, global step 1376051: learning rate 0.0010
[2019-04-01 17:01:48,152] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86000, global step 1376214: loss 0.0018
[2019-04-01 17:01:48,165] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86000, global step 1376216: learning rate 0.0010
[2019-04-01 17:01:48,411] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86000, global step 1376314: loss 0.0365
[2019-04-01 17:01:48,412] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86000, global step 1376314: learning rate 0.0010
[2019-04-01 17:01:48,625] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86000, global step 1376394: loss 0.0271
[2019-04-01 17:01:48,625] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86000, global step 1376394: learning rate 0.0010
[2019-04-01 17:01:48,941] A3C_AGENT_WORKER-Thread-9 INFO:Local step 86000, global step 1376500: loss 0.0092
[2019-04-01 17:01:48,943] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 86000, global step 1376503: learning rate 0.0010
[2019-04-01 17:01:49,530] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86000, global step 1376715: loss 0.0721
[2019-04-01 17:01:49,532] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86000, global step 1376715: learning rate 0.0010
[2019-04-01 17:01:50,457] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86000, global step 1377011: loss 0.0025
[2019-04-01 17:01:50,460] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86000, global step 1377011: learning rate 0.0010
[2019-04-01 17:01:50,568] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86000, global step 1377046: loss 0.0011
[2019-04-01 17:01:50,572] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86000, global step 1377046: learning rate 0.0010
[2019-04-01 17:01:53,210] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8375884e-31 1.8553629e-35 1.8690809e-27 4.7241349e-36 4.5610619e-35
 9.9999893e-01 1.0642871e-06], sum to 1.0000
[2019-04-01 17:01:53,212] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3166
[2019-04-01 17:01:53,239] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.65, 70.0, 0.0, 0.0, 26.0, 24.70895498289579, 5.990054823807571, 0.0, 1.0, 41.18204308397495], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 685800.0000, 
sim time next is 686400.0000, 
raw observation next is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 24.716019339741, 5.954903715500971, 0.0, 1.0, 41.09387710315178], 
processed observation next is [0.0, 0.9565217391304348, 0.35918744228993543, 0.7033333333333333, 0.0, 0.0, 1.0, 0.8165741913915713, 0.05954903715500971, 0.0, 1.0, 0.2935276935939413], 
reward next is 0.7065, 
noisyNet noise sample is [array([0.77898747], dtype=float32), -0.94614774]. 
=============================================
[2019-04-01 17:01:55,086] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.0181328e-35
 1.0000000e+00 2.3483567e-19], sum to 1.0000
[2019-04-01 17:01:55,086] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8138
[2019-04-01 17:01:55,100] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 50.0, 110.0, 611.0, 26.0, 25.58152760866015, 8.16531089798461, 1.0, 1.0, 14.33583112882791], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 738000.0000, 
sim time next is 738600.0000, 
raw observation next is [0.5, 49.16666666666667, 103.0, 665.0, 26.0, 25.50982607196218, 7.94740298447137, 1.0, 1.0, 13.17797981213793], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.4916666666666667, 0.3433333333333333, 0.7348066298342542, 1.0, 0.9299751531374544, 0.07947402984471369, 1.0, 1.0, 0.09412842722955665], 
reward next is 0.9059, 
noisyNet noise sample is [array([-0.26321903], dtype=float32), -0.8263445]. 
=============================================
[2019-04-01 17:01:55,340] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.2235096e-33 0.0000000e+00 7.5794355e-33
 1.0000000e+00 2.6551401e-08], sum to 1.0000
[2019-04-01 17:01:55,341] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2449
[2019-04-01 17:01:55,374] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.633333333333333, 64.0, 0.0, 0.0, 26.0, 24.96915398466712, 6.830511026011121, 0.0, 1.0, 43.50196113854218], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 675600.0000, 
sim time next is 676200.0000, 
raw observation next is [-2.716666666666667, 64.5, 0.0, 0.0, 26.0, 24.9869915912149, 6.791483219172243, 0.0, 1.0, 43.0932768708527], 
processed observation next is [0.0, 0.8260869565217391, 0.3873499538319483, 0.645, 0.0, 0.0, 1.0, 0.8552845130306999, 0.06791483219172244, 0.0, 1.0, 0.3078091205060907], 
reward next is 0.6922, 
noisyNet noise sample is [array([-0.6732841], dtype=float32), 0.12751463]. 
=============================================
[2019-04-01 17:02:03,052] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.7027486e-32], sum to 1.0000
[2019-04-01 17:02:03,052] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9021
[2019-04-01 17:02:03,086] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 83.33333333333334, 0.0, 0.0, 26.0, 25.4115568767942, 7.882364676487259, 1.0, 1.0, 25.83557279530625], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 841200.0000, 
sim time next is 841800.0000, 
raw observation next is [-3.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.19374787413183, 7.654435635961588, 1.0, 1.0, 27.66651821794536], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8266666666666667, 0.0, 0.0, 1.0, 0.8848211248759758, 0.07654435635961587, 1.0, 1.0, 0.19761798727103827], 
reward next is 0.8024, 
noisyNet noise sample is [array([0.65638894], dtype=float32), -0.16095144]. 
=============================================
[2019-04-01 17:02:03,229] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.1772249e-33
 1.0000000e+00 2.2051738e-29], sum to 1.0000
[2019-04-01 17:02:03,236] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3449
[2019-04-01 17:02:03,250] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.58070049190673, 6.246367070055366, 0.0, 1.0, 39.29256114681203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 865800.0000, 
sim time next is 866400.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.65455046903917, 6.306324400030976, 0.0, 1.0, 39.16036786664187], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 1.0, 0.8077929241484527, 0.06306324400030977, 0.0, 1.0, 0.27971691333315624], 
reward next is 0.7203, 
noisyNet noise sample is [array([0.1563202], dtype=float32), 0.7622435]. 
=============================================
[2019-04-01 17:02:03,356] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86500, global step 1381874: loss 0.1668
[2019-04-01 17:02:03,356] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86500, global step 1381874: learning rate 0.0010
[2019-04-01 17:02:04,440] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86500, global step 1382242: loss 0.1638
[2019-04-01 17:02:04,441] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86500, global step 1382242: learning rate 0.0010
[2019-04-01 17:02:06,933] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86500, global step 1383300: loss 0.1944
[2019-04-01 17:02:06,934] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86500, global step 1383300: learning rate 0.0010
[2019-04-01 17:02:07,530] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86500, global step 1383556: loss 0.3683
[2019-04-01 17:02:07,531] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86500, global step 1383557: learning rate 0.0010
[2019-04-01 17:02:07,672] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86500, global step 1383614: loss 0.2195
[2019-04-01 17:02:07,674] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86500, global step 1383615: learning rate 0.0010
[2019-04-01 17:02:08,303] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86500, global step 1383931: loss 0.3574
[2019-04-01 17:02:08,305] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86500, global step 1383932: learning rate 0.0010
[2019-04-01 17:02:08,753] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86500, global step 1384146: loss 0.1746
[2019-04-01 17:02:08,755] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86500, global step 1384146: learning rate 0.0010
[2019-04-01 17:02:08,843] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86500, global step 1384194: loss 0.4716
[2019-04-01 17:02:08,846] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86500, global step 1384195: learning rate 0.0010
[2019-04-01 17:02:09,166] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86500, global step 1384365: loss 0.2770
[2019-04-01 17:02:09,167] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86500, global step 1384365: learning rate 0.0010
[2019-04-01 17:02:09,476] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86500, global step 1384529: loss 0.5242
[2019-04-01 17:02:09,477] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86500, global step 1384529: learning rate 0.0010
[2019-04-01 17:02:09,486] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86500, global step 1384535: loss 0.3283
[2019-04-01 17:02:09,487] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86500, global step 1384536: learning rate 0.0010
[2019-04-01 17:02:09,883] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86500, global step 1384740: loss 0.3686
[2019-04-01 17:02:09,884] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86500, global step 1384740: learning rate 0.0010
[2019-04-01 17:02:10,492] A3C_AGENT_WORKER-Thread-9 INFO:Local step 86500, global step 1385079: loss 0.8213
[2019-04-01 17:02:10,494] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 86500, global step 1385080: learning rate 0.0010
[2019-04-01 17:02:10,928] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86500, global step 1385345: loss 0.7424
[2019-04-01 17:02:10,931] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86500, global step 1385348: learning rate 0.0010
[2019-04-01 17:02:11,832] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86500, global step 1385868: loss 1.5752
[2019-04-01 17:02:11,832] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86500, global step 1385868: learning rate 0.0010
[2019-04-01 17:02:11,922] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86500, global step 1385928: loss 2.0545
[2019-04-01 17:02:11,924] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86500, global step 1385930: learning rate 0.0010
[2019-04-01 17:02:12,306] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.0251065e-28], sum to 1.0000
[2019-04-01 17:02:12,311] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6531
[2019-04-01 17:02:12,340] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.28333333333333, 86.0, 125.3333333333333, 0.0, 26.0, 26.79013192111225, 16.09249763038053, 1.0, 1.0, 5.674674044804164], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 994200.0000, 
sim time next is 994800.0000, 
raw observation next is [12.36666666666667, 86.0, 126.6666666666667, 0.0, 26.0, 26.83161470565146, 13.48812623722388, 1.0, 1.0, 6.059792488969586], 
processed observation next is [1.0, 0.5217391304347826, 0.8051708217913206, 0.86, 0.42222222222222233, 0.0, 1.0, 1.1188021008073517, 0.1348812623722388, 1.0, 1.0, 0.04328423206406847], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.48058972], dtype=float32), 0.51885116]. 
=============================================
[2019-04-01 17:02:13,822] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87000, global step 1387166: loss 0.6132
[2019-04-01 17:02:13,823] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87000, global step 1387166: learning rate 0.0010
[2019-04-01 17:02:15,707] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87000, global step 1388411: loss 0.5669
[2019-04-01 17:02:15,709] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87000, global step 1388411: learning rate 0.0010
[2019-04-01 17:02:17,361] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 9.9999690e-01 3.0812087e-06], sum to 1.0000
[2019-04-01 17:02:17,361] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9812
[2019-04-01 17:02:17,370] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.70428432273093, 13.84616083544682, 0.0, 1.0, 22.49025022699996], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1038000.0000, 
sim time next is 1038600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 26.00211501145331, 14.19497801284579, 0.0, 1.0, 18.26087185078256], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 1.0, 1.00030214449333, 0.14194978012845788, 0.0, 1.0, 0.13043479893416116], 
reward next is 0.8696, 
noisyNet noise sample is [array([1.377176], dtype=float32), 0.6173941]. 
=============================================
[2019-04-01 17:02:18,499] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87000, global step 1390310: loss 0.0225
[2019-04-01 17:02:18,502] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87000, global step 1390313: learning rate 0.0010
[2019-04-01 17:02:19,066] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87000, global step 1390688: loss 0.1065
[2019-04-01 17:02:19,067] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87000, global step 1390688: learning rate 0.0010
[2019-04-01 17:02:19,511] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87000, global step 1390988: loss 0.0156
[2019-04-01 17:02:19,514] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87000, global step 1390988: learning rate 0.0010
[2019-04-01 17:02:20,428] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87000, global step 1391595: loss 0.0054
[2019-04-01 17:02:20,432] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87000, global step 1391595: learning rate 0.0010
[2019-04-01 17:02:20,706] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87000, global step 1391777: loss 0.0003
[2019-04-01 17:02:20,708] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87000, global step 1391778: learning rate 0.0010
[2019-04-01 17:02:21,029] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87000, global step 1391995: loss 0.0021
[2019-04-01 17:02:21,029] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87000, global step 1391996: learning rate 0.0010
[2019-04-01 17:02:21,553] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87000, global step 1392352: loss 0.0323
[2019-04-01 17:02:21,556] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87000, global step 1392354: learning rate 0.0010
[2019-04-01 17:02:21,717] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87000, global step 1392460: loss 0.0128
[2019-04-01 17:02:21,720] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87000, global step 1392462: learning rate 0.0010
[2019-04-01 17:02:21,749] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87000, global step 1392477: loss 0.0081
[2019-04-01 17:02:21,750] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87000, global step 1392477: learning rate 0.0010
[2019-04-01 17:02:21,855] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87000, global step 1392553: loss 0.0010
[2019-04-01 17:02:21,857] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87000, global step 1392554: learning rate 0.0010
[2019-04-01 17:02:22,818] A3C_AGENT_WORKER-Thread-9 INFO:Local step 87000, global step 1393186: loss 0.0136
[2019-04-01 17:02:22,821] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 87000, global step 1393187: learning rate 0.0010
[2019-04-01 17:02:23,228] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87000, global step 1393438: loss 0.0103
[2019-04-01 17:02:23,231] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87000, global step 1393438: learning rate 0.0010
[2019-04-01 17:02:23,902] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87000, global step 1393863: loss 0.0092
[2019-04-01 17:02:23,902] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87000, global step 1393863: learning rate 0.0010
[2019-04-01 17:02:23,931] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87000, global step 1393881: loss 0.0024
[2019-04-01 17:02:23,936] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87000, global step 1393882: learning rate 0.0010
[2019-04-01 17:02:26,072] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 3.3127547e-31], sum to 1.0000
[2019-04-01 17:02:26,073] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0581
[2019-04-01 17:02:26,102] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.75, 96.0, 0.0, 0.0, 26.0, 24.70763081965731, 9.590904507089633, 0.0, 1.0, 20.92024710589114], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1276200.0000, 
sim time next is 1276800.0000, 
raw observation next is [7.566666666666666, 96.0, 0.0, 0.0, 26.0, 24.69178144209809, 9.53915193176644, 0.0, 1.0, 22.59488573612251], 
processed observation next is [0.0, 0.782608695652174, 0.672206832871653, 0.96, 0.0, 0.0, 1.0, 0.8131116345854414, 0.0953915193176644, 0.0, 1.0, 0.16139204097230364], 
reward next is 0.8386, 
noisyNet noise sample is [array([0.80779636], dtype=float32), -0.2757202]. 
=============================================
[2019-04-01 17:02:26,359] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3218962e-24 7.3608911e-31 5.3521542e-20 1.3905968e-26 1.6395743e-18
 1.0000000e+00 1.0679842e-15], sum to 1.0000
[2019-04-01 17:02:26,364] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0617
[2019-04-01 17:02:26,382] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.8261953e-36 0.0000000e+00 1.8272055e-36 0.0000000e+00 2.5814441e-30
 1.0000000e+00 1.2182649e-19], sum to 1.0000
[2019-04-01 17:02:26,382] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0757
[2019-04-01 17:02:26,403] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.1, 92.0, 0.0, 0.0, 26.0, 25.2973915699052, 11.75365516687102, 0.0, 1.0, 40.57440182725279], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1304400.0000, 
sim time next is 1305000.0000, 
raw observation next is [3.0, 92.0, 0.0, 0.0, 26.0, 25.36216094151852, 12.00556652331361, 0.0, 1.0, 40.00722083394837], 
processed observation next is [1.0, 0.08695652173913043, 0.5457063711911359, 0.92, 0.0, 0.0, 1.0, 0.9088801345026459, 0.1200556652331361, 0.0, 1.0, 0.2857658630996312], 
reward next is 0.7142, 
noisyNet noise sample is [array([0.23543905], dtype=float32), 0.8503106]. 
=============================================
[2019-04-01 17:02:26,416] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[68.63692]
 [69.01466]
 [69.3741 ]
 [69.63422]
 [69.89134]], R is [[68.30247498]
 [68.32963562]
 [68.3473053 ]
 [68.36683655]
 [68.42684174]].
[2019-04-01 17:02:26,417] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8, 92.0, 18.0, 0.0, 26.0, 25.72904208164208, 12.11061472020276, 1.0, 1.0, 21.58351860014847], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1326600.0000, 
sim time next is 1327200.0000, 
raw observation next is [0.7000000000000001, 92.0, 22.5, 0.0, 26.0, 25.91011221737037, 12.33325174925356, 1.0, 1.0, 20.31873016070381], 
processed observation next is [1.0, 0.34782608695652173, 0.4819944598337951, 0.92, 0.075, 0.0, 1.0, 0.9871588881957669, 0.1233325174925356, 1.0, 1.0, 0.14513378686217007], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4107804], dtype=float32), 1.1883721]. 
=============================================
[2019-04-01 17:02:29,290] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9315384e-20 2.6328647e-27 1.3595141e-18 7.1967954e-24 1.2966114e-15
 1.0000000e+00 1.2659874e-13], sum to 1.0000
[2019-04-01 17:02:29,292] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3322
[2019-04-01 17:02:29,337] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.6, 100.0, 32.5, 0.0, 26.0, 25.62452539963503, 9.499832340899781, 1.0, 1.0, 15.22739012958236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1501200.0000, 
sim time next is 1501800.0000, 
raw observation next is [1.7, 100.0, 37.33333333333334, 0.0, 26.0, 25.65313099010712, 9.483264267207764, 1.0, 1.0, 14.56596229135286], 
processed observation next is [1.0, 0.391304347826087, 0.5096952908587258, 1.0, 0.12444444444444448, 0.0, 1.0, 0.950447284301017, 0.09483264267207764, 1.0, 1.0, 0.10404258779537758], 
reward next is 0.8960, 
noisyNet noise sample is [array([-0.26164714], dtype=float32), 0.076612525]. 
=============================================
[2019-04-01 17:02:30,253] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87500, global step 1397064: loss 0.1770
[2019-04-01 17:02:30,255] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87500, global step 1397065: learning rate 0.0010
[2019-04-01 17:02:31,766] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87500, global step 1397799: loss 0.0522
[2019-04-01 17:02:31,769] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87500, global step 1397799: learning rate 0.0010
[2019-04-01 17:02:34,306] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87500, global step 1399053: loss 0.1579
[2019-04-01 17:02:34,308] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87500, global step 1399054: learning rate 0.0010
[2019-04-01 17:02:34,809] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87500, global step 1399340: loss 0.0678
[2019-04-01 17:02:34,810] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87500, global step 1399342: learning rate 0.0010
[2019-04-01 17:02:35,169] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87500, global step 1399554: loss 0.1319
[2019-04-01 17:02:35,171] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87500, global step 1399555: learning rate 0.0010
[2019-04-01 17:02:35,579] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87500, global step 1399802: loss 0.0382
[2019-04-01 17:02:35,581] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87500, global step 1399802: learning rate 0.0010
[2019-04-01 17:02:35,931] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-01 17:02:35,937] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 17:02:35,938] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:02:35,938] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 17:02:35,939] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:02:35,939] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 17:02:35,939] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:02:35,943] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run15
[2019-04-01 17:02:35,959] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run15
[2019-04-01 17:02:35,975] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run15
[2019-04-01 17:03:27,191] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.70986986], dtype=float32), -0.40042666]
[2019-04-01 17:03:27,191] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-5.416666666666667, 85.5, 0.0, 0.0, 26.0, 24.94329253268185, 6.898612324604223, 0.0, 1.0, 44.17489515122228]
[2019-04-01 17:03:27,191] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 17:03:27,192] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 8.4951903e-29], sampled 0.9615447088629252
[2019-04-01 17:04:12,442] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.70986986], dtype=float32), -0.40042666]
[2019-04-01 17:04:12,442] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [14.4, 72.0, 0.0, 0.0, 26.0, 26.66621316991355, 23.48076844073707, 0.0, 1.0, 4.853034522890563]
[2019-04-01 17:04:12,442] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 17:04:12,443] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.8649643e-29], sampled 0.724921722494355
[2019-04-01 17:04:17,440] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 17:04:36,375] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 17:04:42,136] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 17:04:43,159] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1400000, evaluation results [1400000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 17:04:43,544] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87500, global step 1400230: loss 0.1179
[2019-04-01 17:04:43,547] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87500, global step 1400231: learning rate 0.0010
[2019-04-01 17:04:43,865] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87500, global step 1400424: loss 0.1521
[2019-04-01 17:04:43,865] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87500, global step 1400424: learning rate 0.0010
[2019-04-01 17:04:43,962] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87500, global step 1400481: loss 0.1650
[2019-04-01 17:04:43,964] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87500, global step 1400481: learning rate 0.0010
[2019-04-01 17:04:44,310] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87500, global step 1400693: loss 0.0294
[2019-04-01 17:04:44,311] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87500, global step 1400693: learning rate 0.0010
[2019-04-01 17:04:44,465] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87500, global step 1400784: loss 0.0833
[2019-04-01 17:04:44,470] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87500, global step 1400784: learning rate 0.0010
[2019-04-01 17:04:44,481] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87500, global step 1400790: loss 0.0545
[2019-04-01 17:04:44,484] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87500, global step 1400792: learning rate 0.0010
[2019-04-01 17:04:45,579] A3C_AGENT_WORKER-Thread-9 INFO:Local step 87500, global step 1401427: loss 0.0358
[2019-04-01 17:04:45,580] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 87500, global step 1401428: learning rate 0.0010
[2019-04-01 17:04:45,969] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87500, global step 1401661: loss 0.0239
[2019-04-01 17:04:45,969] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87500, global step 1401662: learning rate 0.0010
[2019-04-01 17:04:46,387] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87500, global step 1401900: loss 0.1581
[2019-04-01 17:04:46,390] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87500, global step 1401900: learning rate 0.0010
[2019-04-01 17:04:46,556] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87500, global step 1402002: loss 0.1911
[2019-04-01 17:04:46,559] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87500, global step 1402003: learning rate 0.0010
[2019-04-01 17:04:53,257] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88000, global step 1405557: loss 3.9943
[2019-04-01 17:04:53,257] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88000, global step 1405557: learning rate 0.0010
[2019-04-01 17:04:54,782] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88000, global step 1406174: loss 2.9868
[2019-04-01 17:04:54,783] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88000, global step 1406175: learning rate 0.0010
[2019-04-01 17:04:58,069] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88000, global step 1407257: loss 2.2580
[2019-04-01 17:04:58,082] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88000, global step 1407261: learning rate 0.0010
[2019-04-01 17:04:58,750] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88000, global step 1407497: loss 1.8450
[2019-04-01 17:04:58,754] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88000, global step 1407498: learning rate 0.0010
[2019-04-01 17:04:59,009] A3C_AGENT_WORKER-Thread-13 INFO:Local step 88000, global step 1407591: loss 0.8454
[2019-04-01 17:04:59,010] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 88000, global step 1407591: learning rate 0.0010
[2019-04-01 17:04:59,177] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88000, global step 1407656: loss 0.6929
[2019-04-01 17:04:59,183] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88000, global step 1407658: learning rate 0.0010
[2019-04-01 17:05:00,020] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88000, global step 1408008: loss 0.7881
[2019-04-01 17:05:00,021] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88000, global step 1408008: learning rate 0.0010
[2019-04-01 17:05:00,065] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 5.241014e-34 0.000000e+00 0.000000e+00
 1.000000e+00 8.549645e-14], sum to 1.0000
[2019-04-01 17:05:00,068] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0392
[2019-04-01 17:05:00,114] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.3, 87.0, 53.33333333333334, 0.0, 26.0, 24.99812145376912, 7.992762641227299, 0.0, 1.0, 39.00582391789355], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1785000.0000, 
sim time next is 1785600.0000, 
raw observation next is [-3.4, 87.0, 47.0, 0.0, 26.0, 24.94920475697435, 7.95693118468089, 0.0, 1.0, 42.79698535898756], 
processed observation next is [0.0, 0.6956521739130435, 0.368421052631579, 0.87, 0.15666666666666668, 0.0, 1.0, 0.8498863938534785, 0.0795693118468089, 0.0, 1.0, 0.30569275256419687], 
reward next is 0.6943, 
noisyNet noise sample is [array([1.156939], dtype=float32), -0.10758444]. 
=============================================
[2019-04-01 17:05:00,203] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88000, global step 1408081: loss 0.3390
[2019-04-01 17:05:00,204] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88000, global step 1408081: learning rate 0.0010
[2019-04-01 17:05:00,513] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.7020620e-21 2.2897682e-28 4.9147106e-21 3.2727580e-26 1.4808806e-20
 2.1336582e-03 9.9786633e-01], sum to 1.0000
[2019-04-01 17:05:00,513] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3857
[2019-04-01 17:05:00,535] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3744782e-34 0.0000000e+00 9.8887232e-34 0.0000000e+00 3.7575802e-32
 1.0000000e+00 2.0312596e-23], sum to 1.0000
[2019-04-01 17:05:00,540] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4359
[2019-04-01 17:05:00,552] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 78.0, 0.0, 0.0, 26.0, 24.30840771972294, 5.746937379229024, 0.0, 1.0, 45.34411101801216], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1818000.0000, 
sim time next is 1818600.0000, 
raw observation next is [-5.666666666666667, 78.83333333333333, 0.0, 0.0, 26.0, 24.27687761211762, 5.697940109127778, 0.0, 1.0, 45.42778495960234], 
processed observation next is [0.0, 0.043478260869565216, 0.30563250230840255, 0.7883333333333333, 0.0, 0.0, 1.0, 0.7538396588739457, 0.056979401091277776, 0.0, 1.0, 0.32448417828287385], 
reward next is 0.6755, 
noisyNet noise sample is [array([-0.8219667], dtype=float32), -1.2629539]. 
=============================================
[2019-04-01 17:05:00,560] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.933333333333333, 82.16666666666667, 0.0, 0.0, 26.0, 24.16756821507332, 5.546757239364588, 0.0, 1.0, 45.70779340075531], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1821000.0000, 
sim time next is 1821600.0000, 
raw observation next is [-6.0, 83.0, 0.0, 0.0, 26.0, 24.13323030792209, 5.507841843420103, 0.0, 1.0, 45.77601472410436], 
processed observation next is [0.0, 0.08695652173913043, 0.296398891966759, 0.83, 0.0, 0.0, 1.0, 0.7333186154174415, 0.055078418434201026, 0.0, 1.0, 0.32697153374360255], 
reward next is 0.6730, 
noisyNet noise sample is [array([-1.0863167], dtype=float32), 1.2030587]. 
=============================================
[2019-04-01 17:05:00,673] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88000, global step 1408259: loss 0.6571
[2019-04-01 17:05:00,678] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88000, global step 1408259: learning rate 0.0010
[2019-04-01 17:05:00,741] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88000, global step 1408289: loss 0.7021
[2019-04-01 17:05:00,742] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88000, global step 1408289: learning rate 0.0010
[2019-04-01 17:05:00,816] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88000, global step 1408320: loss 0.7431
[2019-04-01 17:05:00,817] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88000, global step 1408320: learning rate 0.0010
[2019-04-01 17:05:01,012] A3C_AGENT_WORKER-Thread-11 INFO:Local step 88000, global step 1408385: loss 0.5247
[2019-04-01 17:05:01,015] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 88000, global step 1408385: learning rate 0.0010
[2019-04-01 17:05:02,284] A3C_AGENT_WORKER-Thread-9 INFO:Local step 88000, global step 1408832: loss 0.3948
[2019-04-01 17:05:02,285] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 88000, global step 1408832: learning rate 0.0010
[2019-04-01 17:05:02,881] A3C_AGENT_WORKER-Thread-12 INFO:Local step 88000, global step 1409023: loss 0.3733
[2019-04-01 17:05:02,885] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 88000, global step 1409023: learning rate 0.0010
[2019-04-01 17:05:03,022] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88000, global step 1409070: loss 0.3565
[2019-04-01 17:05:03,023] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88000, global step 1409070: learning rate 0.0010
[2019-04-01 17:05:03,314] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88000, global step 1409169: loss 0.3314
[2019-04-01 17:05:03,328] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88000, global step 1409169: learning rate 0.0010
[2019-04-01 17:05:05,883] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.7927488e-37 0.0000000e+00 2.3092403e-36 0.0000000e+00 8.6386835e-37
 1.0000000e+00 3.2770029e-27], sum to 1.0000
[2019-04-01 17:05:05,883] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3645
[2019-04-01 17:05:05,921] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.36079875131144, 5.675571906706746, 0.0, 1.0, 44.32750701585842], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1897200.0000, 
sim time next is 1897800.0000, 
raw observation next is [-7.3, 79.50000000000001, 0.0, 0.0, 26.0, 24.31957068318577, 5.632506703568805, 0.0, 1.0, 44.33215439696885], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.7950000000000002, 0.0, 0.0, 1.0, 0.7599386690265385, 0.05632506703568805, 0.0, 1.0, 0.3166582456926347], 
reward next is 0.6833, 
noisyNet noise sample is [array([-0.18897372], dtype=float32), 0.22400957]. 
=============================================
[2019-04-01 17:05:15,670] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88500, global step 1413564: loss 0.1186
[2019-04-01 17:05:15,671] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88500, global step 1413564: learning rate 0.0010
[2019-04-01 17:05:17,566] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2160458e-26 0.0000000e+00 2.8506206e-34 0.0000000e+00 6.9323559e-29
 1.0000000e+00 3.5292051e-14], sum to 1.0000
[2019-04-01 17:05:17,566] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3846
[2019-04-01 17:05:17,609] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 86.0, 49.0, 0.0, 26.0, 25.81132382454813, 7.658256620233762, 1.0, 1.0, 29.22609612113251], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2019600.0000, 
sim time next is 2020200.0000, 
raw observation next is [-5.933333333333334, 85.5, 55.66666666666667, 0.0, 26.0, 25.74468074441541, 7.471363011503651, 1.0, 1.0, 27.73501408877782], 
processed observation next is [1.0, 0.391304347826087, 0.29824561403508776, 0.855, 0.18555555555555558, 0.0, 1.0, 0.9635258206307731, 0.07471363011503651, 1.0, 1.0, 0.19810724349127015], 
reward next is 0.8019, 
noisyNet noise sample is [array([-0.30254185], dtype=float32), -1.8148708]. 
=============================================
[2019-04-01 17:05:17,967] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.2708776e-23
 1.0000000e+00 6.0397968e-30], sum to 1.0000
[2019-04-01 17:05:17,967] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3737
[2019-04-01 17:05:17,982] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88500, global step 1414237: loss 0.7338
[2019-04-01 17:05:17,983] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88500, global step 1414237: learning rate 0.0010
[2019-04-01 17:05:17,998] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.00000000000001, 45.0, 0.0, 26.0, 25.99828891111786, 9.632961498129006, 1.0, 1.0, 20.4704059196984], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2045400.0000, 
sim time next is 2046000.0000, 
raw observation next is [-3.9, 82.0, 38.5, 0.0, 26.0, 25.89796330600536, 9.375174725889343, 1.0, 1.0, 19.40608653352334], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.12833333333333333, 0.0, 1.0, 0.985423329429337, 0.09375174725889343, 1.0, 1.0, 0.13861490381088099], 
reward next is 0.8614, 
noisyNet noise sample is [array([0.07537862], dtype=float32), -0.035390623]. 
=============================================
[2019-04-01 17:05:18,014] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[58.976643]
 [58.686123]
 [58.510883]
 [58.370094]
 [58.443375]], R is [[59.39852905]
 [59.6583252 ]
 [59.9077034 ]
 [60.14446259]
 [60.36988068]].
[2019-04-01 17:05:19,843] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.5187308e-33 0.0000000e+00 2.4030048e-31 0.0000000e+00 1.5633874e-35
 1.0000000e+00 1.2826654e-21], sum to 1.0000
[2019-04-01 17:05:19,843] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4299
[2019-04-01 17:05:19,860] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 91.0, 0.0, 0.0, 26.0, 24.81601054074467, 6.771635333224535, 0.0, 1.0, 42.0782778908813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2077200.0000, 
sim time next is 2077800.0000, 
raw observation next is [-4.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.78650625176075, 6.747355822144523, 0.0, 1.0, 42.08013235749066], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.9016666666666667, 0.0, 0.0, 1.0, 0.8266437502515355, 0.06747355822144523, 0.0, 1.0, 0.30057237398207615], 
reward next is 0.6994, 
noisyNet noise sample is [array([0.11439697], dtype=float32), 0.85075855]. 
=============================================
[2019-04-01 17:05:20,770] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88500, global step 1415226: loss 0.8293
[2019-04-01 17:05:20,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88500, global step 1415226: learning rate 0.0010
[2019-04-01 17:05:21,151] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.5148867e-38
 1.0000000e+00 5.7823112e-36], sum to 1.0000
[2019-04-01 17:05:21,152] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3107
[2019-04-01 17:05:21,196] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 84.0, 0.0, 0.0, 26.0, 26.33281498579365, 12.99972717730546, 0.0, 1.0, 43.34878891776144], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2057400.0000, 
sim time next is 2058000.0000, 
raw observation next is [-3.899999999999999, 84.66666666666666, 0.0, 0.0, 26.0, 26.35860546315507, 12.82658134972951, 0.0, 1.0, 39.91386233063594], 
processed observation next is [1.0, 0.8260869565217391, 0.35457063711911363, 0.8466666666666666, 0.0, 0.0, 1.0, 1.0512293518792954, 0.1282658134972951, 0.0, 1.0, 0.2850990166473996], 
reward next is 0.7149, 
noisyNet noise sample is [array([-0.6898645], dtype=float32), -0.8185364]. 
=============================================
[2019-04-01 17:05:21,206] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[61.411   ]
 [59.567722]
 [59.2921  ]
 [58.91092 ]
 [58.88674 ]], R is [[62.32365036]
 [62.39077759]
 [61.76686859]
 [61.14920044]
 [60.89373398]].
[2019-04-01 17:05:21,742] A3C_AGENT_WORKER-Thread-13 INFO:Local step 88500, global step 1415576: loss 1.3226
[2019-04-01 17:05:21,744] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 88500, global step 1415577: learning rate 0.0010
[2019-04-01 17:05:21,868] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88500, global step 1415615: loss 1.2021
[2019-04-01 17:05:21,869] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88500, global step 1415615: learning rate 0.0010
[2019-04-01 17:05:21,918] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88500, global step 1415637: loss 1.2221
[2019-04-01 17:05:21,920] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88500, global step 1415638: learning rate 0.0010
[2019-04-01 17:05:23,254] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88500, global step 1416132: loss 1.1465
[2019-04-01 17:05:23,255] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88500, global step 1416132: learning rate 0.0010
[2019-04-01 17:05:23,625] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88500, global step 1416248: loss 0.7095
[2019-04-01 17:05:23,625] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88500, global step 1416248: learning rate 0.0010
[2019-04-01 17:05:23,718] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88500, global step 1416275: loss 0.4725
[2019-04-01 17:05:23,719] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88500, global step 1416275: learning rate 0.0010
[2019-04-01 17:05:23,775] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88500, global step 1416286: loss 0.4662
[2019-04-01 17:05:23,777] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88500, global step 1416286: learning rate 0.0010
[2019-04-01 17:05:23,805] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88500, global step 1416294: loss 0.4635
[2019-04-01 17:05:23,806] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88500, global step 1416294: learning rate 0.0010
[2019-04-01 17:05:24,573] A3C_AGENT_WORKER-Thread-11 INFO:Local step 88500, global step 1416527: loss 0.4086
[2019-04-01 17:05:24,573] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 88500, global step 1416527: learning rate 0.0010
[2019-04-01 17:05:25,708] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88500, global step 1416964: loss 0.7286
[2019-04-01 17:05:25,709] A3C_AGENT_WORKER-Thread-9 INFO:Local step 88500, global step 1416964: loss 0.9541
[2019-04-01 17:05:25,711] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 88500, global step 1416965: learning rate 0.0010
[2019-04-01 17:05:25,711] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88500, global step 1416965: learning rate 0.0010
[2019-04-01 17:05:25,738] A3C_AGENT_WORKER-Thread-12 INFO:Local step 88500, global step 1416975: loss 0.8025
[2019-04-01 17:05:25,739] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 88500, global step 1416975: learning rate 0.0010
[2019-04-01 17:05:25,876] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88500, global step 1417013: loss 0.5896
[2019-04-01 17:05:25,877] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88500, global step 1417013: learning rate 0.0010
[2019-04-01 17:05:28,485] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:05:28,486] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6266
[2019-04-01 17:05:28,501] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 26.0, 24.72960057973366, 6.753977637752925, 0.0, 1.0, 41.72931462863047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2156400.0000, 
sim time next is 2157000.0000, 
raw observation next is [-7.3, 82.00000000000001, 0.0, 0.0, 26.0, 24.65956324725271, 6.61115387891594, 0.0, 1.0, 41.76622769487129], 
processed observation next is [1.0, 1.0, 0.26038781163434904, 0.8200000000000002, 0.0, 0.0, 1.0, 0.8085090353218156, 0.0661115387891594, 0.0, 1.0, 0.29833019782050924], 
reward next is 0.7017, 
noisyNet noise sample is [array([0.5036396], dtype=float32), 0.21691906]. 
=============================================
[2019-04-01 17:05:28,517] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[72.41179]
 [72.17386]
 [71.89386]
 [71.6869 ]
 [71.48806]], R is [[72.53972626]
 [72.51626587]
 [72.49331665]
 [72.47088623]
 [72.45039368]].
[2019-04-01 17:05:29,060] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 8.2170024e-29], sum to 1.0000
[2019-04-01 17:05:29,061] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8427
[2019-04-01 17:05:29,118] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 25.25583348524151, 9.972510409394282, 1.0, 1.0, 41.14128509094515], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2224800.0000, 
sim time next is 2225400.0000, 
raw observation next is [-4.516666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 25.65133274103389, 10.36689725671955, 1.0, 1.0, 31.72248228234127], 
processed observation next is [1.0, 0.782608695652174, 0.337488457987073, 0.6833333333333335, 0.0, 0.0, 1.0, 0.95019039157627, 0.1036689725671955, 1.0, 1.0, 0.2265891591595805], 
reward next is 0.6267, 
noisyNet noise sample is [array([0.54454666], dtype=float32), -0.80738574]. 
=============================================
[2019-04-01 17:05:37,495] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89000, global step 1421176: loss 2.3165
[2019-04-01 17:05:37,502] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89000, global step 1421177: learning rate 0.0010
[2019-04-01 17:05:39,111] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:05:39,112] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8544
[2019-04-01 17:05:39,133] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 54.66666666666667, 0.0, 0.0, 26.0, 25.51131824492693, 9.579128181889443, 0.0, 1.0, 28.18046569648646], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2323200.0000, 
sim time next is 2323800.0000, 
raw observation next is [-1.7, 55.0, 0.0, 0.0, 26.0, 25.46247681486867, 9.365388698182166, 0.0, 1.0, 30.98381555263465], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.55, 0.0, 0.0, 1.0, 0.923210973552667, 0.09365388698182166, 0.0, 1.0, 0.22131296823310465], 
reward next is 0.7787, 
noisyNet noise sample is [array([0.4190032], dtype=float32), -0.81348497]. 
=============================================
[2019-04-01 17:05:40,065] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89000, global step 1422233: loss 3.0035
[2019-04-01 17:05:40,065] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89000, global step 1422233: learning rate 0.0010
[2019-04-01 17:05:42,632] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89000, global step 1423227: loss 3.5719
[2019-04-01 17:05:42,636] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89000, global step 1423227: learning rate 0.0010
[2019-04-01 17:05:42,711] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:05:42,711] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4651
[2019-04-01 17:05:42,728] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 38.33333333333334, 0.0, 0.0, 26.0, 25.00104803422381, 6.148482612619546, 0.0, 1.0, 38.98707143121118], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2506200.0000, 
sim time next is 2506800.0000, 
raw observation next is [-1.7, 38.66666666666667, 0.0, 0.0, 26.0, 24.97269079895552, 6.349556107286912, 0.0, 1.0, 39.45296187401591], 
processed observation next is [1.0, 0.0, 0.4155124653739613, 0.3866666666666667, 0.0, 0.0, 1.0, 0.8532415427079313, 0.06349556107286912, 0.0, 1.0, 0.2818068705286851], 
reward next is 0.7182, 
noisyNet noise sample is [array([-0.68280685], dtype=float32), 0.9454382]. 
=============================================
[2019-04-01 17:05:43,247] A3C_AGENT_WORKER-Thread-13 INFO:Local step 89000, global step 1423470: loss 4.5384
[2019-04-01 17:05:43,250] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 89000, global step 1423471: learning rate 0.0010
[2019-04-01 17:05:43,462] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.5455778e-38 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.4520822e-24], sum to 1.0000
[2019-04-01 17:05:43,464] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2157
[2019-04-01 17:05:43,496] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.7, 43.83333333333334, 0.0, 0.0, 26.0, 24.55200240464331, 5.748274916323417, 0.0, 1.0, 42.55494010332913], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2419800.0000, 
sim time next is 2420400.0000, 
raw observation next is [-5.8, 44.66666666666667, 0.0, 0.0, 26.0, 24.52347890542046, 5.741071544754512, 0.0, 1.0, 42.56463732579047], 
processed observation next is [0.0, 0.0, 0.30193905817174516, 0.4466666666666667, 0.0, 0.0, 1.0, 0.7890684150600659, 0.05741071544754513, 0.0, 1.0, 0.30403312375564623], 
reward next is 0.6960, 
noisyNet noise sample is [array([-0.20021261], dtype=float32), -0.13741921]. 
=============================================
[2019-04-01 17:05:43,684] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89000, global step 1423652: loss 4.0528
[2019-04-01 17:05:43,686] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89000, global step 1423652: learning rate 0.0010
[2019-04-01 17:05:44,008] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89000, global step 1423761: loss 4.1123
[2019-04-01 17:05:44,010] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89000, global step 1423762: learning rate 0.0010
[2019-04-01 17:05:44,748] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89000, global step 1424043: loss 3.4850
[2019-04-01 17:05:44,749] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89000, global step 1424043: learning rate 0.0010
[2019-04-01 17:05:45,079] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89000, global step 1424177: loss 5.2532
[2019-04-01 17:05:45,079] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89000, global step 1424177: learning rate 0.0010
[2019-04-01 17:05:45,372] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89000, global step 1424314: loss 4.0830
[2019-04-01 17:05:45,373] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89000, global step 1424314: learning rate 0.0010
[2019-04-01 17:05:45,471] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89000, global step 1424356: loss 3.1587
[2019-04-01 17:05:45,472] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89000, global step 1424356: learning rate 0.0010
[2019-04-01 17:05:45,819] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89000, global step 1424523: loss 3.0424
[2019-04-01 17:05:45,820] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89000, global step 1424523: learning rate 0.0010
[2019-04-01 17:05:46,259] A3C_AGENT_WORKER-Thread-11 INFO:Local step 89000, global step 1424709: loss 2.4520
[2019-04-01 17:05:46,262] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 89000, global step 1424709: learning rate 0.0010
[2019-04-01 17:05:47,153] A3C_AGENT_WORKER-Thread-12 INFO:Local step 89000, global step 1425071: loss 1.9009
[2019-04-01 17:05:47,153] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 89000, global step 1425071: learning rate 0.0010
[2019-04-01 17:05:47,238] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89000, global step 1425107: loss 1.4369
[2019-04-01 17:05:47,240] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89000, global step 1425109: learning rate 0.0010
[2019-04-01 17:05:47,539] A3C_AGENT_WORKER-Thread-9 INFO:Local step 89000, global step 1425246: loss 1.3273
[2019-04-01 17:05:47,541] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 89000, global step 1425246: learning rate 0.0010
[2019-04-01 17:05:47,747] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89000, global step 1425346: loss 1.6913
[2019-04-01 17:05:47,749] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89000, global step 1425346: learning rate 0.0010
[2019-04-01 17:05:48,133] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 4.665699e-37 0.000000e+00 0.000000e+00
 1.000000e+00 6.395287e-28], sum to 1.0000
[2019-04-01 17:05:48,133] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8500
[2019-04-01 17:05:48,154] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 60.0, 0.0, 0.0, 26.0, 22.9134280781166, 6.287808153165962, 0.0, 1.0, 43.58589021311335], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2445600.0000, 
sim time next is 2446200.0000, 
raw observation next is [-9.5, 59.5, 0.0, 0.0, 26.0, 22.88318564486316, 6.367718742128152, 0.0, 1.0, 43.63797915467949], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.595, 0.0, 0.0, 1.0, 0.554740806409023, 0.06367718742128152, 0.0, 1.0, 0.3116998511048535], 
reward next is 0.6883, 
noisyNet noise sample is [array([0.385948], dtype=float32), -1.3845205]. 
=============================================
[2019-04-01 17:05:50,741] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 5.9804643e-37], sum to 1.0000
[2019-04-01 17:05:50,741] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0132
[2019-04-01 17:05:50,752] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.1, 78.66666666666667, 0.0, 0.0, 26.0, 23.82871342843816, 5.258024767147416, 0.0, 1.0, 44.07576292549736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2616000.0000, 
sim time next is 2616600.0000, 
raw observation next is [-7.199999999999999, 78.83333333333334, 0.0, 0.0, 26.0, 23.78068480357998, 5.294256851564749, 0.0, 1.0, 44.19303338123836], 
processed observation next is [1.0, 0.2608695652173913, 0.26315789473684215, 0.7883333333333334, 0.0, 0.0, 1.0, 0.6829549719399972, 0.05294256851564749, 0.0, 1.0, 0.31566452415170254], 
reward next is 0.6843, 
noisyNet noise sample is [array([-0.11327349], dtype=float32), -0.6374465]. 
=============================================
[2019-04-01 17:05:53,102] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 4.940703e-22], sum to 1.0000
[2019-04-01 17:05:53,103] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0856
[2019-04-01 17:05:53,116] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 57.0, 0.0, 0.0, 26.0, 24.77871503753207, 5.596399804482812, 0.0, 1.0, 37.96603481915833], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2525400.0000, 
sim time next is 2526000.0000, 
raw observation next is [-2.3, 57.0, 0.0, 0.0, 26.0, 24.78405312619655, 5.587504110488855, 0.0, 1.0, 38.01207164276574], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.57, 0.0, 0.0, 1.0, 0.8262933037423643, 0.055875041104888556, 0.0, 1.0, 0.27151479744832674], 
reward next is 0.7285, 
noisyNet noise sample is [array([-0.10550456], dtype=float32), -0.30611882]. 
=============================================
[2019-04-01 17:05:53,130] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[70.595474]
 [70.62846 ]
 [70.68036 ]
 [70.72969 ]
 [70.8153  ]], R is [[70.59733582]
 [70.62017822]
 [70.64295959]
 [70.66556549]
 [70.68780518]].
[2019-04-01 17:05:54,881] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.8884558e-26], sum to 1.0000
[2019-04-01 17:05:54,882] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1329
[2019-04-01 17:05:54,893] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.533333333333333, 58.0, 0.0, 0.0, 26.0, 25.19434940332718, 7.50626166122404, 0.0, 1.0, 40.96337988640307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2587200.0000, 
sim time next is 2587800.0000, 
raw observation next is [-3.716666666666666, 58.5, 0.0, 0.0, 26.0, 25.11841521961483, 7.360992395325145, 0.0, 1.0, 41.80212427920415], 
processed observation next is [1.0, 0.9565217391304348, 0.3596491228070176, 0.585, 0.0, 0.0, 1.0, 0.8740593170878326, 0.07360992395325144, 0.0, 1.0, 0.29858660199431536], 
reward next is 0.7014, 
noisyNet noise sample is [array([0.19869983], dtype=float32), -0.17860746]. 
=============================================
[2019-04-01 17:05:56,180] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89500, global step 1429360: loss 0.3219
[2019-04-01 17:05:56,181] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89500, global step 1429360: learning rate 0.0010
[2019-04-01 17:05:57,599] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 3.748966e-37
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 17:05:57,599] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2431
[2019-04-01 17:05:57,615] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.9, 80.5, 0.0, 0.0, 26.0, 24.35931268823196, 5.6136494841504, 0.0, 1.0, 42.20088589065929], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2608200.0000, 
sim time next is 2608800.0000, 
raw observation next is [-6.0, 81.33333333333333, 0.0, 0.0, 26.0, 24.37449234557796, 5.573373087785228, 0.0, 1.0, 42.27200323206426], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.8133333333333332, 0.0, 0.0, 1.0, 0.7677846207968513, 0.05573373087785228, 0.0, 1.0, 0.30194288022903043], 
reward next is 0.6981, 
noisyNet noise sample is [array([0.54163337], dtype=float32), -0.57009685]. 
=============================================
[2019-04-01 17:05:57,623] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7131244e-22 2.5398085e-34 3.6142372e-24 1.8797840e-30 9.9200824e-19
 1.0000000e+00 3.4905905e-18], sum to 1.0000
[2019-04-01 17:05:57,624] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3843
[2019-04-01 17:05:57,668] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.800000000000001, 75.66666666666666, 82.33333333333333, 30.33333333333333, 26.0, 25.69810466858792, 7.718570215829243, 1.0, 1.0, 31.1046982273291], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2623800.0000, 
sim time next is 2624400.0000, 
raw observation next is [-6.7, 75.0, 85.0, 45.5, 26.0, 25.72575272148649, 7.568281422012499, 1.0, 1.0, 33.31923199856106], 
processed observation next is [1.0, 0.391304347826087, 0.2770083102493075, 0.75, 0.2833333333333333, 0.05027624309392265, 1.0, 0.9608218173552129, 0.07568281422012499, 1.0, 1.0, 0.23799451427543614], 
reward next is 0.7620, 
noisyNet noise sample is [array([0.8382484], dtype=float32), 1.4311144]. 
=============================================
[2019-04-01 17:05:57,915] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89500, global step 1430059: loss 0.0302
[2019-04-01 17:05:57,916] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89500, global step 1430059: learning rate 0.0010
[2019-04-01 17:06:00,745] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89500, global step 1431196: loss 0.1580
[2019-04-01 17:06:00,748] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89500, global step 1431199: learning rate 0.0010
[2019-04-01 17:06:01,366] A3C_AGENT_WORKER-Thread-13 INFO:Local step 89500, global step 1431484: loss 0.2154
[2019-04-01 17:06:01,367] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 89500, global step 1431485: learning rate 0.0010
[2019-04-01 17:06:01,843] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89500, global step 1431676: loss 0.0271
[2019-04-01 17:06:01,844] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89500, global step 1431676: learning rate 0.0010
[2019-04-01 17:06:01,949] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89500, global step 1431710: loss 0.2180
[2019-04-01 17:06:01,968] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89500, global step 1431710: learning rate 0.0010
[2019-04-01 17:06:02,954] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89500, global step 1432081: loss 0.0198
[2019-04-01 17:06:02,955] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89500, global step 1432081: learning rate 0.0010
[2019-04-01 17:06:03,028] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89500, global step 1432113: loss 0.0104
[2019-04-01 17:06:03,028] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89500, global step 1432113: learning rate 0.0010
[2019-04-01 17:06:03,694] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89500, global step 1432363: loss 0.1136
[2019-04-01 17:06:03,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89500, global step 1432363: learning rate 0.0010
[2019-04-01 17:06:03,812] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89500, global step 1432403: loss 0.1195
[2019-04-01 17:06:03,815] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89500, global step 1432403: learning rate 0.0010
[2019-04-01 17:06:03,818] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89500, global step 1432405: loss 0.0903
[2019-04-01 17:06:03,819] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89500, global step 1432405: learning rate 0.0010
[2019-04-01 17:06:04,336] A3C_AGENT_WORKER-Thread-11 INFO:Local step 89500, global step 1432572: loss 0.0143
[2019-04-01 17:06:04,337] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 89500, global step 1432572: learning rate 0.0010
[2019-04-01 17:06:05,468] A3C_AGENT_WORKER-Thread-12 INFO:Local step 89500, global step 1433032: loss 1.0245
[2019-04-01 17:06:05,469] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 89500, global step 1433032: learning rate 0.0010
[2019-04-01 17:06:05,543] A3C_AGENT_WORKER-Thread-9 INFO:Local step 89500, global step 1433054: loss 1.0443
[2019-04-01 17:06:05,544] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 89500, global step 1433054: learning rate 0.0010
[2019-04-01 17:06:05,860] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89500, global step 1433175: loss 0.5122
[2019-04-01 17:06:05,860] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89500, global step 1433175: learning rate 0.0010
[2019-04-01 17:06:05,868] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89500, global step 1433181: loss 0.6311
[2019-04-01 17:06:05,869] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89500, global step 1433181: learning rate 0.0010
[2019-04-01 17:06:07,511] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:06:07,511] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3204
[2019-04-01 17:06:07,534] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.666666666666666, 26.0, 114.1666666666667, 0.0, 26.0, 25.99930962463529, 8.745493696297117, 1.0, 1.0, 18.62813822773189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2817600.0000, 
sim time next is 2818200.0000, 
raw observation next is [6.833333333333334, 25.0, 110.3333333333333, 0.0, 26.0, 25.51325010994763, 8.167705468586194, 1.0, 1.0, 17.36735922210761], 
processed observation next is [1.0, 0.6086956521739131, 0.651892890120037, 0.25, 0.36777777777777765, 0.0, 1.0, 0.9304643014210902, 0.08167705468586194, 1.0, 1.0, 0.1240525658721972], 
reward next is 0.8759, 
noisyNet noise sample is [array([0.5017596], dtype=float32), 0.74772704]. 
=============================================
[2019-04-01 17:06:11,244] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 2.3957873e-35 0.0000000e+00 9.3566805e-32
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:06:11,244] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4073
[2019-04-01 17:06:11,265] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.37855367158901, 8.518282024039058, 0.0, 1.0, 42.27524135264736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2848800.0000, 
sim time next is 2849400.0000, 
raw observation next is [1.5, 67.0, 0.0, 0.0, 26.0, 25.37852952669915, 8.464501290478516, 0.0, 1.0, 41.69872044695452], 
processed observation next is [1.0, 1.0, 0.5041551246537397, 0.67, 0.0, 0.0, 1.0, 0.9112185038141644, 0.08464501290478516, 0.0, 1.0, 0.2978480031925323], 
reward next is 0.7022, 
noisyNet noise sample is [array([0.34723836], dtype=float32), -1.2109175]. 
=============================================
[2019-04-01 17:06:12,436] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:06:12,438] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5210
[2019-04-01 17:06:12,463] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 68.66666666666667, 0.0, 0.0, 26.0, 25.37269415427325, 8.39855287974582, 0.0, 1.0, 41.34360254167908], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2850000.0000, 
sim time next is 2850600.0000, 
raw observation next is [1.166666666666667, 70.33333333333333, 0.0, 0.0, 26.0, 25.36042509030887, 8.069562848542823, 0.0, 1.0, 50.09974066164497], 
processed observation next is [1.0, 1.0, 0.49492151431209613, 0.7033333333333333, 0.0, 0.0, 1.0, 0.9086321557584098, 0.08069562848542823, 0.0, 1.0, 0.3578552904403212], 
reward next is 0.6421, 
noisyNet noise sample is [array([0.7668592], dtype=float32), 2.4292407]. 
=============================================
[2019-04-01 17:06:13,979] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.6176894e-37
 1.0000000e+00 2.6754543e-38], sum to 1.0000
[2019-04-01 17:06:13,980] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9010
[2019-04-01 17:06:14,017] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 80.33333333333333, 0.0, 0.0, 26.0, 25.18374209747579, 8.844445251337675, 1.0, 1.0, 22.63068390229292], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2922600.0000, 
sim time next is 2923200.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.06175005975832, 8.455014321506079, 0.0, 1.0, 21.4402455785264], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.78, 0.0, 0.0, 1.0, 0.8659642942511887, 0.08455014321506078, 0.0, 1.0, 0.15314461127518858], 
reward next is 0.8469, 
noisyNet noise sample is [array([0.91995394], dtype=float32), -0.32168087]. 
=============================================
[2019-04-01 17:06:15,259] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90000, global step 1437353: loss 1.7299
[2019-04-01 17:06:15,259] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90000, global step 1437353: learning rate 0.0010
[2019-04-01 17:06:17,095] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90000, global step 1438160: loss 0.8964
[2019-04-01 17:06:17,096] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90000, global step 1438160: learning rate 0.0010
[2019-04-01 17:06:18,351] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:06:18,353] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2229
[2019-04-01 17:06:18,366] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 85.0, 0.0, 0.0, 26.0, 25.17228176883246, 10.06159036214835, 0.0, 1.0, 44.50159914757941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2926800.0000, 
sim time next is 2927400.0000, 
raw observation next is [-1.0, 83.83333333333334, 0.0, 0.0, 26.0, 25.22984214975126, 10.15244763738602, 0.0, 1.0, 44.18488812245566], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.8383333333333334, 0.0, 0.0, 1.0, 0.8899774499644657, 0.1015244763738602, 0.0, 1.0, 0.31560634373182617], 
reward next is 0.6844, 
noisyNet noise sample is [array([0.41499445], dtype=float32), 0.54399055]. 
=============================================
[2019-04-01 17:06:19,751] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90000, global step 1439221: loss 0.4406
[2019-04-01 17:06:19,752] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90000, global step 1439221: learning rate 0.0010
[2019-04-01 17:06:20,163] A3C_AGENT_WORKER-Thread-13 INFO:Local step 90000, global step 1439416: loss 1.2680
[2019-04-01 17:06:20,164] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 90000, global step 1439416: learning rate 0.0010
[2019-04-01 17:06:20,201] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0131011e-38 0.0000000e+00 1.3911982e-31 0.0000000e+00 0.0000000e+00
 1.0000000e+00 8.0662973e-33], sum to 1.0000
[2019-04-01 17:06:20,203] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9721
[2019-04-01 17:06:20,215] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.25891430566864, 5.501487851690196, 0.0, 1.0, 38.96065428152622], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3034800.0000, 
sim time next is 3035400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.22395888626292, 5.484179146352538, 0.0, 1.0, 39.10060519476693], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.7462798408947029, 0.054841791463525386, 0.0, 1.0, 0.27929003710547806], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.52098477], dtype=float32), -0.21830466]. 
=============================================
[2019-04-01 17:06:20,503] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90000, global step 1439570: loss 0.8176
[2019-04-01 17:06:20,504] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90000, global step 1439571: learning rate 0.0010
[2019-04-01 17:06:20,872] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90000, global step 1439732: loss 0.9640
[2019-04-01 17:06:20,873] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90000, global step 1439732: learning rate 0.0010
[2019-04-01 17:06:21,720] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 5.001374e-26], sum to 1.0000
[2019-04-01 17:06:21,729] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3618
[2019-04-01 17:06:21,801] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90000, global step 1440100: loss 0.5324
[2019-04-01 17:06:21,806] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90000, global step 1440100: learning rate 0.0010
[2019-04-01 17:06:21,815] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 62.33333333333334, 69.33333333333334, 310.8333333333334, 26.0, 24.1308431487802, 6.617670707790381, 0.0, 1.0, 90.50288568809205], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3054000.0000, 
sim time next is 3054600.0000, 
raw observation next is [-6.0, 61.5, 83.0, 359.0, 26.0, 25.05188244283184, 7.886272596295792, 0.0, 1.0, 58.12801201781097], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.615, 0.27666666666666667, 0.3966850828729282, 1.0, 0.8645546346902628, 0.07886272596295792, 0.0, 1.0, 0.4152000858415069], 
reward next is 0.5848, 
noisyNet noise sample is [array([-0.6365753], dtype=float32), -0.7120045]. 
=============================================
[2019-04-01 17:06:22,094] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90000, global step 1440228: loss 0.6341
[2019-04-01 17:06:22,097] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90000, global step 1440228: learning rate 0.0010
[2019-04-01 17:06:22,154] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90000, global step 1440251: loss 0.4180
[2019-04-01 17:06:22,170] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90000, global step 1440254: learning rate 0.0010
[2019-04-01 17:06:22,456] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90000, global step 1440397: loss 0.6022
[2019-04-01 17:06:22,456] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90000, global step 1440397: learning rate 0.0010
[2019-04-01 17:06:22,723] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90000, global step 1440534: loss 0.7687
[2019-04-01 17:06:22,724] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90000, global step 1440534: learning rate 0.0010
[2019-04-01 17:06:22,942] A3C_AGENT_WORKER-Thread-11 INFO:Local step 90000, global step 1440648: loss 0.6476
[2019-04-01 17:06:22,943] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 90000, global step 1440649: learning rate 0.0010
[2019-04-01 17:06:24,365] A3C_AGENT_WORKER-Thread-9 INFO:Local step 90000, global step 1441317: loss 0.3279
[2019-04-01 17:06:24,366] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 90000, global step 1441317: learning rate 0.0010
[2019-04-01 17:06:24,472] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90000, global step 1441370: loss 0.2100
[2019-04-01 17:06:24,473] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90000, global step 1441370: learning rate 0.0010
[2019-04-01 17:06:24,522] A3C_AGENT_WORKER-Thread-12 INFO:Local step 90000, global step 1441391: loss 0.2772
[2019-04-01 17:06:24,522] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 90000, global step 1441391: learning rate 0.0010
[2019-04-01 17:06:24,736] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90000, global step 1441491: loss 0.1906
[2019-04-01 17:06:24,737] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90000, global step 1441491: learning rate 0.0010
[2019-04-01 17:06:25,696] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.3272231e-37], sum to 1.0000
[2019-04-01 17:06:25,708] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6342
[2019-04-01 17:06:25,721] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 26.0, 25.40704126583783, 7.685884076396192, 0.0, 1.0, 38.29039955580002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3114000.0000, 
sim time next is 3114600.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 26.0, 25.47670740986487, 7.809559731809237, 0.0, 1.0, 36.86903659820997], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 1.0, 0.0, 0.0, 1.0, 0.9252439156949812, 0.07809559731809237, 0.0, 1.0, 0.26335026141578555], 
reward next is 0.7366, 
noisyNet noise sample is [array([0.72485155], dtype=float32), 1.2213105]. 
=============================================
[2019-04-01 17:06:28,323] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:06:28,324] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7350
[2019-04-01 17:06:28,336] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.166666666666667, 100.0, 0.0, 0.0, 26.0, 26.08303261529434, 17.18390699230863, 0.0, 1.0, 22.35940507957803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3189000.0000, 
sim time next is 3189600.0000, 
raw observation next is [2.0, 100.0, 0.0, 0.0, 26.0, 26.10532270712946, 16.80678894187099, 0.0, 1.0, 21.19687962266588], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 1.0, 0.0, 0.0, 1.0, 1.0150461010184944, 0.1680678894187099, 0.0, 1.0, 0.151406283019042], 
reward next is 0.8486, 
noisyNet noise sample is [array([1.3234925], dtype=float32), -0.44783726]. 
=============================================
[2019-04-01 17:06:30,604] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90500, global step 1444793: loss 2.9056
[2019-04-01 17:06:30,605] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90500, global step 1444793: learning rate 0.0010
[2019-04-01 17:06:32,384] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90500, global step 1445759: loss 0.0066
[2019-04-01 17:06:32,385] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90500, global step 1445759: learning rate 0.0010
[2019-04-01 17:06:33,471] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8720926e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.5083915e-23
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:06:33,473] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0687
[2019-04-01 17:06:33,481] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.133333333333333, 74.0, 113.3333333333333, 813.0, 26.0, 26.47001457417421, 16.26038730401768, 1.0, 1.0, 7.278406741869941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3238800.0000, 
sim time next is 3239400.0000, 
raw observation next is [-2.066666666666666, 72.5, 113.6666666666667, 815.0, 26.0, 26.4789567498645, 16.40283769161737, 1.0, 1.0, 7.022534339012283], 
processed observation next is [1.0, 0.4782608695652174, 0.40535549399815335, 0.725, 0.378888888888889, 0.9005524861878453, 1.0, 1.0684223928377858, 0.16402837691617372, 1.0, 1.0, 0.050160959564373456], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5391497], dtype=float32), -1.0206465]. 
=============================================
[2019-04-01 17:06:35,274] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90500, global step 1447171: loss 3.4820
[2019-04-01 17:06:35,278] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90500, global step 1447173: learning rate 0.0010
[2019-04-01 17:06:35,777] A3C_AGENT_WORKER-Thread-13 INFO:Local step 90500, global step 1447457: loss 1.8787
[2019-04-01 17:06:35,779] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 90500, global step 1447457: learning rate 0.0010
[2019-04-01 17:06:36,127] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90500, global step 1447633: loss 3.5460
[2019-04-01 17:06:36,129] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90500, global step 1447633: learning rate 0.0010
[2019-04-01 17:06:36,609] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90500, global step 1447839: loss 4.7325
[2019-04-01 17:06:36,611] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90500, global step 1447840: learning rate 0.0010
[2019-04-01 17:06:37,045] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90500, global step 1448029: loss 3.4027
[2019-04-01 17:06:37,049] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90500, global step 1448030: learning rate 0.0010
[2019-04-01 17:06:37,255] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90500, global step 1448132: loss 2.5936
[2019-04-01 17:06:37,256] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90500, global step 1448133: learning rate 0.0010
[2019-04-01 17:06:37,331] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90500, global step 1448168: loss 4.3046
[2019-04-01 17:06:37,332] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90500, global step 1448168: learning rate 0.0010
[2019-04-01 17:06:37,811] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.2595751e-38 0.0000000e+00 7.0692402e-38 0.0000000e+00 0.0000000e+00
 1.0000000e+00 5.4418924e-36], sum to 1.0000
[2019-04-01 17:06:37,813] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9891
[2019-04-01 17:06:37,842] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.833333333333334, 75.83333333333334, 0.0, 0.0, 26.0, 24.86243420152334, 7.803790592323703, 0.0, 1.0, 43.27167132594884], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3289800.0000, 
sim time next is 3290400.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.87872486528951, 7.842550804815971, 0.0, 1.0, 43.23930025591445], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 1.0, 0.8398178378985014, 0.07842550804815972, 0.0, 1.0, 0.3088521446851032], 
reward next is 0.6911, 
noisyNet noise sample is [array([1.5560441], dtype=float32), -1.2664807]. 
=============================================
[2019-04-01 17:06:38,196] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90500, global step 1448624: loss 4.6277
[2019-04-01 17:06:38,197] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90500, global step 1448624: learning rate 0.0010
[2019-04-01 17:06:38,484] A3C_AGENT_WORKER-Thread-11 INFO:Local step 90500, global step 1448804: loss 18.2922
[2019-04-01 17:06:38,486] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 90500, global step 1448804: learning rate 0.0010
[2019-04-01 17:06:38,525] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90500, global step 1448826: loss 4.0555
[2019-04-01 17:06:38,525] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90500, global step 1448826: learning rate 0.0010
[2019-04-01 17:06:39,408] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.4405068e-15 2.3629337e-25 7.2876269e-14 2.3125173e-23 2.4203568e-15
 1.0000000e+00 3.8024451e-14], sum to 1.0000
[2019-04-01 17:06:39,410] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7337
[2019-04-01 17:06:39,465] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.33333333333333, 81.66666666666667, 72.0, 345.6666666666667, 26.0, 25.61405730903171, 9.692383706813692, 1.0, 1.0, 42.25949464020469], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3313200.0000, 
sim time next is 3313800.0000, 
raw observation next is [-10.0, 80.5, 86.0, 396.0, 26.0, 25.77169853566843, 9.998303593238832, 1.0, 1.0, 38.36857638312775], 
processed observation next is [1.0, 0.34782608695652173, 0.18559556786703602, 0.805, 0.2866666666666667, 0.4375690607734807, 1.0, 0.9673855050954902, 0.09998303593238832, 1.0, 1.0, 0.27406125987948393], 
reward next is 0.7259, 
noisyNet noise sample is [array([-0.50390893], dtype=float32), 0.34903187]. 
=============================================
[2019-04-01 17:06:39,921] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90500, global step 1449477: loss 7.2047
[2019-04-01 17:06:39,922] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90500, global step 1449477: learning rate 0.0010
[2019-04-01 17:06:39,985] A3C_AGENT_WORKER-Thread-9 INFO:Local step 90500, global step 1449514: loss 112.2768
[2019-04-01 17:06:39,999] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 90500, global step 1449521: learning rate 0.0010
[2019-04-01 17:06:40,237] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90500, global step 1449651: loss 3.2676
[2019-04-01 17:06:40,238] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90500, global step 1449651: learning rate 0.0010
[2019-04-01 17:06:40,251] A3C_AGENT_WORKER-Thread-12 INFO:Local step 90500, global step 1449658: loss 4.9345
[2019-04-01 17:06:40,252] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 90500, global step 1449658: learning rate 0.0010
[2019-04-01 17:06:41,318] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:06:41,319] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0387
[2019-04-01 17:06:41,344] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 50.5, 106.3333333333333, 785.3333333333334, 26.0, 26.72263685548031, 15.5870121319426, 1.0, 1.0, 15.32063289792757], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3420600.0000, 
sim time next is 3421200.0000, 
raw observation next is [3.0, 52.0, 104.6666666666667, 780.1666666666667, 26.0, 26.79949811511782, 13.3752752810418, 1.0, 1.0, 15.26974652736875], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.52, 0.348888888888889, 0.8620626151012892, 1.0, 1.114214016445403, 0.13375275281041799, 1.0, 1.0, 0.10906961805263392], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4029124], dtype=float32), 1.6320646]. 
=============================================
[2019-04-01 17:06:43,282] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.1665464e-37
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:06:43,286] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7995
[2019-04-01 17:06:43,306] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 116.0, 810.5, 26.0, 26.43110057991577, 13.80930932182158, 1.0, 1.0, 13.45464793083149], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3412800.0000, 
sim time next is 3413400.0000, 
raw observation next is [3.0, 45.66666666666666, 116.3333333333333, 812.6666666666667, 26.0, 26.61814256234494, 14.18992466754173, 1.0, 1.0, 13.93275037051427], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.45666666666666655, 0.38777777777777767, 0.897974217311234, 1.0, 1.0883060803349913, 0.1418992466754173, 1.0, 1.0, 0.09951964550367336], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0220622], dtype=float32), 1.6022488]. 
=============================================
[2019-04-01 17:06:45,544] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91000, global step 1452527: loss 0.1315
[2019-04-01 17:06:45,549] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91000, global step 1452527: learning rate 0.0010
[2019-04-01 17:06:47,436] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91000, global step 1453644: loss 0.2572
[2019-04-01 17:06:47,437] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91000, global step 1453644: learning rate 0.0010
[2019-04-01 17:06:48,216] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1606250e-27 6.9181516e-34 1.5679728e-22 1.2955229e-31 2.2502882e-22
 1.0000000e+00 4.0693025e-25], sum to 1.0000
[2019-04-01 17:06:48,217] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9681
[2019-04-01 17:06:48,232] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.833333333333334, 66.0, 0.0, 0.0, 26.0, 24.87890903220439, 7.175758015759662, 0.0, 1.0, 40.35419405837252], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3559800.0000, 
sim time next is 3560400.0000, 
raw observation next is [-5.0, 65.0, 0.0, 0.0, 26.0, 24.83177516293782, 7.034486234601796, 0.0, 1.0, 40.36696585669159], 
processed observation next is [0.0, 0.21739130434782608, 0.32409972299168976, 0.65, 0.0, 0.0, 1.0, 0.8331107375625457, 0.07034486234601796, 0.0, 1.0, 0.28833547040493995], 
reward next is 0.7117, 
noisyNet noise sample is [array([1.6378446], dtype=float32), 1.2474837]. 
=============================================
[2019-04-01 17:06:49,277] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8393429e-29 3.0052953e-38 9.5489727e-26 1.9354682e-33 1.2823054e-33
 1.0000000e+00 2.7625740e-15], sum to 1.0000
[2019-04-01 17:06:49,280] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1146
[2019-04-01 17:06:49,291] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 67.0, 0.0, 0.0, 26.0, 25.37958652854865, 7.434032293565629, 0.0, 1.0, 32.24248664695289], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3721200.0000, 
sim time next is 3721800.0000, 
raw observation next is [-3.0, 66.0, 0.0, 0.0, 26.0, 25.25211369694625, 7.392807383705829, 0.0, 1.0, 39.35731648937368], 
processed observation next is [1.0, 0.043478260869565216, 0.3795013850415513, 0.66, 0.0, 0.0, 1.0, 0.89315909956375, 0.0739280738370583, 0.0, 1.0, 0.281123689209812], 
reward next is 0.7189, 
noisyNet noise sample is [array([0.59440154], dtype=float32), 0.39958742]. 
=============================================
[2019-04-01 17:06:50,335] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91000, global step 1455113: loss 0.2761
[2019-04-01 17:06:50,336] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91000, global step 1455113: learning rate 0.0010
[2019-04-01 17:06:50,686] A3C_AGENT_WORKER-Thread-13 INFO:Local step 91000, global step 1455294: loss 0.1423
[2019-04-01 17:06:50,687] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 91000, global step 1455294: learning rate 0.0010
[2019-04-01 17:06:51,497] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91000, global step 1455712: loss 0.1904
[2019-04-01 17:06:51,498] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91000, global step 1455713: learning rate 0.0010
[2019-04-01 17:06:51,766] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91000, global step 1455866: loss 0.2926
[2019-04-01 17:06:51,767] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91000, global step 1455866: learning rate 0.0010
[2019-04-01 17:06:52,045] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91000, global step 1456024: loss 0.2015
[2019-04-01 17:06:52,049] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91000, global step 1456024: learning rate 0.0010
[2019-04-01 17:06:52,521] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91000, global step 1456256: loss 0.1903
[2019-04-01 17:06:52,531] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91000, global step 1456259: learning rate 0.0010
[2019-04-01 17:06:52,627] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91000, global step 1456313: loss 0.0069
[2019-04-01 17:06:52,628] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91000, global step 1456313: learning rate 0.0010
[2019-04-01 17:06:53,015] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91000, global step 1456514: loss 0.1663
[2019-04-01 17:06:53,016] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91000, global step 1456514: learning rate 0.0010
[2019-04-01 17:06:53,824] A3C_AGENT_WORKER-Thread-11 INFO:Local step 91000, global step 1456965: loss 0.3012
[2019-04-01 17:06:53,826] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 91000, global step 1456965: learning rate 0.0010
[2019-04-01 17:06:54,219] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91000, global step 1457221: loss 0.2481
[2019-04-01 17:06:54,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91000, global step 1457222: learning rate 0.0010
[2019-04-01 17:06:55,180] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91000, global step 1457779: loss 0.0089
[2019-04-01 17:06:55,188] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91000, global step 1457779: learning rate 0.0010
[2019-04-01 17:06:55,331] A3C_AGENT_WORKER-Thread-9 INFO:Local step 91000, global step 1457873: loss 0.0358
[2019-04-01 17:06:55,332] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 91000, global step 1457873: learning rate 0.0010
[2019-04-01 17:06:55,507] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91000, global step 1457986: loss 0.0150
[2019-04-01 17:06:55,509] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91000, global step 1457986: learning rate 0.0010
[2019-04-01 17:06:55,656] A3C_AGENT_WORKER-Thread-12 INFO:Local step 91000, global step 1458072: loss 0.0236
[2019-04-01 17:06:55,660] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 91000, global step 1458072: learning rate 0.0010
[2019-04-01 17:07:00,131] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91500, global step 1460615: loss 21.8729
[2019-04-01 17:07:00,131] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91500, global step 1460615: learning rate 0.0010
[2019-04-01 17:07:02,028] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91500, global step 1461554: loss 13.2034
[2019-04-01 17:07:02,029] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91500, global step 1461554: learning rate 0.0010
[2019-04-01 17:07:03,337] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 9.772345e-37
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 17:07:03,338] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4692
[2019-04-01 17:07:03,352] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.26524764359304, 9.241022153248418, 0.0, 1.0, 43.32084720862341], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3798000.0000, 
sim time next is 3798600.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.2677291453844, 9.184606757926575, 0.0, 1.0, 43.13072234851857], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.71, 0.0, 0.0, 1.0, 0.8953898779120573, 0.09184606757926575, 0.0, 1.0, 0.30807658820370404], 
reward next is 0.6919, 
noisyNet noise sample is [array([0.00433745], dtype=float32), 0.63260305]. 
=============================================
[2019-04-01 17:07:03,527] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5472409e-31 0.0000000e+00 4.0345258e-32 0.0000000e+00 1.5499939e-31
 1.0000000e+00 8.6555420e-29], sum to 1.0000
[2019-04-01 17:07:03,532] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9739
[2019-04-01 17:07:03,546] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.18662482168977, 7.634846783297796, 0.0, 1.0, 40.71619756525885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3907200.0000, 
sim time next is 3907800.0000, 
raw observation next is [-5.0, 68.0, 0.0, 0.0, 26.0, 25.12565063144937, 7.402788182001477, 0.0, 1.0, 40.88915220534231], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.68, 0.0, 0.0, 1.0, 0.8750929473499102, 0.07402788182001477, 0.0, 1.0, 0.2920653728953022], 
reward next is 0.7079, 
noisyNet noise sample is [array([-0.21814759], dtype=float32), -1.1625178]. 
=============================================
[2019-04-01 17:07:04,903] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91500, global step 1463037: loss 2.6553
[2019-04-01 17:07:04,908] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91500, global step 1463038: learning rate 0.0010
[2019-04-01 17:07:05,398] A3C_AGENT_WORKER-Thread-13 INFO:Local step 91500, global step 1463254: loss 5.4697
[2019-04-01 17:07:05,399] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 91500, global step 1463254: learning rate 0.0010
[2019-04-01 17:07:05,994] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91500, global step 1463529: loss 10.0109
[2019-04-01 17:07:05,996] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91500, global step 1463529: learning rate 0.0010
[2019-04-01 17:07:06,385] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91500, global step 1463713: loss 4.0113
[2019-04-01 17:07:06,386] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91500, global step 1463713: learning rate 0.0010
[2019-04-01 17:07:06,951] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91500, global step 1463979: loss 0.0377
[2019-04-01 17:07:06,954] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91500, global step 1463980: learning rate 0.0010
[2019-04-01 17:07:06,990] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91500, global step 1463998: loss 3.3053
[2019-04-01 17:07:06,991] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91500, global step 1463998: learning rate 0.0010
[2019-04-01 17:07:07,497] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91500, global step 1464223: loss 0.0284
[2019-04-01 17:07:07,498] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91500, global step 1464223: learning rate 0.0010
[2019-04-01 17:07:07,655] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91500, global step 1464298: loss 0.8955
[2019-04-01 17:07:07,657] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91500, global step 1464298: learning rate 0.0010
[2019-04-01 17:07:08,085] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.3762443e-18
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:07:08,085] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1389
[2019-04-01 17:07:08,113] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.166666666666667, 38.0, 99.33333333333334, 766.6666666666666, 26.0, 26.68048516589993, 17.55013155670962, 1.0, 1.0, 13.70802419098474], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3941400.0000, 
sim time next is 3942000.0000, 
raw observation next is [-4.0, 38.0, 96.5, 756.0, 26.0, 26.86214258427072, 18.25912550579529, 1.0, 1.0, 12.72258087659835], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.38, 0.32166666666666666, 0.8353591160220994, 1.0, 1.1231632263243887, 0.1825912550579529, 1.0, 1.0, 0.09087557768998822], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.03688464], dtype=float32), 0.11117511]. 
=============================================
[2019-04-01 17:07:08,120] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[37.735714]
 [37.372982]
 [37.03888 ]
 [36.670544]
 [36.565136]], R is [[37.74786377]
 [37.37038422]
 [36.99668121]
 [36.62671661]
 [36.26044846]].
[2019-04-01 17:07:08,408] A3C_AGENT_WORKER-Thread-11 INFO:Local step 91500, global step 1464650: loss 0.1537
[2019-04-01 17:07:08,408] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 91500, global step 1464650: learning rate 0.0010
[2019-04-01 17:07:08,737] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91500, global step 1464820: loss 0.0702
[2019-04-01 17:07:08,740] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91500, global step 1464823: learning rate 0.0010
[2019-04-01 17:07:09,991] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91500, global step 1465318: loss 0.0106
[2019-04-01 17:07:09,992] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91500, global step 1465319: learning rate 0.0010
[2019-04-01 17:07:10,037] A3C_AGENT_WORKER-Thread-12 INFO:Local step 91500, global step 1465341: loss 1.3214
[2019-04-01 17:07:10,054] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 91500, global step 1465347: learning rate 0.0010
[2019-04-01 17:07:10,089] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91500, global step 1465367: loss 4.4952
[2019-04-01 17:07:10,095] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91500, global step 1465367: learning rate 0.0010
[2019-04-01 17:07:10,740] A3C_AGENT_WORKER-Thread-9 INFO:Local step 91500, global step 1465658: loss 0.1086
[2019-04-01 17:07:10,740] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 91500, global step 1465658: learning rate 0.0010
[2019-04-01 17:07:12,363] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:07:12,363] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7336
[2019-04-01 17:07:12,379] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.166666666666667, 45.66666666666666, 0.0, 0.0, 26.0, 25.51975161212188, 10.70661930610861, 0.0, 1.0, 33.7636536005432], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3964200.0000, 
sim time next is 3964800.0000, 
raw observation next is [-7.333333333333334, 46.33333333333334, 0.0, 0.0, 26.0, 25.47220150736284, 10.50144950948576, 0.0, 1.0, 35.34359701461693], 
processed observation next is [1.0, 0.9130434782608695, 0.2594644506001847, 0.46333333333333343, 0.0, 0.0, 1.0, 0.9246002153375485, 0.1050144950948576, 0.0, 1.0, 0.2524542643901209], 
reward next is 0.7475, 
noisyNet noise sample is [array([-0.577242], dtype=float32), -0.58062667]. 
=============================================
[2019-04-01 17:07:16,479] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92000, global step 1468439: loss 0.0901
[2019-04-01 17:07:16,488] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92000, global step 1468440: learning rate 0.0010
[2019-04-01 17:07:17,919] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92000, global step 1469265: loss 0.0703
[2019-04-01 17:07:17,921] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92000, global step 1469266: learning rate 0.0010
[2019-04-01 17:07:18,136] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.4451432e-32 0.0000000e+00 2.0772222e-26 4.2417366e-38 4.7997782e-29
 1.0000000e+00 1.4049006e-21], sum to 1.0000
[2019-04-01 17:07:18,136] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6316
[2019-04-01 17:07:18,153] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.10369786882341, 7.549452659166327, 0.0, 1.0, 40.31787670620795], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4066200.0000, 
sim time next is 4066800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.16756814170134, 7.509115817682951, 0.0, 1.0, 40.3165019389754], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 1.0, 0.8810811631001917, 0.07509115817682951, 0.0, 1.0, 0.28797501384982427], 
reward next is 0.7120, 
noisyNet noise sample is [array([-0.5787368], dtype=float32), 0.24190126]. 
=============================================
[2019-04-01 17:07:20,957] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92000, global step 1470972: loss 0.3249
[2019-04-01 17:07:20,958] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92000, global step 1470972: learning rate 0.0010
[2019-04-01 17:07:21,419] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 6.9292149e-35 0.0000000e+00 0.0000000e+00
 1.0000000e+00 5.2552976e-23], sum to 1.0000
[2019-04-01 17:07:21,421] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6811
[2019-04-01 17:07:21,432] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.612548583262, 8.560478973445969, 0.0, 1.0, 25.40190490342103], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4221600.0000, 
sim time next is 4222200.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.55917551036561, 8.32762791544865, 0.0, 1.0, 24.1035005973469], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 1.0, 0.9370250729093728, 0.0832762791544865, 0.0, 1.0, 0.17216786140962073], 
reward next is 0.8278, 
noisyNet noise sample is [array([1.1544087], dtype=float32), -0.9131005]. 
=============================================
[2019-04-01 17:07:21,521] A3C_AGENT_WORKER-Thread-13 INFO:Local step 92000, global step 1471294: loss 0.1840
[2019-04-01 17:07:21,521] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 92000, global step 1471294: learning rate 0.0010
[2019-04-01 17:07:21,906] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92000, global step 1471523: loss 0.2616
[2019-04-01 17:07:21,907] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92000, global step 1471523: learning rate 0.0010
[2019-04-01 17:07:22,285] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92000, global step 1471762: loss 0.1397
[2019-04-01 17:07:22,288] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92000, global step 1471762: learning rate 0.0010
[2019-04-01 17:07:22,811] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92000, global step 1472075: loss 0.0078
[2019-04-01 17:07:22,812] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92000, global step 1472075: learning rate 0.0010
[2019-04-01 17:07:22,823] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92000, global step 1472082: loss 0.0039
[2019-04-01 17:07:22,825] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92000, global step 1472083: learning rate 0.0010
[2019-04-01 17:07:22,990] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92000, global step 1472179: loss 0.0068
[2019-04-01 17:07:22,994] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92000, global step 1472180: learning rate 0.0010
[2019-04-01 17:07:23,307] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 2.960570e-35 0.000000e+00 4.404217e-31
 1.000000e+00 1.350784e-24], sum to 1.0000
[2019-04-01 17:07:23,307] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4729
[2019-04-01 17:07:23,337] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.266666666666667, 42.33333333333334, 0.0, 0.0, 26.0, 24.99603747151163, 7.675735660507823, 0.0, 1.0, 46.96740462350697], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4216800.0000, 
sim time next is 4217400.0000, 
raw observation next is [1.2, 42.5, 0.0, 0.0, 26.0, 25.07353425162805, 8.225565529312254, 0.0, 1.0, 50.5004555451895], 
processed observation next is [0.0, 0.8260869565217391, 0.4958448753462604, 0.425, 0.0, 0.0, 1.0, 0.8676477502325787, 0.08225565529312254, 0.0, 1.0, 0.36071753960849645], 
reward next is 0.6393, 
noisyNet noise sample is [array([0.772934], dtype=float32), 2.452554]. 
=============================================
[2019-04-01 17:07:23,655] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92000, global step 1472517: loss 0.0593
[2019-04-01 17:07:23,656] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92000, global step 1472517: learning rate 0.0010
[2019-04-01 17:07:24,140] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92000, global step 1472777: loss 0.0023
[2019-04-01 17:07:24,142] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92000, global step 1472777: learning rate 0.0010
[2019-04-01 17:07:24,223] A3C_AGENT_WORKER-Thread-11 INFO:Local step 92000, global step 1472824: loss 0.0118
[2019-04-01 17:07:24,226] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 92000, global step 1472824: learning rate 0.0010
[2019-04-01 17:07:25,744] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92000, global step 1473700: loss 0.0562
[2019-04-01 17:07:25,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92000, global step 1473700: learning rate 0.0010
[2019-04-01 17:07:25,764] A3C_AGENT_WORKER-Thread-12 INFO:Local step 92000, global step 1473708: loss 0.0905
[2019-04-01 17:07:25,768] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 92000, global step 1473711: learning rate 0.0010
[2019-04-01 17:07:25,914] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92000, global step 1473811: loss 0.0168
[2019-04-01 17:07:25,915] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92000, global step 1473811: learning rate 0.0010
[2019-04-01 17:07:26,812] A3C_AGENT_WORKER-Thread-9 INFO:Local step 92000, global step 1474331: loss 0.2828
[2019-04-01 17:07:26,815] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 92000, global step 1474333: learning rate 0.0010
[2019-04-01 17:07:27,447] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:07:27,448] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4438
[2019-04-01 17:07:27,462] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333334, 90.0, 117.6666666666667, 0.9999999999999998, 26.0, 26.39730526161047, 13.24042340669505, 1.0, 1.0, 10.49371903808186], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4452000.0000, 
sim time next is 4452600.0000, 
raw observation next is [0.1666666666666666, 91.0, 133.3333333333333, 2.0, 26.0, 26.28768527295176, 13.20237609782441, 1.0, 1.0, 10.53818991508926], 
processed observation next is [1.0, 0.5217391304347826, 0.4672206832871654, 0.91, 0.4444444444444443, 0.0022099447513812156, 1.0, 1.0410978961359656, 0.1320237609782441, 1.0, 1.0, 0.07527278510778043], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.03789972], dtype=float32), -0.303849]. 
=============================================
[2019-04-01 17:07:30,354] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92500, global step 1476345: loss 9.3169
[2019-04-01 17:07:30,355] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92500, global step 1476345: learning rate 0.0010
[2019-04-01 17:07:31,878] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92500, global step 1477256: loss 10.7876
[2019-04-01 17:07:31,879] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92500, global step 1477257: learning rate 0.0010
[2019-04-01 17:07:32,655] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.247781e-21
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 17:07:32,656] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6717
[2019-04-01 17:07:32,667] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.5, 31.0, 155.0, 735.5, 26.0, 28.54180604958741, 32.42106283934343, 1.0, 1.0, 1.258516442590876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4370400.0000, 
sim time next is 4371000.0000, 
raw observation next is [14.4, 31.16666666666667, 168.3333333333333, 700.0, 26.0, 28.60658329787118, 33.26721045650761, 1.0, 1.0, 1.203013729504134], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.3116666666666667, 0.561111111111111, 0.7734806629834254, 1.0, 1.3723690425530255, 0.3326721045650761, 1.0, 1.0, 0.008592955210743816], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5895567], dtype=float32), 0.7401956]. 
=============================================
[2019-04-01 17:07:32,681] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[36.324722]
 [35.891945]
 [35.43022 ]
 [35.00878 ]
 [34.80658 ]], R is [[36.34928131]
 [35.98579025]
 [35.62593079]
 [35.26967239]
 [34.91697693]].
[2019-04-01 17:07:33,394] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8823385e-30 1.9409563e-31 6.7666481e-31 3.5263488e-27 3.2435077e-14
 1.0000000e+00 1.8858585e-32], sum to 1.0000
[2019-04-01 17:07:33,395] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2845
[2019-04-01 17:07:33,432] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 159.0, 4.0, 26.0, 25.34032637054274, 12.51627453582377, 1.0, 1.0, 27.70352530545205], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4456200.0000, 
sim time next is 4456800.0000, 
raw observation next is [0.0, 92.0, 140.5, 3.0, 26.0, 25.84840540555779, 13.52499707615137, 1.0, 1.0, 18.64401939930331], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.92, 0.4683333333333333, 0.0033149171270718232, 1.0, 0.9783436293653988, 0.1352499707615137, 1.0, 1.0, 0.13317156713788078], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8591958], dtype=float32), -0.16716293]. 
=============================================
[2019-04-01 17:07:33,493] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.2692735e-17 1.3223030e-16 3.5159661e-24 1.5489515e-12 9.7435874e-01
 2.5641326e-02 1.0611604e-11], sum to 1.0000
[2019-04-01 17:07:33,496] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0933
[2019-04-01 17:07:33,506] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.86666666666667, 28.33333333333334, 118.3333333333333, 848.8333333333334, 26.0, 28.40342970363206, 28.70120873339511, 1.0, 1.0, 1.663384479929136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4364400.0000, 
sim time next is 4365000.0000, 
raw observation next is [14.8, 28.5, 118.0, 853.0, 26.0, 28.37479586861114, 28.43839019837889, 1.0, 1.0, 1.620086094269785], 
processed observation next is [1.0, 0.5217391304347826, 0.8725761772853187, 0.285, 0.3933333333333333, 0.9425414364640884, 1.0, 1.3392565526587343, 0.2843839019837889, 1.0, 1.0, 0.011572043530498463], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.0166314], dtype=float32), -1.3521324]. 
=============================================
[2019-04-01 17:07:33,519] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[33.32673 ]
 [32.742603]
 [32.113914]
 [31.382061]
 [30.653751]], R is [[33.59699249]
 [33.26102448]
 [32.92841339]
 [32.59912872]
 [32.27313614]].
[2019-04-01 17:07:34,757] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4889586e-30 1.2049796e-37 1.1643514e-23 1.3191038e-34 1.1096167e-24
 1.0000000e+00 2.8052669e-21], sum to 1.0000
[2019-04-01 17:07:34,758] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6549
[2019-04-01 17:07:34,779] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.51561268278828, 11.17182034342624, 0.0, 1.0, 38.01440439535855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4428600.0000, 
sim time next is 4429200.0000, 
raw observation next is [2.666666666666667, 72.0, 0.0, 0.0, 26.0, 25.56389904548907, 11.14885735244404, 0.0, 1.0, 35.15616795048642], 
processed observation next is [1.0, 0.2608695652173913, 0.5364727608494922, 0.72, 0.0, 0.0, 1.0, 0.9376998636412957, 0.1114885735244404, 0.0, 1.0, 0.2511154853606173], 
reward next is 0.7489, 
noisyNet noise sample is [array([0.05539492], dtype=float32), -0.039049696]. 
=============================================
[2019-04-01 17:07:34,954] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92500, global step 1478934: loss 9.3367
[2019-04-01 17:07:34,956] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92500, global step 1478934: learning rate 0.0010
[2019-04-01 17:07:35,731] A3C_AGENT_WORKER-Thread-13 INFO:Local step 92500, global step 1479343: loss 9.2793
[2019-04-01 17:07:35,733] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 92500, global step 1479343: learning rate 0.0010
[2019-04-01 17:07:36,257] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92500, global step 1479598: loss 10.6611
[2019-04-01 17:07:36,259] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92500, global step 1479598: learning rate 0.0010
[2019-04-01 17:07:36,676] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92500, global step 1479821: loss 10.9176
[2019-04-01 17:07:36,678] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92500, global step 1479821: learning rate 0.0010
[2019-04-01 17:07:37,249] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.24124729e-31 1.09001757e-36 3.43034282e-19 4.79076367e-35
 0.00000000e+00 1.00000000e+00 1.02559105e-14], sum to 1.0000
[2019-04-01 17:07:37,250] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6940
[2019-04-01 17:07:37,312] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92500, global step 1480141: loss 11.4571
[2019-04-01 17:07:37,317] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92500, global step 1480143: learning rate 0.0010
[2019-04-01 17:07:37,336] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.9666666666666667, 71.33333333333334, 0.0, 0.0, 26.0, 25.27930725899334, 8.999799055729794, 0.0, 1.0, 55.94583347558288], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4518600.0000, 
sim time next is 4519200.0000, 
raw observation next is [-0.9333333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.53561492581434, 9.451505113749677, 1.0, 1.0, 37.49177791094476], 
processed observation next is [1.0, 0.30434782608695654, 0.4367497691597415, 0.7166666666666667, 0.0, 0.0, 1.0, 0.9336592751163343, 0.09451505113749677, 1.0, 1.0, 0.26779841364960544], 
reward next is 0.7322, 
noisyNet noise sample is [array([0.22203553], dtype=float32), -0.9066777]. 
=============================================
[2019-04-01 17:07:37,461] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92500, global step 1480208: loss 9.1550
[2019-04-01 17:07:37,462] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92500, global step 1480208: learning rate 0.0010
[2019-04-01 17:07:37,848] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92500, global step 1480385: loss 8.2489
[2019-04-01 17:07:37,850] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92500, global step 1480385: learning rate 0.0010
[2019-04-01 17:07:38,072] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92500, global step 1480506: loss 8.1375
[2019-04-01 17:07:38,075] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92500, global step 1480507: learning rate 0.0010
[2019-04-01 17:07:38,684] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92500, global step 1480832: loss 5.9752
[2019-04-01 17:07:38,685] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92500, global step 1480833: learning rate 0.0010
[2019-04-01 17:07:38,804] A3C_AGENT_WORKER-Thread-11 INFO:Local step 92500, global step 1480886: loss 4.8447
[2019-04-01 17:07:38,805] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 92500, global step 1480887: learning rate 0.0010
[2019-04-01 17:07:40,167] A3C_AGENT_WORKER-Thread-12 INFO:Local step 92500, global step 1481620: loss 3.3198
[2019-04-01 17:07:40,169] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 92500, global step 1481620: learning rate 0.0010
[2019-04-01 17:07:40,373] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92500, global step 1481717: loss 3.3758
[2019-04-01 17:07:40,376] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92500, global step 1481718: learning rate 0.0010
[2019-04-01 17:07:40,431] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92500, global step 1481750: loss 2.4400
[2019-04-01 17:07:40,433] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92500, global step 1481750: learning rate 0.0010
[2019-04-01 17:07:41,255] A3C_AGENT_WORKER-Thread-9 INFO:Local step 92500, global step 1482201: loss 4.1821
[2019-04-01 17:07:41,255] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 92500, global step 1482201: learning rate 0.0010
[2019-04-01 17:07:42,898] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 5.7986243e-32], sum to 1.0000
[2019-04-01 17:07:42,898] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2237
[2019-04-01 17:07:42,913] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 84.5, 0.0, 0.0, 26.0, 25.45814255836644, 11.01897639721283, 0.0, 1.0, 39.93535770572483], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4743000.0000, 
sim time next is 4743600.0000, 
raw observation next is [-2.666666666666667, 84.33333333333334, 0.0, 0.0, 26.0, 25.47438138258784, 10.88991970455326, 0.0, 1.0, 36.67596132461656], 
processed observation next is [1.0, 0.9130434782608695, 0.38873499538319484, 0.8433333333333334, 0.0, 0.0, 1.0, 0.9249116260839771, 0.1088991970455326, 0.0, 1.0, 0.2619711523186897], 
reward next is 0.7380, 
noisyNet noise sample is [array([-0.34624162], dtype=float32), -1.1446851]. 
=============================================
[2019-04-01 17:07:44,580] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6299124e-28 2.5724893e-22 1.3431751e-15 1.6376994e-25 9.6522009e-21
 1.0000000e+00 1.0029280e-08], sum to 1.0000
[2019-04-01 17:07:44,580] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0080
[2019-04-01 17:07:44,596] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 42.0, 162.0, 795.6666666666667, 26.0, 25.02766650674322, 9.125089856567195, 0.0, 1.0, 17.72015477193791], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4796400.0000, 
sim time next is 4797000.0000, 
raw observation next is [1.5, 41.5, 170.0, 787.0, 26.0, 25.03976914444173, 9.275329623374809, 0.0, 1.0, 15.8411225366761], 
processed observation next is [0.0, 0.5217391304347826, 0.5041551246537397, 0.415, 0.5666666666666667, 0.8696132596685083, 1.0, 0.8628241634916759, 0.09275329623374809, 0.0, 1.0, 0.11315087526197214], 
reward next is 0.8868, 
noisyNet noise sample is [array([-0.6014798], dtype=float32), 0.5711753]. 
=============================================
[2019-04-01 17:07:44,599] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[69.43562]
 [69.35766]
 [69.18796]
 [68.8452 ]
 [68.47299]], R is [[69.69532013]
 [69.87179565]
 [70.05046082]
 [70.2516098 ]
 [70.44708252]].
[2019-04-01 17:07:44,939] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3779981e-17 3.3050262e-21 4.8346631e-14 2.2704523e-21 1.7135449e-11
 9.9998617e-01 1.3857431e-05], sum to 1.0000
[2019-04-01 17:07:44,939] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3182
[2019-04-01 17:07:44,983] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.333333333333333, 67.33333333333334, 157.1666666666667, 452.6666666666667, 26.0, 26.00778669319417, 11.04764378200453, 1.0, 1.0, 14.01938307151872], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4612800.0000, 
sim time next is 4613400.0000, 
raw observation next is [-1.0, 65.5, 164.0, 509.0, 26.0, 26.04464967600364, 11.74371624097603, 1.0, 1.0, 14.10629477870818], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.655, 0.5466666666666666, 0.5624309392265193, 1.0, 1.0063785251433772, 0.11743716240976029, 1.0, 1.0, 0.10075924841934415], 
reward next is 0.2018, 
noisyNet noise sample is [array([1.4664901], dtype=float32), -0.62017035]. 
=============================================
[2019-04-01 17:07:45,589] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6863729e-36 0.0000000e+00 6.0838448e-31 0.0000000e+00 8.7294801e-38
 1.0000000e+00 7.8719047e-17], sum to 1.0000
[2019-04-01 17:07:45,592] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8784
[2019-04-01 17:07:45,622] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 54.5, 0.0, 0.0, 26.0, 25.71555790448484, 12.15035080775855, 0.0, 1.0, 31.34517202525936], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4663800.0000, 
sim time next is 4664400.0000, 
raw observation next is [2.0, 53.66666666666667, 0.0, 0.0, 26.0, 25.76913065173714, 11.96319565525638, 0.0, 1.0, 27.48058617004342], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.5366666666666667, 0.0, 0.0, 1.0, 0.9670186645338771, 0.1196319565525638, 0.0, 1.0, 0.19628990121459586], 
reward next is 0.8037, 
noisyNet noise sample is [array([-1.4284109], dtype=float32), 0.5538576]. 
=============================================
[2019-04-01 17:07:45,806] A3C_AGENT_WORKER-Thread-18 INFO:Local step 93000, global step 1484606: loss 0.0132
[2019-04-01 17:07:45,808] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 93000, global step 1484606: learning rate 0.0010
[2019-04-01 17:07:47,146] A3C_AGENT_WORKER-Thread-17 INFO:Local step 93000, global step 1485316: loss 1.4434
[2019-04-01 17:07:47,146] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 93000, global step 1485316: learning rate 0.0010
[2019-04-01 17:07:47,270] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 3.4833323e-38 0.0000000e+00 0.0000000e+00
 1.0000000e+00 9.8343795e-31], sum to 1.0000
[2019-04-01 17:07:47,272] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9116
[2019-04-01 17:07:47,285] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.83333333333334, 0.0, 0.0, 26.0, 25.78054658143155, 11.46537875864004, 0.0, 1.0, 26.65652331139892], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4665000.0000, 
sim time next is 4665600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.76374807038498, 11.18680739362032, 0.0, 1.0, 25.28611613654139], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.9662497243407115, 0.1118680739362032, 0.0, 1.0, 0.18061511526100993], 
reward next is 0.8194, 
noisyNet noise sample is [array([0.9765381], dtype=float32), 0.17159362]. 
=============================================
[2019-04-01 17:07:50,165] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 5.343284e-26], sum to 1.0000
[2019-04-01 17:07:50,170] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3136
[2019-04-01 17:07:50,199] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 72.33333333333334, 187.8333333333333, 5.0, 26.0, 25.76192447778875, 11.08540948729727, 1.0, 1.0, 21.14579920550468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4718400.0000, 
sim time next is 4719000.0000, 
raw observation next is [1.166666666666667, 72.16666666666666, 179.6666666666667, 4.0, 26.0, 26.01876156004554, 11.35941688605954, 1.0, 1.0, 19.42550173318981], 
processed observation next is [1.0, 0.6086956521739131, 0.49492151431209613, 0.7216666666666666, 0.598888888888889, 0.004419889502762431, 1.0, 1.0026802228636487, 0.1135941688605954, 1.0, 1.0, 0.13875358380849864], 
reward next is 0.3175, 
noisyNet noise sample is [array([-0.74390566], dtype=float32), 0.52811736]. 
=============================================
[2019-04-01 17:07:50,207] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[66.831116]
 [67.083015]
 [67.11284 ]
 [67.71728 ]
 [67.87907 ]], R is [[66.35315704]
 [66.10442352]
 [66.07454681]
 [65.99928284]
 [66.20093536]].
[2019-04-01 17:07:50,560] A3C_AGENT_WORKER-Thread-3 INFO:Local step 93000, global step 1486964: loss 0.1028
[2019-04-01 17:07:50,564] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 93000, global step 1486965: learning rate 0.0010
[2019-04-01 17:07:51,254] A3C_AGENT_WORKER-Thread-13 INFO:Local step 93000, global step 1487269: loss 0.0011
[2019-04-01 17:07:51,255] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 93000, global step 1487269: learning rate 0.0010
[2019-04-01 17:07:51,459] A3C_AGENT_WORKER-Thread-2 INFO:Local step 93000, global step 1487365: loss 0.0717
[2019-04-01 17:07:51,460] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 93000, global step 1487365: learning rate 0.0010
[2019-04-01 17:07:52,356] A3C_AGENT_WORKER-Thread-4 INFO:Local step 93000, global step 1487836: loss -1.2522
[2019-04-01 17:07:52,359] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 93000, global step 1487838: learning rate 0.0010
[2019-04-01 17:07:52,644] A3C_AGENT_WORKER-Thread-14 INFO:Local step 93000, global step 1487990: loss 0.1031
[2019-04-01 17:07:52,647] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 93000, global step 1487991: learning rate 0.0010
[2019-04-01 17:07:53,079] A3C_AGENT_WORKER-Thread-16 INFO:Local step 93000, global step 1488230: loss 0.7640
[2019-04-01 17:07:53,079] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 93000, global step 1488230: learning rate 0.0010
[2019-04-01 17:07:53,271] A3C_AGENT_WORKER-Thread-15 INFO:Local step 93000, global step 1488335: loss 0.1816
[2019-04-01 17:07:53,272] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 93000, global step 1488335: learning rate 0.0010
[2019-04-01 17:07:53,521] A3C_AGENT_WORKER-Thread-5 INFO:Local step 93000, global step 1488457: loss 3.3495
[2019-04-01 17:07:53,522] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 93000, global step 1488457: learning rate 0.0010
[2019-04-01 17:07:53,872] A3C_AGENT_WORKER-Thread-20 INFO:Local step 93000, global step 1488635: loss 3.6906
[2019-04-01 17:07:53,872] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 93000, global step 1488635: learning rate 0.0010
[2019-04-01 17:07:54,528] A3C_AGENT_WORKER-Thread-11 INFO:Local step 93000, global step 1488977: loss 2.9502
[2019-04-01 17:07:54,532] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 93000, global step 1488978: learning rate 0.0010
[2019-04-01 17:07:55,421] A3C_AGENT_WORKER-Thread-12 INFO:Local step 93000, global step 1489488: loss 4.3371
[2019-04-01 17:07:55,425] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 93000, global step 1489488: learning rate 0.0010
[2019-04-01 17:07:55,673] A3C_AGENT_WORKER-Thread-10 INFO:Local step 93000, global step 1489616: loss 5.9207
[2019-04-01 17:07:55,675] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 93000, global step 1489616: learning rate 0.0010
[2019-04-01 17:07:55,894] A3C_AGENT_WORKER-Thread-19 INFO:Local step 93000, global step 1489750: loss 4.6935
[2019-04-01 17:07:55,897] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 93000, global step 1489751: learning rate 0.0010
[2019-04-01 17:07:56,175] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 7.316504e-20], sum to 1.0000
[2019-04-01 17:07:56,179] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2287
[2019-04-01 17:07:56,209] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1666666666666666, 38.5, 0.0, 0.0, 26.0, 25.43110742523684, 7.735514441377551, 0.0, 1.0, 31.98516544695104], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4920600.0000, 
sim time next is 4921200.0000, 
raw observation next is [0.0, 39.0, 0.0, 0.0, 26.0, 25.39963594712715, 7.642131549864589, 0.0, 1.0, 35.0300343519413], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 0.39, 0.0, 0.0, 1.0, 0.9142337067324497, 0.07642131549864589, 0.0, 1.0, 0.250214531085295], 
reward next is 0.7498, 
noisyNet noise sample is [array([-1.1722666], dtype=float32), -1.115372]. 
=============================================
[2019-04-01 17:07:56,527] A3C_AGENT_WORKER-Thread-9 INFO:Local step 93000, global step 1490103: loss 5.3284
[2019-04-01 17:07:56,530] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 93000, global step 1490106: learning rate 0.0010
[2019-04-01 17:07:57,836] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3546579e-38 0.0000000e+00 1.5242015e-30 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.8947028e-28], sum to 1.0000
[2019-04-01 17:07:57,838] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9795
[2019-04-01 17:07:57,856] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 39.33333333333334, 0.0, 0.0, 26.0, 25.45020434075141, 8.70069105474073, 0.0, 1.0, 39.5952694997845], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4911000.0000, 
sim time next is 4911600.0000, 
raw observation next is [1.0, 38.66666666666667, 0.0, 0.0, 26.0, 25.52327586591574, 8.739177511822186, 0.0, 1.0, 32.50140544174755], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.3866666666666667, 0.0, 0.0, 1.0, 0.931896552273677, 0.08739177511822187, 0.0, 1.0, 0.2321528960124825], 
reward next is 0.7678, 
noisyNet noise sample is [array([2.5591712], dtype=float32), 0.52777153]. 
=============================================
[2019-04-01 17:07:59,359] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.5945424e-29 4.1433217e-29 1.6829059e-32 1.9679368e-32 1.3054990e-17
 1.0000000e+00 3.2578993e-32], sum to 1.0000
[2019-04-01 17:07:59,360] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4221
[2019-04-01 17:07:59,371] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.5, 28.33333333333334, 120.3333333333333, 831.0, 26.0, 26.67066716035016, 13.60419659797691, 1.0, 1.0, 6.774442611432311], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4965000.0000, 
sim time next is 4965600.0000, 
raw observation next is [4.0, 27.66666666666667, 121.1666666666667, 838.0, 26.0, 26.74028525425123, 14.05902473568927, 1.0, 1.0, 6.169388801691269], 
processed observation next is [1.0, 0.4782608695652174, 0.5734072022160666, 0.2766666666666667, 0.403888888888889, 0.9259668508287293, 1.0, 1.1057550363216042, 0.1405902473568927, 1.0, 1.0, 0.04406706286922335], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.73043317], dtype=float32), -0.31152716]. 
=============================================
[2019-04-01 17:07:59,628] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:07:59,628] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:07:59,663] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run12
[2019-04-01 17:08:01,012] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:01,013] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:01,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run12
[2019-04-01 17:08:01,312] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.3228275e-17
 1.0000000e+00 1.7303550e-26], sum to 1.0000
[2019-04-01 17:08:01,316] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0003
[2019-04-01 17:08:01,329] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.666666666666667, 25.33333333333333, 28.33333333333334, 253.3333333333333, 26.0, 26.92095669501877, 18.83590260238634, 1.0, 1.0, 1.01020412545855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4988400.0000, 
sim time next is 4989000.0000, 
raw observation next is [6.333333333333333, 25.16666666666667, 22.66666666666667, 202.6666666666667, 26.0, 26.94130543677506, 18.8624362940142, 1.0, 1.0, 4.379943979513675], 
processed observation next is [1.0, 0.7391304347826086, 0.6380424746075716, 0.2516666666666667, 0.07555555555555557, 0.22394106813996323, 1.0, 1.13447220525358, 0.18862436294014198, 1.0, 1.0, 0.031285314139383395], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.33498865], dtype=float32), -0.30745706]. 
=============================================
[2019-04-01 17:08:01,351] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[36.528732]
 [36.091156]
 [35.74536 ]
 [35.150963]
 [34.562695]], R is [[36.55403519]
 [36.18849564]
 [35.82661057]
 [35.46834564]
 [35.11366272]].
[2019-04-01 17:08:03,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:03,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:04,002] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run12
[2019-04-01 17:08:04,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:04,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:04,568] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run12
[2019-04-01 17:08:04,624] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:04,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:04,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run12
[2019-04-01 17:08:05,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:05,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:05,826] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run12
[2019-04-01 17:08:05,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:05,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:05,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run12
[2019-04-01 17:08:06,308] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:06,308] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:06,318] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run12
[2019-04-01 17:08:06,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:06,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:06,472] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run12
[2019-04-01 17:08:06,675] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:06,675] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:06,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run12
[2019-04-01 17:08:06,789] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:06,789] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:06,795] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run12
[2019-04-01 17:08:07,499] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:07,500] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:07,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run12
[2019-04-01 17:08:08,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:08,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:08,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run12
[2019-04-01 17:08:08,914] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:08,914] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:08,918] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run12
[2019-04-01 17:08:09,039] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:09,039] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:09,043] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run12
[2019-04-01 17:08:09,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:08:09,150] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:09,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run12
[2019-04-01 17:08:20,323] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6368476e-38 1.1166490e-36 4.5393310e-27 0.0000000e+00 7.4834544e-37
 1.1288337e-06 9.9999893e-01], sum to 1.0000
[2019-04-01 17:08:20,325] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4703
[2019-04-01 17:08:20,387] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.95, 87.5, 0.0, 0.0, 26.0, 24.54039100821814, 6.944597930643506, 0.0, 1.0, 43.39639432046912], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 63000.0000, 
sim time next is 63600.0000, 
raw observation next is [4.766666666666667, 88.0, 0.0, 0.0, 26.0, 24.58619189830052, 7.022516844801345, 0.0, 1.0, 41.32102990206715], 
processed observation next is [0.0, 0.7391304347826086, 0.5946445060018468, 0.88, 0.0, 0.0, 1.0, 0.7980274140429314, 0.07022516844801345, 0.0, 1.0, 0.2951502135861939], 
reward next is 0.7048, 
noisyNet noise sample is [array([1.7902881], dtype=float32), -1.0217954]. 
=============================================
[2019-04-01 17:08:21,408] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.1778583e-27 0.0000000e+00 0.0000000e+00
 3.3735593e-24 1.0000000e+00], sum to 1.0000
[2019-04-01 17:08:21,415] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7458
[2019-04-01 17:08:21,428] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 24.29889063619081, 5.898019593113379, 0.0, 1.0, 39.60299653754293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 86400.0000, 
sim time next is 87000.0000, 
raw observation next is [-0.09999999999999999, 94.33333333333334, 0.0, 0.0, 26.0, 24.29575581819556, 5.865834616976014, 0.0, 1.0, 39.64467681055822], 
processed observation next is [1.0, 0.0, 0.4598337950138504, 0.9433333333333335, 0.0, 0.0, 1.0, 0.7565365454565084, 0.05865834616976014, 0.0, 1.0, 0.2831762629325587], 
reward next is 0.7168, 
noisyNet noise sample is [array([0.7137128], dtype=float32), -0.035853807]. 
=============================================
[2019-04-01 17:08:21,432] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[58.99041 ]
 [61.36965 ]
 [61.42739 ]
 [61.454315]
 [61.467163]], R is [[57.81832886]
 [57.95726776]
 [58.09491348]
 [58.23129272]
 [58.36640167]].
[2019-04-01 17:08:27,546] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-01 17:08:27,551] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 17:08:27,551] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:27,553] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 17:08:27,554] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:27,554] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 17:08:27,554] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run16
[2019-04-01 17:08:27,555] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:08:28,249] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run16
[2019-04-01 17:08:28,268] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run16
[2019-04-01 17:08:57,085] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.54553246], dtype=float32), -0.5076642]
[2019-04-01 17:08:57,085] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.9, 63.0, 19.5, 101.0, 26.0, 25.6283248089294, 7.755350050840526, 1.0, 1.0, 6.630736712596056]
[2019-04-01 17:08:57,085] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 17:08:57,086] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.6721175e-38 1.4464488e-34 3.5288358e-35 0.0000000e+00 2.8868984e-25
 1.1867497e-04 9.9988127e-01], sampled 0.7264092911052742
[2019-04-01 17:09:03,261] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.54553246], dtype=float32), -0.5076642]
[2019-04-01 17:09:03,261] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [14.4, 77.0, 0.0, 0.0, 26.0, 24.92280794876286, 10.76926691324615, 0.0, 1.0, 60.42858042789277]
[2019-04-01 17:09:03,261] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 17:09:03,262] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.7636966e-38 1.8568530e-38 2.8545864e-35 0.0000000e+00 2.3600772e-30
 3.3102214e-02 9.6689779e-01], sampled 0.9385111790803279
[2019-04-01 17:09:08,897] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.54553246], dtype=float32), -0.5076642]
[2019-04-01 17:09:08,898] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.121551873666667, 98.905830115, 0.0, 0.0, 26.0, 25.27752475306819, 8.620176062843354, 0.0, 1.0, 38.37286040260594]
[2019-04-01 17:09:08,898] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 17:09:08,898] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.8082782e-33 5.0093084e-37 5.1172109e-24 0.0000000e+00 4.8688759e-37
 1.0284658e-12 1.0000000e+00], sampled 0.5621122623316451
[2019-04-01 17:09:27,880] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.54553246], dtype=float32), -0.5076642]
[2019-04-01 17:09:27,881] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-4.4, 76.33333333333334, 0.0, 0.0, 26.0, 24.39330894409672, 6.562348971101311, 0.0, 1.0, 90.11382191878761]
[2019-04-01 17:09:27,881] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 17:09:27,882] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.2735763e-30 3.8374346e-35 4.3284366e-22 1.3170253e-37 1.8707443e-35
 4.4721748e-12 1.0000000e+00], sampled 0.11287771431581506
[2019-04-01 17:10:09,511] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 17:10:28,960] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 17:10:32,115] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 17:10:33,138] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1500000, evaluation results [1500000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 17:10:40,876] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.2560907e-36 0.0000000e+00 1.6536469e-30 0.0000000e+00 0.0000000e+00
 7.8092488e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 17:10:40,877] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2645
[2019-04-01 17:10:40,890] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.41666666666667, 67.5, 0.0, 0.0, 26.0, 23.06920248825834, 5.869205838855429, 0.0, 1.0, 46.8009287862907], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 276600.0000, 
sim time next is 277200.0000, 
raw observation next is [-10.6, 67.0, 0.0, 0.0, 26.0, 23.00284948320962, 5.945321364976469, 0.0, 1.0, 46.92401865021827], 
processed observation next is [1.0, 0.21739130434782608, 0.1689750692520776, 0.67, 0.0, 0.0, 1.0, 0.5718356404585171, 0.05945321364976469, 0.0, 1.0, 0.33517156178727336], 
reward next is 0.6648, 
noisyNet noise sample is [array([0.6862111], dtype=float32), 0.76205426]. 
=============================================
[2019-04-01 17:10:53,358] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2083323e-37 0.0000000e+00 1.2966464e-32 0.0000000e+00 3.5808155e-37
 1.6909513e-03 9.9830902e-01], sum to 1.0000
[2019-04-01 17:10:53,359] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1607
[2019-04-01 17:10:53,409] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 25.0, 125.5, 0.0, 26.0, 25.16297509674453, 6.134201424156238, 1.0, 1.0, 26.38906632439607], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 475200.0000, 
sim time next is 475800.0000, 
raw observation next is [-1.616666666666667, 25.5, 126.6666666666667, 0.0, 26.0, 25.18823470507732, 6.165883462343413, 1.0, 1.0, 25.27396919065769], 
processed observation next is [1.0, 0.5217391304347826, 0.4178208679593721, 0.255, 0.42222222222222233, 0.0, 1.0, 0.88403352929676, 0.06165883462343413, 1.0, 1.0, 0.18052835136184064], 
reward next is 0.8195, 
noisyNet noise sample is [array([0.36218584], dtype=float32), 1.6615497]. 
=============================================
[2019-04-01 17:10:54,374] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.8493500e-33
 3.4057627e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 17:10:54,374] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1387
[2019-04-01 17:10:54,478] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 36.33333333333333, 14.0, 274.3333333333334, 26.0, 25.94934270071529, 9.256259813531926, 1.0, 1.0, 19.23149579580592], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 406200.0000, 
sim time next is 406800.0000, 
raw observation next is [-8.9, 36.0, 10.5, 210.0, 26.0, 25.58578113185132, 8.864718659074276, 1.0, 1.0, 52.61440394712569], 
processed observation next is [1.0, 0.7391304347826086, 0.21606648199445982, 0.36, 0.035, 0.23204419889502761, 1.0, 0.94082587597876, 0.08864718659074276, 1.0, 1.0, 0.3758171710508978], 
reward next is 0.6242, 
noisyNet noise sample is [array([-0.7522877], dtype=float32), -0.24466011]. 
=============================================
[2019-04-01 17:10:54,527] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4558135e-30 3.6158503e-27 9.2734316e-26 8.2463574e-33 7.3781953e-11
 1.7678633e-05 9.9998236e-01], sum to 1.0000
[2019-04-01 17:10:54,528] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8170
[2019-04-01 17:10:54,587] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.3, 38.66666666666666, 0.0, 0.0, 26.0, 25.17581638089504, 8.208537887245269, 1.0, 1.0, 54.26661305707314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 409200.0000, 
sim time next is 409800.0000, 
raw observation next is [-9.4, 39.33333333333334, 0.0, 0.0, 26.0, 25.24607349798259, 8.471997891357006, 1.0, 1.0, 50.770716324346], 
processed observation next is [1.0, 0.7391304347826086, 0.20221606648199447, 0.3933333333333334, 0.0, 0.0, 1.0, 0.8922962139975128, 0.08471997891357005, 1.0, 1.0, 0.36264797374532853], 
reward next is 0.6374, 
noisyNet noise sample is [array([-0.14526063], dtype=float32), 1.0096657]. 
=============================================
[2019-04-01 17:11:13,098] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 8.632855e-29 1.000000e+00], sum to 1.0000
[2019-04-01 17:11:13,105] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4999
[2019-04-01 17:11:13,174] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 54.00000000000001, 82.66666666666667, 41.0, 26.0, 24.95749740912554, 7.171099127682972, 0.0, 1.0, 37.79741066098853], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 660000.0000, 
sim time next is 660600.0000, 
raw observation next is [-0.6, 54.0, 83.0, 38.0, 26.0, 24.98817737345549, 7.156410208370822, 0.0, 1.0, 31.25735422502747], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.27666666666666667, 0.041988950276243095, 1.0, 0.8554539104936413, 0.07156410208370823, 0.0, 1.0, 0.22326681589305336], 
reward next is 0.7767, 
noisyNet noise sample is [array([0.96558905], dtype=float32), -0.9518141]. 
=============================================
[2019-04-01 17:11:14,329] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7054133e-26 5.5910285e-32 2.9803525e-13 4.1223141e-32 0.0000000e+00
 8.4262711e-01 1.5737295e-01], sum to 1.0000
[2019-04-01 17:11:14,329] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9487
[2019-04-01 17:11:14,347] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.53139907154453, 5.704182115968567, 0.0, 1.0, 40.79572727279902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 689400.0000, 
sim time next is 690000.0000, 
raw observation next is [-3.899999999999999, 71.0, 0.0, 0.0, 26.0, 24.49828907154301, 5.666243678267016, 0.0, 1.0, 40.72057891549786], 
processed observation next is [0.0, 1.0, 0.35457063711911363, 0.71, 0.0, 0.0, 1.0, 0.785469867363287, 0.056662436782670154, 0.0, 1.0, 0.29086127796784184], 
reward next is 0.7091, 
noisyNet noise sample is [array([0.75424594], dtype=float32), 1.0930176]. 
=============================================
[2019-04-01 17:11:14,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[65.01998]
 [65.1268 ]
 [65.24623]
 [65.37838]
 [65.53143]], R is [[64.97660065]
 [65.03543854]
 [65.09318542]
 [65.14987946]
 [65.20558167]].
[2019-04-01 17:11:15,863] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6625466e-24 1.5481450e-32 5.4046276e-25 1.5427330e-32 2.8580078e-25
 9.4257283e-01 5.7427164e-02], sum to 1.0000
[2019-04-01 17:11:15,864] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7900
[2019-04-01 17:11:15,883] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 75.0, 0.0, 0.0, 26.0, 23.87619927360952, 5.255571656149795, 0.0, 1.0, 40.7873642523077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 792000.0000, 
sim time next is 792600.0000, 
raw observation next is [-7.3, 74.33333333333333, 0.0, 0.0, 26.0, 23.83063687414383, 5.2601355166506, 0.0, 1.0, 40.87623164443352], 
processed observation next is [1.0, 0.17391304347826086, 0.26038781163434904, 0.7433333333333333, 0.0, 0.0, 1.0, 0.6900909820205473, 0.052601355166506, 0.0, 1.0, 0.29197308317452514], 
reward next is 0.7080, 
noisyNet noise sample is [array([-0.73320997], dtype=float32), -0.13216935]. 
=============================================
[2019-04-01 17:11:19,383] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.8020897e-35 0.0000000e+00 4.8816923e-36 0.0000000e+00 4.0455966e-37
 1.0000000e+00 5.9516198e-25], sum to 1.0000
[2019-04-01 17:11:19,383] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8562
[2019-04-01 17:11:19,398] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 64.0, 0.0, 0.0, 26.0, 24.73478997733068, 6.696676616054684, 0.0, 1.0, 42.83991371103782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 770400.0000, 
sim time next is 771000.0000, 
raw observation next is [-6.283333333333333, 64.5, 0.0, 0.0, 26.0, 24.69870379715199, 6.610869013999834, 0.0, 1.0, 42.72455689612003], 
processed observation next is [1.0, 0.9565217391304348, 0.288550323176362, 0.645, 0.0, 0.0, 1.0, 0.8141005424502842, 0.06610869013999834, 0.0, 1.0, 0.30517540640085733], 
reward next is 0.6948, 
noisyNet noise sample is [array([1.3588272], dtype=float32), -0.35512763]. 
=============================================
[2019-04-01 17:11:19,422] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[62.56633 ]
 [62.78881 ]
 [63.681705]
 [64.96754 ]
 [66.39876 ]], R is [[62.58076477]
 [62.6489563 ]
 [62.71571732]
 [62.78115082]
 [62.8455925 ]].
[2019-04-01 17:11:20,326] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.0937732e-36
 8.8842371e-11 1.0000000e+00], sum to 1.0000
[2019-04-01 17:11:20,329] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6380
[2019-04-01 17:11:20,398] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 54.0, 34.0, 2.5, 26.0, 26.54869255462905, 11.64063280749284, 1.0, 1.0, 23.33665373786904], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 752400.0000, 
sim time next is 753000.0000, 
raw observation next is [-2.983333333333333, 54.33333333333333, 0.0, 0.0, 26.0, 26.52763207253398, 10.0938091747084, 1.0, 1.0, 23.79692933215114], 
processed observation next is [1.0, 0.7391304347826086, 0.37996306555863346, 0.5433333333333333, 0.0, 0.0, 1.0, 1.0753760103619971, 0.10093809174708399, 1.0, 1.0, 0.16997806665822243], 
reward next is 0.7925, 
noisyNet noise sample is [array([0.19059317], dtype=float32), -2.1887243]. 
=============================================
[2019-04-01 17:11:20,404] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[82.69581 ]
 [83.084305]
 [83.295006]
 [83.28894 ]
 [82.9914  ]], R is [[81.97724915]
 [81.33453369]
 [80.70223999]
 [80.17738342]
 [80.0951767 ]].
[2019-04-01 17:11:21,059] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 2.329248e-10], sum to 1.0000
[2019-04-01 17:11:21,060] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2315
[2019-04-01 17:11:21,119] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.35, 55.0, 0.0, 0.0, 26.0, 24.82109994991974, 7.558130819481758, 1.0, 1.0, 66.01637296251216], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 754200.0000, 
sim time next is 754800.0000, 
raw observation next is [-3.533333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 24.91616879555493, 8.274655717900059, 1.0, 1.0, 57.27993175192171], 
processed observation next is [1.0, 0.7391304347826086, 0.36472760849492153, 0.5533333333333332, 0.0, 0.0, 1.0, 0.8451669707935616, 0.08274655717900059, 1.0, 1.0, 0.4091423696565836], 
reward next is 0.5909, 
noisyNet noise sample is [array([0.8471352], dtype=float32), 0.07199026]. 
=============================================
[2019-04-01 17:11:33,105] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 9.9999988e-01 1.4490097e-07], sum to 1.0000
[2019-04-01 17:11:33,108] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1701
[2019-04-01 17:11:33,116] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 80.33333333333333, 0.0, 0.0, 26.0, 25.65308634470041, 10.89468348522298, 1.0, 1.0, 4.315259381094256], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1019400.0000, 
sim time next is 1020000.0000, 
raw observation next is [14.4, 79.66666666666667, 0.0, 0.0, 26.0, 25.5378721221315, 10.5171971092296, 1.0, 1.0, 3.972091740471508], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.7966666666666667, 0.0, 0.0, 1.0, 0.9339817317330714, 0.10517197109229601, 1.0, 1.0, 0.02837208386051077], 
reward next is 0.7647, 
noisyNet noise sample is [array([-0.5432057], dtype=float32), 0.16424985]. 
=============================================
[2019-04-01 17:11:33,124] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[66.89159 ]
 [67.612656]
 [66.17299 ]
 [66.08744 ]
 [66.006966]], R is [[66.48265076]
 [66.42913055]
 [66.73487091]
 [66.54150391]
 [66.29911804]].
[2019-04-01 17:11:36,074] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4827325e-36 0.0000000e+00 3.9469207e-31 0.0000000e+00 2.2116730e-33
 3.4665997e-25 1.0000000e+00], sum to 1.0000
[2019-04-01 17:11:36,075] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8588
[2019-04-01 17:11:36,081] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.2, 83.0, 22.0, 69.0, 26.0, 25.86892617529472, 13.88022855067038, 1.0, 1.0, 11.66457461341184], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1067400.0000, 
sim time next is 1068000.0000, 
raw observation next is [12.2, 83.0, 35.0, 96.5, 26.0, 26.14552465213474, 14.79680488534422, 1.0, 1.0, 10.37222571034568], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.11666666666666667, 0.10662983425414364, 1.0, 1.0207892360192485, 0.1479680488534422, 1.0, 1.0, 0.07408732650246914], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.3833005], dtype=float32), 0.331518]. 
=============================================
[2019-04-01 17:11:36,097] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[59.536087]
 [60.39609 ]
 [61.003155]
 [61.835186]
 [62.693226]], R is [[57.85752869]
 [57.27895355]
 [56.70616531]
 [56.13910294]
 [55.57771301]].
[2019-04-01 17:11:36,315] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 4.1939333e-16], sum to 1.0000
[2019-04-01 17:11:36,319] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6739
[2019-04-01 17:11:36,333] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.2, 62.33333333333334, 0.0, 0.0, 26.0, 25.65563322071132, 15.92239571258718, 0.0, 1.0, 30.92610921391621], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1113000.0000, 
sim time next is 1113600.0000, 
raw observation next is [13.1, 62.66666666666667, 0.0, 0.0, 26.0, 25.79263907285289, 16.28596207687146, 0.0, 1.0, 24.17098482918323], 
processed observation next is [1.0, 0.9130434782608695, 0.8254847645429363, 0.6266666666666667, 0.0, 0.0, 1.0, 0.9703770104075556, 0.1628596207687146, 0.0, 1.0, 0.17264989163702307], 
reward next is 0.8274, 
noisyNet noise sample is [array([-0.81176], dtype=float32), -0.11018214]. 
=============================================
[2019-04-01 17:11:36,640] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.4684750e-33 0.0000000e+00 8.2283909e-31 0.0000000e+00 0.0000000e+00
 1.0168635e-04 9.9989831e-01], sum to 1.0000
[2019-04-01 17:11:36,644] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8454
[2019-04-01 17:11:36,656] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.0, 77.66666666666667, 0.0, 0.0, 26.0, 25.79767275270292, 12.32897328505334, 0.0, 1.0, 12.70467809026142], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1053600.0000, 
sim time next is 1054200.0000, 
raw observation next is [13.9, 77.83333333333333, 0.0, 0.0, 26.0, 25.72156296175651, 12.0450415714459, 0.0, 1.0, 12.21071506445096], 
processed observation next is [1.0, 0.17391304347826086, 0.847645429362881, 0.7783333333333333, 0.0, 0.0, 1.0, 0.9602232802509302, 0.120450415714459, 0.0, 1.0, 0.08721939331750686], 
reward next is 0.9128, 
noisyNet noise sample is [array([-0.7622993], dtype=float32), -0.57972974]. 
=============================================
[2019-04-01 17:11:38,301] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5992691e-31
 1.0000000e+00 5.0418530e-10], sum to 1.0000
[2019-04-01 17:11:38,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1182
[2019-04-01 17:11:38,317] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 79.66666666666667, 0.0, 0.0, 26.0, 25.5378721221315, 10.5171971092296, 1.0, 1.0, 3.972091740471508], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1020000.0000, 
sim time next is 1020600.0000, 
raw observation next is [14.4, 79.0, 0.0, 0.0, 26.0, 25.38718779435893, 10.08799435458894, 0.0, 1.0, 3.830764977701804], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.79, 0.0, 0.0, 1.0, 0.9124553991941328, 0.1008799435458894, 0.0, 1.0, 0.027362606983584312], 
reward next is 0.9726, 
noisyNet noise sample is [array([-0.13013862], dtype=float32), -0.5646809]. 
=============================================
[2019-04-01 17:11:40,789] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.7429323e-29 1.6766422e-31 1.4103527e-21 1.8057679e-37 1.7182203e-35
 2.4518881e-06 9.9999750e-01], sum to 1.0000
[2019-04-01 17:11:40,790] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4892
[2019-04-01 17:11:40,800] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.16666666666667, 82.5, 21.0, 0.3333333333333333, 26.0, 25.65952719644553, 12.96897571159521, 0.0, 1.0, 21.47309713379548], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1152600.0000, 
sim time next is 1153200.0000, 
raw observation next is [13.63333333333333, 81.0, 26.0, 0.1666666666666666, 26.0, 25.64820708053584, 12.95644548514255, 0.0, 1.0, 18.07144579236826], 
processed observation next is [0.0, 0.34782608695652173, 0.840258541089566, 0.81, 0.08666666666666667, 0.00018416206261510123, 1.0, 0.9497438686479772, 0.1295644548514255, 0.0, 1.0, 0.12908175565977328], 
reward next is 0.8709, 
noisyNet noise sample is [array([1.0850999], dtype=float32), 0.096084654]. 
=============================================
[2019-04-01 17:11:47,726] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 3.882541e-25 1.000000e+00], sum to 1.0000
[2019-04-01 17:11:47,726] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7983
[2019-04-01 17:11:47,749] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.200000000000001, 95.0, 0.0, 0.0, 26.0, 25.40029627271032, 12.61835181365251, 0.0, 1.0, 39.90734848656264], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1297200.0000, 
sim time next is 1297800.0000, 
raw observation next is [4.1, 94.5, 0.0, 0.0, 26.0, 25.50132572608767, 12.79379445324102, 0.0, 1.0, 38.31603954869977], 
processed observation next is [1.0, 0.0, 0.5761772853185596, 0.945, 0.0, 0.0, 1.0, 0.9287608180125245, 0.1279379445324102, 0.0, 1.0, 0.2736859967764269], 
reward next is 0.7263, 
noisyNet noise sample is [array([1.0985397], dtype=float32), -2.52891]. 
=============================================
[2019-04-01 17:11:50,443] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4088700e-24 1.3601238e-29 1.1393693e-21 6.7808461e-35 7.7907554e-20
 3.7410582e-11 1.0000000e+00], sum to 1.0000
[2019-04-01 17:11:50,448] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7104
[2019-04-01 17:11:50,464] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 95.0, 0.0, 0.0, 26.0, 25.36856076940172, 9.637723259726641, 0.0, 1.0, 35.97460808412003], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1481400.0000, 
sim time next is 1482000.0000, 
raw observation next is [2.2, 95.33333333333334, 0.0, 0.0, 26.0, 25.39507829901318, 9.599307594331192, 0.0, 1.0, 36.05086799618813], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9533333333333335, 0.0, 0.0, 1.0, 0.9135826141447397, 0.09599307594331191, 0.0, 1.0, 0.25750619997277235], 
reward next is 0.7425, 
noisyNet noise sample is [array([-0.01065285], dtype=float32), 0.9861301]. 
=============================================
[2019-04-01 17:11:50,472] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[65.12575 ]
 [65.17912 ]
 [65.246574]
 [65.357216]
 [65.27116 ]], R is [[65.13458252]
 [65.22628021]
 [65.30683899]
 [65.40137482]
 [65.51083374]].
[2019-04-01 17:11:59,037] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.5750167e-32 3.9068658e-38 1.4537775e-21 0.0000000e+00 2.6046075e-32
 5.9280436e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 17:11:59,037] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7665
[2019-04-01 17:11:59,058] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.433333333333334, 85.83333333333334, 0.0, 0.0, 26.0, 25.66823616677433, 11.45735249706858, 0.0, 1.0, 24.07325136829067], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1566600.0000, 
sim time next is 1567200.0000, 
raw observation next is [4.466666666666667, 85.66666666666667, 0.0, 0.0, 26.0, 25.69090194307896, 11.15527113476426, 0.0, 1.0, 22.87529764105457], 
processed observation next is [1.0, 0.13043478260869565, 0.5863342566943676, 0.8566666666666667, 0.0, 0.0, 1.0, 0.9558431347255656, 0.1115527113476426, 0.0, 1.0, 0.16339498315038978], 
reward next is 0.8366, 
noisyNet noise sample is [array([1.7051051], dtype=float32), 0.8420491]. 
=============================================
[2019-04-01 17:12:00,844] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.7842537e-30 1.2442910e-32 5.0436423e-25 1.1310942e-35 3.1049604e-27
 1.0442491e-35 1.0000000e+00], sum to 1.0000
[2019-04-01 17:12:00,844] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1670
[2019-04-01 17:12:00,872] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.166666666666666, 81.0, 0.0, 0.0, 26.0, 25.67530213158793, 10.34320025754228, 1.0, 1.0, 20.95244527083744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1582800.0000, 
sim time next is 1583400.0000, 
raw observation next is [5.083333333333334, 81.5, 13.0, 15.0, 26.0, 25.60912690377683, 10.1157270365015, 1.0, 1.0, 19.65527109974964], 
processed observation next is [1.0, 0.30434782608695654, 0.6034164358264081, 0.815, 0.043333333333333335, 0.016574585635359115, 1.0, 0.9441609862538327, 0.101157270365015, 1.0, 1.0, 0.1403947935696403], 
reward next is 0.8133, 
noisyNet noise sample is [array([-0.6470827], dtype=float32), -0.948432]. 
=============================================
[2019-04-01 17:12:06,707] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.8025188e-29 1.2951590e-36 2.0250876e-21 1.8416428e-35 3.3456727e-31
 5.2311906e-25 1.0000000e+00], sum to 1.0000
[2019-04-01 17:12:06,707] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7548
[2019-04-01 17:12:06,725] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8999999999999999, 85.0, 0.0, 0.0, 26.0, 24.87868897216773, 7.845264379865, 0.0, 1.0, 43.11178066693237], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1747800.0000, 
sim time next is 1748400.0000, 
raw observation next is [-1.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.86602685332972, 7.73790032046183, 0.0, 1.0, 43.18243702217481], 
processed observation next is [0.0, 0.21739130434782608, 0.4349030470914128, 0.8566666666666667, 0.0, 0.0, 1.0, 0.8380038361899602, 0.0773790032046183, 0.0, 1.0, 0.30844597872982005], 
reward next is 0.6916, 
noisyNet noise sample is [array([1.1297233], dtype=float32), -0.3388082]. 
=============================================
[2019-04-01 17:12:10,100] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:12:10,102] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4045
[2019-04-01 17:12:10,150] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 75.0, 183.3333333333333, 76.66666666666667, 26.0, 25.0914662638342, 7.536363859165611, 0.0, 1.0, 32.63297104170043], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1866000.0000, 
sim time next is 1866600.0000, 
raw observation next is [-4.5, 77.0, 186.0, 84.0, 26.0, 25.0292921216731, 7.444688070723761, 0.0, 1.0, 36.49350265487224], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.77, 0.62, 0.09281767955801105, 1.0, 0.8613274459533002, 0.07444688070723761, 0.0, 1.0, 0.26066787610623027], 
reward next is 0.7393, 
noisyNet noise sample is [array([0.79908204], dtype=float32), -0.08818206]. 
=============================================
[2019-04-01 17:12:13,047] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8737968e-33 1.7476318e-38 1.3498732e-30 0.0000000e+00 1.4744245e-37
 1.0432917e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 17:12:13,052] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0602
[2019-04-01 17:12:13,073] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 78.16666666666666, 0.0, 0.0, 26.0, 24.32664110917708, 5.791734827786844, 0.0, 1.0, 45.27148968757839], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1817400.0000, 
sim time next is 1818000.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 26.0, 24.30998866989292, 5.746287807448037, 0.0, 1.0, 45.34155317027741], 
processed observation next is [0.0, 0.043478260869565216, 0.30747922437673136, 0.78, 0.0, 0.0, 1.0, 0.7585698099847029, 0.05746287807448037, 0.0, 1.0, 0.3238682369305529], 
reward next is 0.6761, 
noisyNet noise sample is [array([-1.4872494], dtype=float32), -0.21866776]. 
=============================================
[2019-04-01 17:12:13,080] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[65.784424]
 [65.76796 ]
 [65.66133 ]
 [65.58064 ]
 [65.57735 ]], R is [[65.82641602]
 [65.8447876 ]
 [65.86348724]
 [65.88249969]
 [65.9017868 ]].
[2019-04-01 17:12:23,991] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1130386e-30 0.0000000e+00 2.9429772e-23 0.0000000e+00 7.5227316e-37
 2.5815682e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 17:12:23,991] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0659
[2019-04-01 17:12:24,004] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.2, 86.5, 0.0, 0.0, 26.0, 23.15973250842448, 5.991182355853802, 0.0, 1.0, 44.00519724854955], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1924200.0000, 
sim time next is 1924800.0000, 
raw observation next is [-9.3, 88.0, 0.0, 0.0, 26.0, 23.1094570090361, 6.004316038777179, 0.0, 1.0, 43.96113078686563], 
processed observation next is [1.0, 0.2608695652173913, 0.20498614958448752, 0.88, 0.0, 0.0, 1.0, 0.587065287005157, 0.06004316038777179, 0.0, 1.0, 0.3140080770490402], 
reward next is 0.6860, 
noisyNet noise sample is [array([0.8689369], dtype=float32), -0.72512543]. 
=============================================
[2019-04-01 17:12:25,212] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.7586566e-34 1.0000000e+00], sum to 1.0000
[2019-04-01 17:12:25,213] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9955
[2019-04-01 17:12:25,227] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.566666666666666, 79.33333333333334, 0.0, 0.0, 26.0, 23.65232836899435, 5.410387295487145, 0.0, 1.0, 44.66500770785643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1916400.0000, 
sim time next is 1917000.0000, 
raw observation next is [-8.65, 80.0, 0.0, 0.0, 26.0, 23.62614993523314, 5.429796830006993, 0.0, 1.0, 44.68786497589722], 
processed observation next is [1.0, 0.17391304347826086, 0.22299168975069253, 0.8, 0.0, 0.0, 1.0, 0.6608785621761629, 0.054297968300069936, 0.0, 1.0, 0.319199035542123], 
reward next is 0.6808, 
noisyNet noise sample is [array([-0.5620274], dtype=float32), -0.41696617]. 
=============================================
[2019-04-01 17:12:25,230] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[68.12935 ]
 [68.205345]
 [68.2709  ]
 [68.30162 ]
 [68.29431 ]], R is [[68.05618286]
 [68.05657959]
 [68.05727386]
 [68.05886841]
 [68.061203  ]].
[2019-04-01 17:12:33,970] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.258071e-38 1.000000e+00], sum to 1.0000
[2019-04-01 17:12:33,970] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5844
[2019-04-01 17:12:34,016] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.14401175820341, 8.511877544861782, 1.0, 1.0, 37.26872629079021], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2051400.0000, 
sim time next is 2052000.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.22872158776928, 8.87626099525928, 1.0, 1.0, 36.84646477806292], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 1.0, 0.8898173696813255, 0.0887626099525928, 1.0, 1.0, 0.2631890341290209], 
reward next is 0.7368, 
noisyNet noise sample is [array([-1.2349405], dtype=float32), -1.8504394]. 
=============================================
[2019-04-01 17:12:34,037] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[58.118587]
 [58.29135 ]
 [58.39617 ]
 [58.500645]
 [58.642548]], R is [[58.20519638]
 [58.35694122]
 [58.56183243]
 [58.76837921]
 [58.97219086]].
[2019-04-01 17:12:42,985] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 8.752955e-38 1.000000e+00], sum to 1.0000
[2019-04-01 17:12:42,988] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3307
[2019-04-01 17:12:43,018] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 82.00000000000001, 0.0, 0.0, 26.0, 24.65956324725271, 6.61115387891594, 0.0, 1.0, 41.76622769487129], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2157000.0000, 
sim time next is 2157600.0000, 
raw observation next is [-7.300000000000001, 82.0, 0.0, 0.0, 26.0, 24.59267327690451, 6.483343434872384, 0.0, 1.0, 41.80092758514252], 
processed observation next is [1.0, 1.0, 0.26038781163434904, 0.82, 0.0, 0.0, 1.0, 0.7989533252720727, 0.06483343434872384, 0.0, 1.0, 0.29857805417958944], 
reward next is 0.7014, 
noisyNet noise sample is [array([1.2524505], dtype=float32), 1.2020795]. 
=============================================
[2019-04-01 17:12:58,302] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0845721e-27 4.3487858e-33 2.7345841e-19 7.9202896e-37 1.2318160e-28
 7.4373047e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 17:12:58,303] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7875
[2019-04-01 17:12:58,320] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.3, 68.33333333333333, 0.0, 0.0, 26.0, 24.27567434893414, 5.617758014064715, 0.0, 1.0, 40.47604949791623], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2351400.0000, 
sim time next is 2352000.0000, 
raw observation next is [-3.2, 67.66666666666667, 0.0, 0.0, 26.0, 24.24500713316977, 5.597005296398433, 0.0, 1.0, 40.53936851230711], 
processed observation next is [0.0, 0.21739130434782608, 0.37396121883656513, 0.6766666666666667, 0.0, 0.0, 1.0, 0.7492867333099673, 0.05597005296398433, 0.0, 1.0, 0.28956691794505074], 
reward next is 0.7104, 
noisyNet noise sample is [array([-1.5544442], dtype=float32), 1.2859956]. 
=============================================
[2019-04-01 17:12:58,346] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[71.247025]
 [71.275986]
 [71.36094 ]
 [71.44598 ]
 [71.52902 ]], R is [[71.23668671]
 [71.2352066 ]
 [71.23432922]
 [71.23414612]
 [71.23466492]].
[2019-04-01 17:13:00,826] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6837613e-38 0.0000000e+00 3.5266549e-29 0.0000000e+00 1.6052382e-34
 3.8772600e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 17:13:00,830] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9058
[2019-04-01 17:13:00,877] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.85, 37.16666666666666, 79.33333333333333, 794.3333333333334, 26.0, 25.04876897575308, 7.085439747321831, 0.0, 1.0, 31.85687186996927], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2458200.0000, 
sim time next is 2458800.0000, 
raw observation next is [-2.3, 36.0, 81.0, 803.0, 26.0, 25.09154725526873, 7.124529296625958, 0.0, 1.0, 27.69960938838201], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.36, 0.27, 0.887292817679558, 1.0, 0.8702210364669613, 0.07124529296625959, 0.0, 1.0, 0.19785435277415722], 
reward next is 0.8021, 
noisyNet noise sample is [array([-0.16616689], dtype=float32), 0.30943623]. 
=============================================
[2019-04-01 17:13:00,945] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 4.4924544e-38 0.0000000e+00 0.0000000e+00
 5.1248138e-30 1.0000000e+00], sum to 1.0000
[2019-04-01 17:13:00,945] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8716
[2019-04-01 17:13:00,967] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.8, 44.66666666666667, 0.0, 0.0, 26.0, 24.52347890542046, 5.741071544754512, 0.0, 1.0, 42.56463732579047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2420400.0000, 
sim time next is 2421000.0000, 
raw observation next is [-5.9, 45.5, 0.0, 0.0, 26.0, 24.54943782623271, 5.696088851692697, 0.0, 1.0, 42.56849778469079], 
processed observation next is [0.0, 0.0, 0.2991689750692521, 0.455, 0.0, 0.0, 1.0, 0.7927768323189588, 0.056960888516926975, 0.0, 1.0, 0.30406069846207706], 
reward next is 0.6959, 
noisyNet noise sample is [array([0.04492392], dtype=float32), 0.14788881]. 
=============================================
[2019-04-01 17:13:00,978] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[74.49916 ]
 [74.86627 ]
 [75.321396]
 [75.4751  ]
 [75.5172  ]], R is [[74.04699707]
 [74.00249481]
 [73.95851135]
 [73.9150238 ]
 [73.87197876]].
[2019-04-01 17:13:09,199] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.9687698e-06 9.9999797e-01], sum to 1.0000
[2019-04-01 17:13:09,200] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9660
[2019-04-01 17:13:09,229] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.9, 29.0, 59.5, 135.8333333333333, 26.0, 25.57860028988378, 8.61106939777986, 1.0, 1.0, 10.81607374518249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2565600.0000, 
sim time next is 2566200.0000, 
raw observation next is [2.8, 29.0, 49.00000000000001, 109.6666666666667, 26.0, 25.73693996699989, 8.76544555289429, 1.0, 1.0, 10.40275902197358], 
processed observation next is [1.0, 0.6956521739130435, 0.5401662049861496, 0.29, 0.16333333333333336, 0.12117863720073668, 1.0, 0.9624199952856988, 0.0876544555289429, 1.0, 1.0, 0.07430542158552557], 
reward next is 0.9257, 
noisyNet noise sample is [array([-0.39597723], dtype=float32), -1.1805403]. 
=============================================
[2019-04-01 17:13:09,714] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 2.1854487e-32 1.0000000e+00], sum to 1.0000
[2019-04-01 17:13:09,716] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7935
[2019-04-01 17:13:09,739] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.80778909245603, 6.257979494783949, 0.0, 1.0, 41.22268282237501], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2601600.0000, 
sim time next is 2602200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.81771570346349, 6.152059641225994, 0.0, 1.0, 41.25581282532288], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 1.0, 0.8311022433519274, 0.06152059641225994, 0.0, 1.0, 0.2946843773237348], 
reward next is 0.7053, 
noisyNet noise sample is [array([-0.5061192], dtype=float32), -1.1032177]. 
=============================================
[2019-04-01 17:13:10,560] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 2.416322e-25 1.000000e+00], sum to 1.0000
[2019-04-01 17:13:10,560] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9842
[2019-04-01 17:13:10,578] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.533333333333333, 58.0, 0.0, 0.0, 26.0, 25.19434940332718, 7.50626166122404, 0.0, 1.0, 40.96337988640307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2587200.0000, 
sim time next is 2587800.0000, 
raw observation next is [-3.716666666666666, 58.5, 0.0, 0.0, 26.0, 25.11841521961483, 7.360992395325145, 0.0, 1.0, 41.80212427920415], 
processed observation next is [1.0, 0.9565217391304348, 0.3596491228070176, 0.585, 0.0, 0.0, 1.0, 0.8740593170878326, 0.07360992395325144, 0.0, 1.0, 0.29858660199431536], 
reward next is 0.7014, 
noisyNet noise sample is [array([0.25625014], dtype=float32), -0.9952369]. 
=============================================
[2019-04-01 17:13:17,248] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.32374715e-33 0.00000000e+00 1.21187829e-33 0.00000000e+00
 0.00000000e+00 1.24659934e-18 1.00000000e+00], sum to 1.0000
[2019-04-01 17:13:17,248] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8145
[2019-04-01 17:13:17,300] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.16666666666667, 83.0, 0.0, 0.0, 26.0, 23.2855843703136, 5.384903169028441, 0.0, 1.0, 43.12780594763699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2697000.0000, 
sim time next is 2697600.0000, 
raw observation next is [-15.33333333333334, 83.0, 0.0, 0.0, 26.0, 23.25164369618441, 5.412271545891866, 0.0, 1.0, 43.01450834839687], 
processed observation next is [1.0, 0.21739130434782608, 0.03785780240073851, 0.83, 0.0, 0.0, 1.0, 0.607377670883487, 0.05412271545891866, 0.0, 1.0, 0.3072464882028348], 
reward next is 0.6928, 
noisyNet noise sample is [array([1.051064], dtype=float32), 0.4910227]. 
=============================================
[2019-04-01 17:13:26,212] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 2.5114532e-35 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 17:13:26,212] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9299
[2019-04-01 17:13:26,230] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 95.33333333333334, 0.0, 0.0, 26.0, 24.9737375508095, 7.066241053066608, 0.0, 1.0, 55.45863312049838], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2870400.0000, 
sim time next is 2871000.0000, 
raw observation next is [1.0, 96.5, 0.0, 0.0, 26.0, 25.10301052336651, 7.06524533190763, 0.0, 1.0, 54.08792529342669], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.965, 0.0, 0.0, 1.0, 0.8718586461952158, 0.0706524533190763, 0.0, 1.0, 0.3863423235244764], 
reward next is 0.6137, 
noisyNet noise sample is [array([0.22778495], dtype=float32), 0.2560625]. 
=============================================
[2019-04-01 17:13:26,248] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[61.447693]
 [61.594753]
 [61.74319 ]
 [61.917065]
 [62.041164]], R is [[61.37524414]
 [61.36536026]
 [61.36133575]
 [61.35702515]
 [61.3529892 ]].
[2019-04-01 17:13:33,949] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.1514451e-32 2.6004488e-31 3.4650006e-32 1.1989637e-37 5.8776526e-35
 8.1525059e-08 9.9999988e-01], sum to 1.0000
[2019-04-01 17:13:33,950] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5109
[2019-04-01 17:13:34,007] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 26.0, 24.93762743718537, 9.525150778596313, 1.0, 1.0, 45.50018472105731], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2919600.0000, 
sim time next is 2920200.0000, 
raw observation next is [-1.0, 89.66666666666667, 0.0, 0.0, 26.0, 25.17716279395186, 9.866590132214993, 1.0, 1.0, 31.49157808305926], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.8966666666666667, 0.0, 0.0, 1.0, 0.8824518277074088, 0.09866590132214993, 1.0, 1.0, 0.22493984345042328], 
reward next is 0.7751, 
noisyNet noise sample is [array([1.6041579], dtype=float32), -2.346736]. 
=============================================
[2019-04-01 17:13:52,502] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.5884698e-33
 6.6443476e-11 1.0000000e+00], sum to 1.0000
[2019-04-01 17:13:52,503] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6993
[2019-04-01 17:13:52,530] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 53.33333333333333, 9.166666666666668, 110.8333333333333, 26.0, 25.55042110392515, 10.49191062755787, 1.0, 1.0, 28.20276692928041], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3346800.0000, 
sim time next is 3347400.0000, 
raw observation next is [-2.833333333333333, 54.16666666666667, 0.0, 0.0, 26.0, 25.41161675310212, 10.80804429564231, 1.0, 1.0, 30.28087652811117], 
processed observation next is [1.0, 0.7391304347826086, 0.3841181902123731, 0.5416666666666667, 0.0, 0.0, 1.0, 0.91594525044316, 0.10808044295642309, 1.0, 1.0, 0.21629197520079405], 
reward next is 0.4605, 
noisyNet noise sample is [array([-1.8654476], dtype=float32), 0.5263574]. 
=============================================
[2019-04-01 17:13:52,939] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3276586e-27 4.3330213e-33 1.8731508e-20 1.0264954e-36 3.5174684e-27
 3.1252538e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 17:13:52,940] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0191
[2019-04-01 17:13:52,957] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.9218590890406, 7.166432167273835, 0.0, 1.0, 40.64615053475741], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3376200.0000, 
sim time next is 3376800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.87113374593915, 7.010778879954306, 0.0, 1.0, 40.66455712763331], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.8387333922770216, 0.07010778879954306, 0.0, 1.0, 0.2904611223402379], 
reward next is 0.7095, 
noisyNet noise sample is [array([0.13075656], dtype=float32), 0.9331839]. 
=============================================
[2019-04-01 17:13:53,107] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0520702e-26 2.2929556e-30 8.9992481e-20 1.2742540e-34 2.0124006e-17
 2.4184930e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 17:13:53,109] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6184
[2019-04-01 17:13:53,124] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 26.0, 23.8570857433476, 5.408637216978742, 0.0, 1.0, 43.44308011748175], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3307200.0000, 
sim time next is 3307800.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 26.0, 23.81502382705399, 5.405091107104944, 0.0, 1.0, 43.45514161116142], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 1.0, 0.6878605467219988, 0.054050911071049444, 0.0, 1.0, 0.31039386865115304], 
reward next is 0.6896, 
noisyNet noise sample is [array([0.2770222], dtype=float32), 0.24395613]. 
=============================================
[2019-04-01 17:14:00,392] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 2.165428e-36
 1.000000e+00 1.208493e-13], sum to 1.0000
[2019-04-01 17:14:00,393] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0435
[2019-04-01 17:14:00,407] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 75.0, 606.0, 26.0, 26.80862771108381, 17.58237402838055, 1.0, 1.0, 8.496881512275815], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3513600.0000, 
sim time next is 3514200.0000, 
raw observation next is [3.0, 49.0, 70.66666666666666, 579.0, 26.0, 26.91303791239273, 18.08278136220058, 1.0, 1.0, 8.656496348177967], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.23555555555555552, 0.6397790055248619, 1.0, 1.1304339874846758, 0.1808278136220058, 1.0, 1.0, 0.061832116772699765], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5359422], dtype=float32), -0.39005613]. 
=============================================
[2019-04-01 17:14:03,882] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:14:03,886] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6628
[2019-04-01 17:14:03,899] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.3599195515397, 9.4974634380261, 0.0, 1.0, 41.83713825200345], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3544200.0000, 
sim time next is 3544800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.35989768008789, 9.371110886406361, 0.0, 1.0, 41.71370636510016], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 1.0, 0.9085568114411272, 0.09371110886406361, 0.0, 1.0, 0.2979550454650011], 
reward next is 0.7020, 
noisyNet noise sample is [array([-0.5767999], dtype=float32), -1.1060876]. 
=============================================
[2019-04-01 17:14:04,266] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.9825772e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 17:14:04,267] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4935
[2019-04-01 17:14:04,277] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8333333333333334, 42.16666666666666, 91.33333333333334, 728.6666666666667, 26.0, 25.18226433921798, 9.861243106322133, 0.0, 1.0, 15.42912551592308], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3597000.0000, 
sim time next is 3597600.0000, 
raw observation next is [-0.6666666666666667, 42.33333333333334, 88.66666666666667, 713.8333333333333, 26.0, 25.21622396704379, 10.05962361164507, 0.0, 1.0, 15.84831926370285], 
processed observation next is [0.0, 0.6521739130434783, 0.44413665743305636, 0.42333333333333345, 0.29555555555555557, 0.7887661141804787, 1.0, 0.8880319952919697, 0.1005962361164507, 0.0, 1.0, 0.11320228045502036], 
reward next is 0.8868, 
noisyNet noise sample is [array([-1.3281455], dtype=float32), -0.68219084]. 
=============================================
[2019-04-01 17:14:05,742] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9462915e-27 1.0979787e-31 6.9550261e-15 4.2852107e-30 8.0878840e-36
 9.7532454e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 17:14:05,744] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4757
[2019-04-01 17:14:05,759] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.2677291453844, 9.184606757926575, 0.0, 1.0, 43.13072234851857], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3798600.0000, 
sim time next is 3799200.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.26720659909944, 9.184072443796964, 0.0, 1.0, 43.06144030930952], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.71, 0.0, 0.0, 1.0, 0.8953152284427769, 0.09184072443796965, 0.0, 1.0, 0.307581716495068], 
reward next is 0.6924, 
noisyNet noise sample is [array([1.221503], dtype=float32), -1.3102975]. 
=============================================
[2019-04-01 17:14:06,852] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 6.704796e-25 1.000000e+00], sum to 1.0000
[2019-04-01 17:14:06,853] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0549
[2019-04-01 17:14:06,892] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.166666666666667, 65.83333333333334, 103.6666666666667, 703.3333333333334, 26.0, 25.87952331975281, 11.17185224227552, 0.0, 1.0, 29.52810995253828], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3577800.0000, 
sim time next is 3578400.0000, 
raw observation next is [-5.0, 65.0, 105.5, 717.0, 26.0, 25.8201982655751, 11.01146115215282, 0.0, 1.0, 27.73763852064296], 
processed observation next is [0.0, 0.43478260869565216, 0.32409972299168976, 0.65, 0.3516666666666667, 0.7922651933701658, 1.0, 0.9743140379393002, 0.11011461152152821, 0.0, 1.0, 0.198125989433164], 
reward next is 0.8019, 
noisyNet noise sample is [array([-1.6387619], dtype=float32), -1.0444576]. 
=============================================
[2019-04-01 17:14:08,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:14:08,188] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5123
[2019-04-01 17:14:08,198] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 24.93566452260399, 7.476440847967251, 0.0, 1.0, 23.29950595576806], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3697200.0000, 
sim time next is 3697800.0000, 
raw observation next is [3.833333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 24.92119760585301, 7.442433465138246, 0.0, 1.0, 19.4941240008265], 
processed observation next is [0.0, 0.8260869565217391, 0.5687903970452447, 0.5966666666666667, 0.0, 0.0, 1.0, 0.8458853722647157, 0.07442433465138247, 0.0, 1.0, 0.13924374286304642], 
reward next is 0.8608, 
noisyNet noise sample is [array([-0.82679], dtype=float32), -1.2160219]. 
=============================================
[2019-04-01 17:14:09,573] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 5.4188612e-27 0.0000000e+00 4.1088040e-37
 1.7158976e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 17:14:09,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0150
[2019-04-01 17:14:09,600] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.44410395565532, 7.837112333779108, 0.0, 1.0, 32.36014570514538], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3634800.0000, 
sim time next is 3635400.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.43742252786125, 7.933654496819059, 0.0, 1.0, 34.12670267404399], 
processed observation next is [0.0, 0.043478260869565216, 0.7119113573407203, 0.25, 0.0, 0.0, 1.0, 0.9196317896944645, 0.07933654496819059, 0.0, 1.0, 0.24376216195745706], 
reward next is 0.7562, 
noisyNet noise sample is [array([0.16691974], dtype=float32), -0.9603455]. 
=============================================
[2019-04-01 17:14:15,309] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.38229246e-35 0.00000000e+00 1.12422354e-29 0.00000000e+00
 0.00000000e+00 8.75526815e-21 1.00000000e+00], sum to 1.0000
[2019-04-01 17:14:15,311] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7427
[2019-04-01 17:14:15,343] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.26524764359304, 9.241022153248418, 0.0, 1.0, 43.32084720862341], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3798000.0000, 
sim time next is 3798600.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.2677291453844, 9.184606757926575, 0.0, 1.0, 43.13072234851857], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.71, 0.0, 0.0, 1.0, 0.8953898779120573, 0.09184606757926575, 0.0, 1.0, 0.30807658820370404], 
reward next is 0.6919, 
noisyNet noise sample is [array([0.19983748], dtype=float32), 1.1279463]. 
=============================================
[2019-04-01 17:14:16,279] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9352790e-23 3.3886944e-28 1.0525913e-21 4.2836372e-32 8.9615486e-31
 4.0369613e-08 1.0000000e+00], sum to 1.0000
[2019-04-01 17:14:16,281] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8800
[2019-04-01 17:14:16,299] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.9572700595745, 7.145215855061839, 0.0, 1.0, 43.28501276453712], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3816600.0000, 
sim time next is 3817200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.9456989945723, 7.035114950662996, 0.0, 1.0, 43.33604255770793], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 1.0, 0.8493855706531858, 0.07035114950662996, 0.0, 1.0, 0.30954316112648517], 
reward next is 0.6905, 
noisyNet noise sample is [array([0.08277231], dtype=float32), 0.75237703]. 
=============================================
[2019-04-01 17:14:19,185] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0209870e-31 2.6139925e-33 1.7174386e-27 4.5808503e-38 2.8899402e-33
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 17:14:19,185] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4437
[2019-04-01 17:14:19,236] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 76.0, 62.33333333333334, 347.6666666666667, 26.0, 25.21233770256169, 7.820883092556902, 1.0, 1.0, 30.42242066006368], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3831000.0000, 
sim time next is 3831600.0000, 
raw observation next is [-4.666666666666667, 75.0, 76.66666666666667, 397.3333333333333, 26.0, 25.20302674851564, 8.402438927066926, 1.0, 1.0, 30.62690379868989], 
processed observation next is [1.0, 0.34782608695652173, 0.3333333333333333, 0.75, 0.2555555555555556, 0.43904235727440144, 1.0, 0.8861466783593771, 0.08402438927066927, 1.0, 1.0, 0.21876359856207064], 
reward next is 0.7812, 
noisyNet noise sample is [array([0.75824744], dtype=float32), 0.81419647]. 
=============================================
[2019-04-01 17:14:21,715] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-01 17:14:21,716] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 17:14:21,719] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:14:21,719] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 17:14:21,720] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 17:14:21,720] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:14:21,721] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:14:21,726] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run17
[2019-04-01 17:14:21,749] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run17
[2019-04-01 17:14:21,749] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run17
[2019-04-01 17:14:47,925] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.63996977], dtype=float32), -0.42073575]
[2019-04-01 17:14:47,925] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.2, 57.0, 0.0, 0.0, 26.0, 24.93280934898113, 7.040329218920841, 0.0, 1.0, 39.23661061234085]
[2019-04-01 17:14:47,925] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 17:14:47,926] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 2.2309856e-32 0.0000000e+00 0.0000000e+00
 1.3470718e-27 1.0000000e+00], sampled 0.6842902002419123
[2019-04-01 17:16:03,512] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 17:16:23,121] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 17:16:27,212] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 17:16:28,236] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 1600000, evaluation results [1600000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 17:16:31,994] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.1745705e-37 0.0000000e+00 1.3747680e-38 1.5026014e-38
 9.9999070e-01 9.2723294e-06], sum to 1.0000
[2019-04-01 17:16:31,995] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8488
[2019-04-01 17:16:32,032] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666666, 36.66666666666666, 58.16666666666666, 474.8333333333334, 26.0, 26.93015522253439, 17.52168690049567, 1.0, 1.0, 8.792105720297652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3948000.0000, 
sim time next is 3948600.0000, 
raw observation next is [-4.833333333333334, 37.33333333333334, 50.33333333333334, 413.6666666666667, 26.0, 27.04306855145064, 14.45271373206834, 1.0, 1.0, 8.91732673787011], 
processed observation next is [1.0, 0.6956521739130435, 0.32871652816251157, 0.3733333333333334, 0.1677777777777778, 0.45709023941068144, 1.0, 1.1490097930643774, 0.1445271373206834, 1.0, 1.0, 0.0636951909847865], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.36605528], dtype=float32), 0.81410563]. 
=============================================
[2019-04-01 17:16:36,499] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4074364e-38 1.8393661e-37 7.8194096e-32 0.0000000e+00 4.9088277e-35
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 17:16:36,504] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2176
[2019-04-01 17:16:36,565] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 53.0, 97.0, 571.0, 26.0, 26.21479942289891, 10.57580483739819, 1.0, 1.0, 32.17454768397674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4006800.0000, 
sim time next is 4007400.0000, 
raw observation next is [-10.66666666666667, 51.50000000000001, 98.33333333333334, 613.3333333333334, 26.0, 26.31066717902505, 10.92087320714452, 1.0, 1.0, 29.90470025462226], 
processed observation next is [1.0, 0.391304347826087, 0.16712834718374878, 0.5150000000000001, 0.32777777777777783, 0.6777163904235728, 1.0, 1.044381025575007, 0.1092087320714452, 1.0, 1.0, 0.21360500181873043], 
reward next is 0.4180, 
noisyNet noise sample is [array([-0.03618834], dtype=float32), 1.1999559]. 
=============================================
[2019-04-01 17:16:37,755] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.9281746e-31 0.0000000e+00 4.2818681e-34
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 17:16:37,755] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6850
[2019-04-01 17:16:37,769] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.833333333333333, 36.0, 0.0, 0.0, 26.0, 25.2902505891748, 8.636088399322572, 0.0, 1.0, 40.76762922594319], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4056600.0000, 
sim time next is 4057200.0000, 
raw observation next is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.27030387183024, 8.52256474648184, 0.0, 1.0, 40.49269647215781], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.37, 0.0, 0.0, 1.0, 0.8957576959757486, 0.08522564746481841, 0.0, 1.0, 0.28923354622969866], 
reward next is 0.7108, 
noisyNet noise sample is [array([1.540001], dtype=float32), 0.89334875]. 
=============================================
[2019-04-01 17:16:39,959] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:16:39,962] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3977
[2019-04-01 17:16:39,987] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 28.0, 120.5, 828.5, 26.0, 26.24317763350571, 10.93409738579528, 1.0, 1.0, 6.627766641675177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4104000.0000, 
sim time next is 4104600.0000, 
raw observation next is [1.333333333333333, 28.16666666666667, 120.3333333333333, 832.6666666666667, 26.0, 26.01396920548858, 12.2406277767426, 1.0, 1.0, 6.136203331348944], 
processed observation next is [1.0, 0.5217391304347826, 0.4995383194829178, 0.28166666666666673, 0.401111111111111, 0.9200736648250462, 1.0, 1.0019956007840827, 0.122406277767426, 1.0, 1.0, 0.0438300237953496], 
reward next is 0.0599, 
noisyNet noise sample is [array([0.03037647], dtype=float32), -2.4128408]. 
=============================================
[2019-04-01 17:16:53,807] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 2.8068961e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 17:16:53,809] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6089
[2019-04-01 17:16:53,818] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.8, 60.0, 0.0, 0.0, 26.0, 26.82215419188543, 20.16751530224919, 0.0, 1.0, 6.488192468382697], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4397400.0000, 
sim time next is 4398000.0000, 
raw observation next is [9.666666666666668, 60.33333333333333, 0.0, 0.0, 26.0, 26.77190816800675, 19.81851192169398, 0.0, 1.0, 6.523025191023404], 
processed observation next is [1.0, 0.9130434782608695, 0.7303785780240075, 0.6033333333333333, 0.0, 0.0, 1.0, 1.1102725954295356, 0.1981851192169398, 0.0, 1.0, 0.046593037078738596], 
reward next is 0.9534, 
noisyNet noise sample is [array([0.68042636], dtype=float32), -0.47080305]. 
=============================================
[2019-04-01 17:16:53,828] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[69.05408 ]
 [68.71909 ]
 [68.09385 ]
 [67.29525 ]
 [65.713875]], R is [[69.46970367]
 [69.72866821]
 [69.98531342]
 [70.23972321]
 [70.49191284]].
[2019-04-01 17:16:54,458] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1691767e-34 5.4492877e-36 4.2650056e-22 0.0000000e+00 2.8800633e-29
 5.5503761e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 17:16:54,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5170
[2019-04-01 17:16:54,468] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.5, 62.0, 0.0, 0.0, 26.0, 26.05406386649985, 14.04216106261698, 0.0, 1.0, 6.68930163998083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4402800.0000, 
sim time next is 4403400.0000, 
raw observation next is [8.350000000000001, 62.16666666666667, 0.0, 0.0, 26.0, 25.97563405102298, 13.62256664344761, 0.0, 1.0, 6.718866851182528], 
processed observation next is [1.0, 1.0, 0.6939058171745154, 0.6216666666666667, 0.0, 0.0, 1.0, 0.9965191501461399, 0.13622566643447612, 0.0, 1.0, 0.047991906079875205], 
reward next is 0.9520, 
noisyNet noise sample is [array([0.01910151], dtype=float32), 0.54514855]. 
=============================================
[2019-04-01 17:16:58,983] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:16:58,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9920
[2019-04-01 17:16:59,003] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2, 72.0, 0.0, 0.0, 26.0, 25.24811075926631, 9.677308150740364, 0.0, 1.0, 42.929507988339], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4488000.0000, 
sim time next is 4488600.0000, 
raw observation next is [-0.25, 72.0, 0.0, 0.0, 26.0, 25.20633966040135, 9.799007678434483, 0.0, 1.0, 45.85140439454418], 
processed observation next is [1.0, 0.9565217391304348, 0.45567867036011084, 0.72, 0.0, 0.0, 1.0, 0.8866199514859073, 0.09799007678434483, 0.0, 1.0, 0.32751003138960133], 
reward next is 0.6725, 
noisyNet noise sample is [array([-0.4726434], dtype=float32), 0.5852975]. 
=============================================
[2019-04-01 17:17:00,034] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.7004847e-32 1.0000000e+00], sum to 1.0000
[2019-04-01 17:17:00,035] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8294
[2019-04-01 17:17:00,074] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.14789327725634, 10.37318249029573, 1.0, 1.0, 29.46499465253671], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4734600.0000, 
sim time next is 4735200.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.40020807043004, 10.34045271856985, 1.0, 1.0, 23.95858752449513], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 1.0, 0.9143154386328628, 0.1034045271856985, 1.0, 1.0, 0.17113276803210806], 
reward next is 0.6927, 
noisyNet noise sample is [array([0.95925593], dtype=float32), -1.5552593]. 
=============================================
[2019-04-01 17:17:04,713] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 8.0465236e-36 2.9290443e-23 0.0000000e+00 9.9603092e-35
 5.2887995e-13 1.0000000e+00], sum to 1.0000
[2019-04-01 17:17:04,716] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4805
[2019-04-01 17:17:04,731] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 39.0, 22.0, 123.3333333333333, 26.0, 25.04144463178018, 8.189118593029251, 0.0, 1.0, 14.99134127802685], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4816200.0000, 
sim time next is 4816800.0000, 
raw observation next is [2.0, 40.0, 16.5, 92.5, 26.0, 25.00074094006097, 7.963829719720363, 0.0, 1.0, 18.81296777812788], 
processed observation next is [0.0, 0.782608695652174, 0.518005540166205, 0.4, 0.055, 0.10220994475138122, 1.0, 0.8572487057229959, 0.07963829719720363, 0.0, 1.0, 0.134378341272342], 
reward next is 0.8656, 
noisyNet noise sample is [array([0.00100467], dtype=float32), -0.4709463]. 
=============================================
[2019-04-01 17:17:05,481] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8448630e-34 4.5967695e-35 0.0000000e+00 6.9572832e-37 4.2874343e-15
 2.1228873e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 17:17:05,482] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2966
[2019-04-01 17:17:05,488] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 45.5, 91.33333333333334, 146.3333333333333, 26.0, 27.31026753080704, 20.22037771993627, 1.0, 1.0, 1.026529795767476], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4639800.0000, 
sim time next is 4640400.0000, 
raw observation next is [5.2, 46.0, 78.5, 146.0, 26.0, 27.41709092958044, 20.32552304623361, 1.0, 1.0, 1.080055549524673], 
processed observation next is [1.0, 0.7391304347826086, 0.6066481994459835, 0.46, 0.26166666666666666, 0.16132596685082873, 1.0, 1.2024415613686341, 0.2032552304623361, 1.0, 1.0, 0.007714682496604808], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6024532], dtype=float32), -0.8204823]. 
=============================================
[2019-04-01 17:17:12,569] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1827845e-28 4.7688976e-27 1.9199687e-29 1.5414880e-33 1.6743947e-16
 9.9992263e-01 7.7411481e-05], sum to 1.0000
[2019-04-01 17:17:12,571] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2394
[2019-04-01 17:17:12,627] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 72.66666666666667, 185.8333333333333, 5.0, 26.0, 24.84098949629208, 8.80793111730712, 1.0, 1.0, 58.03331440533978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4717200.0000, 
sim time next is 4717800.0000, 
raw observation next is [1.5, 72.5, 196.0, 6.0, 26.0, 25.26997189541161, 10.3523749563676, 1.0, 1.0, 31.90416277648732], 
processed observation next is [1.0, 0.6086956521739131, 0.5041551246537397, 0.725, 0.6533333333333333, 0.0066298342541436465, 1.0, 0.8957102707730874, 0.10352374956367599, 1.0, 1.0, 0.22788687697490942], 
reward next is 0.6312, 
noisyNet noise sample is [array([-0.74208], dtype=float32), 1.5995381]. 
=============================================
[2019-04-01 17:17:15,584] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:17:15,584] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9744
[2019-04-01 17:17:15,641] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 68.0, 141.0, 313.0, 26.0, 25.63933037237723, 9.064625952261066, 0.0, 1.0, 42.90339259674133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4869000.0000, 
sim time next is 4869600.0000, 
raw observation next is [-3.333333333333333, 67.0, 152.0, 290.6666666666667, 26.0, 25.7549308792183, 9.193866210055518, 0.0, 1.0, 38.1390612626365], 
processed observation next is [0.0, 0.34782608695652173, 0.37026777469990774, 0.67, 0.5066666666666667, 0.3211786372007367, 1.0, 0.9649901256026142, 0.09193866210055518, 0.0, 1.0, 0.2724218661616893], 
reward next is 0.7276, 
noisyNet noise sample is [array([-0.12957951], dtype=float32), 2.2408316]. 
=============================================
[2019-04-01 17:17:19,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:19,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:19,958] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run13
[2019-04-01 17:17:20,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.6070433e-37 0.0000000e+00 0.0000000e+00
 4.9646194e-38 1.0000000e+00], sum to 1.0000
[2019-04-01 17:17:20,391] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8007
[2019-04-01 17:17:20,409] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666667, 39.16666666666666, 0.0, 0.0, 26.0, 25.34938474451118, 7.598961587695023, 0.0, 1.0, 37.52969997704141], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4921800.0000, 
sim time next is 4922400.0000, 
raw observation next is [0.3333333333333333, 39.33333333333334, 0.0, 0.0, 26.0, 25.31826463250419, 7.613862129361924, 0.0, 1.0, 38.96850092134918], 
processed observation next is [0.0, 1.0, 0.4718374884579871, 0.3933333333333334, 0.0, 0.0, 1.0, 0.9026092332148841, 0.07613862129361924, 0.0, 1.0, 0.2783464351524942], 
reward next is 0.7217, 
noisyNet noise sample is [array([0.69847524], dtype=float32), 0.9996064]. 
=============================================
[2019-04-01 17:17:22,363] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:22,364] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:22,379] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run13
[2019-04-01 17:17:26,208] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:26,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:26,215] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run13
[2019-04-01 17:17:26,475] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:26,475] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:26,478] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run13
[2019-04-01 17:17:26,910] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:26,911] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:26,935] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run13
[2019-04-01 17:17:27,156] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:27,156] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:27,161] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run13
[2019-04-01 17:17:27,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:27,317] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:27,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run13
[2019-04-01 17:17:27,838] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.5833234e-30 4.1003527e-35 1.4574041e-35 1.1547234e-28
 1.0000000e+00 6.5317216e-23], sum to 1.0000
[2019-04-01 17:17:27,840] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8143
[2019-04-01 17:17:27,868] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.0, 19.0, 96.0, 745.0, 26.0, 28.72071502254261, 27.92799411163977, 1.0, 1.0, 0.988879389107086], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5067000.0000, 
sim time next is 5067600.0000, 
raw observation next is [12.0, 19.0, 92.66666666666666, 718.3333333333334, 26.0, 28.26109420203811, 29.72575085486626, 1.0, 1.0, 0.9375273568617888], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.19, 0.3088888888888889, 0.7937384898710866, 1.0, 1.3230134574340155, 0.2972575085486626, 1.0, 1.0, 0.006696623977584206], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.294422], dtype=float32), -0.3843042]. 
=============================================
[2019-04-01 17:17:28,786] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:28,786] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:28,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run13
[2019-04-01 17:17:28,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:28,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:28,831] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run13
[2019-04-01 17:17:28,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:28,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:28,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run13
[2019-04-01 17:17:29,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:29,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:29,057] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run13
[2019-04-01 17:17:29,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:29,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:29,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run13
[2019-04-01 17:17:29,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:29,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:29,366] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run13
[2019-04-01 17:17:29,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:29,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:29,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run13
[2019-04-01 17:17:30,446] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:30,446] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:30,463] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run13
[2019-04-01 17:17:31,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:17:31,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:17:31,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run13
[2019-04-01 17:17:39,084] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8295085e-26 4.8777356e-24 2.3569540e-33 2.2362783e-22 5.3633744e-04
 6.3035017e-01 3.6911342e-01], sum to 1.0000
[2019-04-01 17:17:39,084] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0193
[2019-04-01 17:17:39,146] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 61.0, 145.0, 231.9999999999999, 26.0, 25.99029759763395, 10.12944964272775, 1.0, 1.0, 25.64723096074252], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 137400.0000, 
sim time next is 138000.0000, 
raw observation next is [-6.700000000000001, 61.0, 146.5, 169.0, 26.0, 25.98598423403662, 9.967829895003854, 1.0, 1.0, 24.19220045429043], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.48833333333333334, 0.1867403314917127, 1.0, 0.9979977477195173, 0.09967829895003855, 1.0, 1.0, 0.1728014318163602], 
reward next is 0.8272, 
noisyNet noise sample is [array([-0.1975856], dtype=float32), -1.7404864]. 
=============================================
[2019-04-01 17:17:39,151] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[39.253082]
 [38.70571 ]
 [38.179512]
 [37.78324 ]
 [37.37671 ]], R is [[40.23318863]
 [40.59588242]
 [40.8830719 ]
 [41.13845062]
 [41.39078903]].
[2019-04-01 17:17:43,783] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:17:43,784] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7503
[2019-04-01 17:17:43,827] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.2, 61.0, 139.0, 504.6666666666667, 26.0, 25.77558408749712, 8.977802237684543, 1.0, 1.0, 30.64234479270688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 130800.0000, 
sim time next is 131400.0000, 
raw observation next is [-8.1, 61.0, 130.0, 603.0, 26.0, 25.67280109206044, 8.838356817909933, 1.0, 1.0, 27.17711909654282], 
processed observation next is [1.0, 0.5217391304347826, 0.23822714681440446, 0.61, 0.43333333333333335, 0.6662983425414365, 1.0, 0.953257298865777, 0.08838356817909933, 1.0, 1.0, 0.19412227926102016], 
reward next is 0.8059, 
noisyNet noise sample is [array([0.71317756], dtype=float32), 2.16719]. 
=============================================
[2019-04-01 17:17:54,700] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0850803e-33 0.0000000e+00 0.0000000e+00 9.4786799e-38 1.5438237e-11
 3.5993850e-03 9.9640059e-01], sum to 1.0000
[2019-04-01 17:17:54,701] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3859
[2019-04-01 17:17:54,794] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.133333333333334, 64.0, 145.6666666666667, 0.0, 26.0, 25.1655418775925, 6.957363541677417, 1.0, 1.0, 34.21215867781086], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 220800.0000, 
sim time next is 221400.0000, 
raw observation next is [-3.95, 63.5, 149.0, 0.0, 26.0, 24.95207140305884, 6.914185078636829, 1.0, 1.0, 53.24549998662033], 
processed observation next is [1.0, 0.5652173913043478, 0.3531855955678671, 0.635, 0.49666666666666665, 0.0, 1.0, 0.8502959147226916, 0.0691418507863683, 1.0, 1.0, 0.3803249999044309], 
reward next is 0.6197, 
noisyNet noise sample is [array([-2.0290356], dtype=float32), 0.65030104]. 
=============================================
[2019-04-01 17:17:56,458] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.1590348e-36 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 17:17:56,458] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7866
[2019-04-01 17:17:56,481] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 77.33333333333334, 0.0, 0.0, 26.0, 24.43137970177805, 6.077554281318991, 0.0, 1.0, 43.56025680851187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 253200.0000, 
sim time next is 253800.0000, 
raw observation next is [-3.9, 78.5, 0.0, 0.0, 26.0, 24.39854778952397, 6.019534284301706, 0.0, 1.0, 43.56912498028985], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.785, 0.0, 0.0, 1.0, 0.7712211127891386, 0.06019534284301706, 0.0, 1.0, 0.3112080355734989], 
reward next is 0.6888, 
noisyNet noise sample is [array([2.1390476], dtype=float32), -0.5579766]. 
=============================================
[2019-04-01 17:17:56,618] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4203850e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.8293573e-36
 1.2126281e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 17:17:56,618] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3264
[2019-04-01 17:17:56,660] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 62.5, 30.66666666666666, 0.0, 26.0, 25.54783609145973, 7.917158911736808, 1.0, 1.0, 26.86855275214886], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 231000.0000, 
sim time next is 231600.0000, 
raw observation next is [-3.4, 63.0, 24.33333333333333, 0.0, 26.0, 25.45846303278404, 7.306053459929427, 1.0, 1.0, 28.01854737323323], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.63, 0.08111111111111109, 0.0, 1.0, 0.9226375761120055, 0.07306053459929428, 1.0, 1.0, 0.2001324812373802], 
reward next is 0.7999, 
noisyNet noise sample is [array([-0.10078081], dtype=float32), -0.8163472]. 
=============================================
[2019-04-01 17:18:03,309] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9230664e-22 8.6763445e-30 3.9971883e-15 1.2943746e-30 1.6241587e-24
 9.9997866e-01 2.1285086e-05], sum to 1.0000
[2019-04-01 17:18:03,310] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8331
[2019-04-01 17:18:03,323] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-14.83333333333333, 69.0, 0.0, 0.0, 26.0, 23.26010711549903, 5.781190802840629, 0.0, 1.0, 47.42084900503455], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 351600.0000, 
sim time next is 352200.0000, 
raw observation next is [-14.91666666666667, 69.0, 0.0, 0.0, 26.0, 23.27218394869478, 5.796129226154785, 0.0, 1.0, 47.57658006214069], 
processed observation next is [1.0, 0.043478260869565216, 0.04939981532779307, 0.69, 0.0, 0.0, 1.0, 0.610311992670683, 0.05796129226154785, 0.0, 1.0, 0.3398327147295764], 
reward next is 0.6602, 
noisyNet noise sample is [array([0.41937613], dtype=float32), 0.07592185]. 
=============================================
[2019-04-01 17:18:26,824] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 8.8201059e-26 0.0000000e+00 6.6564076e-37
 1.3404996e-04 9.9986589e-01], sum to 1.0000
[2019-04-01 17:18:26,824] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4065
[2019-04-01 17:18:26,844] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.29725028631274, 5.554248092202767, 0.0, 1.0, 41.68023464774035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 612000.0000, 
sim time next is 612600.0000, 
raw observation next is [-3.9, 84.16666666666667, 0.0, 0.0, 26.0, 24.26245802055632, 5.518827587940059, 0.0, 1.0, 41.74915843904629], 
processed observation next is [0.0, 0.08695652173913043, 0.3545706371191136, 0.8416666666666667, 0.0, 0.0, 1.0, 0.7517797172223314, 0.05518827587940059, 0.0, 1.0, 0.29820827456461635], 
reward next is 0.7018, 
noisyNet noise sample is [array([-0.02695299], dtype=float32), -0.87364674]. 
=============================================
[2019-04-01 17:18:29,230] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4798564e-37 0.0000000e+00 2.5825763e-25 0.0000000e+00 0.0000000e+00
 2.7206520e-06 9.9999726e-01], sum to 1.0000
[2019-04-01 17:18:29,232] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7201
[2019-04-01 17:18:29,282] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 57.00000000000001, 0.0, 0.0, 26.0, 24.97428633850138, 7.021409668154415, 0.0, 1.0, 37.11369420737091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 667200.0000, 
sim time next is 667800.0000, 
raw observation next is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.96126681353896, 6.942348243166244, 0.0, 1.0, 33.19165155336239], 
processed observation next is [0.0, 0.7391304347826086, 0.42936288088642666, 0.57, 0.0, 0.0, 1.0, 0.8516095447912798, 0.06942348243166245, 0.0, 1.0, 0.23708322538115995], 
reward next is 0.7629, 
noisyNet noise sample is [array([-0.2805699], dtype=float32), -0.9897132]. 
=============================================
[2019-04-01 17:18:38,336] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0619561e-18 4.6176770e-28 5.2170000e-12 1.1496210e-25 3.1057052e-18
 2.6729446e-02 9.7327054e-01], sum to 1.0000
[2019-04-01 17:18:38,339] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3015
[2019-04-01 17:18:38,351] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.466666666666667, 71.0, 0.0, 0.0, 26.0, 24.1985714980155, 5.56075452001765, 0.0, 1.0, 41.06385946474403], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 782400.0000, 
sim time next is 783000.0000, 
raw observation next is [-7.55, 71.0, 0.0, 0.0, 26.0, 24.18645975675103, 5.535119498949165, 0.0, 1.0, 41.03539331550929], 
processed observation next is [1.0, 0.043478260869565216, 0.25346260387811637, 0.71, 0.0, 0.0, 1.0, 0.7409228223930044, 0.055351194989491655, 0.0, 1.0, 0.2931099522536378], 
reward next is 0.7069, 
noisyNet noise sample is [array([0.5415391], dtype=float32), 1.8925843]. 
=============================================
[2019-04-01 17:18:38,357] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[69.57772 ]
 [69.663284]
 [69.736694]
 [69.81757 ]
 [69.78909 ]], R is [[69.50727844]
 [69.51889801]
 [69.53041077]
 [69.54171753]
 [69.55298615]].
[2019-04-01 17:18:48,291] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:18:48,291] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7494
[2019-04-01 17:18:48,310] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 98.66666666666667, 0.0, 0.0, 26.0, 25.04083868620403, 8.676807652695494, 0.0, 1.0, 40.19042019899707], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 940800.0000, 
sim time next is 941400.0000, 
raw observation next is [5.0, 98.0, 0.0, 0.0, 26.0, 25.09310080132659, 8.772388454182414, 0.0, 1.0, 39.95622438317078], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.98, 0.0, 0.0, 1.0, 0.8704429716180841, 0.08772388454182414, 0.0, 1.0, 0.2854016027369341], 
reward next is 0.7146, 
noisyNet noise sample is [array([-0.5876278], dtype=float32), 0.45500243]. 
=============================================
[2019-04-01 17:18:50,013] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9685344e-20
 1.0000000e+00 1.8611227e-33], sum to 1.0000
[2019-04-01 17:18:50,014] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2728
[2019-04-01 17:18:50,024] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.5, 93.0, 78.0, 0.0, 26.0, 26.47383632065329, 13.58813896269289, 1.0, 1.0, 10.62643398798066], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 986400.0000, 
sim time next is 987000.0000, 
raw observation next is [10.68333333333333, 91.83333333333333, 84.0, 0.0, 26.0, 26.52782111557137, 13.87963505618061, 1.0, 1.0, 9.940965767721606], 
processed observation next is [1.0, 0.43478260869565216, 0.7585410895660203, 0.9183333333333333, 0.28, 0.0, 1.0, 1.0754030165101958, 0.13879635056180611, 1.0, 1.0, 0.07100689834086861], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5142376], dtype=float32), 0.46829936]. 
=============================================
[2019-04-01 17:18:50,045] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[75.86317 ]
 [76.043495]
 [76.68589 ]
 [77.48707 ]
 [78.35117 ]], R is [[75.2341156 ]
 [74.48177338]
 [73.73695374]
 [72.99958801]
 [72.26959229]].
[2019-04-01 17:18:51,775] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 2.4971502e-29 0.0000000e+00 0.0000000e+00
 1.0000000e+00 9.9999488e-11], sum to 1.0000
[2019-04-01 17:18:51,779] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6841
[2019-04-01 17:18:51,794] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.05, 85.5, 0.0, 0.0, 26.0, 25.39042781277573, 9.374225223918017, 0.0, 1.0, 37.70314270267329], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 955800.0000, 
sim time next is 956400.0000, 
raw observation next is [6.233333333333333, 84.33333333333334, 0.0, 0.0, 26.0, 25.40026272593415, 9.624534685047536, 0.0, 1.0, 36.71726861566024], 
processed observation next is [1.0, 0.043478260869565216, 0.6352723915050786, 0.8433333333333334, 0.0, 0.0, 1.0, 0.9143232465620217, 0.09624534685047535, 0.0, 1.0, 0.26226620439757314], 
reward next is 0.7377, 
noisyNet noise sample is [array([-0.68872696], dtype=float32), 1.1261036]. 
=============================================
[2019-04-01 17:18:51,916] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 3.212534e-24], sum to 1.0000
[2019-04-01 17:18:51,920] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0554
[2019-04-01 17:18:51,937] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.33333333333333, 92.66666666666667, 66.0, 0.0, 26.0, 26.41152323237349, 13.09632429399176, 1.0, 1.0, 12.09559769279167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 985200.0000, 
sim time next is 985800.0000, 
raw observation next is [10.41666666666667, 92.83333333333333, 72.0, 0.0, 26.0, 26.4337528274414, 13.27145993923149, 1.0, 1.0, 11.39655860168827], 
processed observation next is [1.0, 0.391304347826087, 0.7511542012927056, 0.9283333333333332, 0.24, 0.0, 1.0, 1.0619646896344856, 0.1327145993923149, 1.0, 1.0, 0.08140399001205907], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.35014144], dtype=float32), -0.34612358]. 
=============================================
[2019-04-01 17:18:54,632] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 5.6784739e-38 0.0000000e+00 0.0000000e+00
 1.5842541e-07 9.9999988e-01], sum to 1.0000
[2019-04-01 17:18:54,633] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7491
[2019-04-01 17:18:54,644] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.8, 69.33333333333333, 0.0, 0.0, 26.0, 25.61836963276745, 13.6504114354036, 0.0, 1.0, 20.79434658069888], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1122000.0000, 
sim time next is 1122600.0000, 
raw observation next is [11.7, 70.16666666666667, 0.0, 0.0, 26.0, 25.58857510090334, 13.99777183237163, 0.0, 1.0, 27.68683596054052], 
processed observation next is [1.0, 1.0, 0.7867036011080333, 0.7016666666666667, 0.0, 0.0, 1.0, 0.941225014414763, 0.1399777183237163, 0.0, 1.0, 0.19776311400386085], 
reward next is 0.8022, 
noisyNet noise sample is [array([1.2510154], dtype=float32), 0.9076034]. 
=============================================
[2019-04-01 17:18:54,772] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.4784063e-35 0.0000000e+00 3.1221994e-19 0.0000000e+00 4.2116783e-35
 6.0875580e-04 9.9939120e-01], sum to 1.0000
[2019-04-01 17:18:54,775] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2391
[2019-04-01 17:18:54,785] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.05, 74.0, 0.0, 0.0, 26.0, 25.73871364190127, 14.61703878188436, 0.0, 1.0, 19.44087111366319], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1125000.0000, 
sim time next is 1125600.0000, 
raw observation next is [10.86666666666667, 75.0, 0.0, 0.0, 26.0, 25.76778001547642, 14.48171548233775, 0.0, 1.0, 18.32708434237479], 
processed observation next is [0.0, 0.0, 0.7636195752539245, 0.75, 0.0, 0.0, 1.0, 0.9668257164966312, 0.1448171548233775, 0.0, 1.0, 0.13090774530267707], 
reward next is 0.8691, 
noisyNet noise sample is [array([0.5398715], dtype=float32), 0.6384302]. 
=============================================
[2019-04-01 17:18:55,506] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.9522381e-30 3.1194137e-38 2.4498246e-19 1.3709690e-36 6.2981498e-10
 1.1738230e-01 8.8261771e-01], sum to 1.0000
[2019-04-01 17:18:55,507] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5049
[2019-04-01 17:18:55,514] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.9, 77.83333333333333, 0.0, 0.0, 26.0, 25.72156296175651, 12.0450415714459, 0.0, 1.0, 12.21071506445096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1054200.0000, 
sim time next is 1054800.0000, 
raw observation next is [13.8, 78.0, 0.0, 0.0, 26.0, 25.66219954960224, 11.79239506565898, 0.0, 1.0, 11.65005848692695], 
processed observation next is [1.0, 0.21739130434782608, 0.844875346260388, 0.78, 0.0, 0.0, 1.0, 0.9517427928003198, 0.1179239506565898, 0.0, 1.0, 0.08321470347804964], 
reward next is 0.9168, 
noisyNet noise sample is [array([0.16337928], dtype=float32), 1.9216504]. 
=============================================
[2019-04-01 17:18:59,513] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 7.3925999e-32 0.0000000e+00 0.0000000e+00
 7.2231195e-08 9.9999988e-01], sum to 1.0000
[2019-04-01 17:18:59,514] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9512
[2019-04-01 17:18:59,521] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.51666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 24.54148286964565, 8.1606801965441, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1199400.0000, 
sim time next is 1200000.0000, 
raw observation next is [17.33333333333334, 69.66666666666667, 0.0, 0.0, 26.0, 24.51643886539405, 8.077494228776876, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.9427516158818101, 0.6966666666666668, 0.0, 0.0, 1.0, 0.7880626950562929, 0.08077494228776877, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.092651], dtype=float32), 1.0405462]. 
=============================================
[2019-04-01 17:18:59,533] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[99.85875 ]
 [99.8913  ]
 [99.86439 ]
 [99.884895]
 [99.89682 ]], R is [[99.77740479]
 [99.77963257]
 [99.78183746]
 [99.78401947]
 [99.78617859]].
[2019-04-01 17:19:05,072] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0794030e-23 3.1093663e-29 1.7140868e-13 8.9992321e-31 1.3905544e-26
 1.0000000e+00 4.7041895e-09], sum to 1.0000
[2019-04-01 17:19:05,072] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5179
[2019-04-01 17:19:05,086] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.633333333333334, 92.66666666666667, 0.0, 0.0, 26.0, 25.69071480330167, 12.14715583106736, 0.0, 1.0, 26.70206861852481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1300800.0000, 
sim time next is 1301400.0000, 
raw observation next is [3.55, 92.5, 0.0, 0.0, 26.0, 25.57351086428708, 11.69434889615898, 0.0, 1.0, 23.6621400596233], 
processed observation next is [1.0, 0.043478260869565216, 0.5609418282548477, 0.925, 0.0, 0.0, 1.0, 0.9390729806124399, 0.11694348896158979, 0.0, 1.0, 0.16901528614016645], 
reward next is 0.8310, 
noisyNet noise sample is [array([1.4140255], dtype=float32), 0.5267325]. 
=============================================
[2019-04-01 17:19:07,657] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.3359210e-34 0.0000000e+00 1.7264544e-23 0.0000000e+00 0.0000000e+00
 1.0000000e+00 5.1319571e-14], sum to 1.0000
[2019-04-01 17:19:07,658] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5691
[2019-04-01 17:19:07,672] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.21813816311164, 9.82900310820564, 0.0, 1.0, 39.52345401417937], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1389600.0000, 
sim time next is 1390200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.20448909394713, 9.867366370695649, 0.0, 1.0, 39.3877270693642], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.8863555848495902, 0.09867366370695649, 0.0, 1.0, 0.28134090763831576], 
reward next is 0.7187, 
noisyNet noise sample is [array([2.0752656], dtype=float32), 1.678007]. 
=============================================
[2019-04-01 17:19:11,112] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0190033e-36 0.0000000e+00 3.0293183e-22 0.0000000e+00 0.0000000e+00
 5.1801926e-26 1.0000000e+00], sum to 1.0000
[2019-04-01 17:19:11,114] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0736
[2019-04-01 17:19:11,129] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.30599855749443, 9.251724928426247, 0.0, 1.0, 36.53233786708662], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1485000.0000, 
sim time next is 1485600.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.29148405908735, 9.177237455695073, 0.0, 1.0, 36.54805886783322], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 1.0, 0.8987834370124784, 0.09177237455695073, 0.0, 1.0, 0.26105756334166585], 
reward next is 0.7389, 
noisyNet noise sample is [array([0.31343612], dtype=float32), -0.22483326]. 
=============================================
[2019-04-01 17:19:19,095] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 5.321366e-30], sum to 1.0000
[2019-04-01 17:19:19,095] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2225
[2019-04-01 17:19:19,133] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 26.0, 27.51776556390263, 21.03668731190351, 1.0, 1.0, 6.558505232959135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1615800.0000, 
sim time next is 1616400.0000, 
raw observation next is [12.2, 54.0, 25.5, 18.5, 26.0, 27.4595732970142, 17.7570832050684, 1.0, 1.0, 6.759241493818349], 
processed observation next is [1.0, 0.7391304347826086, 0.8005540166204987, 0.54, 0.085, 0.020441988950276244, 1.0, 1.2085104710020285, 0.17757083205068402, 1.0, 1.0, 0.048280296384416777], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.67306656], dtype=float32), -0.15859595]. 
=============================================
[2019-04-01 17:19:24,804] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.5789984e-25 0.0000000e+00 2.9596923e-20
 9.9999726e-01 2.7890085e-06], sum to 1.0000
[2019-04-01 17:19:24,805] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3944
[2019-04-01 17:19:24,866] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 24.96435629806204, 7.891889729140492, 0.0, 1.0, 43.78914339353813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1791000.0000, 
sim time next is 1791600.0000, 
raw observation next is [-3.899999999999999, 82.0, 0.0, 0.0, 26.0, 24.93526282427132, 7.940445223225242, 0.0, 1.0, 47.78338066852992], 
processed observation next is [0.0, 0.7391304347826086, 0.35457063711911363, 0.82, 0.0, 0.0, 1.0, 0.847894689181617, 0.07940445223225243, 0.0, 1.0, 0.3413098619180709], 
reward next is 0.6587, 
noisyNet noise sample is [array([-2.2192829], dtype=float32), 2.0495946]. 
=============================================
[2019-04-01 17:19:26,400] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.4165263e-29 1.2201441e-31 1.6883873e-11 2.6830423e-32 1.6769004e-38
 3.3750985e-02 9.6624905e-01], sum to 1.0000
[2019-04-01 17:19:26,402] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4683
[2019-04-01 17:19:26,433] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 78.16666666666666, 0.0, 0.0, 26.0, 24.32514940375489, 5.792408800254802, 0.0, 1.0, 45.27390032487876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1817400.0000, 
sim time next is 1818000.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 26.0, 24.30840771972294, 5.746937379229024, 0.0, 1.0, 45.34411101801216], 
processed observation next is [0.0, 0.043478260869565216, 0.30747922437673136, 0.78, 0.0, 0.0, 1.0, 0.7583439599604199, 0.057469373792290245, 0.0, 1.0, 0.3238865072715154], 
reward next is 0.6761, 
noisyNet noise sample is [array([-0.9937938], dtype=float32), 0.43020758]. 
=============================================
[2019-04-01 17:19:26,439] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[62.871006]
 [62.748913]
 [62.539364]
 [62.364365]
 [62.143654]], R is [[63.0402832 ]
 [63.08649826]
 [63.13276291]
 [63.1790657 ]
 [63.22537231]].
[2019-04-01 17:19:30,579] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.6341660e-34 0.0000000e+00 0.0000000e+00
 1.7931848e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 17:19:30,582] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6151
[2019-04-01 17:19:30,599] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.583333333333333, 83.5, 0.0, 0.0, 26.0, 25.03156486101137, 7.672388524051688, 0.0, 1.0, 45.94898187383355], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1800600.0000, 
sim time next is 1801200.0000, 
raw observation next is [-4.666666666666667, 84.0, 0.0, 0.0, 26.0, 25.01294679126295, 7.588038114229145, 0.0, 1.0, 45.21339928901853], 
processed observation next is [0.0, 0.8695652173913043, 0.3333333333333333, 0.84, 0.0, 0.0, 1.0, 0.8589923987518502, 0.07588038114229145, 0.0, 1.0, 0.3229528520644181], 
reward next is 0.6770, 
noisyNet noise sample is [array([-0.12049466], dtype=float32), 0.8570666]. 
=============================================
[2019-04-01 17:19:32,614] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 3.5516708e-37 0.0000000e+00 0.0000000e+00
 3.4027181e-31 1.0000000e+00], sum to 1.0000
[2019-04-01 17:19:32,614] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7907
[2019-04-01 17:19:32,665] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 73.66666666666667, 173.3333333333333, 76.0, 26.0, 24.96559262969063, 6.865808693229607, 0.0, 1.0, 42.06196764683492], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1855200.0000, 
sim time next is 1855800.0000, 
raw observation next is [-5.3, 73.0, 184.0, 81.0, 26.0, 24.89406936241725, 6.895496218996996, 0.0, 1.0, 49.15337851510468], 
processed observation next is [0.0, 0.4782608695652174, 0.31578947368421056, 0.73, 0.6133333333333333, 0.08950276243093923, 1.0, 0.8420099089167502, 0.06895496218996996, 0.0, 1.0, 0.35109556082217624], 
reward next is 0.6489, 
noisyNet noise sample is [array([1.2830814], dtype=float32), -1.6587406]. 
=============================================
[2019-04-01 17:19:42,410] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00
 5.52171e-21], sum to 1.0000
[2019-04-01 17:19:42,411] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1833
[2019-04-01 17:19:42,472] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 79.0, 0.0, 0.0, 26.0, 26.00089272708409, 9.762563151443253, 1.0, 1.0, 31.11862586635658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1965600.0000, 
sim time next is 1966200.0000, 
raw observation next is [-4.916666666666667, 77.66666666666667, 0.0, 0.0, 26.0, 25.9602940436392, 9.537565871711912, 1.0, 1.0, 27.31611332021115], 
processed observation next is [1.0, 0.782608695652174, 0.32640812557710064, 0.7766666666666667, 0.0, 0.0, 1.0, 0.9943277205198855, 0.09537565871711912, 1.0, 1.0, 0.19511509514436537], 
reward next is 0.8049, 
noisyNet noise sample is [array([-1.1706442], dtype=float32), -0.07033033]. 
=============================================
[2019-04-01 17:19:58,611] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 6.7241814e-36 0.0000000e+00 0.0000000e+00
 9.4620073e-01 5.3799253e-02], sum to 1.0000
[2019-04-01 17:19:58,613] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9888
[2019-04-01 17:19:58,659] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.516666666666667, 83.0, 0.0, 0.0, 26.0, 25.16719524238106, 8.684116203867106, 0.0, 1.0, 41.78409424768166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2152200.0000, 
sim time next is 2152800.0000, 
raw observation next is [-6.7, 83.0, 0.0, 0.0, 26.0, 25.15485798799743, 8.599946459156158, 0.0, 1.0, 41.65304752309405], 
processed observation next is [1.0, 0.9565217391304348, 0.2770083102493075, 0.83, 0.0, 0.0, 1.0, 0.8792654268567759, 0.08599946459156158, 0.0, 1.0, 0.2975217680221004], 
reward next is 0.7025, 
noisyNet noise sample is [array([0.49386972], dtype=float32), 3.0516102]. 
=============================================
[2019-04-01 17:20:09,203] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.4247539e-17 0.0000000e+00 2.8836882e-31
 2.6794621e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 17:20:09,204] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0967
[2019-04-01 17:20:09,220] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 13.66666666666666, 0.0, 26.0, 23.97212379143214, 5.311076964195967, 0.0, 1.0, 41.2829187998824], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2361000.0000, 
sim time next is 2361600.0000, 
raw observation next is [-3.4, 69.0, 19.5, 0.0, 26.0, 23.94320916482754, 5.436449131780063, 0.0, 1.0, 41.95789959182169], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.065, 0.0, 1.0, 0.7061727378325057, 0.05436449131780063, 0.0, 1.0, 0.2996992827987264], 
reward next is 0.7003, 
noisyNet noise sample is [array([-0.6417871], dtype=float32), -0.18317002]. 
=============================================
[2019-04-01 17:20:11,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 3.4041143e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 17:20:11,169] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8433
[2019-04-01 17:20:11,181] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 62.5, 0.0, 0.0, 26.0, 24.86267984760443, 6.914592176487067, 0.0, 1.0, 38.07134462510419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2335800.0000, 
sim time next is 2336400.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.83409407770485, 6.965865868028634, 0.0, 1.0, 38.08087951768473], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 1.0, 0.8334420111006929, 0.06965865868028634, 0.0, 1.0, 0.2720062822691766], 
reward next is 0.7280, 
noisyNet noise sample is [array([0.9615263], dtype=float32), 1.0531268]. 
=============================================
[2019-04-01 17:20:14,551] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 7.0471606e-31 0.0000000e+00 0.0000000e+00
 5.8595942e-25 1.0000000e+00], sum to 1.0000
[2019-04-01 17:20:14,555] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9103
[2019-04-01 17:20:14,571] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 57.0, 0.0, 0.0, 26.0, 23.77412711091322, 5.287576923617014, 0.0, 1.0, 43.48947957087827], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2431200.0000, 
sim time next is 2431800.0000, 
raw observation next is [-8.1, 58.0, 0.0, 0.0, 26.0, 23.72704934446385, 5.299449954309376, 0.0, 1.0, 43.56665296248409], 
processed observation next is [0.0, 0.13043478260869565, 0.23822714681440446, 0.58, 0.0, 0.0, 1.0, 0.6752927634948358, 0.05299449954309376, 0.0, 1.0, 0.3111903783034578], 
reward next is 0.6888, 
noisyNet noise sample is [array([1.4923303], dtype=float32), -0.21293713]. 
=============================================
[2019-04-01 17:20:23,578] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-01 17:20:23,590] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 17:20:23,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:20:23,590] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 17:20:23,590] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 17:20:23,590] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:20:23,591] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:20:23,594] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run18
[2019-04-01 17:20:23,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run18
[2019-04-01 17:20:23,630] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run18
[2019-04-01 17:21:02,769] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.5754906], dtype=float32), -0.589364]
[2019-04-01 17:21:02,769] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.1, 92.0, 82.66666666666666, 0.0, 26.0, 25.81386095972889, 11.57837645618012, 1.0, 1.0, 10.13241260737102]
[2019-04-01 17:21:02,769] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 17:21:02,770] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.1273471e-14], sampled 0.1322524522311217
[2019-04-01 17:22:05,028] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 17:22:07,632] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.5754906], dtype=float32), -0.589364]
[2019-04-01 17:22:07,632] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.5, 37.5, 0.0, 0.0, 26.0, 25.44666063283586, 7.655885721860322, 1.0, 1.0, 43.46117356131369]
[2019-04-01 17:22:07,632] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 17:22:07,633] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.00000000e+00 0.00000000e+00 7.22669770e-32 0.00000000e+00
 0.00000000e+00 1.10179266e-16 1.00000000e+00], sampled 0.5401109194256033
[2019-04-01 17:22:25,021] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 17:22:27,512] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 17:22:28,536] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1700000, evaluation results [1700000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 17:22:30,234] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7968134e-38 0.0000000e+00 1.3357209e-24 0.0000000e+00 6.0150748e-34
 1.7253351e-03 9.9827468e-01], sum to 1.0000
[2019-04-01 17:22:30,234] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1313
[2019-04-01 17:22:30,247] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 73.0, 0.0, 0.0, 26.0, 24.89065927479994, 6.621059553264165, 0.0, 1.0, 41.09449065196403], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2598600.0000, 
sim time next is 2599200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.9687635727798, 6.56507754116862, 0.0, 1.0, 41.04175203131194], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 1.0, 0.8526805103971142, 0.0656507754116862, 0.0, 1.0, 0.2931553716522281], 
reward next is 0.7068, 
noisyNet noise sample is [array([0.21067388], dtype=float32), -1.0134214]. 
=============================================
[2019-04-01 17:22:37,202] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 6.5520803e-32 0.0000000e+00 0.0000000e+00
 1.7622979e-25 1.0000000e+00], sum to 1.0000
[2019-04-01 17:22:37,203] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2308
[2019-04-01 17:22:37,219] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 98.83333333333334, 0.0, 0.0, 26.0, 24.97447875877472, 6.632160698885042, 0.0, 1.0, 54.37703538341984], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2873400.0000, 
sim time next is 2874000.0000, 
raw observation next is [1.333333333333333, 97.66666666666667, 0.0, 0.0, 26.0, 24.92491663090127, 6.556349156065996, 0.0, 1.0, 54.33994058280314], 
processed observation next is [1.0, 0.2608695652173913, 0.4995383194829178, 0.9766666666666667, 0.0, 0.0, 1.0, 0.8464166615573241, 0.06556349156065995, 0.0, 1.0, 0.38814243273430815], 
reward next is 0.6119, 
noisyNet noise sample is [array([-0.60516804], dtype=float32), 1.0369251]. 
=============================================
[2019-04-01 17:22:37,228] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[75.56131 ]
 [75.529366]
 [75.494484]
 [75.49663 ]
 [75.463715]], R is [[75.44038391]
 [75.2975769 ]
 [75.15636444]
 [75.01825714]
 [74.87283325]].
[2019-04-01 17:22:39,298] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 4.1343560e-34 0.0000000e+00 7.8530806e-35
 6.2628224e-02 9.3737185e-01], sum to 1.0000
[2019-04-01 17:22:39,307] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7872
[2019-04-01 17:22:39,342] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.54483436133037, 5.855590415340681, 0.0, 1.0, 40.20849411247497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2778600.0000, 
sim time next is 2779200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.50951150991022, 5.814860777141891, 0.0, 1.0, 40.23518218995542], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.59, 0.0, 0.0, 1.0, 0.7870730728443169, 0.058148607771418906, 0.0, 1.0, 0.28739415849968153], 
reward next is 0.7126, 
noisyNet noise sample is [array([0.11378291], dtype=float32), 0.607804]. 
=============================================
[2019-04-01 17:22:48,296] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8006272e-37 0.0000000e+00 7.3368621e-32 0.0000000e+00 0.0000000e+00
 5.1913154e-09 1.0000000e+00], sum to 1.0000
[2019-04-01 17:22:48,298] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5034
[2019-04-01 17:22:48,324] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 95.33333333333334, 0.0, 0.0, 26.0, 24.89291518557912, 6.827407333166086, 0.0, 1.0, 55.51604564682735], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2870400.0000, 
sim time next is 2871000.0000, 
raw observation next is [1.0, 96.5, 0.0, 0.0, 26.0, 25.02320159952483, 6.827318713095441, 0.0, 1.0, 54.14396659789268], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.965, 0.0, 0.0, 1.0, 0.8604573713606898, 0.06827318713095441, 0.0, 1.0, 0.38674261855637626], 
reward next is 0.6133, 
noisyNet noise sample is [array([-0.8147173], dtype=float32), -0.52609307]. 
=============================================
[2019-04-01 17:22:48,329] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[63.4626  ]
 [63.56791 ]
 [63.67748 ]
 [63.80118 ]
 [63.888668]], R is [[63.41328049]
 [63.38260651]
 [63.35799026]
 [63.33329391]
 [63.3090744 ]].
[2019-04-01 17:22:52,171] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 2.6891441e-31 0.0000000e+00 4.5000642e-33
 7.0845946e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 17:22:52,172] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1802
[2019-04-01 17:22:52,192] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 84.83333333333334, 0.0, 0.0, 26.0, 24.718483698066, 7.076758534693146, 0.0, 1.0, 42.52017610070814], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2945400.0000, 
sim time next is 2946000.0000, 
raw observation next is [-2.333333333333333, 84.66666666666667, 0.0, 0.0, 26.0, 24.71246879517164, 6.988616870898357, 0.0, 1.0, 42.44629840443859], 
processed observation next is [0.0, 0.08695652173913043, 0.3979686057248385, 0.8466666666666667, 0.0, 0.0, 1.0, 0.816066970738806, 0.06988616870898358, 0.0, 1.0, 0.30318784574598995], 
reward next is 0.6968, 
noisyNet noise sample is [array([1.003461], dtype=float32), -0.60733056]. 
=============================================
[2019-04-01 17:22:52,197] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[65.82481 ]
 [65.70593 ]
 [65.35825 ]
 [65.0237  ]
 [64.820984]], R is [[66.0728302 ]
 [66.10838318]
 [66.14325714]
 [66.17760468]
 [66.21116638]].
[2019-04-01 17:22:58,085] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0878713e-30 0.0000000e+00 2.8835495e-13 0.0000000e+00 0.0000000e+00
 2.4555381e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 17:22:58,087] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3475
[2019-04-01 17:22:58,100] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 100.0, 0.0, 0.0, 26.0, 25.39978473682423, 7.775877657096122, 0.0, 1.0, 38.17730923196653], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3104400.0000, 
sim time next is 3105000.0000, 
raw observation next is [-0.5, 100.0, 0.0, 0.0, 26.0, 25.39818032414795, 7.77388690384057, 0.0, 1.0, 38.2486621711371], 
processed observation next is [0.0, 0.9565217391304348, 0.44875346260387816, 1.0, 0.0, 0.0, 1.0, 0.9140257605925645, 0.0777388690384057, 0.0, 1.0, 0.2732047297938364], 
reward next is 0.7268, 
noisyNet noise sample is [array([-0.06563146], dtype=float32), -0.9872148]. 
=============================================
[2019-04-01 17:22:58,106] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[75.32103]
 [75.32716]
 [75.29993]
 [75.30792]
 [75.30345]], R is [[75.31420135]
 [75.2883606 ]
 [75.25450897]
 [75.21244812]
 [75.16953278]].
[2019-04-01 17:22:58,476] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.3981360e-38 0.0000000e+00 0.0000000e+00
 3.2928497e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 17:22:58,479] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1656
[2019-04-01 17:22:58,512] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3, 77.0, 7.0, 88.0, 26.0, 25.0706505239889, 7.617858150127065, 0.0, 1.0, 23.46769856520395], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3087000.0000, 
sim time next is 3087600.0000, 
raw observation next is [-0.4, 78.66666666666667, 0.0, 0.0, 26.0, 25.04907063467796, 7.45518069147582, 0.0, 1.0, 24.40329927104895], 
processed observation next is [0.0, 0.7391304347826086, 0.45152354570637127, 0.7866666666666667, 0.0, 0.0, 1.0, 0.8641529478111372, 0.0745518069147582, 0.0, 1.0, 0.1743092805074925], 
reward next is 0.8257, 
noisyNet noise sample is [array([0.65827274], dtype=float32), -0.035402723]. 
=============================================
[2019-04-01 17:23:00,914] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 8.8454302e-26 0.0000000e+00 0.0000000e+00
 4.8949685e-02 9.5105028e-01], sum to 1.0000
[2019-04-01 17:23:00,914] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5240
[2019-04-01 17:23:00,961] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9333333333333333, 90.33333333333334, 0.0, 0.0, 26.0, 25.01962139873007, 7.469614910065886, 0.0, 1.0, 31.99351509607596], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3091800.0000, 
sim time next is 3092400.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.03035773713267, 7.422727553567275, 0.0, 1.0, 30.36086107032738], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 1.0, 0.8614796767332388, 0.07422727553567275, 0.0, 1.0, 0.2168632933594813], 
reward next is 0.7831, 
noisyNet noise sample is [array([1.6919566], dtype=float32), 1.2145835]. 
=============================================
[2019-04-01 17:23:04,236] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 7.7749906e-35 2.0871732e-35
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:23:04,239] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7115
[2019-04-01 17:23:04,254] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 80.66666666666666, 98.33333333333334, 755.1666666666667, 26.0, 25.94737198071456, 15.8296123528396, 1.0, 1.0, 17.561763890133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3249600.0000, 
sim time next is 3250200.0000, 
raw observation next is [-2.333333333333333, 75.83333333333334, 95.66666666666667, 741.3333333333334, 26.0, 26.25384044698031, 17.08902100185602, 1.0, 1.0, 12.30350732829914], 
processed observation next is [1.0, 0.6086956521739131, 0.3979686057248385, 0.7583333333333334, 0.3188888888888889, 0.8191528545119706, 1.0, 1.036262920997187, 0.1708902100185602, 1.0, 1.0, 0.08788219520213672], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.20092148], dtype=float32), 0.116365574]. 
=============================================
[2019-04-01 17:23:05,955] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.7128546e-35 0.0000000e+00 4.5711162e-31 0.0000000e+00 1.7744050e-30
 9.9995589e-01 4.4050386e-05], sum to 1.0000
[2019-04-01 17:23:05,956] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4189
[2019-04-01 17:23:05,968] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.166666666666667, 100.0, 0.0, 0.0, 26.0, 26.08187545129584, 17.17948089209661, 0.0, 1.0, 22.35540718632825], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3189000.0000, 
sim time next is 3189600.0000, 
raw observation next is [2.0, 100.0, 0.0, 0.0, 26.0, 26.10435501658756, 16.80290896282049, 0.0, 1.0, 21.1928737129565], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 1.0, 0.0, 0.0, 1.0, 1.0149078595125087, 0.1680290896282049, 0.0, 1.0, 0.15137766937826072], 
reward next is 0.8486, 
noisyNet noise sample is [array([-0.98889023], dtype=float32), -0.6981879]. 
=============================================
[2019-04-01 17:23:23,467] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.8476080e-37 2.9568995e-29 0.0000000e+00 1.3843464e-33
 2.5819807e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 17:23:23,467] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4666
[2019-04-01 17:23:23,514] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 59.5, 111.0, 758.0, 26.0, 25.62090838596404, 10.45397805927974, 0.0, 1.0, 23.00814977471652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3580200.0000, 
sim time next is 3580800.0000, 
raw observation next is [-4.333333333333334, 57.66666666666667, 111.5, 767.6666666666667, 26.0, 25.54854862430648, 10.25767981976363, 0.0, 1.0, 21.6270898248518], 
processed observation next is [0.0, 0.43478260869565216, 0.3425669436749769, 0.5766666666666667, 0.37166666666666665, 0.8482504604051566, 1.0, 0.9355069463294973, 0.1025767981976363, 0.0, 1.0, 0.15447921303465573], 
reward next is 0.8455, 
noisyNet noise sample is [array([1.0513067], dtype=float32), -0.9915436]. 
=============================================
[2019-04-01 17:23:26,007] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 2.4809329e-36 6.2618457e-25 0.0000000e+00 8.9069590e-36
 7.9099809e-06 9.9999213e-01], sum to 1.0000
[2019-04-01 17:23:26,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5743
[2019-04-01 17:23:26,036] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 40.33333333333334, 54.83333333333334, 452.8333333333334, 26.0, 25.29564724044039, 9.490061548630052, 0.0, 1.0, 9.667569124961283], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3602400.0000, 
sim time next is 3603000.0000, 
raw observation next is [0.0, 39.66666666666666, 46.66666666666667, 390.6666666666667, 26.0, 25.26418588154124, 9.250079011176886, 0.0, 1.0, 9.983466849859202], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.39666666666666656, 0.15555555555555556, 0.43167587476979746, 1.0, 0.8948836973630344, 0.09250079011176887, 0.0, 1.0, 0.0713104774989943], 
reward next is 0.9287, 
noisyNet noise sample is [array([-0.37666124], dtype=float32), 1.3817441]. 
=============================================
[2019-04-01 17:23:26,041] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[76.19813 ]
 [75.93104 ]
 [75.727234]
 [75.503365]
 [75.24938 ]], R is [[76.67861938]
 [76.84278107]
 [77.00805664]
 [77.17175293]
 [77.32997894]].
[2019-04-01 17:23:29,103] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 4.2762488e-36 0.0000000e+00 5.2793292e-38
 1.4316694e-37 1.0000000e+00], sum to 1.0000
[2019-04-01 17:23:29,104] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1335
[2019-04-01 17:23:29,118] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.66666666666667, 25.33333333333334, 111.8333333333333, 771.8333333333334, 26.0, 25.72843377779613, 10.57129183435251, 0.0, 1.0, 10.6332924632763], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3667200.0000, 
sim time next is 3667800.0000, 
raw observation next is [11.83333333333333, 24.66666666666666, 112.6666666666667, 780.6666666666667, 26.0, 25.74273485664409, 10.75761215054248, 0.0, 1.0, 10.12510096402712], 
processed observation next is [0.0, 0.43478260869565216, 0.7903970452446908, 0.24666666666666662, 0.37555555555555564, 0.8626151012891345, 1.0, 0.9632478366634416, 0.1075761215054248, 0.0, 1.0, 0.07232214974305085], 
reward next is 0.9277, 
noisyNet noise sample is [array([-1.3308822], dtype=float32), -0.9736633]. 
=============================================
[2019-04-01 17:23:30,954] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 2.6325002e-32 0.0000000e+00 9.2972658e-34
 7.5509660e-02 9.2449033e-01], sum to 1.0000
[2019-04-01 17:23:30,955] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3903
[2019-04-01 17:23:30,971] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.666666666666666, 48.0, 89.83333333333333, 716.8333333333333, 26.0, 25.56448246775567, 10.62612697770731, 0.0, 1.0, 3.155978587850066], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3684000.0000, 
sim time next is 3684600.0000, 
raw observation next is [5.5, 48.5, 87.0, 705.0, 26.0, 25.56838491906858, 10.57641044402817, 0.0, 1.0, 3.253547194333975], 
processed observation next is [0.0, 0.6521739130434783, 0.6149584487534627, 0.485, 0.29, 0.7790055248618785, 1.0, 0.938340702724083, 0.1057641044402817, 0.0, 1.0, 0.02323962281667125], 
reward next is 0.9768, 
noisyNet noise sample is [array([-0.51365656], dtype=float32), 2.4305532]. 
=============================================
[2019-04-01 17:23:34,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5498252e-35 1.7218505e-38 1.5694090e-37 2.9619678e-37 2.5691348e-15
 1.0000000e+00 1.2344399e-37], sum to 1.0000
[2019-04-01 17:23:34,840] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8965
[2019-04-01 17:23:34,842] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9628892e-18 4.9392806e-23 5.7888240e-25 3.7673694e-14 4.8519373e-01
 5.1480621e-01 4.6781155e-14], sum to 1.0000
[2019-04-01 17:23:34,843] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8394
[2019-04-01 17:23:34,857] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 60.0, 48.33333333333334, 408.3333333333334, 26.0, 26.75994486489428, 16.07287632548723, 1.0, 1.0, 9.012850501434835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3775800.0000, 
sim time next is 3776400.0000, 
raw observation next is [0.0, 60.0, 40.5, 343.0, 26.0, 26.88626679505893, 16.3671142400967, 1.0, 1.0, 9.272073467176932], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.6, 0.135, 0.37900552486187844, 1.0, 1.1266095421512756, 0.163671142400967, 1.0, 1.0, 0.06622909619412094], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.28286794], dtype=float32), -0.37909245]. 
=============================================
[2019-04-01 17:23:34,882] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.666666666666667, 51.66666666666667, 98.5, 654.3333333333334, 26.0, 25.97613755628869, 11.38695595867733, 1.0, 1.0, 22.70124017609139], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3921600.0000, 
sim time next is 3922200.0000, 
raw observation next is [-7.5, 51.0, 100.0, 692.0, 26.0, 26.08572864990267, 11.87407083176732, 1.0, 1.0, 19.68240430921341], 
processed observation next is [1.0, 0.391304347826087, 0.2548476454293629, 0.51, 0.3333333333333333, 0.7646408839779005, 1.0, 1.0122469499860958, 0.1187407083176732, 1.0, 1.0, 0.14058860220866723], 
reward next is 0.1098, 
noisyNet noise sample is [array([1.6276355], dtype=float32), -1.6225983]. 
=============================================
[2019-04-01 17:23:45,633] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1399559e-34 1.6666751e-36 0.0000000e+00 4.5301216e-32 9.9544424e-16
 1.0000000e+00 8.6231896e-34], sum to 1.0000
[2019-04-01 17:23:45,646] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6958
[2019-04-01 17:23:45,693] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.833333333333334, 43.33333333333334, 108.3333333333333, 753.3333333333334, 26.0, 26.54259009143336, 12.14722834315339, 1.0, 1.0, 20.21405257036499], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4011000.0000, 
sim time next is 4011600.0000, 
raw observation next is [-8.666666666666668, 42.66666666666667, 110.1666666666667, 767.1666666666667, 26.0, 26.54802258388671, 12.32116718472232, 1.0, 1.0, 19.05367118665136], 
processed observation next is [1.0, 0.43478260869565216, 0.22253000923361033, 0.4266666666666667, 0.36722222222222234, 0.8476979742173113, 1.0, 1.0782889405552443, 0.1232116718472232, 1.0, 1.0, 0.13609765133322402], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.20804796], dtype=float32), 0.25947717]. 
=============================================
[2019-04-01 17:23:48,061] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1510647e-35 0.0000000e+00 1.4459073e-21 0.0000000e+00 4.0894528e-26
 6.1049377e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 17:23:48,066] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1946
[2019-04-01 17:23:48,081] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 53.33333333333333, 0.0, 0.0, 26.0, 24.98505715562605, 7.009941462390192, 0.0, 1.0, 38.99552061886268], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4165800.0000, 
sim time next is 4166400.0000, 
raw observation next is [-4.0, 52.66666666666667, 0.0, 0.0, 26.0, 24.96984131724589, 6.924961799177527, 0.0, 1.0, 38.99815536121061], 
processed observation next is [0.0, 0.21739130434782608, 0.3518005540166205, 0.5266666666666667, 0.0, 0.0, 1.0, 0.8528344738922701, 0.06924961799177527, 0.0, 1.0, 0.27855825258007577], 
reward next is 0.7214, 
noisyNet noise sample is [array([1.8472654], dtype=float32), -1.7984186]. 
=============================================
[2019-04-01 17:24:03,065] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:24:03,073] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3667
[2019-04-01 17:24:03,086] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.34392914403112, 7.264386975721767, 0.0, 1.0, 38.78269772839545], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4258200.0000, 
sim time next is 4258800.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.33989968538021, 7.248423581552015, 0.0, 1.0, 38.75602339353061], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 1.0, 0.9056999550543158, 0.07248423581552016, 0.0, 1.0, 0.27682873852521866], 
reward next is 0.7232, 
noisyNet noise sample is [array([-1.5283794], dtype=float32), 0.6446109]. 
=============================================
[2019-04-01 17:24:03,097] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:24:03,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6494
[2019-04-01 17:24:03,115] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.33989968538021, 7.248423581552015, 0.0, 1.0, 38.75602339353061], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4258800.0000, 
sim time next is 4259400.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.33443139845689, 7.232074733650371, 0.0, 1.0, 38.73695006013047], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 1.0, 0.904918771208127, 0.0723207473365037, 0.0, 1.0, 0.27669250042950333], 
reward next is 0.7233, 
noisyNet noise sample is [array([-1.5283794], dtype=float32), 0.6446109]. 
=============================================
[2019-04-01 17:24:03,949] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0577588e-30 0.0000000e+00 1.2730229e-37 0.0000000e+00 9.9853878e-26
 1.0000000e+00 1.6151160e-18], sum to 1.0000
[2019-04-01 17:24:03,953] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0611
[2019-04-01 17:24:03,960] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.73333333333333, 58.33333333333334, 0.0, 0.0, 26.0, 27.11194705865022, 22.21285481963514, 0.0, 1.0, 6.221778654280045], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4393200.0000, 
sim time next is 4393800.0000, 
raw observation next is [10.6, 58.5, 0.0, 0.0, 26.0, 27.05179379423945, 21.93672459030097, 0.0, 1.0, 6.239857140385682], 
processed observation next is [1.0, 0.8695652173913043, 0.7562326869806094, 0.585, 0.0, 0.0, 1.0, 1.1502562563199217, 0.21936724590300968, 0.0, 1.0, 0.044570408145612014], 
reward next is 0.9554, 
noisyNet noise sample is [array([-0.455059], dtype=float32), 0.24618615]. 
=============================================
[2019-04-01 17:24:04,688] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5924949e-24 1.4210650e-29 1.8940240e-20 4.5123617e-30 1.4818539e-10
 6.9273171e-11 1.0000000e+00], sum to 1.0000
[2019-04-01 17:24:04,693] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4331
[2019-04-01 17:24:04,698] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9614738e-35 0.0000000e+00 1.0139661e-24 0.0000000e+00 8.6523358e-30
 1.9234029e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 17:24:04,699] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4362
[2019-04-01 17:24:04,722] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.25, 73.0, 0.0, 0.0, 26.0, 25.88564594324514, 10.15321616773193, 0.0, 1.0, 28.68344951266361], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4307400.0000, 
sim time next is 4308000.0000, 
raw observation next is [5.199999999999999, 73.0, 0.0, 0.0, 26.0, 25.8908937499362, 10.02457616473369, 0.0, 1.0, 27.12006016918633], 
processed observation next is [0.0, 0.8695652173913043, 0.6066481994459835, 0.73, 0.0, 0.0, 1.0, 0.9844133928480286, 0.1002457616473369, 0.0, 1.0, 0.1937147154941881], 
reward next is 0.8063, 
noisyNet noise sample is [array([0.33587888], dtype=float32), 0.8366875]. 
=============================================
[2019-04-01 17:24:04,731] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 72.83333333333334, 74.00000000000001, 44.00000000000001, 26.0, 25.49031440340891, 8.600629974458455, 0.0, 1.0, 23.75196397989442], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4522200.0000, 
sim time next is 4522800.0000, 
raw observation next is [-0.5333333333333334, 72.66666666666667, 92.5, 55.0, 26.0, 25.37163449782709, 9.526897457900306, 1.0, 1.0, 24.00174949393801], 
processed observation next is [1.0, 0.34782608695652173, 0.44783010156971376, 0.7266666666666667, 0.30833333333333335, 0.06077348066298342, 1.0, 0.910233499689584, 0.09526897457900306, 1.0, 1.0, 0.17144106781384294], 
reward next is 0.8286, 
noisyNet noise sample is [array([0.76333547], dtype=float32), -0.6526988]. 
=============================================
[2019-04-01 17:24:04,736] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[71.9062  ]
 [71.912056]
 [71.931625]
 [71.87098 ]
 [71.78118 ]], R is [[72.07521057]
 [72.14958191]
 [72.21620941]
 [72.25125122]
 [72.23033142]].
[2019-04-01 17:24:05,565] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:24:05,565] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0736
[2019-04-01 17:24:05,595] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.3, 34.0, 92.33333333333334, 0.0, 26.0, 28.75897783631835, 26.99832585865535, 1.0, 1.0, 1.233377392958176], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4376400.0000, 
sim time next is 4377000.0000, 
raw observation next is [13.15, 34.5, 81.66666666666667, 0.0, 26.0, 28.7306249872323, 32.52368149035606, 1.0, 1.0, 1.295969331117953], 
processed observation next is [1.0, 0.6521739130434783, 0.826869806094183, 0.345, 0.27222222222222225, 0.0, 1.0, 1.3900892838903285, 0.3252368149035606, 1.0, 1.0, 0.009256923793699664], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3482282], dtype=float32), 0.0983756]. 
=============================================
[2019-04-01 17:24:05,611] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[42.570217]
 [42.29331 ]
 [41.943504]
 [41.615967]
 [41.311306]], R is [[42.38446426]
 [41.96062088]
 [41.54101562]
 [41.12560654]
 [40.71435165]].
[2019-04-01 17:24:05,844] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7997776e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9324787e-01
 6.7521748e-03 0.0000000e+00], sum to 1.0000
[2019-04-01 17:24:05,848] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0112
[2019-04-01 17:24:05,859] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.666666666666667, 53.33333333333334, 126.6666666666667, 789.0, 26.0, 26.67218710375844, 15.22452298294564, 1.0, 1.0, 7.65154653940786], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4618200.0000, 
sim time next is 4618800.0000, 
raw observation next is [2.0, 52.0, 125.5, 800.0, 26.0, 26.70665320482539, 15.81344919455507, 1.0, 1.0, 7.049648619146546], 
processed observation next is [1.0, 0.4782608695652174, 0.518005540166205, 0.52, 0.41833333333333333, 0.8839779005524862, 1.0, 1.1009504578321985, 0.15813449194555068, 1.0, 1.0, 0.0503546329939039], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.509422], dtype=float32), -0.16937368]. 
=============================================
[2019-04-01 17:24:07,725] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0799341e-23 1.5459545e-31 8.9400440e-29 2.7426104e-24 9.9999976e-01
 2.6764195e-07 1.3837657e-10], sum to 1.0000
[2019-04-01 17:24:07,731] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3226
[2019-04-01 17:24:07,757] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 89.66666666666667, 103.5, 0.9999999999999998, 26.0, 26.38028549431746, 14.55127824827959, 1.0, 1.0, 16.7453371292408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4458000.0000, 
sim time next is 4458600.0000, 
raw observation next is [0.0, 88.5, 85.0, 0.0, 26.0, 26.46848257075787, 14.64529069675971, 1.0, 1.0, 15.98692608386147], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.885, 0.2833333333333333, 0.0, 1.0, 1.0669260815368387, 0.1464529069675971, 1.0, 1.0, 0.11419232917043906], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8199614], dtype=float32), -0.6951696]. 
=============================================
[2019-04-01 17:24:09,418] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3533911e-28 2.6139586e-36 8.3239194e-22 6.7600225e-37 2.3825974e-29
 6.4384373e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 17:24:09,420] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7228
[2019-04-01 17:24:09,432] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.55515649964501, 9.834799988072975, 0.0, 1.0, 33.85768004888651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4497000.0000, 
sim time next is 4497600.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.54520195386404, 9.304946762840041, 0.0, 1.0, 33.20699346599415], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 1.0, 0.9350288505520058, 0.09304946762840041, 0.0, 1.0, 0.2371928104713868], 
reward next is 0.7628, 
noisyNet noise sample is [array([-1.0488912], dtype=float32), 0.8336295]. 
=============================================
[2019-04-01 17:24:11,169] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0415145e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 3.9272822e-28 1.0000000e+00], sum to 1.0000
[2019-04-01 17:24:11,169] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4145
[2019-04-01 17:24:11,181] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.55, 73.0, 0.0, 0.0, 26.0, 25.43847620411614, 10.03376872094159, 0.0, 1.0, 38.00912667657572], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4494600.0000, 
sim time next is 4495200.0000, 
raw observation next is [-0.5666666666666667, 73.0, 0.0, 0.0, 26.0, 25.51389528760728, 9.959249983921772, 0.0, 1.0, 39.20671813367426], 
processed observation next is [1.0, 0.0, 0.44690674053554946, 0.73, 0.0, 0.0, 1.0, 0.9305564696581828, 0.09959249983921772, 0.0, 1.0, 0.28004798666910186], 
reward next is 0.7200, 
noisyNet noise sample is [array([-0.46871266], dtype=float32), -0.34523883]. 
=============================================
[2019-04-01 17:24:13,790] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.7219252e-29], sum to 1.0000
[2019-04-01 17:24:13,793] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2079
[2019-04-01 17:24:13,802] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.50941189568555, 10.16198975913134, 1.0, 1.0, 9.776322222089245], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4557600.0000, 
sim time next is 4558200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.56346824085876, 10.19269765567215, 1.0, 1.0, 9.353925710681258], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.9376383201226801, 0.1019269765567215, 1.0, 1.0, 0.0668137550762947], 
reward next is 0.8561, 
noisyNet noise sample is [array([0.17021838], dtype=float32), -1.0798208]. 
=============================================
[2019-04-01 17:24:22,741] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 3.3084366e-23 1.0000000e+00], sum to 1.0000
[2019-04-01 17:24:22,742] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0889
[2019-04-01 17:24:22,757] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 55.33333333333334, 0.0, 0.0, 26.0, 25.51359837669155, 11.62142717605076, 0.0, 1.0, 39.38238431930856], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4663200.0000, 
sim time next is 4663800.0000, 
raw observation next is [2.0, 54.5, 0.0, 0.0, 26.0, 25.69711678199494, 11.83808372279403, 0.0, 1.0, 31.4980248289978], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.545, 0.0, 0.0, 1.0, 0.95673096885642, 0.1183808372279403, 0.0, 1.0, 0.22498589163569857], 
reward next is 0.7750, 
noisyNet noise sample is [array([0.7167266], dtype=float32), -0.3718154]. 
=============================================
[2019-04-01 17:24:30,515] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:24:30,520] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9842
[2019-04-01 17:24:30,540] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 53.0, 0.0, 0.0, 26.0, 25.35453758489065, 8.550180842431217, 0.0, 1.0, 40.78238041239157], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4829400.0000, 
sim time next is 4830000.0000, 
raw observation next is [-0.6666666666666666, 53.66666666666667, 0.0, 0.0, 26.0, 25.35105916486309, 8.589651752484917, 0.0, 1.0, 41.07430634877051], 
processed observation next is [0.0, 0.9130434782608695, 0.44413665743305636, 0.5366666666666667, 0.0, 0.0, 1.0, 0.9072941664090131, 0.08589651752484917, 0.0, 1.0, 0.29338790249121793], 
reward next is 0.7066, 
noisyNet noise sample is [array([0.04072207], dtype=float32), 1.5134254]. 
=============================================
[2019-04-01 17:24:30,562] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[72.76797 ]
 [72.83473 ]
 [72.76641 ]
 [72.59717 ]
 [72.387314]], R is [[72.68680573]
 [72.66863251]
 [72.67149353]
 [72.70111847]
 [72.75614929]].
[2019-04-01 17:24:31,046] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:24:31,049] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0518
[2019-04-01 17:24:31,072] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.96747224392967, 6.711450868426638, 0.0, 1.0, 38.78904952266817], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4849800.0000, 
sim time next is 4850400.0000, 
raw observation next is [-3.0, 60.00000000000001, 0.0, 0.0, 26.0, 24.93923611910477, 6.644582128238736, 0.0, 1.0, 38.79581609723729], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.6000000000000001, 0.0, 0.0, 1.0, 0.8484623027292528, 0.06644582128238735, 0.0, 1.0, 0.2771129721231235], 
reward next is 0.7229, 
noisyNet noise sample is [array([0.38879657], dtype=float32), -0.30906832]. 
=============================================
[2019-04-01 17:24:31,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:31,074] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:31,122] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run14
[2019-04-01 17:24:34,611] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:34,611] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:34,692] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run14
[2019-04-01 17:24:37,412] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5895927e-23 1.7726347e-24 7.2688259e-34 1.7285320e-21 1.1170827e-17
 1.0000000e+00 4.5036428e-15], sum to 1.0000
[2019-04-01 17:24:37,413] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6632
[2019-04-01 17:24:37,431] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 27.0, 122.0, 845.0, 26.0, 26.83090031993451, 14.55453606033668, 1.0, 1.0, 5.848424886292884], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4966200.0000, 
sim time next is 4966800.0000, 
raw observation next is [5.0, 26.33333333333333, 122.1666666666667, 848.3333333333334, 26.0, 26.91270155672469, 15.05848364722966, 1.0, 1.0, 5.493154061204335], 
processed observation next is [1.0, 0.4782608695652174, 0.6011080332409973, 0.2633333333333333, 0.4072222222222223, 0.9373848987108656, 1.0, 1.130385936674956, 0.15058483647229662, 1.0, 1.0, 0.039236814722888105], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4374322], dtype=float32), 0.3035618]. 
=============================================
[2019-04-01 17:24:37,587] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:37,588] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:37,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run14
[2019-04-01 17:24:38,289] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.8984659e-29 1.2803700e-27 9.4171359e-24 2.6526089e-25 2.1176681e-19
 9.9952424e-01 4.7580875e-04], sum to 1.0000
[2019-04-01 17:24:38,289] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2057
[2019-04-01 17:24:38,308] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.0, 19.0, 92.66666666666666, 718.3333333333334, 26.0, 28.26109420203811, 29.72575085486626, 1.0, 1.0, 0.9375273568617888], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5067600.0000, 
sim time next is 5068200.0000, 
raw observation next is [12.0, 19.0, 89.33333333333333, 691.6666666666667, 26.0, 27.62414717708947, 27.96431617231344, 1.0, 1.0, 0.8447985358956537], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.19, 0.29777777777777775, 0.7642725598526704, 1.0, 1.2320210252984956, 0.27964316172313436, 1.0, 1.0, 0.006034275256397526], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8601842], dtype=float32), 0.12716079]. 
=============================================
[2019-04-01 17:24:38,505] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.3739193e-38
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:24:38,514] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6790
[2019-04-01 17:24:38,530] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.5, 19.5, 111.0, 819.0, 26.0, 28.13025630254053, 27.62298386242103, 1.0, 1.0, 1.326756635634446], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5063400.0000, 
sim time next is 5064000.0000, 
raw observation next is [11.66666666666667, 19.33333333333334, 108.5, 806.6666666666666, 26.0, 28.31935501978796, 28.84584581031584, 1.0, 1.0, 1.269309089751175], 
processed observation next is [1.0, 0.6086956521739131, 0.785780240073869, 0.19333333333333338, 0.3616666666666667, 0.8913443830570902, 1.0, 1.3313364313982798, 0.2884584581031584, 1.0, 1.0, 0.009066493498222679], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6360787], dtype=float32), -0.9932814]. 
=============================================
[2019-04-01 17:24:38,547] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[48.623566]
 [48.072746]
 [47.324734]
 [46.834618]
 [46.682762]], R is [[48.69484329]
 [48.20789719]
 [47.72581863]
 [47.24856186]
 [46.77607727]].
[2019-04-01 17:24:40,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:40,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:40,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run14
[2019-04-01 17:24:40,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:40,358] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:40,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run14
[2019-04-01 17:24:40,695] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:40,695] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:40,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run14
[2019-04-01 17:24:40,879] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:40,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:40,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run14
[2019-04-01 17:24:41,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:41,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:41,375] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run14
[2019-04-01 17:24:41,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:41,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:41,515] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run14
[2019-04-01 17:24:41,807] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:41,807] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:41,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run14
[2019-04-01 17:24:42,666] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:42,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:42,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run14
[2019-04-01 17:24:43,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:43,208] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:43,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run14
[2019-04-01 17:24:43,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:43,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:43,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run14
[2019-04-01 17:24:43,583] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:43,583] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:43,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run14
[2019-04-01 17:24:43,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:43,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:43,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run14
[2019-04-01 17:24:43,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:24:43,982] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:24:43,986] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run14
[2019-04-01 17:24:50,756] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.6180101e-38 1.0000000e+00], sum to 1.0000
[2019-04-01 17:24:50,756] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3377
[2019-04-01 17:24:50,778] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 26.0, 20.33809210750304, 18.25237401206585, 0.0, 1.0, 42.81790721096496], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 7200.0000, 
sim time next is 7800.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 26.0, 20.39157905586699, 17.90486735058782, 0.0, 1.0, 42.57656599808955], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 1.0, 0.19879700798099872, 0.17904867350587822, 0.0, 1.0, 0.3041183285577825], 
reward next is 0.6959, 
noisyNet noise sample is [array([-0.26858142], dtype=float32), -0.8684911]. 
=============================================
[2019-04-01 17:24:51,700] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.5366193e-37
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 17:24:51,700] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2859
[2019-04-01 17:24:51,762] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 72.5, 0.0, 26.0, 24.32547491242937, 5.941377335898221, 0.0, 1.0, 43.3114160211479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 40800.0000, 
sim time next is 41400.0000, 
raw observation next is [7.7, 93.0, 75.0, 0.0, 26.0, 24.37650253989656, 5.991230490379209, 0.0, 1.0, 36.67782546226734], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.25, 0.0, 1.0, 0.7680717914137942, 0.05991230490379209, 0.0, 1.0, 0.2619844675876239], 
reward next is 0.7380, 
noisyNet noise sample is [array([0.54421264], dtype=float32), 2.0319474]. 
=============================================
[2019-04-01 17:24:52,486] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8438017e-19 9.4501624e-24 5.1045695e-12 3.9616080e-26 6.4161389e-18
 6.4640624e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 17:24:52,487] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6879
[2019-04-01 17:24:52,550] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.65, 78.0, 34.0, 296.0, 26.0, 25.20635924849193, 6.821365288505227, 1.0, 1.0, 49.04779112214676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 203400.0000, 
sim time next is 204000.0000, 
raw observation next is [-8.566666666666666, 78.0, 41.5, 246.6666666666667, 26.0, 25.35183664762779, 6.947448092226407, 1.0, 1.0, 45.7564680524394], 
processed observation next is [1.0, 0.34782608695652173, 0.22530009233610343, 0.78, 0.13833333333333334, 0.27255985267034993, 1.0, 0.9074052353753983, 0.06947448092226406, 1.0, 1.0, 0.32683191466028144], 
reward next is 0.6732, 
noisyNet noise sample is [array([-0.09175784], dtype=float32), 2.4954603]. 
=============================================
[2019-04-01 17:24:52,553] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[39.561546]
 [40.026604]
 [40.817417]
 [43.259388]
 [46.284386]], R is [[40.13715744]
 [40.38544464]
 [40.55614853]
 [40.65626144]
 [40.73677444]].
[2019-04-01 17:25:01,564] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.6962907e-27 0.0000000e+00 1.0081125e-37
 2.3228995e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 17:25:01,564] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8561
[2019-04-01 17:25:01,577] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.88738187669821, 6.111398563601661, 0.0, 1.0, 44.00206686172675], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 190200.0000, 
sim time next is 190800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.89043774246197, 6.154213796632637, 0.0, 1.0, 44.08119837518512], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.78, 0.0, 0.0, 1.0, 0.5557768203517099, 0.06154213796632637, 0.0, 1.0, 0.3148657026798937], 
reward next is 0.6851, 
noisyNet noise sample is [array([-0.6056442], dtype=float32), -0.14529346]. 
=============================================
[2019-04-01 17:25:16,523] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.1512304e-37 0.0000000e+00 0.0000000e+00
 1.1096542e-25 1.0000000e+00], sum to 1.0000
[2019-04-01 17:25:16,523] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1244
[2019-04-01 17:25:16,558] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.1, 67.0, 0.0, 0.0, 26.0, 23.45050523946951, 5.670105643821539, 0.0, 1.0, 46.81250939808432], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 346800.0000, 
sim time next is 347400.0000, 
raw observation next is [-14.2, 67.5, 0.0, 0.0, 26.0, 23.38297656912033, 5.709557542425969, 0.0, 1.0, 46.87545712217431], 
processed observation next is [1.0, 0.0, 0.06925207756232687, 0.675, 0.0, 0.0, 1.0, 0.626139509874333, 0.05709557542425969, 0.0, 1.0, 0.3348246937298165], 
reward next is 0.6652, 
noisyNet noise sample is [array([-2.4218862], dtype=float32), 0.026617942]. 
=============================================
[2019-04-01 17:25:42,665] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:25:42,665] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9975
[2019-04-01 17:25:42,691] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.89350721630473, 6.333828747455179, 0.0, 1.0, 41.64964839510083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 681600.0000, 
sim time next is 682200.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.85283805885156, 6.262767755514548, 0.0, 1.0, 41.59164463644647], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 1.0, 0.83611972269308, 0.06262767755514548, 0.0, 1.0, 0.29708317597461764], 
reward next is 0.7029, 
noisyNet noise sample is [array([0.2886376], dtype=float32), 0.62594485]. 
=============================================
[2019-04-01 17:25:52,764] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2564043e-27
 1.0000000e+00 5.2531131e-23], sum to 1.0000
[2019-04-01 17:25:52,764] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2417
[2019-04-01 17:25:52,820] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 98.5, 0.0, 26.0, 25.62958522822914, 7.469904436462485, 1.0, 1.0, 27.4390737521743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 817200.0000, 
sim time next is 817800.0000, 
raw observation next is [-4.5, 71.0, 102.3333333333333, 0.0, 26.0, 25.63503554487924, 7.519523406323167, 1.0, 1.0, 26.51484285133867], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.341111111111111, 0.0, 1.0, 0.9478622206970344, 0.07519523406323167, 1.0, 1.0, 0.18939173465241907], 
reward next is 0.8106, 
noisyNet noise sample is [array([0.32119033], dtype=float32), -0.13654347]. 
=============================================
[2019-04-01 17:25:55,383] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:25:55,384] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2406
[2019-04-01 17:25:55,400] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.58609093833697, 6.204894776248914, 0.0, 1.0, 39.54788306605298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 864600.0000, 
sim time next is 865200.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.56268616290413, 6.188434915197597, 0.0, 1.0, 39.42944627666317], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 1.0, 0.794669451843447, 0.061884349151975965, 0.0, 1.0, 0.28163890197616553], 
reward next is 0.7184, 
noisyNet noise sample is [array([0.98159677], dtype=float32), 0.43532997]. 
=============================================
[2019-04-01 17:26:05,109] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 3.2453327e-33 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 17:26:05,115] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8211
[2019-04-01 17:26:05,129] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.416666666666667, 90.16666666666667, 0.0, 0.0, 26.0, 25.39036890147204, 9.258744076408043, 0.0, 1.0, 36.07689642189847], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 953400.0000, 
sim time next is 954000.0000, 
raw observation next is [5.5, 89.0, 0.0, 0.0, 26.0, 25.35418132308087, 9.255442620392229, 0.0, 1.0, 36.47702368114663], 
processed observation next is [1.0, 0.043478260869565216, 0.6149584487534627, 0.89, 0.0, 0.0, 1.0, 0.9077401890115527, 0.09255442620392229, 0.0, 1.0, 0.2605501691510474], 
reward next is 0.7394, 
noisyNet noise sample is [array([0.5780425], dtype=float32), -1.7027715]. 
=============================================
[2019-04-01 17:26:05,141] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[70.11856 ]
 [70.883545]
 [71.750534]
 [72.79506 ]
 [73.86095 ]], R is [[69.64393616]
 [69.68980408]
 [69.73189545]
 [69.7662735 ]
 [69.79995728]].
[2019-04-01 17:26:06,522] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:26:06,524] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7548
[2019-04-01 17:26:06,534] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.31666666666667, 78.5, 0.0, 0.0, 26.0, 26.71164066340625, 15.91983172320066, 1.0, 1.0, 5.356813025797885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1012200.0000, 
sim time next is 1012800.0000, 
raw observation next is [15.13333333333333, 79.0, 0.0, 0.0, 26.0, 26.83489538400545, 15.38379060536798, 1.0, 1.0, 5.578321189985535], 
processed observation next is [1.0, 0.7391304347826086, 0.8818097876269622, 0.79, 0.0, 0.0, 1.0, 1.1192707691436357, 0.15383790605367978, 1.0, 1.0, 0.03984515135703953], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3677708], dtype=float32), -0.46384764]. 
=============================================
[2019-04-01 17:26:07,530] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:26:07,532] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4085
[2019-04-01 17:26:07,542] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.2, 83.0, 18.5, 58.66666666666666, 26.0, 25.70553888769643, 13.03110008777458, 1.0, 1.0, 12.27100648201], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1066800.0000, 
sim time next is 1067400.0000, 
raw observation next is [12.2, 83.0, 22.0, 69.0, 26.0, 25.87047306237821, 13.85763131329637, 1.0, 1.0, 11.51284199481486], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.07333333333333333, 0.07624309392265194, 1.0, 0.9814961517683157, 0.1385763131329637, 1.0, 1.0, 0.08223458567724899], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.26168627], dtype=float32), -1.1619964]. 
=============================================
[2019-04-01 17:26:08,612] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:26:08,612] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6197
[2019-04-01 17:26:08,643] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.950000000000001, 96.66666666666666, 0.0, 0.0, 26.0, 24.64252001374385, 9.701333839922052, 0.0, 1.0, 26.17326800925534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1273800.0000, 
sim time next is 1274400.0000, 
raw observation next is [8.3, 96.0, 0.0, 0.0, 26.0, 24.69011750523077, 9.760048290953039, 0.0, 1.0, 23.93777932598812], 
processed observation next is [0.0, 0.782608695652174, 0.6925207756232689, 0.96, 0.0, 0.0, 1.0, 0.8128739293186814, 0.09760048290953038, 0.0, 1.0, 0.1709841380427723], 
reward next is 0.8290, 
noisyNet noise sample is [array([1.4839411], dtype=float32), 0.37383083]. 
=============================================
[2019-04-01 17:26:09,523] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:26:09,531] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3725
[2019-04-01 17:26:09,540] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.2, 66.0, 0.0, 0.0, 26.0, 25.8451239022504, 14.65074085972161, 0.0, 1.0, 13.31955825568403], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1119600.0000, 
sim time next is 1120200.0000, 
raw observation next is [12.1, 66.83333333333333, 0.0, 0.0, 26.0, 25.79357835874144, 14.32071945605032, 0.0, 1.0, 12.74557666419716], 
processed observation next is [1.0, 1.0, 0.7977839335180056, 0.6683333333333333, 0.0, 0.0, 1.0, 0.9705111941059198, 0.1432071945605032, 0.0, 1.0, 0.091039833315694], 
reward next is 0.9090, 
noisyNet noise sample is [array([0.42701328], dtype=float32), -1.5383025]. 
=============================================
[2019-04-01 17:26:11,814] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-01 17:26:11,815] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 17:26:11,816] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:26:11,817] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 17:26:11,818] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:26:11,819] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 17:26:11,820] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:26:11,828] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run19
[2019-04-01 17:26:11,828] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run19
[2019-04-01 17:26:11,866] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run19
[2019-04-01 17:27:54,258] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 17:28:08,390] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.57318586], dtype=float32), -0.64552397]
[2019-04-01 17:28:08,390] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.183086509666667, 92.11816794666666, 286.356287, 0.0, 26.0, 25.5185103518601, 9.268036853697993, 1.0, 1.0, 11.25736260263059]
[2019-04-01 17:28:08,390] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 17:28:08,392] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.6220654e-38 0.0000000e+00 1.0062089e-33 0.0000000e+00 8.0506279e-31
 5.1123618e-13 1.0000000e+00], sampled 0.6630646656795626
[2019-04-01 17:28:12,984] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 17:28:15,608] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 17:28:16,631] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 1800000, evaluation results [1800000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 17:28:20,187] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3853590e-28 3.6942523e-37 4.0850883e-33 1.8014612e-34 2.2381550e-24
 1.0000000e+00 1.1681719e-25], sum to 1.0000
[2019-04-01 17:28:20,190] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1035
[2019-04-01 17:28:20,210] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 113.0, 0.0, 26.0, 25.90417415873494, 11.48508498474554, 1.0, 1.0, 9.400563028057334], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1341000.0000, 
sim time next is 1341600.0000, 
raw observation next is [1.1, 92.0, 111.8333333333333, 0.0, 26.0, 25.81077394162542, 11.31339649907731, 1.0, 1.0, 9.490690089193027], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.37277777777777765, 0.0, 1.0, 0.9729677059464886, 0.1131339649907731, 1.0, 1.0, 0.06779064349423591], 
reward next is 0.4069, 
noisyNet noise sample is [array([-0.30802527], dtype=float32), -0.7159932]. 
=============================================
[2019-04-01 17:28:21,272] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.49095840e-34 5.73815690e-38 1.49593815e-33 6.31570599e-38
 9.00483291e-29 9.57603159e-04 9.99042332e-01], sum to 1.0000
[2019-04-01 17:28:21,272] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1515
[2019-04-01 17:28:21,292] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 45.0, 0.0, 26.0, 25.98842078963406, 12.29388872747755, 1.0, 1.0, 15.86453186430955], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1330200.0000, 
sim time next is 1330800.0000, 
raw observation next is [0.5, 92.0, 54.5, 0.0, 26.0, 26.04442284211407, 12.34382958441448, 1.0, 1.0, 15.11126541034609], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.18166666666666667, 0.0, 1.0, 1.00634612030201, 0.12343829584414481, 1.0, 1.0, 0.10793761007390065], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.08565869], dtype=float32), -0.6754331]. 
=============================================
[2019-04-01 17:28:21,503] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.8705246e-21 4.2222446e-26 1.2397996e-24 1.1171688e-23 2.3211472e-12
 1.1400069e-05 9.9998856e-01], sum to 1.0000
[2019-04-01 17:28:21,507] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4774
[2019-04-01 17:28:21,528] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.83333333333333, 35.33333333333334, 0.0, 26.0, 25.71780134908285, 10.80053884220075, 1.0, 1.0, 10.84239828961677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1353000.0000, 
sim time next is 1353600.0000, 
raw observation next is [1.1, 93.0, 31.0, 0.0, 26.0, 25.71674881733396, 10.76446265935943, 1.0, 1.0, 10.92890599113433], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.93, 0.10333333333333333, 0.0, 1.0, 0.959535545333423, 0.10764462659359429, 1.0, 1.0, 0.07806361422238807], 
reward next is 0.6162, 
noisyNet noise sample is [array([0.7351999], dtype=float32), -0.71238613]. 
=============================================
[2019-04-01 17:28:42,172] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.6423649e-27 0.0000000e+00 3.1047441e-29
 1.7828360e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 17:28:42,172] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0193
[2019-04-01 17:28:42,193] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.9, 78.0, 0.0, 0.0, 26.0, 23.95864031205683, 5.32128466476355, 0.0, 1.0, 44.28583045302743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1908600.0000, 
sim time next is 1909200.0000, 
raw observation next is [-8.0, 78.0, 0.0, 0.0, 26.0, 23.90721735693247, 5.319562681100021, 0.0, 1.0, 44.24243805915699], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.78, 0.0, 0.0, 1.0, 0.7010310509903529, 0.053195626811000214, 0.0, 1.0, 0.3160174147082642], 
reward next is 0.6840, 
noisyNet noise sample is [array([-0.27362832], dtype=float32), -0.78853196]. 
=============================================
[2019-04-01 17:28:44,995] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:28:44,995] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1417
[2019-04-01 17:28:45,026] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.23035541364514, 9.723493594079036, 0.0, 1.0, 42.45982922531173], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1730400.0000, 
sim time next is 1731000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.26512110784862, 9.673267830975256, 0.0, 1.0, 42.40460485273054], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 1.0, 0.8950173011212316, 0.09673267830975256, 0.0, 1.0, 0.30289003466236103], 
reward next is 0.6971, 
noisyNet noise sample is [array([0.25211167], dtype=float32), -0.6961679]. 
=============================================
[2019-04-01 17:28:45,034] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[76.39559 ]
 [76.04906 ]
 [75.690384]
 [75.22502 ]
 [74.28244 ]], R is [[76.76612854]
 [76.6951828 ]
 [76.62464142]
 [76.55454254]
 [76.48483276]].
[2019-04-01 17:28:59,295] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.07136425e-20 1.00000000e+00 0.00000000e+00], sum to 1.0000
[2019-04-01 17:28:59,298] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0244
[2019-04-01 17:28:59,357] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.566666666666666, 63.0, 132.8333333333333, 0.0, 26.0, 25.65018692792779, 7.907468107004138, 1.0, 1.0, 26.70649124116035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1950000.0000, 
sim time next is 1950600.0000, 
raw observation next is [-3.483333333333333, 62.5, 128.6666666666667, 0.0, 26.0, 25.69079089741074, 8.005816875754983, 1.0, 1.0, 26.56255836336614], 
processed observation next is [1.0, 0.5652173913043478, 0.3661126500461681, 0.625, 0.42888888888888904, 0.0, 1.0, 0.955827271058677, 0.08005816875754983, 1.0, 1.0, 0.18973255973832956], 
reward next is 0.8103, 
noisyNet noise sample is [array([2.0716062], dtype=float32), 0.32462987]. 
=============================================
[2019-04-01 17:29:08,130] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7217841e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.5953683e-36
 9.9991226e-01 8.7768080e-05], sum to 1.0000
[2019-04-01 17:29:08,130] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0608
[2019-04-01 17:29:08,175] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.00000000000001, 0.0, 0.0, 26.0, 25.37958739371722, 9.011372092925333, 1.0, 1.0, 27.77012285277757], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2052600.0000, 
sim time next is 2053200.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.42245883426907, 8.572714131847773, 1.0, 1.0, 26.53890514294563], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 1.0, 0.9174941191812955, 0.08572714131847774, 1.0, 1.0, 0.18956360816389736], 
reward next is 0.8104, 
noisyNet noise sample is [array([0.18845311], dtype=float32), 0.5120532]. 
=============================================
[2019-04-01 17:29:10,254] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5386731e-36 0.0000000e+00 5.3275895e-27 0.0000000e+00 6.5867203e-30
 4.9506755e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 17:29:10,255] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9350
[2019-04-01 17:29:10,268] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.533333333333333, 84.33333333333333, 0.0, 0.0, 26.0, 24.28511749368752, 5.521505203016903, 0.0, 1.0, 42.83153921100624], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2094000.0000, 
sim time next is 2094600.0000, 
raw observation next is [-6.616666666666667, 83.66666666666667, 0.0, 0.0, 26.0, 24.2385864844322, 5.455384080424906, 0.0, 1.0, 42.89130402509662], 
processed observation next is [1.0, 0.21739130434782608, 0.2793167128347184, 0.8366666666666667, 0.0, 0.0, 1.0, 0.7483694977760287, 0.05455384080424906, 0.0, 1.0, 0.30636645732211876], 
reward next is 0.6936, 
noisyNet noise sample is [array([1.593515], dtype=float32), 0.021502782]. 
=============================================
[2019-04-01 17:29:19,205] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:29:19,206] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3322
[2019-04-01 17:29:19,254] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.933333333333334, 43.66666666666667, 0.0, 0.0, 26.0, 24.92724552535859, 7.318062105419554, 0.0, 1.0, 42.12318435300284], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2398800.0000, 
sim time next is 2399400.0000, 
raw observation next is [-2.05, 43.5, 0.0, 0.0, 26.0, 24.95362976014646, 7.370143216283608, 0.0, 1.0, 40.85511531495433], 
processed observation next is [0.0, 0.782608695652174, 0.40581717451523547, 0.435, 0.0, 0.0, 1.0, 0.8505185371637799, 0.07370143216283608, 0.0, 1.0, 0.2918222522496738], 
reward next is 0.7082, 
noisyNet noise sample is [array([-1.538969], dtype=float32), -0.27892786]. 
=============================================
[2019-04-01 17:29:22,827] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.0493995e-33
 7.6348262e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 17:29:22,827] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6246
[2019-04-01 17:29:22,845] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.1, 79.66666666666667, 0.0, 0.0, 26.0, 24.25548184121751, 5.709111118142997, 0.0, 1.0, 43.3893265359869], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2252400.0000, 
sim time next is 2253000.0000, 
raw observation next is [-7.199999999999999, 80.83333333333334, 0.0, 0.0, 26.0, 24.20086335245191, 5.677547441626906, 0.0, 1.0, 43.403148596077], 
processed observation next is [1.0, 0.043478260869565216, 0.26315789473684215, 0.8083333333333335, 0.0, 0.0, 1.0, 0.7429804789217016, 0.05677547441626906, 0.0, 1.0, 0.31002248997197857], 
reward next is 0.6900, 
noisyNet noise sample is [array([-0.3748776], dtype=float32), -0.05592754]. 
=============================================
[2019-04-01 17:29:22,882] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[69.72834 ]
 [69.81417 ]
 [69.823265]
 [69.98064 ]
 [70.12204 ]], R is [[69.70431519]
 [69.69734955]
 [69.69025421]
 [69.68283844]
 [69.6752243 ]].
[2019-04-01 17:29:28,397] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 7.370197e-22], sum to 1.0000
[2019-04-01 17:29:28,399] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1109
[2019-04-01 17:29:28,443] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.616666666666667, 55.66666666666667, 0.0, 0.0, 26.0, 24.97635832610116, 8.409052454725641, 1.0, 1.0, 38.70173267777642], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2317800.0000, 
sim time next is 2318400.0000, 
raw observation next is [-1.7, 56.0, 0.0, 0.0, 26.0, 25.03992813395886, 8.492345234843846, 0.0, 1.0, 35.39788275029396], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.56, 0.0, 0.0, 1.0, 0.8628468762798371, 0.08492345234843846, 0.0, 1.0, 0.25284201964495684], 
reward next is 0.7472, 
noisyNet noise sample is [array([0.70068383], dtype=float32), 0.97738385]. 
=============================================
[2019-04-01 17:29:28,885] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0970967e-37 0.0000000e+00 4.6503053e-32 0.0000000e+00 5.3695242e-32
 3.0494066e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 17:29:28,885] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8514
[2019-04-01 17:29:28,908] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.9, 57.0, 0.0, 0.0, 26.0, 25.32834596889652, 8.32996044154478, 0.0, 1.0, 39.09050549334984], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2326800.0000, 
sim time next is 2327400.0000, 
raw observation next is [-2.0, 57.5, 0.0, 0.0, 26.0, 25.29721390163208, 8.192093617107249, 0.0, 1.0, 38.60887780646513], 
processed observation next is [1.0, 0.9565217391304348, 0.40720221606648205, 0.575, 0.0, 0.0, 1.0, 0.89960198594744, 0.08192093617107249, 0.0, 1.0, 0.2757776986176081], 
reward next is 0.7242, 
noisyNet noise sample is [array([-0.62675875], dtype=float32), -0.80877215]. 
=============================================
[2019-04-01 17:29:29,654] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.2829851e-26 1.0000000e+00], sum to 1.0000
[2019-04-01 17:29:29,654] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0362
[2019-04-01 17:29:29,675] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 67.0, 0.0, 0.0, 26.0, 24.41903987798079, 5.796411384907482, 0.0, 1.0, 40.09594926035573], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2349000.0000, 
sim time next is 2349600.0000, 
raw observation next is [-3.2, 67.66666666666667, 0.0, 0.0, 26.0, 24.38211300973309, 5.744765882454613, 0.0, 1.0, 40.19923902451681], 
processed observation next is [0.0, 0.17391304347826086, 0.37396121883656513, 0.6766666666666667, 0.0, 0.0, 1.0, 0.768873287104727, 0.057447658824546124, 0.0, 1.0, 0.2871374216036915], 
reward next is 0.7129, 
noisyNet noise sample is [array([0.7290465], dtype=float32), 0.60034907]. 
=============================================
[2019-04-01 17:29:49,270] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 9.6373252e-31 0.0000000e+00 7.6530491e-35
 1.1525205e-25 1.0000000e+00], sum to 1.0000
[2019-04-01 17:29:49,271] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4216
[2019-04-01 17:29:49,296] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.68557196403264, 6.243269257969547, 0.0, 1.0, 40.79320753853023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2773800.0000, 
sim time next is 2774400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.67875707997937, 6.234828947214979, 0.0, 1.0, 40.6606792594288], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 1.0, 0.8112510114256243, 0.062348289472149786, 0.0, 1.0, 0.29043342328163424], 
reward next is 0.7096, 
noisyNet noise sample is [array([-0.49060884], dtype=float32), 0.12872732]. 
=============================================
[2019-04-01 17:29:59,266] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9457324e-29 0.0000000e+00 7.9462788e-31 1.1627077e-33 2.7349328e-16
 1.0000000e+00 4.8921754e-26], sum to 1.0000
[2019-04-01 17:29:59,266] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8439
[2019-04-01 17:29:59,303] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.5, 49.0, 0.0, 26.0, 25.45558560681108, 7.513112370570224, 1.0, 1.0, 22.01355597784454], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2885400.0000, 
sim time next is 2886000.0000, 
raw observation next is [0.3333333333333334, 97.66666666666666, 53.83333333333333, 0.0, 26.0, 25.49723586787623, 7.557849254346486, 1.0, 1.0, 20.16260626691192], 
processed observation next is [1.0, 0.391304347826087, 0.4718374884579871, 0.9766666666666666, 0.17944444444444443, 0.0, 1.0, 0.9281765525537473, 0.07557849254346487, 1.0, 1.0, 0.144018616192228], 
reward next is 0.8560, 
noisyNet noise sample is [array([0.24941608], dtype=float32), 0.012973799]. 
=============================================
[2019-04-01 17:29:59,306] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[66.53416 ]
 [66.32804 ]
 [66.20413 ]
 [66.175415]
 [66.21233 ]], R is [[67.0643692 ]
 [67.23648071]
 [67.39297485]
 [67.53239441]
 [67.64507294]].
[2019-04-01 17:30:03,847] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 6.893741e-16], sum to 1.0000
[2019-04-01 17:30:03,850] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4699
[2019-04-01 17:30:03,868] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666666, 95.33333333333334, 79.5, 0.0, 26.0, 25.49697216564578, 7.605759759535815, 1.0, 1.0, 21.05274906348968], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2889600.0000, 
sim time next is 2890200.0000, 
raw observation next is [0.8333333333333334, 94.16666666666666, 81.0, 0.0, 26.0, 25.51322108367378, 7.639725565848352, 1.0, 1.0, 20.04758356311974], 
processed observation next is [1.0, 0.43478260869565216, 0.4856879039704525, 0.9416666666666665, 0.27, 0.0, 1.0, 0.9304601548105401, 0.07639725565848351, 1.0, 1.0, 0.1431970254508553], 
reward next is 0.8568, 
noisyNet noise sample is [array([1.7354733], dtype=float32), -0.40845948]. 
=============================================
[2019-04-01 17:30:07,498] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.4190195e-26 0.0000000e+00 1.6748153e-27
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 17:30:07,499] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8026
[2019-04-01 17:30:07,543] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 93.33333333333334, 0.0, 0.0, 26.0, 25.02503257148335, 8.871378185567197, 0.0, 1.0, 40.53913719762273], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3221400.0000, 
sim time next is 3222000.0000, 
raw observation next is [-3.0, 92.0, 0.0, 0.0, 26.0, 24.98797035151392, 8.756117357966792, 0.0, 1.0, 40.595148641512], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.0, 0.0, 1.0, 0.8554243359305599, 0.08756117357966792, 0.0, 1.0, 0.2899653474393714], 
reward next is 0.7100, 
noisyNet noise sample is [array([0.29357204], dtype=float32), 0.56700104]. 
=============================================
[2019-04-01 17:30:07,553] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[69.210236]
 [69.20474 ]
 [69.190636]
 [69.176254]
 [69.17638 ]], R is [[69.24947357]
 [69.26741028]
 [69.28562927]
 [69.30418396]
 [69.3232193 ]].
[2019-04-01 17:30:08,874] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:30:08,874] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3444
[2019-04-01 17:30:08,901] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.99777695137447, 5.286697024985419, 0.0, 1.0, 39.71125134539744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3040200.0000, 
sim time next is 3040800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 23.97004308604141, 5.273071790638959, 0.0, 1.0, 39.71692991284204], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.710006155148773, 0.052730717906389585, 0.0, 1.0, 0.2836923565203003], 
reward next is 0.7163, 
noisyNet noise sample is [array([-1.1572312], dtype=float32), -0.07047755]. 
=============================================
[2019-04-01 17:30:10,677] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 2.9977874e-36 0.0000000e+00 5.0519091e-30
 1.0376867e-32 1.0000000e+00], sum to 1.0000
[2019-04-01 17:30:10,680] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1872
[2019-04-01 17:30:10,716] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 61.66666666666667, 108.5, 791.8333333333334, 26.0, 25.14907632296296, 9.076760160226366, 0.0, 1.0, 21.37113260722006], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2986800.0000, 
sim time next is 2987400.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 107.0, 783.6666666666666, 26.0, 25.11430080426326, 9.00509576160001, 0.0, 1.0, 23.24618128005499], 
processed observation next is [0.0, 0.5652173913043478, 0.4025854108956602, 0.6083333333333333, 0.3566666666666667, 0.8659300184162062, 1.0, 0.8734715434661798, 0.0900509576160001, 0.0, 1.0, 0.1660441520003928], 
reward next is 0.8340, 
noisyNet noise sample is [array([0.56695694], dtype=float32), 0.33944613]. 
=============================================
[2019-04-01 17:30:19,083] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:30:19,085] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7324
[2019-04-01 17:30:19,100] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 0.0, 0.0, 26.0, 25.49538032425264, 7.17445898548322, 0.0, 1.0, 43.73953877060077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3121200.0000, 
sim time next is 3121800.0000, 
raw observation next is [2.1, 100.0, 0.0, 0.0, 26.0, 25.38319399197142, 7.035517417985372, 0.0, 1.0, 45.17825432654867], 
processed observation next is [1.0, 0.13043478260869565, 0.5207756232686982, 1.0, 0.0, 0.0, 1.0, 0.9118848559959173, 0.07035517417985372, 0.0, 1.0, 0.3227018166182048], 
reward next is 0.6773, 
noisyNet noise sample is [array([-0.44790718], dtype=float32), -0.52829576]. 
=============================================
[2019-04-01 17:30:20,079] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2270017e-32
 1.0000000e+00 3.0889784e-33], sum to 1.0000
[2019-04-01 17:30:20,080] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9919
[2019-04-01 17:30:20,090] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 100.0, 33.0, 307.5, 26.0, 27.11945789001119, 21.79336291982495, 1.0, 1.0, 1.094414772492947], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3171600.0000, 
sim time next is 3172200.0000, 
raw observation next is [6.0, 100.0, 24.66666666666666, 243.6666666666666, 26.0, 27.28658518900589, 22.37214455351753, 1.0, 1.0, 1.245002460427203], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.0822222222222222, 0.269244935543278, 1.0, 1.1837978841436987, 0.22372144553517528, 1.0, 1.0, 0.008892874717337164], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.16819349], dtype=float32), -0.7674426]. 
=============================================
[2019-04-01 17:30:20,915] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6329712e-32 1.3437129e-38 8.8580424e-34 3.9929464e-37 2.3147255e-29
 5.2076197e-09 1.0000000e+00], sum to 1.0000
[2019-04-01 17:30:20,915] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5327
[2019-04-01 17:30:20,952] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 60.0, 89.0, 461.3333333333333, 26.0, 25.98339012469243, 10.08594333977711, 1.0, 1.0, 25.46460359505288], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3400800.0000, 
sim time next is 3401400.0000, 
raw observation next is [-1.166666666666667, 60.0, 91.0, 500.6666666666666, 26.0, 26.05157501176848, 10.76036617003442, 1.0, 1.0, 25.54865937624239], 
processed observation next is [1.0, 0.34782608695652173, 0.43028624192059095, 0.6, 0.30333333333333334, 0.5532228360957642, 1.0, 1.0073678588240687, 0.1076036617003442, 1.0, 1.0, 0.18249042411601707], 
reward next is 0.5134, 
noisyNet noise sample is [array([1.0166755], dtype=float32), 0.35317242]. 
=============================================
[2019-04-01 17:30:31,282] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 3.7893684e-37 0.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 17:30:31,283] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8736
[2019-04-01 17:30:31,303] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.28331127947758, 8.294797531310772, 0.0, 1.0, 41.3287263239033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3477600.0000, 
sim time next is 3478200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.25294167551448, 8.231719862769273, 0.0, 1.0, 41.29432872393456], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 1.0, 0.8932773822163546, 0.08231719862769273, 0.0, 1.0, 0.29495949088524687], 
reward next is 0.7050, 
noisyNet noise sample is [array([-1.4910027], dtype=float32), -0.31235528]. 
=============================================
[2019-04-01 17:30:36,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:30:36,169] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2844
[2019-04-01 17:30:36,183] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 75.0, 0.0, 0.0, 26.0, 25.93460084422612, 13.07662119470956, 0.0, 1.0, 30.31740993974168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3533400.0000, 
sim time next is 3534000.0000, 
raw observation next is [-0.6666666666666666, 76.0, 0.0, 0.0, 26.0, 25.89519549829945, 12.8877259705403, 0.0, 1.0, 28.78412707554266], 
processed observation next is [1.0, 0.9130434782608695, 0.44413665743305636, 0.76, 0.0, 0.0, 1.0, 0.985027928328493, 0.128877259705403, 0.0, 1.0, 0.20560090768244757], 
reward next is 0.7944, 
noisyNet noise sample is [array([-1.0000542], dtype=float32), 0.49772766]. 
=============================================
[2019-04-01 17:30:36,188] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[66.21412]
 [67.3591 ]
 [68.25807]
 [68.83743]
 [69.37871]], R is [[65.08955383]
 [65.22210693]
 [65.34571838]
 [65.43671417]
 [65.47129059]].
[2019-04-01 17:30:38,712] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:30:38,714] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5639
[2019-04-01 17:30:38,731] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 70.0, 0.0, 0.0, 26.0, 24.98917303967616, 7.627575706741422, 0.0, 1.0, 40.63227072667817], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3556200.0000, 
sim time next is 3556800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.95040127012639, 7.52623710429453, 0.0, 1.0, 40.5709640023251], 
processed observation next is [0.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 1.0, 0.8500573243037702, 0.07526237104294531, 0.0, 1.0, 0.28979260001660784], 
reward next is 0.7102, 
noisyNet noise sample is [array([-1.0097618], dtype=float32), 0.24569589]. 
=============================================
[2019-04-01 17:30:40,256] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:30:40,256] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7148
[2019-04-01 17:30:40,267] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.0, 30.0, 74.83333333333334, 356.0000000000001, 26.0, 25.5725722938494, 8.96317563067076, 0.0, 1.0, 24.52042016817835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3658800.0000, 
sim time next is 3659400.0000, 
raw observation next is [9.5, 29.0, 89.0, 403.0, 26.0, 25.60920552797835, 9.193920508186928, 0.0, 1.0, 22.22795795432374], 
processed observation next is [0.0, 0.34782608695652173, 0.7257617728531857, 0.29, 0.2966666666666667, 0.4453038674033149, 1.0, 0.9441722182826214, 0.09193920508186927, 0.0, 1.0, 0.1587711282451696], 
reward next is 0.8412, 
noisyNet noise sample is [array([-0.62276095], dtype=float32), 0.99632674]. 
=============================================
[2019-04-01 17:30:44,603] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:30:44,606] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2710
[2019-04-01 17:30:44,622] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.46492549554841, 9.430182840883388, 0.0, 1.0, 38.56375701695045], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3892800.0000, 
sim time next is 3893400.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.47040736930274, 9.197657315904145, 0.0, 1.0, 31.65464300076137], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 1.0, 0.9243439099003916, 0.09197657315904145, 0.0, 1.0, 0.22610459286258122], 
reward next is 0.7739, 
noisyNet noise sample is [array([1.8431976], dtype=float32), 0.06595507]. 
=============================================
[2019-04-01 17:30:44,750] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 4.8960847e-38 1.0000000e+00], sum to 1.0000
[2019-04-01 17:30:44,758] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6181
[2019-04-01 17:30:44,771] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.34628454950309, 7.497641402744819, 0.0, 1.0, 40.64696162790847], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3729600.0000, 
sim time next is 3730200.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.37031235751382, 7.489368291681878, 0.0, 1.0, 40.77823882368234], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 1.0, 0.9100446225019745, 0.07489368291681878, 0.0, 1.0, 0.29127313445487385], 
reward next is 0.7087, 
noisyNet noise sample is [array([-0.45944548], dtype=float32), 0.59125]. 
=============================================
[2019-04-01 17:30:45,190] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:30:45,193] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4483
[2019-04-01 17:30:45,201] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.166666666666667, 49.5, 79.33333333333334, 644.0, 26.0, 25.54531139563733, 10.38932817817369, 0.0, 1.0, 3.225340627930348], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3685800.0000, 
sim time next is 3686400.0000, 
raw observation next is [5.0, 50.0, 75.5, 613.5, 26.0, 25.53543445466318, 10.31341478401739, 0.0, 1.0, 3.219843050659864], 
processed observation next is [0.0, 0.6956521739130435, 0.6011080332409973, 0.5, 0.25166666666666665, 0.6779005524861879, 1.0, 0.9336334935233113, 0.1031341478401739, 0.0, 1.0, 0.02299887893328474], 
reward next is 0.9770, 
noisyNet noise sample is [array([0.17375103], dtype=float32), -0.5907544]. 
=============================================
[2019-04-01 17:30:45,284] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:30:45,287] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1691
[2019-04-01 17:30:45,304] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.833333333333332, 27.33333333333333, 0.0, 0.0, 26.0, 25.46609506735429, 7.955324271239102, 0.0, 1.0, 32.84639781743396], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3646200.0000, 
sim time next is 3646800.0000, 
raw observation next is [9.0, 27.0, 0.0, 0.0, 26.0, 25.51733998757961, 7.998988819354963, 0.0, 1.0, 31.44477044620323], 
processed observation next is [0.0, 0.21739130434782608, 0.7119113573407203, 0.27, 0.0, 0.0, 1.0, 0.9310485696542301, 0.07998988819354963, 0.0, 1.0, 0.22460550318716593], 
reward next is 0.7754, 
noisyNet noise sample is [array([0.1713793], dtype=float32), 0.9509434]. 
=============================================
[2019-04-01 17:30:46,852] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4214063e-30 0.0000000e+00 8.5720084e-38 2.5230724e-35 2.8393367e-32
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:30:46,853] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7732
[2019-04-01 17:30:46,882] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 77.0, 101.8333333333333, 690.6666666666666, 26.0, 26.14204355909046, 10.54043355411796, 1.0, 1.0, 17.90147545652476], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3750000.0000, 
sim time next is 3750600.0000, 
raw observation next is [-3.166666666666667, 77.0, 103.6666666666667, 706.3333333333333, 26.0, 26.19525385978624, 11.00906749452903, 1.0, 1.0, 16.63995096074529], 
processed observation next is [1.0, 0.391304347826087, 0.3748845798707295, 0.77, 0.34555555555555567, 0.7804788213627992, 1.0, 1.0278934085408917, 0.11009067494529029, 1.0, 1.0, 0.11885679257675207], 
reward next is 0.4775, 
noisyNet noise sample is [array([1.8703929], dtype=float32), 0.19628373]. 
=============================================
[2019-04-01 17:30:55,487] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:30:55,488] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3844
[2019-04-01 17:30:55,502] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 49.0, 0.0, 0.0, 26.0, 25.3881596038037, 10.63552687996673, 0.0, 1.0, 44.01668327351786], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3967200.0000, 
sim time next is 3967800.0000, 
raw observation next is [-8.166666666666668, 49.66666666666667, 0.0, 0.0, 26.0, 25.4143598560915, 9.56040988047515, 0.0, 1.0, 41.92237117940949], 
processed observation next is [1.0, 0.9565217391304348, 0.2363804247460757, 0.4966666666666667, 0.0, 0.0, 1.0, 0.9163371222987858, 0.0956040988047515, 0.0, 1.0, 0.29944550842435347], 
reward next is 0.7006, 
noisyNet noise sample is [array([1.1524343], dtype=float32), 1.3607084]. 
=============================================
[2019-04-01 17:30:59,254] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4569593e-25 1.1428517e-36 1.9370612e-28 2.4864800e-33 1.4900437e-19
 6.5302039e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 17:30:59,255] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8130
[2019-04-01 17:30:59,279] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 29.33333333333333, 0.0, 0.0, 26.0, 25.59081716778786, 11.53017159243839, 0.0, 1.0, 37.49014752927212], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4049400.0000, 
sim time next is 4050000.0000, 
raw observation next is [-4.0, 29.0, 0.0, 0.0, 26.0, 25.70917769968346, 11.55288463100083, 0.0, 1.0, 31.16371152988933], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.29, 0.0, 0.0, 1.0, 0.9584539570976369, 0.11552884631000832, 0.0, 1.0, 0.2225979394992095], 
reward next is 0.7774, 
noisyNet noise sample is [array([-0.34307492], dtype=float32), -1.3003216]. 
=============================================
[2019-04-01 17:30:59,290] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[59.562943]
 [58.126434]
 [56.1464  ]
 [54.10064 ]
 [50.942226]], R is [[60.73896408]
 [60.8637886 ]
 [60.96216965]
 [60.99404526]
 [61.1179657 ]].
[2019-04-01 17:31:00,600] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 7.09233e-32 0.00000e+00 0.00000e+00 0.00000e+00
 1.00000e+00], sum to 1.0000
[2019-04-01 17:31:00,601] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2732
[2019-04-01 17:31:00,615] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.14248767728677, 7.933432215660808, 0.0, 1.0, 40.19796079591875], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4060200.0000, 
sim time next is 4060800.0000, 
raw observation next is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.11583131487464, 7.823596088052649, 0.0, 1.0, 40.17612211935424], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.37, 0.0, 0.0, 1.0, 0.8736901878392344, 0.07823596088052649, 0.0, 1.0, 0.2869723008525303], 
reward next is 0.7130, 
noisyNet noise sample is [array([0.28881603], dtype=float32), -0.20345691]. 
=============================================
[2019-04-01 17:31:02,685] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3539034e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.4542642e-21
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:31:02,685] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5358
[2019-04-01 17:31:02,702] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.833333333333333, 34.5, 110.6666666666667, 739.0, 26.0, 26.61271911576961, 12.89821477513149, 1.0, 1.0, 13.02029875239174], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4097400.0000, 
sim time next is 4098000.0000, 
raw observation next is [-1.666666666666667, 34.0, 112.3333333333333, 754.0, 26.0, 26.67805986623258, 13.02387380617739, 1.0, 1.0, 12.157659712547], 
processed observation next is [1.0, 0.43478260869565216, 0.4164358264081256, 0.34, 0.37444444444444436, 0.8331491712707182, 1.0, 1.096865695176083, 0.13023873806177388, 1.0, 1.0, 0.08684042651819286], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4376924], dtype=float32), -0.4183259]. 
=============================================
[2019-04-01 17:31:02,712] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[40.57364 ]
 [40.364075]
 [39.85537 ]
 [39.3385  ]
 [38.816074]], R is [[40.00893021]
 [39.60884094]
 [39.2127533 ]
 [38.93630219]
 [38.79295731]].
[2019-04-01 17:31:13,301] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4432502e-12 1.5986783e-30 7.2339072e-29 1.3455036e-22 2.1308270e-18
 1.0000000e+00 2.2956937e-31], sum to 1.0000
[2019-04-01 17:31:13,301] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5980
[2019-04-01 17:31:13,321] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 24.99084257254909, 8.36221666967233, 1.0, 1.0, 19.49897304819442], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4563000.0000, 
sim time next is 4563600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 24.90225734140247, 8.370371546643378, 1.0, 1.0, 27.12554137456946], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.8431796202003531, 0.08370371546643378, 1.0, 1.0, 0.1937538669612104], 
reward next is 0.8062, 
noisyNet noise sample is [array([0.8514863], dtype=float32), 0.7087584]. 
=============================================
[2019-04-01 17:31:13,933] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.0599036e-17
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:31:13,934] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1420
[2019-04-01 17:31:13,949] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.6, 29.0, 116.5, 847.5, 26.0, 27.1871107345555, 23.57430855434959, 1.0, 1.0, 1.576712407529776], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4366800.0000, 
sim time next is 4367400.0000, 
raw observation next is [14.58333333333333, 29.33333333333334, 116.0, 845.6666666666666, 26.0, 27.49248454811494, 24.77327484191799, 1.0, 1.0, 1.481937944030479], 
processed observation next is [1.0, 0.5652173913043478, 0.8665743305632503, 0.2933333333333334, 0.38666666666666666, 0.9344383057090239, 1.0, 1.2132120783021345, 0.2477327484191799, 1.0, 1.0, 0.010585271028789136], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.36898458], dtype=float32), -1.5658351]. 
=============================================
[2019-04-01 17:31:18,460] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 2.2442736e-19 1.0000000e+00], sum to 1.0000
[2019-04-01 17:31:18,461] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1198
[2019-04-01 17:31:18,468] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.65, 61.83333333333334, 0.0, 0.0, 26.0, 26.14009930695452, 14.50509314284544, 0.0, 1.0, 6.692964055443402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4402200.0000, 
sim time next is 4402800.0000, 
raw observation next is [8.5, 62.0, 0.0, 0.0, 26.0, 26.05406386649985, 14.04216106261698, 0.0, 1.0, 6.68930163998083], 
processed observation next is [1.0, 1.0, 0.698060941828255, 0.62, 0.0, 0.0, 1.0, 1.0077234094999785, 0.1404216106261698, 0.0, 1.0, 0.04778072599986307], 
reward next is 0.9522, 
noisyNet noise sample is [array([0.563821], dtype=float32), -1.0703368]. 
=============================================
[2019-04-01 17:31:19,261] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4748824e-34 0.0000000e+00 6.6397985e-30 0.0000000e+00 8.0000593e-22
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 17:31:19,264] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1089
[2019-04-01 17:31:19,284] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.25, 72.0, 0.0, 0.0, 26.0, 25.24226380097511, 10.01177304465567, 0.0, 1.0, 44.2049195966899], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4488600.0000, 
sim time next is 4489200.0000, 
raw observation next is [-0.3, 72.0, 0.0, 0.0, 26.0, 25.25280823290958, 10.03274186691569, 0.0, 1.0, 43.66765534406199], 
processed observation next is [1.0, 1.0, 0.4542936288088643, 0.72, 0.0, 0.0, 1.0, 0.8932583189870827, 0.1003274186691569, 0.0, 1.0, 0.31191182388615707], 
reward next is 0.6881, 
noisyNet noise sample is [array([-1.8608966], dtype=float32), 1.1940122]. 
=============================================
[2019-04-01 17:31:23,909] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0483762e-26 2.4940773e-35 8.8000116e-19 2.6689091e-31 3.0589198e-21
 1.4953631e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 17:31:23,912] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2909
[2019-04-01 17:31:23,938] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 71.0, 0.0, 0.0, 26.0, 25.34098158570555, 8.655427053262473, 0.0, 1.0, 40.77837291603903], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4512000.0000, 
sim time next is 4512600.0000, 
raw observation next is [-0.9, 71.0, 0.0, 0.0, 26.0, 25.33391685401551, 8.577538821544593, 0.0, 1.0, 40.7312862281058], 
processed observation next is [1.0, 0.21739130434782608, 0.43767313019390586, 0.71, 0.0, 0.0, 1.0, 0.9048452648593585, 0.08577538821544593, 0.0, 1.0, 0.2909377587721843], 
reward next is 0.7091, 
noisyNet noise sample is [array([-0.97148126], dtype=float32), 0.7866592]. 
=============================================
[2019-04-01 17:31:27,069] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:31:27,070] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1749
[2019-04-01 17:31:27,086] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 53.0, 0.0, 0.0, 26.0, 25.35438232114134, 8.548269632688418, 0.0, 1.0, 40.78036267048125], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4829400.0000, 
sim time next is 4830000.0000, 
raw observation next is [-0.6666666666666666, 53.66666666666667, 0.0, 0.0, 26.0, 25.35085306648113, 8.587349638399159, 0.0, 1.0, 41.06791379759328], 
processed observation next is [0.0, 0.9130434782608695, 0.44413665743305636, 0.5366666666666667, 0.0, 0.0, 1.0, 0.9072647237830184, 0.0858734963839916, 0.0, 1.0, 0.29334224141138054], 
reward next is 0.7067, 
noisyNet noise sample is [array([-0.95901746], dtype=float32), -0.5483859]. 
=============================================
[2019-04-01 17:31:27,091] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[81.64763 ]
 [81.90403 ]
 [82.337944]
 [82.736115]
 [83.14187 ]], R is [[81.33438873]
 [81.22975159]
 [81.14698792]
 [81.09177399]
 [81.06276703]].
[2019-04-01 17:31:41,718] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:31:41,718] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:31:41,808] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run15
[2019-04-01 17:31:43,817] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.5401898e-34 1.0000000e+00], sum to 1.0000
[2019-04-01 17:31:43,824] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6091
[2019-04-01 17:31:43,834] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.85493659389357, 6.421079106269029, 0.0, 1.0, 38.75757370463521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4852800.0000, 
sim time next is 4853400.0000, 
raw observation next is [-3.166666666666667, 61.83333333333333, 0.0, 0.0, 26.0, 24.82905282851976, 6.358253213352818, 0.0, 1.0, 38.75613011392286], 
processed observation next is [0.0, 0.17391304347826086, 0.3748845798707295, 0.6183333333333333, 0.0, 0.0, 1.0, 0.83272183264568, 0.06358253213352817, 0.0, 1.0, 0.2768295008137347], 
reward next is 0.7232, 
noisyNet noise sample is [array([0.04907123], dtype=float32), 0.47871813]. 
=============================================
[2019-04-01 17:31:46,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:31:46,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:31:46,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run15
[2019-04-01 17:31:47,248] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:31:47,248] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:31:47,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run15
[2019-04-01 17:31:47,343] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-01 17:31:47,349] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 17:31:47,351] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 17:31:47,351] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:31:47,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:31:47,352] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 17:31:47,353] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:31:47,356] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run20
[2019-04-01 17:31:47,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run20
[2019-04-01 17:31:47,373] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run20
[2019-04-01 17:32:56,118] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.5238361], dtype=float32), -0.6652176]
[2019-04-01 17:32:56,118] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.733333333333334, 60.33333333333334, 0.0, 0.0, 26.0, 23.25983595076226, 5.681518409970056, 0.0, 1.0, 43.57817592536708]
[2019-04-01 17:32:56,118] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 17:32:56,119] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 2.6428717e-31 1.0000000e+00], sampled 0.9869786856720999
[2019-04-01 17:33:29,838] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 17:33:49,607] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 17:33:52,259] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 17:33:53,282] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1900000, evaluation results [1900000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 17:33:53,509] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 4.6937417e-38 0.0000000e+00 0.0000000e+00
 3.8784334e-21 1.0000000e+00], sum to 1.0000
[2019-04-01 17:33:53,513] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1610
[2019-04-01 17:33:53,539] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 26.0, 20.09084448904801, 19.68168796735439, 0.0, 1.0, 43.72001242351597], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5400.0000, 
sim time next is 6000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 26.0, 20.17761501908349, 19.08038811052853, 0.0, 1.0, 43.38839473399693], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 1.0, 0.1682307170119271, 0.19080388110528532, 0.0, 1.0, 0.3099171052428352], 
reward next is 0.6901, 
noisyNet noise sample is [array([-0.8973275], dtype=float32), 1.416058]. 
=============================================
[2019-04-01 17:33:53,548] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[62.111717]
 [59.354053]
 [56.04384 ]
 [51.77691 ]
 [46.058094]], R is [[65.14714813]
 [65.18338776]
 [65.21674347]
 [65.24668884]
 [65.27267456]].
[2019-04-01 17:33:54,848] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3764989e-35 0.0000000e+00 2.7590193e-34 0.0000000e+00 2.7712975e-34
 4.4533604e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 17:33:54,849] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5385
[2019-04-01 17:33:54,863] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 47.33333333333334, 0.0, 0.0, 26.0, 25.17006065525342, 6.550013439601391, 0.0, 1.0, 38.13497101988064], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4944000.0000, 
sim time next is 4944600.0000, 
raw observation next is [-2.5, 48.0, 0.0, 0.0, 26.0, 25.16620706049856, 6.438644697927136, 0.0, 1.0, 38.16098222949297], 
processed observation next is [1.0, 0.21739130434782608, 0.39335180055401664, 0.48, 0.0, 0.0, 1.0, 0.8808867229283658, 0.06438644697927136, 0.0, 1.0, 0.27257844449637836], 
reward next is 0.7274, 
noisyNet noise sample is [array([-1.4649299], dtype=float32), 0.044374388]. 
=============================================
[2019-04-01 17:33:56,788] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:33:56,789] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:33:56,802] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run15
[2019-04-01 17:33:58,179] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:33:58,179] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:33:58,188] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run15
[2019-04-01 17:33:58,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:33:58,328] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:33:58,341] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run15
[2019-04-01 17:33:58,750] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:33:58,751] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:33:58,771] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run15
[2019-04-01 17:33:59,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:33:59,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:33:59,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run15
[2019-04-01 17:34:00,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:34:00,155] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:34:00,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run15
[2019-04-01 17:34:00,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:34:00,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:34:00,238] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run15
[2019-04-01 17:34:02,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:34:02,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:34:02,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:34:02,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:34:02,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run15
[2019-04-01 17:34:02,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run15
[2019-04-01 17:34:02,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:34:02,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:34:02,197] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run15
[2019-04-01 17:34:02,711] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8606259e-33
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:34:02,711] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9333
[2019-04-01 17:34:02,736] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:34:02,736] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:34:02,740] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run15
[2019-04-01 17:34:02,782] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 61.0, 131.5, 583.1666666666666, 26.0, 25.54967821647951, 8.837246168559846, 1.0, 1.0, 38.77079125428948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 132000.0000, 
sim time next is 132600.0000, 
raw observation next is [-7.899999999999999, 61.0, 133.0, 563.3333333333334, 26.0, 25.53834987700818, 9.008424235702037, 1.0, 1.0, 43.64829911594011], 
processed observation next is [1.0, 0.5217391304347826, 0.24376731301939064, 0.61, 0.44333333333333336, 0.6224677716390424, 1.0, 0.93404998242974, 0.09008424235702037, 1.0, 1.0, 0.31177356511385795], 
reward next is 0.6882, 
noisyNet noise sample is [array([1.2186581], dtype=float32), -0.452877]. 
=============================================
[2019-04-01 17:34:03,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:34:03,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:34:03,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run15
[2019-04-01 17:34:03,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:34:03,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:34:03,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run15
[2019-04-01 17:34:20,851] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 8.41136e-34 1.00000e+00
 0.00000e+00], sum to 1.0000
[2019-04-01 17:34:20,851] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7841
[2019-04-01 17:34:20,917] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 26.0, 25.17955426115731, 8.539361033151868, 0.0, 1.0, 47.37580234760083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 158400.0000, 
sim time next is 159000.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 26.0, 25.17029052979799, 8.22023471036359, 1.0, 1.0, 45.22597576138253], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 1.0, 0.8814700756854273, 0.0822023471036359, 1.0, 1.0, 0.3230426840098752], 
reward next is 0.6770, 
noisyNet noise sample is [array([-1.4528713], dtype=float32), -0.39566898]. 
=============================================
[2019-04-01 17:34:20,933] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[48.326145]
 [41.54248 ]
 [41.60568 ]
 [41.61214 ]
 [41.78855 ]], R is [[46.79971313]
 [46.99331665]
 [47.16698837]
 [47.21717834]
 [47.22397995]].
[2019-04-01 17:34:21,675] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3459558e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.1870644e-34
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:34:21,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2623
[2019-04-01 17:34:21,739] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.566666666666666, 78.0, 41.5, 246.6666666666667, 26.0, 25.35183664762779, 6.947448092226407, 1.0, 1.0, 45.7564680524394], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 204000.0000, 
sim time next is 204600.0000, 
raw observation next is [-8.483333333333334, 78.0, 49.0, 197.3333333333333, 26.0, 25.44887983556614, 6.811266302184921, 1.0, 1.0, 46.44030288410686], 
processed observation next is [1.0, 0.34782608695652173, 0.2276084949215143, 0.78, 0.16333333333333333, 0.21804788213627987, 1.0, 0.9212685479380198, 0.0681126630218492, 1.0, 1.0, 0.3317164491721919], 
reward next is 0.6683, 
noisyNet noise sample is [array([1.0921166], dtype=float32), -0.21099623]. 
=============================================
[2019-04-01 17:34:26,870] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:34:26,871] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3660
[2019-04-01 17:34:26,929] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.79019911673746, 8.849473516955767, 1.0, 1.0, 36.24228997610773], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 238200.0000, 
sim time next is 238800.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.84446592645214, 8.506514590075653, 1.0, 1.0, 33.53571361852772], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 1.0, 0.9777808466360202, 0.08506514590075653, 1.0, 1.0, 0.2395408115609123], 
reward next is 0.7605, 
noisyNet noise sample is [array([0.47128585], dtype=float32), 1.7576716]. 
=============================================
[2019-04-01 17:34:29,028] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.6813223e-38
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:34:29,029] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4691
[2019-04-01 17:34:29,090] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.37078288217008, 8.342842936622041, 1.0, 1.0, 46.57557739975445], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 413400.0000, 
sim time next is 414000.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.25777697801819, 8.118958509766577, 1.0, 1.0, 46.6880090952973], 
processed observation next is [1.0, 0.8260869565217391, 0.1994459833795014, 0.4, 0.0, 0.0, 1.0, 0.8939681397168842, 0.08118958509766577, 1.0, 1.0, 0.3334857792521236], 
reward next is 0.6665, 
noisyNet noise sample is [array([-0.44125825], dtype=float32), 1.8037977]. 
=============================================
[2019-04-01 17:34:29,115] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[56.38101 ]
 [55.631447]
 [55.017647]
 [54.358948]
 [53.85292 ]], R is [[57.12444305]
 [57.2205162 ]
 [57.31628036]
 [57.41151428]
 [57.50656891]].
[2019-04-01 17:34:29,667] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.1437092e-10
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:34:29,667] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6731
[2019-04-01 17:34:29,709] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.78333333333333, 60.5, 93.33333333333334, 560.6666666666666, 26.0, 25.94943595153516, 8.837854523318393, 1.0, 1.0, 27.75601975636687], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 298200.0000, 
sim time next is 298800.0000, 
raw observation next is [-10.6, 60.0, 96.5, 585.0, 26.0, 25.93124512956868, 8.801070486595478, 1.0, 1.0, 26.25140995373532], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.6, 0.32166666666666666, 0.6464088397790055, 1.0, 0.9901778756526686, 0.08801070486595478, 1.0, 1.0, 0.1875100710981094], 
reward next is 0.8125, 
noisyNet noise sample is [array([-1.151409], dtype=float32), -0.6886106]. 
=============================================
[2019-04-01 17:34:31,275] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.3319956e-19 4.0488956e-27 3.7063210e-17 2.1089028e-22 2.4516544e-06
 9.9999750e-01 6.4749740e-14], sum to 1.0000
[2019-04-01 17:34:31,275] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2579
[2019-04-01 17:34:31,337] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.71666666666667, 69.5, 9.999999999999998, 145.3333333333333, 26.0, 24.38104788149638, 5.970890674714489, 1.0, 1.0, 81.2870760143798], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 287400.0000, 
sim time next is 288000.0000, 
raw observation next is [-12.8, 70.0, 15.0, 205.5, 26.0, 24.80732870644848, 6.225477791266125, 1.0, 1.0, 80.57944949407923], 
processed observation next is [1.0, 0.34782608695652173, 0.1080332409972299, 0.7, 0.05, 0.22707182320441988, 1.0, 0.8296183866354971, 0.06225477791266125, 1.0, 1.0, 0.5755674963862802], 
reward next is 0.4244, 
noisyNet noise sample is [array([-0.03044984], dtype=float32), -0.5642775]. 
=============================================
[2019-04-01 17:34:31,339] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[47.901695]
 [49.1047  ]
 [51.644855]
 [55.20009 ]
 [60.586624]], R is [[48.05026245]
 [47.98913574]
 [47.91680527]
 [47.81642151]
 [47.63039398]].
[2019-04-01 17:34:32,193] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6507045e-29 0.0000000e+00 2.9777742e-28 3.3118130e-35 4.7770474e-17
 1.0000000e+00 3.8234416e-10], sum to 1.0000
[2019-04-01 17:34:32,193] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4577
[2019-04-01 17:34:32,215] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.2, 68.5, 0.0, 0.0, 26.0, 23.50445681459592, 5.42583232713686, 0.0, 1.0, 45.49938559218361], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 271800.0000, 
sim time next is 272400.0000, 
raw observation next is [-9.3, 69.0, 0.0, 0.0, 26.0, 23.45336840781442, 5.441237450347259, 0.0, 1.0, 45.64517158365322], 
processed observation next is [1.0, 0.13043478260869565, 0.20498614958448752, 0.69, 0.0, 0.0, 1.0, 0.6361954868306314, 0.05441237450347259, 0.0, 1.0, 0.3260369398832373], 
reward next is 0.6740, 
noisyNet noise sample is [array([-1.5344195], dtype=float32), -0.80603814]. 
=============================================
[2019-04-01 17:34:48,768] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:34:48,768] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3858
[2019-04-01 17:34:48,803] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666666, 28.66666666666667, 106.0, 0.0, 26.0, 25.104965492092, 5.937742850162787, 1.0, 1.0, 29.93316930740645], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 471000.0000, 
sim time next is 471600.0000, 
raw observation next is [-2.3, 28.0, 110.0, 0.0, 26.0, 25.09683215101138, 6.000601237138603, 1.0, 1.0, 33.73883848791528], 
processed observation next is [1.0, 0.4782608695652174, 0.3988919667590028, 0.28, 0.36666666666666664, 0.0, 1.0, 0.8709760215730543, 0.060006012371386024, 1.0, 1.0, 0.24099170348510915], 
reward next is 0.7590, 
noisyNet noise sample is [array([0.0478559], dtype=float32), -1.1859244]. 
=============================================
[2019-04-01 17:34:50,793] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:34:50,795] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3590
[2019-04-01 17:34:50,852] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1833333333333333, 36.5, 68.66666666666666, 0.0, 26.0, 26.2147504035552, 8.860736742628035, 1.0, 1.0, 27.61753827953155], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 486600.0000, 
sim time next is 487200.0000, 
raw observation next is [0.3666666666666667, 36.0, 62.33333333333334, 0.0, 26.0, 26.250137993143, 8.883301426886758, 1.0, 1.0, 25.84179417509128], 
processed observation next is [1.0, 0.6521739130434783, 0.4727608494921515, 0.36, 0.2077777777777778, 0.0, 1.0, 1.0357339990204284, 0.08883301426886758, 1.0, 1.0, 0.18458424410779486], 
reward next is 0.8154, 
noisyNet noise sample is [array([-1.6124402], dtype=float32), -0.65390193]. 
=============================================
[2019-04-01 17:34:52,989] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.6309836e-35
 2.0385517e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 17:34:52,990] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0396
[2019-04-01 17:34:53,002] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 26.0, 24.89497672656292, 6.75054987055309, 0.0, 1.0, 42.38187587253328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 597600.0000, 
sim time next is 598200.0000, 
raw observation next is [-2.9, 83.0, 0.0, 0.0, 26.0, 24.86473780096197, 6.675382743112135, 0.0, 1.0, 42.34950858387246], 
processed observation next is [0.0, 0.9565217391304348, 0.38227146814404434, 0.83, 0.0, 0.0, 1.0, 0.8378196858517098, 0.06675382743112135, 0.0, 1.0, 0.3024964898848033], 
reward next is 0.6975, 
noisyNet noise sample is [array([0.02336187], dtype=float32), 1.0523819]. 
=============================================
[2019-04-01 17:34:53,351] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.2703686e-35 0.0000000e+00 9.8421954e-29 1.8842707e-38 5.2607200e-28
 1.0000000e+00 1.6756170e-16], sum to 1.0000
[2019-04-01 17:34:53,351] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7707
[2019-04-01 17:34:53,365] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.2, 94.33333333333334, 0.0, 0.0, 26.0, 24.94781233741558, 6.757537478476308, 0.0, 1.0, 39.12582795584517], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 519600.0000, 
sim time next is 520200.0000, 
raw observation next is [4.4, 93.0, 0.0, 0.0, 26.0, 24.93840141189388, 6.739193024170317, 0.0, 1.0, 39.10304444922073], 
processed observation next is [0.0, 0.0, 0.5844875346260389, 0.93, 0.0, 0.0, 1.0, 0.848343058841983, 0.06739193024170316, 0.0, 1.0, 0.27930746035157666], 
reward next is 0.7207, 
noisyNet noise sample is [array([1.034165], dtype=float32), 0.8650158]. 
=============================================
[2019-04-01 17:35:04,847] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:35:04,847] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9481
[2019-04-01 17:35:04,897] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.533333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 24.91616879555493, 8.274655717900059, 1.0, 1.0, 57.27993175192171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 754800.0000, 
sim time next is 755400.0000, 
raw observation next is [-3.716666666666666, 55.66666666666667, 0.0, 0.0, 26.0, 25.19628800722327, 8.917466945988734, 1.0, 1.0, 36.83246471261894], 
processed observation next is [1.0, 0.7391304347826086, 0.3596491228070176, 0.5566666666666668, 0.0, 0.0, 1.0, 0.8851840010318958, 0.08917466945988733, 1.0, 1.0, 0.2630890336615639], 
reward next is 0.7369, 
noisyNet noise sample is [array([2.4280448], dtype=float32), -0.95534396]. 
=============================================
[2019-04-01 17:35:40,354] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:35:40,357] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8166
[2019-04-01 17:35:40,364] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 75.0, 0.0, 26.0, 25.85089372792721, 10.18385601385887, 1.0, 1.0, 10.40355882169328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1420800.0000, 
sim time next is 1421400.0000, 
raw observation next is [0.0, 95.0, 78.0, 0.0, 26.0, 25.82358701970082, 10.16627560088396, 1.0, 1.0, 9.797899523476469], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.26, 0.0, 1.0, 0.9747981456715458, 0.1016627560088396, 1.0, 1.0, 0.0699849965962605], 
reward next is 0.8635, 
noisyNet noise sample is [array([-1.935481], dtype=float32), -0.2856219]. 
=============================================
[2019-04-01 17:35:41,462] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:35:41,473] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0876
[2019-04-01 17:35:41,497] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333333, 93.0, 95.0, 0.0, 26.0, 25.48105096789766, 9.012901313329666, 1.0, 1.0, 14.03398445744093], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1428000.0000, 
sim time next is 1428600.0000, 
raw observation next is [0.4166666666666667, 92.5, 94.0, 0.0, 26.0, 25.29093152671762, 8.053934487063925, 1.0, 1.0, 14.17230405348976], 
processed observation next is [1.0, 0.5217391304347826, 0.47414589104339805, 0.925, 0.31333333333333335, 0.0, 1.0, 0.898704503816803, 0.08053934487063925, 1.0, 1.0, 0.10123074323921258], 
reward next is 0.8988, 
noisyNet noise sample is [array([0.4321793], dtype=float32), -1.5367128]. 
=============================================
[2019-04-01 17:35:42,219] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 8.1094995e-31 0.0000000e+00 8.6571820e-24
 1.0000000e+00 2.5452129e-25], sum to 1.0000
[2019-04-01 17:35:42,221] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7830
[2019-04-01 17:35:42,237] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 94.0, 0.0, 0.0, 26.0, 25.37970714253436, 9.345172349318196, 0.0, 1.0, 33.11361463915236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1479600.0000, 
sim time next is 1480200.0000, 
raw observation next is [2.2, 94.33333333333334, 0.0, 0.0, 26.0, 25.31706923371708, 9.208397145585222, 0.0, 1.0, 35.33540697771382], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9433333333333335, 0.0, 0.0, 1.0, 0.902438461959583, 0.09208397145585222, 0.0, 1.0, 0.2523957641265273], 
reward next is 0.7476, 
noisyNet noise sample is [array([-0.10203818], dtype=float32), -0.3311217]. 
=============================================
[2019-04-01 17:35:59,023] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 2.2145407e-28 1.0000000e+00], sum to 1.0000
[2019-04-01 17:35:59,024] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4926
[2019-04-01 17:35:59,106] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 85.0, 99.0, 0.0, 26.0, 24.90926920623133, 8.121490901812349, 0.0, 1.0, 46.18384230506884], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1780200.0000, 
sim time next is 1780800.0000, 
raw observation next is [-2.8, 85.66666666666667, 93.5, 0.0, 26.0, 24.91961906968768, 8.300392968785902, 0.0, 1.0, 48.73317955244497], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8566666666666667, 0.31166666666666665, 0.0, 1.0, 0.84565986709824, 0.08300392968785902, 0.0, 1.0, 0.3480941396603212], 
reward next is 0.6519, 
noisyNet noise sample is [array([0.2892284], dtype=float32), 0.6116459]. 
=============================================
[2019-04-01 17:36:01,526] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.2877134e-38 0.0000000e+00 1.9659919e-26 0.0000000e+00 1.3540479e-34
 3.5230549e-12 1.0000000e+00], sum to 1.0000
[2019-04-01 17:36:01,527] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2682
[2019-04-01 17:36:01,540] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 26.0, 24.98917726374149, 7.439536693779324, 0.0, 1.0, 45.34445655959163], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1803600.0000, 
sim time next is 1804200.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 26.0, 25.01232652411599, 7.367169578511041, 0.0, 1.0, 45.31232700393838], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 1.0, 0.8589037891594273, 0.07367169578511042, 0.0, 1.0, 0.32365947859955985], 
reward next is 0.6763, 
noisyNet noise sample is [array([1.2711489], dtype=float32), -0.0317654]. 
=============================================
[2019-04-01 17:36:02,137] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 3.1421017e-26], sum to 1.0000
[2019-04-01 17:36:02,140] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8070
[2019-04-01 17:36:02,150] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.9, 85.0, 0.0, 0.0, 26.0, 24.2046006712478, 5.442800560292421, 0.0, 1.0, 40.3622563821705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2003400.0000, 
sim time next is 2004000.0000, 
raw observation next is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.23504655131077, 5.428855838760545, 0.0, 1.0, 40.6074071228643], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.8566666666666667, 0.0, 0.0, 1.0, 0.7478637930443958, 0.05428855838760545, 0.0, 1.0, 0.2900529080204593], 
reward next is 0.7099, 
noisyNet noise sample is [array([0.00315675], dtype=float32), -0.15639111]. 
=============================================
[2019-04-01 17:36:02,158] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[65.87305 ]
 [65.921394]
 [66.033104]
 [66.14206 ]
 [66.218796]], R is [[65.87696838]
 [65.92990112]
 [65.97515106]
 [66.02416992]
 [66.07289886]].
[2019-04-01 17:36:06,520] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 7.6528406e-36 0.0000000e+00 7.4849467e-37
 1.0000000e+00 7.3541702e-12], sum to 1.0000
[2019-04-01 17:36:06,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8899
[2019-04-01 17:36:06,535] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.80128888915242, 5.340599878169371, 0.0, 1.0, 44.31540119288466], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1914000.0000, 
sim time next is 1914600.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.77378419136899, 5.338503501414422, 0.0, 1.0, 44.39117603693089], 
processed observation next is [1.0, 0.13043478260869565, 0.2299168975069252, 0.78, 0.0, 0.0, 1.0, 0.6819691701955699, 0.053385035014144225, 0.0, 1.0, 0.3170798288352206], 
reward next is 0.6829, 
noisyNet noise sample is [array([0.25447366], dtype=float32), 0.6423863]. 
=============================================
[2019-04-01 17:36:15,386] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.8524162e-35
 1.0000000e+00 2.2777492e-17], sum to 1.0000
[2019-04-01 17:36:15,394] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5140
[2019-04-01 17:36:15,429] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.55976419400739, 6.265997795973322, 0.0, 1.0, 42.28396842862157], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1985400.0000, 
sim time next is 1986000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.52211357886634, 6.194643774199274, 0.0, 1.0, 42.20830312638734], 
processed observation next is [1.0, 1.0, 0.30747922437673136, 0.83, 0.0, 0.0, 1.0, 0.788873368409477, 0.06194643774199274, 0.0, 1.0, 0.3014878794741953], 
reward next is 0.6985, 
noisyNet noise sample is [array([-0.5712725], dtype=float32), -0.83536595]. 
=============================================
[2019-04-01 17:36:15,434] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.20237 ]
 [73.287346]
 [73.36486 ]
 [73.446884]
 [72.96105 ]], R is [[73.08444977]
 [73.05158234]
 [73.01862335]
 [72.98570251]
 [72.95294952]].
[2019-04-01 17:36:16,927] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5430494e-32 0.0000000e+00 8.6791318e-33 0.0000000e+00 3.9216010e-13
 1.0000000e+00 5.9784764e-23], sum to 1.0000
[2019-04-01 17:36:16,928] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4376
[2019-04-01 17:36:16,942] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.25530897903938, 5.423909017618651, 0.0, 1.0, 40.74399881465406], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2001600.0000, 
sim time next is 2002200.0000, 
raw observation next is [-5.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.17853248320431, 5.378039661704065, 0.0, 1.0, 40.7726061166332], 
processed observation next is [1.0, 0.17391304347826086, 0.30470914127423826, 0.8366666666666667, 0.0, 0.0, 1.0, 0.739790354743473, 0.05378039661704065, 0.0, 1.0, 0.2912329008330943], 
reward next is 0.7088, 
noisyNet noise sample is [array([-0.17558245], dtype=float32), 0.21703984]. 
=============================================
[2019-04-01 17:36:20,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:36:20,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9555
[2019-04-01 17:36:20,854] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.300000000000001, 84.66666666666667, 76.5, 0.0, 26.0, 26.95569933740423, 11.72469968778733, 1.0, 1.0, 27.22198027818465], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2042400.0000, 
sim time next is 2043000.0000, 
raw observation next is [-4.2, 84.0, 71.0, 0.0, 26.0, 26.25601817879022, 10.7472452748642, 1.0, 1.0, 25.38363345952219], 
processed observation next is [1.0, 0.6521739130434783, 0.34626038781163443, 0.84, 0.23666666666666666, 0.0, 1.0, 1.0365740255414602, 0.107472452748642, 1.0, 1.0, 0.18131166756801564], 
reward next is 0.5198, 
noisyNet noise sample is [array([0.5740929], dtype=float32), -0.3873323]. 
=============================================
[2019-04-01 17:36:20,857] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[57.790356]
 [57.4419  ]
 [57.203186]
 [57.037876]
 [56.940567]], R is [[57.90500259]
 [57.44163132]
 [56.8672142 ]
 [56.29854202]
 [55.73555756]].
[2019-04-01 17:36:29,638] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 3.6198105e-17 1.0000000e+00], sum to 1.0000
[2019-04-01 17:36:29,638] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3279
[2019-04-01 17:36:29,733] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 31.16666666666667, 0.0, 26.0, 24.40248000511763, 7.007303936771052, 0.0, 1.0, 92.27371562763133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2362800.0000, 
sim time next is 2363400.0000, 
raw observation next is [-3.4, 69.0, 37.0, 0.0, 26.0, 25.11283251231447, 7.788825041816497, 0.0, 1.0, 60.38849584486335], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.12333333333333334, 0.0, 1.0, 0.8732617874734956, 0.07788825041816497, 0.0, 1.0, 0.4313463988918811], 
reward next is 0.5687, 
noisyNet noise sample is [array([-1.0596493], dtype=float32), 0.835141]. 
=============================================
[2019-04-01 17:36:29,751] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.0801392e-29 0.0000000e+00 1.4772380e-24 5.4781572e-35 5.8570587e-21
 1.0000000e+00 2.2660372e-20], sum to 1.0000
[2019-04-01 17:36:29,751] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3289
[2019-04-01 17:36:29,767] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.7, 75.66666666666666, 0.0, 0.0, 26.0, 23.63788424809364, 5.276573461196345, 0.0, 1.0, 41.39227703184054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2184600.0000, 
sim time next is 2185200.0000, 
raw observation next is [-5.6, 75.0, 0.0, 0.0, 26.0, 23.60580202429735, 5.289814424951726, 0.0, 1.0, 41.42946503540024], 
processed observation next is [1.0, 0.30434782608695654, 0.30747922437673136, 0.75, 0.0, 0.0, 1.0, 0.6579717177567643, 0.05289814424951726, 0.0, 1.0, 0.29592475025285886], 
reward next is 0.7041, 
noisyNet noise sample is [array([2.0829885], dtype=float32), 0.5315521]. 
=============================================
[2019-04-01 17:36:32,588] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1296105e-35 0.0000000e+00 1.4550265e-35 1.0151669e-37 2.1009128e-35
 1.0000000e+00 1.3690052e-20], sum to 1.0000
[2019-04-01 17:36:32,588] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0575
[2019-04-01 17:36:32,628] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.8, 76.33333333333334, 0.0, 0.0, 26.0, 23.63253334145589, 5.279333691344327, 0.0, 1.0, 41.38108567937494], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2184000.0000, 
sim time next is 2184600.0000, 
raw observation next is [-5.7, 75.66666666666666, 0.0, 0.0, 26.0, 23.63788424809364, 5.276573461196345, 0.0, 1.0, 41.39227703184054], 
processed observation next is [1.0, 0.2608695652173913, 0.30470914127423826, 0.7566666666666666, 0.0, 0.0, 1.0, 0.6625548925848059, 0.052765734611963454, 0.0, 1.0, 0.2956591216560039], 
reward next is 0.7043, 
noisyNet noise sample is [array([1.2070619], dtype=float32), 1.1627569]. 
=============================================
[2019-04-01 17:36:37,366] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.1741406e-30
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:36:37,368] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7386
[2019-04-01 17:36:37,407] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 25.06761959213751, 9.156772084233074, 1.0, 1.0, 42.48487329196957], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2232000.0000, 
sim time next is 2232600.0000, 
raw observation next is [-5.0, 70.5, 0.0, 0.0, 26.0, 25.24598399240283, 9.135115661912222, 0.0, 1.0, 34.25989854279925], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.705, 0.0, 0.0, 1.0, 0.8922834274861187, 0.09135115661912221, 0.0, 1.0, 0.24471356101999464], 
reward next is 0.7553, 
noisyNet noise sample is [array([-1.0962433], dtype=float32), 0.6251829]. 
=============================================
[2019-04-01 17:36:48,620] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6321585e-37 0.0000000e+00 1.3074742e-37 0.0000000e+00 4.2545569e-37
 1.0000000e+00 1.4884262e-32], sum to 1.0000
[2019-04-01 17:36:48,633] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2969
[2019-04-01 17:36:48,650] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.666666666666666, 69.5, 0.0, 0.0, 26.0, 24.57679152489416, 6.410448871622663, 0.0, 1.0, 43.78437651175913], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2681400.0000, 
sim time next is 2682000.0000, 
raw observation next is [-9.0, 69.0, 0.0, 0.0, 26.0, 24.54054368082158, 6.39372732674187, 0.0, 1.0, 43.77905168941097], 
processed observation next is [1.0, 0.043478260869565216, 0.21329639889196678, 0.69, 0.0, 0.0, 1.0, 0.7915062401173688, 0.0639372732674187, 0.0, 1.0, 0.3127075120672212], 
reward next is 0.6873, 
noisyNet noise sample is [array([0.5722582], dtype=float32), 0.07080696]. 
=============================================
[2019-04-01 17:36:48,658] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[64.74405]
 [65.0795 ]
 [65.47254]
 [65.85759]
 [66.24806]], R is [[64.58835602]
 [64.62973022]
 [64.67053986]
 [64.7110672 ]
 [64.75098419]].
[2019-04-01 17:36:50,368] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 6.864402e-24 1.000000e+00], sum to 1.0000
[2019-04-01 17:36:50,368] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2943
[2019-04-01 17:36:50,382] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.9, 56.0, 0.0, 0.0, 26.0, 23.81845192240451, 5.276677620108669, 0.0, 1.0, 43.40130877328091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2430600.0000, 
sim time next is 2431200.0000, 
raw observation next is [-8.0, 57.0, 0.0, 0.0, 26.0, 23.77412711091322, 5.287576923617014, 0.0, 1.0, 43.48947957087827], 
processed observation next is [0.0, 0.13043478260869565, 0.24099722991689754, 0.57, 0.0, 0.0, 1.0, 0.6820181587018885, 0.052875769236170135, 0.0, 1.0, 0.3106391397919876], 
reward next is 0.6894, 
noisyNet noise sample is [array([-0.46355066], dtype=float32), 1.2228572]. 
=============================================
[2019-04-01 17:36:54,560] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.0118188e-38
 1.0000000e+00 2.2891206e-36], sum to 1.0000
[2019-04-01 17:36:54,560] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6626
[2019-04-01 17:36:54,647] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 54.0, 0.0, 0.0, 26.0, 24.79026132931752, 5.524898849579396, 0.0, 1.0, 38.35302333697467], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2530800.0000, 
sim time next is 2531400.0000, 
raw observation next is [-2.8, 54.0, 0.0, 0.0, 26.0, 24.76393088344955, 5.7054757248295, 0.0, 1.0, 67.4475019408137], 
processed observation next is [1.0, 0.30434782608695654, 0.38504155124653744, 0.54, 0.0, 0.0, 1.0, 0.82341869763565, 0.057054757248295, 0.0, 1.0, 0.4817678710058121], 
reward next is 0.5182, 
noisyNet noise sample is [array([0.8681868], dtype=float32), 0.41254333]. 
=============================================
[2019-04-01 17:36:55,508] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:36:55,508] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3381
[2019-04-01 17:36:55,522] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.2500000000000001, 43.0, 232.0, 71.0, 26.0, 25.77710908511723, 7.688126243691514, 1.0, 1.0, 9.970825893885504], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2547000.0000, 
sim time next is 2547600.0000, 
raw observation next is [0.5333333333333334, 41.66666666666667, 229.6666666666667, 62.83333333333334, 26.0, 25.79523877270145, 7.744151788334793, 1.0, 1.0, 9.330309976478157], 
processed observation next is [1.0, 0.4782608695652174, 0.4773776546629733, 0.41666666666666674, 0.7655555555555558, 0.06942909760589319, 1.0, 0.970748396100207, 0.07744151788334792, 1.0, 1.0, 0.06664507126055826], 
reward next is 0.9334, 
noisyNet noise sample is [array([0.44740048], dtype=float32), -1.5109167]. 
=============================================
[2019-04-01 17:36:57,185] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.3029604e-35 0.0000000e+00 4.2620707e-33 0.0000000e+00 5.5077258e-19
 1.0000000e+00 3.3075220e-19], sum to 1.0000
[2019-04-01 17:36:57,187] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2793
[2019-04-01 17:36:57,215] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.80778909245603, 6.257979494783949, 0.0, 1.0, 41.22268282237501], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2601600.0000, 
sim time next is 2602200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.81771570346349, 6.152059641225994, 0.0, 1.0, 41.25581282532288], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 1.0, 0.8311022433519274, 0.06152059641225994, 0.0, 1.0, 0.2946843773237348], 
reward next is 0.7053, 
noisyNet noise sample is [array([-1.0143628], dtype=float32), -1.0222816]. 
=============================================
[2019-04-01 17:37:09,029] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3605595e-29 0.0000000e+00 1.2753722e-27 1.4555673e-32 1.3195122e-18
 1.0000000e+00 7.6616137e-11], sum to 1.0000
[2019-04-01 17:37:09,030] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4169
[2019-04-01 17:37:09,043] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.35450668300552, 5.50281829289955, 0.0, 1.0, 40.68286886253172], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2784600.0000, 
sim time next is 2785200.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.29452376981603, 5.484418542195937, 0.0, 1.0, 40.7390814125915], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 1.0, 0.7563605385451473, 0.05484418542195937, 0.0, 1.0, 0.2909934386613679], 
reward next is 0.7090, 
noisyNet noise sample is [array([1.2996092], dtype=float32), -0.49357656]. 
=============================================
[2019-04-01 17:37:12,041] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4983923e-29 0.0000000e+00 5.6668123e-30 1.4991096e-33 5.8162572e-25
 1.0000000e+00 1.0219284e-28], sum to 1.0000
[2019-04-01 17:37:12,041] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7283
[2019-04-01 17:37:12,059] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.9702446345179, 6.883463744634703, 0.0, 1.0, 54.69856811382954], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2868000.0000, 
sim time next is 2868600.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 25.01149275646402, 6.801032215809305, 0.0, 1.0, 54.66529002704886], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 1.0, 0.8587846794948598, 0.06801032215809305, 0.0, 1.0, 0.3904663573360633], 
reward next is 0.6095, 
noisyNet noise sample is [array([0.04835617], dtype=float32), 0.5494648]. 
=============================================
[2019-04-01 17:37:16,536] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.4521206e-28], sum to 1.0000
[2019-04-01 17:37:16,543] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1247
[2019-04-01 17:37:16,558] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.34676023362282, 7.69614251252167, 0.0, 1.0, 55.10320229499445], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2858400.0000, 
sim time next is 2859000.0000, 
raw observation next is [1.0, 80.16666666666667, 0.0, 0.0, 26.0, 25.30127755217635, 7.629261980503027, 0.0, 1.0, 55.13761311415195], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.8016666666666667, 0.0, 0.0, 1.0, 0.9001825074537645, 0.07629261980503027, 0.0, 1.0, 0.3938400936725139], 
reward next is 0.6062, 
noisyNet noise sample is [array([-0.5614622], dtype=float32), 1.1915407]. 
=============================================
[2019-04-01 17:37:16,561] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[70.32519 ]
 [70.291046]
 [70.312164]
 [70.24913 ]
 [70.25558 ]], R is [[70.21420288]
 [70.11846924]
 [70.02458954]
 [69.93586731]
 [69.86312866]].
[2019-04-01 17:37:23,749] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.7051378e-37 0.0000000e+00 1.3862982e-23
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:37:23,749] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9597
[2019-04-01 17:37:23,762] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.0569163900496, 5.320881890176449, 0.0, 1.0, 39.65284833437784], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3039000.0000, 
sim time next is 3039600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.02706656545398, 5.302830132063963, 0.0, 1.0, 39.69012197383146], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.7181523664934255, 0.053028301320639626, 0.0, 1.0, 0.2835008712416533], 
reward next is 0.7165, 
noisyNet noise sample is [array([-0.6496285], dtype=float32), 1.5147917]. 
=============================================
[2019-04-01 17:37:30,335] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:37:30,335] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2896
[2019-04-01 17:37:30,355] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 99.0, 647.0, 26.0, 26.26277894603754, 11.56533812037098, 1.0, 1.0, 14.87925712298181], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3144600.0000, 
sim time next is 3145200.0000, 
raw observation next is [7.0, 100.0, 100.5, 663.5, 26.0, 26.34064174034935, 12.20489871304754, 1.0, 1.0, 13.711444563569], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.335, 0.7331491712707182, 1.0, 1.048663105764193, 0.12204898713047539, 1.0, 1.0, 0.09793888973977857], 
reward next is 0.0201, 
noisyNet noise sample is [array([0.53695387], dtype=float32), -1.7890264]. 
=============================================
[2019-04-01 17:37:32,156] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.1885302e-23
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:37:32,157] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9723
[2019-04-01 17:37:32,164] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 92.5, 721.0, 26.0, 26.85792697092146, 20.44505111063243, 1.0, 1.0, 1.513221929922908], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3164400.0000, 
sim time next is 3165000.0000, 
raw observation next is [6.933333333333334, 99.83333333333334, 89.66666666666666, 707.0, 26.0, 27.04530606182406, 21.42978296337579, 1.0, 1.0, 1.447940019719437], 
processed observation next is [1.0, 0.6521739130434783, 0.6546629732225301, 0.9983333333333334, 0.29888888888888887, 0.7812154696132597, 1.0, 1.1493294374034373, 0.2142978296337579, 1.0, 1.0, 0.010342428712281693], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3189569], dtype=float32), 0.2299066]. 
=============================================
[2019-04-01 17:37:32,167] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[29.04452 ]
 [28.678888]
 [28.419485]
 [28.216366]
 [28.370907]], R is [[29.13767624]
 [28.84630013]
 [28.55783653]
 [28.27225876]
 [27.98953629]].
[2019-04-01 17:37:51,670] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-01 17:37:51,672] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 17:37:51,672] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:37:51,674] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 17:37:51,677] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 17:37:51,677] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:37:51,678] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:37:52,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run21
[2019-04-01 17:37:52,996] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run21
[2019-04-01 17:37:53,289] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run21
[2019-04-01 17:38:24,039] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.5758196], dtype=float32), -0.7827447]
[2019-04-01 17:38:24,039] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.899999999999999, 84.66666666666666, 0.0, 0.0, 26.0, 24.73808547439616, 7.381018517987101, 1.0, 1.0, 68.30279121624372]
[2019-04-01 17:38:24,039] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 17:38:24,040] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.6033123e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2950443e-30
 1.0000000e+00 0.0000000e+00], sampled 0.09716326652701468
[2019-04-01 17:38:59,767] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.5758196], dtype=float32), -0.7827447]
[2019-04-01 17:38:59,768] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-0.3, 63.0, 143.0, 182.5, 26.0, 25.46525353143292, 8.796443814062885, 1.0, 1.0, 23.24362780452562]
[2019-04-01 17:38:59,768] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 17:38:59,769] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.8850566e-34 0.0000000e+00 1.8481094e-36 1.0210669e-36 5.1707467e-22
 1.0000000e+00 1.5895620e-31], sampled 0.8564011648432331
[2019-04-01 17:39:01,264] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.5758196], dtype=float32), -0.7827447]
[2019-04-01 17:39:01,265] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.9254715935, 63.443208215, 120.2987113, 471.9503088, 26.0, 25.12662776487547, 8.218517935430013, 0.0, 1.0, 29.088460145669]
[2019-04-01 17:39:01,265] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 17:39:01,266] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.5190279e-35 0.0000000e+00 7.5385882e-27 6.6312678e-35 6.2128271e-24
 9.9896538e-01 1.0346293e-03], sampled 0.894610454351844
[2019-04-01 17:39:04,904] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.5758196], dtype=float32), -0.7827447]
[2019-04-01 17:39:04,905] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.1, 33.5, 97.0, 616.0, 26.0, 25.09786942880134, 8.117912039353982, 0.0, 1.0, 22.43139858316109]
[2019-04-01 17:39:04,905] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 17:39:04,907] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 1.8856179e-35 0.0000000e+00 1.6483312e-30
 9.1567948e-08 9.9999988e-01], sampled 0.8208406959962633
[2019-04-01 17:39:08,584] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.5758196], dtype=float32), -0.7827447]
[2019-04-01 17:39:08,585] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [8.7, 53.33333333333333, 0.0, 0.0, 26.0, 25.44048484933826, 9.799330947719225, 0.0, 1.0, 45.55496974375624]
[2019-04-01 17:39:08,585] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 17:39:08,585] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.0048640e-37 0.0000000e+00 5.2993876e-29 0.0000000e+00 5.8639958e-33
 3.6936169e-07 9.9999964e-01], sampled 0.5245764909495176
[2019-04-01 17:39:20,576] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.5758196], dtype=float32), -0.7827447]
[2019-04-01 17:39:20,576] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [8.850000000000001, 34.5, 99.0, 684.0, 26.0, 26.56606106500144, 15.71276960424977, 1.0, 1.0, 2.979623478455629]
[2019-04-01 17:39:20,576] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 17:39:20,577] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.3039665e-32
 1.0000000e+00 0.0000000e+00], sampled 0.3141272787269176
[2019-04-01 17:39:31,137] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.5758196], dtype=float32), -0.7827447]
[2019-04-01 17:39:31,137] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.08978913433333324, 42.93262252333334, 89.63068141333333, 907.50838115, 26.0, 25.38477040063003, 10.36967728745327, 0.0, 1.0, 7.005854917017428]
[2019-04-01 17:39:31,137] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 17:39:31,138] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 4.1801043e-37 0.0000000e+00 3.7154506e-30
 3.2722379e-08 1.0000000e+00], sampled 0.8162787326146224
[2019-04-01 17:39:35,583] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 17:39:53,882] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 17:39:59,433] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 17:40:00,456] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2000000, evaluation results [2000000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 17:40:03,302] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.1720979e-36 0.0000000e+00 5.4089853e-21
 2.1231669e-11 1.0000000e+00], sum to 1.0000
[2019-04-01 17:40:03,313] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2774
[2019-04-01 17:40:03,324] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 43.33333333333333, 0.0, 0.0, 26.0, 25.51185883163429, 8.547192821880863, 0.0, 1.0, 28.21017943851479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3618600.0000, 
sim time next is 3619200.0000, 
raw observation next is [-1.333333333333333, 44.66666666666667, 0.0, 0.0, 26.0, 25.45675199536216, 8.377477934811296, 0.0, 1.0, 30.56894844924291], 
processed observation next is [0.0, 0.9130434782608695, 0.42566943674976926, 0.4466666666666667, 0.0, 0.0, 1.0, 0.9223931421945945, 0.08377477934811296, 0.0, 1.0, 0.2183496317803065], 
reward next is 0.7817, 
noisyNet noise sample is [array([0.30505913], dtype=float32), 0.92214125]. 
=============================================
[2019-04-01 17:40:14,175] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7240812e-25 0.0000000e+00 1.6178188e-23 2.2311387e-32 3.7499975e-21
 1.0000000e+00 2.6685407e-10], sum to 1.0000
[2019-04-01 17:40:14,176] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9122
[2019-04-01 17:40:14,188] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.83333333333334, 0.0, 0.0, 26.0, 25.73685045832053, 10.2503906546133, 0.0, 1.0, 24.01818670430395], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3881400.0000, 
sim time next is 3882000.0000, 
raw observation next is [-1.0, 56.66666666666667, 0.0, 0.0, 26.0, 25.64313138483227, 9.652696866627117, 0.0, 1.0, 27.03047906222901], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.5666666666666668, 0.0, 0.0, 1.0, 0.9490187692617528, 0.09652696866627117, 0.0, 1.0, 0.19307485044449293], 
reward next is 0.8069, 
noisyNet noise sample is [array([-0.36272871], dtype=float32), 0.82051915]. 
=============================================
[2019-04-01 17:40:14,205] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[64.85697]
 [65.29467]
 [65.81934]
 [66.19485]
 [66.44593]], R is [[64.7026825 ]
 [64.88409424]
 [65.06899261]
 [65.242836  ]
 [65.40494537]].
[2019-04-01 17:40:17,638] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0556291e-22 2.6763061e-27 4.0808963e-19 1.6958516e-26 8.3923076e-15
 1.0000000e+00 6.7986421e-27], sum to 1.0000
[2019-04-01 17:40:17,638] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2843
[2019-04-01 17:40:17,703] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.16666666666667, 64.0, 30.99999999999999, 148.0, 26.0, 25.22378207210775, 7.852035738353071, 1.0, 1.0, 61.4563852630906], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4002600.0000, 
sim time next is 4003200.0000, 
raw observation next is [-13.0, 63.0, 46.5, 222.0, 26.0, 25.48669262202952, 8.093595380980561, 1.0, 1.0, 49.6999732631093], 
processed observation next is [1.0, 0.34782608695652173, 0.10249307479224376, 0.63, 0.155, 0.24530386740331492, 1.0, 0.9266703745756456, 0.08093595380980562, 1.0, 1.0, 0.3549998090222093], 
reward next is 0.6450, 
noisyNet noise sample is [array([-0.79040986], dtype=float32), -1.5785806]. 
=============================================
[2019-04-01 17:40:24,989] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.7117876e-31 0.0000000e+00 8.4242348e-27 0.0000000e+00 6.9230849e-35
 1.0000000e+00 5.7332204e-14], sum to 1.0000
[2019-04-01 17:40:24,989] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4100
[2019-04-01 17:40:25,013] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 34.66666666666667, 0.0, 0.0, 26.0, 24.81183937329068, 6.325095083471138, 0.0, 1.0, 39.74268572556182], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4079400.0000, 
sim time next is 4080000.0000, 
raw observation next is [-4.0, 35.33333333333334, 0.0, 0.0, 26.0, 24.85587545976287, 6.228819803581466, 0.0, 1.0, 39.70016598801985], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.35333333333333344, 0.0, 0.0, 1.0, 0.8365536371089816, 0.06228819803581466, 0.0, 1.0, 0.2835726142001418], 
reward next is 0.7164, 
noisyNet noise sample is [array([-0.35069153], dtype=float32), -1.1089152]. 
=============================================
[2019-04-01 17:40:25,029] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[68.158   ]
 [68.179726]
 [68.15834 ]
 [68.17549 ]
 [68.201546]], R is [[68.19226837]
 [68.22647095]
 [68.26007843]
 [68.29277039]
 [68.32489014]].
[2019-04-01 17:40:25,604] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3674061e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.0141643e-37
 1.0000000e+00 5.5286260e-35], sum to 1.0000
[2019-04-01 17:40:25,605] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8416
[2019-04-01 17:40:25,625] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 26.0, 0.0, 0.0, 26.0, 25.79129722084698, 11.58643026065976, 1.0, 1.0, 15.22017672184698], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4039200.0000, 
sim time next is 4039800.0000, 
raw observation next is [-3.166666666666667, 26.83333333333334, 0.0, 0.0, 26.0, 25.87134704365191, 12.01390518796718, 1.0, 1.0, 10.92159589611706], 
processed observation next is [1.0, 0.782608695652174, 0.3748845798707295, 0.26833333333333337, 0.0, 0.0, 1.0, 0.981621006235987, 0.1201390518796718, 1.0, 1.0, 0.078011399257979], 
reward next is 0.1164, 
noisyNet noise sample is [array([-0.64490527], dtype=float32), -1.2093263]. 
=============================================
[2019-04-01 17:40:30,081] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.1272230e-38
 2.9580533e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 17:40:30,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9996
[2019-04-01 17:40:30,106] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 39.0, 205.0, 475.6666666666667, 26.0, 25.14916412318497, 8.776150687286552, 0.0, 1.0, 8.002503533750417], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4197000.0000, 
sim time next is 4197600.0000, 
raw observation next is [2.0, 40.0, 200.5, 379.0, 26.0, 25.14074728331158, 8.7626480206918, 0.0, 1.0, 7.477335522059007], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.4, 0.6683333333333333, 0.41878453038674035, 1.0, 0.8772496119016543, 0.087626480206918, 0.0, 1.0, 0.05340953944327862], 
reward next is 0.9466, 
noisyNet noise sample is [array([0.1776578], dtype=float32), -1.5808736]. 
=============================================
[2019-04-01 17:40:36,534] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 2.583069e-38 0.000000e+00 0.000000e+00
 1.000000e+00 3.196388e-38], sum to 1.0000
[2019-04-01 17:40:36,534] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3487
[2019-04-01 17:40:36,556] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.166666666666667, 54.16666666666666, 196.6666666666667, 446.3333333333334, 26.0, 25.47251409339547, 8.452125214077746, 0.0, 1.0, 16.93055426222555], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4270200.0000, 
sim time next is 4270800.0000, 
raw observation next is [4.333333333333334, 54.33333333333334, 200.3333333333333, 525.1666666666667, 26.0, 25.41188265179783, 8.449612221786131, 0.0, 1.0, 15.59454043039447], 
processed observation next is [0.0, 0.43478260869565216, 0.58264081255771, 0.5433333333333334, 0.6677777777777776, 0.5802946593001842, 1.0, 0.9159832359711183, 0.0844961222178613, 0.0, 1.0, 0.11138957450281765], 
reward next is 0.8886, 
noisyNet noise sample is [array([1.6736226], dtype=float32), -1.3577079]. 
=============================================
[2019-04-01 17:40:38,236] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7416095e-31 0.0000000e+00 1.3362067e-28 9.9184432e-38 3.3139106e-29
 4.1193032e-01 5.8806974e-01], sum to 1.0000
[2019-04-01 17:40:38,241] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3661
[2019-04-01 17:40:38,260] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.4, 70.33333333333333, 0.0, 0.0, 26.0, 25.49376231594118, 8.241300851189507, 0.0, 1.0, 36.24828509266602], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4340400.0000, 
sim time next is 4341000.0000, 
raw observation next is [3.35, 70.66666666666667, 0.0, 0.0, 26.0, 25.50342104424677, 8.369497948372926, 0.0, 1.0, 29.31084568142234], 
processed observation next is [1.0, 0.21739130434782608, 0.5554016620498616, 0.7066666666666667, 0.0, 0.0, 1.0, 0.9290601491781102, 0.08369497948372925, 0.0, 1.0, 0.209363183438731], 
reward next is 0.7906, 
noisyNet noise sample is [array([0.00483753], dtype=float32), -0.007901839]. 
=============================================
[2019-04-01 17:40:38,268] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[71.714096]
 [71.695465]
 [71.709785]
 [71.78465 ]
 [71.865875]], R is [[71.81041718]
 [71.83339691]
 [71.82418823]
 [71.81079102]
 [71.80249786]].
[2019-04-01 17:40:41,049] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1204361e-29 0.0000000e+00 1.8365411e-29 6.4524046e-29 9.7653493e-35
 1.0000000e+00 5.2541980e-37], sum to 1.0000
[2019-04-01 17:40:41,049] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8606
[2019-04-01 17:40:41,057] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 122.0, 0.0, 26.0, 26.59465700582043, 14.46069165472918, 1.0, 1.0, 9.78050690138875], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4449600.0000, 
sim time next is 4450200.0000, 
raw observation next is [0.8333333333333334, 87.0, 115.3333333333333, 0.0, 26.0, 26.59431864746696, 14.31256462465194, 1.0, 1.0, 9.960099561354614], 
processed observation next is [1.0, 0.5217391304347826, 0.4856879039704525, 0.87, 0.3844444444444443, 0.0, 1.0, 1.0849026639238513, 0.14312564624651938, 1.0, 1.0, 0.0711435682953901], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1064696], dtype=float32), 1.2563134]. 
=============================================
[2019-04-01 17:40:41,953] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.9935473e-25 0.0000000e+00 5.8592130e-33 6.9832665e-28 2.4090874e-18
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:40:41,954] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1112
[2019-04-01 17:40:41,971] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 83.83333333333334, 75.66666666666666, 0.0, 26.0, 25.96344539004459, 12.02691754853807, 1.0, 1.0, 14.27666787588245], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4461000.0000, 
sim time next is 4461600.0000, 
raw observation next is [0.0, 82.66666666666667, 73.33333333333334, 0.0, 26.0, 25.26733787945776, 10.56269375550918, 1.0, 1.0, 18.82569574702384], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.8266666666666667, 0.24444444444444446, 0.0, 1.0, 0.8953339827796799, 0.1056269375550918, 1.0, 1.0, 0.13446925533588458], 
reward next is 0.6405, 
noisyNet noise sample is [array([-1.6590123], dtype=float32), 0.75343835]. 
=============================================
[2019-04-01 17:40:43,105] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9996736e-18 3.6276825e-36 6.6136552e-24 3.7346817e-20 5.7985951e-12
 1.0000000e+00 1.4531310e-33], sum to 1.0000
[2019-04-01 17:40:43,106] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8779
[2019-04-01 17:40:43,137] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 86.16666666666667, 80.33333333333333, 0.0, 26.0, 26.43772998346855, 14.14877098540232, 1.0, 1.0, 14.50287293135664], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4459800.0000, 
sim time next is 4460400.0000, 
raw observation next is [0.0, 85.0, 78.0, 0.0, 26.0, 26.43074478250518, 12.09188077964196, 1.0, 1.0, 14.74722346082653], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.85, 0.26, 0.0, 1.0, 1.0615349689293114, 0.1209188077964196, 1.0, 1.0, 0.10533731043447522], 
reward next is 0.0579, 
noisyNet noise sample is [array([-0.6875595], dtype=float32), -0.048913423]. 
=============================================
[2019-04-01 17:40:44,366] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4030621e-29 0.0000000e+00 1.7657401e-36 1.7676673e-29 2.6228024e-18
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:40:44,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8511
[2019-04-01 17:40:44,411] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 89.66666666666667, 103.5, 0.9999999999999998, 26.0, 26.38029592679533, 14.55080732185314, 1.0, 1.0, 16.74602781851534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4458000.0000, 
sim time next is 4458600.0000, 
raw observation next is [0.0, 88.5, 85.0, 0.0, 26.0, 26.46854084870093, 14.64525303819007, 1.0, 1.0, 15.98652546053975], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.885, 0.2833333333333333, 0.0, 1.0, 1.0669344069572755, 0.1464525303819007, 1.0, 1.0, 0.11418946757528393], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.59985703], dtype=float32), -0.3876093]. 
=============================================
[2019-04-01 17:40:44,681] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5404558e-17 8.5058742e-33 7.4280437e-26 1.1357566e-25 1.4921927e-28
 1.0000000e+00 3.4843822e-23], sum to 1.0000
[2019-04-01 17:40:44,682] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7127
[2019-04-01 17:40:44,713] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.79598777596461, 11.96643375183603, 1.0, 1.0, 22.82531271570508], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4478400.0000, 
sim time next is 4479000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.70908660174288, 11.50313780311657, 0.0, 1.0, 19.53298878651866], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 1.0, 0.9584409431061255, 0.1150313780311657, 0.0, 1.0, 0.13952134847513328], 
reward next is 0.8605, 
noisyNet noise sample is [array([0.9852403], dtype=float32), 1.2998356]. 
=============================================
[2019-04-01 17:40:44,719] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[35.609383]
 [38.019447]
 [31.52724 ]
 [33.399155]
 [35.365253]], R is [[42.58709717]
 [42.21161652]
 [42.61677933]
 [42.19061279]
 [41.76870728]].
[2019-04-01 17:40:51,984] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1569041e-31 0.0000000e+00 3.9802608e-23 1.4100327e-37 0.0000000e+00
 1.0000000e+00 2.1933173e-16], sum to 1.0000
[2019-04-01 17:40:51,986] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3066
[2019-04-01 17:40:52,006] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.55952843236771, 10.16864465452284, 0.0, 1.0, 30.18459680551442], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4579200.0000, 
sim time next is 4579800.0000, 
raw observation next is [0.9, 61.33333333333334, 0.0, 0.0, 26.0, 25.56312356230266, 9.89526308953017, 0.0, 1.0, 26.75892311000342], 
processed observation next is [1.0, 0.0, 0.48753462603878117, 0.6133333333333334, 0.0, 0.0, 1.0, 0.9375890803289515, 0.0989526308953017, 0.0, 1.0, 0.191135165071453], 
reward next is 0.8089, 
noisyNet noise sample is [array([-0.19985695], dtype=float32), -0.7573949]. 
=============================================
[2019-04-01 17:40:53,272] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1029147e-27 2.7981063e-36 1.9102621e-21 5.2298699e-34 1.1359853e-34
 1.0000000e+00 2.2779166e-15], sum to 1.0000
[2019-04-01 17:40:53,273] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2810
[2019-04-01 17:40:53,304] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.58744844889052, 9.484243344965584, 0.0, 1.0, 30.28476455072965], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4687200.0000, 
sim time next is 4687800.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.60612670709867, 9.289151525321033, 0.0, 1.0, 26.51058712941756], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 1.0, 0.9437323867283816, 0.09289151525321034, 0.0, 1.0, 0.18936133663869684], 
reward next is 0.8106, 
noisyNet noise sample is [array([-1.1237959], dtype=float32), 1.0876064]. 
=============================================
[2019-04-01 17:40:55,180] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8720316e-14 1.9617489e-21 2.1471626e-10 3.0921963e-18 5.1921307e-15
 7.0560360e-01 2.9439640e-01], sum to 1.0000
[2019-04-01 17:40:55,183] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1648
[2019-04-01 17:40:55,220] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.1666666666666666, 93.33333333333334, 21.0, 0.0, 26.0, 25.6847231314499, 9.225388602932595, 1.0, 1.0, 21.87816056442749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4693800.0000, 
sim time next is 4694400.0000, 
raw observation next is [0.0, 92.0, 31.5, 0.0, 26.0, 25.62660859722641, 8.98553291654587, 1.0, 1.0, 20.63830042697105], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.105, 0.0, 1.0, 0.9466583710323445, 0.0898553291654587, 1.0, 1.0, 0.14741643162122178], 
reward next is 0.8526, 
noisyNet noise sample is [array([0.2615624], dtype=float32), 0.059361994]. 
=============================================
[2019-04-01 17:40:55,586] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7177851e-38 0.0000000e+00 1.7510370e-31 0.0000000e+00 0.0000000e+00
 1.3961893e-05 9.9998605e-01], sum to 1.0000
[2019-04-01 17:40:55,587] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5940
[2019-04-01 17:40:55,599] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 26.0, 25.46894640909827, 8.015349000977226, 0.0, 1.0, 30.77720519391299], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4914600.0000, 
sim time next is 4915200.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 26.0, 25.41330187403376, 7.928839728400878, 0.0, 1.0, 34.04223664257624], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 1.0, 0.9161859820048228, 0.07928839728400879, 0.0, 1.0, 0.24315883316125886], 
reward next is 0.7568, 
noisyNet noise sample is [array([1.6548648], dtype=float32), -0.12024601]. 
=============================================
[2019-04-01 17:40:57,245] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2579243e-32 0.0000000e+00 4.2950396e-33 7.0643648e-37 6.0161654e-20
 1.0000000e+00 1.4515045e-15], sum to 1.0000
[2019-04-01 17:40:57,255] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8102
[2019-04-01 17:40:57,270] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333333, 87.0, 0.0, 0.0, 26.0, 25.52305149250122, 8.872209611643505, 0.0, 1.0, 25.75304109501433], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4679400.0000, 
sim time next is 4680000.0000, 
raw observation next is [0.0, 92.0, 0.0, 0.0, 26.0, 25.40874158485362, 9.38201682524653, 0.0, 1.0, 39.52308668106325], 
processed observation next is [1.0, 0.17391304347826086, 0.46260387811634357, 0.92, 0.0, 0.0, 1.0, 0.9155345121219456, 0.09382016825246531, 0.0, 1.0, 0.2823077620075946], 
reward next is 0.7177, 
noisyNet noise sample is [array([-1.2802467], dtype=float32), -0.91107285]. 
=============================================
[2019-04-01 17:40:57,283] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[69.26979 ]
 [69.53025 ]
 [69.703766]
 [69.86459 ]
 [70.019615]], R is [[68.94550323]
 [69.07209778]
 [69.24248505]
 [69.40794373]
 [69.56480408]].
[2019-04-01 17:40:58,422] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9136418e-35 0.0000000e+00 2.4628126e-30 4.7397575e-38 4.1390163e-29
 9.8828727e-01 1.1712753e-02], sum to 1.0000
[2019-04-01 17:40:58,427] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0456
[2019-04-01 17:40:58,447] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.80248975006564, 7.308870792965766, 0.0, 1.0, 40.15860695182857], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4759200.0000, 
sim time next is 4759800.0000, 
raw observation next is [-4.333333333333334, 74.5, 0.0, 0.0, 26.0, 24.80194042618295, 7.252643348723119, 0.0, 1.0, 40.09816575285966], 
processed observation next is [0.0, 0.08695652173913043, 0.3425669436749769, 0.745, 0.0, 0.0, 1.0, 0.8288486323118498, 0.07252643348723119, 0.0, 1.0, 0.28641546966328324], 
reward next is 0.7136, 
noisyNet noise sample is [array([-0.276179], dtype=float32), -0.2858539]. 
=============================================
[2019-04-01 17:41:01,078] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6475016e-34 0.0000000e+00 2.1927205e-26 0.0000000e+00 2.1547797e-23
 1.1178727e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 17:41:01,080] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0963
[2019-04-01 17:41:01,139] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 66.0, 163.0, 268.3333333333333, 26.0, 25.79310657131656, 9.16372437852449, 0.0, 1.0, 38.13183617878468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4870200.0000, 
sim time next is 4870800.0000, 
raw observation next is [-3.0, 65.0, 174.0, 246.0, 26.0, 25.7908548202514, 9.051314252493556, 0.0, 1.0, 32.77840205696351], 
processed observation next is [0.0, 0.391304347826087, 0.3795013850415513, 0.65, 0.58, 0.2718232044198895, 1.0, 0.9701221171787715, 0.09051314252493556, 0.0, 1.0, 0.23413144326402505], 
reward next is 0.7659, 
noisyNet noise sample is [array([1.4634527], dtype=float32), 0.59766686]. 
=============================================
[2019-04-01 17:41:03,278] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:03,279] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:03,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run16
[2019-04-01 17:41:03,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:03,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:03,947] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run16
[2019-04-01 17:41:05,320] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:05,320] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:05,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run16
[2019-04-01 17:41:07,559] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.3302810e-38 0.0000000e+00 4.0064392e-33 0.0000000e+00 0.0000000e+00
 9.9999928e-01 7.7482110e-07], sum to 1.0000
[2019-04-01 17:41:07,559] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8297
[2019-04-01 17:41:07,577] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.333333333333333, 34.33333333333333, 0.0, 0.0, 26.0, 25.72098199313515, 12.56181944305867, 0.0, 1.0, 27.21197431118279], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5002800.0000, 
sim time next is 5003400.0000, 
raw observation next is [3.166666666666667, 35.66666666666667, 0.0, 0.0, 26.0, 25.79178824478761, 12.58166543264979, 0.0, 1.0, 21.96518836326031], 
processed observation next is [1.0, 0.9130434782608695, 0.5503231763619576, 0.3566666666666667, 0.0, 0.0, 1.0, 0.9702554635410873, 0.1258166543264979, 0.0, 1.0, 0.1568942025947165], 
reward next is 0.8431, 
noisyNet noise sample is [array([-0.6252671], dtype=float32), 0.5859151]. 
=============================================
[2019-04-01 17:41:07,843] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 3.4971935e-30 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.2260594e-20], sum to 1.0000
[2019-04-01 17:41:07,844] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3419
[2019-04-01 17:41:07,859] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666666, 47.66666666666666, 0.0, 0.0, 26.0, 25.39303006389988, 7.068805993618224, 0.0, 1.0, 30.74312919762835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4930800.0000, 
sim time next is 4931400.0000, 
raw observation next is [-0.8333333333333334, 48.83333333333334, 0.0, 0.0, 26.0, 25.31497451080612, 7.075644723069073, 0.0, 1.0, 37.04713831164518], 
processed observation next is [1.0, 0.043478260869565216, 0.43951985226223456, 0.48833333333333345, 0.0, 0.0, 1.0, 0.9021392158294458, 0.07075644723069073, 0.0, 1.0, 0.2646224165117513], 
reward next is 0.7354, 
noisyNet noise sample is [array([-0.47057772], dtype=float32), -0.7230773]. 
=============================================
[2019-04-01 17:41:08,326] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7182172e-36 0.0000000e+00 3.9636303e-35 0.0000000e+00 1.6237201e-27
 1.0000000e+00 2.1007129e-27], sum to 1.0000
[2019-04-01 17:41:08,333] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9311
[2019-04-01 17:41:08,356] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 46.66666666666667, 0.0, 0.0, 26.0, 25.19934575225271, 6.680962973316222, 0.0, 1.0, 38.03046269701157], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4942200.0000, 
sim time next is 4942800.0000, 
raw observation next is [-2.0, 46.0, 0.0, 0.0, 26.0, 25.20790991806253, 6.658943840865497, 0.0, 1.0, 37.98228441143418], 
processed observation next is [1.0, 0.21739130434782608, 0.40720221606648205, 0.46, 0.0, 0.0, 1.0, 0.8868442740089328, 0.06658943840865497, 0.0, 1.0, 0.2713020315102441], 
reward next is 0.7287, 
noisyNet noise sample is [array([-0.989689], dtype=float32), -1.3188534]. 
=============================================
[2019-04-01 17:41:10,084] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.9062481e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3297902e-33
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:41:10,086] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0432
[2019-04-01 17:41:10,103] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 24.0, 0.0, 0.0, 26.0, 26.29073926130991, 14.09683252519216, 1.0, 1.0, 6.21097074233681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4991400.0000, 
sim time next is 4992000.0000, 
raw observation next is [6.0, 23.66666666666666, 0.0, 0.0, 26.0, 25.28031752189467, 12.28878573508313, 1.0, 1.0, 4.363736785772038], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2366666666666666, 0.0, 0.0, 1.0, 0.8971882174135243, 0.1228878573508313, 1.0, 1.0, 0.03116954846980027], 
reward next is 0.0533, 
noisyNet noise sample is [array([-0.07298645], dtype=float32), 0.9382001]. 
=============================================
[2019-04-01 17:41:10,108] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[20.494106]
 [19.597347]
 [18.364088]
 [17.0947  ]
 [16.286633]], R is [[21.22616768]
 [21.01390648]
 [20.80376816]
 [20.59572983]
 [20.38977242]].
[2019-04-01 17:41:11,301] A3C_AGENT_WORKER-Thread-18 INFO:Local step 127500, global step 2036861: loss 1.2761
[2019-04-01 17:41:11,304] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 127500, global step 2036862: learning rate 0.0010
[2019-04-01 17:41:12,005] A3C_AGENT_WORKER-Thread-17 INFO:Local step 127500, global step 2037253: loss 1.2073
[2019-04-01 17:41:12,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 127500, global step 2037253: learning rate 0.0010
[2019-04-01 17:41:12,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:12,726] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:12,736] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run16
[2019-04-01 17:41:13,122] A3C_AGENT_WORKER-Thread-4 INFO:Local step 127500, global step 2037841: loss 1.3522
[2019-04-01 17:41:13,125] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 127500, global step 2037842: learning rate 0.0010
[2019-04-01 17:41:14,516] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:14,516] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:14,520] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run16
[2019-04-01 17:41:14,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:14,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:14,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run16
[2019-04-01 17:41:15,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:15,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:15,302] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run16
[2019-04-01 17:41:15,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:15,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:15,778] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run16
[2019-04-01 17:41:16,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:16,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:16,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run16
[2019-04-01 17:41:16,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:16,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:16,662] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run16
[2019-04-01 17:41:17,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:17,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:17,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run16
[2019-04-01 17:41:17,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:17,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:17,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run16
[2019-04-01 17:41:17,930] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:17,930] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:17,934] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run16
[2019-04-01 17:41:17,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:17,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:17,972] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run16
[2019-04-01 17:41:18,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:18,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:18,444] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run16
[2019-04-01 17:41:19,517] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:41:19,517] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:41:19,522] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run16
[2019-04-01 17:41:20,980] A3C_AGENT_WORKER-Thread-20 INFO:Local step 127500, global step 2039817: loss -1.6486
[2019-04-01 17:41:20,980] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 127500, global step 2039817: learning rate 0.0010
[2019-04-01 17:41:23,725] A3C_AGENT_WORKER-Thread-2 INFO:Local step 127500, global step 2040140: loss 0.8515
[2019-04-01 17:41:23,726] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 127500, global step 2040140: learning rate 0.0010
[2019-04-01 17:41:24,353] A3C_AGENT_WORKER-Thread-11 INFO:Local step 127500, global step 2040270: loss 0.5823
[2019-04-01 17:41:24,353] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 127500, global step 2040270: learning rate 0.0010
[2019-04-01 17:41:24,996] A3C_AGENT_WORKER-Thread-14 INFO:Local step 127500, global step 2040452: loss 0.6609
[2019-04-01 17:41:24,997] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 127500, global step 2040452: learning rate 0.0010
[2019-04-01 17:41:25,884] A3C_AGENT_WORKER-Thread-12 INFO:Local step 127500, global step 2040693: loss 0.5366
[2019-04-01 17:41:25,889] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 127500, global step 2040693: learning rate 0.0010
[2019-04-01 17:41:26,160] A3C_AGENT_WORKER-Thread-13 INFO:Local step 127500, global step 2040775: loss 0.4354
[2019-04-01 17:41:26,161] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 127500, global step 2040775: learning rate 0.0010
[2019-04-01 17:41:26,431] A3C_AGENT_WORKER-Thread-10 INFO:Local step 127500, global step 2040861: loss 0.6333
[2019-04-01 17:41:26,432] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 127500, global step 2040862: learning rate 0.0010
[2019-04-01 17:41:26,632] A3C_AGENT_WORKER-Thread-3 INFO:Local step 127500, global step 2040935: loss 0.6457
[2019-04-01 17:41:26,633] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 127500, global step 2040935: learning rate 0.0010
[2019-04-01 17:41:26,658] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4078361e-27 1.0564765e-30 1.7853606e-15 7.7287730e-27 2.2330216e-16
 4.6079349e-09 1.0000000e+00], sum to 1.0000
[2019-04-01 17:41:26,658] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2526
[2019-04-01 17:41:26,751] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 60.0, 0.0, 26.0, 23.85062645134961, 5.568415406695363, 0.0, 1.0, 55.44545174747143], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 37800.0000, 
sim time next is 38400.0000, 
raw observation next is [7.699999999999999, 93.0, 62.5, 0.0, 26.0, 23.9841518135754, 5.629582614547732, 0.0, 1.0, 55.24979657106073], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.20833333333333334, 0.0, 1.0, 0.7120216876536288, 0.05629582614547732, 0.0, 1.0, 0.3946414040790052], 
reward next is 0.6054, 
noisyNet noise sample is [array([-0.83916], dtype=float32), -1.6778979]. 
=============================================
[2019-04-01 17:41:27,662] A3C_AGENT_WORKER-Thread-9 INFO:Local step 127500, global step 2041214: loss -0.1472
[2019-04-01 17:41:27,665] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 127500, global step 2041214: learning rate 0.0010
[2019-04-01 17:41:27,857] A3C_AGENT_WORKER-Thread-15 INFO:Local step 127500, global step 2041270: loss 0.1784
[2019-04-01 17:41:27,858] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 127500, global step 2041270: learning rate 0.0010
[2019-04-01 17:41:27,904] A3C_AGENT_WORKER-Thread-19 INFO:Local step 127500, global step 2041281: loss 0.2558
[2019-04-01 17:41:27,905] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 127500, global step 2041281: learning rate 0.0010
[2019-04-01 17:41:28,682] A3C_AGENT_WORKER-Thread-16 INFO:Local step 127500, global step 2041507: loss -0.6506
[2019-04-01 17:41:28,683] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 127500, global step 2041507: learning rate 0.0010
[2019-04-01 17:41:29,412] A3C_AGENT_WORKER-Thread-5 INFO:Local step 127500, global step 2041723: loss -0.9785
[2019-04-01 17:41:29,412] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 127500, global step 2041723: learning rate 0.0010
[2019-04-01 17:41:32,364] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.92339749e-33 1.32434615e-36 2.58790898e-25 3.78646103e-31
 1.83330304e-26 1.00000000e+00 8.72807271e-10], sum to 1.0000
[2019-04-01 17:41:32,364] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8940
[2019-04-01 17:41:32,377] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 78.16666666666667, 0.0, 0.0, 26.0, 23.90560152709772, 5.341535842104285, 0.0, 1.0, 43.08990756538505], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 101400.0000, 
sim time next is 102000.0000, 
raw observation next is [-3.933333333333334, 77.33333333333334, 0.0, 0.0, 26.0, 23.95317023431367, 5.327033469447104, 0.0, 1.0, 43.11905639725546], 
processed observation next is [1.0, 0.17391304347826086, 0.3536472760849492, 0.7733333333333334, 0.0, 0.0, 1.0, 0.7075957477590958, 0.053270334694471046, 0.0, 1.0, 0.3079932599803962], 
reward next is 0.6920, 
noisyNet noise sample is [array([1.3120416], dtype=float32), -1.184646]. 
=============================================
[2019-04-01 17:41:32,395] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[56.369354]
 [56.454823]
 [56.51674 ]
 [56.550983]
 [56.59407 ]], R is [[56.42484665]
 [56.55281448]
 [56.67974472]
 [56.80585861]
 [56.93112946]].
[2019-04-01 17:41:35,305] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.2236705e-37
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:41:35,306] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7409
[2019-04-01 17:41:35,368] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.300000000000001, 67.66666666666667, 0.0, 0.0, 26.0, 26.48112002489416, 10.96610254446336, 1.0, 1.0, 36.97936760590312], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 148800.0000, 
sim time next is 149400.0000, 
raw observation next is [-7.3, 66.0, 0.0, 0.0, 26.0, 26.2768767660129, 10.19000530009662, 1.0, 1.0, 34.75337136296183], 
processed observation next is [1.0, 0.7391304347826086, 0.26038781163434904, 0.66, 0.0, 0.0, 1.0, 1.0395538237161286, 0.1019000530009662, 1.0, 1.0, 0.24823836687829878], 
reward next is 0.6758, 
noisyNet noise sample is [array([1.2053915], dtype=float32), -1.0018795]. 
=============================================
[2019-04-01 17:41:37,755] A3C_AGENT_WORKER-Thread-17 INFO:Local step 128000, global step 2044437: loss 10.0339
[2019-04-01 17:41:37,758] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 128000, global step 2044437: learning rate 0.0010
[2019-04-01 17:41:37,881] A3C_AGENT_WORKER-Thread-18 INFO:Local step 128000, global step 2044487: loss 9.6060
[2019-04-01 17:41:37,887] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 128000, global step 2044489: learning rate 0.0010
[2019-04-01 17:41:39,768] A3C_AGENT_WORKER-Thread-4 INFO:Local step 128000, global step 2045171: loss 24.6522
[2019-04-01 17:41:39,769] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 128000, global step 2045171: learning rate 0.0010
[2019-04-01 17:41:45,216] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2565521e-16 3.1718880e-24 2.0964893e-17 1.2548346e-18 1.5205845e-03
 9.9847943e-01 1.5095840e-13], sum to 1.0000
[2019-04-01 17:41:45,216] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7888
[2019-04-01 17:41:45,240] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.733333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 24.54629425228074, 6.319118672183552, 0.0, 1.0, 43.61551987891414], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 250800.0000, 
sim time next is 251400.0000, 
raw observation next is [-3.816666666666666, 73.33333333333333, 0.0, 0.0, 26.0, 24.51513789970531, 6.252006282555428, 0.0, 1.0, 43.62504997082115], 
processed observation next is [1.0, 0.9130434782608695, 0.3568790397045245, 0.7333333333333333, 0.0, 0.0, 1.0, 0.7878768428150441, 0.06252006282555428, 0.0, 1.0, 0.31160749979157965], 
reward next is 0.6884, 
noisyNet noise sample is [array([1.352169], dtype=float32), -0.99156]. 
=============================================
[2019-04-01 17:41:47,014] A3C_AGENT_WORKER-Thread-20 INFO:Local step 128000, global step 2047426: loss 17.6676
[2019-04-01 17:41:47,014] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 128000, global step 2047426: learning rate 0.0010
[2019-04-01 17:41:48,553] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3135046e-36 0.0000000e+00 0.0000000e+00 6.0529188e-38 1.1657033e-29
 1.0000000e+00 1.2757689e-38], sum to 1.0000
[2019-04-01 17:41:48,554] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9132
[2019-04-01 17:41:48,602] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 54.5, 106.0, 658.0, 26.0, 25.77512798278667, 8.426597055245288, 1.0, 1.0, 22.43765510606187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 300600.0000, 
sim time next is 301200.0000, 
raw observation next is [-10.6, 52.66666666666667, 102.1666666666667, 674.6666666666666, 26.0, 25.70982512249739, 8.24430361596633, 1.0, 1.0, 21.36865981274751], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.5266666666666667, 0.34055555555555567, 0.74548802946593, 1.0, 0.9585464460710555, 0.08244303615966331, 1.0, 1.0, 0.15263328437676793], 
reward next is 0.8474, 
noisyNet noise sample is [array([0.8314794], dtype=float32), -0.39335132]. 
=============================================
[2019-04-01 17:41:50,207] A3C_AGENT_WORKER-Thread-11 INFO:Local step 128000, global step 2048347: loss 13.2195
[2019-04-01 17:41:50,208] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 128000, global step 2048347: learning rate 0.0010
[2019-04-01 17:41:50,209] A3C_AGENT_WORKER-Thread-2 INFO:Local step 128000, global step 2048347: loss 14.2931
[2019-04-01 17:41:50,209] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 128000, global step 2048347: learning rate 0.0010
[2019-04-01 17:41:50,723] A3C_AGENT_WORKER-Thread-14 INFO:Local step 128000, global step 2048487: loss 17.6388
[2019-04-01 17:41:50,723] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 128000, global step 2048487: learning rate 0.0010
[2019-04-01 17:41:51,503] A3C_AGENT_WORKER-Thread-12 INFO:Local step 128000, global step 2048727: loss 23.4850
[2019-04-01 17:41:51,510] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 128000, global step 2048729: learning rate 0.0010
[2019-04-01 17:41:51,800] A3C_AGENT_WORKER-Thread-13 INFO:Local step 128000, global step 2048824: loss 21.8523
[2019-04-01 17:41:51,803] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 128000, global step 2048824: learning rate 0.0010
[2019-04-01 17:41:51,841] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.56312893e-34 0.00000000e+00 2.87772480e-21 2.53925237e-37
 1.04403696e-22 1.00000000e+00 1.36462790e-24], sum to 1.0000
[2019-04-01 17:41:51,841] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2148
[2019-04-01 17:41:51,862] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.0, 76.33333333333334, 0.0, 0.0, 26.0, 22.08007589327058, 8.941718767666883, 0.0, 1.0, 47.85651139322508], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 366000.0000, 
sim time next is 366600.0000, 
raw observation next is [-16.1, 77.16666666666666, 0.0, 0.0, 26.0, 21.98774895550582, 9.335434879200799, 0.0, 1.0, 47.95836198641961], 
processed observation next is [1.0, 0.21739130434782608, 0.01662049861495839, 0.7716666666666666, 0.0, 0.0, 1.0, 0.42682127935797404, 0.09335434879200799, 0.0, 1.0, 0.3425597284744258], 
reward next is 0.6574, 
noisyNet noise sample is [array([0.07962012], dtype=float32), -0.071551874]. 
=============================================
[2019-04-01 17:41:52,487] A3C_AGENT_WORKER-Thread-10 INFO:Local step 128000, global step 2049039: loss 36.9618
[2019-04-01 17:41:52,488] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 128000, global step 2049039: learning rate 0.0010
[2019-04-01 17:41:52,558] A3C_AGENT_WORKER-Thread-3 INFO:Local step 128000, global step 2049065: loss 34.2184
[2019-04-01 17:41:52,559] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 128000, global step 2049065: learning rate 0.0010
[2019-04-01 17:41:52,913] A3C_AGENT_WORKER-Thread-15 INFO:Local step 128000, global step 2049196: loss 23.2863
[2019-04-01 17:41:52,928] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 128000, global step 2049196: learning rate 0.0010
[2019-04-01 17:41:53,421] A3C_AGENT_WORKER-Thread-9 INFO:Local step 128000, global step 2049380: loss 36.5977
[2019-04-01 17:41:53,435] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 128000, global step 2049380: learning rate 0.0010
[2019-04-01 17:41:53,756] A3C_AGENT_WORKER-Thread-19 INFO:Local step 128000, global step 2049502: loss 36.9409
[2019-04-01 17:41:53,757] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 128000, global step 2049502: learning rate 0.0010
[2019-04-01 17:41:53,873] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:41:53,874] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3016
[2019-04-01 17:41:53,945] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.1, 61.0, 0.0, 0.0, 26.0, 25.53228099464746, 9.254129291764515, 1.0, 1.0, 54.75126948461988], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 326400.0000, 
sim time next is 327000.0000, 
raw observation next is [-12.2, 62.0, 0.0, 0.0, 26.0, 25.4504301488414, 9.109423778235763, 1.0, 1.0, 55.09983970655391], 
processed observation next is [1.0, 0.782608695652174, 0.12465373961218838, 0.62, 0.0, 0.0, 1.0, 0.9214900212630569, 0.09109423778235763, 1.0, 1.0, 0.3935702836182422], 
reward next is 0.6064, 
noisyNet noise sample is [array([0.89109474], dtype=float32), 0.25191963]. 
=============================================
[2019-04-01 17:41:53,954] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[66.956345]
 [66.61014 ]
 [66.03294 ]
 [65.55643 ]
 [65.100296]], R is [[67.14411926]
 [67.08159637]
 [67.01038361]
 [66.95235443]
 [66.89505005]].
[2019-04-01 17:41:54,810] A3C_AGENT_WORKER-Thread-16 INFO:Local step 128000, global step 2049873: loss 28.6750
[2019-04-01 17:41:54,811] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 128000, global step 2049873: learning rate 0.0010
[2019-04-01 17:41:55,965] A3C_AGENT_WORKER-Thread-5 INFO:Local step 128000, global step 2050216: loss 36.1148
[2019-04-01 17:41:55,965] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 128000, global step 2050216: learning rate 0.0010
[2019-04-01 17:41:58,258] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:41:58,260] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0172
[2019-04-01 17:41:58,321] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 36.33333333333333, 14.0, 274.3333333333334, 26.0, 25.94934270071529, 9.256259813531926, 1.0, 1.0, 19.23149579580592], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 406200.0000, 
sim time next is 406800.0000, 
raw observation next is [-8.9, 36.0, 10.5, 210.0, 26.0, 25.58578113185132, 8.864718659074276, 1.0, 1.0, 52.61440394712569], 
processed observation next is [1.0, 0.7391304347826086, 0.21606648199445982, 0.36, 0.035, 0.23204419889502761, 1.0, 0.94082587597876, 0.08864718659074276, 1.0, 1.0, 0.3758171710508978], 
reward next is 0.6242, 
noisyNet noise sample is [array([1.1812637], dtype=float32), 0.2687611]. 
=============================================
[2019-04-01 17:41:59,834] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.6673397e-27 1.2071352e-33 5.8233558e-27 6.9100283e-34 4.6538156e-26
 1.0000000e+00 1.3617395e-20], sum to 1.0000
[2019-04-01 17:41:59,835] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6472
[2019-04-01 17:41:59,859] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.23333333333333, 49.33333333333334, 0.0, 0.0, 26.0, 22.69146440041121, 7.029902487848521, 0.0, 1.0, 46.01688245256885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 451200.0000, 
sim time next is 451800.0000, 
raw observation next is [-10.05, 48.0, 0.0, 0.0, 26.0, 22.64858066203557, 7.164872710399773, 0.0, 1.0, 46.01889487774723], 
processed observation next is [1.0, 0.21739130434782608, 0.18421052631578946, 0.48, 0.0, 0.0, 1.0, 0.5212258088622244, 0.07164872710399772, 0.0, 1.0, 0.3287063919839088], 
reward next is 0.6713, 
noisyNet noise sample is [array([0.8203727], dtype=float32), -1.2156439]. 
=============================================
[2019-04-01 17:42:00,963] A3C_AGENT_WORKER-Thread-17 INFO:Local step 128500, global step 2051815: loss 0.2849
[2019-04-01 17:42:00,967] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 128500, global step 2051815: learning rate 0.0010
[2019-04-01 17:42:01,188] A3C_AGENT_WORKER-Thread-18 INFO:Local step 128500, global step 2051897: loss 0.2669
[2019-04-01 17:42:01,189] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 128500, global step 2051897: learning rate 0.0010
[2019-04-01 17:42:02,163] A3C_AGENT_WORKER-Thread-4 INFO:Local step 128500, global step 2052238: loss 0.2646
[2019-04-01 17:42:02,166] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 128500, global step 2052238: learning rate 0.0010
[2019-04-01 17:42:06,626] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 2.3682343e-37 0.0000000e+00 0.0000000e+00
 1.5851159e-33 1.0000000e+00], sum to 1.0000
[2019-04-01 17:42:06,628] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1864
[2019-04-01 17:42:06,643] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.89350721630473, 6.333828747455179, 0.0, 1.0, 41.64964839510083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 681600.0000, 
sim time next is 682200.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.85283805885156, 6.262767755514548, 0.0, 1.0, 41.59164463644647], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 1.0, 0.83611972269308, 0.06262767755514548, 0.0, 1.0, 0.29708317597461764], 
reward next is 0.7029, 
noisyNet noise sample is [array([-0.6576823], dtype=float32), 1.0511377]. 
=============================================
[2019-04-01 17:42:10,112] A3C_AGENT_WORKER-Thread-20 INFO:Local step 128500, global step 2055037: loss 0.3430
[2019-04-01 17:42:10,112] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 128500, global step 2055037: learning rate 0.0010
[2019-04-01 17:42:12,548] A3C_AGENT_WORKER-Thread-2 INFO:Local step 128500, global step 2055992: loss 0.4536
[2019-04-01 17:42:12,549] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 128500, global step 2055992: learning rate 0.0010
[2019-04-01 17:42:12,740] A3C_AGENT_WORKER-Thread-11 INFO:Local step 128500, global step 2056061: loss 0.4069
[2019-04-01 17:42:12,740] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 128500, global step 2056061: learning rate 0.0010
[2019-04-01 17:42:13,367] A3C_AGENT_WORKER-Thread-14 INFO:Local step 128500, global step 2056295: loss 0.3964
[2019-04-01 17:42:13,369] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 128500, global step 2056295: learning rate 0.0010
[2019-04-01 17:42:14,103] A3C_AGENT_WORKER-Thread-12 INFO:Local step 128500, global step 2056580: loss 0.4372
[2019-04-01 17:42:14,104] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 128500, global step 2056580: learning rate 0.0010
[2019-04-01 17:42:14,264] A3C_AGENT_WORKER-Thread-13 INFO:Local step 128500, global step 2056644: loss 0.4618
[2019-04-01 17:42:14,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 128500, global step 2056645: learning rate 0.0010
[2019-04-01 17:42:14,921] A3C_AGENT_WORKER-Thread-10 INFO:Local step 128500, global step 2056887: loss 0.4558
[2019-04-01 17:42:14,921] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 128500, global step 2056887: learning rate 0.0010
[2019-04-01 17:42:15,275] A3C_AGENT_WORKER-Thread-3 INFO:Local step 128500, global step 2057006: loss 0.7192
[2019-04-01 17:42:15,275] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 128500, global step 2057006: learning rate 0.0010
[2019-04-01 17:42:15,751] A3C_AGENT_WORKER-Thread-15 INFO:Local step 128500, global step 2057154: loss 0.4653
[2019-04-01 17:42:15,752] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 128500, global step 2057154: learning rate 0.0010
[2019-04-01 17:42:15,861] A3C_AGENT_WORKER-Thread-9 INFO:Local step 128500, global step 2057186: loss 0.4857
[2019-04-01 17:42:15,861] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 128500, global step 2057186: learning rate 0.0010
[2019-04-01 17:42:16,235] A3C_AGENT_WORKER-Thread-19 INFO:Local step 128500, global step 2057273: loss 0.5253
[2019-04-01 17:42:16,236] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 128500, global step 2057273: learning rate 0.0010
[2019-04-01 17:42:17,941] A3C_AGENT_WORKER-Thread-16 INFO:Local step 128500, global step 2057880: loss 0.5610
[2019-04-01 17:42:17,944] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 128500, global step 2057882: learning rate 0.0010
[2019-04-01 17:42:18,763] A3C_AGENT_WORKER-Thread-5 INFO:Local step 128500, global step 2058201: loss 0.4904
[2019-04-01 17:42:18,764] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 128500, global step 2058201: learning rate 0.0010
[2019-04-01 17:42:22,307] A3C_AGENT_WORKER-Thread-17 INFO:Local step 129000, global step 2059677: loss 8.2972
[2019-04-01 17:42:22,309] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 129000, global step 2059678: learning rate 0.0010
[2019-04-01 17:42:22,552] A3C_AGENT_WORKER-Thread-18 INFO:Local step 129000, global step 2059770: loss 5.4371
[2019-04-01 17:42:22,553] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 129000, global step 2059770: learning rate 0.0010
[2019-04-01 17:42:23,776] A3C_AGENT_WORKER-Thread-4 INFO:Local step 129000, global step 2060362: loss 7.6350
[2019-04-01 17:42:23,777] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 129000, global step 2060362: learning rate 0.0010
[2019-04-01 17:42:27,689] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 3.3773447e-24], sum to 1.0000
[2019-04-01 17:42:27,689] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8295
[2019-04-01 17:42:27,709] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.55, 79.0, 0.0, 0.0, 26.0, 25.65341353804819, 13.13747156720172, 0.0, 1.0, 30.40836790416785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1056600.0000, 
sim time next is 1057200.0000, 
raw observation next is [13.46666666666667, 79.33333333333333, 0.0, 0.0, 26.0, 25.78298644477719, 13.45180291810751, 0.0, 1.0, 22.74281004303936], 
processed observation next is [1.0, 0.21739130434782608, 0.8356417359187445, 0.7933333333333333, 0.0, 0.0, 1.0, 0.9689980635395986, 0.1345180291810751, 0.0, 1.0, 0.16244864316456686], 
reward next is 0.8376, 
noisyNet noise sample is [array([-1.0810273], dtype=float32), -1.3269798]. 
=============================================
[2019-04-01 17:42:29,156] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:42:29,157] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9166
[2019-04-01 17:42:29,195] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 83.33333333333334, 0.0, 0.0, 26.0, 25.4115568767942, 7.882364676487259, 1.0, 1.0, 25.83557279530625], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 841200.0000, 
sim time next is 841800.0000, 
raw observation next is [-3.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.19374787413183, 7.654435635961588, 1.0, 1.0, 27.66651821794536], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8266666666666667, 0.0, 0.0, 1.0, 0.8848211248759758, 0.07654435635961587, 1.0, 1.0, 0.19761798727103827], 
reward next is 0.8024, 
noisyNet noise sample is [array([-0.56761676], dtype=float32), -0.015621147]. 
=============================================
[2019-04-01 17:42:31,466] A3C_AGENT_WORKER-Thread-20 INFO:Local step 129000, global step 2063724: loss 2.4470
[2019-04-01 17:42:31,471] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 129000, global step 2063727: learning rate 0.0010
[2019-04-01 17:42:32,533] A3C_AGENT_WORKER-Thread-17 INFO:Local step 129500, global step 2064293: loss 0.2803
[2019-04-01 17:42:32,534] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 129500, global step 2064293: learning rate 0.0010
[2019-04-01 17:42:32,577] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:42:32,578] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6490
[2019-04-01 17:42:32,655] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 84.66666666666666, 24.16666666666667, 0.0, 26.0, 24.70874311166364, 7.761007052002131, 1.0, 1.0, 80.68532655752514], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 837600.0000, 
sim time next is 838200.0000, 
raw observation next is [-3.9, 85.33333333333334, 19.33333333333334, 0.0, 26.0, 25.26080397564569, 9.409897496901534, 1.0, 1.0, 44.35597325551915], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.8533333333333334, 0.06444444444444447, 0.0, 1.0, 0.8944005679493843, 0.09409897496901534, 1.0, 1.0, 0.31682838039656536], 
reward next is 0.6832, 
noisyNet noise sample is [array([0.63127387], dtype=float32), -0.60356385]. 
=============================================
[2019-04-01 17:42:33,036] A3C_AGENT_WORKER-Thread-18 INFO:Local step 129500, global step 2064613: loss 0.0623
[2019-04-01 17:42:33,040] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 129500, global step 2064613: learning rate 0.0010
[2019-04-01 17:42:33,050] A3C_AGENT_WORKER-Thread-11 INFO:Local step 129000, global step 2064619: loss 2.5811
[2019-04-01 17:42:33,055] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 129000, global step 2064621: learning rate 0.0010
[2019-04-01 17:42:33,642] A3C_AGENT_WORKER-Thread-2 INFO:Local step 129000, global step 2064887: loss 3.6689
[2019-04-01 17:42:33,643] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 129000, global step 2064887: learning rate 0.0010
[2019-04-01 17:42:34,102] A3C_AGENT_WORKER-Thread-4 INFO:Local step 129500, global step 2065107: loss 0.2090
[2019-04-01 17:42:34,103] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 129500, global step 2065107: learning rate 0.0010
[2019-04-01 17:42:34,725] A3C_AGENT_WORKER-Thread-13 INFO:Local step 129000, global step 2065419: loss 2.3976
[2019-04-01 17:42:34,725] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 129000, global step 2065419: learning rate 0.0010
[2019-04-01 17:42:34,881] A3C_AGENT_WORKER-Thread-14 INFO:Local step 129000, global step 2065493: loss 2.7579
[2019-04-01 17:42:34,881] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 129000, global step 2065494: learning rate 0.0010
[2019-04-01 17:42:34,886] A3C_AGENT_WORKER-Thread-12 INFO:Local step 129000, global step 2065498: loss 2.0042
[2019-04-01 17:42:34,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 129000, global step 2065500: learning rate 0.0010
[2019-04-01 17:42:34,930] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:42:34,931] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4534
[2019-04-01 17:42:34,940] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.800000000000001, 93.33333333333334, 0.0, 0.0, 26.0, 25.6740261451725, 9.049775741579936, 1.0, 1.0, 10.65194370413375], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 926400.0000, 
sim time next is 927000.0000, 
raw observation next is [4.7, 94.0, 0.0, 0.0, 26.0, 25.61850456246492, 8.877932995931104, 1.0, 1.0, 9.468805716421924], 
processed observation next is [1.0, 0.7391304347826086, 0.592797783933518, 0.94, 0.0, 0.0, 1.0, 0.945500651780703, 0.08877932995931104, 1.0, 1.0, 0.06763432654587088], 
reward next is 0.9324, 
noisyNet noise sample is [array([1.360149], dtype=float32), 0.5630975]. 
=============================================
[2019-04-01 17:42:34,946] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[72.73035 ]
 [72.67247 ]
 [72.679855]
 [72.63749 ]
 [72.35152 ]], R is [[72.94268036]
 [73.13716888]
 [73.32472229]
 [73.50546265]
 [73.67698669]].
[2019-04-01 17:42:35,147] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:42:35,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1520
[2019-04-01 17:42:35,258] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.8, 93.0, 96.0, 0.0, 26.0, 24.5966807340847, 6.748358500955942, 1.0, 1.0, 64.42181596274733], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 912600.0000, 
sim time next is 913200.0000, 
raw observation next is [3.8, 93.0, 95.0, 0.0, 26.0, 24.20811920085632, 8.559523928671103, 1.0, 1.0, 97.04689340624122], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.31666666666666665, 0.0, 1.0, 0.74401702869376, 0.08559523928671103, 1.0, 1.0, 0.6931920957588659], 
reward next is 0.3068, 
noisyNet noise sample is [array([2.208024], dtype=float32), -2.41337]. 
=============================================
[2019-04-01 17:42:35,566] A3C_AGENT_WORKER-Thread-10 INFO:Local step 129000, global step 2065885: loss 2.7033
[2019-04-01 17:42:35,567] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 129000, global step 2065886: learning rate 0.0010
[2019-04-01 17:42:36,024] A3C_AGENT_WORKER-Thread-3 INFO:Local step 129000, global step 2066146: loss 2.1695
[2019-04-01 17:42:36,027] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 129000, global step 2066146: learning rate 0.0010
[2019-04-01 17:42:36,483] A3C_AGENT_WORKER-Thread-9 INFO:Local step 129000, global step 2066413: loss 2.0189
[2019-04-01 17:42:36,485] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 129000, global step 2066413: learning rate 0.0010
[2019-04-01 17:42:36,840] A3C_AGENT_WORKER-Thread-15 INFO:Local step 129000, global step 2066578: loss 2.2816
[2019-04-01 17:42:36,842] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 129000, global step 2066578: learning rate 0.0010
[2019-04-01 17:42:36,972] A3C_AGENT_WORKER-Thread-19 INFO:Local step 129000, global step 2066649: loss 2.2694
[2019-04-01 17:42:36,975] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 129000, global step 2066649: learning rate 0.0010
[2019-04-01 17:42:38,037] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.1230968e-28], sum to 1.0000
[2019-04-01 17:42:38,040] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0474
[2019-04-01 17:42:38,053] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 82.5, 0.0, 0.0, 26.0, 25.5559825779531, 9.952139002318999, 0.0, 1.0, 26.97551544588774], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 964200.0000, 
sim time next is 964800.0000, 
raw observation next is [7.7, 83.0, 0.0, 0.0, 26.0, 25.62185171702175, 9.759264278709518, 0.0, 1.0, 26.26145377659721], 
processed observation next is [1.0, 0.17391304347826086, 0.6759002770083103, 0.83, 0.0, 0.0, 1.0, 0.9459788167173931, 0.09759264278709517, 0.0, 1.0, 0.1875818126899801], 
reward next is 0.8124, 
noisyNet noise sample is [array([-0.3238953], dtype=float32), -0.03685278]. 
=============================================
[2019-04-01 17:42:38,772] A3C_AGENT_WORKER-Thread-16 INFO:Local step 129000, global step 2067768: loss 10.8935
[2019-04-01 17:42:38,774] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 129000, global step 2067768: learning rate 0.0010
[2019-04-01 17:42:39,138] A3C_AGENT_WORKER-Thread-5 INFO:Local step 129000, global step 2068004: loss 13.3744
[2019-04-01 17:42:39,141] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 129000, global step 2068005: learning rate 0.0010
[2019-04-01 17:42:42,219] A3C_AGENT_WORKER-Thread-20 INFO:Local step 129500, global step 2070016: loss 0.1402
[2019-04-01 17:42:42,222] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 129500, global step 2070017: learning rate 0.0010
[2019-04-01 17:42:43,473] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 5.5407606e-22], sum to 1.0000
[2019-04-01 17:42:43,476] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0031
[2019-04-01 17:42:43,480] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.8, 88.0, 0.0, 0.0, 26.0, 23.99746875561257, 6.679288398377142, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1218600.0000, 
sim time next is 1219200.0000, 
raw observation next is [15.7, 89.66666666666667, 0.0, 0.0, 26.0, 23.9691600854516, 6.624505983573236, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.8975069252077563, 0.8966666666666667, 0.0, 0.0, 1.0, 0.7098800122073712, 0.06624505983573237, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1186712], dtype=float32), 0.101340055]. 
=============================================
[2019-04-01 17:42:43,998] A3C_AGENT_WORKER-Thread-11 INFO:Local step 129500, global step 2071192: loss 0.2661
[2019-04-01 17:42:44,002] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 129500, global step 2071194: learning rate 0.0010
[2019-04-01 17:42:44,596] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.         0.         0.         0.         0.         0.02111163
 0.97888833], sum to 1.0000
[2019-04-01 17:42:44,600] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6765
[2019-04-01 17:42:44,607] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.38333333333333, 64.66666666666667, 167.0, 0.0, 26.0, 25.06512531852323, 10.50474702331568, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1169400.0000, 
sim time next is 1170000.0000, 
raw observation next is [18.3, 65.0, 165.0, 0.0, 26.0, 25.05504674460581, 10.47137300924591, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.55, 0.0, 1.0, 0.8650066778008301, 0.1047137300924591, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2645608], dtype=float32), -0.29893455]. 
=============================================
[2019-04-01 17:42:44,619] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[103.3171  ]
 [103.308395]
 [103.3207  ]
 [102.90357 ]
 [102.717705]], R is [[103.41667175]
 [103.38250732]
 [103.34868622]
 [103.31520081]
 [103.28205109]].
[2019-04-01 17:42:44,769] A3C_AGENT_WORKER-Thread-2 INFO:Local step 129500, global step 2071703: loss 0.2645
[2019-04-01 17:42:44,771] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 129500, global step 2071704: learning rate 0.0010
[2019-04-01 17:42:45,699] A3C_AGENT_WORKER-Thread-13 INFO:Local step 129500, global step 2072326: loss 0.0876
[2019-04-01 17:42:45,701] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 129500, global step 2072326: learning rate 0.0010
[2019-04-01 17:42:45,892] A3C_AGENT_WORKER-Thread-12 INFO:Local step 129500, global step 2072458: loss 0.2044
[2019-04-01 17:42:45,894] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 129500, global step 2072458: learning rate 0.0010
[2019-04-01 17:42:46,085] A3C_AGENT_WORKER-Thread-14 INFO:Local step 129500, global step 2072590: loss 0.1215
[2019-04-01 17:42:46,086] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 129500, global step 2072590: learning rate 0.0010
[2019-04-01 17:42:46,770] A3C_AGENT_WORKER-Thread-10 INFO:Local step 129500, global step 2073035: loss 0.1321
[2019-04-01 17:42:46,771] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 129500, global step 2073035: learning rate 0.0010
[2019-04-01 17:42:47,175] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.9827326e-13], sum to 1.0000
[2019-04-01 17:42:47,180] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5157
[2019-04-01 17:42:47,187] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.32173965628719, 7.505415151466527, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1206000.0000, 
sim time next is 1206600.0000, 
raw observation next is [16.51666666666667, 75.5, 0.0, 0.0, 26.0, 24.29410819272093, 7.447556787876763, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 1.0, 0.9201292705447832, 0.755, 0.0, 0.0, 1.0, 0.7563011703887044, 0.07447556787876762, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1664455], dtype=float32), -0.35374448]. 
=============================================
[2019-04-01 17:42:47,439] A3C_AGENT_WORKER-Thread-3 INFO:Local step 129500, global step 2073482: loss 0.1722
[2019-04-01 17:42:47,440] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 129500, global step 2073482: learning rate 0.0010
[2019-04-01 17:42:47,657] A3C_AGENT_WORKER-Thread-17 INFO:Local step 130000, global step 2073623: loss 6.7235
[2019-04-01 17:42:47,658] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 130000, global step 2073624: learning rate 0.0010
[2019-04-01 17:42:48,024] A3C_AGENT_WORKER-Thread-9 INFO:Local step 129500, global step 2073851: loss 0.1128
[2019-04-01 17:42:48,030] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 129500, global step 2073851: learning rate 0.0010
[2019-04-01 17:42:48,251] A3C_AGENT_WORKER-Thread-15 INFO:Local step 129500, global step 2073999: loss 0.1471
[2019-04-01 17:42:48,254] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 129500, global step 2074000: learning rate 0.0010
[2019-04-01 17:42:48,398] A3C_AGENT_WORKER-Thread-19 INFO:Local step 129500, global step 2074087: loss 0.2438
[2019-04-01 17:42:48,399] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 129500, global step 2074088: learning rate 0.0010
[2019-04-01 17:42:48,424] A3C_AGENT_WORKER-Thread-18 INFO:Local step 130000, global step 2074104: loss 8.7407
[2019-04-01 17:42:48,433] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 130000, global step 2074112: learning rate 0.0010
[2019-04-01 17:42:49,191] A3C_AGENT_WORKER-Thread-4 INFO:Local step 130000, global step 2074569: loss 9.6386
[2019-04-01 17:42:49,193] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 130000, global step 2074570: learning rate 0.0010
[2019-04-01 17:42:50,271] A3C_AGENT_WORKER-Thread-16 INFO:Local step 129500, global step 2075240: loss 0.1328
[2019-04-01 17:42:50,275] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 129500, global step 2075241: learning rate 0.0010
[2019-04-01 17:42:50,758] A3C_AGENT_WORKER-Thread-5 INFO:Local step 129500, global step 2075515: loss 0.0775
[2019-04-01 17:42:50,759] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 129500, global step 2075515: learning rate 0.0010
[2019-04-01 17:42:51,346] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2252586e-37 0.0000000e+00 1.3674477e-38 1.7826038e-38 4.2130342e-22
 1.0000000e+00 4.9866482e-11], sum to 1.0000
[2019-04-01 17:42:51,347] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8168
[2019-04-01 17:42:51,375] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.266666666666667, 81.00000000000001, 25.0, 25.0, 26.0, 25.43466437301608, 9.704293061306638, 1.0, 1.0, 17.72047013314772], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1584600.0000, 
sim time next is 1585200.0000, 
raw observation next is [5.533333333333333, 80.0, 31.0, 30.0, 26.0, 25.4101519018867, 10.30527692032302, 1.0, 1.0, 18.01056856654312], 
processed observation next is [1.0, 0.34782608695652173, 0.6158818097876271, 0.8, 0.10333333333333333, 0.03314917127071823, 1.0, 0.9157359859838142, 0.1030527692032302, 1.0, 1.0, 0.12864691833245087], 
reward next is 0.7492, 
noisyNet noise sample is [array([-0.5402639], dtype=float32), -0.43870762]. 
=============================================
[2019-04-01 17:42:51,815] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7213982e-37
 1.0000000e+00 6.8643003e-38], sum to 1.0000
[2019-04-01 17:42:51,820] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8400
[2019-04-01 17:42:51,835] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.35, 92.0, 0.0, 0.0, 26.0, 25.2656345094037, 10.64725701053052, 0.0, 1.0, 42.03830627698676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1319400.0000, 
sim time next is 1320000.0000, 
raw observation next is [1.266666666666667, 92.0, 0.0, 0.0, 26.0, 25.31008795843212, 10.87636406011452, 0.0, 1.0, 41.22932275290091], 
processed observation next is [1.0, 0.2608695652173913, 0.4976915974145891, 0.92, 0.0, 0.0, 1.0, 0.9014411369188744, 0.1087636406011452, 0.0, 1.0, 0.29449516252072083], 
reward next is 0.7055, 
noisyNet noise sample is [array([0.14629029], dtype=float32), 0.25492713]. 
=============================================
[2019-04-01 17:42:51,848] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[66.697235]
 [66.76777 ]
 [66.851776]
 [66.85556 ]
 [66.79887 ]], R is [[66.66072083]
 [66.69384766]
 [66.73927307]
 [66.81313324]
 [66.89083862]].
[2019-04-01 17:42:58,230] A3C_AGENT_WORKER-Thread-20 INFO:Local step 130000, global step 2079219: loss 16.8319
[2019-04-01 17:42:58,231] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 130000, global step 2079219: learning rate 0.0010
[2019-04-01 17:42:59,469] A3C_AGENT_WORKER-Thread-11 INFO:Local step 130000, global step 2079842: loss 11.6460
[2019-04-01 17:42:59,470] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 130000, global step 2079842: learning rate 0.0010
[2019-04-01 17:43:00,539] A3C_AGENT_WORKER-Thread-2 INFO:Local step 130000, global step 2080358: loss 15.5306
[2019-04-01 17:43:00,540] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 130000, global step 2080358: learning rate 0.0010
[2019-04-01 17:43:01,526] A3C_AGENT_WORKER-Thread-13 INFO:Local step 130000, global step 2080872: loss 4.5475
[2019-04-01 17:43:01,528] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 130000, global step 2080874: learning rate 0.0010
[2019-04-01 17:43:01,662] A3C_AGENT_WORKER-Thread-14 INFO:Local step 130000, global step 2080945: loss 3.6595
[2019-04-01 17:43:01,664] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 130000, global step 2080945: learning rate 0.0010
[2019-04-01 17:43:01,682] A3C_AGENT_WORKER-Thread-12 INFO:Local step 130000, global step 2080957: loss 2.8660
[2019-04-01 17:43:01,683] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 130000, global step 2080957: learning rate 0.0010
[2019-04-01 17:43:02,363] A3C_AGENT_WORKER-Thread-10 INFO:Local step 130000, global step 2081329: loss 2.8928
[2019-04-01 17:43:02,365] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 130000, global step 2081330: learning rate 0.0010
[2019-04-01 17:43:02,737] A3C_AGENT_WORKER-Thread-3 INFO:Local step 130000, global step 2081558: loss 1.9637
[2019-04-01 17:43:02,739] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 130000, global step 2081560: learning rate 0.0010
[2019-04-01 17:43:03,397] A3C_AGENT_WORKER-Thread-9 INFO:Local step 130000, global step 2081958: loss 2.7047
[2019-04-01 17:43:03,400] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 130000, global step 2081958: learning rate 0.0010
[2019-04-01 17:43:03,696] A3C_AGENT_WORKER-Thread-17 INFO:Local step 130500, global step 2082124: loss 0.0008
[2019-04-01 17:43:03,696] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 130500, global step 2082124: learning rate 0.0010
[2019-04-01 17:43:03,727] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5090096e-37
 1.0000000e+00 8.6791491e-29], sum to 1.0000
[2019-04-01 17:43:03,728] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1385
[2019-04-01 17:43:03,737] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.0, 50.66666666666666, 78.66666666666667, 403.0000000000001, 26.0, 26.77588923173579, 17.25023622413143, 1.0, 1.0, 1.233159449282741], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1525200.0000, 
sim time next is 1525800.0000, 
raw observation next is [12.1, 50.33333333333334, 80.33333333333333, 328.0, 26.0, 26.85250046596029, 17.60364835501982, 1.0, 1.0, 1.195326539961087], 
processed observation next is [1.0, 0.6521739130434783, 0.7977839335180056, 0.5033333333333334, 0.2677777777777778, 0.3624309392265193, 1.0, 1.12178578085147, 0.17603648355019821, 1.0, 1.0, 0.008538046714007765], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1849582], dtype=float32), 0.9491545]. 
=============================================
[2019-04-01 17:43:03,933] A3C_AGENT_WORKER-Thread-15 INFO:Local step 130000, global step 2082264: loss 1.1503
[2019-04-01 17:43:03,935] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 130000, global step 2082264: learning rate 0.0010
[2019-04-01 17:43:04,144] A3C_AGENT_WORKER-Thread-19 INFO:Local step 130000, global step 2082388: loss 3.0467
[2019-04-01 17:43:04,149] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 130000, global step 2082388: learning rate 0.0010
[2019-04-01 17:43:04,315] A3C_AGENT_WORKER-Thread-18 INFO:Local step 130500, global step 2082483: loss 0.0203
[2019-04-01 17:43:04,317] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 130500, global step 2082484: learning rate 0.0010
[2019-04-01 17:43:04,887] A3C_AGENT_WORKER-Thread-4 INFO:Local step 130500, global step 2082788: loss 0.0358
[2019-04-01 17:43:04,889] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 130500, global step 2082788: learning rate 0.0010
[2019-04-01 17:43:05,797] A3C_AGENT_WORKER-Thread-16 INFO:Local step 130000, global step 2083280: loss 0.8188
[2019-04-01 17:43:05,798] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 130000, global step 2083280: learning rate 0.0010
[2019-04-01 17:43:06,357] A3C_AGENT_WORKER-Thread-5 INFO:Local step 130000, global step 2083574: loss 1.6310
[2019-04-01 17:43:06,368] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 130000, global step 2083574: learning rate 0.0010
[2019-04-01 17:43:13,769] A3C_AGENT_WORKER-Thread-20 INFO:Local step 130500, global step 2086998: loss 0.0316
[2019-04-01 17:43:13,774] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 130500, global step 2086999: learning rate 0.0010
[2019-04-01 17:43:15,519] A3C_AGENT_WORKER-Thread-11 INFO:Local step 130500, global step 2087592: loss 0.0429
[2019-04-01 17:43:15,522] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 130500, global step 2087592: learning rate 0.0010
[2019-04-01 17:43:16,663] A3C_AGENT_WORKER-Thread-2 INFO:Local step 130500, global step 2088012: loss 0.0143
[2019-04-01 17:43:16,664] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 130500, global step 2088012: learning rate 0.0010
[2019-04-01 17:43:17,683] A3C_AGENT_WORKER-Thread-13 INFO:Local step 130500, global step 2088419: loss 0.0329
[2019-04-01 17:43:17,685] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 130500, global step 2088421: learning rate 0.0010
[2019-04-01 17:43:17,827] A3C_AGENT_WORKER-Thread-14 INFO:Local step 130500, global step 2088474: loss 0.0066
[2019-04-01 17:43:17,828] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 130500, global step 2088474: learning rate 0.0010
[2019-04-01 17:43:18,143] A3C_AGENT_WORKER-Thread-12 INFO:Local step 130500, global step 2088598: loss 0.0137
[2019-04-01 17:43:18,144] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 130500, global step 2088598: learning rate 0.0010
[2019-04-01 17:43:18,598] A3C_AGENT_WORKER-Thread-10 INFO:Local step 130500, global step 2088742: loss 0.0010
[2019-04-01 17:43:18,604] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 130500, global step 2088742: learning rate 0.0010
[2019-04-01 17:43:19,292] A3C_AGENT_WORKER-Thread-3 INFO:Local step 130500, global step 2088974: loss 0.0007
[2019-04-01 17:43:19,292] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 130500, global step 2088974: learning rate 0.0010
[2019-04-01 17:43:19,927] A3C_AGENT_WORKER-Thread-9 INFO:Local step 130500, global step 2089194: loss 0.0127
[2019-04-01 17:43:19,930] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 130500, global step 2089195: learning rate 0.0010
[2019-04-01 17:43:20,687] A3C_AGENT_WORKER-Thread-15 INFO:Local step 130500, global step 2089432: loss 0.0024
[2019-04-01 17:43:20,691] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 130500, global step 2089433: learning rate 0.0010
[2019-04-01 17:43:20,783] A3C_AGENT_WORKER-Thread-19 INFO:Local step 130500, global step 2089458: loss 0.0049
[2019-04-01 17:43:20,783] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 130500, global step 2089458: learning rate 0.0010
[2019-04-01 17:43:22,281] A3C_AGENT_WORKER-Thread-16 INFO:Local step 130500, global step 2089950: loss 0.0065
[2019-04-01 17:43:22,282] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 130500, global step 2089950: learning rate 0.0010
[2019-04-01 17:43:22,875] A3C_AGENT_WORKER-Thread-5 INFO:Local step 130500, global step 2090161: loss 0.0060
[2019-04-01 17:43:22,876] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 130500, global step 2090161: learning rate 0.0010
[2019-04-01 17:43:27,161] A3C_AGENT_WORKER-Thread-17 INFO:Local step 131000, global step 2091724: loss 0.0126
[2019-04-01 17:43:27,171] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 131000, global step 2091724: learning rate 0.0010
[2019-04-01 17:43:27,929] A3C_AGENT_WORKER-Thread-18 INFO:Local step 131000, global step 2091987: loss 0.0297
[2019-04-01 17:43:27,930] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 131000, global step 2091988: learning rate 0.0010
[2019-04-01 17:43:28,579] A3C_AGENT_WORKER-Thread-4 INFO:Local step 131000, global step 2092222: loss 0.0630
[2019-04-01 17:43:28,581] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 131000, global step 2092222: learning rate 0.0010
[2019-04-01 17:43:37,442] A3C_AGENT_WORKER-Thread-20 INFO:Local step 131000, global step 2095306: loss 1.3981
[2019-04-01 17:43:37,447] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 131000, global step 2095307: learning rate 0.0010
[2019-04-01 17:43:38,106] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:43:38,108] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3734
[2019-04-01 17:43:38,159] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.466666666666667, 77.33333333333334, 222.1666666666667, 66.83333333333333, 26.0, 25.91384540762128, 9.053082287117457, 1.0, 1.0, 18.97785020380845], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2112000.0000, 
sim time next is 2112600.0000, 
raw observation next is [-7.383333333333333, 76.16666666666666, 236.3333333333333, 73.66666666666666, 26.0, 25.89166268355306, 9.029416940220477, 1.0, 1.0, 18.10861263946431], 
processed observation next is [1.0, 0.43478260869565216, 0.25807940904893817, 0.7616666666666666, 0.7877777777777776, 0.08139963167587476, 1.0, 0.9845232405075802, 0.09029416940220476, 1.0, 1.0, 0.1293472331390308], 
reward next is 0.8707, 
noisyNet noise sample is [array([0.9472091], dtype=float32), -0.005694912]. 
=============================================
[2019-04-01 17:43:39,309] A3C_AGENT_WORKER-Thread-11 INFO:Local step 131000, global step 2095908: loss 0.5207
[2019-04-01 17:43:39,310] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 131000, global step 2095908: learning rate 0.0010
[2019-04-01 17:43:40,172] A3C_AGENT_WORKER-Thread-2 INFO:Local step 131000, global step 2096240: loss 1.0839
[2019-04-01 17:43:40,172] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 131000, global step 2096240: learning rate 0.0010
[2019-04-01 17:43:41,123] A3C_AGENT_WORKER-Thread-13 INFO:Local step 131000, global step 2096541: loss 0.6344
[2019-04-01 17:43:41,145] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 131000, global step 2096541: learning rate 0.0010
[2019-04-01 17:43:41,580] A3C_AGENT_WORKER-Thread-12 INFO:Local step 131000, global step 2096704: loss 0.9166
[2019-04-01 17:43:41,580] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 131000, global step 2096704: learning rate 0.0010
[2019-04-01 17:43:41,761] A3C_AGENT_WORKER-Thread-14 INFO:Local step 131000, global step 2096779: loss 0.5835
[2019-04-01 17:43:41,763] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 131000, global step 2096779: learning rate 0.0010
[2019-04-01 17:43:41,928] A3C_AGENT_WORKER-Thread-10 INFO:Local step 131000, global step 2096854: loss 0.6162
[2019-04-01 17:43:41,928] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 131000, global step 2096854: learning rate 0.0010
[2019-04-01 17:43:43,224] A3C_AGENT_WORKER-Thread-3 INFO:Local step 131000, global step 2097361: loss 0.5684
[2019-04-01 17:43:43,226] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 131000, global step 2097362: learning rate 0.0010
[2019-04-01 17:43:43,610] A3C_AGENT_WORKER-Thread-9 INFO:Local step 131000, global step 2097518: loss 0.5053
[2019-04-01 17:43:43,612] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 131000, global step 2097518: learning rate 0.0010
[2019-04-01 17:43:44,044] A3C_AGENT_WORKER-Thread-15 INFO:Local step 131000, global step 2097692: loss 0.5032
[2019-04-01 17:43:44,044] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 131000, global step 2097692: learning rate 0.0010
[2019-04-01 17:43:44,671] A3C_AGENT_WORKER-Thread-19 INFO:Local step 131000, global step 2097885: loss 0.3360
[2019-04-01 17:43:44,672] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 131000, global step 2097885: learning rate 0.0010
[2019-04-01 17:43:45,456] A3C_AGENT_WORKER-Thread-16 INFO:Local step 131000, global step 2098115: loss 0.2775
[2019-04-01 17:43:45,458] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 131000, global step 2098115: learning rate 0.0010
[2019-04-01 17:43:45,835] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:43:45,835] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3935
[2019-04-01 17:43:45,916] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 68.0, 144.6666666666667, 190.0, 26.0, 25.3472329328687, 9.762304431223567, 1.0, 1.0, 40.89441391365693], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2214600.0000, 
sim time next is 2215200.0000, 
raw observation next is [-3.9, 68.0, 150.8333333333333, 237.5, 26.0, 25.91679393882428, 10.77452818716831, 1.0, 1.0, 29.28486930983567], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.68, 0.5027777777777777, 0.26243093922651933, 1.0, 0.9881134198320402, 0.1077452818716831, 1.0, 1.0, 0.20917763792739766], 
reward next is 0.4810, 
noisyNet noise sample is [array([-1.0539953], dtype=float32), 0.7622651]. 
=============================================
[2019-04-01 17:43:46,537] A3C_AGENT_WORKER-Thread-5 INFO:Local step 131000, global step 2098515: loss 0.3480
[2019-04-01 17:43:46,538] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 131000, global step 2098515: learning rate 0.0010
[2019-04-01 17:43:47,129] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 4.109131e-23], sum to 1.0000
[2019-04-01 17:43:47,129] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1325
[2019-04-01 17:43:47,152] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.3, 91.0, 0.0, 0.0, 26.0, 23.51439930180865, 5.362645519103204, 0.0, 1.0, 42.51559021353367], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2270400.0000, 
sim time next is 2271000.0000, 
raw observation next is [-9.4, 91.0, 0.0, 0.0, 26.0, 23.45593165583421, 5.402761745580563, 0.0, 1.0, 42.49580747176682], 
processed observation next is [1.0, 0.2608695652173913, 0.20221606648199447, 0.91, 0.0, 0.0, 1.0, 0.6365616651191728, 0.054027617455805636, 0.0, 1.0, 0.30354148194119157], 
reward next is 0.6965, 
noisyNet noise sample is [array([-0.42836487], dtype=float32), 0.5913227]. 
=============================================
[2019-04-01 17:43:47,189] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[68.109886]
 [68.14802 ]
 [68.19796 ]
 [68.21674 ]
 [68.247375]], R is [[68.06269836]
 [68.07839203]
 [68.09346771]
 [68.10842133]
 [68.12297821]].
[2019-04-01 17:43:48,232] A3C_AGENT_WORKER-Thread-17 INFO:Local step 131500, global step 2099112: loss 0.0302
[2019-04-01 17:43:48,232] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 131500, global step 2099112: learning rate 0.0010
[2019-04-01 17:43:49,294] A3C_AGENT_WORKER-Thread-18 INFO:Local step 131500, global step 2099519: loss 0.0045
[2019-04-01 17:43:49,294] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 131500, global step 2099519: learning rate 0.0010
[2019-04-01 17:43:49,793] A3C_AGENT_WORKER-Thread-4 INFO:Local step 131500, global step 2099688: loss 0.0006
[2019-04-01 17:43:49,795] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 131500, global step 2099688: learning rate 0.0010
[2019-04-01 17:43:50,659] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-01 17:43:50,660] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 17:43:50,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:43:50,662] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run22
[2019-04-01 17:43:50,694] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 17:43:50,695] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:43:50,695] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 17:43:50,697] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:43:50,700] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run22
[2019-04-01 17:43:50,719] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run22
[2019-04-01 17:44:52,927] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.45764798], dtype=float32), -0.5596967]
[2019-04-01 17:44:52,927] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-4.5, 64.0, 0.0, 0.0, 26.0, 24.60081869025811, 6.044665140200678, 0.0, 1.0, 40.46371392669224]
[2019-04-01 17:44:52,928] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 17:44:52,930] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 3.3986929e-33 0.0000000e+00 3.7146569e-30
 1.0000000e+00 6.1846257e-09], sampled 0.9535621944582
[2019-04-01 17:45:12,117] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.45764798], dtype=float32), -0.5596967]
[2019-04-01 17:45:12,117] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.0, 85.0, 0.0, 0.0, 26.0, 24.8641269849984, 7.555897452752748, 0.0, 1.0, 42.72448731059895]
[2019-04-01 17:45:12,117] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 17:45:12,118] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.7508187e-38 0.0000000e+00 9.9503352e-32 0.0000000e+00 1.2632463e-29
 9.9999988e-01 1.1937200e-07], sampled 0.7171468418364203
[2019-04-01 17:45:33,317] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 17:45:42,127] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.45764798], dtype=float32), -0.5596967]
[2019-04-01 17:45:42,127] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.6, 73.0, 0.0, 0.0, 26.0, 25.29245726518687, 8.986823544789196, 0.0, 1.0, 39.13912655617337]
[2019-04-01 17:45:42,127] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 17:45:42,127] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 2.9539172e-33 0.0000000e+00 2.9803053e-30
 1.0000000e+00 8.6296836e-09], sampled 0.4708146354610998
[2019-04-01 17:45:43,002] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.45764798], dtype=float32), -0.5596967]
[2019-04-01 17:45:43,002] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [14.10655212066667, 52.02603562500001, 0.0, 0.0, 26.0, 27.58152467072266, 23.92739393063543, 1.0, 1.0, 5.541099726878018]
[2019-04-01 17:45:43,002] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 17:45:43,003] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.6440973322548017
[2019-04-01 17:45:52,871] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 17:45:56,038] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 17:45:57,067] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 2100000, evaluation results [2100000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 17:46:03,739] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7487939e-36 0.0000000e+00 2.7198287e-29 0.0000000e+00 1.3887357e-19
 1.0000000e+00 5.7307903e-15], sum to 1.0000
[2019-04-01 17:46:03,742] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8181
[2019-04-01 17:46:03,768] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 69.0, 7.833333333333332, 0.0, 26.0, 24.00414245280129, 5.323790300218792, 0.0, 1.0, 41.19475981211605], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2360400.0000, 
sim time next is 2361000.0000, 
raw observation next is [-3.4, 69.0, 13.66666666666666, 0.0, 26.0, 23.97212379143214, 5.311076964195967, 0.0, 1.0, 41.2829187998824], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.04555555555555554, 0.0, 1.0, 0.7103033987760199, 0.053110769641959675, 0.0, 1.0, 0.29487799142773147], 
reward next is 0.7051, 
noisyNet noise sample is [array([-1.2264777], dtype=float32), 1.1904396]. 
=============================================
[2019-04-01 17:46:03,779] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[70.31422 ]
 [70.2865  ]
 [70.344345]
 [70.40354 ]
 [70.45577 ]], R is [[70.35050201]
 [70.35274506]
 [70.35559082]
 [70.35905457]
 [70.36306763]].
[2019-04-01 17:46:04,881] A3C_AGENT_WORKER-Thread-20 INFO:Local step 131500, global step 2103076: loss 0.0041
[2019-04-01 17:46:04,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 131500, global step 2103076: learning rate 0.0010
[2019-04-01 17:46:05,562] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.9648727e-28], sum to 1.0000
[2019-04-01 17:46:05,562] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8019
[2019-04-01 17:46:05,575] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.3, 61.0, 0.0, 0.0, 26.0, 24.87175530061706, 6.815683560145184, 0.0, 1.0, 41.3473004007576], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2590800.0000, 
sim time next is 2591400.0000, 
raw observation next is [-4.399999999999999, 61.5, 0.0, 0.0, 26.0, 24.8318293284, 6.70505757913953, 0.0, 1.0, 41.3604242932384], 
processed observation next is [1.0, 1.0, 0.3407202216066483, 0.615, 0.0, 0.0, 1.0, 0.8331184754857145, 0.0670505757913953, 0.0, 1.0, 0.29543160209456004], 
reward next is 0.7046, 
noisyNet noise sample is [array([0.1844161], dtype=float32), -0.84646773]. 
=============================================
[2019-04-01 17:46:06,919] A3C_AGENT_WORKER-Thread-11 INFO:Local step 131500, global step 2103923: loss 0.0004
[2019-04-01 17:46:06,920] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 131500, global step 2103923: learning rate 0.0010
[2019-04-01 17:46:07,503] A3C_AGENT_WORKER-Thread-2 INFO:Local step 131500, global step 2104128: loss 0.0028
[2019-04-01 17:46:07,506] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 131500, global step 2104129: learning rate 0.0010
[2019-04-01 17:46:08,810] A3C_AGENT_WORKER-Thread-13 INFO:Local step 131500, global step 2104636: loss 0.0034
[2019-04-01 17:46:08,812] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 131500, global step 2104636: learning rate 0.0010
[2019-04-01 17:46:09,063] A3C_AGENT_WORKER-Thread-14 INFO:Local step 131500, global step 2104737: loss 0.0040
[2019-04-01 17:46:09,063] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 131500, global step 2104737: learning rate 0.0010
[2019-04-01 17:46:09,204] A3C_AGENT_WORKER-Thread-10 INFO:Local step 131500, global step 2104798: loss 0.0042
[2019-04-01 17:46:09,206] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 131500, global step 2104800: learning rate 0.0010
[2019-04-01 17:46:09,384] A3C_AGENT_WORKER-Thread-12 INFO:Local step 131500, global step 2104876: loss 0.0006
[2019-04-01 17:46:09,385] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 131500, global step 2104876: learning rate 0.0010
[2019-04-01 17:46:10,780] A3C_AGENT_WORKER-Thread-3 INFO:Local step 131500, global step 2105433: loss 0.0079
[2019-04-01 17:46:10,782] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 131500, global step 2105434: learning rate 0.0010
[2019-04-01 17:46:11,213] A3C_AGENT_WORKER-Thread-9 INFO:Local step 131500, global step 2105617: loss 0.0025
[2019-04-01 17:46:11,214] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 131500, global step 2105617: learning rate 0.0010
[2019-04-01 17:46:12,010] A3C_AGENT_WORKER-Thread-15 INFO:Local step 131500, global step 2106014: loss 0.0001
[2019-04-01 17:46:12,013] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 131500, global step 2106014: learning rate 0.0010
[2019-04-01 17:46:12,039] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:46:12,040] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7211
[2019-04-01 17:46:12,081] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 43.33333333333333, 0.0, 0.0, 26.0, 24.97954717138894, 7.384118910358914, 0.0, 1.0, 38.6101641961703], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2400000.0000, 
sim time next is 2400600.0000, 
raw observation next is [-2.283333333333333, 43.16666666666667, 0.0, 0.0, 26.0, 25.00278207362512, 7.356034016015438, 0.0, 1.0, 36.78363997068116], 
processed observation next is [0.0, 0.782608695652174, 0.399353647276085, 0.4316666666666667, 0.0, 0.0, 1.0, 0.85754029623216, 0.07356034016015438, 0.0, 1.0, 0.2627402855048654], 
reward next is 0.7373, 
noisyNet noise sample is [array([-1.3232317], dtype=float32), -0.51590025]. 
=============================================
[2019-04-01 17:46:12,132] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 17:46:12,135] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7141
[2019-04-01 17:46:12,169] A3C_AGENT_WORKER-Thread-19 INFO:Local step 131500, global step 2106102: loss 0.0106
[2019-04-01 17:46:12,172] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 131500, global step 2106102: learning rate 0.0010
[2019-04-01 17:46:12,187] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.4, 43.0, 0.0, 0.0, 26.0, 25.00257403001857, 7.287733502300718, 0.0, 1.0, 35.51121464467595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2401200.0000, 
sim time next is 2401800.0000, 
raw observation next is [-2.566666666666666, 42.83333333333334, 0.0, 0.0, 26.0, 24.98499983077509, 7.251727706079961, 0.0, 1.0, 45.60892249143802], 
processed observation next is [0.0, 0.8260869565217391, 0.39150507848568794, 0.42833333333333345, 0.0, 0.0, 1.0, 0.8549999758250131, 0.07251727706079962, 0.0, 1.0, 0.32577801779598586], 
reward next is 0.6742, 
noisyNet noise sample is [array([-1.3232317], dtype=float32), -0.51590025]. 
=============================================
[2019-04-01 17:46:12,953] A3C_AGENT_WORKER-Thread-16 INFO:Local step 131500, global step 2106404: loss -0.1809
[2019-04-01 17:46:12,972] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 131500, global step 2106404: learning rate 0.0010
[2019-04-01 17:46:13,756] A3C_AGENT_WORKER-Thread-5 INFO:Local step 131500, global step 2106738: loss 0.0578
[2019-04-01 17:46:13,757] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 131500, global step 2106738: learning rate 0.0010
[2019-04-01 17:46:13,801] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132000, global step 2106759: loss 11.0446
[2019-04-01 17:46:13,801] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132000, global step 2106759: learning rate 0.0010
[2019-04-01 17:46:15,118] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132000, global step 2107349: loss 6.4566
[2019-04-01 17:46:15,119] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132000, global step 2107350: learning rate 0.0010
[2019-04-01 17:46:15,161] A3C_AGENT_WORKER-Thread-4 INFO:Local step 132000, global step 2107367: loss 3.9629
[2019-04-01 17:46:15,173] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 132000, global step 2107368: learning rate 0.0010
[2019-04-01 17:46:18,793] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:46:18,793] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9361
[2019-04-01 17:46:18,841] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 64.0, 115.3333333333333, 211.3333333333333, 26.0, 25.99296593173429, 9.075201329969302, 1.0, 1.0, 39.9837672123556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2796000.0000, 
sim time next is 2796600.0000, 
raw observation next is [-6.0, 64.0, 122.6666666666667, 215.6666666666667, 26.0, 26.05792526248221, 8.977743558223045, 1.0, 1.0, 36.82912489485383], 
processed observation next is [1.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.408888888888889, 0.23830570902394113, 1.0, 1.0082750374974587, 0.08977743558223045, 1.0, 1.0, 0.26306517782038447], 
reward next is 0.7369, 
noisyNet noise sample is [array([3.0584483], dtype=float32), 0.29766098]. 
=============================================
[2019-04-01 17:46:23,343] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132000, global step 2111108: loss 2.3985
[2019-04-01 17:46:23,344] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132000, global step 2111108: learning rate 0.0010
[2019-04-01 17:46:25,400] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132000, global step 2111919: loss 1.2562
[2019-04-01 17:46:25,401] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132000, global step 2111919: learning rate 0.0010
[2019-04-01 17:46:25,720] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132000, global step 2112053: loss 0.4731
[2019-04-01 17:46:25,726] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132000, global step 2112054: learning rate 0.0010
[2019-04-01 17:46:27,438] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132000, global step 2112759: loss 0.1106
[2019-04-01 17:46:27,440] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132000, global step 2112762: learning rate 0.0010
[2019-04-01 17:46:27,446] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132000, global step 2112765: loss 0.2424
[2019-04-01 17:46:27,448] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132000, global step 2112765: learning rate 0.0010
[2019-04-01 17:46:27,904] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132000, global step 2112977: loss 0.2130
[2019-04-01 17:46:27,905] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132000, global step 2112977: learning rate 0.0010
[2019-04-01 17:46:28,011] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9431028e-33 0.0000000e+00 1.9753250e-37 0.0000000e+00 1.2228070e-30
 1.0000000e+00 1.2611630e-27], sum to 1.0000
[2019-04-01 17:46:28,011] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8672
[2019-04-01 17:46:28,027] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.516666666666667, 65.66666666666667, 0.0, 0.0, 26.0, 25.44677010656066, 10.03364075909368, 0.0, 1.0, 44.20954314887338], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2668200.0000, 
sim time next is 2668800.0000, 
raw observation next is [-1.833333333333333, 66.33333333333334, 0.0, 0.0, 26.0, 25.4792002138682, 9.958218128818404, 0.0, 1.0, 38.39310725928236], 
processed observation next is [1.0, 0.9130434782608695, 0.41181902123730385, 0.6633333333333334, 0.0, 0.0, 1.0, 0.9256000305526, 0.09958218128818404, 0.0, 1.0, 0.27423648042344545], 
reward next is 0.7258, 
noisyNet noise sample is [array([0.49629265], dtype=float32), 0.44194502]. 
=============================================
[2019-04-01 17:46:28,161] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132000, global step 2113096: loss 0.0106
[2019-04-01 17:46:28,162] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132000, global step 2113096: learning rate 0.0010
[2019-04-01 17:46:29,550] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132000, global step 2113573: loss 0.0181
[2019-04-01 17:46:29,554] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132000, global step 2113575: learning rate 0.0010
[2019-04-01 17:46:29,853] A3C_AGENT_WORKER-Thread-9 INFO:Local step 132000, global step 2113693: loss 0.0070
[2019-04-01 17:46:29,855] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 132000, global step 2113695: learning rate 0.0010
[2019-04-01 17:46:30,616] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132000, global step 2114010: loss 0.2281
[2019-04-01 17:46:30,617] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132000, global step 2114010: learning rate 0.0010
[2019-04-01 17:46:31,231] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132000, global step 2114241: loss 0.2111
[2019-04-01 17:46:31,231] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132000, global step 2114241: learning rate 0.0010
[2019-04-01 17:46:31,491] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132000, global step 2114347: loss 0.2334
[2019-04-01 17:46:31,491] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132000, global step 2114347: learning rate 0.0010
[2019-04-01 17:46:32,249] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2114643: loss 0.0157
[2019-04-01 17:46:32,249] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2114643: learning rate 0.0010
[2019-04-01 17:46:32,754] A3C_AGENT_WORKER-Thread-5 INFO:Local step 132000, global step 2114866: loss 0.7320
[2019-04-01 17:46:32,754] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 132000, global step 2114866: learning rate 0.0010
[2019-04-01 17:46:33,389] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.9459265e-35
 2.1820906e-06 9.9999785e-01], sum to 1.0000
[2019-04-01 17:46:33,390] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2084
[2019-04-01 17:46:33,403] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 69.0, 0.0, 0.0, 26.0, 24.70935017669725, 6.258580671480433, 0.0, 1.0, 37.44804990309179], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3026400.0000, 
sim time next is 3027000.0000, 
raw observation next is [-4.833333333333334, 70.0, 0.0, 0.0, 26.0, 24.66917860726615, 6.176732073446362, 0.0, 1.0, 37.49168290787422], 
processed observation next is [0.0, 0.0, 0.32871652816251157, 0.7, 0.0, 0.0, 1.0, 0.8098826581808787, 0.06176732073446362, 0.0, 1.0, 0.26779773505624443], 
reward next is 0.7322, 
noisyNet noise sample is [array([0.37147158], dtype=float32), 0.884601]. 
=============================================
[2019-04-01 17:46:33,407] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[63.847366]
 [63.662647]
 [63.567486]
 [63.472115]
 [63.6091  ]], R is [[64.07540894]
 [64.16716766]
 [64.25818634]
 [64.34832001]
 [64.43741608]].
[2019-04-01 17:46:33,733] A3C_AGENT_WORKER-Thread-4 INFO:Local step 132500, global step 2115354: loss 0.0317
[2019-04-01 17:46:33,736] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2115356: loss 0.0483
[2019-04-01 17:46:33,736] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 132500, global step 2115354: learning rate 0.0010
[2019-04-01 17:46:33,737] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2115356: learning rate 0.0010
[2019-04-01 17:46:34,842] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.322077e-37
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 17:46:34,842] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1104
[2019-04-01 17:46:34,872] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.166666666666667, 93.0, 86.33333333333334, 104.0, 26.0, 25.40828847268252, 7.563901201672583, 1.0, 1.0, 28.70838602264336], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2883000.0000, 
sim time next is 2883600.0000, 
raw observation next is [1.0, 93.0, 77.0, 78.0, 26.0, 25.3718496975426, 7.607773990102518, 1.0, 1.0, 29.68293761843258], 
processed observation next is [1.0, 0.391304347826087, 0.4903047091412743, 0.93, 0.25666666666666665, 0.0861878453038674, 1.0, 0.9102642425060858, 0.07607773990102518, 1.0, 1.0, 0.21202098298880415], 
reward next is 0.7880, 
noisyNet noise sample is [array([0.71457475], dtype=float32), 0.50107104]. 
=============================================
[2019-04-01 17:46:36,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 4.0626921e-34 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.5545969e-21], sum to 1.0000
[2019-04-01 17:46:36,546] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3974
[2019-04-01 17:46:36,558] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 56.0, 0.0, 0.0, 26.0, 25.35616219130136, 8.565098500490913, 0.0, 1.0, 44.35456112984098], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2846400.0000, 
sim time next is 2847000.0000, 
raw observation next is [2.0, 59.0, 0.0, 0.0, 26.0, 25.35524250877757, 8.57414374393872, 0.0, 1.0, 43.90038687661403], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.59, 0.0, 0.0, 1.0, 0.9078917869682245, 0.0857414374393872, 0.0, 1.0, 0.3135741919758145], 
reward next is 0.6864, 
noisyNet noise sample is [array([-0.5105371], dtype=float32), 2.06706]. 
=============================================
[2019-04-01 17:46:36,560] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[59.33351 ]
 [59.33769 ]
 [59.245216]
 [59.32074 ]
 [59.49302 ]], R is [[59.44987106]
 [59.53855515]
 [59.6265564 ]
 [59.70749664]
 [59.77225113]].
[2019-04-01 17:46:38,671] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:46:38,672] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4612
[2019-04-01 17:46:38,712] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 25.82550183921055, 11.31046765660531, 1.0, 1.0, 53.16942026417218], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2833200.0000, 
sim time next is 2833800.0000, 
raw observation next is [2.833333333333333, 38.16666666666667, 0.0, 0.0, 26.0, 26.00735325899696, 11.60665756683311, 1.0, 1.0, 47.72963645176324], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.3816666666666667, 0.0, 0.0, 1.0, 1.0010504655709944, 0.1160665756683311, 1.0, 1.0, 0.3409259746554517], 
reward next is 0.0164, 
noisyNet noise sample is [array([0.6955928], dtype=float32), 1.6006082]. 
=============================================
[2019-04-01 17:46:41,745] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2119136: loss 0.0608
[2019-04-01 17:46:41,746] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2119137: learning rate 0.0010
[2019-04-01 17:46:43,647] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132500, global step 2119964: loss 0.0056
[2019-04-01 17:46:43,649] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132500, global step 2119966: learning rate 0.0010
[2019-04-01 17:46:44,088] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2120170: loss 0.0317
[2019-04-01 17:46:44,107] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2120176: learning rate 0.0010
[2019-04-01 17:46:45,504] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4054785e-30 5.7531920e-32 1.3911217e-19 1.7294096e-28 1.3213100e-26
 1.0000000e+00 3.3125375e-10], sum to 1.0000
[2019-04-01 17:46:45,504] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4696
[2019-04-01 17:46:45,553] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 67.0, 191.0, 67.33333333333331, 26.0, 25.05041819144813, 8.318721741272357, 0.0, 1.0, 49.98782383226249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2976000.0000, 
sim time next is 2976600.0000, 
raw observation next is [-3.166666666666667, 66.0, 204.0, 110.6666666666666, 26.0, 25.16746595091693, 8.557239123432625, 0.0, 1.0, 40.59918829610932], 
processed observation next is [0.0, 0.43478260869565216, 0.3748845798707295, 0.66, 0.68, 0.12228360957642719, 1.0, 0.8810665644167044, 0.08557239123432625, 0.0, 1.0, 0.28999420211506655], 
reward next is 0.7100, 
noisyNet noise sample is [array([0.6676234], dtype=float32), -2.1089818]. 
=============================================
[2019-04-01 17:46:45,803] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132500, global step 2120923: loss 0.0047
[2019-04-01 17:46:45,805] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132500, global step 2120923: learning rate 0.0010
[2019-04-01 17:46:45,806] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132500, global step 2120923: loss 0.0005
[2019-04-01 17:46:45,807] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132500, global step 2120923: learning rate 0.0010
[2019-04-01 17:46:46,214] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2121102: loss 0.0118
[2019-04-01 17:46:46,219] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2121105: learning rate 0.0010
[2019-04-01 17:46:46,286] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2121138: loss 0.0004
[2019-04-01 17:46:46,288] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2121138: learning rate 0.0010
[2019-04-01 17:46:47,581] A3C_AGENT_WORKER-Thread-9 INFO:Local step 132500, global step 2121705: loss 0.0276
[2019-04-01 17:46:47,617] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 132500, global step 2121719: learning rate 0.0010
[2019-04-01 17:46:47,646] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2121732: loss 3.3075
[2019-04-01 17:46:47,647] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2121732: learning rate 0.0010
[2019-04-01 17:46:47,760] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2121780: loss 0.0021
[2019-04-01 17:46:47,777] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2121782: learning rate 0.0010
[2019-04-01 17:46:48,368] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2122079: loss 0.1039
[2019-04-01 17:46:48,369] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2122079: learning rate 0.0010
[2019-04-01 17:46:48,861] A3C_AGENT_WORKER-Thread-4 INFO:Local step 133000, global step 2122339: loss 1.9470
[2019-04-01 17:46:48,862] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 133000, global step 2122339: learning rate 0.0010
[2019-04-01 17:46:48,862] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2122339: loss 0.0823
[2019-04-01 17:46:48,865] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2122340: learning rate 0.0010
[2019-04-01 17:46:49,250] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2122543: loss 0.0255
[2019-04-01 17:46:49,252] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2122543: learning rate 0.0010
[2019-04-01 17:46:49,453] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2122643: loss 1.3343
[2019-04-01 17:46:49,461] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2122644: learning rate 0.0010
[2019-04-01 17:46:50,516] A3C_AGENT_WORKER-Thread-5 INFO:Local step 132500, global step 2123199: loss 0.0729
[2019-04-01 17:46:50,520] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 132500, global step 2123202: learning rate 0.0010
[2019-04-01 17:46:53,089] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.5924726e-29 4.6571330e-35 5.1135308e-21 4.7213547e-33 6.3944508e-30
 1.0000000e+00 1.2625438e-13], sum to 1.0000
[2019-04-01 17:46:53,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5500
[2019-04-01 17:46:53,120] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.30926797033492, 7.7520343510587, 0.0, 1.0, 41.85001278177106], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3100200.0000, 
sim time next is 3100800.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.30645047158449, 7.76263486169664, 0.0, 1.0, 41.95898047545244], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 1.0, 0.0, 0.0, 1.0, 0.9009214959406415, 0.07762634861696639, 0.0, 1.0, 0.29970700339608886], 
reward next is 0.7003, 
noisyNet noise sample is [array([0.59378344], dtype=float32), 3.0619967]. 
=============================================
[2019-04-01 17:46:56,135] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:46:56,135] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4162
[2019-04-01 17:46:56,151] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.816666666666666, 76.16666666666667, 0.0, 0.0, 26.0, 24.43020479885713, 6.238827862189768, 0.0, 1.0, 43.18311450256954], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3300600.0000, 
sim time next is 3301200.0000, 
raw observation next is [-10.0, 76.0, 0.0, 0.0, 26.0, 24.42234915119627, 6.206493371425441, 0.0, 1.0, 43.10415579150184], 
processed observation next is [1.0, 0.21739130434782608, 0.18559556786703602, 0.76, 0.0, 0.0, 1.0, 0.7746213073137527, 0.06206493371425441, 0.0, 1.0, 0.307886827082156], 
reward next is 0.6921, 
noisyNet noise sample is [array([0.3916005], dtype=float32), -1.3306652]. 
=============================================
[2019-04-01 17:46:56,280] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8213618e-34 0.0000000e+00 4.4627328e-31 9.6730956e-37 2.3659812e-26
 1.0000000e+00 7.8832337e-17], sum to 1.0000
[2019-04-01 17:46:56,280] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4369
[2019-04-01 17:46:56,296] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.83333333333333, 76.0, 0.0, 0.0, 26.0, 24.232305428379, 5.71887405799431, 0.0, 1.0, 43.15399873373178], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3304200.0000, 
sim time next is 3304800.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 26.0, 24.13178913675652, 5.638074423578996, 0.0, 1.0, 43.23388290253625], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 1.0, 0.7331127338223601, 0.05638074423578996, 0.0, 1.0, 0.30881344930383037], 
reward next is 0.6912, 
noisyNet noise sample is [array([-0.6960902], dtype=float32), -1.2876111]. 
=============================================
[2019-04-01 17:46:56,798] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2126776: loss 0.0146
[2019-04-01 17:46:56,798] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2126776: learning rate 0.0010
[2019-04-01 17:46:57,529] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 4.2451681e-38 0.0000000e+00 0.0000000e+00
 1.0000000e+00 6.8548943e-19], sum to 1.0000
[2019-04-01 17:46:57,529] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4294
[2019-04-01 17:46:57,541] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.3599195515397, 9.4974634380261, 0.0, 1.0, 41.83713825200345], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3544200.0000, 
sim time next is 3544800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.35989768008789, 9.371110886406361, 0.0, 1.0, 41.71370636510016], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 1.0, 0.9085568114411272, 0.09371110886406361, 0.0, 1.0, 0.2979550454650011], 
reward next is 0.7020, 
noisyNet noise sample is [array([-1.0329887], dtype=float32), 0.4125915]. 
=============================================
[2019-04-01 17:46:58,022] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.2308896e-34 0.0000000e+00 3.2776625e-29 1.8728150e-35 2.3301556e-09
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:46:58,022] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1442
[2019-04-01 17:46:58,065] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 57.00000000000001, 274.0, 26.0, 25.47068859486787, 10.26803732509035, 1.0, 1.0, 24.0873320161394], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3226200.0000, 
sim time next is 3226800.0000, 
raw observation next is [-3.0, 92.0, 71.00000000000001, 322.0000000000001, 26.0, 25.42345919672537, 10.46807770799781, 1.0, 1.0, 22.66932323165621], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.23666666666666672, 0.3558011049723758, 1.0, 0.9176370281036245, 0.10468077707997811, 1.0, 1.0, 0.16192373736897295], 
reward next is 0.6508, 
noisyNet noise sample is [array([0.22768816], dtype=float32), 0.6553651]. 
=============================================
[2019-04-01 17:46:58,696] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133000, global step 2127745: loss 0.0444
[2019-04-01 17:46:58,697] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133000, global step 2127745: learning rate 0.0010
[2019-04-01 17:46:59,485] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2128152: loss 0.5619
[2019-04-01 17:46:59,490] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2128156: learning rate 0.0010
[2019-04-01 17:47:00,833] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:47:00,833] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3104
[2019-04-01 17:47:00,858] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.45815141943481, 11.27864365579336, 0.0, 1.0, 42.42339222510589], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3358800.0000, 
sim time next is 3359400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.52900514059471, 11.29485726890504, 0.0, 1.0, 34.7340440773656], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 1.0, 0.9327150200849584, 0.11294857268905041, 0.0, 1.0, 0.24810031483832573], 
reward next is 0.7519, 
noisyNet noise sample is [array([0.00696453], dtype=float32), -1.0440663]. 
=============================================
[2019-04-01 17:47:01,297] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:47:01,300] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2798
[2019-04-01 17:47:01,313] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.81416365590479, 6.753710167736021, 0.0, 1.0, 40.59180803617959], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3381000.0000, 
sim time next is 3381600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.79851517171381, 6.74166917728409, 0.0, 1.0, 40.62222394211789], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.8283593102448299, 0.0674166917728409, 0.0, 1.0, 0.2901587424436992], 
reward next is 0.7098, 
noisyNet noise sample is [array([0.29114357], dtype=float32), -0.107458346]. 
=============================================
[2019-04-01 17:47:01,394] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133000, global step 2129004: loss 0.5029
[2019-04-01 17:47:01,395] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133000, global step 2129004: learning rate 0.0010
[2019-04-01 17:47:01,732] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133000, global step 2129171: loss 0.7193
[2019-04-01 17:47:01,733] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133000, global step 2129171: learning rate 0.0010
[2019-04-01 17:47:01,921] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2129262: loss 0.5036
[2019-04-01 17:47:01,923] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2129262: learning rate 0.0010
[2019-04-01 17:47:02,223] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2129406: loss 0.3918
[2019-04-01 17:47:02,224] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2129406: learning rate 0.0010
[2019-04-01 17:47:02,519] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2129542: loss 2.0660
[2019-04-01 17:47:02,521] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2129542: learning rate 0.0010
[2019-04-01 17:47:03,471] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133000, global step 2130023: loss 1.0297
[2019-04-01 17:47:03,473] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133000, global step 2130023: learning rate 0.0010
[2019-04-01 17:47:03,570] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2130079: loss 0.8081
[2019-04-01 17:47:03,573] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2130080: learning rate 0.0010
[2019-04-01 17:47:03,790] A3C_AGENT_WORKER-Thread-4 INFO:Local step 133500, global step 2130223: loss 0.7528
[2019-04-01 17:47:03,794] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 133500, global step 2130226: learning rate 0.0010
[2019-04-01 17:47:04,109] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2130406: loss 1.0319
[2019-04-01 17:47:04,112] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2130407: learning rate 0.0010
[2019-04-01 17:47:04,158] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 5.8930471e-34 0.0000000e+00 0.0000000e+00
 1.0576993e-02 9.8942298e-01], sum to 1.0000
[2019-04-01 17:47:04,158] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3779
[2019-04-01 17:47:04,173] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.833333333333333, 47.5, 92.66666666666666, 728.6666666666667, 26.0, 25.50391185268908, 10.54567275290084, 0.0, 1.0, 4.262520371750927], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3683400.0000, 
sim time next is 3684000.0000, 
raw observation next is [5.666666666666666, 48.0, 89.83333333333333, 716.8333333333333, 26.0, 25.5532416922056, 10.57379875490291, 0.0, 1.0, 3.164339339060156], 
processed observation next is [0.0, 0.6521739130434783, 0.6195752539242845, 0.48, 0.2994444444444444, 0.7920810313075506, 1.0, 0.9361773846007999, 0.1057379875490291, 0.0, 1.0, 0.022602423850429685], 
reward next is 0.9774, 
noisyNet noise sample is [array([-0.8508279], dtype=float32), -0.6577153]. 
=============================================
[2019-04-01 17:47:04,187] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[84.43824 ]
 [84.38739 ]
 [84.29983 ]
 [84.22777 ]
 [84.122505]], R is [[84.63627625]
 [84.75946808]
 [84.88147736]
 [85.00114441]
 [85.11167908]].
[2019-04-01 17:47:04,342] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2130546: loss 1.8625
[2019-04-01 17:47:04,343] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2130548: learning rate 0.0010
[2019-04-01 17:47:04,354] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2130556: loss 1.3029
[2019-04-01 17:47:04,356] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2130558: learning rate 0.0010
[2019-04-01 17:47:04,710] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2130722: loss 1.2056
[2019-04-01 17:47:04,711] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2130722: learning rate 0.0010
[2019-04-01 17:47:05,846] A3C_AGENT_WORKER-Thread-5 INFO:Local step 133000, global step 2131347: loss 1.5332
[2019-04-01 17:47:05,849] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 133000, global step 2131348: learning rate 0.0010
[2019-04-01 17:47:06,667] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5882201e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:47:06,667] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9657
[2019-04-01 17:47:06,682] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 67.0, 28.33333333333333, 251.6666666666666, 26.0, 26.64818423688915, 14.49122326166706, 1.0, 1.0, 10.43986633699805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3431400.0000, 
sim time next is 3432000.0000, 
raw observation next is [2.0, 67.0, 20.16666666666666, 186.3333333333333, 26.0, 26.38914029482829, 12.66834022298695, 1.0, 1.0, 10.03031126540906], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.0672222222222222, 0.2058931860036832, 1.0, 1.0555914706897558, 0.1266834022298695, 1.0, 1.0, 0.07164508046720756], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.39526108], dtype=float32), 0.11712132]. 
=============================================
[2019-04-01 17:47:06,696] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[38.040894]
 [37.662914]
 [37.255993]
 [36.89732 ]
 [36.60932 ]], R is [[37.94599533]
 [37.56653595]
 [37.19087219]
 [36.8189621 ]
 [36.45077133]].
[2019-04-01 17:47:07,091] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 7.3517257e-28], sum to 1.0000
[2019-04-01 17:47:07,093] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7600
[2019-04-01 17:47:07,107] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.45815141943481, 11.27864365579336, 0.0, 1.0, 42.42339222510589], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3358800.0000, 
sim time next is 3359400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.52900514059471, 11.29485726890504, 0.0, 1.0, 34.7340440773656], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 1.0, 0.9327150200849584, 0.11294857268905041, 0.0, 1.0, 0.24810031483832573], 
reward next is 0.7519, 
noisyNet noise sample is [array([3.0353477], dtype=float32), -1.4925694]. 
=============================================
[2019-04-01 17:47:09,822] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.3364036e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.5662836e-30
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:47:09,823] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2860
[2019-04-01 17:47:09,837] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 48.5, 109.0, 764.0, 26.0, 26.58889087062421, 13.70353033421008, 1.0, 1.0, 12.34844136828508], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3407400.0000, 
sim time next is 3408000.0000, 
raw observation next is [2.666666666666667, 48.66666666666666, 110.0, 770.6666666666667, 26.0, 26.60810555390613, 14.08303442793292, 1.0, 1.0, 11.62753605588563], 
processed observation next is [1.0, 0.43478260869565216, 0.5364727608494922, 0.4866666666666666, 0.36666666666666664, 0.8515653775322285, 1.0, 1.0868722219865903, 0.1408303442793292, 1.0, 1.0, 0.08305382897061164], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.003137], dtype=float32), 0.99985474]. 
=============================================
[2019-04-01 17:47:09,849] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[32.295918]
 [31.30301 ]
 [30.47552 ]
 [29.604397]
 [28.458693]], R is [[32.40351486]
 [32.07947922]
 [31.75868416]
 [31.44109726]
 [31.1266861 ]].
[2019-04-01 17:47:12,205] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2134791: loss 1.9713
[2019-04-01 17:47:12,205] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2134791: learning rate 0.0010
[2019-04-01 17:47:13,845] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133500, global step 2135753: loss 1.6482
[2019-04-01 17:47:13,850] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133500, global step 2135754: learning rate 0.0010
[2019-04-01 17:47:13,934] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:47:13,935] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7153
[2019-04-01 17:47:13,969] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 58.33333333333334, 115.1666666666667, 812.1666666666666, 26.0, 26.32232185871425, 13.54962406508014, 1.0, 1.0, 7.44433414454693], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3498000.0000, 
sim time next is 3498600.0000, 
raw observation next is [1.833333333333333, 57.66666666666666, 115.3333333333333, 814.3333333333334, 26.0, 26.31842282783144, 11.59881153371345, 1.0, 1.0, 7.674202734774461], 
processed observation next is [1.0, 0.4782608695652174, 0.5133887349953832, 0.5766666666666665, 0.3844444444444443, 0.899815837937385, 1.0, 1.0454889754044916, 0.1159881153371345, 1.0, 1.0, 0.05481573381981758], 
reward next is 0.3057, 
noisyNet noise sample is [array([-0.8092748], dtype=float32), 1.2081543]. 
=============================================
[2019-04-01 17:47:14,681] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2136218: loss 3.0113
[2019-04-01 17:47:14,681] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2136218: learning rate 0.0010
[2019-04-01 17:47:16,528] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2137179: loss 1.4831
[2019-04-01 17:47:16,531] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2137179: learning rate 0.0010
[2019-04-01 17:47:16,706] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2137285: loss 3.0052
[2019-04-01 17:47:16,707] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2137285: learning rate 0.0010
[2019-04-01 17:47:16,824] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133500, global step 2137343: loss 4.0050
[2019-04-01 17:47:16,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133500, global step 2137345: learning rate 0.0010
[2019-04-01 17:47:16,830] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133500, global step 2137346: loss 3.7528
[2019-04-01 17:47:16,832] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133500, global step 2137347: learning rate 0.0010
[2019-04-01 17:47:17,070] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2137476: loss 3.4217
[2019-04-01 17:47:17,073] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00
 6.06828e-20], sum to 1.0000
[2019-04-01 17:47:17,074] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6195
[2019-04-01 17:47:17,074] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2137477: learning rate 0.0010
[2019-04-01 17:47:17,092] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 69.0, 0.0, 0.0, 26.0, 25.5574071731789, 8.120127440326007, 0.0, 1.0, 28.35705717203453], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3720000.0000, 
sim time next is 3720600.0000, 
raw observation next is [-3.0, 68.0, 0.0, 0.0, 26.0, 25.50235255189353, 7.768969626952521, 0.0, 1.0, 27.0988491156137], 
processed observation next is [1.0, 0.043478260869565216, 0.3795013850415513, 0.68, 0.0, 0.0, 1.0, 0.9289075074133615, 0.07768969626952521, 0.0, 1.0, 0.19356320796866927], 
reward next is 0.8064, 
noisyNet noise sample is [array([0.1430514], dtype=float32), 0.24847722]. 
=============================================
[2019-04-01 17:47:17,613] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5635462e-37 0.0000000e+00 5.1593589e-35 0.0000000e+00 1.2670780e-28
 1.2374904e-05 9.9998760e-01], sum to 1.0000
[2019-04-01 17:47:17,616] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6027
[2019-04-01 17:47:17,649] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.0, 32.0, 46.5, 262.0, 26.0, 25.47160757527686, 8.508966773970997, 0.0, 1.0, 31.98509901467042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3657600.0000, 
sim time next is 3658200.0000, 
raw observation next is [8.5, 31.0, 60.66666666666668, 309.0, 26.0, 25.53097405126149, 8.75313002075553, 0.0, 1.0, 30.28606859209281], 
processed observation next is [0.0, 0.34782608695652173, 0.698060941828255, 0.31, 0.20222222222222228, 0.3414364640883978, 1.0, 0.932996293037356, 0.0875313002075553, 0.0, 1.0, 0.2163290613720915], 
reward next is 0.7837, 
noisyNet noise sample is [array([-1.4975045], dtype=float32), -0.73236257]. 
=============================================
[2019-04-01 17:47:17,656] A3C_AGENT_WORKER-Thread-4 INFO:Local step 134000, global step 2137810: loss 1.7461
[2019-04-01 17:47:17,660] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 134000, global step 2137811: learning rate 0.0010
[2019-04-01 17:47:18,375] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2138237: loss 2.4950
[2019-04-01 17:47:18,376] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2138237: learning rate 0.0010
[2019-04-01 17:47:18,486] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133500, global step 2138303: loss 2.1757
[2019-04-01 17:47:18,488] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133500, global step 2138304: learning rate 0.0010
[2019-04-01 17:47:18,616] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2138387: loss 3.3096
[2019-04-01 17:47:18,618] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2138388: learning rate 0.0010
[2019-04-01 17:47:18,841] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2138534: loss 3.5174
[2019-04-01 17:47:18,844] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2138537: learning rate 0.0010
[2019-04-01 17:47:19,019] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2138657: loss 3.3559
[2019-04-01 17:47:19,021] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2138658: learning rate 0.0010
[2019-04-01 17:47:19,664] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2139064: loss 4.0467
[2019-04-01 17:47:19,671] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2139066: learning rate 0.0010
[2019-04-01 17:47:20,509] A3C_AGENT_WORKER-Thread-5 INFO:Local step 133500, global step 2139571: loss 5.1544
[2019-04-01 17:47:20,512] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 133500, global step 2139571: learning rate 0.0010
[2019-04-01 17:47:20,574] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.2152117e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 17:47:20,585] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2715
[2019-04-01 17:47:20,592] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 25.14477301790303, 7.806125595994513, 0.0, 1.0, 6.559085325508971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3694200.0000, 
sim time next is 3694800.0000, 
raw observation next is [4.0, 59.0, 0.0, 0.0, 26.0, 25.08261831881148, 7.639233197972279, 0.0, 1.0, 8.878005772634108], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.59, 0.0, 0.0, 1.0, 0.8689454741159258, 0.0763923319797228, 0.0, 1.0, 0.06341432694738648], 
reward next is 0.9366, 
noisyNet noise sample is [array([0.17486545], dtype=float32), -0.938649]. 
=============================================
[2019-04-01 17:47:22,792] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:47:22,792] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7546
[2019-04-01 17:47:22,844] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.36776835110804, 10.02052741432003, 0.0, 1.0, 26.96994130847488], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4046400.0000, 
sim time next is 4047000.0000, 
raw observation next is [-4.0, 30.66666666666667, 0.0, 0.0, 26.0, 25.3927197662793, 9.919777249623584, 1.0, 1.0, 22.73958376545664], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.3066666666666667, 0.0, 0.0, 1.0, 0.9132456808970427, 0.09919777249623583, 1.0, 1.0, 0.16242559832469028], 
reward next is 0.8376, 
noisyNet noise sample is [array([-1.6689258], dtype=float32), 0.41304114]. 
=============================================
[2019-04-01 17:47:22,856] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[31.762085]
 [28.063656]
 [23.509945]
 [24.238209]
 [19.649982]], R is [[30.65971184]
 [31.16047287]
 [31.65632629]
 [32.09125137]
 [32.55383301]].
[2019-04-01 17:47:24,262] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.4893042e-20 0.0000000e+00 0.0000000e+00 2.4235373e-28 6.6298334e-13
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:47:24,265] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6167
[2019-04-01 17:47:24,273] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 60.0, 96.5, 743.5, 26.0, 26.87314883294417, 16.48412386781904, 1.0, 1.0, 8.468935828525595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3769200.0000, 
sim time next is 3769800.0000, 
raw observation next is [0.0, 60.0, 93.33333333333334, 732.6666666666667, 26.0, 26.92656165488302, 16.84563901829148, 1.0, 1.0, 8.546365369794438], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.3111111111111111, 0.8095764272559853, 1.0, 1.1323659506975743, 0.16845639018291478, 1.0, 1.0, 0.06104546692710313], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.27923453], dtype=float32), -1.2509868]. 
=============================================
[2019-04-01 17:47:26,135] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2142476: loss 0.2825
[2019-04-01 17:47:26,137] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2142476: learning rate 0.0010
[2019-04-01 17:47:27,285] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.7479327e-37 0.0000000e+00 1.1747131e-34
 1.0000000e+00 6.3549675e-35], sum to 1.0000
[2019-04-01 17:47:27,286] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1083
[2019-04-01 17:47:27,351] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.333333333333334, 62.0, 5.0, 135.8333333333333, 26.0, 25.03521653353365, 9.523313976024859, 1.0, 1.0, 60.50267161694163], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3914400.0000, 
sim time next is 3915000.0000, 
raw observation next is [-7.5, 61.0, 6.0, 163.0, 26.0, 25.72849241048349, 9.558255133455804, 1.0, 1.0, 45.97133588865292], 
processed observation next is [1.0, 0.30434782608695654, 0.2548476454293629, 0.61, 0.02, 0.18011049723756906, 1.0, 0.9612132014976414, 0.09558255133455804, 1.0, 1.0, 0.32836668491894944], 
reward next is 0.6716, 
noisyNet noise sample is [array([-0.14790495], dtype=float32), 0.20703533]. 
=============================================
[2019-04-01 17:47:27,352] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[48.4352  ]
 [54.93497 ]
 [55.164677]
 [55.14809 ]
 [55.180077]], R is [[42.91704941]
 [43.05571747]
 [42.86175537]
 [43.13177109]
 [43.40003586]].
[2019-04-01 17:47:28,856] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134000, global step 2143859: loss 0.7136
[2019-04-01 17:47:28,857] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134000, global step 2143859: learning rate 0.0010
[2019-04-01 17:47:28,948] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2143913: loss 0.4739
[2019-04-01 17:47:28,951] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2143915: learning rate 0.0010
[2019-04-01 17:47:31,406] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2145111: loss 0.8981
[2019-04-01 17:47:31,406] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2145111: learning rate 0.0010
[2019-04-01 17:47:31,562] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134000, global step 2145174: loss 1.0176
[2019-04-01 17:47:31,564] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134000, global step 2145175: learning rate 0.0010
[2019-04-01 17:47:31,676] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134000, global step 2145220: loss 1.0566
[2019-04-01 17:47:31,678] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134000, global step 2145220: learning rate 0.0010
[2019-04-01 17:47:31,911] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2145329: loss 0.5859
[2019-04-01 17:47:31,912] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2145329: learning rate 0.0010
[2019-04-01 17:47:32,278] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2145510: loss 2.6706
[2019-04-01 17:47:32,280] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2145510: learning rate 0.0010
[2019-04-01 17:47:33,120] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:47:33,124] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0495
[2019-04-01 17:47:33,172] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0628896e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.3746953e-37
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:47:33,173] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0856
[2019-04-01 17:47:33,174] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.166666666666666, 41.66666666666667, 0.0, 0.0, 26.0, 25.78196576212125, 11.79521481991308, 1.0, 1.0, 21.46585977490865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3957000.0000, 
sim time next is 3957600.0000, 
raw observation next is [-6.333333333333333, 42.33333333333334, 0.0, 0.0, 26.0, 25.74591269300517, 11.186874459941, 1.0, 1.0, 22.13507306069252], 
processed observation next is [1.0, 0.8260869565217391, 0.28716528162511545, 0.42333333333333345, 0.0, 0.0, 1.0, 0.9637018132864529, 0.11186874459941, 1.0, 1.0, 0.1581076647192323], 
reward next is 0.3671, 
noisyNet noise sample is [array([0.5403245], dtype=float32), 0.28333452]. 
=============================================
[2019-04-01 17:47:33,216] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 46.33333333333334, 118.5, 833.1666666666667, 26.0, 25.97475576767689, 14.49617593910776, 1.0, 1.0, 22.65418298334484], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3933600.0000, 
sim time next is 3934200.0000, 
raw observation next is [-6.0, 45.66666666666666, 118.0, 831.3333333333334, 26.0, 26.31231738990316, 15.64455037333648, 1.0, 1.0, 20.88865133561872], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.45666666666666655, 0.3933333333333333, 0.9186003683241253, 1.0, 1.0446167699861657, 0.1564455037333648, 1.0, 1.0, 0.1492046523972766], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6742827], dtype=float32), -0.7591064]. 
=============================================
[2019-04-01 17:47:33,233] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2145905: loss 0.5113
[2019-04-01 17:47:33,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2145905: learning rate 0.0010
[2019-04-01 17:47:33,347] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134000, global step 2145966: loss 0.9612
[2019-04-01 17:47:33,348] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134000, global step 2145967: learning rate 0.0010
[2019-04-01 17:47:33,711] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2146140: loss 0.7642
[2019-04-01 17:47:33,715] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2146141: learning rate 0.0010
[2019-04-01 17:47:33,856] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2146215: loss 0.8045
[2019-04-01 17:47:33,857] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2146215: learning rate 0.0010
[2019-04-01 17:47:33,941] A3C_AGENT_WORKER-Thread-4 INFO:Local step 134500, global step 2146263: loss 0.8059
[2019-04-01 17:47:33,942] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 134500, global step 2146263: learning rate 0.0010
[2019-04-01 17:47:34,586] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2146615: loss 1.7693
[2019-04-01 17:47:34,587] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2146615: learning rate 0.0010
[2019-04-01 17:47:34,745] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2146702: loss 0.7760
[2019-04-01 17:47:34,746] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2146702: learning rate 0.0010
[2019-04-01 17:47:35,755] A3C_AGENT_WORKER-Thread-5 INFO:Local step 134000, global step 2147154: loss 0.7578
[2019-04-01 17:47:35,757] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 134000, global step 2147155: learning rate 0.0010
[2019-04-01 17:47:37,710] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1235763e-28 9.1040789e-34 3.1005963e-24 7.4364797e-36 4.0098359e-25
 3.0744232e-07 9.9999964e-01], sum to 1.0000
[2019-04-01 17:47:37,712] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2106
[2019-04-01 17:47:37,727] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.35, 75.5, 0.0, 0.0, 26.0, 25.48117087649091, 8.652568129304688, 0.0, 1.0, 36.05773152035499], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4321800.0000, 
sim time next is 4322400.0000, 
raw observation next is [4.3, 75.33333333333334, 0.0, 0.0, 26.0, 25.49298843978675, 8.688635611217032, 0.0, 1.0, 34.19248883650449], 
processed observation next is [1.0, 0.0, 0.5817174515235458, 0.7533333333333334, 0.0, 0.0, 1.0, 0.9275697771123929, 0.08688635611217033, 0.0, 1.0, 0.2442320631178892], 
reward next is 0.7558, 
noisyNet noise sample is [array([0.8053919], dtype=float32), 0.42071357]. 
=============================================
[2019-04-01 17:47:38,090] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1464995e-33 0.0000000e+00 0.0000000e+00 1.3068857e-37 4.8049319e-19
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:47:38,098] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2760
[2019-04-01 17:47:38,112] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.6, 29.0, 116.5, 847.5, 26.0, 26.86396558796802, 22.43639265604389, 1.0, 1.0, 1.569930816956743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4366800.0000, 
sim time next is 4367400.0000, 
raw observation next is [14.58333333333333, 29.33333333333334, 116.0, 845.6666666666666, 26.0, 27.27821674776133, 23.97642114473986, 1.0, 1.0, 1.542080210651041], 
processed observation next is [1.0, 0.5652173913043478, 0.8665743305632503, 0.2933333333333334, 0.38666666666666666, 0.9344383057090239, 1.0, 1.182602392537333, 0.2397642114473986, 1.0, 1.0, 0.011014858647507435], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.84556526], dtype=float32), 0.43291932]. 
=============================================
[2019-04-01 17:47:38,536] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.0795683e-37 0.0000000e+00 6.7603542e-38
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:47:38,538] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0257
[2019-04-01 17:47:38,550] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.3, 71.0, 0.0, 0.0, 26.0, 25.59268080555864, 8.464398023219703, 0.0, 1.0, 29.53307538374958], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4341600.0000, 
sim time next is 4342200.0000, 
raw observation next is [3.233333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.63946788814867, 8.27405075131337, 0.0, 1.0, 26.92218727480638], 
processed observation next is [1.0, 0.2608695652173913, 0.5521698984302863, 0.7166666666666667, 0.0, 0.0, 1.0, 0.948495412592667, 0.0827405075131337, 0.0, 1.0, 0.19230133767718843], 
reward next is 0.8077, 
noisyNet noise sample is [array([0.5575401], dtype=float32), -0.6578867]. 
=============================================
[2019-04-01 17:47:40,154] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2030794e-35
 1.0000000e+00 1.4799690e-30], sum to 1.0000
[2019-04-01 17:47:40,155] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8073
[2019-04-01 17:47:40,192] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.2, 31.5, 195.0, 629.0, 26.0, 28.79161891398876, 35.44655861698647, 1.0, 1.0, 1.080372850440519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4372200.0000, 
sim time next is 4372800.0000, 
raw observation next is [14.1, 31.66666666666666, 179.6666666666667, 524.1666666666667, 26.0, 28.89363545151189, 30.61010230745886, 1.0, 1.0, 1.160561351587439], 
processed observation next is [1.0, 0.6086956521739131, 0.8531855955678671, 0.3166666666666666, 0.598888888888889, 0.5791896869244937, 1.0, 1.4133764930731272, 0.3061010230745886, 1.0, 1.0, 0.008289723939910279], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.730281], dtype=float32), -0.4929241]. 
=============================================
[2019-04-01 17:47:40,706] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:47:40,707] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3141
[2019-04-01 17:47:40,726] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 208.0, 88.5, 26.0, 26.38566591211664, 13.8591181072527, 1.0, 1.0, 9.358790386587486], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4442400.0000, 
sim time next is 4443000.0000, 
raw observation next is [1.0, 86.0, 222.3333333333333, 107.6666666666667, 26.0, 26.4220464879096, 14.28206794184157, 1.0, 1.0, 8.993186118531153], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.7411111111111109, 0.11896869244935547, 1.0, 1.0602923554156571, 0.1428206794184157, 1.0, 1.0, 0.06423704370379395], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.05917177], dtype=float32), 1.6053259]. 
=============================================
[2019-04-01 17:47:40,732] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[40.58753 ]
 [40.199806]
 [39.715603]
 [39.048046]
 [38.12127 ]], R is [[40.48009109]
 [40.07529068]
 [39.67453766]
 [39.27779388]
 [38.8850174 ]].
[2019-04-01 17:47:41,350] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6015264e-38 0.0000000e+00 1.2487265e-31 0.0000000e+00 6.2575721e-30
 9.9983847e-01 1.6155381e-04], sum to 1.0000
[2019-04-01 17:47:41,352] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0157
[2019-04-01 17:47:41,376] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.166666666666667, 49.83333333333334, 0.0, 0.0, 26.0, 24.79687745118334, 6.536282967322407, 0.0, 1.0, 39.26562063126106], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4169400.0000, 
sim time next is 4170000.0000, 
raw observation next is [-4.333333333333334, 49.66666666666667, 0.0, 0.0, 26.0, 24.78890890656112, 6.466590657667624, 0.0, 1.0, 39.33356434705458], 
processed observation next is [0.0, 0.2608695652173913, 0.3425669436749769, 0.4966666666666667, 0.0, 0.0, 1.0, 0.8269869866515884, 0.06466590657667624, 0.0, 1.0, 0.2809540310503899], 
reward next is 0.7190, 
noisyNet noise sample is [array([-0.59009624], dtype=float32), 0.08947564]. 
=============================================
[2019-04-01 17:47:41,394] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[71.12234]
 [71.21031]
 [71.27747]
 [71.34462]
 [71.43155]], R is [[71.03636932]
 [71.04553223]
 [71.05509949]
 [71.06504059]
 [71.07526398]].
[2019-04-01 17:47:41,662] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 2.959221e-31
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 17:47:41,664] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4963
[2019-04-01 17:47:41,707] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 36.66666666666666, 94.0, 504.0, 26.0, 25.99725937840034, 10.14564411355236, 1.0, 1.0, 24.31576629319218], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4092000.0000, 
sim time next is 4092600.0000, 
raw observation next is [-3.166666666666667, 37.33333333333334, 96.0, 539.0, 26.0, 26.22157822015604, 10.04380772967382, 1.0, 1.0, 24.4541639095823], 
processed observation next is [1.0, 0.34782608695652173, 0.3748845798707295, 0.3733333333333334, 0.32, 0.5955801104972376, 1.0, 1.0316540314508629, 0.10043807729673819, 1.0, 1.0, 0.17467259935415927], 
reward next is 0.8078, 
noisyNet noise sample is [array([0.66317004], dtype=float32), 1.4652262]. 
=============================================
[2019-04-01 17:47:42,290] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2150603: loss 1.3243
[2019-04-01 17:47:42,293] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2150604: learning rate 0.0010
[2019-04-01 17:47:44,489] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134500, global step 2151852: loss 1.7974
[2019-04-01 17:47:44,490] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134500, global step 2151853: learning rate 0.0010
[2019-04-01 17:47:44,795] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2152002: loss 0.2602
[2019-04-01 17:47:44,796] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2152003: learning rate 0.0010
[2019-04-01 17:47:45,665] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2152505: loss 1.2892
[2019-04-01 17:47:45,666] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2152505: learning rate 0.0010
[2019-04-01 17:47:46,993] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:47:46,993] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9086
[2019-04-01 17:47:47,006] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 26.0, 25.49600524901417, 8.072318078492154, 0.0, 1.0, 30.53211641397464], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4228200.0000, 
sim time next is 4228800.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 25.47384077317072, 7.949958210413719, 0.0, 1.0, 32.35986788748217], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.47, 0.0, 0.0, 1.0, 0.9248343961672459, 0.07949958210413718, 0.0, 1.0, 0.2311419134820155], 
reward next is 0.7689, 
noisyNet noise sample is [array([0.06705201], dtype=float32), -1.1322068]. 
=============================================
[2019-04-01 17:47:47,017] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2153262: loss 0.2131
[2019-04-01 17:47:47,018] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2153264: learning rate 0.0010
[2019-04-01 17:47:47,104] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134500, global step 2153316: loss 0.8974
[2019-04-01 17:47:47,107] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134500, global step 2153316: learning rate 0.0010
[2019-04-01 17:47:47,294] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134500, global step 2153432: loss 0.5160
[2019-04-01 17:47:47,295] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134500, global step 2153432: learning rate 0.0010
[2019-04-01 17:47:47,407] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2153506: loss 0.2570
[2019-04-01 17:47:47,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2153506: learning rate 0.0010
[2019-04-01 17:47:47,522] A3C_AGENT_WORKER-Thread-4 INFO:Local step 135000, global step 2153580: loss 0.7223
[2019-04-01 17:47:47,531] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 135000, global step 2153583: learning rate 0.0010
[2019-04-01 17:47:47,921] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2153839: loss 0.5764
[2019-04-01 17:47:47,924] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2153840: learning rate 0.0010
[2019-04-01 17:47:48,513] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2154185: loss 1.6012
[2019-04-01 17:47:48,514] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2154187: learning rate 0.0010
[2019-04-01 17:47:48,627] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134500, global step 2154252: loss 2.0869
[2019-04-01 17:47:48,629] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134500, global step 2154252: learning rate 0.0010
[2019-04-01 17:47:49,044] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2154490: loss 3.3184
[2019-04-01 17:47:49,046] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2154492: learning rate 0.0010
[2019-04-01 17:47:49,104] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2154521: loss 1.7068
[2019-04-01 17:47:49,108] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2154521: learning rate 0.0010
[2019-04-01 17:47:49,933] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 9.999975e-01 2.547232e-06], sum to 1.0000
[2019-04-01 17:47:49,934] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5054
[2019-04-01 17:47:49,941] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.333333333333333, 62.66666666666667, 20.0, 190.0, 26.0, 25.39031745330221, 8.65175128539343, 0.0, 1.0, 3.879822086908214], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4297200.0000, 
sim time next is 4297800.0000, 
raw observation next is [6.266666666666667, 63.33333333333334, 16.0, 152.0, 26.0, 25.34687782465273, 8.424702759808541, 0.0, 1.0, 3.938455294770476], 
processed observation next is [0.0, 0.7391304347826086, 0.6361957525392429, 0.6333333333333334, 0.05333333333333334, 0.16795580110497238, 1.0, 0.906696832093247, 0.08424702759808542, 0.0, 1.0, 0.028131823534074826], 
reward next is 0.9719, 
noisyNet noise sample is [array([-0.361946], dtype=float32), -0.44000024]. 
=============================================
[2019-04-01 17:47:50,048] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.771046e-36
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 17:47:50,050] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3989
[2019-04-01 17:47:50,065] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.3, 38.0, 115.0, 780.0, 26.0, 27.59134761629115, 20.12807983205133, 1.0, 1.0, 6.594166838516332], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4357800.0000, 
sim time next is 4358400.0000, 
raw observation next is [11.73333333333333, 36.66666666666666, 115.8333333333333, 788.0, 26.0, 27.61315295100853, 21.03991050076426, 1.0, 1.0, 5.880632332876776], 
processed observation next is [1.0, 0.43478260869565216, 0.7876269621421976, 0.3666666666666666, 0.386111111111111, 0.8707182320441988, 1.0, 1.230450421572647, 0.2103991050076426, 1.0, 1.0, 0.042004516663405544], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.06518915], dtype=float32), 0.73886263]. 
=============================================
[2019-04-01 17:47:50,198] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2155193: loss 1.1630
[2019-04-01 17:47:50,198] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2155193: learning rate 0.0010
[2019-04-01 17:47:51,380] A3C_AGENT_WORKER-Thread-5 INFO:Local step 134500, global step 2155883: loss 0.9692
[2019-04-01 17:47:51,382] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 134500, global step 2155883: learning rate 0.0010
[2019-04-01 17:47:55,456] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.0457074e-36
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:47:55,457] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2140
[2019-04-01 17:47:55,511] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 52.5, 0.0, 26.0, 25.49674184150992, 9.636526253879758, 1.0, 1.0, 19.76505354019724], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4695600.0000, 
sim time next is 4696200.0000, 
raw observation next is [0.0, 92.0, 63.0, 0.0, 26.0, 25.76545526332869, 9.916832311150557, 1.0, 1.0, 18.2534956756608], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.21, 0.0, 1.0, 0.9664936090469557, 0.09916832311150557, 1.0, 1.0, 0.13038211196900573], 
reward next is 0.8696, 
noisyNet noise sample is [array([-1.7038745], dtype=float32), 1.6138924]. 
=============================================
[2019-04-01 17:47:56,157] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2158582: loss 0.0875
[2019-04-01 17:47:56,159] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2158582: learning rate 0.0010
[2019-04-01 17:47:58,851] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135000, global step 2160009: loss 1.2886
[2019-04-01 17:47:58,857] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135000, global step 2160009: learning rate 0.0010
[2019-04-01 17:47:58,955] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2160072: loss 1.1379
[2019-04-01 17:47:58,956] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2160072: learning rate 0.0010
[2019-04-01 17:47:59,247] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.918810e-33 0.000000e+00 0.000000e+00 0.000000e+00 2.008766e-31
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 17:47:59,247] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3294
[2019-04-01 17:47:59,305] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 255.5, 80.5, 26.0, 25.57368461629969, 9.578442185220874, 1.0, 1.0, 7.621887378905676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4543200.0000, 
sim time next is 4543800.0000, 
raw observation next is [3.0, 48.33333333333334, 258.3333333333334, 91.33333333333334, 26.0, 24.83258449825621, 9.107261091527066, 1.0, 1.0, 43.1805900351749], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.48333333333333345, 0.8611111111111114, 0.10092081031307552, 1.0, 0.8332263568937444, 0.09107261091527066, 1.0, 1.0, 0.30843278596553503], 
reward next is 0.6916, 
noisyNet noise sample is [array([1.381864], dtype=float32), -0.9044657]. 
=============================================
[2019-04-01 17:48:00,793] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2160948: loss 0.1202
[2019-04-01 17:48:00,794] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2160949: learning rate 0.0010
[2019-04-01 17:48:01,227] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2161162: loss 0.7084
[2019-04-01 17:48:01,228] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2161162: learning rate 0.0010
[2019-04-01 17:48:01,345] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135000, global step 2161216: loss 1.1414
[2019-04-01 17:48:01,347] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135000, global step 2161217: learning rate 0.0010
[2019-04-01 17:48:01,861] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2161514: loss 1.0279
[2019-04-01 17:48:01,864] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2161515: learning rate 0.0010
[2019-04-01 17:48:02,009] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135000, global step 2161592: loss 0.1786
[2019-04-01 17:48:02,010] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135000, global step 2161592: learning rate 0.0010
[2019-04-01 17:48:02,507] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2161820: loss 0.3192
[2019-04-01 17:48:02,508] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2161820: learning rate 0.0010
[2019-04-01 17:48:03,112] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2162104: loss 0.1094
[2019-04-01 17:48:03,113] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2162104: learning rate 0.0010
[2019-04-01 17:48:03,120] A3C_AGENT_WORKER-Thread-4 INFO:Local step 135500, global step 2162110: loss 0.1023
[2019-04-01 17:48:03,121] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 135500, global step 2162110: learning rate 0.0010
[2019-04-01 17:48:03,290] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135000, global step 2162199: loss 0.0062
[2019-04-01 17:48:03,291] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135000, global step 2162199: learning rate 0.0010
[2019-04-01 17:48:03,554] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:48:03,556] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7048
[2019-04-01 17:48:03,566] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6000000000000001, 62.33333333333334, 0.0, 0.0, 26.0, 25.52284314623039, 9.470671020311896, 0.0, 1.0, 27.21869066734357], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4581600.0000, 
sim time next is 4582200.0000, 
raw observation next is [0.5, 62.66666666666666, 0.0, 0.0, 26.0, 25.50216049810087, 9.198715106832509, 0.0, 1.0, 28.38710387080982], 
processed observation next is [1.0, 0.0, 0.4764542936288089, 0.6266666666666666, 0.0, 0.0, 1.0, 0.9288800711572672, 0.09198715106832508, 0.0, 1.0, 0.20276502764864157], 
reward next is 0.7972, 
noisyNet noise sample is [array([0.05515828], dtype=float32), -0.8500556]. 
=============================================
[2019-04-01 17:48:03,618] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2162391: loss 0.2550
[2019-04-01 17:48:03,620] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2162392: learning rate 0.0010
[2019-04-01 17:48:03,689] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2162436: loss 0.0692
[2019-04-01 17:48:03,690] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2162436: learning rate 0.0010
[2019-04-01 17:48:04,010] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.1161339e-22 1.3229825e-29 1.9026362e-18 3.9098418e-26 5.0547518e-19
 1.0000000e+00 9.0385399e-12], sum to 1.0000
[2019-04-01 17:48:04,014] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2578
[2019-04-01 17:48:04,028] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.9, 61.33333333333334, 0.0, 0.0, 26.0, 25.56312356230266, 9.89526308953017, 0.0, 1.0, 26.75892311000342], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4579800.0000, 
sim time next is 4580400.0000, 
raw observation next is [0.8, 61.66666666666667, 0.0, 0.0, 26.0, 25.53502929694098, 9.757042360951255, 0.0, 1.0, 25.84816568638088], 
processed observation next is [1.0, 0.0, 0.4847645429362882, 0.6166666666666667, 0.0, 0.0, 1.0, 0.9335756138487115, 0.09757042360951255, 0.0, 1.0, 0.18462975490272057], 
reward next is 0.8154, 
noisyNet noise sample is [array([1.0110666], dtype=float32), 0.6399099]. 
=============================================
[2019-04-01 17:48:04,842] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2163073: loss 0.1112
[2019-04-01 17:48:04,845] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2163074: learning rate 0.0010
[2019-04-01 17:48:05,925] A3C_AGENT_WORKER-Thread-5 INFO:Local step 135000, global step 2163599: loss 0.0649
[2019-04-01 17:48:05,929] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 135000, global step 2163602: learning rate 0.0010
[2019-04-01 17:48:11,881] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2166723: loss 0.0235
[2019-04-01 17:48:11,883] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2166723: learning rate 0.0010
[2019-04-01 17:48:12,298] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.0579611e-34], sum to 1.0000
[2019-04-01 17:48:12,299] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7971
[2019-04-01 17:48:12,308] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.9027170573291, 7.567954698902533, 0.0, 1.0, 40.30267278445056], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4757400.0000, 
sim time next is 4758000.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.87314207136227, 7.433488456482124, 0.0, 1.0, 40.26622019755977], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 1.0, 0.8390202959088958, 0.07433488456482124, 0.0, 1.0, 0.28761585855399835], 
reward next is 0.7124, 
noisyNet noise sample is [array([-0.7589955], dtype=float32), -0.25041845]. 
=============================================
[2019-04-01 17:48:12,316] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[70.11416 ]
 [70.42047 ]
 [70.67392 ]
 [70.90449 ]
 [71.071785]], R is [[69.813797  ]
 [69.82778168]
 [69.84106445]
 [69.85343933]
 [69.86508942]].
[2019-04-01 17:48:12,711] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:48:12,712] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0272
[2019-04-01 17:48:12,726] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.88040429916533, 6.483157698989892, 0.0, 1.0, 38.76846036774073], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4852200.0000, 
sim time next is 4852800.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.85493659389357, 6.421079106269029, 0.0, 1.0, 38.75757370463521], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.6, 0.0, 0.0, 1.0, 0.8364195134133671, 0.06421079106269029, 0.0, 1.0, 0.2768398121759658], 
reward next is 0.7232, 
noisyNet noise sample is [array([1.4218888], dtype=float32), -1.009524]. 
=============================================
[2019-04-01 17:48:14,058] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135500, global step 2167784: loss 0.0777
[2019-04-01 17:48:14,059] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135500, global step 2167786: learning rate 0.0010
[2019-04-01 17:48:14,256] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2167883: loss 0.0383
[2019-04-01 17:48:14,259] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2167884: learning rate 0.0010
[2019-04-01 17:48:14,345] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 5.539784e-31], sum to 1.0000
[2019-04-01 17:48:14,345] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8201
[2019-04-01 17:48:14,368] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 81.66666666666667, 0.0, 0.0, 26.0, 25.05084939510355, 8.333794011883839, 0.0, 1.0, 41.09270000983803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4750800.0000, 
sim time next is 4751400.0000, 
raw observation next is [-3.833333333333333, 82.83333333333333, 0.0, 0.0, 26.0, 25.01571669130895, 8.356449520574396, 0.0, 1.0, 41.02934789189647], 
processed observation next is [1.0, 1.0, 0.3564173591874424, 0.8283333333333333, 0.0, 0.0, 1.0, 0.8593880987584213, 0.08356449520574395, 0.0, 1.0, 0.29306677065640335], 
reward next is 0.7069, 
noisyNet noise sample is [array([-0.24873547], dtype=float32), -0.3849725]. 
=============================================
[2019-04-01 17:48:14,648] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 2.4915071e-37 0.0000000e+00 1.4719069e-37
 1.0000000e+00 4.0634495e-20], sum to 1.0000
[2019-04-01 17:48:14,649] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6111
[2019-04-01 17:48:14,665] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.033333333333333, 92.16666666666667, 41.33333333333332, 139.6666666666666, 26.0, 23.75527653144221, 5.400094900419821, 0.0, 1.0, 41.20690092469613], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4780200.0000, 
sim time next is 4780800.0000, 
raw observation next is [-6.0, 92.0, 62.0, 209.5, 26.0, 23.74630292485883, 5.496043090701524, 0.0, 1.0, 40.99172661670006], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.92, 0.20666666666666667, 0.23149171270718233, 1.0, 0.6780432749798331, 0.05496043090701524, 0.0, 1.0, 0.2927980472621433], 
reward next is 0.7072, 
noisyNet noise sample is [array([-0.7734125], dtype=float32), 0.91730845]. 
=============================================
[2019-04-01 17:48:14,810] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:14,810] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:14,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run17
[2019-04-01 17:48:16,482] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135500, global step 2168944: loss 0.3095
[2019-04-01 17:48:16,485] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135500, global step 2168944: learning rate 0.0010
[2019-04-01 17:48:16,485] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2168945: loss 0.3083
[2019-04-01 17:48:16,491] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2168945: learning rate 0.0010
[2019-04-01 17:48:16,970] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:16,970] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:17,002] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run17
[2019-04-01 17:48:17,095] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:17,096] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:17,138] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run17
[2019-04-01 17:48:17,284] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2169314: loss 0.1253
[2019-04-01 17:48:17,286] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2169314: learning rate 0.0010
[2019-04-01 17:48:17,523] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135500, global step 2169434: loss 0.0913
[2019-04-01 17:48:17,525] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135500, global step 2169435: learning rate 0.0010
[2019-04-01 17:48:17,926] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2169607: loss 0.0449
[2019-04-01 17:48:17,939] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2169609: learning rate 0.0010
[2019-04-01 17:48:18,805] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135500, global step 2169966: loss 0.0722
[2019-04-01 17:48:18,806] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135500, global step 2169967: learning rate 0.0010
[2019-04-01 17:48:19,018] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2170052: loss 0.0419
[2019-04-01 17:48:19,020] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2170052: learning rate 0.0010
[2019-04-01 17:48:19,110] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2170084: loss 0.0297
[2019-04-01 17:48:19,111] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2170084: learning rate 0.0010
[2019-04-01 17:48:19,924] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2170424: loss 0.0783
[2019-04-01 17:48:19,925] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2170424: learning rate 0.0010
[2019-04-01 17:48:21,315] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:48:21,316] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9559
[2019-04-01 17:48:21,366] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 253.5, 171.5, 26.0, 25.48140518151719, 8.31759093128934, 0.0, 1.0, 22.37347257649912], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4874400.0000, 
sim time next is 4875000.0000, 
raw observation next is [-1.733333333333333, 58.66666666666666, 269.0, 169.0, 26.0, 25.41263888328987, 8.203872643605322, 0.0, 1.0, 21.11288091722496], 
processed observation next is [0.0, 0.43478260869565216, 0.41458910433979695, 0.5866666666666666, 0.8966666666666666, 0.1867403314917127, 1.0, 0.9160912690414099, 0.08203872643605323, 0.0, 1.0, 0.15080629226589257], 
reward next is 0.8492, 
noisyNet noise sample is [array([-0.4332664], dtype=float32), 0.6782527]. 
=============================================
[2019-04-01 17:48:21,373] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.21591 ]
 [82.695   ]
 [82.31303 ]
 [81.778564]
 [81.34348 ]], R is [[83.75498962]
 [83.75762939]
 [83.75049591]
 [83.73271179]
 [83.7036438 ]].
[2019-04-01 17:48:21,416] A3C_AGENT_WORKER-Thread-5 INFO:Local step 135500, global step 2171086: loss 0.0472
[2019-04-01 17:48:21,420] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 135500, global step 2171087: learning rate 0.0010
[2019-04-01 17:48:25,269] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:25,269] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:25,279] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run17
[2019-04-01 17:48:27,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:27,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:27,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run17
[2019-04-01 17:48:27,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:27,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:27,434] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run17
[2019-04-01 17:48:29,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:29,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:29,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run17
[2019-04-01 17:48:29,795] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:29,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:29,802] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run17
[2019-04-01 17:48:30,236] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:30,236] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:30,243] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run17
[2019-04-01 17:48:30,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:30,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:30,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run17
[2019-04-01 17:48:30,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:30,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:30,774] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run17
[2019-04-01 17:48:31,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:31,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:31,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run17
[2019-04-01 17:48:31,665] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:31,666] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:31,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run17
[2019-04-01 17:48:31,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:31,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:31,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run17
[2019-04-01 17:48:32,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:32,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:32,650] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run17
[2019-04-01 17:48:34,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:48:34,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:48:34,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run17
[2019-04-01 17:48:40,999] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 5.2216739e-35 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.1883907e-12], sum to 1.0000
[2019-04-01 17:48:41,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0708
[2019-04-01 17:48:41,064] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.533333333333333, 86.0, 80.33333333333334, 0.0, 26.0, 24.43077304641658, 6.581975543999325, 0.0, 1.0, 42.31507464046746], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 51600.0000, 
sim time next is 52200.0000, 
raw observation next is [7.45, 86.0, 79.0, 0.0, 26.0, 24.48859244849571, 6.681986729673433, 0.0, 1.0, 37.71142408225333], 
processed observation next is [0.0, 0.6086956521739131, 0.6689750692520776, 0.86, 0.2633333333333333, 0.0, 1.0, 0.784084635499387, 0.06681986729673434, 0.0, 1.0, 0.26936731487323806], 
reward next is 0.7306, 
noisyNet noise sample is [array([0.72876585], dtype=float32), -0.3157704]. 
=============================================
[2019-04-01 17:48:51,186] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8191810e-27 0.0000000e+00 2.7844402e-32 3.3982669e-32 2.3650434e-18
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:48:51,187] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4872
[2019-04-01 17:48:51,242] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.23333333333333, 82.0, 36.66666666666666, 694.5, 26.0, 25.51119216451427, 7.045414766815616, 1.0, 1.0, 59.2042107089153], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 379200.0000, 
sim time next is 379800.0000, 
raw observation next is [-15.05, 78.0, 39.0, 738.0, 26.0, 25.52462425307006, 7.336495523677563, 1.0, 1.0, 60.40093431973803], 
processed observation next is [1.0, 0.391304347826087, 0.0457063711911357, 0.78, 0.13, 0.8154696132596685, 1.0, 0.9320891790100083, 0.07336495523677562, 1.0, 1.0, 0.4314352451409859], 
reward next is 0.5686, 
noisyNet noise sample is [array([-0.14988802], dtype=float32), 0.19749177]. 
=============================================
[2019-04-01 17:48:52,583] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.5480746e-37 0.0000000e+00 2.5904775e-32
 1.0000000e+00 8.5297126e-28], sum to 1.0000
[2019-04-01 17:48:52,583] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5933
[2019-04-01 17:48:52,604] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.65703738299893, 6.700799198803497, 0.0, 1.0, 44.33142597193974], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 195600.0000, 
sim time next is 196200.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.64528296992453, 6.790452152906878, 0.0, 1.0, 44.33203927745968], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 1.0, 0.5207547099892186, 0.06790452152906878, 0.0, 1.0, 0.3166574234104263], 
reward next is 0.6833, 
noisyNet noise sample is [array([-0.8567819], dtype=float32), -1.4337283]. 
=============================================
[2019-04-01 17:48:55,889] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 9.058581e-30], sum to 1.0000
[2019-04-01 17:48:55,889] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7662
[2019-04-01 17:48:55,914] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.399999999999999, 79.5, 0.0, 0.0, 26.0, 24.16883361922463, 5.661067527569803, 0.0, 1.0, 43.75026506231969], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 258600.0000, 
sim time next is 259200.0000, 
raw observation next is [-4.5, 79.0, 0.0, 0.0, 26.0, 24.13109910685991, 5.620442711886231, 0.0, 1.0, 43.75868979795113], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.79, 0.0, 0.0, 1.0, 0.7330141581228443, 0.0562044271188623, 0.0, 1.0, 0.3125620699853652], 
reward next is 0.6874, 
noisyNet noise sample is [array([0.75599664], dtype=float32), 0.8847414]. 
=============================================
[2019-04-01 17:49:00,123] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:49:00,123] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4594
[2019-04-01 17:49:00,142] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.333333333333334, 69.0, 0.0, 0.0, 26.0, 24.12332836932416, 5.528785448913058, 0.0, 1.0, 44.15013571212118], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 262200.0000, 
sim time next is 262800.0000, 
raw observation next is [-6.7, 67.0, 0.0, 0.0, 26.0, 24.12070646607608, 5.506240254478793, 0.0, 1.0, 44.33478500765112], 
processed observation next is [1.0, 0.043478260869565216, 0.2770083102493075, 0.67, 0.0, 0.0, 1.0, 0.7315294951537256, 0.055062402544787926, 0.0, 1.0, 0.3166770357689366], 
reward next is 0.6833, 
noisyNet noise sample is [array([0.99988455], dtype=float32), 0.7550788]. 
=============================================
[2019-04-01 17:49:00,482] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0239252e-30 0.0000000e+00 1.7902354e-24 7.1969546e-33 3.4269481e-23
 1.0000000e+00 1.5366931e-10], sum to 1.0000
[2019-04-01 17:49:00,483] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2036
[2019-04-01 17:49:00,506] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 67.0, 0.0, 0.0, 26.0, 23.73532969613035, 5.365265547432176, 0.0, 1.0, 45.15811259455728], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 270000.0000, 
sim time next is 270600.0000, 
raw observation next is [-9.0, 67.5, 0.0, 0.0, 26.0, 23.64371915971075, 5.408694226685339, 0.0, 1.0, 45.28095853529921], 
processed observation next is [1.0, 0.13043478260869565, 0.21329639889196678, 0.675, 0.0, 0.0, 1.0, 0.6633884513872498, 0.054086942266853386, 0.0, 1.0, 0.3234354181092801], 
reward next is 0.6766, 
noisyNet noise sample is [array([1.4677907], dtype=float32), 1.0990001]. 
=============================================
[2019-04-01 17:49:00,971] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:49:00,971] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9757
[2019-04-01 17:49:01,010] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.683333333333334, 43.16666666666666, 39.33333333333333, 319.0, 26.0, 26.51742097842168, 10.75454888172966, 1.0, 1.0, 23.64239007151143], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 317400.0000, 
sim time next is 318000.0000, 
raw observation next is [-9.866666666666667, 44.33333333333334, 31.66666666666666, 279.5, 26.0, 26.35697659246833, 10.73264610705495, 1.0, 1.0, 22.36779516781477], 
processed observation next is [1.0, 0.6956521739130435, 0.18928901200369344, 0.4433333333333334, 0.10555555555555554, 0.30883977900552484, 1.0, 1.0509966560669046, 0.10732646107054951, 1.0, 1.0, 0.1597699654843912], 
reward next is 0.5472, 
noisyNet noise sample is [array([-0.60514253], dtype=float32), -0.4240053]. 
=============================================
[2019-04-01 17:49:01,013] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[52.71176]
 [52.46737]
 [52.1131 ]
 [51.67943]
 [51.28862]], R is [[52.7890625 ]
 [52.79047775]
 [52.26257324]
 [51.73994827]
 [51.22254944]].
[2019-04-01 17:49:02,931] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.0026879e-36 0.0000000e+00 4.8927875e-35 0.0000000e+00 1.6783435e-37
 1.0000000e+00 4.7575451e-35], sum to 1.0000
[2019-04-01 17:49:02,931] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3201
[2019-04-01 17:49:02,965] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.65, 70.0, 0.0, 0.0, 26.0, 24.58315202796645, 6.396192134751594, 0.0, 1.0, 43.36028800545726], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 250200.0000, 
sim time next is 250800.0000, 
raw observation next is [-3.733333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 24.54629425228074, 6.319118672183552, 0.0, 1.0, 43.61551987891414], 
processed observation next is [1.0, 0.9130434782608695, 0.35918744228993543, 0.7166666666666667, 0.0, 0.0, 1.0, 0.7923277503258203, 0.06319118672183552, 0.0, 1.0, 0.3115394277065296], 
reward next is 0.6885, 
noisyNet noise sample is [array([-0.01068732], dtype=float32), -0.43005934]. 
=============================================
[2019-04-01 17:49:06,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:49:06,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1807
[2019-04-01 17:49:06,654] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.9, 66.66666666666666, 0.0, 0.0, 26.0, 23.64645651609342, 5.602430363649607, 0.0, 1.0, 46.60171172369187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 345000.0000, 
sim time next is 345600.0000, 
raw observation next is [-13.9, 66.0, 0.0, 0.0, 26.0, 23.58654702115108, 5.623595934246545, 0.0, 1.0, 46.67780395117834], 
processed observation next is [1.0, 0.0, 0.07756232686980608, 0.66, 0.0, 0.0, 1.0, 0.6552210030215829, 0.05623595934246545, 0.0, 1.0, 0.3334128853655596], 
reward next is 0.6666, 
noisyNet noise sample is [array([-0.11109868], dtype=float32), 0.1493834]. 
=============================================
[2019-04-01 17:49:17,930] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:49:17,932] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6351
[2019-04-01 17:49:17,990] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.466666666666667, 34.16666666666666, 36.0, 0.0, 26.0, 25.28440660772389, 6.115934338138007, 1.0, 1.0, 40.1057832684437], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 463800.0000, 
sim time next is 464400.0000, 
raw observation next is [-6.2, 33.0, 42.5, 0.0, 26.0, 25.29676309759889, 6.138542004393773, 1.0, 1.0, 37.67082996735571], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.33, 0.14166666666666666, 0.0, 1.0, 0.8995375853712702, 0.061385420043937725, 1.0, 1.0, 0.26907735690968365], 
reward next is 0.7309, 
noisyNet noise sample is [array([-1.1641303], dtype=float32), 0.14714338]. 
=============================================
[2019-04-01 17:49:20,876] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:49:20,878] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7237
[2019-04-01 17:49:20,891] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 89.0, 0.0, 0.0, 26.0, 24.92490041909697, 6.893461438343297, 0.0, 1.0, 39.61157406000461], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 522000.0000, 
sim time next is 522600.0000, 
raw observation next is [4.883333333333334, 88.83333333333334, 0.0, 0.0, 26.0, 25.04863112171326, 6.937317871490509, 0.0, 1.0, 38.71255858728424], 
processed observation next is [0.0, 0.043478260869565216, 0.597876269621422, 0.8883333333333334, 0.0, 0.0, 1.0, 0.8640901602447515, 0.06937317871490509, 0.0, 1.0, 0.2765182756234589], 
reward next is 0.7235, 
noisyNet noise sample is [array([1.865999], dtype=float32), 2.8085258]. 
=============================================
[2019-04-01 17:49:24,091] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9933102e-27 2.5768456e-33 4.1349386e-21 8.2166536e-30 1.4181400e-15
 1.0000000e+00 2.6265109e-15], sum to 1.0000
[2019-04-01 17:49:24,091] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8167
[2019-04-01 17:49:24,130] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 83.66666666666667, 81.0, 139.0, 26.0, 24.97771686538387, 7.212433366350841, 0.0, 1.0, 27.59332585362488], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 557400.0000, 
sim time next is 558000.0000, 
raw observation next is [-0.6, 83.0, 83.0, 138.0, 26.0, 24.92902534882056, 7.094906191078524, 0.0, 1.0, 26.33082025496695], 
processed observation next is [0.0, 0.4782608695652174, 0.44598337950138506, 0.83, 0.27666666666666667, 0.15248618784530388, 1.0, 0.8470036212600799, 0.07094906191078523, 0.0, 1.0, 0.18807728753547823], 
reward next is 0.8119, 
noisyNet noise sample is [array([1.9480804], dtype=float32), -1.1489955]. 
=============================================
[2019-04-01 17:49:24,133] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[70.14493 ]
 [70.10907 ]
 [70.20006 ]
 [70.50197 ]
 [70.735405]], R is [[70.33898163]
 [70.43849182]
 [70.51329803]
 [70.55060577]
 [70.57100677]].
[2019-04-01 17:49:29,335] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:49:29,335] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6465
[2019-04-01 17:49:29,389] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 26.316112202981, 10.71540631194812, 1.0, 1.0, 30.7947106853804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 839400.0000, 
sim time next is 840000.0000, 
raw observation next is [-3.9, 84.66666666666667, 0.0, 0.0, 26.0, 26.39356590335213, 9.863495826896282, 1.0, 1.0, 29.38500018078838], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8466666666666667, 0.0, 0.0, 1.0, 1.0562237004788757, 0.09863495826896282, 1.0, 1.0, 0.20989285843420272], 
reward next is 0.7901, 
noisyNet noise sample is [array([-1.0443151], dtype=float32), -0.08527567]. 
=============================================
[2019-04-01 17:49:29,392] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[62.027866]
 [62.40145 ]
 [62.281307]
 [62.171406]
 [62.635338]], R is [[62.00739288]
 [61.88119125]
 [61.81496429]
 [61.87998581]
 [61.68486023]].
[2019-04-01 17:49:30,788] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4755198e-35 0.0000000e+00 2.9756673e-32 0.0000000e+00 1.8318596e-37
 4.3513093e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 17:49:30,788] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0795
[2019-04-01 17:49:30,828] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.99786786856043, 7.597125103171474, 0.0, 1.0, 44.47338851951497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 588600.0000, 
sim time next is 589200.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 25.04593012763157, 7.602546629172752, 0.0, 1.0, 43.47568980473503], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 1.0, 0.8637043039473673, 0.07602546629172752, 0.0, 1.0, 0.3105406414623931], 
reward next is 0.6895, 
noisyNet noise sample is [array([-1.5032681], dtype=float32), -0.61848307]. 
=============================================
[2019-04-01 17:49:33,150] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:49:33,150] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2653
[2019-04-01 17:49:33,170] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 84.0, 67.33333333333333, 0.0, 26.0, 25.49510405844939, 7.288618664695488, 1.0, 1.0, 14.85987882833171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 900600.0000, 
sim time next is 901200.0000, 
raw observation next is [1.1, 84.0, 72.16666666666667, 0.0, 26.0, 25.52793302758882, 7.333201831582701, 1.0, 1.0, 14.17530556040976], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.24055555555555558, 0.0, 1.0, 0.932561861084117, 0.07333201831582702, 1.0, 1.0, 0.10125218257435542], 
reward next is 0.8987, 
noisyNet noise sample is [array([-0.5984181], dtype=float32), 1.0612491]. 
=============================================
[2019-04-01 17:49:43,854] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:49:43,855] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0935
[2019-04-01 17:49:43,902] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 14.5, 0.0, 26.0, 25.00009534008347, 6.431867438575679, 1.0, 1.0, 28.4756519851101], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 892800.0000, 
sim time next is 893400.0000, 
raw observation next is [0.1833333333333333, 73.33333333333333, 19.33333333333334, 0.0, 26.0, 24.92283115790691, 6.356988325157114, 1.0, 1.0, 26.86951886010678], 
processed observation next is [1.0, 0.34782608695652173, 0.46768236380424755, 0.7333333333333333, 0.06444444444444447, 0.0, 1.0, 0.8461187368438442, 0.06356988325157115, 1.0, 1.0, 0.19192513471504843], 
reward next is 0.8081, 
noisyNet noise sample is [array([-0.23831607], dtype=float32), 0.08411271]. 
=============================================
[2019-04-01 17:49:44,946] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-01 17:49:44,947] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 17:49:44,947] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:49:44,948] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 17:49:44,949] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 17:49:44,949] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:49:44,950] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:49:44,951] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run23
[2019-04-01 17:49:44,983] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run23
[2019-04-01 17:49:45,003] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run23
[2019-04-01 17:51:05,499] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.5084163], dtype=float32), -0.73974395]
[2019-04-01 17:51:05,499] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.403343962666667, 80.09609095333334, 0.0, 0.0, 26.0, 24.51237938493338, 5.793281893440408, 0.0, 1.0, 55.18250216808021]
[2019-04-01 17:51:05,499] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 17:51:05,501] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.3310329e-34], sampled 0.496326544114004
[2019-04-01 17:51:13,702] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.5084163], dtype=float32), -0.73974395]
[2019-04-01 17:51:13,703] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.8140815375, 96.37400167, 0.0, 0.0, 26.0, 25.18099521280476, 9.14126682173069, 0.0, 1.0, 40.24581662520811]
[2019-04-01 17:51:13,703] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 17:51:13,703] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 8.837243e-36], sampled 0.46930717987321857
[2019-04-01 17:51:26,710] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 17:51:46,097] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 17:51:50,018] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 17:51:51,041] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2200000, evaluation results [2200000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 17:51:53,918] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3579157e-36 0.0000000e+00 3.2898564e-33 0.0000000e+00 2.1811234e-27
 1.0000000e+00 4.1599846e-28], sum to 1.0000
[2019-04-01 17:51:53,919] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8886
[2019-04-01 17:51:53,952] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 74.66666666666667, 0.0, 0.0, 26.0, 24.65455788678413, 5.840533619688, 0.0, 1.0, 38.59840289872244], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 879600.0000, 
sim time next is 880200.0000, 
raw observation next is [-0.8999999999999999, 74.0, 0.0, 0.0, 26.0, 24.58503118987099, 5.787652208283849, 0.0, 1.0, 38.6222371117766], 
processed observation next is [1.0, 0.17391304347826086, 0.43767313019390586, 0.74, 0.0, 0.0, 1.0, 0.7978615985529984, 0.05787652208283849, 0.0, 1.0, 0.2758731222269757], 
reward next is 0.7241, 
noisyNet noise sample is [array([-2.6514664], dtype=float32), -1.0163345]. 
=============================================
[2019-04-01 17:52:01,408] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 6.9804575e-09], sum to 1.0000
[2019-04-01 17:52:01,416] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3182
[2019-04-01 17:52:01,421] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.51666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 24.54148286964565, 8.1606801965441, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1199400.0000, 
sim time next is 1200000.0000, 
raw observation next is [17.33333333333334, 69.66666666666667, 0.0, 0.0, 26.0, 24.51643886539405, 8.077494228776876, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.9427516158818101, 0.6966666666666668, 0.0, 0.0, 1.0, 0.7880626950562929, 0.08077494228776877, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4640168], dtype=float32), 0.45999065]. 
=============================================
[2019-04-01 17:52:01,426] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[107.87167 ]
 [107.88037 ]
 [107.82831 ]
 [107.82528 ]
 [107.815384]], R is [[107.71874237]
 [107.64155579]
 [107.56513977]
 [107.48948669]
 [107.41459656]].
[2019-04-01 17:52:03,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.3459161e-27], sum to 1.0000
[2019-04-01 17:52:03,762] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8350
[2019-04-01 17:52:03,775] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.0, 67.66666666666667, 0.0, 0.0, 26.0, 25.73880725016269, 13.98796388303996, 0.0, 1.0, 12.16786048457208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1120800.0000, 
sim time next is 1121400.0000, 
raw observation next is [11.9, 68.5, 0.0, 0.0, 26.0, 25.67901657065765, 13.64923901481661, 0.0, 1.0, 11.67411143322617], 
processed observation next is [1.0, 1.0, 0.7922437673130196, 0.685, 0.0, 0.0, 1.0, 0.9541452243796645, 0.1364923901481661, 0.0, 1.0, 0.08338651023732978], 
reward next is 0.9166, 
noisyNet noise sample is [array([1.6214582], dtype=float32), 0.53167737]. 
=============================================
[2019-04-01 17:52:05,134] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6370368e-34 0.0000000e+00 2.6253302e-29 0.0000000e+00 4.9834446e-22
 1.0000000e+00 6.1185511e-22], sum to 1.0000
[2019-04-01 17:52:05,138] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7418
[2019-04-01 17:52:05,159] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.55067566316252, 12.12245314088702, 0.0, 1.0, 32.84678511740476], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1558200.0000, 
sim time next is 1558800.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.66108648959389, 11.96054685111484, 0.0, 1.0, 25.97342940205199], 
processed observation next is [1.0, 0.043478260869565216, 0.6011080332409973, 0.82, 0.0, 0.0, 1.0, 0.9515837842276988, 0.1196054685111484, 0.0, 1.0, 0.1855244957289428], 
reward next is 0.8145, 
noisyNet noise sample is [array([0.46197116], dtype=float32), -0.7548798]. 
=============================================
[2019-04-01 17:52:06,590] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:52:06,592] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1275
[2019-04-01 17:52:06,604] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.98333333333333, 49.83333333333334, 23.66666666666667, 0.9999999999999998, 26.0, 27.93854826575488, 27.36327180133293, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1097400.0000, 
sim time next is 1098000.0000, 
raw observation next is [17.7, 50.0, 18.0, 1.5, 26.0, 27.95228876823864, 23.53116063740122, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.9529085872576178, 0.5, 0.06, 0.0016574585635359116, 1.0, 1.2788983954626627, 0.23531160637401222, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.02972466], dtype=float32), -1.0740803]. 
=============================================
[2019-04-01 17:52:06,621] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[74.12956 ]
 [73.841545]
 [73.543365]
 [73.219475]
 [72.99941 ]], R is [[73.70527649]
 [72.96822357]
 [72.23854065]
 [71.51615906]
 [70.80099487]].
[2019-04-01 17:52:14,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6667197e-32
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:52:14,986] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2882
[2019-04-01 17:52:14,997] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.283333333333333, 99.33333333333334, 0.0, 0.0, 26.0, 25.57850308102491, 9.237697962489925, 0.0, 1.0, 29.50334368690164], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1493400.0000, 
sim time next is 1494000.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 26.0, 25.44883556905609, 8.743971578137328, 0.0, 1.0, 32.11114292561384], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 1.0, 0.92126222415087, 0.08743971578137329, 0.0, 1.0, 0.22936530661152743], 
reward next is 0.7706, 
noisyNet noise sample is [array([-0.8569602], dtype=float32), 0.24739794]. 
=============================================
[2019-04-01 17:52:15,009] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[71.031906]
 [71.1141  ]
 [71.16975 ]
 [71.249146]
 [71.34306 ]], R is [[71.00125885]
 [71.08050537]
 [71.15391541]
 [71.18083954]
 [71.20691681]].
[2019-04-01 17:52:15,839] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7671141e-30 0.0000000e+00 4.2089604e-34 0.0000000e+00 8.1384717e-32
 1.0000000e+00 3.5886089e-35], sum to 1.0000
[2019-04-01 17:52:15,840] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3559
[2019-04-01 17:52:15,857] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.29148405908735, 9.177237455695073, 0.0, 1.0, 36.54805886783322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1485600.0000, 
sim time next is 1486200.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.28754122732929, 9.243308456120767, 0.0, 1.0, 36.42585685230133], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 1.0, 0.8982201753327556, 0.09243308456120766, 0.0, 1.0, 0.26018469180215237], 
reward next is 0.7398, 
noisyNet noise sample is [array([1.0428065], dtype=float32), 1.9838704]. 
=============================================
[2019-04-01 17:52:18,266] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.5419047e-36 0.0000000e+00 9.1765371e-34
 1.0000000e+00 1.6727141e-20], sum to 1.0000
[2019-04-01 17:52:18,267] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7615
[2019-04-01 17:52:18,311] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.1, 82.33333333333334, 0.0, 0.0, 26.0, 25.06333575234983, 8.261974707368195, 0.0, 1.0, 40.97242895419295], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1794000.0000, 
sim time next is 1794600.0000, 
raw observation next is [-4.2, 82.5, 0.0, 0.0, 26.0, 25.06867904586359, 8.142799285874169, 0.0, 1.0, 38.93274902100025], 
processed observation next is [0.0, 0.782608695652174, 0.34626038781163443, 0.825, 0.0, 0.0, 1.0, 0.8669541494090842, 0.0814279928587417, 0.0, 1.0, 0.2780910644357161], 
reward next is 0.7219, 
noisyNet noise sample is [array([-0.45341858], dtype=float32), -1.1606674]. 
=============================================
[2019-04-01 17:52:21,394] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:52:21,395] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9481
[2019-04-01 17:52:21,412] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.65, 98.0, 0.0, 0.0, 26.0, 25.4450909191235, 9.678567477889485, 0.0, 1.0, 36.61506233613959], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1492200.0000, 
sim time next is 1492800.0000, 
raw observation next is [1.466666666666667, 98.66666666666666, 0.0, 0.0, 26.0, 25.50375468031714, 9.895539788211748, 0.0, 1.0, 30.2096216546212], 
processed observation next is [1.0, 0.2608695652173913, 0.5032317636195753, 0.9866666666666666, 0.0, 0.0, 1.0, 0.9291078114738772, 0.09895539788211748, 0.0, 1.0, 0.21578301181872284], 
reward next is 0.7842, 
noisyNet noise sample is [array([-0.868493], dtype=float32), -0.99662995]. 
=============================================
[2019-04-01 17:52:26,187] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.5787794e-37 0.0000000e+00 5.6479186e-30
 3.6178458e-01 6.3821542e-01], sum to 1.0000
[2019-04-01 17:52:26,187] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1319
[2019-04-01 17:52:26,204] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.86602685332972, 7.73790032046183, 0.0, 1.0, 43.18243702217481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1748400.0000, 
sim time next is 1749000.0000, 
raw observation next is [-1.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.84044612764454, 7.643561741516291, 0.0, 1.0, 43.24834226147474], 
processed observation next is [0.0, 0.21739130434782608, 0.4321329639889197, 0.8633333333333333, 0.0, 0.0, 1.0, 0.8343494468063629, 0.07643561741516292, 0.0, 1.0, 0.3089167304391053], 
reward next is 0.6911, 
noisyNet noise sample is [array([-1.3443217], dtype=float32), -1.086963]. 
=============================================
[2019-04-01 17:52:26,209] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[65.39779]
 [65.39106]
 [65.3776 ]
 [65.38747]
 [65.37146]], R is [[65.43686676]
 [65.47405243]
 [65.51137543]
 [65.54843903]
 [65.58539581]].
[2019-04-01 17:52:26,401] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6275752e-36
 1.0000000e+00 5.8243924e-37], sum to 1.0000
[2019-04-01 17:52:26,403] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7917
[2019-04-01 17:52:26,418] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 87.0, 0.0, 0.0, 26.0, 24.65835270124473, 7.164900434709705, 0.0, 1.0, 43.4768308575269], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1753200.0000, 
sim time next is 1753800.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 26.0, 24.65352930171908, 7.10492370676193, 0.0, 1.0, 43.47978136029199], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 1.0, 0.8076470431027255, 0.0710492370676193, 0.0, 1.0, 0.3105698668592285], 
reward next is 0.6894, 
noisyNet noise sample is [array([0.5951933], dtype=float32), -1.3827256]. 
=============================================
[2019-04-01 17:52:26,601] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:52:26,603] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2344
[2019-04-01 17:52:26,622] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1666666666666667, 91.00000000000001, 0.0, 0.0, 26.0, 25.17275915081858, 9.123168742938754, 0.0, 1.0, 42.39817286803166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1735800.0000, 
sim time next is 1736400.0000, 
raw observation next is [0.1333333333333334, 91.0, 0.0, 0.0, 26.0, 25.15826562606866, 9.081733360525943, 0.0, 1.0, 42.41079675690181], 
processed observation next is [0.0, 0.08695652173913043, 0.46629732225300097, 0.91, 0.0, 0.0, 1.0, 0.8797522322955226, 0.09081733360525943, 0.0, 1.0, 0.3029342625492986], 
reward next is 0.6971, 
noisyNet noise sample is [array([-0.33393806], dtype=float32), -0.6805193]. 
=============================================
[2019-04-01 17:52:28,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1469696e-37
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:52:28,101] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8467
[2019-04-01 17:52:28,118] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.800000000000001, 94.0, 0.0, 0.0, 26.0, 25.5565191542844, 11.73225625748032, 0.0, 1.0, 27.88867060875675], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1646400.0000, 
sim time next is 1647000.0000, 
raw observation next is [6.9, 94.5, 0.0, 0.0, 26.0, 25.53909941431048, 12.43557786632235, 0.0, 1.0, 33.38244358673744], 
processed observation next is [1.0, 0.043478260869565216, 0.6537396121883658, 0.945, 0.0, 0.0, 1.0, 0.9341570591872116, 0.12435577866322349, 0.0, 1.0, 0.23844602561955314], 
reward next is 0.7616, 
noisyNet noise sample is [array([1.0235226], dtype=float32), -1.1984708]. 
=============================================
[2019-04-01 17:52:28,131] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.92546]
 [71.83052]
 [71.78387]
 [71.72014]
 [71.77278]], R is [[71.98573303]
 [72.06667328]
 [72.17447662]
 [72.33036041]
 [72.47774506]].
[2019-04-01 17:52:30,183] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:52:30,183] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6996
[2019-04-01 17:52:30,237] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.166666666666666, 76.33333333333334, 181.1666666666667, 198.3333333333333, 26.0, 25.8490448988046, 7.865669487190028, 1.0, 1.0, 23.22727971900423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1939200.0000, 
sim time next is 1939800.0000, 
raw observation next is [-5.883333333333333, 75.66666666666666, 191.3333333333333, 160.6666666666667, 26.0, 25.8006270093649, 7.796322959740692, 1.0, 1.0, 22.08151982010872], 
processed observation next is [1.0, 0.43478260869565216, 0.2996306555863343, 0.7566666666666666, 0.6377777777777777, 0.1775322283609577, 1.0, 0.9715181441949855, 0.07796322959740692, 1.0, 1.0, 0.15772514157220516], 
reward next is 0.8423, 
noisyNet noise sample is [array([-0.19729877], dtype=float32), -0.008269694]. 
=============================================
[2019-04-01 17:52:42,794] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 2.9866697e-36 0.0000000e+00 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:52:42,795] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7247
[2019-04-01 17:52:42,870] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 81.0, 148.0, 56.00000000000001, 26.0, 24.95156275225551, 7.524861398234418, 0.0, 1.0, 47.24994895822595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1867800.0000, 
sim time next is 1868400.0000, 
raw observation next is [-4.5, 83.0, 129.0, 42.0, 26.0, 24.98788807105153, 7.710475116410247, 0.0, 1.0, 47.82087408034723], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.83, 0.43, 0.04640883977900553, 1.0, 0.8554125815787899, 0.07710475116410248, 0.0, 1.0, 0.34157767200248024], 
reward next is 0.6584, 
noisyNet noise sample is [array([-0.17253257], dtype=float32), 1.5609076]. 
=============================================
[2019-04-01 17:52:46,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9126527e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:52:46,514] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2280
[2019-04-01 17:52:46,552] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.483333333333333, 62.5, 128.6666666666667, 0.0, 26.0, 25.69079089741074, 8.005816875754983, 1.0, 1.0, 26.56255836336614], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1950600.0000, 
sim time next is 1951200.0000, 
raw observation next is [-3.4, 62.0, 124.5, 0.0, 26.0, 25.741843380296, 8.04731323334993, 1.0, 1.0, 26.37728749522582], 
processed observation next is [1.0, 0.6086956521739131, 0.368421052631579, 0.62, 0.415, 0.0, 1.0, 0.9631204828994286, 0.0804731323334993, 1.0, 1.0, 0.18840919639447012], 
reward next is 0.8116, 
noisyNet noise sample is [array([-0.05901426], dtype=float32), -0.7916417]. 
=============================================
[2019-04-01 17:52:49,599] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:52:49,600] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4443
[2019-04-01 17:52:49,612] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.100000000000001, 86.33333333333333, 0.0, 0.0, 26.0, 24.31321143950779, 5.845186349982169, 0.0, 1.0, 41.31353321432303], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1991400.0000, 
sim time next is 1992000.0000, 
raw observation next is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.37222364046249, 5.821513751990442, 0.0, 1.0, 41.2051471045114], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.8566666666666667, 0.0, 0.0, 1.0, 0.76746052006607, 0.05821513751990442, 0.0, 1.0, 0.29432247931793853], 
reward next is 0.7057, 
noisyNet noise sample is [array([0.13712752], dtype=float32), 0.15128212]. 
=============================================
[2019-04-01 17:52:49,616] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[71.913216]
 [72.05288 ]
 [72.1433  ]
 [72.13942 ]
 [72.01773 ]], R is [[71.77970886]
 [71.76681519]
 [71.75348663]
 [71.73960114]
 [71.72515869]].
[2019-04-01 17:52:52,086] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:52:52,087] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2779
[2019-04-01 17:52:52,135] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 82.0, 0.0, 0.0, 26.0, 25.23837221421457, 8.183753304265458, 1.0, 1.0, 29.61572140815081], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2050800.0000, 
sim time next is 2051400.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.14401175820341, 8.511877544861782, 1.0, 1.0, 37.26872629079021], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.0, 0.0, 1.0, 0.8777159654576298, 0.08511877544861783, 1.0, 1.0, 0.26620518779135866], 
reward next is 0.7338, 
noisyNet noise sample is [array([-0.8329675], dtype=float32), -0.66869396]. 
=============================================
[2019-04-01 17:52:55,521] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:52:55,525] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6633
[2019-04-01 17:52:55,596] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.1, 68.0, 112.0, 0.0, 26.0, 26.52863520479462, 12.5572254383866, 1.0, 1.0, 31.73981527974722], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2127000.0000, 
sim time next is 2127600.0000, 
raw observation next is [-5.0, 68.0, 105.5, 0.0, 26.0, 26.67476329266833, 12.83374422084597, 1.0, 1.0, 28.11607062513973], 
processed observation next is [1.0, 0.6521739130434783, 0.32409972299168976, 0.68, 0.3516666666666667, 0.0, 1.0, 1.0963947560954759, 0.1283374422084597, 1.0, 1.0, 0.2008290758938552], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8165726], dtype=float32), -0.14114283]. 
=============================================
[2019-04-01 17:53:03,255] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 4.618263e-36
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 17:53:03,258] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4262
[2019-04-01 17:53:03,282] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.800000000000001, 78.16666666666666, 0.0, 0.0, 26.0, 24.21721687165901, 5.723885934620587, 0.0, 1.0, 42.09838174507536], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2166600.0000, 
sim time next is 2167200.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.24236078714326, 5.777420183902071, 0.0, 1.0, 42.07177318969975], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 1.0, 0.7489086838776083, 0.057774201839020704, 0.0, 1.0, 0.3005126656407125], 
reward next is 0.6995, 
noisyNet noise sample is [array([-0.41340908], dtype=float32), -1.0698216]. 
=============================================
[2019-04-01 17:53:08,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1533592e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.1958571e-27
 1.0000000e+00 1.1816602e-36], sum to 1.0000
[2019-04-01 17:53:08,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7123
[2019-04-01 17:53:08,838] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.133333333333333, 89.66666666666667, 38.0, 16.66666666666667, 26.0, 25.24462304630725, 7.028638655063053, 1.0, 1.0, 44.94606647693164], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2276400.0000, 
sim time next is 2277000.0000, 
raw observation next is [-8.95, 89.0, 45.0, 16.0, 26.0, 25.38404248376433, 7.191102442895975, 1.0, 1.0, 38.81229084109179], 
processed observation next is [1.0, 0.34782608695652173, 0.21468144044321333, 0.89, 0.15, 0.017679558011049725, 1.0, 0.9120060691091899, 0.07191102442895975, 1.0, 1.0, 0.27723064886494136], 
reward next is 0.7228, 
noisyNet noise sample is [array([-0.3718798], dtype=float32), 0.26893076]. 
=============================================
[2019-04-01 17:53:08,840] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[67.09866]
 [66.32497]
 [65.26736]
 [64.3508 ]
 [63.74054]], R is [[67.55660248]
 [67.55998993]
 [67.56652832]
 [67.55316162]
 [67.52050018]].
[2019-04-01 17:53:16,794] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:53:16,795] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5563
[2019-04-01 17:53:16,842] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.466666666666667, 63.0, 143.6666666666667, 426.0, 26.0, 24.9524570980571, 7.22458112525305, 0.0, 1.0, 29.84478982452314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2371200.0000, 
sim time next is 2371800.0000, 
raw observation next is [-2.383333333333333, 62.5, 148.3333333333333, 402.0, 26.0, 24.88275887865181, 7.25972685472811, 0.0, 1.0, 37.20614770600758], 
processed observation next is [0.0, 0.43478260869565216, 0.3965835641735919, 0.625, 0.4944444444444443, 0.44419889502762433, 1.0, 0.8403941255216871, 0.0725972685472811, 0.0, 1.0, 0.2657581979000541], 
reward next is 0.7342, 
noisyNet noise sample is [array([-0.9732975], dtype=float32), 2.6588042]. 
=============================================
[2019-04-01 17:53:19,076] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:53:19,077] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6301
[2019-04-01 17:53:19,122] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 75.0, 85.0, 45.5, 26.0, 25.72575272148649, 7.568281422012499, 1.0, 1.0, 33.31923199856106], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2624400.0000, 
sim time next is 2625000.0000, 
raw observation next is [-6.416666666666667, 73.33333333333333, 87.66666666666667, 60.66666666666667, 26.0, 25.66543752857378, 7.533916502876608, 1.0, 1.0, 28.68545074839889], 
processed observation next is [1.0, 0.391304347826087, 0.2848568790397045, 0.7333333333333333, 0.2922222222222222, 0.06703499079189687, 1.0, 0.9522053612248256, 0.07533916502876609, 1.0, 1.0, 0.2048960767742778], 
reward next is 0.7951, 
noisyNet noise sample is [array([-1.1350173], dtype=float32), -0.39409846]. 
=============================================
[2019-04-01 17:53:19,136] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[73.16353]
 [73.00021]
 [72.65171]
 [72.02055]
 [71.52205]], R is [[73.40930176]
 [73.43721771]
 [73.48066711]
 [73.50884247]
 [73.5030899 ]].
[2019-04-01 17:53:23,878] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.         0.         0.         0.         0.         0.99271303
 0.00728692], sum to 1.0000
[2019-04-01 17:53:23,879] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8180
[2019-04-01 17:53:23,907] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 59.5, 0.0, 0.0, 26.0, 22.88318564486316, 6.367718742128152, 0.0, 1.0, 43.63797915467949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2446200.0000, 
sim time next is 2446800.0000, 
raw observation next is [-9.5, 59.0, 9.166666666666664, 102.6666666666667, 26.0, 22.84470495864452, 6.35126490898779, 0.0, 1.0, 43.60776701423212], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.59, 0.030555555555555548, 0.11344383057090243, 1.0, 0.5492435655206458, 0.0635126490898779, 0.0, 1.0, 0.31148405010165797], 
reward next is 0.6885, 
noisyNet noise sample is [array([1.4590893], dtype=float32), -1.1794503]. 
=============================================
[2019-04-01 17:53:33,767] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.22872913e-26 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.29513325e-30 1.00000000e+00 0.00000000e+00], sum to 1.0000
[2019-04-01 17:53:33,767] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2068
[2019-04-01 17:53:33,784] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.633333333333333, 60.66666666666667, 200.0, 205.6666666666667, 26.0, 25.71798042848701, 8.15611923917261, 1.0, 1.0, 16.4462206025087], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2632200.0000, 
sim time next is 2632800.0000, 
raw observation next is [-3.366666666666667, 59.33333333333334, 212.0, 188.3333333333333, 26.0, 25.68397471983064, 8.162103275152637, 1.0, 1.0, 15.51840373041091], 
processed observation next is [1.0, 0.4782608695652174, 0.36934441366574333, 0.5933333333333334, 0.7066666666666667, 0.2081031307550644, 1.0, 0.9548535314043772, 0.08162103275152637, 1.0, 1.0, 0.1108457409315065], 
reward next is 0.8892, 
noisyNet noise sample is [array([-0.91260266], dtype=float32), -0.5770983]. 
=============================================
[2019-04-01 17:53:36,040] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.2295358e-20], sum to 1.0000
[2019-04-01 17:53:36,041] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3489
[2019-04-01 17:53:36,059] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.333333333333334, 70.16666666666667, 0.0, 0.0, 26.0, 24.54828294048647, 6.257490471693127, 0.0, 1.0, 43.76110531404102], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2682600.0000, 
sim time next is 2683200.0000, 
raw observation next is [-9.666666666666668, 71.33333333333334, 0.0, 0.0, 26.0, 24.47902396433921, 6.095749863723789, 0.0, 1.0, 43.80443311247051], 
processed observation next is [1.0, 0.043478260869565216, 0.19482917820867957, 0.7133333333333334, 0.0, 0.0, 1.0, 0.7827177091913156, 0.06095749863723789, 0.0, 1.0, 0.31288880794621793], 
reward next is 0.6871, 
noisyNet noise sample is [array([-0.272775], dtype=float32), -0.5672765]. 
=============================================
[2019-04-01 17:53:37,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:53:37,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1950
[2019-04-01 17:53:37,840] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.666666666666667, 71.0, 0.0, 0.0, 26.0, 24.61847922346226, 6.645318534434483, 0.0, 1.0, 43.8166826301581], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2679600.0000, 
sim time next is 2680200.0000, 
raw observation next is [-8.0, 70.5, 0.0, 0.0, 26.0, 24.63282278076876, 6.535510464089579, 0.0, 1.0, 43.78761480262857], 
processed observation next is [1.0, 0.0, 0.24099722991689754, 0.705, 0.0, 0.0, 1.0, 0.8046889686812513, 0.06535510464089579, 0.0, 1.0, 0.31276867716163265], 
reward next is 0.6872, 
noisyNet noise sample is [array([0.5696737], dtype=float32), -0.26318067]. 
=============================================
[2019-04-01 17:53:44,419] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:53:44,419] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3547
[2019-04-01 17:53:44,441] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 63.16666666666667, 0.0, 0.0, 26.0, 25.45755745338738, 9.692619002273076, 0.0, 1.0, 45.55650639015748], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2754600.0000, 
sim time next is 2755200.0000, 
raw observation next is [-6.0, 62.33333333333334, 0.0, 0.0, 26.0, 25.39229845934013, 9.53671240277919, 0.0, 1.0, 45.2981102705596], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.6233333333333334, 0.0, 0.0, 1.0, 0.913185494191447, 0.09536712402779189, 0.0, 1.0, 0.3235579305039971], 
reward next is 0.6764, 
noisyNet noise sample is [array([-0.18037473], dtype=float32), 0.60091686]. 
=============================================
[2019-04-01 17:53:47,976] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2415176e-24 0.0000000e+00 2.7835395e-38 3.3841516e-33 3.2408229e-37
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:53:47,977] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3304
[2019-04-01 17:53:48,013] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 93.66666666666667, 562.0, 26.0, 26.05697527592036, 10.64557116398557, 1.0, 1.0, 17.58083272446558], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3143400.0000, 
sim time next is 3144000.0000, 
raw observation next is [7.0, 100.0, 96.33333333333333, 604.5, 26.0, 26.15403940492554, 11.02030557643557, 1.0, 1.0, 16.17437127617407], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.32111111111111107, 0.6679558011049723, 1.0, 1.0220056292750772, 0.1102030557643557, 1.0, 1.0, 0.11553122340124336], 
reward next is 0.4763, 
noisyNet noise sample is [array([-1.0195254], dtype=float32), 1.0697778]. 
=============================================
[2019-04-01 17:53:48,018] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.99819 ]
 [71.93633 ]
 [71.87769 ]
 [71.921455]
 [71.84457 ]], R is [[71.87928772]
 [71.77668762]
 [71.90761566]
 [72.04158783]
 [72.17362976]].
[2019-04-01 17:53:50,738] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:53:50,739] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9387
[2019-04-01 17:53:50,764] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 53.0, 0.0, 0.0, 26.0, 25.36762507591261, 8.547891731712411, 0.0, 1.0, 44.32564233428897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2845800.0000, 
sim time next is 2846400.0000, 
raw observation next is [2.0, 56.0, 0.0, 0.0, 26.0, 25.35616219130136, 8.565098500490913, 0.0, 1.0, 44.35456112984098], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.56, 0.0, 0.0, 1.0, 0.9080231701859088, 0.08565098500490914, 0.0, 1.0, 0.31681829378457843], 
reward next is 0.6832, 
noisyNet noise sample is [array([-0.52733654], dtype=float32), 0.15037932]. 
=============================================
[2019-04-01 17:53:51,514] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.0059139e-32], sum to 1.0000
[2019-04-01 17:53:51,515] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6601
[2019-04-01 17:53:51,531] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 94.16666666666666, 0.0, 0.0, 26.0, 24.99238734024765, 6.784153195752761, 0.0, 1.0, 54.65242496313908], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2869800.0000, 
sim time next is 2870400.0000, 
raw observation next is [1.0, 95.33333333333334, 0.0, 0.0, 26.0, 24.9737375508095, 7.066241053066608, 0.0, 1.0, 55.45863312049838], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9533333333333335, 0.0, 0.0, 1.0, 0.8533910786870713, 0.07066241053066608, 0.0, 1.0, 0.39613309371784555], 
reward next is 0.6039, 
noisyNet noise sample is [array([0.5045047], dtype=float32), 0.8392034]. 
=============================================
[2019-04-01 17:53:53,770] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6592746e-36
 3.2350695e-01 6.7649299e-01], sum to 1.0000
[2019-04-01 17:53:53,770] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6748
[2019-04-01 17:53:53,783] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.53551062020363, 6.524326510193968, 0.0, 1.0, 42.09911707975898], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2950200.0000, 
sim time next is 2950800.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.50489460504119, 6.436249507635199, 0.0, 1.0, 42.14852717349649], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 1.0, 0.7864135150058844, 0.06436249507635199, 0.0, 1.0, 0.3010609083821178], 
reward next is 0.6989, 
noisyNet noise sample is [array([1.1299957], dtype=float32), 1.0504231]. 
=============================================
[2019-04-01 17:54:00,741] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:54:00,751] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8089
[2019-04-01 17:54:00,765] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.33191794342418, 7.142801947876447, 0.0, 1.0, 51.98212883250345], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3133200.0000, 
sim time next is 3133800.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 25.33264556039105, 7.212250393161248, 0.0, 1.0, 51.75358085030633], 
processed observation next is [1.0, 0.2608695652173913, 0.6011080332409973, 1.0, 0.0, 0.0, 1.0, 0.9046636514844357, 0.07212250393161247, 0.0, 1.0, 0.3696684346450452], 
reward next is 0.6303, 
noisyNet noise sample is [array([-0.09730529], dtype=float32), 0.52615327]. 
=============================================
[2019-04-01 17:54:00,802] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.2786107e-16], sum to 1.0000
[2019-04-01 17:54:00,809] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0835
[2019-04-01 17:54:00,849] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 54.0, 106.8333333333333, 766.6666666666666, 26.0, 25.20665315486291, 7.760151084233233, 0.0, 1.0, 19.79466333339518], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3062400.0000, 
sim time next is 3063000.0000, 
raw observation next is [-4.0, 54.0, 107.6666666666667, 774.3333333333333, 26.0, 25.13662130429118, 7.726010650334719, 0.0, 1.0, 18.67103683835729], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.358888888888889, 0.8556169429097605, 1.0, 0.8766601863273112, 0.07726010650334719, 0.0, 1.0, 0.13336454884540921], 
reward next is 0.8666, 
noisyNet noise sample is [array([0.43554935], dtype=float32), -0.25946623]. 
=============================================
[2019-04-01 17:54:00,863] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[71.39605]
 [71.11366]
 [70.68528]
 [70.29128]
 [69.84077]], R is [[71.82125092]
 [71.96165466]
 [72.09185028]
 [72.21073914]
 [72.31718445]].
[2019-04-01 17:54:02,313] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:54:02,316] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5062
[2019-04-01 17:54:02,360] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.666666666666666, 100.0, 0.0, 0.0, 26.0, 26.14191195001239, 16.67010598711562, 0.0, 1.0, 5.597417635586182], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3177600.0000, 
sim time next is 3178200.0000, 
raw observation next is [4.333333333333333, 100.0, 0.0, 0.0, 26.0, 26.02245287241839, 16.03361337142039, 1.0, 1.0, 5.839049682926114], 
processed observation next is [1.0, 0.782608695652174, 0.58264081255771, 1.0, 0.0, 0.0, 1.0, 1.0032075532026272, 0.1603361337142039, 1.0, 1.0, 0.041707497735186526], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.91922784], dtype=float32), 0.81128234]. 
=============================================
[2019-04-01 17:54:03,209] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.9126102e-27
 1.0000000e+00 2.6238977e-37], sum to 1.0000
[2019-04-01 17:54:03,210] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8917
[2019-04-01 17:54:03,222] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 96.5, 0.0, 0.0, 26.0, 25.53467194002208, 12.34021254169866, 0.0, 1.0, 25.16916276894412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3198600.0000, 
sim time next is 3199200.0000, 
raw observation next is [1.333333333333333, 97.66666666666666, 0.0, 0.0, 26.0, 25.54413080699532, 12.22340841874174, 0.0, 1.0, 29.33398261449859], 
processed observation next is [1.0, 0.0, 0.4995383194829178, 0.9766666666666666, 0.0, 0.0, 1.0, 0.9348758295707599, 0.1222340841874174, 0.0, 1.0, 0.2095284472464185], 
reward next is 0.7905, 
noisyNet noise sample is [array([0.7088879], dtype=float32), -0.18864264]. 
=============================================
[2019-04-01 17:54:05,826] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.1309885e-33 0.0000000e+00 1.5299242e-27 1.3659150e-38 4.5335643e-31
 1.0000000e+00 7.7098441e-33], sum to 1.0000
[2019-04-01 17:54:05,829] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6299
[2019-04-01 17:54:05,841] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 68.66666666666667, 0.0, 0.0, 26.0, 25.55527251891541, 9.279349337748252, 0.0, 1.0, 34.06617816207732], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3469200.0000, 
sim time next is 3469800.0000, 
raw observation next is [1.0, 67.83333333333333, 0.0, 0.0, 26.0, 25.5255917828204, 8.969951994647358, 0.0, 1.0, 29.41419395988059], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.6783333333333332, 0.0, 0.0, 1.0, 0.9322273975457713, 0.08969951994647359, 0.0, 1.0, 0.2101013854277185], 
reward next is 0.7899, 
noisyNet noise sample is [array([0.57296723], dtype=float32), -0.26058793]. 
=============================================
[2019-04-01 17:54:06,941] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6442714e-36 0.0000000e+00 3.6633908e-38 0.0000000e+00 2.9009214e-24
 1.0000000e+00 7.1962956e-33], sum to 1.0000
[2019-04-01 17:54:06,943] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8346
[2019-04-01 17:54:06,987] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 1.0, 82.0, 26.0, 25.63455177533934, 10.41453877809171, 1.0, 1.0, 33.59490974576318], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3223800.0000, 
sim time next is 3224400.0000, 
raw observation next is [-3.0, 92.0, 15.0, 130.0, 26.0, 25.63627162573527, 10.40099202385137, 1.0, 1.0, 29.34231306792449], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.05, 0.143646408839779, 1.0, 0.948038803676467, 0.10400992023851369, 1.0, 1.0, 0.20958795048517492], 
reward next is 0.6300, 
noisyNet noise sample is [array([0.60502887], dtype=float32), -0.53922254]. 
=============================================
[2019-04-01 17:54:07,258] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8909721e-18 0.0000000e+00 5.9939589e-38 4.6170285e-29 1.2157242e-36
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:54:07,259] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3834
[2019-04-01 17:54:07,269] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 100.5, 663.5, 26.0, 26.34064174034935, 12.20489871304754, 1.0, 1.0, 13.711444563569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3145200.0000, 
sim time next is 3145800.0000, 
raw observation next is [7.0, 100.0, 102.0, 680.0, 26.0, 26.45064960565092, 12.38555210530751, 1.0, 1.0, 12.58611062975777], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.34, 0.7513812154696132, 1.0, 1.0643785150929885, 0.12385552105307511, 1.0, 1.0, 0.0899007902125555], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.30738425], dtype=float32), -0.6828529]. 
=============================================
[2019-04-01 17:54:12,838] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.6483034e-31
 1.0000000e+00 1.7334064e-12], sum to 1.0000
[2019-04-01 17:54:12,838] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6181
[2019-04-01 17:54:12,869] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 54.33333333333334, 113.5, 806.3333333333333, 26.0, 25.29018076345773, 9.571217594494799, 0.0, 1.0, 20.198098313608], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3583200.0000, 
sim time next is 3583800.0000, 
raw observation next is [-3.5, 54.5, 114.0, 816.0, 26.0, 25.23406408494419, 9.436587355756378, 0.0, 1.0, 20.25443041008707], 
processed observation next is [0.0, 0.4782608695652174, 0.36565096952908593, 0.545, 0.38, 0.901657458563536, 1.0, 0.8905805835634558, 0.09436587355756379, 0.0, 1.0, 0.14467450292919337], 
reward next is 0.8553, 
noisyNet noise sample is [array([0.9193412], dtype=float32), 1.3390353]. 
=============================================
[2019-04-01 17:54:13,454] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:54:13,455] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5905
[2019-04-01 17:54:13,497] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.333333333333333, 61.66666666666667, 16.16666666666666, 159.5, 26.0, 25.71493284538604, 8.706802361380499, 1.0, 1.0, 39.42844846293131], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3397200.0000, 
sim time next is 3397800.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 30.33333333333333, 212.0, 26.0, 25.75667220699512, 8.930147724075375, 1.0, 1.0, 36.72353661674105], 
processed observation next is [1.0, 0.30434782608695654, 0.4025854108956602, 0.6083333333333333, 0.1011111111111111, 0.23425414364640884, 1.0, 0.9652388867135885, 0.08930147724075375, 1.0, 1.0, 0.26231097583386465], 
reward next is 0.7377, 
noisyNet noise sample is [array([0.9988957], dtype=float32), 0.34223387]. 
=============================================
[2019-04-01 17:54:21,009] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:54:21,013] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2135
[2019-04-01 17:54:21,015] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.2229195e-14 1.0000000e+00], sum to 1.0000
[2019-04-01 17:54:21,018] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1402
[2019-04-01 17:54:21,024] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 24.91121433613108, 7.424771092717921, 0.0, 1.0, 40.36997642665295], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3558600.0000, 
sim time next is 3559200.0000, 
raw observation next is [-4.666666666666666, 67.0, 0.0, 0.0, 26.0, 24.90165324979154, 7.31807628750462, 0.0, 1.0, 40.35513945223344], 
processed observation next is [0.0, 0.17391304347826086, 0.33333333333333337, 0.67, 0.0, 0.0, 1.0, 0.8430933213987915, 0.0731807628750462, 0.0, 1.0, 0.2882509960873817], 
reward next is 0.7117, 
noisyNet noise sample is [array([1.2579439], dtype=float32), -1.0059127]. 
=============================================
[2019-04-01 17:54:21,033] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.166666666666667, 70.0, 0.0, 0.0, 26.0, 24.86318654061698, 7.253436993369976, 0.0, 1.0, 40.56136941583019], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3557400.0000, 
sim time next is 3558000.0000, 
raw observation next is [-4.333333333333334, 69.0, 0.0, 0.0, 26.0, 24.82767399799493, 7.308761598899625, 0.0, 1.0, 40.49748467145317], 
processed observation next is [0.0, 0.17391304347826086, 0.3425669436749769, 0.69, 0.0, 0.0, 1.0, 0.8325248568564183, 0.07308761598899624, 0.0, 1.0, 0.28926774765323693], 
reward next is 0.7107, 
noisyNet noise sample is [array([0.8026836], dtype=float32), 0.22914706]. 
=============================================
[2019-04-01 17:54:21,038] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[69.70877 ]
 [69.80633 ]
 [69.885956]
 [70.00554 ]
 [70.09527 ]], R is [[69.57796478]
 [69.59246063]
 [69.60638428]
 [69.61972046]
 [69.63267517]].
[2019-04-01 17:54:24,031] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:54:24,032] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0958
[2019-04-01 17:54:24,046] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.45442093109448, 8.098541531125415, 0.0, 1.0, 27.52129082932967], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3898200.0000, 
sim time next is 3898800.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.29786696000021, 7.714967611545134, 0.0, 1.0, 33.3586790833063], 
processed observation next is [1.0, 0.13043478260869565, 0.40720221606648205, 0.65, 0.0, 0.0, 1.0, 0.89969528000003, 0.07714967611545134, 0.0, 1.0, 0.23827627916647356], 
reward next is 0.7617, 
noisyNet noise sample is [array([-0.60430956], dtype=float32), 0.7885364]. 
=============================================
[2019-04-01 17:54:31,778] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:54:31,779] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8491
[2019-04-01 17:54:31,792] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 27.0, 0.0, 0.0, 26.0, 25.48283386414145, 7.548132274646794, 0.0, 1.0, 30.41043762563743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3654000.0000, 
sim time next is 3654600.0000, 
raw observation next is [8.833333333333334, 27.83333333333333, 0.0, 0.0, 26.0, 25.46222858842382, 7.502023786173742, 0.0, 1.0, 30.5993613591239], 
processed observation next is [0.0, 0.30434782608695654, 0.7072945521698984, 0.27833333333333327, 0.0, 0.0, 1.0, 0.9231755126319742, 0.07502023786173742, 0.0, 1.0, 0.218566866850885], 
reward next is 0.7814, 
noisyNet noise sample is [array([0.6919615], dtype=float32), -1.4389731]. 
=============================================
[2019-04-01 17:54:32,793] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 6.1914762e-30], sum to 1.0000
[2019-04-01 17:54:32,796] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2561
[2019-04-01 17:54:32,846] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 76.0, 0.0, 0.0, 26.0, 24.98809072820647, 7.019852188793195, 0.0, 1.0, 57.81905979896893], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3741000.0000, 
sim time next is 3741600.0000, 
raw observation next is [-4.0, 75.0, 0.0, 0.0, 26.0, 25.29256871453864, 7.108178552663705, 1.0, 1.0, 39.12204594878674], 
processed observation next is [1.0, 0.30434782608695654, 0.3518005540166205, 0.75, 0.0, 0.0, 1.0, 0.8989383877912341, 0.07108178552663705, 1.0, 1.0, 0.27944318534847673], 
reward next is 0.7206, 
noisyNet noise sample is [array([-0.05850331], dtype=float32), -0.3992632]. 
=============================================
[2019-04-01 17:54:36,641] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6876830e-12 3.1388007e-15 3.3651891e-18 1.0000000e+00 9.2648562e-27
 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:54:36,643] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7024
[2019-04-01 17:54:36,651] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 66.0, 0.0, 0.0, 19.0, 24.66427720153054, 7.506898808593661, 1.0, 1.0, 10.74613747414775], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3786600.0000, 
sim time next is 3787200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 24.58489394122588, 7.299439462732977, 1.0, 1.0, 10.77829119825166], 
processed observation next is [1.0, 0.8695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.0, 0.797841991603697, 0.07299439462732976, 1.0, 1.0, 0.07698779427322615], 
reward next is 0.9230, 
noisyNet noise sample is [array([-0.20435707], dtype=float32), 0.56037873]. 
=============================================
[2019-04-01 17:54:36,844] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.8906745e-36 1.0768969e-33
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:54:36,848] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2864
[2019-04-01 17:54:36,889] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.833333333333333, 77.0, 96.33333333333333, 593.0, 26.0, 25.80592473226785, 9.620540386777344, 1.0, 1.0, 25.31626955985273], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3748200.0000, 
sim time next is 3748800.0000, 
raw observation next is [-3.666666666666667, 77.0, 98.16666666666667, 634.0, 26.0, 25.98847366554067, 10.1770295401488, 1.0, 1.0, 20.73003234422924], 
processed observation next is [1.0, 0.391304347826087, 0.3610341643582641, 0.77, 0.32722222222222225, 0.7005524861878453, 1.0, 0.9983533807915244, 0.10177029540148799, 1.0, 1.0, 0.14807165960163743], 
reward next is 0.7811, 
noisyNet noise sample is [array([0.64856374], dtype=float32), 1.2868776]. 
=============================================
[2019-04-01 17:54:37,069] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 4.216316e-36], sum to 1.0000
[2019-04-01 17:54:37,076] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1018
[2019-04-01 17:54:37,085] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.833333333333334, 30.66666666666667, 0.0, 0.0, 26.0, 25.49989065577103, 9.910310265830537, 0.0, 1.0, 30.22166506786123], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4053000.0000, 
sim time next is 4053600.0000, 
raw observation next is [-5.0, 31.0, 0.0, 0.0, 26.0, 25.43877189513987, 9.889937785937633, 0.0, 1.0, 34.63578146081799], 
processed observation next is [1.0, 0.9565217391304348, 0.32409972299168976, 0.31, 0.0, 0.0, 1.0, 0.9198245564485527, 0.09889937785937633, 0.0, 1.0, 0.24739843900584277], 
reward next is 0.7526, 
noisyNet noise sample is [array([0.42620102], dtype=float32), -0.9365114]. 
=============================================
[2019-04-01 17:54:46,022] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 9.742791e-37
 1.000000e+00 9.309194e-23], sum to 1.0000
[2019-04-01 17:54:46,022] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7313
[2019-04-01 17:54:46,038] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.500000000000001, 72.33333333333333, 0.0, 0.0, 26.0, 25.37007904442751, 9.457333959062703, 0.0, 1.0, 42.51154984579481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4305000.0000, 
sim time next is 4305600.0000, 
raw observation next is [5.4, 73.0, 0.0, 0.0, 26.0, 25.59522082509254, 9.898195666450926, 0.0, 1.0, 41.78515358471202], 
processed observation next is [0.0, 0.8695652173913043, 0.6121883656509697, 0.73, 0.0, 0.0, 1.0, 0.9421744035846485, 0.09898195666450926, 0.0, 1.0, 0.298465382747943], 
reward next is 0.7015, 
noisyNet noise sample is [array([-0.22177657], dtype=float32), -1.1911035]. 
=============================================
[2019-04-01 17:54:48,603] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3773329e-29
 1.0000000e+00 2.4619210e-26], sum to 1.0000
[2019-04-01 17:54:48,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8575
[2019-04-01 17:54:48,665] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.333333333333334, 36.33333333333333, 15.33333333333333, 78.16666666666664, 26.0, 25.56376315197292, 7.799741217570637, 1.0, 1.0, 38.07455288153528], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4088400.0000, 
sim time next is 4089000.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 30.66666666666666, 156.3333333333333, 26.0, 25.55529760056188, 7.871148173013431, 1.0, 1.0, 35.33567032723657], 
processed observation next is [1.0, 0.30434782608695654, 0.3471837488457987, 0.35166666666666674, 0.1022222222222222, 0.17274401473296497, 1.0, 0.9364710857945541, 0.07871148173013431, 1.0, 1.0, 0.25239764519454694], 
reward next is 0.7476, 
noisyNet noise sample is [array([-2.0233681], dtype=float32), 1.6932504]. 
=============================================
[2019-04-01 17:54:48,677] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[52.0325  ]
 [56.90322 ]
 [62.663506]
 [62.38098 ]
 [62.919483]], R is [[47.87747955]
 [48.12674332]
 [48.33532715]
 [48.4362793 ]
 [48.24964142]].
[2019-04-01 17:54:49,063] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5527210e-17 1.9334402e-29 1.5815364e-20 3.6246179e-15 2.1123946e-04
 9.9978882e-01 1.5284186e-25], sum to 1.0000
[2019-04-01 17:54:49,066] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9109
[2019-04-01 17:54:49,105] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 25.0, 107.3333333333333, 806.0, 26.0, 27.23257140147042, 17.87959951067494, 1.0, 1.0, 16.7713378336989], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4025400.0000, 
sim time next is 4026000.0000, 
raw observation next is [-2.666666666666667, 24.0, 105.6666666666667, 800.0, 26.0, 27.25768299200466, 18.16016558900976, 1.0, 1.0, 15.71269979406872], 
processed observation next is [1.0, 0.6086956521739131, 0.38873499538319484, 0.24, 0.3522222222222223, 0.8839779005524862, 1.0, 1.1796689988578086, 0.1816016558900976, 1.0, 1.0, 0.11223356995763371], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6132531], dtype=float32), 0.8695473]. 
=============================================
[2019-04-01 17:54:49,112] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[38.53434 ]
 [38.384357]
 [38.278255]
 [38.13336 ]
 [38.07114 ]], R is [[38.29573822]
 [37.91278076]
 [37.53365326]
 [37.15831757]
 [36.78673553]].
[2019-04-01 17:54:51,206] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.0873049e-31
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:54:51,207] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9725
[2019-04-01 17:54:51,224] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.2457592747927, 5.829804608261694, 0.0, 1.0, 43.07872688764289], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3985200.0000, 
sim time next is 3985800.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.28774292435774, 5.835619589922608, 0.0, 1.0, 43.01500511004917], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 1.0, 0.75539184633682, 0.05835619589922608, 0.0, 1.0, 0.3072500365003512], 
reward next is 0.6927, 
noisyNet noise sample is [array([1.7571075], dtype=float32), -0.5352963]. 
=============================================
[2019-04-01 17:54:51,757] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6931396e-37 0.0000000e+00 2.7912924e-37 1.7023187e-35 3.5318534e-28
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:54:51,757] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4586
[2019-04-01 17:54:51,795] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 36.0, 92.0, 469.0, 26.0, 25.69037643675991, 9.477375832669205, 1.0, 1.0, 27.99016336449868], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4091400.0000, 
sim time next is 4092000.0000, 
raw observation next is [-3.333333333333333, 36.66666666666666, 94.0, 504.0, 26.0, 25.99725937840034, 10.14564411355236, 1.0, 1.0, 24.31576629319218], 
processed observation next is [1.0, 0.34782608695652173, 0.37026777469990774, 0.3666666666666666, 0.31333333333333335, 0.5569060773480663, 1.0, 0.99960848262862, 0.1014564411355236, 1.0, 1.0, 0.1736840449513727], 
reward next is 0.7681, 
noisyNet noise sample is [array([-0.46615866], dtype=float32), -0.14551778]. 
=============================================
[2019-04-01 17:54:51,804] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[34.744534]
 [36.106174]
 [38.34162 ]
 [41.150753]
 [45.093227]], R is [[34.59698868]
 [35.05109024]
 [35.48168945]
 [35.90841675]
 [36.31448364]].
[2019-04-01 17:54:51,996] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.3155449e-21 3.5692188e-37 2.2125794e-30 9.1415655e-26 4.3149465e-24
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:54:51,999] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3289
[2019-04-01 17:54:52,020] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 25.0, 107.3333333333333, 806.0, 26.0, 27.1404172848208, 17.82178816463256, 1.0, 1.0, 14.01411635157213], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4025400.0000, 
sim time next is 4026000.0000, 
raw observation next is [-2.666666666666667, 24.0, 105.6666666666667, 800.0, 26.0, 27.17126476566713, 18.13209130692588, 1.0, 1.0, 13.07867925149753], 
processed observation next is [1.0, 0.6086956521739131, 0.38873499538319484, 0.24, 0.3522222222222223, 0.8839779005524862, 1.0, 1.167323537952447, 0.1813209130692588, 1.0, 1.0, 0.09341913751069664], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4792783], dtype=float32), 2.298614]. 
=============================================
[2019-04-01 17:54:52,025] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[35.294144]
 [35.11005 ]
 [34.947517]
 [34.776733]
 [34.677025]], R is [[35.14359665]
 [34.79216003]
 [34.44424057]
 [34.09980011]
 [33.75880432]].
[2019-04-01 17:54:52,937] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1620426e-29 2.6043715e-37 1.3826371e-24 0.0000000e+00 7.2673258e-34
 5.6006378e-01 4.3993631e-01], sum to 1.0000
[2019-04-01 17:54:52,939] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5546
[2019-04-01 17:54:52,952] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.833333333333333, 53.33333333333333, 0.0, 0.0, 26.0, 24.98492113855342, 7.000487199736416, 0.0, 1.0, 38.98599843829508], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4164600.0000, 
sim time next is 4165200.0000, 
raw observation next is [-4.0, 54.0, 0.0, 0.0, 26.0, 24.95694485837329, 6.910437177449751, 0.0, 1.0, 38.97153017553194], 
processed observation next is [0.0, 0.21739130434782608, 0.3518005540166205, 0.54, 0.0, 0.0, 1.0, 0.8509921226247558, 0.0691043717744975, 0.0, 1.0, 0.278368072682371], 
reward next is 0.7216, 
noisyNet noise sample is [array([-2.266735], dtype=float32), 0.48264754]. 
=============================================
[2019-04-01 17:54:53,857] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:54:53,860] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9461
[2019-04-01 17:54:53,876] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 39.5, 0.0, 0.0, 26.0, 25.41718616263391, 11.62661326377215, 0.0, 1.0, 51.11416114015652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4134600.0000, 
sim time next is 4135200.0000, 
raw observation next is [1.0, 38.33333333333334, 0.0, 0.0, 26.0, 25.51698059385349, 12.34374648932506, 0.0, 1.0, 36.88517012921086], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.3833333333333334, 0.0, 0.0, 1.0, 0.9309972276933556, 0.1234374648932506, 0.0, 1.0, 0.26346550092293475], 
reward next is 0.7365, 
noisyNet noise sample is [array([1.8215767], dtype=float32), -2.573572]. 
=============================================
[2019-04-01 17:55:04,027] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0247495e-32 0.0000000e+00 0.0000000e+00 9.6518205e-31 5.5552676e-26
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:55:04,028] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8865
[2019-04-01 17:55:04,040] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.43333333333333, 40.66666666666666, 112.3333333333333, 745.6666666666667, 26.0, 27.40275954752412, 18.81661871570625, 1.0, 1.0, 8.251981237867472], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4356600.0000, 
sim time next is 4357200.0000, 
raw observation next is [10.86666666666667, 39.33333333333334, 113.6666666666667, 762.8333333333333, 26.0, 27.4981567539861, 19.68236961215213, 1.0, 1.0, 7.360776328475422], 
processed observation next is [1.0, 0.43478260869565216, 0.7636195752539245, 0.3933333333333334, 0.378888888888889, 0.8429097605893185, 1.0, 1.2140223934265855, 0.19682369612152129, 1.0, 1.0, 0.05257697377482445], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.17317852], dtype=float32), 0.59630483]. 
=============================================
[2019-04-01 17:55:07,038] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:55:07,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9268
[2019-04-01 17:55:07,060] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 73.16666666666667, 0.0, 0.0, 26.0, 24.70842478148984, 7.087090467668443, 0.0, 1.0, 40.83107871385979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4755000.0000, 
sim time next is 4755600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.68643978329042, 7.000850270555048, 0.0, 1.0, 40.73705722592783], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 1.0, 0.8123485404700601, 0.07000850270555048, 0.0, 1.0, 0.29097898018519874], 
reward next is 0.7090, 
noisyNet noise sample is [array([0.63170505], dtype=float32), 0.8405706]. 
=============================================
[2019-04-01 17:55:17,933] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.2042792e-18 1.6480580e-26 2.8941738e-20 9.8005238e-16 7.9856130e-13
 1.0000000e+00 1.5975744e-38], sum to 1.0000
[2019-04-01 17:55:17,935] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1103
[2019-04-01 17:55:17,967] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 106.3333333333333, 0.0, 26.0, 26.00708102989731, 10.4665022812373, 1.0, 1.0, 13.33444710350421], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4699200.0000, 
sim time next is 4699800.0000, 
raw observation next is [0.0, 92.0, 115.0, 0.0, 26.0, 26.09863615250892, 10.74362123732401, 1.0, 1.0, 12.70645621931491], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.38333333333333336, 0.0, 1.0, 1.0140908789298457, 0.10743621237324011, 1.0, 1.0, 0.09076040156653507], 
reward next is 0.6118, 
noisyNet noise sample is [array([0.4170809], dtype=float32), 0.06376736]. 
=============================================
[2019-04-01 17:55:18,867] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-01 17:55:18,870] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 17:55:18,870] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:55:18,871] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 17:55:18,871] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 17:55:18,871] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:55:18,871] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:55:18,877] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run24
[2019-04-01 17:55:18,900] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run24
[2019-04-01 17:55:18,920] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run24
[2019-04-01 17:56:34,383] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.54169834], dtype=float32), -0.7293798]
[2019-04-01 17:56:34,384] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.2999999999999999, 68.66666666666666, 62.33333333333333, 499.8333333333333, 26.0, 25.11492117754882, 6.970137053215375, 1.0, 1.0, 21.2754470023258]
[2019-04-01 17:56:34,384] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 17:56:34,385] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.8909676e-30 1.9391087e-36 2.2605163e-31 4.8679835e-30 3.5409619e-19
 1.0000000e+00 4.1522447e-22], sampled 0.6009602469813963
[2019-04-01 17:56:57,168] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.54169834], dtype=float32), -0.7293798]
[2019-04-01 17:56:57,168] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [18.15, 54.66666666666667, 144.3333333333333, 540.3333333333334, 26.0, 27.52673172595566, 31.33506952834936, 1.0, 0.0, 0.0]
[2019-04-01 17:56:57,168] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 17:56:57,169] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.3257584e-33 0.0000000e+00 3.9688091e-33 1.3719455e-32 1.1219447e-21
 1.0000000e+00 1.8076539e-22], sampled 0.26523285006234043
[2019-04-01 17:57:02,177] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 17:57:18,909] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.54169834], dtype=float32), -0.7293798]
[2019-04-01 17:57:18,910] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.166666666666667, 39.0, 0.0, 0.0, 26.0, 25.56873878564155, 10.10938737125024, 0.0, 1.0, 27.77419678004866]
[2019-04-01 17:57:18,910] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 17:57:18,911] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 8.3498707e-37 0.0000000e+00 4.8708974e-31
 1.0000000e+00 7.0025961e-16], sampled 0.545849171734948
[2019-04-01 17:57:20,298] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 17:57:23,387] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 17:57:24,409] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2300000, evaluation results [2300000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 17:57:25,950] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3345149e-23 5.5783367e-31 1.9557670e-18 3.9108790e-22 3.0798589e-11
 1.0000000e+00 1.4867250e-08], sum to 1.0000
[2019-04-01 17:57:25,951] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0563
[2019-04-01 17:57:25,976] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 26.0, 25.96423616799912, 13.33622444439142, 0.0, 1.0, 17.90157943736763], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4658400.0000, 
sim time next is 4659000.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 26.0, 25.91737952712714, 11.6935496683367, 0.0, 1.0, 18.5063741833348], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.57, 0.0, 0.0, 1.0, 0.988197075303877, 0.116935496683367, 0.0, 1.0, 0.13218838702382], 
reward next is 0.8678, 
noisyNet noise sample is [array([1.2456301], dtype=float32), -0.046362862]. 
=============================================
[2019-04-01 17:57:25,989] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.367065]
 [72.54138 ]
 [72.7899  ]
 [72.86963 ]
 [73.03218 ]], R is [[72.51477051]
 [72.66175842]
 [72.80043793]
 [72.93023682]
 [73.05049133]].
[2019-04-01 17:57:31,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:31,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:31,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run18
[2019-04-01 17:57:32,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:32,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:32,907] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run18
[2019-04-01 17:57:35,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:35,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:35,126] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run18
[2019-04-01 17:57:36,712] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:57:36,713] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8432
[2019-04-01 17:57:36,735] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.34763198129162, 8.1902410354334, 0.0, 1.0, 38.65971142667887], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4834200.0000, 
sim time next is 4834800.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.31733046828226, 8.163101345245787, 0.0, 1.0, 39.98671441190832], 
processed observation next is [0.0, 1.0, 0.4349030470914128, 0.55, 0.0, 0.0, 1.0, 0.9024757811831802, 0.08163101345245787, 0.0, 1.0, 0.285619388656488], 
reward next is 0.7144, 
noisyNet noise sample is [array([-0.28537577], dtype=float32), -0.16661924]. 
=============================================
[2019-04-01 17:57:41,782] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.4241998e-33
 1.0000000e+00 6.7317107e-16], sum to 1.0000
[2019-04-01 17:57:41,785] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3554
[2019-04-01 17:57:41,796] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666667, 41.0, 0.0, 0.0, 26.0, 25.38794661133217, 7.580979546853404, 0.0, 1.0, 36.16695051533758], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4926000.0000, 
sim time next is 4926600.0000, 
raw observation next is [0.5, 41.5, 0.0, 0.0, 26.0, 25.3647036963677, 7.79044947868091, 0.0, 1.0, 38.47405091724041], 
processed observation next is [1.0, 0.0, 0.4764542936288089, 0.415, 0.0, 0.0, 1.0, 0.909243385195386, 0.0779044947868091, 0.0, 1.0, 0.2748146494088601], 
reward next is 0.7252, 
noisyNet noise sample is [array([1.5951669], dtype=float32), -0.55691797]. 
=============================================
[2019-04-01 17:57:43,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:43,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:43,766] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run18
[2019-04-01 17:57:43,883] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:43,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:43,900] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run18
[2019-04-01 17:57:44,341] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:44,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:44,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run18
[2019-04-01 17:57:46,322] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:46,322] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:46,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run18
[2019-04-01 17:57:46,924] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:46,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:46,940] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run18
[2019-04-01 17:57:47,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:47,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:47,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run18
[2019-04-01 17:57:48,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:48,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:48,232] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run18
[2019-04-01 17:57:48,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:48,310] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:48,316] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run18
[2019-04-01 17:57:48,575] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:48,575] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:48,579] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run18
[2019-04-01 17:57:49,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:49,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:49,556] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run18
[2019-04-01 17:57:50,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:50,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:50,626] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run18
[2019-04-01 17:57:50,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:50,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:50,732] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run18
[2019-04-01 17:57:51,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 17:57:51,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 17:57:51,728] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run18
[2019-04-01 17:58:06,360] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.00762885e-28 1.00000000e+00 8.44850440e-11], sum to 1.0000
[2019-04-01 17:58:06,361] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2729
[2019-04-01 17:58:06,375] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.64528296992453, 6.790452152906878, 0.0, 1.0, 44.33203927745968], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 196200.0000, 
sim time next is 196800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.59831649170665, 6.854990458832414, 0.0, 1.0, 44.35272843281939], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 1.0, 0.5140452131009502, 0.06854990458832413, 0.0, 1.0, 0.31680520309156707], 
reward next is 0.6832, 
noisyNet noise sample is [array([-0.64796484], dtype=float32), 0.14627373]. 
=============================================
[2019-04-01 17:58:11,092] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.7651367e-37 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:58:11,092] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5267
[2019-04-01 17:58:11,178] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.95, 63.5, 149.0, 0.0, 26.0, 24.95207140305884, 6.914185078636829, 1.0, 1.0, 53.24549998662033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 221400.0000, 
sim time next is 222000.0000, 
raw observation next is [-3.766666666666667, 63.0, 143.6666666666667, 0.0, 26.0, 24.9103197039591, 7.181574271412934, 1.0, 1.0, 55.65016682568694], 
processed observation next is [1.0, 0.5652173913043478, 0.358264081255771, 0.63, 0.47888888888888903, 0.0, 1.0, 0.8443313862798715, 0.07181574271412934, 1.0, 1.0, 0.39750119161204955], 
reward next is 0.6025, 
noisyNet noise sample is [array([-0.1314843], dtype=float32), -1.3800073]. 
=============================================
[2019-04-01 17:58:11,185] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[64.33157]
 [64.44962]
 [64.33892]
 [64.34621]
 [64.43661]], R is [[64.18863678]
 [64.16642761]
 [64.28039551]
 [64.35714722]
 [64.44380951]].
[2019-04-01 17:58:32,970] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5952161e-32 0.0000000e+00 2.4601886e-30 5.3685946e-35 4.9041737e-25
 1.0000000e+00 3.3886730e-18], sum to 1.0000
[2019-04-01 17:58:32,972] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8481
[2019-04-01 17:58:32,995] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.633333333333333, 96.66666666666666, 0.0, 0.0, 26.0, 24.93177082526534, 6.829271863713046, 0.0, 1.0, 39.28862111962166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 517200.0000, 
sim time next is 517800.0000, 
raw observation next is [3.716666666666666, 96.83333333333334, 0.0, 0.0, 26.0, 24.97180111277242, 6.835393571648685, 0.0, 1.0, 39.22471508795832], 
processed observation next is [1.0, 1.0, 0.5655586334256695, 0.9683333333333334, 0.0, 0.0, 1.0, 0.8531144446817746, 0.06835393571648685, 0.0, 1.0, 0.28017653634255946], 
reward next is 0.7198, 
noisyNet noise sample is [array([0.42655414], dtype=float32), -0.37577716]. 
=============================================
[2019-04-01 17:58:46,066] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:58:46,066] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6269
[2019-04-01 17:58:46,093] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.65, 86.5, 0.0, 0.0, 26.0, 24.47679048669004, 5.848560815976053, 0.0, 1.0, 41.71110738914371], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 606600.0000, 
sim time next is 607200.0000, 
raw observation next is [-3.733333333333333, 86.33333333333333, 0.0, 0.0, 26.0, 24.47816668791721, 5.811589670801015, 0.0, 1.0, 41.66557672136035], 
processed observation next is [0.0, 0.0, 0.35918744228993543, 0.8633333333333333, 0.0, 0.0, 1.0, 0.7825952411310299, 0.05811589670801015, 0.0, 1.0, 0.2976112622954311], 
reward next is 0.7024, 
noisyNet noise sample is [array([-2.0878327], dtype=float32), 0.51402956]. 
=============================================
[2019-04-01 17:58:51,340] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 5.3249454e-28], sum to 1.0000
[2019-04-01 17:58:51,344] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1693
[2019-04-01 17:58:51,392] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 76.0, 29.0, 0.0, 26.0, 25.66073420145026, 7.16941803479325, 1.0, 1.0, 33.14881208815437], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 721800.0000, 
sim time next is 722400.0000, 
raw observation next is [-2.3, 76.0, 41.0, 8.166666666666664, 26.0, 25.69700297917317, 7.225128230390254, 1.0, 1.0, 31.1424901115141], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.13666666666666666, 0.009023941068139961, 1.0, 0.9567147113104529, 0.07225128230390254, 1.0, 1.0, 0.2224463579393864], 
reward next is 0.7776, 
noisyNet noise sample is [array([0.4682364], dtype=float32), -0.6623098]. 
=============================================
[2019-04-01 17:58:55,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:58:55,380] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9424
[2019-04-01 17:58:55,436] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.533333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 24.91616879555493, 8.274655717900059, 1.0, 1.0, 57.27993175192171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 754800.0000, 
sim time next is 755400.0000, 
raw observation next is [-3.716666666666666, 55.66666666666667, 0.0, 0.0, 26.0, 25.19628800722327, 8.917466945988734, 1.0, 1.0, 36.83246471261894], 
processed observation next is [1.0, 0.7391304347826086, 0.3596491228070176, 0.5566666666666668, 0.0, 0.0, 1.0, 0.8851840010318958, 0.08917466945988733, 1.0, 1.0, 0.2630890336615639], 
reward next is 0.7369, 
noisyNet noise sample is [array([-0.26859787], dtype=float32), 1.2860103]. 
=============================================
[2019-04-01 17:58:55,934] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:58:55,946] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2134
[2019-04-01 17:58:56,018] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.2, 79.0, 91.0, 0.0, 26.0, 25.47761614555357, 7.242321962330318, 1.0, 1.0, 27.05923438479298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 826200.0000, 
sim time next is 826800.0000, 
raw observation next is [-4.1, 79.0, 85.66666666666667, 0.0, 26.0, 25.13724511213966, 6.979723065167974, 1.0, 1.0, 36.71329928049699], 
processed observation next is [1.0, 0.5652173913043478, 0.3490304709141275, 0.79, 0.28555555555555556, 0.0, 1.0, 0.876749301734237, 0.06979723065167974, 1.0, 1.0, 0.26223785200354993], 
reward next is 0.7378, 
noisyNet noise sample is [array([-0.08620362], dtype=float32), -1.2014711]. 
=============================================
[2019-04-01 17:59:02,851] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:59:02,851] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3536
[2019-04-01 17:59:02,860] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.7, 97.0, 100.5, 0.0, 26.0, 25.33219849381534, 7.013876088368704, 1.0, 1.0, 8.453299368684405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 907200.0000, 
sim time next is 907800.0000, 
raw observation next is [2.883333333333334, 96.33333333333334, 101.6666666666667, 0.0, 26.0, 25.30909492943805, 7.016509552456502, 1.0, 1.0, 8.298895817522649], 
processed observation next is [1.0, 0.5217391304347826, 0.5424746075715605, 0.9633333333333334, 0.338888888888889, 0.0, 1.0, 0.9012992756340071, 0.07016509552456501, 1.0, 1.0, 0.05927782726801892], 
reward next is 0.9407, 
noisyNet noise sample is [array([-1.3969071], dtype=float32), 1.1867977]. 
=============================================
[2019-04-01 17:59:07,367] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:59:07,368] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2495
[2019-04-01 17:59:07,388] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.433333333333334, 92.0, 0.0, 0.0, 26.0, 25.30307684907947, 10.47535112300256, 0.0, 1.0, 40.2687255452119], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1318800.0000, 
sim time next is 1319400.0000, 
raw observation next is [1.35, 92.0, 0.0, 0.0, 26.0, 25.2656345094037, 10.64725701053052, 0.0, 1.0, 42.03830627698676], 
processed observation next is [1.0, 0.2608695652173913, 0.5000000000000001, 0.92, 0.0, 0.0, 1.0, 0.8950906442005285, 0.1064725701053052, 0.0, 1.0, 0.30027361626419113], 
reward next is 0.6997, 
noisyNet noise sample is [array([-0.68916225], dtype=float32), -0.99691975]. 
=============================================
[2019-04-01 17:59:11,154] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 4.2682444e-22], sum to 1.0000
[2019-04-01 17:59:11,155] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5413
[2019-04-01 17:59:11,178] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.7, 84.0, 16.0, 0.5, 26.0, 25.67812562633928, 12.95816251077599, 0.0, 1.0, 20.66888012796012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1152000.0000, 
sim time next is 1152600.0000, 
raw observation next is [13.16666666666667, 82.5, 21.0, 0.3333333333333333, 26.0, 25.65952719644553, 12.96897571159521, 0.0, 1.0, 21.47309713379548], 
processed observation next is [0.0, 0.34782608695652173, 0.8273314866112651, 0.825, 0.07, 0.00036832412523020257, 1.0, 0.9513610280636472, 0.1296897571159521, 0.0, 1.0, 0.15337926524139628], 
reward next is 0.8466, 
noisyNet noise sample is [array([-0.4565537], dtype=float32), 1.5857685]. 
=============================================
[2019-04-01 17:59:13,412] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9830427e-34
 1.2195374e-05 9.9998784e-01], sum to 1.0000
[2019-04-01 17:59:13,415] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0900
[2019-04-01 17:59:13,427] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.41666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.72085123697109, 13.79693372146281, 0.0, 1.0, 15.95732095893178], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1127400.0000, 
sim time next is 1128000.0000, 
raw observation next is [10.33333333333333, 77.66666666666667, 0.0, 0.0, 26.0, 25.68192577099785, 13.52927529909963, 0.0, 1.0, 15.23516335502085], 
processed observation next is [0.0, 0.043478260869565216, 0.7488457987072946, 0.7766666666666667, 0.0, 0.0, 1.0, 0.9545608244282643, 0.1352927529909963, 0.0, 1.0, 0.10882259539300607], 
reward next is 0.8912, 
noisyNet noise sample is [array([-0.92410934], dtype=float32), 0.31793004]. 
=============================================
[2019-04-01 17:59:13,447] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[91.2051  ]
 [91.32818 ]
 [91.68629 ]
 [91.99916 ]
 [92.446526]], R is [[91.29958344]
 [91.2726059 ]
 [91.24032593]
 [91.20239258]
 [91.15946198]].
[2019-04-01 17:59:13,693] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:59:13,696] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1555
[2019-04-01 17:59:13,710] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.95, 79.5, 0.0, 0.0, 26.0, 26.60650930234446, 14.54668904641963, 1.0, 1.0, 5.586398598895235], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1013400.0000, 
sim time next is 1014000.0000, 
raw observation next is [14.76666666666667, 80.0, 0.0, 0.0, 26.0, 25.91451223038243, 12.71589909829221, 1.0, 1.0, 5.072301860550771], 
processed observation next is [1.0, 0.7391304347826086, 0.8716528162511544, 0.8, 0.0, 0.0, 1.0, 0.9877874614832045, 0.1271589909829221, 1.0, 1.0, 0.036230727575362644], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.10561042], dtype=float32), 3.2004597]. 
=============================================
[2019-04-01 17:59:13,718] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[55.114872]
 [54.946617]
 [54.57558 ]
 [54.34594 ]
 [53.827385]], R is [[54.44904327]
 [53.90455246]
 [53.36550903]
 [52.83185577]
 [52.30353928]].
[2019-04-01 17:59:15,900] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:59:15,909] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8235
[2019-04-01 17:59:15,915] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.71666666666667, 60.33333333333333, 0.0, 0.0, 26.0, 25.89904181776244, 15.51392301643875, 1.0, 1.0, 4.429955519816349], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1109400.0000, 
sim time next is 1110000.0000, 
raw observation next is [13.63333333333333, 60.66666666666667, 0.0, 0.0, 26.0, 25.82062575407827, 15.06875703703211, 0.0, 1.0, 4.323169733518993], 
processed observation next is [1.0, 0.8695652173913043, 0.840258541089566, 0.6066666666666667, 0.0, 0.0, 1.0, 0.9743751077254673, 0.1506875703703211, 0.0, 1.0, 0.03087978381084995], 
reward next is 0.9691, 
noisyNet noise sample is [array([1.8411853], dtype=float32), -1.3204225]. 
=============================================
[2019-04-01 17:59:15,940] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.71387 ]
 [79.186554]
 [75.28581 ]
 [78.26873 ]
 [80.70936 ]], R is [[79.74547577]
 [78.94802094]
 [79.12607574]
 [78.33481598]
 [77.5514679 ]].
[2019-04-01 17:59:16,965] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:59:16,967] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6249
[2019-04-01 17:59:16,976] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.7, 50.0, 18.0, 1.5, 26.0, 27.95228876823864, 23.53116063740122, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1098000.0000, 
sim time next is 1098600.0000, 
raw observation next is [17.43333333333333, 50.5, 0.0, 0.0, 26.0, 27.39453410815631, 23.60801361119136, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.9455216989843028, 0.505, 0.0, 0.0, 1.0, 1.1992191583080445, 0.2360801361119136, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9962952], dtype=float32), -0.4604293]. 
=============================================
[2019-04-01 17:59:17,563] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:59:17,570] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8483
[2019-04-01 17:59:17,581] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.3, 63.0, 0.0, 0.0, 26.0, 24.80179657434966, 9.199372206814493, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1188000.0000, 
sim time next is 1188600.0000, 
raw observation next is [18.2, 63.66666666666666, 0.0, 0.0, 26.0, 24.78378621286429, 9.127435729663496, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.9667590027700832, 0.6366666666666666, 0.0, 0.0, 1.0, 0.826255173266327, 0.09127435729663497, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.949861], dtype=float32), 0.41760227]. 
=============================================
[2019-04-01 17:59:17,965] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.2920863e-33], sum to 1.0000
[2019-04-01 17:59:17,967] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4123
[2019-04-01 17:59:17,975] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 26.0, 24.67199906276914, 8.678444192855688, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1192800.0000, 
sim time next is 1193400.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 26.0, 24.649073391534, 8.604813928828422, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 1.0, 0.807010484504857, 0.08604813928828423, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28634405], dtype=float32), -1.9507145]. 
=============================================
[2019-04-01 17:59:24,183] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1574127e-37 0.0000000e+00 2.7163130e-37 0.0000000e+00 1.1684687e-28
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:59:24,183] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0387
[2019-04-01 17:59:24,208] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 118.6666666666667, 0.0, 26.0, 26.09894155835899, 12.50954955983956, 1.0, 1.0, 9.430820659804219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1336200.0000, 
sim time next is 1336800.0000, 
raw observation next is [1.1, 92.0, 122.8333333333333, 0.0, 26.0, 26.10195437917863, 12.50928202241411, 1.0, 1.0, 9.09557443524254], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.40944444444444433, 0.0, 1.0, 1.0145649113112327, 0.12509282022414112, 1.0, 1.0, 0.064968388823161], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7196406], dtype=float32), 0.48733208]. 
=============================================
[2019-04-01 17:59:24,970] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8503164e-37 0.0000000e+00 0.0000000e+00 1.2073511e-34 3.0270658e-22
 1.0000000e+00 4.8620199e-25], sum to 1.0000
[2019-04-01 17:59:24,971] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2625
[2019-04-01 17:59:24,992] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.58552615412126, 10.86437765955569, 0.0, 1.0, 28.82799766468498], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1385400.0000, 
sim time next is 1386000.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.49942321550322, 10.35766516373209, 0.0, 1.0, 28.04364290606494], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.9284890307861744, 0.1035766516373209, 0.0, 1.0, 0.200311735043321], 
reward next is 0.7997, 
noisyNet noise sample is [array([0.04269804], dtype=float32), 0.48533472]. 
=============================================
[2019-04-01 17:59:24,998] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[49.51976 ]
 [49.252   ]
 [49.088196]
 [49.111816]
 [49.43853 ]], R is [[50.27200317]
 [50.56336975]
 [50.82429123]
 [51.03477097]
 [51.2355423 ]].
[2019-04-01 17:59:25,439] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 3.710166e-31
 1.000000e+00 6.638195e-25], sum to 1.0000
[2019-04-01 17:59:25,439] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6254
[2019-04-01 17:59:25,465] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.29148405908735, 9.177237455695073, 0.0, 1.0, 36.54805886783322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1485600.0000, 
sim time next is 1486200.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.28754122732929, 9.243308456120767, 0.0, 1.0, 36.42585685230133], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 1.0, 0.8982201753327556, 0.09243308456120766, 0.0, 1.0, 0.26018469180215237], 
reward next is 0.7398, 
noisyNet noise sample is [array([0.6165075], dtype=float32), -1.3048065]. 
=============================================
[2019-04-01 17:59:26,789] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0719177e-32 0.0000000e+00 2.8672535e-33 3.1599539e-32 1.5862732e-23
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 17:59:26,790] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1921
[2019-04-01 17:59:26,805] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.21813816311164, 9.82900310820564, 0.0, 1.0, 39.52345401417937], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1389600.0000, 
sim time next is 1390200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.20448909394713, 9.867366370695649, 0.0, 1.0, 39.3877270693642], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.8863555848495902, 0.09867366370695649, 0.0, 1.0, 0.28134090763831576], 
reward next is 0.7187, 
noisyNet noise sample is [array([-1.4141268], dtype=float32), -0.8619291]. 
=============================================
[2019-04-01 17:59:28,645] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 17:59:28,646] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7882
[2019-04-01 17:59:28,657] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.3193987317361, 10.75114234456566, 0.0, 1.0, 40.41957024973917], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1381800.0000, 
sim time next is 1382400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.34347094536225, 10.70180948012691, 0.0, 1.0, 40.20449058089913], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.9062101350517499, 0.1070180948012691, 0.0, 1.0, 0.2871749327207081], 
reward next is 0.7128, 
noisyNet noise sample is [array([-0.7599331], dtype=float32), 0.22672358]. 
=============================================
[2019-04-01 17:59:35,885] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2671462e-29
 1.0000000e+00 5.6212530e-23], sum to 1.0000
[2019-04-01 17:59:35,887] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9243
[2019-04-01 17:59:35,938] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.966666666666667, 78.0, 143.3333333333333, 86.83333333333334, 26.0, 24.96591095931423, 6.440219418544068, 0.0, 1.0, 43.91293324633783], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1849200.0000, 
sim time next is 1849800.0000, 
raw observation next is [-5.783333333333333, 78.0, 138.6666666666667, 79.66666666666667, 26.0, 24.87919947196229, 6.369818732496825, 0.0, 1.0, 49.30894277416601], 
processed observation next is [0.0, 0.391304347826087, 0.3024007386888274, 0.78, 0.46222222222222237, 0.08802946593001842, 1.0, 0.8398856388517559, 0.06369818732496825, 0.0, 1.0, 0.3522067341011858], 
reward next is 0.6478, 
noisyNet noise sample is [array([1.2471645], dtype=float32), 0.18724477]. 
=============================================
[2019-04-01 17:59:51,222] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.6262140e-27
 1.0000000e+00 2.8384644e-21], sum to 1.0000
[2019-04-01 17:59:51,222] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5957
[2019-04-01 17:59:51,238] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.63503733154425, 5.24000712636655, 0.0, 1.0, 46.41298659479734], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1832400.0000, 
sim time next is 1833000.0000, 
raw observation next is [-6.2, 79.00000000000001, 0.0, 0.0, 26.0, 23.60318738290706, 5.242681263050327, 0.0, 1.0, 46.44190062647056], 
processed observation next is [0.0, 0.21739130434782608, 0.2908587257617729, 0.7900000000000001, 0.0, 0.0, 1.0, 0.6575981975581513, 0.052426812630503264, 0.0, 1.0, 0.3317278616176469], 
reward next is 0.6683, 
noisyNet noise sample is [array([-1.1444198], dtype=float32), 0.21207747]. 
=============================================
[2019-04-01 17:59:51,280] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[60.992043]
 [61.09265 ]
 [61.21451 ]
 [61.331257]
 [61.4491  ]], R is [[60.96064377]
 [61.01951599]
 [61.07806778]
 [61.13628006]
 [61.19412613]].
[2019-04-01 17:59:55,007] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 6.232519e-30], sum to 1.0000
[2019-04-01 17:59:55,008] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7930
[2019-04-01 17:59:55,030] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.383333333333333, 75.5, 0.0, 0.0, 26.0, 24.08926798775745, 5.370057730531147, 0.0, 1.0, 44.59213296506903], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1905000.0000, 
sim time next is 1905600.0000, 
raw observation next is [-7.466666666666667, 76.0, 0.0, 0.0, 26.0, 24.03339002610542, 5.354225899741626, 0.0, 1.0, 44.612947616513], 
processed observation next is [1.0, 0.043478260869565216, 0.25577100646352724, 0.76, 0.0, 0.0, 1.0, 0.7190557180150599, 0.053542258997416256, 0.0, 1.0, 0.3186639115465214], 
reward next is 0.6813, 
noisyNet noise sample is [array([0.06347277], dtype=float32), -0.52971256]. 
=============================================
[2019-04-01 18:00:01,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:00:01,951] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7852
[2019-04-01 18:00:01,985] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.733333333333333, 67.0, 130.5, 0.0, 26.0, 25.58289405272129, 7.856106045680036, 1.0, 1.0, 28.19191613373187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2204400.0000, 
sim time next is 2205000.0000, 
raw observation next is [-3.65, 66.5, 128.0, 0.0, 26.0, 25.51629239656855, 7.724356585164522, 1.0, 1.0, 28.11246030307131], 
processed observation next is [1.0, 0.5217391304347826, 0.3614958448753463, 0.665, 0.4266666666666667, 0.0, 1.0, 0.9308989137955069, 0.07724356585164523, 1.0, 1.0, 0.20080328787908078], 
reward next is 0.7992, 
noisyNet noise sample is [array([0.23619998], dtype=float32), -1.3966647]. 
=============================================
[2019-04-01 18:00:01,992] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[62.50148 ]
 [62.618378]
 [62.94795 ]
 [63.383507]
 [63.781376]], R is [[62.66365433]
 [62.83564758]
 [63.00475693]
 [63.17068481]
 [63.33391571]].
[2019-04-01 18:00:06,876] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6061030e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.4959139e-34
 1.0000000e+00 5.2659536e-23], sum to 1.0000
[2019-04-01 18:00:06,876] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8954
[2019-04-01 18:00:06,942] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.166666666666667, 86.83333333333333, 19.66666666666667, 0.0, 26.0, 25.49538042286367, 7.142283616588941, 1.0, 1.0, 39.5699778977317], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2016600.0000, 
sim time next is 2017200.0000, 
raw observation next is [-6.133333333333334, 86.66666666666667, 24.33333333333334, 0.0, 26.0, 25.40405225468037, 7.504635752057425, 1.0, 1.0, 40.13998290634654], 
processed observation next is [1.0, 0.34782608695652173, 0.2927054478301016, 0.8666666666666667, 0.08111111111111113, 0.0, 1.0, 0.9148646078114814, 0.07504635752057424, 1.0, 1.0, 0.286714163616761], 
reward next is 0.7133, 
noisyNet noise sample is [array([0.3721981], dtype=float32), 0.8020989]. 
=============================================
[2019-04-01 18:00:09,157] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:00:09,162] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0979
[2019-04-01 18:00:09,215] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 79.66666666666667, 0.0, 0.0, 26.0, 23.97818742441603, 5.31102931144235, 0.0, 1.0, 42.91041306125374], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2097600.0000, 
sim time next is 2098200.0000, 
raw observation next is [-6.7, 78.83333333333333, 0.0, 0.0, 26.0, 23.9308849033238, 5.300049432661133, 0.0, 1.0, 42.83615762034024], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.7883333333333333, 0.0, 0.0, 1.0, 0.7044121290462572, 0.05300049432661133, 0.0, 1.0, 0.3059725544310017], 
reward next is 0.6940, 
noisyNet noise sample is [array([-1.39851], dtype=float32), -0.09607117]. 
=============================================
[2019-04-01 18:00:22,766] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 6.1982759e-32 0.0000000e+00 7.6002063e-30
 1.0000000e+00 1.5968803e-15], sum to 1.0000
[2019-04-01 18:00:22,766] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6703
[2019-04-01 18:00:22,782] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.933333333333333, 27.0, 75.33333333333333, 766.6666666666667, 26.0, 24.93683504128057, 7.280729020705851, 0.0, 1.0, 19.77257508541602], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2472000.0000, 
sim time next is 2472600.0000, 
raw observation next is [3.116666666666667, 27.0, 72.66666666666667, 749.3333333333334, 26.0, 24.94198286254291, 7.329352061288134, 0.0, 1.0, 20.2079052676361], 
processed observation next is [0.0, 0.6086956521739131, 0.5489381348107111, 0.27, 0.24222222222222223, 0.8279926335174954, 1.0, 0.8488546946489873, 0.07329352061288134, 0.0, 1.0, 0.144342180483115], 
reward next is 0.8557, 
noisyNet noise sample is [array([0.8263109], dtype=float32), 0.9834614]. 
=============================================
[2019-04-01 18:00:35,670] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.376079e-32
 1.000000e+00 4.085778e-16], sum to 1.0000
[2019-04-01 18:00:35,670] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4844
[2019-04-01 18:00:35,720] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.016666666666667, 35.16666666666666, 82.66666666666667, 811.6666666666667, 26.0, 25.09441676128825, 7.098813949629772, 0.0, 1.0, 26.32403423813598], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2459400.0000, 
sim time next is 2460000.0000, 
raw observation next is [-1.733333333333333, 34.33333333333334, 84.33333333333334, 820.3333333333334, 26.0, 25.07522299235357, 7.05768596475266, 0.0, 1.0, 24.75345486611612], 
processed observation next is [0.0, 0.4782608695652174, 0.41458910433979695, 0.34333333333333343, 0.28111111111111114, 0.9064456721915286, 1.0, 0.8678889989076529, 0.07057685964752661, 0.0, 1.0, 0.17681039190082942], 
reward next is 0.8232, 
noisyNet noise sample is [array([-1.1386595], dtype=float32), -0.8051814]. 
=============================================
[2019-04-01 18:00:35,732] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[76.74662]
 [76.77486]
 [76.45363]
 [76.12258]
 [75.78111]], R is [[76.85496521]
 [76.89839172]
 [76.9315567 ]
 [76.93469238]
 [76.89257812]].
[2019-04-01 18:00:38,674] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.2650009e-38], sum to 1.0000
[2019-04-01 18:00:38,677] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2829
[2019-04-01 18:00:38,690] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 38.33333333333334, 0.0, 0.0, 26.0, 24.98786551967852, 5.970985626098776, 0.0, 1.0, 38.47496224508142], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2512200.0000, 
sim time next is 2512800.0000, 
raw observation next is [-1.7, 38.0, 0.0, 0.0, 26.0, 24.95281075271603, 5.925309212452166, 0.0, 1.0, 38.45106928366035], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.38, 0.0, 0.0, 1.0, 0.85040153610229, 0.05925309212452166, 0.0, 1.0, 0.27465049488328824], 
reward next is 0.7253, 
noisyNet noise sample is [array([-0.49418163], dtype=float32), 2.2878158]. 
=============================================
[2019-04-01 18:00:47,688] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 6.9291921e-34 0.0000000e+00 5.1694014e-31
 1.0000000e+00 1.7639992e-33], sum to 1.0000
[2019-04-01 18:00:47,689] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4518
[2019-04-01 18:00:47,735] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 63.66666666666667, 0.0, 0.0, 26.0, 25.01985441565989, 8.738536948523668, 0.0, 1.0, 49.18953550328365], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2665200.0000, 
sim time next is 2665800.0000, 
raw observation next is [-1.2, 64.0, 0.0, 0.0, 26.0, 25.02287801670641, 9.086255602598484, 0.0, 1.0, 50.25855603409192], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.64, 0.0, 0.0, 1.0, 0.8604111452437729, 0.09086255602598485, 0.0, 1.0, 0.3589896859577994], 
reward next is 0.6410, 
noisyNet noise sample is [array([1.1810426], dtype=float32), -0.81915194]. 
=============================================
[2019-04-01 18:00:48,427] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2385524e-22 1.9936753e-37 1.8403357e-21 1.1427970e-25 6.1697518e-33
 1.0000000e+00 1.0052825e-14], sum to 1.0000
[2019-04-01 18:00:48,427] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8293
[2019-04-01 18:00:48,455] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1333333333333334, 51.33333333333334, 18.33333333333333, 89.0, 26.0, 26.17989704165949, 9.681797762260828, 1.0, 1.0, 13.40884343629359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2654400.0000, 
sim time next is 2655000.0000, 
raw observation next is [-0.04999999999999999, 52.0, 7.0, 82.0, 26.0, 25.87508247304894, 9.280558877614057, 1.0, 1.0, 12.82659309969867], 
processed observation next is [1.0, 0.7391304347826086, 0.461218836565097, 0.52, 0.023333333333333334, 0.09060773480662983, 1.0, 0.9821546390069916, 0.09280558877614058, 1.0, 1.0, 0.09161852214070479], 
reward next is 0.9084, 
noisyNet noise sample is [array([-0.2157711], dtype=float32), -0.1643235]. 
=============================================
[2019-04-01 18:00:48,462] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[62.021645]
 [61.961685]
 [61.937645]
 [61.959045]
 [61.988533]], R is [[62.31074905]
 [62.59186554]
 [62.64479065]
 [62.70806885]
 [62.90143585]].
[2019-04-01 18:00:52,419] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5706059e-22 1.0878124e-35 1.7145649e-27 2.8368486e-23 1.8410242e-13
 1.0000000e+00 4.9378709e-17], sum to 1.0000
[2019-04-01 18:00:52,419] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8871
[2019-04-01 18:00:52,458] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 61.0, 144.6666666666667, 228.6666666666667, 26.0, 25.92415823698686, 8.811404533956418, 1.0, 1.0, 28.90985851470057], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2798400.0000, 
sim time next is 2799000.0000, 
raw observation next is [-4.5, 59.5, 152.0, 233.0, 26.0, 25.94179394063506, 8.91851653855567, 1.0, 1.0, 26.63215238907249], 
processed observation next is [1.0, 0.391304347826087, 0.3379501385041552, 0.595, 0.5066666666666667, 0.2574585635359116, 1.0, 0.9916848486621512, 0.0891851653855567, 1.0, 1.0, 0.19022965992194638], 
reward next is 0.8098, 
noisyNet noise sample is [array([2.3636777], dtype=float32), -1.4137548]. 
=============================================
[2019-04-01 18:00:52,465] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[63.789047]
 [64.73632 ]
 [65.8282  ]
 [67.097725]
 [68.35151 ]], R is [[63.16593933]
 [63.32778168]
 [63.47029495]
 [63.5926857 ]
 [63.69369507]].
[2019-04-01 18:00:53,528] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9992443e-34 1.9154511e-34 1.0486566e-18 1.2502895e-33 3.3735794e-26
 9.9999940e-01 5.4002271e-07], sum to 1.0000
[2019-04-01 18:00:53,528] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0839
[2019-04-01 18:00:53,586] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 169.1666666666667, 709.1666666666667, 26.0, 25.12425396911808, 9.357213455027363, 0.0, 1.0, 32.57361501685646], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2983200.0000, 
sim time next is 2983800.0000, 
raw observation next is [-3.0, 65.0, 157.3333333333333, 727.3333333333334, 26.0, 25.17454003871159, 9.42548604007913, 0.0, 1.0, 27.06686785913441], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.5244444444444443, 0.8036832412523021, 1.0, 0.8820771483873701, 0.0942548604007913, 0.0, 1.0, 0.19333477042238864], 
reward next is 0.8067, 
noisyNet noise sample is [array([-0.3291068], dtype=float32), 0.025861494]. 
=============================================
[2019-04-01 18:00:55,565] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.1273741e-19
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:00:55,570] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7929
[2019-04-01 18:00:55,602] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.833333333333334, 63.16666666666666, 0.0, 0.0, 26.0, 24.48903300272772, 5.652457578450793, 0.0, 1.0, 40.44159207953413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2782200.0000, 
sim time next is 2782800.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.43075894535517, 5.587333893148045, 0.0, 1.0, 40.54015425335979], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 1.0, 0.7758227064793098, 0.05587333893148046, 0.0, 1.0, 0.28957253038114134], 
reward next is 0.7104, 
noisyNet noise sample is [array([-0.22825734], dtype=float32), 0.18336019]. 
=============================================
[2019-04-01 18:01:02,302] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.2158677e-25
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:01:02,302] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6748
[2019-04-01 18:01:02,326] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.45027502191111, 7.159824190132421, 0.0, 1.0, 48.07957432166562], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3129000.0000, 
sim time next is 3129600.0000, 
raw observation next is [3.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.44554924244379, 7.327537243430271, 0.0, 1.0, 46.92435155232598], 
processed observation next is [1.0, 0.21739130434782608, 0.5549399815327793, 1.0, 0.0, 0.0, 1.0, 0.9207927489205413, 0.07327537243430271, 0.0, 1.0, 0.3351739396594713], 
reward next is 0.6648, 
noisyNet noise sample is [array([-0.2153855], dtype=float32), 0.7442471]. 
=============================================
[2019-04-01 18:01:04,138] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:01:04,141] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0941
[2019-04-01 18:01:04,154] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 26.0, 25.25527861534491, 15.01334542546342, 0.0, 1.0, 60.58760908798696], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3184200.0000, 
sim time next is 3184800.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 26.0, 25.42191307916013, 16.42855898337616, 0.0, 1.0, 43.63336344213037], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 1.0, 0.917416154165733, 0.1642855898337616, 0.0, 1.0, 0.31166688172950263], 
reward next is 0.6883, 
noisyNet noise sample is [array([2.5301073], dtype=float32), 1.1581644]. 
=============================================
[2019-04-01 18:01:17,247] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5456737e-36 0.0000000e+00 1.5514609e-31 6.7061197e-27 5.7048982e-23
 1.0000000e+00 3.2584844e-24], sum to 1.0000
[2019-04-01 18:01:17,250] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9123
[2019-04-01 18:01:17,277] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.45982617488659, 11.26140634190877, 1.0, 1.0, 40.79754342985804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3263400.0000, 
sim time next is 3264000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.37410532660305, 11.48425833219486, 1.0, 1.0, 39.19273620981934], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.65, 0.0, 0.0, 1.0, 0.9105864752290073, 0.1148425833219486, 1.0, 1.0, 0.27994811578442386], 
reward next is 0.1263, 
noisyNet noise sample is [array([0.9000841], dtype=float32), -1.2394872]. 
=============================================
[2019-04-01 18:01:17,298] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[20.003134]
 [19.479273]
 [18.880188]
 [18.756302]
 [19.593431]], R is [[20.57710838]
 [20.57536507]
 [20.81536674]
 [20.89281273]
 [20.90223312]].
[2019-04-01 18:01:20,723] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 9.4777949e-36 0.0000000e+00 2.6340287e-28
 1.0000000e+00 1.2828354e-38], sum to 1.0000
[2019-04-01 18:01:20,723] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2241
[2019-04-01 18:01:20,808] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.0, 80.0, 2.0, 94.0, 26.0, 24.90436748160916, 8.418168612748694, 1.0, 1.0, 71.32365373626175], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3310200.0000, 
sim time next is 3310800.0000, 
raw observation next is [-11.0, 81.33333333333334, 16.0, 144.3333333333333, 26.0, 25.45476840824205, 9.0806598872531, 1.0, 1.0, 57.37165942593726], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.8133333333333335, 0.05333333333333334, 0.15948434622467766, 1.0, 0.9221097726060071, 0.09080659887253101, 1.0, 1.0, 0.4097975673281233], 
reward next is 0.5902, 
noisyNet noise sample is [array([1.7795618], dtype=float32), -0.14385448]. 
=============================================
[2019-04-01 18:01:23,593] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-01 18:01:23,596] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 18:01:23,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:01:23,597] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 18:01:23,597] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:01:23,597] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 18:01:23,598] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:01:23,604] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run25
[2019-04-01 18:01:23,640] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run25
[2019-04-01 18:01:23,662] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run25
[2019-04-01 18:02:30,449] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.457829], dtype=float32), -0.9201605]
[2019-04-01 18:02:30,449] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-0.25, 57.5, 241.0, 241.0, 26.0, 25.20500604255618, 8.373248369703015, 1.0, 1.0, 15.75304733969952]
[2019-04-01 18:02:30,449] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 18:02:30,450] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.1879678e-34 0.0000000e+00 1.0057673e-26 2.4630620e-30 2.2186291e-24
 1.0000000e+00 9.0150072e-30], sampled 0.7534567872848019
[2019-04-01 18:02:42,633] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.457829], dtype=float32), -0.9201605]
[2019-04-01 18:02:42,633] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [6.8, 24.5, 95.0, 0.0, 26.0, 25.71423953612541, 8.722773919718486, 1.0, 1.0, 17.32327239294181]
[2019-04-01 18:02:42,634] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 18:02:42,634] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 2.0193097e-36 0.0000000e+00 1.5611856e-31
 1.0000000e+00 2.1731357e-33], sampled 0.3318160030011317
[2019-04-01 18:03:03,864] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.457829], dtype=float32), -0.9201605]
[2019-04-01 18:03:03,864] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.782821222333333, 79.43979641, 3.572261906666666, 296.8634878166666, 26.0, 25.17794360338808, 7.14123316283009, 1.0, 1.0, 32.46108337537337]
[2019-04-01 18:03:03,864] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 18:03:03,866] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1946903e-30
 1.0000000e+00 2.6758672e-27], sampled 0.7479561114465209
[2019-04-01 18:03:06,326] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 18:03:25,916] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 18:03:29,596] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 18:03:30,619] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2400000, evaluation results [2400000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 18:03:31,957] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4409883e-31 0.0000000e+00 6.0929754e-30 0.0000000e+00 1.1046635e-30
 1.0000000e+00 7.3603538e-32], sum to 1.0000
[2019-04-01 18:03:31,959] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7845
[2019-04-01 18:03:31,974] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666667, 69.16666666666667, 0.0, 0.0, 26.0, 24.79078481747239, 6.248739295507417, 0.0, 1.0, 42.58061295310287], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3388200.0000, 
sim time next is 3388800.0000, 
raw observation next is [-4.333333333333334, 67.33333333333334, 0.0, 0.0, 26.0, 24.70285317127484, 6.184429783251079, 0.0, 1.0, 41.81158610872585], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.6733333333333335, 0.0, 0.0, 1.0, 0.8146933101821199, 0.06184429783251079, 0.0, 1.0, 0.2986541864908989], 
reward next is 0.7013, 
noisyNet noise sample is [array([0.6693157], dtype=float32), -0.7860695]. 
=============================================
[2019-04-01 18:03:32,135] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.33578716e-24 3.03038517e-31 3.07561607e-25 9.40993677e-22
 9.99952316e-01 4.77409812e-05 1.06634685e-13], sum to 1.0000
[2019-04-01 18:03:32,137] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1517
[2019-04-01 18:03:32,151] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.666666666666667, 75.0, 50.66666666666667, 443.1666666666667, 26.0, 26.50237147391244, 16.83334503801939, 1.0, 1.0, 11.99580494445076], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3256800.0000, 
sim time next is 3257400.0000, 
raw observation next is [-3.833333333333333, 76.0, 42.33333333333334, 375.3333333333334, 26.0, 26.6690917963531, 16.84275780002744, 1.0, 1.0, 11.62813832512542], 
processed observation next is [1.0, 0.6956521739130435, 0.3564173591874424, 0.76, 0.14111111111111113, 0.4147329650092082, 1.0, 1.0955845423361572, 0.16842757800027439, 1.0, 1.0, 0.083058130893753], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7352123], dtype=float32), -1.3622667]. 
=============================================
[2019-04-01 18:03:33,867] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.7625869e-24 4.2243241e-33 1.3544919e-27 2.8709362e-22 6.7609338e-12
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:03:33,868] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4991
[2019-04-01 18:03:33,882] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.36692939162118, 9.92618153986795, 1.0, 1.0, 15.61847932993145], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3438000.0000, 
sim time next is 3438600.0000, 
raw observation next is [1.0, 79.00000000000001, 0.0, 0.0, 26.0, 25.30570313082845, 9.55846437912598, 1.0, 1.0, 16.33171835168273], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.7900000000000001, 0.0, 0.0, 1.0, 0.9008147329754931, 0.09558464379125979, 1.0, 1.0, 0.11665513108344808], 
reward next is 0.8833, 
noisyNet noise sample is [array([-0.5402339], dtype=float32), 0.5825555]. 
=============================================
[2019-04-01 18:03:38,924] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:03:38,936] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8929
[2019-04-01 18:03:38,955] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 63.66666666666667, 0.0, 0.0, 26.0, 24.77673151799807, 6.403576416454214, 0.0, 1.0, 41.81843227152428], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3390000.0000, 
sim time next is 3390600.0000, 
raw observation next is [-3.333333333333333, 61.83333333333333, 0.0, 0.0, 26.0, 24.80901956393799, 6.276015164834763, 0.0, 1.0, 42.1838057463272], 
processed observation next is [1.0, 0.21739130434782608, 0.37026777469990774, 0.6183333333333333, 0.0, 0.0, 1.0, 0.8298599377054272, 0.06276015164834763, 0.0, 1.0, 0.30131289818805146], 
reward next is 0.6987, 
noisyNet noise sample is [array([0.3970738], dtype=float32), 0.2894972]. 
=============================================
[2019-04-01 18:03:39,575] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6877983e-24 2.7351908e-32 5.9475634e-22 4.7785263e-30 2.2706239e-26
 9.9999189e-01 8.0523541e-06], sum to 1.0000
[2019-04-01 18:03:39,577] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9161
[2019-04-01 18:03:39,595] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.58950972770179, 6.481449655753747, 0.0, 1.0, 40.37410723399834], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3564000.0000, 
sim time next is 3564600.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.55960994872815, 6.382409774631324, 0.0, 1.0, 40.38401275453968], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 1.0, 0.7942299926754498, 0.06382409774631324, 0.0, 1.0, 0.2884572339609977], 
reward next is 0.7115, 
noisyNet noise sample is [array([0.0888693], dtype=float32), -0.8303242]. 
=============================================
[2019-04-01 18:03:41,028] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.648728e-25
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:03:41,032] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0984
[2019-04-01 18:03:41,046] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 75.0, 0.0, 0.0, 26.0, 25.29384108494448, 8.732088641423951, 0.0, 1.0, 43.26108358923486], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3804000.0000, 
sim time next is 3804600.0000, 
raw observation next is [-3.833333333333333, 76.0, 0.0, 0.0, 26.0, 25.2782194022881, 8.571796603885046, 0.0, 1.0, 43.38600748305776], 
processed observation next is [1.0, 0.0, 0.3564173591874424, 0.76, 0.0, 0.0, 1.0, 0.8968884860411573, 0.08571796603885046, 0.0, 1.0, 0.30990005345041255], 
reward next is 0.6901, 
noisyNet noise sample is [array([-0.02914292], dtype=float32), 0.82509047]. 
=============================================
[2019-04-01 18:03:41,591] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7022847e-17 6.5671743e-23 1.6582491e-11 8.6752321e-09 2.4138686e-20
 1.0000000e+00 6.8241683e-24], sum to 1.0000
[2019-04-01 18:03:41,594] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2862
[2019-04-01 18:03:41,604] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8333333333333334, 60.83333333333334, 111.0, 783.3333333333333, 26.0, 26.2771613900859, 12.45934788501414, 1.0, 1.0, 9.127888804030782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3495000.0000, 
sim time next is 3495600.0000, 
raw observation next is [1.0, 61.0, 112.0, 790.0, 26.0, 26.18194437920501, 12.57835556233808, 1.0, 1.0, 8.140335396255127], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.61, 0.37333333333333335, 0.8729281767955801, 1.0, 1.0259920541721443, 0.1257835556233808, 1.0, 1.0, 0.05814525283039376], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5329548], dtype=float32), 0.08707056]. 
=============================================
[2019-04-01 18:03:42,213] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 4.2025387e-36 0.0000000e+00 1.0896717e-30
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:03:42,217] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2635
[2019-04-01 18:03:42,231] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 80.16666666666667, 0.0, 0.0, 26.0, 25.80217559869912, 12.61116217595884, 0.0, 1.0, 30.26518041412823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3445800.0000, 
sim time next is 3446400.0000, 
raw observation next is [1.0, 81.33333333333334, 0.0, 0.0, 26.0, 25.80137041921532, 12.39572512895959, 0.0, 1.0, 28.64136132430467], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.8133333333333335, 0.0, 0.0, 1.0, 0.9716243456021887, 0.12395725128959591, 0.0, 1.0, 0.20458115231646193], 
reward next is 0.7954, 
noisyNet noise sample is [array([0.10014782], dtype=float32), 1.9014039]. 
=============================================
[2019-04-01 18:03:45,212] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 5.280237e-15], sum to 1.0000
[2019-04-01 18:03:45,212] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9581
[2019-04-01 18:03:45,243] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 69.0, 0.0, 0.0, 26.0, 25.221793226004, 8.524128070077035, 0.0, 1.0, 40.23996143664434], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3550800.0000, 
sim time next is 3551400.0000, 
raw observation next is [-3.0, 68.0, 0.0, 0.0, 26.0, 25.18895737480962, 8.387978463150482, 0.0, 1.0, 40.24027835131648], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.68, 0.0, 0.0, 1.0, 0.8841367678299454, 0.08387978463150482, 0.0, 1.0, 0.2874305596522606], 
reward next is 0.7126, 
noisyNet noise sample is [array([0.508669], dtype=float32), -1.6641275]. 
=============================================
[2019-04-01 18:03:48,640] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:03:48,642] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1785
[2019-04-01 18:03:48,662] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:03:48,663] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9872
[2019-04-01 18:03:48,676] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 42.0, 104.0, 792.0, 26.0, 25.19339181553778, 9.701859652082057, 0.0, 1.0, 13.33651283795604], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3594000.0000, 
sim time next is 3594600.0000, 
raw observation next is [-1.0, 42.0, 102.0, 788.0, 26.0, 25.18293779447967, 9.762297074387705, 0.0, 1.0, 14.43888243612209], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.34, 0.8707182320441988, 1.0, 0.88327682778281, 0.09762297074387705, 0.0, 1.0, 0.10313487454372922], 
reward next is 0.8969, 
noisyNet noise sample is [array([-1.069244], dtype=float32), 0.01155785]. 
=============================================
[2019-04-01 18:03:48,693] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.166666666666667, 65.83333333333334, 103.6666666666667, 703.3333333333334, 26.0, 25.87921707616386, 11.17068835909966, 0.0, 1.0, 29.52878957332267], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3577800.0000, 
sim time next is 3578400.0000, 
raw observation next is [-5.0, 65.0, 105.5, 717.0, 26.0, 25.81988477446307, 11.01018484379967, 0.0, 1.0, 27.73828731155301], 
processed observation next is [0.0, 0.43478260869565216, 0.32409972299168976, 0.65, 0.3516666666666667, 0.7922651933701658, 1.0, 0.9742692534947243, 0.11010184843799671, 0.0, 1.0, 0.19813062365395007], 
reward next is 0.8019, 
noisyNet noise sample is [array([-1.7002864], dtype=float32), -1.030521]. 
=============================================
[2019-04-01 18:03:49,337] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3905065e-12 1.4478016e-20 1.2953791e-10 1.4930306e-05 9.9998510e-01
 1.7158678e-08 1.0969543e-37], sum to 1.0000
[2019-04-01 18:03:49,337] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1482
[2019-04-01 18:03:49,356] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 34.66666666666667, 80.66666666666667, 661.3333333333334, 26.0, 26.34702927848431, 15.85197190482978, 1.0, 1.0, 10.7032733526184], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3945000.0000, 
sim time next is 3945600.0000, 
raw observation next is [-4.0, 34.0, 77.0, 630.0, 26.0, 25.75728848888492, 14.5106567523813, 1.0, 1.0, 9.986145481512649], 
processed observation next is [1.0, 0.6956521739130435, 0.3518005540166205, 0.34, 0.25666666666666665, 0.6961325966850829, 1.0, 0.9653269269835602, 0.14510656752381298, 1.0, 1.0, 0.07132961058223321], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.346309], dtype=float32), 0.22084709]. 
=============================================
[2019-04-01 18:04:00,149] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3343880e-27 6.3864081e-37 6.3373967e-26 1.1355555e-28 1.0755460e-11
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:04:00,150] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0692
[2019-04-01 18:04:00,169] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.36714186478635, 9.625957889682036, 0.0, 1.0, 39.23432094440516], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3896400.0000, 
sim time next is 3897000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.64356482022495, 9.310712733540095, 0.0, 1.0, 33.32651838883934], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 1.0, 0.9490806886035641, 0.09310712733540095, 0.0, 1.0, 0.238046559920281], 
reward next is 0.7620, 
noisyNet noise sample is [array([0.8589424], dtype=float32), -0.6536395]. 
=============================================
[2019-04-01 18:04:00,187] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[60.30003 ]
 [60.465656]
 [60.670193]
 [60.868317]
 [61.065224]], R is [[60.3419075 ]
 [60.45824051]
 [60.56374359]
 [60.68006134]
 [60.81646729]].
[2019-04-01 18:04:00,309] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:04:00,311] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2880
[2019-04-01 18:04:00,327] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.833333333333334, 76.0, 0.0, 0.0, 26.0, 24.87063764202765, 6.748566949835694, 0.0, 1.0, 42.88082102012696], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3822600.0000, 
sim time next is 3823200.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.87335056170414, 6.808453352697465, 0.0, 1.0, 42.7283675546986], 
processed observation next is [1.0, 0.2608695652173913, 0.32409972299168976, 0.77, 0.0, 0.0, 1.0, 0.8390500802434485, 0.06808453352697465, 0.0, 1.0, 0.3052026253907043], 
reward next is 0.6948, 
noisyNet noise sample is [array([-1.6555462], dtype=float32), -0.4757448]. 
=============================================
[2019-04-01 18:04:05,663] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3577661e-35
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:04:05,663] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5997
[2019-04-01 18:04:05,706] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.666666666666666, 43.66666666666667, 0.0, 0.0, 26.0, 25.49887414995625, 11.07400491941999, 1.0, 1.0, 42.18159782979443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3958800.0000, 
sim time next is 3959400.0000, 
raw observation next is [-6.833333333333334, 44.33333333333333, 0.0, 0.0, 26.0, 25.47432059078431, 11.12446077718757, 0.0, 1.0, 39.38942296467445], 
processed observation next is [1.0, 0.8260869565217391, 0.27331486611265005, 0.4433333333333333, 0.0, 0.0, 1.0, 0.9249029415406156, 0.1112446077718757, 0.0, 1.0, 0.2813530211762461], 
reward next is 0.7186, 
noisyNet noise sample is [array([-0.02621509], dtype=float32), 0.16656123]. 
=============================================
[2019-04-01 18:04:18,721] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 7.4935825e-35 1.1843216e-38 5.3818790e-21
 1.0000000e+00 7.6725694e-14], sum to 1.0000
[2019-04-01 18:04:18,725] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2014
[2019-04-01 18:04:18,760] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 35.0, 114.0, 774.0, 26.0, 25.45762375176356, 9.376685054578244, 0.0, 1.0, 17.14253575212392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4185000.0000, 
sim time next is 4185600.0000, 
raw observation next is [-1.333333333333333, 35.0, 114.8333333333333, 782.0, 26.0, 25.41418293699865, 9.282069107090175, 0.0, 1.0, 16.10546378409397], 
processed observation next is [0.0, 0.43478260869565216, 0.42566943674976926, 0.35, 0.38277777777777766, 0.8640883977900552, 1.0, 0.9163118481426642, 0.09282069107090175, 0.0, 1.0, 0.11503902702924265], 
reward next is 0.8850, 
noisyNet noise sample is [array([-0.6624588], dtype=float32), -1.9350421]. 
=============================================
[2019-04-01 18:04:30,488] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:04:30,489] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9762
[2019-04-01 18:04:30,503] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.25, 72.0, 0.0, 0.0, 26.0, 25.20401491712064, 9.788723119315017, 0.0, 1.0, 45.86983719444039], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4488600.0000, 
sim time next is 4489200.0000, 
raw observation next is [-0.3, 72.0, 0.0, 0.0, 26.0, 25.22936605393692, 9.932582569668561, 0.0, 1.0, 44.50266653355757], 
processed observation next is [1.0, 1.0, 0.4542936288088643, 0.72, 0.0, 0.0, 1.0, 0.889909436276703, 0.09932582569668562, 0.0, 1.0, 0.31787618952541125], 
reward next is 0.6821, 
noisyNet noise sample is [array([0.4038381], dtype=float32), 0.008956464]. 
=============================================
[2019-04-01 18:04:37,777] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.8076272e-25
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:04:37,780] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2670
[2019-04-01 18:04:37,796] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.833333333333333, 50.0, 0.0, 0.0, 26.0, 25.22558301708008, 6.770715643448504, 0.0, 1.0, 37.98534620133867], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4938600.0000, 
sim time next is 4939200.0000, 
raw observation next is [-2.0, 50.0, 0.0, 0.0, 26.0, 25.19305184135299, 6.683172115726975, 0.0, 1.0, 38.00857921473193], 
processed observation next is [1.0, 0.17391304347826086, 0.40720221606648205, 0.5, 0.0, 0.0, 1.0, 0.8847216916218557, 0.06683172115726975, 0.0, 1.0, 0.2714898515337995], 
reward next is 0.7285, 
noisyNet noise sample is [array([0.38607922], dtype=float32), -1.1873345]. 
=============================================
[2019-04-01 18:04:44,996] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1522045e-24
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:04:44,996] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2965
[2019-04-01 18:04:45,025] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.1, 92.5, 0.0, 0.0, 26.0, 24.00121307699986, 5.507850012009722, 0.0, 1.0, 41.23005981283696], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4775400.0000, 
sim time next is 4776000.0000, 
raw observation next is [-6.133333333333333, 92.66666666666667, 0.0, 0.0, 26.0, 23.96390534113761, 5.472309070477851, 0.0, 1.0, 41.27526464057774], 
processed observation next is [0.0, 0.2608695652173913, 0.2927054478301016, 0.9266666666666667, 0.0, 0.0, 1.0, 0.7091293344482301, 0.054723090704778515, 0.0, 1.0, 0.29482331886126956], 
reward next is 0.7052, 
noisyNet noise sample is [array([0.6820157], dtype=float32), -1.1090367]. 
=============================================
[2019-04-01 18:04:45,037] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[68.59706 ]
 [68.64193 ]
 [68.68992 ]
 [68.773056]
 [68.85701 ]], R is [[68.56440735]
 [68.58425903]
 [68.60427094]
 [68.6244812 ]
 [68.64492798]].
[2019-04-01 18:04:45,136] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:04:45,137] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3688
[2019-04-01 18:04:45,154] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.76723673988247, 6.179399661579282, 0.0, 1.0, 39.03505196353391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4857000.0000, 
sim time next is 4857600.0000, 
raw observation next is [-3.666666666666667, 67.33333333333334, 0.0, 0.0, 26.0, 24.76572389002897, 6.123574871819389, 0.0, 1.0, 39.07297380522721], 
processed observation next is [0.0, 0.21739130434782608, 0.3610341643582641, 0.6733333333333335, 0.0, 0.0, 1.0, 0.8236748414327099, 0.061235748718193886, 0.0, 1.0, 0.2790926700373372], 
reward next is 0.7209, 
noisyNet noise sample is [array([0.08218691], dtype=float32), 0.41463313]. 
=============================================
[2019-04-01 18:04:46,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:04:46,726] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:04:46,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run19
[2019-04-01 18:04:47,234] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:04:47,234] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:04:47,244] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run19
[2019-04-01 18:04:47,539] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:04:47,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:04:47,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run19
[2019-04-01 18:04:48,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:04:48,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4250
[2019-04-01 18:04:48,636] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333333, 45.33333333333334, 0.0, 0.0, 26.0, 25.56425889955763, 7.463378930272913, 0.0, 1.0, 25.3445918094823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4929600.0000, 
sim time next is 4930200.0000, 
raw observation next is [-0.5, 46.5, 0.0, 0.0, 26.0, 25.48333789430233, 7.229461021723381, 0.0, 1.0, 24.98354258909534], 
processed observation next is [1.0, 0.043478260869565216, 0.44875346260387816, 0.465, 0.0, 0.0, 1.0, 0.9261911277574758, 0.07229461021723381, 0.0, 1.0, 0.17845387563639528], 
reward next is 0.8215, 
noisyNet noise sample is [array([-0.9257731], dtype=float32), 0.021893386]. 
=============================================
[2019-04-01 18:04:55,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:04:55,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:04:55,844] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run19
[2019-04-01 18:04:57,195] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:04:57,196] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:04:57,208] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run19
[2019-04-01 18:04:57,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:04:57,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:04:57,858] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run19
[2019-04-01 18:04:58,060] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 5.386511e-32 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:04:58,064] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0533
[2019-04-01 18:04:58,095] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.110223024625157e-16, 53.0, 99.5, 560.5, 26.0, 26.08532607021662, 11.57558934913854, 1.0, 1.0, 17.18267821854647], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5042400.0000, 
sim time next is 5043000.0000, 
raw observation next is [0.5000000000000001, 50.0, 102.0, 588.0, 26.0, 26.29970011183971, 12.07674530252748, 1.0, 1.0, 15.90160102960261], 
processed observation next is [1.0, 0.34782608695652173, 0.4764542936288089, 0.5, 0.34, 0.6497237569060773, 1.0, 1.042814301691387, 0.12076745302527479, 1.0, 1.0, 0.1135828644971615], 
reward next is 0.0557, 
noisyNet noise sample is [array([1.0560681], dtype=float32), 0.5691063]. 
=============================================
[2019-04-01 18:04:58,103] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[27.094198]
 [28.90115 ]
 [30.339043]
 [33.184513]
 [37.152164]], R is [[26.0630703 ]
 [26.0494709 ]
 [26.36286354]
 [26.94460869]
 [27.52088928]].
[2019-04-01 18:04:58,407] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1167481e-17 3.7795083e-22 1.0924228e-15 1.0000000e+00 5.3692935e-19
 8.3472743e-11 1.6533613e-17], sum to 1.0000
[2019-04-01 18:04:58,408] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4517
[2019-04-01 18:04:58,430] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.666666666666666, 25.33333333333333, 115.6666666666667, 853.1666666666667, 26.0, 26.60914698292355, 15.70266059799077, 1.0, 1.0, 1.982746447757685], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4974000.0000, 
sim time next is 4974600.0000, 
raw observation next is [7.833333333333334, 25.66666666666667, 114.3333333333333, 846.3333333333333, 26.0, 25.96389281098726, 14.88403578735121, 1.0, 1.0, 1.880975919552045], 
processed observation next is [1.0, 0.5652173913043478, 0.6795937211449677, 0.2566666666666667, 0.381111111111111, 0.9351749539594842, 1.0, 0.9948418301410372, 0.1488403578735121, 1.0, 1.0, 0.013435542282514607], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.20103964], dtype=float32), 0.4611634]. 
=============================================
[2019-04-01 18:04:59,263] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3611263e-37 0.0000000e+00 3.0934767e-37 2.9384047e-35 8.3166639e-34
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:04:59,263] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9538
[2019-04-01 18:04:59,275] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 79.0, 0.0, 0.0, 26.0, 23.92378328560668, 5.335445358874445, 0.0, 1.0, 43.05563168223746], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 100800.0000, 
sim time next is 101400.0000, 
raw observation next is [-3.666666666666667, 78.16666666666667, 0.0, 0.0, 26.0, 23.90560152709772, 5.341535842104285, 0.0, 1.0, 43.08990756538505], 
processed observation next is [1.0, 0.17391304347826086, 0.3610341643582641, 0.7816666666666667, 0.0, 0.0, 1.0, 0.700800218156817, 0.053415358421042856, 0.0, 1.0, 0.3077850540384646], 
reward next is 0.6922, 
noisyNet noise sample is [array([-0.27583867], dtype=float32), 0.6897498]. 
=============================================
[2019-04-01 18:04:59,690] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:04:59,690] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5097
[2019-04-01 18:04:59,704] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 75.0, 0.0, 0.0, 26.0, 23.32203436827253, 5.372341356540247, 0.0, 1.0, 44.58459147450695], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 108000.0000, 
sim time next is 108600.0000, 
raw observation next is [-6.800000000000001, 73.83333333333333, 0.0, 0.0, 26.0, 23.30600022531157, 5.378193824931105, 0.0, 1.0, 44.73993766394146], 
processed observation next is [1.0, 0.2608695652173913, 0.2742382271468144, 0.7383333333333333, 0.0, 0.0, 1.0, 0.6151428893302244, 0.05378193824931105, 0.0, 1.0, 0.3195709833138676], 
reward next is 0.6804, 
noisyNet noise sample is [array([0.23661797], dtype=float32), -0.12706831]. 
=============================================
[2019-04-01 18:04:59,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:04:59,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:04:59,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run19
[2019-04-01 18:05:01,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:05:01,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:05:01,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run19
[2019-04-01 18:05:01,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:05:01,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:05:01,166] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run19
[2019-04-01 18:05:01,259] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1746934e-22 1.0164798e-31 2.4645346e-25 7.5471483e-11 5.5044268e-27
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:05:01,261] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3566
[2019-04-01 18:05:01,294] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:05:01,294] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:05:01,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run19
[2019-04-01 18:05:01,330] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 77.66666666666667, 185.0, 16.83333333333333, 26.0, 25.39580372091961, 7.759755879242945, 1.0, 1.0, 41.35973919407636], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 127200.0000, 
sim time next is 127800.0000, 
raw observation next is [-8.1, 73.5, 184.0, 13.0, 26.0, 25.42389933114709, 7.816175757898184, 1.0, 1.0, 41.7807023385962], 
processed observation next is [1.0, 0.4782608695652174, 0.23822714681440446, 0.735, 0.6133333333333333, 0.014364640883977901, 1.0, 0.9176999044495844, 0.07816175757898183, 1.0, 1.0, 0.29843358813283], 
reward next is 0.7016, 
noisyNet noise sample is [array([1.3797638], dtype=float32), 0.6905272]. 
=============================================
[2019-04-01 18:05:02,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:05:02,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:05:02,076] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run19
[2019-04-01 18:05:02,106] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:05:02,107] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:05:02,119] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run19
[2019-04-01 18:05:02,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:05:02,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:05:02,776] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run19
[2019-04-01 18:05:03,944] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:05:03,944] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:05:03,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run19
[2019-04-01 18:05:04,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:05:04,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:05:04,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run19
[2019-04-01 18:05:04,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:05:04,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:05:04,556] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run19
[2019-04-01 18:05:13,986] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7628158e-33 0.0000000e+00 5.4783121e-32 1.6124032e-38 4.4106778e-29
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:05:13,987] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3101
[2019-04-01 18:05:14,005] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.866666666666667, 77.00000000000001, 0.0, 0.0, 26.0, 24.09360052418762, 5.583368549517307, 0.0, 1.0, 43.79641638575957], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 259800.0000, 
sim time next is 260400.0000, 
raw observation next is [-5.233333333333334, 75.0, 0.0, 0.0, 26.0, 24.05297573532603, 5.553595701555852, 0.0, 1.0, 43.85559265782764], 
processed observation next is [1.0, 0.0, 0.31763619575253926, 0.75, 0.0, 0.0, 1.0, 0.7218536764751471, 0.055535957015558524, 0.0, 1.0, 0.31325423327019747], 
reward next is 0.6867, 
noisyNet noise sample is [array([-0.3294685], dtype=float32), -0.34667796]. 
=============================================
[2019-04-01 18:05:19,296] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.1650366e-19 8.4331634e-27 6.9273231e-22 5.3119034e-02 2.4282859e-10
 9.4688100e-01 5.5492346e-30], sum to 1.0000
[2019-04-01 18:05:19,299] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8764
[2019-04-01 18:05:19,366] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.3, 61.0, 148.0, 406.3333333333334, 26.0, 25.71878530619163, 9.099781068638244, 1.0, 1.0, 35.04437982031806], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 130200.0000, 
sim time next is 130800.0000, 
raw observation next is [-8.2, 61.0, 139.0, 504.6666666666667, 26.0, 25.77558408749712, 8.977802237684543, 1.0, 1.0, 30.64234479270688], 
processed observation next is [1.0, 0.5217391304347826, 0.23545706371191139, 0.61, 0.4633333333333333, 0.5576427255985267, 1.0, 0.9679405839281598, 0.08977802237684543, 1.0, 1.0, 0.2188738913764777], 
reward next is 0.7811, 
noisyNet noise sample is [array([1.7934784], dtype=float32), -0.3892451]. 
=============================================
[2019-04-01 18:05:21,319] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.8262559e-29 1.3784956e-35 5.1277554e-27 2.6569272e-20 2.1358629e-12
 1.0000000e+00 3.8521298e-28], sum to 1.0000
[2019-04-01 18:05:21,319] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3173
[2019-04-01 18:05:21,359] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.8, 75.83333333333334, 0.0, 0.0, 26.0, 24.7328828382863, 7.007178075900057, 0.0, 1.0, 46.92430550766885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 334200.0000, 
sim time next is 334800.0000, 
raw observation next is [-12.8, 77.0, 0.0, 0.0, 26.0, 24.63905041576356, 6.802947343316386, 0.0, 1.0, 46.96746551769535], 
processed observation next is [1.0, 0.9130434782608695, 0.1080332409972299, 0.77, 0.0, 0.0, 1.0, 0.8055786308233657, 0.06802947343316386, 0.0, 1.0, 0.3354818965549668], 
reward next is 0.6645, 
noisyNet noise sample is [array([-1.1354138], dtype=float32), -0.49297595]. 
=============================================
[2019-04-01 18:05:24,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 8.231631e-27 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:05:24,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7353
[2019-04-01 18:05:24,322] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 61.0, 0.0, 0.0, 26.0, 25.16619452993676, 7.896965198161332, 1.0, 1.0, 49.9684213087816], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 151200.0000, 
sim time next is 151800.0000, 
raw observation next is [-7.383333333333333, 61.5, 0.0, 0.0, 26.0, 24.83705048441465, 8.021722588922954, 1.0, 1.0, 80.81140336919474], 
processed observation next is [1.0, 0.782608695652174, 0.25807940904893817, 0.615, 0.0, 0.0, 1.0, 0.8338643549163786, 0.08021722588922954, 1.0, 1.0, 0.5772243097799624], 
reward next is 0.4228, 
noisyNet noise sample is [array([-1.0503117], dtype=float32), 0.90251875]. 
=============================================
[2019-04-01 18:05:32,835] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 3.240691e-27 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:05:32,836] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1472
[2019-04-01 18:05:32,974] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 44.00000000000001, 91.0, 673.3333333333333, 26.0, 25.1420539830292, 8.50609184428513, 1.0, 1.0, 61.04485676149058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 306600.0000, 
sim time next is 307200.0000, 
raw observation next is [-9.5, 44.0, 93.0, 652.1666666666666, 26.0, 24.71441738143464, 10.26278709592938, 1.0, 1.0, 84.48883754259397], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.31, 0.7206261510128913, 1.0, 0.8163453402049485, 0.1026278709592938, 1.0, 1.0, 0.6034916967328141], 
reward next is 0.2914, 
noisyNet noise sample is [array([0.01403919], dtype=float32), 1.9406177]. 
=============================================
[2019-04-01 18:05:39,041] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:05:39,044] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0650
[2019-04-01 18:05:39,068] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.1, 77.16666666666666, 0.0, 0.0, 26.0, 21.98774895550582, 9.335434879200799, 0.0, 1.0, 47.95836198641961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 366600.0000, 
sim time next is 367200.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.88187131832281, 9.710776902133675, 0.0, 1.0, 48.08490053211118], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 1.0, 0.4116959026175441, 0.09710776902133675, 0.0, 1.0, 0.34346357522936555], 
reward next is 0.6565, 
noisyNet noise sample is [array([0.38990453], dtype=float32), -1.1666183]. 
=============================================
[2019-04-01 18:05:43,318] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6778037e-32 0.0000000e+00 8.5382250e-32 4.0667844e-27 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:05:43,318] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4452
[2019-04-01 18:05:43,368] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 36.66666666666667, 17.5, 338.6666666666667, 26.0, 26.06656538947232, 9.480443527527983, 1.0, 1.0, 20.34535050721596], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 405600.0000, 
sim time next is 406200.0000, 
raw observation next is [-8.9, 36.33333333333333, 14.0, 274.3333333333334, 26.0, 25.94934270071529, 9.256259813531926, 1.0, 1.0, 19.23149579580592], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.3633333333333333, 0.04666666666666667, 0.3031307550644568, 1.0, 0.9927632429593274, 0.09256259813531927, 1.0, 1.0, 0.13736782711289944], 
reward next is 0.8626, 
noisyNet noise sample is [array([1.2698822], dtype=float32), 2.2113378]. 
=============================================
[2019-04-01 18:05:46,453] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:05:46,453] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4513
[2019-04-01 18:05:46,516] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.933333333333333, 59.33333333333334, 170.3333333333333, 94.16666666666666, 26.0, 24.84355168063693, 7.016977934327801, 0.0, 1.0, 48.23859007751243], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 652800.0000, 
sim time next is 653400.0000, 
raw observation next is [-1.75, 59.5, 182.0, 93.0, 26.0, 24.88898493518173, 7.201401710898502, 0.0, 1.0, 45.54430091384223], 
processed observation next is [0.0, 0.5652173913043478, 0.4141274238227147, 0.595, 0.6066666666666667, 0.10276243093922652, 1.0, 0.8412835621688183, 0.07201401710898502, 0.0, 1.0, 0.32531643509887304], 
reward next is 0.6747, 
noisyNet noise sample is [array([-1.1802568], dtype=float32), -0.074950136]. 
=============================================
[2019-04-01 18:05:47,127] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:05:47,127] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7060
[2019-04-01 18:05:47,178] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.96126681353896, 6.942348243166244, 0.0, 1.0, 33.19165155336239], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 667800.0000, 
sim time next is 668400.0000, 
raw observation next is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.92264770785238, 6.843835196902293, 0.0, 1.0, 34.75135707902652], 
processed observation next is [0.0, 0.7391304347826086, 0.42936288088642666, 0.57, 0.0, 0.0, 1.0, 0.8460925296931973, 0.06843835196902293, 0.0, 1.0, 0.24822397913590372], 
reward next is 0.7518, 
noisyNet noise sample is [array([-0.3600131], dtype=float32), -0.04305878]. 
=============================================
[2019-04-01 18:06:00,425] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.6317653e-31 0.0000000e+00 1.8601052e-34 1.4623802e-37 7.3149278e-07
 9.9999917e-01 9.6290144e-08], sum to 1.0000
[2019-04-01 18:06:00,426] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0703
[2019-04-01 18:06:00,472] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.483333333333333, 65.0, 96.33333333333333, 12.66666666666666, 26.0, 24.95357090854452, 6.705213665792473, 0.0, 1.0, 45.81201685783142], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 643800.0000, 
sim time next is 644400.0000, 
raw observation next is [-3.4, 65.0, 94.5, 19.0, 26.0, 24.97163781890369, 6.68129792443307, 0.0, 1.0, 38.77652696981512], 
processed observation next is [0.0, 0.4782608695652174, 0.368421052631579, 0.65, 0.315, 0.020994475138121547, 1.0, 0.8530911169862415, 0.0668129792443307, 0.0, 1.0, 0.2769751926415366], 
reward next is 0.7230, 
noisyNet noise sample is [array([1.7934543], dtype=float32), -0.28691018]. 
=============================================
[2019-04-01 18:06:07,894] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:06:07,894] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1367
[2019-04-01 18:06:07,909] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.41666666666667, 87.16666666666667, 104.0, 0.0, 26.0, 26.69584431791693, 14.86731238650693, 1.0, 1.0, 7.833752852460548], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 989400.0000, 
sim time next is 990000.0000, 
raw observation next is [11.6, 86.0, 108.0, 0.0, 26.0, 26.72945854382426, 14.94440245783027, 1.0, 1.0, 7.444084403984705], 
processed observation next is [1.0, 0.4782608695652174, 0.7839335180055402, 0.86, 0.36, 0.0, 1.0, 1.1042083634034658, 0.1494440245783027, 1.0, 1.0, 0.05317203145703361], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.36793685], dtype=float32), 2.251911]. 
=============================================
[2019-04-01 18:06:07,912] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[65.831   ]
 [65.7885  ]
 [65.76465 ]
 [66.030594]
 [66.37542 ]], R is [[65.21124268]
 [64.55912781]
 [63.91353607]
 [63.27440262]
 [62.64165878]].
[2019-04-01 18:06:11,295] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0155554e-04 1.2023330e-15 7.6729614e-01 2.3069736e-01 1.0691318e-16
 1.8048268e-03 2.5925910e-16], sum to 1.0000
[2019-04-01 18:06:11,297] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4801
[2019-04-01 18:06:11,301] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [18.38333333333334, 55.66666666666667, 174.3333333333333, 105.6666666666666, 25.5, 25.92562871743232, 18.61772080101177, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1084200.0000, 
sim time next is 1084800.0000, 
raw observation next is [18.46666666666667, 55.33333333333334, 172.6666666666667, 52.83333333333332, 25.0, 26.52796873213384, 20.05087024732086, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.9741458910433982, 0.5533333333333335, 0.5755555555555557, 0.058379373848987094, 0.8571428571428571, 1.0754241045905484, 0.2005087024732086, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6747805], dtype=float32), -1.1759799]. 
=============================================
[2019-04-01 18:06:18,372] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:06:18,381] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3931
[2019-04-01 18:06:18,391] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.633333333333334, 92.66666666666667, 0.0, 0.0, 26.0, 25.69071480330167, 12.14715583106736, 0.0, 1.0, 26.70206861852481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1300800.0000, 
sim time next is 1301400.0000, 
raw observation next is [3.55, 92.5, 0.0, 0.0, 26.0, 25.57351086428708, 11.69434889615898, 0.0, 1.0, 23.6621400596233], 
processed observation next is [1.0, 0.043478260869565216, 0.5609418282548477, 0.925, 0.0, 0.0, 1.0, 0.9390729806124399, 0.11694348896158979, 0.0, 1.0, 0.16901528614016645], 
reward next is 0.8310, 
noisyNet noise sample is [array([-0.5601636], dtype=float32), 0.7097331]. 
=============================================
[2019-04-01 18:06:21,985] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2779852e-23
 1.0000000e+00 6.8457900e-22], sum to 1.0000
[2019-04-01 18:06:21,985] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3905
[2019-04-01 18:06:22,024] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.38222781095267, 12.211478533319, 0.0, 1.0, 40.3349260812267], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1375200.0000, 
sim time next is 1375800.0000, 
raw observation next is [0.4166666666666667, 95.83333333333333, 0.0, 0.0, 26.0, 25.37636065252915, 11.23950240914039, 0.0, 1.0, 44.70808664546303], 
processed observation next is [1.0, 0.9565217391304348, 0.47414589104339805, 0.9583333333333333, 0.0, 0.0, 1.0, 0.9109086646470216, 0.11239502409140391, 0.0, 1.0, 0.31934347603902163], 
reward next is 0.6807, 
noisyNet noise sample is [array([0.40256506], dtype=float32), 0.34333137]. 
=============================================
[2019-04-01 18:06:29,160] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.7191994e-19], sum to 1.0000
[2019-04-01 18:06:29,165] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2779
[2019-04-01 18:06:29,178] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 26.0, 25.61768133722214, 13.29802114758795, 0.0, 1.0, 25.09355457213042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1137000.0000, 
sim time next is 1137600.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.61674411820032, 13.39088008642608, 0.0, 1.0, 25.63526458686359], 
processed observation next is [0.0, 0.17391304347826086, 0.7700831024930749, 0.77, 0.0, 0.0, 1.0, 0.9452491597429028, 0.1339088008642608, 0.0, 1.0, 0.18310903276331136], 
reward next is 0.8169, 
noisyNet noise sample is [array([-1.8167691], dtype=float32), -0.6916283]. 
=============================================
[2019-04-01 18:06:40,244] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:06:40,244] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4192
[2019-04-01 18:06:40,262] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 78.0, 0.0, 26.0, 25.82984254388524, 10.18871543782587, 1.0, 1.0, 9.780271869611129], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1421400.0000, 
sim time next is 1422000.0000, 
raw observation next is [0.0, 95.0, 81.0, 0.0, 26.0, 25.82438530310144, 10.04050257857569, 1.0, 1.0, 9.208081434270632], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.27, 0.0, 1.0, 0.9749121861573486, 0.1004050257857569, 1.0, 1.0, 0.06577201024479022], 
reward next is 0.9180, 
noisyNet noise sample is [array([0.5912584], dtype=float32), -1.356855]. 
=============================================
[2019-04-01 18:06:40,273] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[51.167004]
 [50.890472]
 [50.69497 ]
 [50.16769 ]
 [49.56864 ]], R is [[51.72462463]
 [52.06203461]
 [52.38294601]
 [52.66601562]
 [52.89383698]].
[2019-04-01 18:07:04,002] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4157300e-36
 9.9999976e-01 2.1995521e-07], sum to 1.0000
[2019-04-01 18:07:04,002] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2917
[2019-04-01 18:07:04,016] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 23.22520810206773, 5.459989798774473, 0.0, 1.0, 46.49461391540735], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1842000.0000, 
sim time next is 1842600.0000, 
raw observation next is [-6.7, 78.0, 9.666666666666664, 0.0, 26.0, 23.19927956750214, 5.479545013817945, 0.0, 1.0, 46.44959172478038], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.032222222222222215, 0.0, 1.0, 0.599897081071734, 0.05479545013817945, 0.0, 1.0, 0.3317827980341456], 
reward next is 0.6682, 
noisyNet noise sample is [array([-0.88896155], dtype=float32), -0.10092416]. 
=============================================
[2019-04-01 18:07:04,788] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2290685e-38 0.0000000e+00 2.6455117e-33 0.0000000e+00 6.8910564e-17
 1.0000000e+00 1.1381251e-24], sum to 1.0000
[2019-04-01 18:07:04,790] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2156
[2019-04-01 18:07:04,836] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 83.0, 15.0, 0.0, 26.0, 25.13106740912202, 7.529826652535831, 0.0, 1.0, 29.8933219076189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1875600.0000, 
sim time next is 1876200.0000, 
raw observation next is [-4.583333333333333, 83.5, 10.33333333333333, 0.0, 26.0, 25.08836918333874, 7.36114621737471, 0.0, 1.0, 33.33997523775795], 
processed observation next is [0.0, 0.7391304347826086, 0.3356417359187443, 0.835, 0.03444444444444444, 0.0, 1.0, 0.8697670261912488, 0.0736114621737471, 0.0, 1.0, 0.23814268026969962], 
reward next is 0.7619, 
noisyNet noise sample is [array([1.1776059], dtype=float32), -0.31785828]. 
=============================================
[2019-04-01 18:07:12,046] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-01 18:07:12,046] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 18:07:12,047] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:07:12,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run26
[2019-04-01 18:07:12,073] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 18:07:12,073] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:07:12,079] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 18:07:12,079] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:07:12,724] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run26
[2019-04-01 18:07:12,798] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run26
[2019-04-01 18:07:37,126] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.292543], dtype=float32), -0.7819525]
[2019-04-01 18:07:37,126] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-13.4, 59.5, 0.0, 0.0, 26.0, 23.03231739670383, 7.024216876625803, 0.0, 1.0, 44.83956793170065]
[2019-04-01 18:07:37,127] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 18:07:37,128] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4979297e-38
 1.0000000e+00 1.9039280e-26], sampled 0.14131633116459252
[2019-04-01 18:08:23,633] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.292543], dtype=float32), -0.7819525]
[2019-04-01 18:08:23,634] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.1, 36.0, 125.0, 714.5, 26.0, 25.03741844395767, 7.940624514121197, 0.0, 1.0, 25.66481519622911]
[2019-04-01 18:08:23,634] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 18:08:23,634] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 3.525099e-34
 1.000000e+00 8.203352e-13], sampled 0.786964562170701
[2019-04-01 18:08:46,019] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.292543], dtype=float32), -0.7819525]
[2019-04-01 18:08:46,020] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.840930199666666, 67.34020314333334, 0.0, 0.0, 26.0, 24.19342574227841, 5.617685222120855, 0.0, 1.0, 42.09620536659369]
[2019-04-01 18:08:46,020] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 18:08:46,021] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 7.6627456e-28], sampled 0.8768482199891737
[2019-04-01 18:08:47,858] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.292543], dtype=float32), -0.7819525]
[2019-04-01 18:08:47,858] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [13.2, 48.0, 0.0, 0.0, 26.0, 26.58282835632675, 19.34988838667018, 0.0, 1.0, 4.940573864097801]
[2019-04-01 18:08:47,859] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 18:08:47,859] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 3.5776194e-35], sampled 0.6435168797728184
[2019-04-01 18:08:54,320] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 18:09:13,082] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 18:09:18,015] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 18:09:19,038] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2500000, evaluation results [2500000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 18:09:22,562] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.9404104e-36
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:09:22,562] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2427
[2019-04-01 18:09:22,605] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.600000000000001, 83.0, 91.00000000000001, 0.0, 26.0, 25.60118328246767, 7.490125447136634, 1.0, 1.0, 31.9050158500277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2023800.0000, 
sim time next is 2024400.0000, 
raw observation next is [-5.6, 83.0, 96.5, 0.0, 26.0, 25.61594106939913, 7.50493215413709, 1.0, 1.0, 32.08242701349762], 
processed observation next is [1.0, 0.43478260869565216, 0.30747922437673136, 0.83, 0.32166666666666666, 0.0, 1.0, 0.9451344384855902, 0.0750493215413709, 1.0, 1.0, 0.22916019295355444], 
reward next is 0.7708, 
noisyNet noise sample is [array([0.9947659], dtype=float32), -0.96771246]. 
=============================================
[2019-04-01 18:09:24,277] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:09:24,278] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0962
[2019-04-01 18:09:24,374] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.166666666666667, 86.83333333333333, 19.66666666666667, 0.0, 26.0, 25.49538042286367, 7.142283616588941, 1.0, 1.0, 39.5699778977317], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2016600.0000, 
sim time next is 2017200.0000, 
raw observation next is [-6.133333333333334, 86.66666666666667, 24.33333333333334, 0.0, 26.0, 25.40405225468037, 7.504635752057425, 1.0, 1.0, 40.13998290634654], 
processed observation next is [1.0, 0.34782608695652173, 0.2927054478301016, 0.8666666666666667, 0.08111111111111113, 0.0, 1.0, 0.9148646078114814, 0.07504635752057424, 1.0, 1.0, 0.286714163616761], 
reward next is 0.7133, 
noisyNet noise sample is [array([-1.1798172], dtype=float32), -1.2074816]. 
=============================================
[2019-04-01 18:09:45,169] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:09:45,170] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6545
[2019-04-01 18:09:45,187] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 47.0, 182.5, 58.0, 26.0, 25.81671399427265, 7.511448783223147, 1.0, 1.0, 12.09482936092129], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2545200.0000, 
sim time next is 2545800.0000, 
raw observation next is [-0.3166666666666667, 45.66666666666667, 199.0, 62.33333333333333, 26.0, 25.80778533864765, 7.511447713131784, 1.0, 1.0, 11.39641148244968], 
processed observation next is [1.0, 0.4782608695652174, 0.45383194829178214, 0.4566666666666667, 0.6633333333333333, 0.06887661141804788, 1.0, 0.9725407626639502, 0.07511447713131784, 1.0, 1.0, 0.08140293916035486], 
reward next is 0.9186, 
noisyNet noise sample is [array([-1.3388755], dtype=float32), 0.67688]. 
=============================================
[2019-04-01 18:09:50,088] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.2784844e-35
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:09:50,089] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9183
[2019-04-01 18:09:50,142] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3, 53.0, 191.0, 0.0, 26.0, 24.97785864685824, 7.626201665094835, 0.0, 1.0, 29.45541125124542], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2381400.0000, 
sim time next is 2382000.0000, 
raw observation next is [-0.2, 52.66666666666667, 185.8333333333333, 0.0, 26.0, 24.96618726524564, 7.60961999395234, 0.0, 1.0, 29.69930805656627], 
processed observation next is [0.0, 0.5652173913043478, 0.4570637119113574, 0.5266666666666667, 0.6194444444444442, 0.0, 1.0, 0.8523124664636629, 0.0760961999395234, 0.0, 1.0, 0.21213791468975907], 
reward next is 0.7879, 
noisyNet noise sample is [array([0.45920375], dtype=float32), -0.3552547]. 
=============================================
[2019-04-01 18:09:50,145] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[76.1986  ]
 [76.34956 ]
 [76.419586]
 [76.358116]
 [76.22496 ]], R is [[75.80484772]
 [75.83640289]
 [75.86112976]
 [75.88069153]
 [75.91844177]].
[2019-04-01 18:09:52,702] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2180121e-37 0.0000000e+00 1.3998680e-34 0.0000000e+00 5.9800761e-22
 1.0000000e+00 1.4481631e-16], sum to 1.0000
[2019-04-01 18:09:52,705] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3467
[2019-04-01 18:09:52,720] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.100000000000001, 41.33333333333334, 0.0, 0.0, 26.0, 24.69592547697644, 6.020651523346463, 0.0, 1.0, 42.51299229469651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2416200.0000, 
sim time next is 2416800.0000, 
raw observation next is [-5.2, 41.66666666666667, 0.0, 0.0, 26.0, 24.6530222084029, 5.980411820217611, 0.0, 1.0, 42.52585068465112], 
processed observation next is [0.0, 1.0, 0.31855955678670367, 0.41666666666666674, 0.0, 0.0, 1.0, 0.807574601200414, 0.05980411820217611, 0.0, 1.0, 0.3037560763189366], 
reward next is 0.6962, 
noisyNet noise sample is [array([0.1343769], dtype=float32), 0.68085563]. 
=============================================
[2019-04-01 18:09:55,436] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.855919e-33
 1.000000e+00 6.206478e-27], sum to 1.0000
[2019-04-01 18:09:55,437] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2635
[2019-04-01 18:09:55,450] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.42845037925037, 5.470743684310665, 0.0, 1.0, 43.798138626213], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2436000.0000, 
sim time next is 2436600.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.39252227008688, 5.50406654849242, 0.0, 1.0, 43.77835101121482], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 1.0, 0.6275031814409827, 0.0550406654849242, 0.0, 1.0, 0.312702507222963], 
reward next is 0.6873, 
noisyNet noise sample is [array([1.4191821], dtype=float32), -0.16506922]. 
=============================================
[2019-04-01 18:10:02,049] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:10:02,050] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8969
[2019-04-01 18:10:02,067] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.466666666666667, 29.33333333333333, 186.6666666666667, 328.6666666666667, 26.0, 25.47448815991542, 7.319761211759335, 1.0, 1.0, 7.036797831203373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2553000.0000, 
sim time next is 2553600.0000, 
raw observation next is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 26.0, 25.34426338087923, 7.547142032114643, 1.0, 1.0, 6.638093488567154], 
processed observation next is [1.0, 0.5652173913043478, 0.538319482917821, 0.28666666666666674, 0.5961111111111109, 0.44788213627992646, 1.0, 0.9063233401256043, 0.07547142032114644, 1.0, 1.0, 0.04741495348976539], 
reward next is 0.9526, 
noisyNet noise sample is [array([0.511256], dtype=float32), -0.16191405]. 
=============================================
[2019-04-01 18:10:04,010] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:10:04,010] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5038
[2019-04-01 18:10:04,047] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.9671757161984, 7.807907916374042, 0.0, 1.0, 37.62244297558836], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2577600.0000, 
sim time next is 2578200.0000, 
raw observation next is [-1.883333333333333, 46.0, 0.0, 0.0, 26.0, 24.9602024935698, 7.775366051206862, 0.0, 1.0, 31.74680038156485], 
processed observation next is [1.0, 0.8695652173913043, 0.4104339796860573, 0.46, 0.0, 0.0, 1.0, 0.8514574990813999, 0.07775366051206861, 0.0, 1.0, 0.22676285986832034], 
reward next is 0.7732, 
noisyNet noise sample is [array([1.2486203], dtype=float32), -0.28101423]. 
=============================================
[2019-04-01 18:10:09,247] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.398166e-23
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:10:09,248] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9863
[2019-04-01 18:10:09,271] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.35450668300552, 5.50281829289955, 0.0, 1.0, 40.68286886253172], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2784600.0000, 
sim time next is 2785200.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.29452376981603, 5.484418542195937, 0.0, 1.0, 40.7390814125915], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 1.0, 0.7563605385451473, 0.05484418542195937, 0.0, 1.0, 0.2909934386613679], 
reward next is 0.7090, 
noisyNet noise sample is [array([-1.5281479], dtype=float32), -0.14806263]. 
=============================================
[2019-04-01 18:10:10,783] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:10:10,783] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0019
[2019-04-01 18:10:10,826] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.4, 57.5, 109.0, 788.0, 26.0, 26.87564007066281, 16.36995674555833, 1.0, 1.0, 28.01131270400099], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2727000.0000, 
sim time next is 2727600.0000, 
raw observation next is [-5.199999999999999, 57.0, 107.8333333333333, 778.8333333333334, 26.0, 27.02365543068769, 16.91022815133042, 1.0, 1.0, 26.13426734552643], 
processed observation next is [1.0, 0.5652173913043478, 0.31855955678670367, 0.57, 0.35944444444444434, 0.8605893186003684, 1.0, 1.1462364900982414, 0.1691022815133042, 1.0, 1.0, 0.18667333818233164], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1999432], dtype=float32), -1.4513378]. 
=============================================
[2019-04-01 18:10:13,863] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 5.324662e-35], sum to 1.0000
[2019-04-01 18:10:13,864] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0683
[2019-04-01 18:10:13,880] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 50.0, 59.33333333333333, 480.6666666666667, 26.0, 25.98992801085627, 12.04192054232274, 1.0, 1.0, 26.52464345581294], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2737200.0000, 
sim time next is 2737800.0000, 
raw observation next is [-3.0, 50.0, 54.0, 454.0, 26.0, 26.18705268780512, 12.0619364017148, 1.0, 1.0, 25.43980810207653], 
processed observation next is [1.0, 0.6956521739130435, 0.3795013850415513, 0.5, 0.18, 0.5016574585635359, 1.0, 1.0267218125435886, 0.120619364017148, 1.0, 1.0, 0.18171291501483236], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.55215544], dtype=float32), 0.5866672]. 
=============================================
[2019-04-01 18:10:13,956] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.9921873e-35], sum to 1.0000
[2019-04-01 18:10:13,959] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6708
[2019-04-01 18:10:13,974] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 50.0, 28.5, 253.5, 26.0, 25.87203485889109, 10.55005149042156, 1.0, 1.0, 22.30441495489913], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2739600.0000, 
sim time next is 2740200.0000, 
raw observation next is [-3.166666666666667, 50.66666666666667, 20.0, 186.6666666666666, 26.0, 25.96116486767414, 10.54733428410057, 1.0, 1.0, 23.12751425762143], 
processed observation next is [1.0, 0.7391304347826086, 0.3748845798707295, 0.5066666666666667, 0.06666666666666667, 0.20626151012891336, 1.0, 0.9944521239534484, 0.1054733428410057, 1.0, 1.0, 0.16519653041158167], 
reward next is 0.6159, 
noisyNet noise sample is [array([-0.4022109], dtype=float32), -1.0939567]. 
=============================================
[2019-04-01 18:10:17,300] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:10:17,301] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9637
[2019-04-01 18:10:17,326] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.84119155586325, 6.574998831036528, 0.0, 1.0, 37.47031524511652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3024000.0000, 
sim time next is 3024600.0000, 
raw observation next is [-4.166666666666667, 66.0, 0.0, 0.0, 26.0, 24.80609202584681, 6.505736467876697, 0.0, 1.0, 37.43897755777667], 
processed observation next is [0.0, 0.0, 0.3471837488457987, 0.66, 0.0, 0.0, 1.0, 0.8294417179781155, 0.06505736467876697, 0.0, 1.0, 0.2674212682698333], 
reward next is 0.7326, 
noisyNet noise sample is [array([0.8443037], dtype=float32), 2.4182584]. 
=============================================
[2019-04-01 18:10:20,779] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:10:20,780] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5660
[2019-04-01 18:10:20,831] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 26.0, 24.93762743718537, 9.525150778596313, 1.0, 1.0, 45.50018472105731], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2919600.0000, 
sim time next is 2920200.0000, 
raw observation next is [-1.0, 89.66666666666667, 0.0, 0.0, 26.0, 25.17716279395186, 9.866590132214993, 1.0, 1.0, 31.49157808305926], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.8966666666666667, 0.0, 0.0, 1.0, 0.8824518277074088, 0.09866590132214993, 1.0, 1.0, 0.22493984345042328], 
reward next is 0.7751, 
noisyNet noise sample is [array([-0.46935606], dtype=float32), 0.8109153]. 
=============================================
[2019-04-01 18:10:22,012] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7336004e-32 0.0000000e+00 5.8271216e-31 1.8004994e-34 4.0178292e-18
 3.1714617e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 18:10:22,017] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4676
[2019-04-01 18:10:22,030] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 96.5, 0.0, 0.0, 26.0, 24.88069197990521, 6.639495546304353, 0.0, 1.0, 54.26482583496945], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2874600.0000, 
sim time next is 2875200.0000, 
raw observation next is [1.666666666666667, 95.33333333333334, 0.0, 0.0, 26.0, 24.92732078424717, 6.773707448071463, 0.0, 1.0, 53.32958211284851], 
processed observation next is [1.0, 0.2608695652173913, 0.5087719298245615, 0.9533333333333335, 0.0, 0.0, 1.0, 0.8467601120353098, 0.06773707448071463, 0.0, 1.0, 0.3809255865203465], 
reward next is 0.6191, 
noisyNet noise sample is [array([-0.14315876], dtype=float32), -0.46954846]. 
=============================================
[2019-04-01 18:10:24,220] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:10:24,221] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7201
[2019-04-01 18:10:24,236] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 25.08388386208017, 7.038692326379952, 0.0, 1.0, 54.63371767828805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2866200.0000, 
sim time next is 2866800.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 25.07019338665767, 6.937375892800714, 0.0, 1.0, 54.65143054034848], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 1.0, 0.8671704838082387, 0.06937375892800714, 0.0, 1.0, 0.39036736100248914], 
reward next is 0.6096, 
noisyNet noise sample is [array([-0.9988112], dtype=float32), 2.581803]. 
=============================================
[2019-04-01 18:10:25,612] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.2937535e-21], sum to 1.0000
[2019-04-01 18:10:25,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2222
[2019-04-01 18:10:25,650] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 39.33333333333334, 349.0000000000001, 26.0, 25.06968720783823, 8.661729702852194, 0.0, 1.0, 25.85748684045409], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2998200.0000, 
sim time next is 2998800.0000, 
raw observation next is [-1.0, 55.0, 31.0, 286.5, 26.0, 25.08799339771797, 8.619139673571597, 0.0, 1.0, 26.33062991277069], 
processed observation next is [0.0, 0.7391304347826086, 0.4349030470914128, 0.55, 0.10333333333333333, 0.3165745856353591, 1.0, 0.8697133425311385, 0.08619139673571598, 0.0, 1.0, 0.18807592794836206], 
reward next is 0.8119, 
noisyNet noise sample is [array([-2.6795619], dtype=float32), 0.8424098]. 
=============================================
[2019-04-01 18:10:26,282] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.2296320e-26 0.0000000e+00 4.9296839e-32 2.9919500e-28 4.5321037e-27
 1.0000000e+00 9.8781640e-27], sum to 1.0000
[2019-04-01 18:10:26,282] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8908
[2019-04-01 18:10:26,292] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333334, 100.0, 0.0, 0.0, 26.0, 25.53553874502287, 11.09118340992476, 0.0, 1.0, 26.65366090425426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3202800.0000, 
sim time next is 3203400.0000, 
raw observation next is [0.1666666666666666, 100.0, 0.0, 0.0, 26.0, 25.40858927875949, 10.8709534505975, 0.0, 1.0, 30.76397024831616], 
processed observation next is [1.0, 0.043478260869565216, 0.4672206832871654, 1.0, 0.0, 0.0, 1.0, 0.9155127541084985, 0.10870953450597501, 0.0, 1.0, 0.2197426446308297], 
reward next is 0.7803, 
noisyNet noise sample is [array([0.08283024], dtype=float32), 1.3537626]. 
=============================================
[2019-04-01 18:10:35,154] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 1.943516e-20], sum to 1.0000
[2019-04-01 18:10:35,154] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5264
[2019-04-01 18:10:35,195] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 50.0, 111.5, 811.5, 26.0, 25.11128371163935, 8.280061787161015, 0.0, 1.0, 20.14091600910112], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3070800.0000, 
sim time next is 3071400.0000, 
raw observation next is [-1.833333333333333, 48.66666666666667, 110.6666666666667, 809.6666666666666, 26.0, 25.1010168466942, 8.292514228118884, 0.0, 1.0, 21.09720568291176], 
processed observation next is [0.0, 0.5652173913043478, 0.41181902123730385, 0.4866666666666667, 0.368888888888889, 0.8946593001841621, 1.0, 0.8715738352420287, 0.08292514228118884, 0.0, 1.0, 0.15069432630651258], 
reward next is 0.8493, 
noisyNet noise sample is [array([0.3068739], dtype=float32), 0.33757287]. 
=============================================
[2019-04-01 18:10:41,791] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 4.4661494e-34 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:10:41,792] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3447
[2019-04-01 18:10:41,807] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 80.33333333333333, 114.3333333333333, 821.1666666666667, 26.0, 25.83197497141055, 14.73610715239649, 1.0, 1.0, 15.34054021074044], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3242400.0000, 
sim time next is 3243000.0000, 
raw observation next is [-2.0, 82.66666666666667, 113.6666666666667, 819.3333333333334, 26.0, 26.00459237403525, 15.06249537788215, 1.0, 1.0, 13.11609736425668], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.8266666666666667, 0.378888888888889, 0.905340699815838, 1.0, 1.0006560534336073, 0.1506249537788215, 1.0, 1.0, 0.09368640974469057], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8400201], dtype=float32), 0.7564322]. 
=============================================
[2019-04-01 18:10:41,816] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[30.27858 ]
 [30.12214 ]
 [29.905306]
 [30.111042]
 [29.972216]], R is [[30.06292725]
 [29.76229858]
 [29.4646759 ]
 [29.17002869]
 [28.87832832]].
[2019-04-01 18:10:51,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6282239e-16 3.5329319e-28 2.9650705e-19 5.3936099e-23 1.7878417e-11
 9.9963021e-01 3.6975613e-04], sum to 1.0000
[2019-04-01 18:10:51,980] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7381
[2019-04-01 18:10:51,996] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.07499810538659, 7.626765816463494, 0.0, 1.0, 40.67716843503144], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3373800.0000, 
sim time next is 3374400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.02165272240106, 7.476641988529444, 0.0, 1.0, 40.69435943242757], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.8602361032001516, 0.07476641988529444, 0.0, 1.0, 0.2906739959459112], 
reward next is 0.7093, 
noisyNet noise sample is [array([1.2808], dtype=float32), 0.85051906]. 
=============================================
[2019-04-01 18:10:57,600] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9287454e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.8721985e-23
 1.0000000e+00 2.3995372e-35], sum to 1.0000
[2019-04-01 18:10:57,604] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5568
[2019-04-01 18:10:57,627] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.92402446255549, 9.589363461174875, 0.0, 1.0, 44.36550658509826], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3529200.0000, 
sim time next is 3529800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.94789545452337, 10.74575362660463, 0.0, 1.0, 56.64636568971338], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 1.0, 0.8496993506461956, 0.1074575362660463, 0.0, 1.0, 0.404616897783667], 
reward next is 0.5954, 
noisyNet noise sample is [array([0.45592648], dtype=float32), -1.271626]. 
=============================================
[2019-04-01 18:10:57,840] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.5337868e-32 0.0000000e+00 4.4066326e-33 1.4778267e-28 0.0000000e+00
 1.0000000e+00 4.8077483e-37], sum to 1.0000
[2019-04-01 18:10:57,846] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2323
[2019-04-01 18:10:57,881] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.333333333333333, 51.0, 115.1666666666667, 808.8333333333334, 26.0, 26.35171404559818, 12.51222882205122, 1.0, 1.0, 7.069307412345713], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3504000.0000, 
sim time next is 3504600.0000, 
raw observation next is [2.5, 50.5, 115.0, 806.0, 26.0, 26.01585777410055, 13.51804771804092, 1.0, 1.0, 6.630678933815303], 
processed observation next is [1.0, 0.5652173913043478, 0.5318559556786704, 0.505, 0.38333333333333336, 0.8906077348066298, 1.0, 1.0022653963000785, 0.1351804771804092, 1.0, 1.0, 0.04736199238439502], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4306633], dtype=float32), 1.8296698]. 
=============================================
[2019-04-01 18:11:02,540] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 5.7873414e-05 9.9994206e-01], sum to 1.0000
[2019-04-01 18:11:02,546] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4498
[2019-04-01 18:11:02,556] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 66.16666666666667, 0.0, 0.0, 26.0, 25.47402271903035, 8.678521435172376, 0.0, 1.0, 36.99242976941699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3711000.0000, 
sim time next is 3711600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.47125749245748, 8.505112143122195, 0.0, 1.0, 33.12896803549967], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.65, 0.0, 0.0, 1.0, 0.9244653560653541, 0.08505112143122195, 0.0, 1.0, 0.23663548596785478], 
reward next is 0.7634, 
noisyNet noise sample is [array([-0.7279424], dtype=float32), 0.15449716]. 
=============================================
[2019-04-01 18:11:08,117] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 6.7360314e-37 0.0000000e+00 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:11:08,117] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9568
[2019-04-01 18:11:08,141] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 26.0, 26.08764877721906, 10.63670839478923, 1.0, 1.0, 14.06935079222941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3781200.0000, 
sim time next is 3781800.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.99106096943614, 11.44518156082101, 1.0, 1.0, 13.59127722409294], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 1.0, 0.9987229956337345, 0.1144518156082101, 1.0, 1.0, 0.09708055160066385], 
reward next is 0.3248, 
noisyNet noise sample is [array([-0.34841233], dtype=float32), 0.8180995]. 
=============================================
[2019-04-01 18:11:09,163] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.8782015e-36], sum to 1.0000
[2019-04-01 18:11:09,165] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2292
[2019-04-01 18:11:09,180] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 68.0, 0.0, 0.0, 26.0, 25.09879695361105, 10.82301272893017, 0.0, 1.0, 55.36755357598656], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3789000.0000, 
sim time next is 3789600.0000, 
raw observation next is [-2.666666666666667, 69.0, 0.0, 0.0, 26.0, 25.25618860434912, 11.52108961556858, 0.0, 1.0, 47.90491737967919], 
processed observation next is [1.0, 0.8695652173913043, 0.38873499538319484, 0.69, 0.0, 0.0, 1.0, 0.8937412291927315, 0.11521089615568579, 0.0, 1.0, 0.3421779812834228], 
reward next is 0.6578, 
noisyNet noise sample is [array([-0.00540507], dtype=float32), -1.3578395]. 
=============================================
[2019-04-01 18:11:16,605] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3259849e-22 1.1559904e-32 1.2625259e-26 8.9955203e-17 0.0000000e+00
 1.0000000e+00 2.0821282e-08], sum to 1.0000
[2019-04-01 18:11:16,606] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3699
[2019-04-01 18:11:16,611] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 49.0, 118.8333333333333, 804.1666666666666, 26.0, 26.40371821355988, 13.71540723419822, 1.0, 1.0, 9.083127462821231], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3928800.0000, 
sim time next is 3929400.0000, 
raw observation next is [-6.0, 49.0, 120.0, 810.0, 26.0, 26.42777411411992, 13.76256798130366, 1.0, 1.0, 8.546761289133421], 
processed observation next is [1.0, 0.4782608695652174, 0.296398891966759, 0.49, 0.4, 0.8950276243093923, 1.0, 1.061110587731417, 0.13762567981303658, 1.0, 1.0, 0.061048294922381584], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.75132847], dtype=float32), -1.8887116]. 
=============================================
[2019-04-01 18:11:16,733] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.6540401e-35], sum to 1.0000
[2019-04-01 18:11:16,733] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6212
[2019-04-01 18:11:16,751] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.43922516299713, 6.038267205630286, 0.0, 1.0, 43.19175285020944], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3982200.0000, 
sim time next is 3982800.0000, 
raw observation next is [-12.0, 63.00000000000001, 0.0, 0.0, 26.0, 24.36933336161964, 6.006021891225044, 0.0, 1.0, 43.20538957189741], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.6300000000000001, 0.0, 0.0, 1.0, 0.7670476230885198, 0.06006021891225043, 0.0, 1.0, 0.30860992551355293], 
reward next is 0.6914, 
noisyNet noise sample is [array([0.36412454], dtype=float32), 0.82394785]. 
=============================================
[2019-04-01 18:11:21,696] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 9.800694e-27], sum to 1.0000
[2019-04-01 18:11:21,699] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9390
[2019-04-01 18:11:21,763] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.48823513033029, 11.07483169168219, 0.0, 1.0, 40.14895709906683], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3961200.0000, 
sim time next is 3961800.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.46296744162429, 11.19591270604464, 0.0, 1.0, 42.30406920008222], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 1.0, 0.9232810630891842, 0.11195912706044639, 0.0, 1.0, 0.30217192285773015], 
reward next is 0.6978, 
noisyNet noise sample is [array([2.1745698], dtype=float32), 0.8918608]. 
=============================================
[2019-04-01 18:11:23,557] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 5.861307e-28], sum to 1.0000
[2019-04-01 18:11:23,558] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6462
[2019-04-01 18:11:23,572] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666666, 30.33333333333333, 0.0, 0.0, 26.0, 25.56930454585608, 10.09065982717104, 0.0, 1.0, 26.82130146834097], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4052400.0000, 
sim time next is 4053000.0000, 
raw observation next is [-4.833333333333334, 30.66666666666667, 0.0, 0.0, 26.0, 25.49989065577103, 9.910310265830537, 0.0, 1.0, 30.22166506786123], 
processed observation next is [1.0, 0.9130434782608695, 0.32871652816251157, 0.3066666666666667, 0.0, 0.0, 1.0, 0.9285558079672899, 0.09910310265830537, 0.0, 1.0, 0.2158690361990088], 
reward next is 0.7841, 
noisyNet noise sample is [array([-1.0742129], dtype=float32), -0.18158334]. 
=============================================
[2019-04-01 18:11:23,586] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.40102 ]
 [74.40086 ]
 [74.48739 ]
 [74.046684]
 [73.046165]], R is [[74.48400879]
 [74.54758453]
 [74.63071442]
 [74.70359039]
 [74.76785278]].
[2019-04-01 18:11:24,569] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4973455e-27 0.0000000e+00 7.7066688e-33 3.1730962e-32 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:11:24,570] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9874
[2019-04-01 18:11:24,578] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1666666666666666, 91.0, 133.3333333333333, 2.0, 26.0, 26.28768527295176, 13.20237609782441, 1.0, 1.0, 10.53818991508926], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4452600.0000, 
sim time next is 4453200.0000, 
raw observation next is [0.0, 92.0, 149.0, 3.0, 26.0, 26.27678880410184, 12.69050940962273, 1.0, 1.0, 10.52623546551444], 
processed observation next is [1.0, 0.5652173913043478, 0.46260387811634357, 0.92, 0.49666666666666665, 0.0033149171270718232, 1.0, 1.0395412577288343, 0.1269050940962273, 1.0, 1.0, 0.075187396182246], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.54941773], dtype=float32), 0.20673232]. 
=============================================
[2019-04-01 18:11:29,805] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.1272477e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6414929e-37
 1.0000000e+00 8.8503727e-12], sum to 1.0000
[2019-04-01 18:11:29,808] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8165
[2019-04-01 18:11:29,902] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.833333333333334, 52.5, 61.33333333333334, 325.3333333333334, 26.0, 24.57977030626984, 7.898479021637175, 0.0, 1.0, 97.06046651462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4176600.0000, 
sim time next is 4177200.0000, 
raw observation next is [-4.666666666666667, 51.00000000000001, 76.66666666666667, 406.6666666666667, 26.0, 25.07769638911126, 8.89653775067404, 0.0, 1.0, 54.80050339935197], 
processed observation next is [0.0, 0.34782608695652173, 0.3333333333333333, 0.5100000000000001, 0.2555555555555556, 0.44935543278084716, 1.0, 0.8682423413016086, 0.08896537750674041, 0.0, 1.0, 0.3914321671382283], 
reward next is 0.6086, 
noisyNet noise sample is [array([-1.1944995], dtype=float32), -0.5676829]. 
=============================================
[2019-04-01 18:11:31,796] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.8824202e-29
 1.1848268e-01 8.8151735e-01], sum to 1.0000
[2019-04-01 18:11:31,799] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9131
[2019-04-01 18:11:31,834] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.55, 41.5, 0.0, 0.0, 26.0, 25.00143801435402, 7.545435177718652, 0.0, 1.0, 25.56145510718171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4213800.0000, 
sim time next is 4214400.0000, 
raw observation next is [1.5, 41.66666666666667, 0.0, 0.0, 26.0, 25.00711486141922, 7.510519826603217, 0.0, 1.0, 24.45049512554718], 
processed observation next is [0.0, 0.782608695652174, 0.5041551246537397, 0.41666666666666674, 0.0, 0.0, 1.0, 0.8581592659170312, 0.07510519826603218, 0.0, 1.0, 0.17464639375390845], 
reward next is 0.8254, 
noisyNet noise sample is [array([-0.30193955], dtype=float32), 2.289048]. 
=============================================
[2019-04-01 18:11:33,061] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.1318617e-13 1.1694346e-31 8.7978881e-21 9.6679483e-26 3.6900947e-31
 1.0000000e+00 6.8755761e-21], sum to 1.0000
[2019-04-01 18:11:33,063] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1315
[2019-04-01 18:11:33,088] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.0, 33.0, 118.3333333333333, 812.0, 26.0, 27.81816988548945, 22.92580819169615, 1.0, 1.0, 4.122040800654093], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4360200.0000, 
sim time next is 4360800.0000, 
raw observation next is [13.4, 32.0, 119.1666666666667, 820.0, 26.0, 27.82728777882999, 23.84972307842551, 1.0, 1.0, 3.930168690294351], 
processed observation next is [1.0, 0.4782608695652174, 0.8337950138504157, 0.32, 0.3972222222222223, 0.9060773480662984, 1.0, 1.2610411112614273, 0.23849723078425508, 1.0, 1.0, 0.02807263350210251], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.46421194], dtype=float32), -0.4642843]. 
=============================================
[2019-04-01 18:11:33,265] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:11:33,268] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2052
[2019-04-01 18:11:33,289] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666666, 57.33333333333334, 134.8333333333333, 724.0, 26.0, 26.49453991395637, 14.15935723641261, 1.0, 1.0, 9.841689105398006], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4616400.0000, 
sim time next is 4617000.0000, 
raw observation next is [1.0, 56.0, 129.0, 767.0, 26.0, 26.59463120468876, 14.40296706149307, 1.0, 1.0, 9.063924974848664], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.56, 0.43, 0.8475138121546961, 1.0, 1.084947314955537, 0.14402967061493072, 1.0, 1.0, 0.06474232124891903], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7994211], dtype=float32), 1.3492141]. 
=============================================
[2019-04-01 18:11:33,304] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[47.134846]
 [47.018734]
 [46.889065]
 [46.526028]
 [46.008083]], R is [[46.72687149]
 [46.25960159]
 [45.7970047 ]
 [45.33903503]
 [44.88564682]].
[2019-04-01 18:11:33,941] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:11:33,942] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2170
[2019-04-01 18:11:33,956] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 68.0, 0.0, 0.0, 26.0, 25.46001354871603, 11.07791695056084, 0.0, 1.0, 38.02842896907888], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4428000.0000, 
sim time next is 4428600.0000, 
raw observation next is [2.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.513963903115, 11.27995030015492, 0.0, 1.0, 38.12909905964838], 
processed observation next is [1.0, 0.2608695652173913, 0.541089566020314, 0.7, 0.0, 0.0, 1.0, 0.9305662718735712, 0.11279950300154921, 0.0, 1.0, 0.272350707568917], 
reward next is 0.7276, 
noisyNet noise sample is [array([-0.06597796], dtype=float32), -0.13648932]. 
=============================================
[2019-04-01 18:11:40,741] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:11:40,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2968
[2019-04-01 18:11:40,763] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.199999999999999, 64.0, 0.0, 0.0, 26.0, 25.65001980153091, 13.65737806883258, 0.0, 1.0, 31.94106119147328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4408200.0000, 
sim time next is 4408800.0000, 
raw observation next is [7.066666666666666, 64.33333333333333, 0.0, 0.0, 26.0, 25.75768811283424, 14.17813308311595, 0.0, 1.0, 28.21212504748723], 
processed observation next is [1.0, 0.0, 0.6583564173591875, 0.6433333333333333, 0.0, 0.0, 1.0, 0.9653840161191773, 0.1417813308311595, 0.0, 1.0, 0.20151517891062307], 
reward next is 0.7985, 
noisyNet noise sample is [array([0.4812578], dtype=float32), 1.105892]. 
=============================================
[2019-04-01 18:11:56,862] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.7182424e-32 0.0000000e+00 6.3181501e-36 0.0000000e+00 0.0000000e+00
 1.0000000e+00 6.7355366e-24], sum to 1.0000
[2019-04-01 18:11:56,863] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8513
[2019-04-01 18:11:56,873] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 84.16666666666666, 0.0, 0.0, 26.0, 25.46895282643748, 10.71221165108088, 0.0, 1.0, 35.93480483712826], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4744200.0000, 
sim time next is 4744800.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 25.42256434350722, 10.50262548838418, 0.0, 1.0, 36.2904100224445], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.84, 0.0, 0.0, 1.0, 0.9175091919296027, 0.10502625488384179, 0.0, 1.0, 0.25921721444603213], 
reward next is 0.7408, 
noisyNet noise sample is [array([0.4407372], dtype=float32), 0.73808324]. 
=============================================
[2019-04-01 18:11:57,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:11:57,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:11:57,359] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run20
[2019-04-01 18:11:58,094] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:11:58,095] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:11:58,120] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run20
[2019-04-01 18:11:59,942] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:11:59,943] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:11:59,964] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run20
[2019-04-01 18:12:04,363] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7058697e-28 1.8285305e-36 1.3666968e-23 6.8750115e-25 1.1303793e-33
 1.0000000e+00 4.1897234e-33], sum to 1.0000
[2019-04-01 18:12:04,365] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5803
[2019-04-01 18:12:04,384] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.0, 26.0, 106.1666666666667, 811.5, 26.0, 27.53463979282321, 20.7974823818357, 1.0, 1.0, 1.440837858067032], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4977600.0000, 
sim time next is 4978200.0000, 
raw observation next is [8.0, 26.0, 103.3333333333333, 804.0, 26.0, 27.6200210630633, 21.37734767717401, 1.0, 1.0, 1.364527335831593], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.34444444444444433, 0.8883977900552487, 1.0, 1.2314315804376146, 0.2137734767717401, 1.0, 1.0, 0.009746623827368522], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.93680406], dtype=float32), 1.1225348]. 
=============================================
[2019-04-01 18:12:04,447] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 4.6414541e-35 0.0000000e+00 4.3757098e-18
 9.9999988e-01 1.5269821e-07], sum to 1.0000
[2019-04-01 18:12:04,450] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0300
[2019-04-01 18:12:04,469] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.333333333333333, 56.66666666666667, 0.0, 0.0, 26.0, 25.35100131288745, 8.321549804682048, 0.0, 1.0, 39.3959609110081], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4836000.0000, 
sim time next is 4836600.0000, 
raw observation next is [-1.5, 57.5, 0.0, 0.0, 26.0, 25.40570330161557, 8.301491304996413, 0.0, 1.0, 39.16970311344514], 
processed observation next is [0.0, 1.0, 0.4210526315789474, 0.575, 0.0, 0.0, 1.0, 0.9151004716593673, 0.08301491304996413, 0.0, 1.0, 0.27978359366746525], 
reward next is 0.7202, 
noisyNet noise sample is [array([-1.5674083], dtype=float32), -0.089484684]. 
=============================================
[2019-04-01 18:12:04,579] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1376616e-22 5.0966567e-32 2.2637569e-27 6.8534319e-26 6.7818207e-30
 1.0000000e+00 1.0303094e-12], sum to 1.0000
[2019-04-01 18:12:04,581] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5240
[2019-04-01 18:12:04,605] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 82.0, 707.5, 26.0, 27.02546854213345, 20.58089720424735, 1.0, 1.0, 0.9688114889326075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4982400.0000, 
sim time next is 4983000.0000, 
raw observation next is [8.833333333333334, 25.16666666666667, 78.66666666666667, 685.3333333333333, 26.0, 27.45710655851436, 22.22852342998836, 1.0, 1.0, 0.906479716276761], 
processed observation next is [1.0, 0.6956521739130435, 0.7072945521698984, 0.2516666666666667, 0.26222222222222225, 0.7572744014732964, 1.0, 1.208158079787766, 0.22228523429988362, 1.0, 1.0, 0.006474855116262579], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.19903196], dtype=float32), 0.06753411]. 
=============================================
[2019-04-01 18:12:04,614] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[41.556118]
 [41.463978]
 [41.585083]
 [41.580788]
 [41.49939 ]], R is [[41.24544525]
 [40.83299255]
 [40.42466354]
 [40.02041626]
 [39.62021255]].
[2019-04-01 18:12:04,840] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:12:04,845] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5298
[2019-04-01 18:12:04,869] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.03278446228394, 13.54411371464521, 0.0, 1.0, 40.19115259201804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 17400.0000, 
sim time next is 18000.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.04982897861431, 13.3853479876934, 0.0, 1.0, 40.11806911466478], 
processed observation next is [0.0, 0.21739130434782608, 0.6759002770083103, 0.93, 0.0, 0.0, 1.0, 0.2928327112306159, 0.133853479876934, 0.0, 1.0, 0.28655763653331984], 
reward next is 0.7134, 
noisyNet noise sample is [array([0.21153767], dtype=float32), -1.7540749]. 
=============================================
[2019-04-01 18:12:04,886] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[73.25967 ]
 [73.3924  ]
 [73.51524 ]
 [73.62862 ]
 [73.745544]], R is [[73.12072754]
 [73.10243988]
 [73.08377838]
 [73.06478119]
 [73.04544067]].
[2019-04-01 18:12:09,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:12:09,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:12:09,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run20
[2019-04-01 18:12:09,410] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:12:09,411] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5739
[2019-04-01 18:12:09,425] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 26.0, 25.45405292493535, 8.898046723164756, 0.0, 1.0, 32.06637029594035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5017200.0000, 
sim time next is 5017800.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 26.0, 25.41766741023268, 8.917113389425895, 0.0, 1.0, 34.70899777030952], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.4, 0.0, 0.0, 1.0, 0.9168096300332402, 0.08917113389425894, 0.0, 1.0, 0.24792141264506803], 
reward next is 0.7521, 
noisyNet noise sample is [array([0.46461207], dtype=float32), 0.59686404]. 
=============================================
[2019-04-01 18:12:10,362] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:12:10,363] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:12:10,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run20
[2019-04-01 18:12:11,902] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.3147879e-32 0.0000000e+00 2.3185715e-35 0.0000000e+00 2.2795810e-19
 1.0000000e+00 5.3807389e-25], sum to 1.0000
[2019-04-01 18:12:11,905] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4042
[2019-04-01 18:12:11,927] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.7833333333333333, 91.00000000000001, 0.0, 0.0, 26.0, 24.35200687579222, 5.797540342893662, 0.0, 1.0, 40.61309237999773], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 90600.0000, 
sim time next is 91200.0000, 
raw observation next is [-0.9666666666666667, 91.0, 0.0, 0.0, 26.0, 24.3352147095372, 5.788279504651055, 0.0, 1.0, 40.81505106756241], 
processed observation next is [1.0, 0.043478260869565216, 0.43582640812557716, 0.91, 0.0, 0.0, 1.0, 0.7621735299338859, 0.05788279504651055, 0.0, 1.0, 0.2915360790540172], 
reward next is 0.7085, 
noisyNet noise sample is [array([-0.80275667], dtype=float32), -0.31679553]. 
=============================================
[2019-04-01 18:12:12,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:12:12,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:12:12,159] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run20
[2019-04-01 18:12:12,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:12:12,190] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:12:12,218] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run20
[2019-04-01 18:12:13,867] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:12:13,867] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:12:13,871] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run20
[2019-04-01 18:12:14,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:12:14,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:12:14,332] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run20
[2019-04-01 18:12:14,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:12:14,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:12:14,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run20
[2019-04-01 18:12:14,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:12:14,662] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:12:14,665] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run20
[2019-04-01 18:12:14,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:12:14,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:12:15,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run20
[2019-04-01 18:12:15,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:12:15,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:12:15,044] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run20
[2019-04-01 18:12:16,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:12:16,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:12:16,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run20
[2019-04-01 18:12:16,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:12:16,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:12:16,899] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run20
[2019-04-01 18:12:16,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:12:16,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:12:16,944] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run20
[2019-04-01 18:12:26,544] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00
 6.75944e-19], sum to 1.0000
[2019-04-01 18:12:26,546] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2568
[2019-04-01 18:12:26,613] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 32.33333333333334, 0.0, 26.0, 23.07906373740434, 6.016510705671084, 0.0, 1.0, 57.3603840436241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 33000.0000, 
sim time next is 33600.0000, 
raw observation next is [7.7, 93.0, 35.16666666666666, 0.0, 26.0, 23.21055739353488, 5.883502161512898, 0.0, 1.0, 57.18986165187906], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.11722222222222219, 0.0, 1.0, 0.6015081990764115, 0.058835021615128984, 0.0, 1.0, 0.40849901179913617], 
reward next is 0.5915, 
noisyNet noise sample is [array([0.52223295], dtype=float32), 0.11483943]. 
=============================================
[2019-04-01 18:12:43,714] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:12:43,715] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6330
[2019-04-01 18:12:43,789] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.9, 59.5, 76.33333333333333, 0.0, 26.0, 25.64695051666238, 8.48136796972299, 1.0, 1.0, 30.50097677020576], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 227400.0000, 
sim time next is 228000.0000, 
raw observation next is [-3.0, 60.0, 66.16666666666667, 0.0, 26.0, 25.83686462517447, 8.63417845330675, 1.0, 1.0, 29.94795696175016], 
processed observation next is [1.0, 0.6521739130434783, 0.3795013850415513, 0.6, 0.22055555555555556, 0.0, 1.0, 0.9766949464534959, 0.08634178453306751, 1.0, 1.0, 0.21391397829821543], 
reward next is 0.7861, 
noisyNet noise sample is [array([0.26645204], dtype=float32), 0.72209966]. 
=============================================
[2019-04-01 18:12:43,796] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[66.698265]
 [66.73251 ]
 [66.65708 ]
 [67.45438 ]
 [67.76882 ]], R is [[66.72740173]
 [66.84226227]
 [66.87814331]
 [66.74304962]
 [66.88665009]].
[2019-04-01 18:12:46,275] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.8300464e-33], sum to 1.0000
[2019-04-01 18:12:46,278] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7692
[2019-04-01 18:12:46,300] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 78.5, 0.0, 0.0, 26.0, 24.39854778952397, 6.019534284301706, 0.0, 1.0, 43.56912498028985], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 253800.0000, 
sim time next is 254400.0000, 
raw observation next is [-3.899999999999999, 79.66666666666667, 0.0, 0.0, 26.0, 24.36576386487753, 5.967039284315931, 0.0, 1.0, 43.58690171622569], 
processed observation next is [1.0, 0.9565217391304348, 0.35457063711911363, 0.7966666666666667, 0.0, 0.0, 1.0, 0.7665376949825041, 0.05967039284315931, 0.0, 1.0, 0.31133501225875493], 
reward next is 0.6887, 
noisyNet noise sample is [array([0.50197], dtype=float32), 1.5783765]. 
=============================================
[2019-04-01 18:12:48,367] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0200215e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 18:12:48,371] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6525
[2019-04-01 18:12:48,388] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.966666666666667, 86.66666666666666, 0.0, 0.0, 26.0, 24.8995760077501, 6.566210347816049, 0.0, 1.0, 39.21870146659162], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 528000.0000, 
sim time next is 528600.0000, 
raw observation next is [3.883333333333333, 86.33333333333334, 0.0, 0.0, 26.0, 24.87896653533478, 6.529230514012563, 0.0, 1.0, 39.26125422512824], 
processed observation next is [0.0, 0.08695652173913043, 0.5701754385964913, 0.8633333333333334, 0.0, 0.0, 1.0, 0.8398523621906827, 0.06529230514012563, 0.0, 1.0, 0.2804375301794874], 
reward next is 0.7196, 
noisyNet noise sample is [array([0.5060169], dtype=float32), -0.09921906]. 
=============================================
[2019-04-01 18:12:53,939] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0440949e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.7611005e-36
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:12:53,939] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8887
[2019-04-01 18:12:53,958] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.9, 52.0, 0.0, 0.0, 26.0, 23.27647029433815, 5.808004216385943, 0.0, 1.0, 45.21447083737074], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 441000.0000, 
sim time next is 441600.0000, 
raw observation next is [-10.8, 51.0, 0.0, 0.0, 26.0, 23.26784748312462, 5.896379424392923, 0.0, 1.0, 45.24956482272356], 
processed observation next is [1.0, 0.08695652173913043, 0.1634349030470914, 0.51, 0.0, 0.0, 1.0, 0.6096924975892313, 0.05896379424392923, 0.0, 1.0, 0.3232111773051683], 
reward next is 0.6768, 
noisyNet noise sample is [array([0.56097394], dtype=float32), -1.3856282]. 
=============================================
[2019-04-01 18:12:54,072] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:12:54,073] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1103
[2019-04-01 18:12:54,134] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.25777697801819, 8.118958509766577, 1.0, 1.0, 46.6880090952973], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 414000.0000, 
sim time next is 414600.0000, 
raw observation next is [-9.583333333333334, 40.33333333333334, 0.0, 0.0, 26.0, 25.15410412684491, 7.970570497194696, 1.0, 1.0, 46.74609862800401], 
processed observation next is [1.0, 0.8260869565217391, 0.1971375807940905, 0.40333333333333343, 0.0, 0.0, 1.0, 0.8791577324064157, 0.07970570497194696, 1.0, 1.0, 0.33390070448574294], 
reward next is 0.6661, 
noisyNet noise sample is [array([2.1878943], dtype=float32), -0.19454205]. 
=============================================
[2019-04-01 18:12:55,387] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:12:55,387] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5023
[2019-04-01 18:12:55,404] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 48.66666666666666, 0.0, 0.0, 26.0, 24.41033222526937, 6.179177686260902, 0.0, 1.0, 44.35996542908565], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 424200.0000, 
sim time next is 424800.0000, 
raw observation next is [-10.6, 49.0, 0.0, 0.0, 26.0, 24.36407771691413, 6.097339471203955, 0.0, 1.0, 44.27429164142895], 
processed observation next is [1.0, 0.9565217391304348, 0.1689750692520776, 0.49, 0.0, 0.0, 1.0, 0.7662968167020188, 0.060973394712039546, 0.0, 1.0, 0.3162449402959211], 
reward next is 0.6838, 
noisyNet noise sample is [array([-0.5092773], dtype=float32), 0.50349694]. 
=============================================
[2019-04-01 18:12:56,402] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4738660e-30 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.5522319e-34
 1.0000000e+00 5.5033213e-32], sum to 1.0000
[2019-04-01 18:12:56,402] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0545
[2019-04-01 18:12:56,429] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.9, 50.5, 0.0, 0.0, 26.0, 23.12598673266104, 6.184502950893112, 0.0, 1.0, 45.47173898099251], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 444600.0000, 
sim time next is 445200.0000, 
raw observation next is [-11.0, 51.0, 0.0, 0.0, 26.0, 23.03606645729748, 6.27104295952072, 0.0, 1.0, 45.58305221906171], 
processed observation next is [1.0, 0.13043478260869565, 0.15789473684210528, 0.51, 0.0, 0.0, 1.0, 0.5765809224710685, 0.0627104295952072, 0.0, 1.0, 0.3255932301361551], 
reward next is 0.6744, 
noisyNet noise sample is [array([-0.00596142], dtype=float32), -1.4958024]. 
=============================================
[2019-04-01 18:12:58,428] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:12:58,428] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9449
[2019-04-01 18:12:58,484] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.783333333333333, 32.16666666666666, 74.0, 0.0, 26.0, 25.33990472887669, 6.111177289184539, 1.0, 1.0, 27.72162071041154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 467400.0000, 
sim time next is 468000.0000, 
raw observation next is [-4.5, 32.0, 80.0, 0.0, 26.0, 25.32305515118952, 6.097889523940128, 1.0, 1.0, 26.11135420293233], 
processed observation next is [1.0, 0.43478260869565216, 0.3379501385041552, 0.32, 0.26666666666666666, 0.0, 1.0, 0.9032935930270745, 0.06097889523940128, 1.0, 1.0, 0.18650967287808806], 
reward next is 0.8135, 
noisyNet noise sample is [array([-1.615183], dtype=float32), -0.003177253]. 
=============================================
[2019-04-01 18:12:58,487] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.695946]
 [72.98373 ]
 [72.298225]
 [71.98966 ]
 [71.54099 ]], R is [[73.98072052]
 [74.04290009]
 [74.09207916]
 [74.12747192]
 [74.14827728]].
[2019-04-01 18:13:04,839] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:13:04,839] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9188
[2019-04-01 18:13:04,885] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 25.04983839353326, 7.440937259584217, 1.0, 1.0, 32.45054608423897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 504000.0000, 
sim time next is 504600.0000, 
raw observation next is [1.183333333333333, 96.0, 0.0, 0.0, 26.0, 25.13053529794428, 7.421126432638872, 1.0, 1.0, 42.72248221487322], 
processed observation next is [1.0, 0.8695652173913043, 0.49538319482917825, 0.96, 0.0, 0.0, 1.0, 0.875790756849183, 0.07421126432638872, 1.0, 1.0, 0.3051605872490944], 
reward next is 0.6948, 
noisyNet noise sample is [array([0.62878615], dtype=float32), -0.49978185]. 
=============================================
[2019-04-01 18:13:08,368] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 6.3079317e-31], sum to 1.0000
[2019-04-01 18:13:08,368] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9503
[2019-04-01 18:13:08,384] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 80.5, 0.0, 0.0, 26.0, 24.20791572832415, 5.476432354276191, 0.0, 1.0, 41.93883062166594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 613800.0000, 
sim time next is 614400.0000, 
raw observation next is [-3.899999999999999, 78.66666666666667, 0.0, 0.0, 26.0, 24.20424091965869, 5.450684307435734, 0.0, 1.0, 42.05719129703903], 
processed observation next is [0.0, 0.08695652173913043, 0.35457063711911363, 0.7866666666666667, 0.0, 0.0, 1.0, 0.7434629885226701, 0.05450684307435734, 0.0, 1.0, 0.3004085092645645], 
reward next is 0.6996, 
noisyNet noise sample is [array([-0.09490737], dtype=float32), 0.34500122]. 
=============================================
[2019-04-01 18:13:10,669] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-01 18:13:10,669] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 18:13:10,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:13:10,671] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 18:13:10,671] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:13:10,673] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 18:13:10,673] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:13:10,674] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run27
[2019-04-01 18:13:10,675] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run27
[2019-04-01 18:13:10,722] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run27
[2019-04-01 18:13:35,308] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.41226548], dtype=float32), -0.86441743]
[2019-04-01 18:13:35,309] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.435891757166667, 76.46654588333332, 0.6030958216666665, 0.0, 26.0, 23.58601187240571, 5.334440594442043, 0.0, 1.0, 43.16456580909512]
[2019-04-01 18:13:35,310] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 18:13:35,312] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 5.2968816e-22], sampled 0.22861022392704966
[2019-04-01 18:13:46,591] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.41226548], dtype=float32), -0.86441743]
[2019-04-01 18:13:46,591] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [14.4, 78.33333333333333, 0.0, 0.0, 26.0, 25.26727898075556, 9.775459569298881, 1.0, 1.0, 3.730146490255271]
[2019-04-01 18:13:46,592] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 18:13:46,593] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.7406851687284735
[2019-04-01 18:14:04,219] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.41226548], dtype=float32), -0.86441743]
[2019-04-01 18:14:04,219] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.1, 73.33333333333333, 13.5, 66.5, 26.0, 25.09802231831584, 7.846715217832646, 0.0, 1.0, 33.02033474311572]
[2019-04-01 18:14:04,219] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 18:14:04,219] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 3.095182e-26], sampled 0.08623378065415521
[2019-04-01 18:14:22,458] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.41226548], dtype=float32), -0.86441743]
[2019-04-01 18:14:22,458] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.516666666666667, 37.66666666666667, 132.3333333333333, 737.6666666666666, 26.0, 25.13433456300773, 8.094537452997097, 0.0, 1.0, 19.95144402649136]
[2019-04-01 18:14:22,458] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 18:14:22,459] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 6.9064447e-38 0.0000000e+00 4.4998241e-37
 9.9997163e-01 2.8311941e-05], sampled 0.2652136783453166
[2019-04-01 18:14:48,231] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.41226548], dtype=float32), -0.86441743]
[2019-04-01 18:14:48,232] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [17.53333333333333, 62.0, 0.0, 0.0, 26.0, 28.62893431763267, 46.19937860771327, 0.0, 0.0, 0.0]
[2019-04-01 18:14:48,232] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 18:14:48,232] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.8210756206836316
[2019-04-01 18:14:51,262] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.41226548], dtype=float32), -0.86441743]
[2019-04-01 18:14:51,262] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 26.0, 24.86819662353102, 6.711484827002995, 0.0, 1.0, 41.91517972734223]
[2019-04-01 18:14:51,263] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 18:14:51,263] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.8120905e-25], sampled 0.30538297380615065
[2019-04-01 18:14:53,589] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 18:15:12,730] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 18:15:15,536] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 18:15:16,560] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 2600000, evaluation results [2600000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 18:15:34,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:15:34,683] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3223
[2019-04-01 18:15:34,712] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 72.0, 0.0, 0.0, 26.0, 24.54837430011369, 5.823810272582056, 0.0, 1.0, 38.52504292189092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 882600.0000, 
sim time next is 883200.0000, 
raw observation next is [-0.4, 72.0, 0.0, 0.0, 26.0, 24.593343326568, 5.839252132402369, 0.0, 1.0, 38.48824655041777], 
processed observation next is [1.0, 0.21739130434782608, 0.45152354570637127, 0.72, 0.0, 0.0, 1.0, 0.7990490466525715, 0.05839252132402369, 0.0, 1.0, 0.27491604678869835], 
reward next is 0.7251, 
noisyNet noise sample is [array([0.39714327], dtype=float32), 1.4370105]. 
=============================================
[2019-04-01 18:15:35,967] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:15:35,967] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1737
[2019-04-01 18:15:35,988] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 76.0, 0.0, 0.0, 26.0, 24.61244216697658, 5.974135137051505, 0.0, 1.0, 38.6132898887561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 878400.0000, 
sim time next is 879000.0000, 
raw observation next is [-1.1, 75.33333333333334, 0.0, 0.0, 26.0, 24.68363555781997, 5.931982528565928, 0.0, 1.0, 38.56312992670339], 
processed observation next is [1.0, 0.17391304347826086, 0.4321329639889197, 0.7533333333333334, 0.0, 0.0, 1.0, 0.8119479368314241, 0.059319825285659274, 0.0, 1.0, 0.27545092804788135], 
reward next is 0.7245, 
noisyNet noise sample is [array([-0.7266563], dtype=float32), -1.3218154]. 
=============================================
[2019-04-01 18:15:35,993] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[74.026794]
 [73.99321 ]
 [73.97538 ]
 [73.986855]
 [73.99544 ]], R is [[74.06448364]
 [74.04802704]
 [74.03128815]
 [74.01435089]
 [73.99737549]].
[2019-04-01 18:15:37,382] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7592355e-34 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:15:37,383] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2720
[2019-04-01 18:15:37,392] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 57.5, 0.0, 26.0, 25.74970894159603, 10.95418556190206, 1.0, 1.0, 9.266263215781594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1350000.0000, 
sim time next is 1350600.0000, 
raw observation next is [1.1, 92.16666666666667, 53.0, 0.0, 26.0, 25.71999062194657, 10.92865333184125, 1.0, 1.0, 10.08185245742763], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.9216666666666667, 0.17666666666666667, 0.0, 1.0, 0.9599986602780817, 0.1092865333184125, 1.0, 1.0, 0.07201323183876879], 
reward next is 0.5565, 
noisyNet noise sample is [array([-0.03758014], dtype=float32), -0.472184]. 
=============================================
[2019-04-01 18:15:40,627] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 6.1039436e-34], sum to 1.0000
[2019-04-01 18:15:40,627] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0204
[2019-04-01 18:15:40,635] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.75, 81.5, 100.0, 234.0, 26.0, 26.5292888771374, 17.10283574083721, 1.0, 1.0, 7.732516168492123], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1071000.0000, 
sim time next is 1071600.0000, 
raw observation next is [12.93333333333333, 81.0, 102.3333333333333, 195.0, 26.0, 26.58270478722343, 17.64924797020353, 1.0, 1.0, 7.475067309697098], 
processed observation next is [1.0, 0.391304347826087, 0.8208679593721145, 0.81, 0.341111111111111, 0.2154696132596685, 1.0, 1.0832435410319188, 0.1764924797020353, 1.0, 1.0, 0.053393337926407845], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0143104], dtype=float32), -1.6825538]. 
=============================================
[2019-04-01 18:15:41,944] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:15:41,945] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0736
[2019-04-01 18:15:41,987] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 92.0, 84.0, 0.0, 26.0, 25.5073434865045, 10.43630705190226, 1.0, 1.0, 19.90394128820072], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1432200.0000, 
sim time next is 1432800.0000, 
raw observation next is [1.1, 92.0, 81.0, 0.0, 26.0, 25.77569333452047, 10.71820710904586, 1.0, 1.0, 18.06876644059028], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.27, 0.0, 1.0, 0.9679561906457812, 0.1071820710904586, 1.0, 1.0, 0.1290626174327877], 
reward next is 0.5837, 
noisyNet noise sample is [array([0.5943934], dtype=float32), 0.31922877]. 
=============================================
[2019-04-01 18:16:06,647] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.6996533e-11], sum to 1.0000
[2019-04-01 18:16:06,649] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8473
[2019-04-01 18:16:06,661] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 78.0, 0.0, 0.0, 26.0, 24.30840771972294, 5.746937379229024, 0.0, 1.0, 45.34411101801216], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1818000.0000, 
sim time next is 1818600.0000, 
raw observation next is [-5.666666666666667, 78.83333333333333, 0.0, 0.0, 26.0, 24.27687761211762, 5.697940109127778, 0.0, 1.0, 45.42778495960234], 
processed observation next is [0.0, 0.043478260869565216, 0.30563250230840255, 0.7883333333333333, 0.0, 0.0, 1.0, 0.7538396588739457, 0.056979401091277776, 0.0, 1.0, 0.32448417828287385], 
reward next is 0.6755, 
noisyNet noise sample is [array([0.25728622], dtype=float32), 0.113360785]. 
=============================================
[2019-04-01 18:16:06,956] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.18655585e-35 0.00000000e+00 0.00000000e+00 1.04791556e-29
 0.00000000e+00 1.00000000e+00 0.00000000e+00], sum to 1.0000
[2019-04-01 18:16:06,959] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8537
[2019-04-01 18:16:06,983] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.866666666666667, 89.66666666666667, 96.0, 702.6666666666667, 26.0, 26.1020638638965, 12.80091469180282, 1.0, 1.0, 4.483098670560461], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1512600.0000, 
sim time next is 1513200.0000, 
raw observation next is [5.333333333333334, 86.33333333333334, 98.0, 701.3333333333334, 26.0, 26.16707423221028, 11.29020178974418, 1.0, 1.0, 4.482710407247723], 
processed observation next is [1.0, 0.5217391304347826, 0.6103416435826409, 0.8633333333333334, 0.32666666666666666, 0.7749539594843463, 1.0, 1.0238677474586113, 0.11290201789744181, 1.0, 1.0, 0.03201936005176945], 
reward next is 0.4519, 
noisyNet noise sample is [array([0.29059964], dtype=float32), -0.3849263]. 
=============================================
[2019-04-01 18:16:09,391] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:16:09,396] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9952
[2019-04-01 18:16:09,416] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.616666666666667, 73.83333333333334, 0.0, 0.0, 26.0, 25.74569523339038, 15.3405443355219, 0.0, 1.0, 34.26091216709151], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1543800.0000, 
sim time next is 1544400.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 26.0, 25.91672317613333, 16.26482881564487, 0.0, 1.0, 29.98736727388477], 
processed observation next is [1.0, 0.9130434782608695, 0.6759002770083103, 0.74, 0.0, 0.0, 1.0, 0.9881033108761902, 0.1626482881564487, 0.0, 1.0, 0.21419548052774837], 
reward next is 0.7858, 
noisyNet noise sample is [array([-0.28904024], dtype=float32), 1.4741577]. 
=============================================
[2019-04-01 18:16:11,424] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:16:11,425] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8302
[2019-04-01 18:16:11,436] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.7, 93.5, 0.0, 0.0, 26.0, 25.62196568398603, 11.6899669017811, 0.0, 1.0, 24.01390643823907], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1645800.0000, 
sim time next is 1646400.0000, 
raw observation next is [6.800000000000001, 94.0, 0.0, 0.0, 26.0, 25.5565191542844, 11.73225625748032, 0.0, 1.0, 27.88867060875675], 
processed observation next is [1.0, 0.043478260869565216, 0.6509695290858727, 0.94, 0.0, 0.0, 1.0, 0.9366455934691997, 0.1173225625748032, 0.0, 1.0, 0.1992047900625482], 
reward next is 0.8008, 
noisyNet noise sample is [array([1.563369], dtype=float32), 0.6677673]. 
=============================================
[2019-04-01 18:16:13,916] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4556476e-25 0.0000000e+00 1.7508592e-33 7.2747474e-38 0.0000000e+00
 1.0000000e+00 4.5068373e-31], sum to 1.0000
[2019-04-01 18:16:13,917] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6657
[2019-04-01 18:16:13,968] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.15832657596884, 10.11290334345738, 0.0, 1.0, 23.52965511657481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1713000.0000, 
sim time next is 1713600.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.12379061441877, 9.866358408567834, 0.0, 1.0, 22.26902781045458], 
processed observation next is [1.0, 0.8695652173913043, 0.49307479224376743, 0.88, 0.0, 0.0, 1.0, 0.8748272306312528, 0.09866358408567834, 0.0, 1.0, 0.15906448436038986], 
reward next is 0.8409, 
noisyNet noise sample is [array([0.9163207], dtype=float32), -2.270648]. 
=============================================
[2019-04-01 18:16:24,565] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7275496e-35 0.0000000e+00 4.2753539e-35 0.0000000e+00 2.5655928e-20
 1.0000000e+00 2.4305696e-24], sum to 1.0000
[2019-04-01 18:16:24,565] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7823
[2019-04-01 18:16:24,584] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.566666666666666, 76.33333333333334, 0.0, 0.0, 26.0, 24.51592981779948, 5.878573824531112, 0.0, 1.0, 44.31066374239322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1894800.0000, 
sim time next is 1895400.0000, 
raw observation next is [-6.75, 77.0, 0.0, 0.0, 26.0, 24.47697315237349, 5.822925876580908, 0.0, 1.0, 44.32314806116393], 
processed observation next is [0.0, 0.9565217391304348, 0.275623268698061, 0.77, 0.0, 0.0, 1.0, 0.7824247360533556, 0.05822925876580908, 0.0, 1.0, 0.31659391472259946], 
reward next is 0.6834, 
noisyNet noise sample is [array([-0.10234769], dtype=float32), -0.6957692]. 
=============================================
[2019-04-01 18:16:28,782] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:16:28,782] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2343
[2019-04-01 18:16:28,805] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.3, 78.0, 0.0, 0.0, 26.0, 23.98795859547779, 5.292014317304397, 0.0, 1.0, 44.01466521129854], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1911000.0000, 
sim time next is 1911600.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 26.0, 24.0673908980003, 5.319506790998723, 0.0, 1.0, 44.74088701290464], 
processed observation next is [1.0, 0.13043478260869565, 0.2299168975069252, 0.78, 0.0, 0.0, 1.0, 0.7239129854286145, 0.05319506790998723, 0.0, 1.0, 0.3195777643778903], 
reward next is 0.6804, 
noisyNet noise sample is [array([-0.1956473], dtype=float32), -0.81354916]. 
=============================================
[2019-04-01 18:16:43,412] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:16:43,414] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0844
[2019-04-01 18:16:43,455] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.633333333333333, 79.66666666666667, 202.3333333333333, 69.66666666666666, 26.0, 25.96139389056014, 9.182010888469774, 1.0, 1.0, 20.93374228089048], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2110800.0000, 
sim time next is 2111400.0000, 
raw observation next is [-7.55, 78.5, 208.0, 60.0, 26.0, 25.96108696375084, 9.07433182313452, 1.0, 1.0, 19.91276179576075], 
processed observation next is [1.0, 0.43478260869565216, 0.25346260387811637, 0.785, 0.6933333333333334, 0.06629834254143646, 1.0, 0.9944409948215487, 0.0907433182313452, 1.0, 1.0, 0.14223401282686252], 
reward next is 0.8578, 
noisyNet noise sample is [array([-0.6155738], dtype=float32), -0.25587007]. 
=============================================
[2019-04-01 18:16:47,819] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 2.720553e-38], sum to 1.0000
[2019-04-01 18:16:47,819] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8984
[2019-04-01 18:16:47,844] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.466666666666667, 53.66666666666667, 0.0, 0.0, 26.0, 24.02056128861541, 5.296039608852648, 0.0, 1.0, 43.02975850643609], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2427600.0000, 
sim time next is 2428200.0000, 
raw observation next is [-7.55, 54.0, 0.0, 0.0, 26.0, 23.99151329936018, 5.284784355147859, 0.0, 1.0, 43.07168366411513], 
processed observation next is [0.0, 0.08695652173913043, 0.25346260387811637, 0.54, 0.0, 0.0, 1.0, 0.7130733284800256, 0.05284784355147859, 0.0, 1.0, 0.30765488331510804], 
reward next is 0.6923, 
noisyNet noise sample is [array([0.92507654], dtype=float32), -0.5341151]. 
=============================================
[2019-04-01 18:17:15,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:17:15,058] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2393
[2019-04-01 18:17:15,088] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 25.02642557610081, 6.823388848503456, 0.0, 1.0, 54.69017049075256], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2867400.0000, 
sim time next is 2868000.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.9702446345179, 6.883463744634703, 0.0, 1.0, 54.69856811382954], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 1.0, 0.8528920906454144, 0.06883463744634703, 0.0, 1.0, 0.3907040579559253], 
reward next is 0.6093, 
noisyNet noise sample is [array([0.68792486], dtype=float32), 0.017233832]. 
=============================================
[2019-04-01 18:17:15,115] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[75.40126]
 [75.44764]
 [75.46318]
 [75.44028]
 [75.41615]], R is [[75.18606567]
 [75.04356384]
 [74.90276337]
 [74.7634964 ]
 [74.62551117]].
[2019-04-01 18:17:23,191] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:17:23,192] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1262
[2019-04-01 18:17:23,212] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.88725458022457, 7.680087863735731, 0.0, 1.0, 42.81442874762236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2938800.0000, 
sim time next is 2939400.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.84765532087502, 7.647201580041343, 0.0, 1.0, 42.80149565527802], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 1.0, 0.8353793315535744, 0.07647201580041343, 0.0, 1.0, 0.30572496896627155], 
reward next is 0.6943, 
noisyNet noise sample is [array([-0.95947844], dtype=float32), 0.16761173]. 
=============================================
[2019-04-01 18:17:24,268] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:17:24,269] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9106
[2019-04-01 18:17:24,303] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.666666666666667, 62.33333333333333, 0.0, 0.0, 26.0, 25.36358318554163, 10.20667416406579, 0.0, 1.0, 54.90026050078843], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2752800.0000, 
sim time next is 2753400.0000, 
raw observation next is [-5.833333333333333, 63.16666666666666, 0.0, 0.0, 26.0, 25.44832699933761, 10.15431515111804, 0.0, 1.0, 48.10549133739166], 
processed observation next is [1.0, 0.8695652173913043, 0.30101569713758086, 0.6316666666666666, 0.0, 0.0, 1.0, 0.9211895713339443, 0.10154315151118039, 0.0, 1.0, 0.34361065240994043], 
reward next is 0.6564, 
noisyNet noise sample is [array([1.1806853], dtype=float32), 0.24022521]. 
=============================================
[2019-04-01 18:17:30,292] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:17:30,292] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1729
[2019-04-01 18:17:30,334] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 97.66666666666667, 0.0, 0.0, 26.0, 24.92502278829316, 6.557300380789111, 0.0, 1.0, 54.33977785938992], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2874000.0000, 
sim time next is 2874600.0000, 
raw observation next is [1.5, 96.5, 0.0, 0.0, 26.0, 24.88069197990521, 6.639495546304353, 0.0, 1.0, 54.26482583496945], 
processed observation next is [1.0, 0.2608695652173913, 0.5041551246537397, 0.965, 0.0, 0.0, 1.0, 0.8400988542721731, 0.06639495546304353, 0.0, 1.0, 0.3876058988212104], 
reward next is 0.6124, 
noisyNet noise sample is [array([-0.2202785], dtype=float32), -0.29062298]. 
=============================================
[2019-04-01 18:17:33,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.6324286e-37
 1.0000000e+00 1.3118917e-28], sum to 1.0000
[2019-04-01 18:17:33,588] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0842
[2019-04-01 18:17:33,600] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 96.5, 0.0, 0.0, 26.0, 25.10301052336651, 7.06524533190763, 0.0, 1.0, 54.08792529342669], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2871000.0000, 
sim time next is 2871600.0000, 
raw observation next is [1.0, 97.66666666666666, 0.0, 0.0, 26.0, 25.14601457010779, 6.834082143656637, 0.0, 1.0, 55.33311898608988], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9766666666666666, 0.0, 0.0, 1.0, 0.8780020814439702, 0.06834082143656638, 0.0, 1.0, 0.3952365641863563], 
reward next is 0.6048, 
noisyNet noise sample is [array([-0.00080525], dtype=float32), 0.77991396]. 
=============================================
[2019-04-01 18:17:38,525] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8473740e-31
 1.0000000e+00 1.4364335e-38], sum to 1.0000
[2019-04-01 18:17:38,525] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3472
[2019-04-01 18:17:38,556] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 84.16666666666666, 0.0, 0.0, 26.0, 24.65144061591307, 6.822812476943149, 0.0, 1.0, 42.22430830645497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2947800.0000, 
sim time next is 2948400.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.6256230497117, 6.715077856787104, 0.0, 1.0, 42.16480039924668], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 1.0, 0.8036604356730999, 0.06715077856787105, 0.0, 1.0, 0.30117714570890486], 
reward next is 0.6988, 
noisyNet noise sample is [array([0.60705155], dtype=float32), 0.067610644]. 
=============================================
[2019-04-01 18:17:40,910] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0252973e-34 0.0000000e+00 1.3942654e-35 0.0000000e+00 4.9011704e-36
 1.4544328e-04 9.9985456e-01], sum to 1.0000
[2019-04-01 18:17:40,911] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3223
[2019-04-01 18:17:40,945] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 98.0, 737.0, 26.0, 25.19664427240567, 9.511274391793222, 0.0, 1.0, 19.02300150822751], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2990400.0000, 
sim time next is 2991000.0000, 
raw observation next is [-2.0, 60.0, 95.0, 723.0, 26.0, 25.19513531936523, 9.42536221576325, 0.0, 1.0, 17.55195346717997], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6, 0.31666666666666665, 0.7988950276243094, 1.0, 0.8850193313378902, 0.0942536221576325, 0.0, 1.0, 0.12537109619414263], 
reward next is 0.8746, 
noisyNet noise sample is [array([-1.8460157], dtype=float32), 0.2126748]. 
=============================================
[2019-04-01 18:17:40,949] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[55.35124 ]
 [55.438602]
 [55.562115]
 [55.706905]
 [55.811275]], R is [[55.58012772]
 [55.88844681]
 [56.14593124]
 [56.3789444 ]
 [56.61510849]].
[2019-04-01 18:17:45,802] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:17:45,805] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1683
[2019-04-01 18:17:45,829] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 100.0, 42.0, 237.0, 26.0, 25.46700889199392, 7.961636193382937, 1.0, 1.0, 29.87442547092686], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3139200.0000, 
sim time next is 3139800.0000, 
raw observation next is [6.166666666666666, 100.0, 55.66666666666668, 288.6666666666667, 26.0, 25.51705298706301, 8.011751861887829, 1.0, 1.0, 25.80910691652184], 
processed observation next is [1.0, 0.34782608695652173, 0.6334256694367498, 1.0, 0.18555555555555558, 0.31896869244935544, 1.0, 0.9310075695804301, 0.0801175186188783, 1.0, 1.0, 0.18435076368944173], 
reward next is 0.8156, 
noisyNet noise sample is [array([-0.70719403], dtype=float32), -1.8811243]. 
=============================================
[2019-04-01 18:17:48,205] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6560584e-30 0.0000000e+00 1.2960795e-34 3.7354993e-17 9.9188192e-37
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:17:48,205] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6865
[2019-04-01 18:17:48,233] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 100.5, 663.5, 26.0, 26.34064174034935, 12.20489871304754, 1.0, 1.0, 13.711444563569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3145200.0000, 
sim time next is 3145800.0000, 
raw observation next is [7.0, 100.0, 102.0, 680.0, 26.0, 26.45064960565092, 12.38555210530751, 1.0, 1.0, 12.58611062975777], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.34, 0.7513812154696132, 1.0, 1.0643785150929885, 0.12385552105307511, 1.0, 1.0, 0.0899007902125555], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8022399], dtype=float32), 0.37187728]. 
=============================================
[2019-04-01 18:18:01,857] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:18:01,859] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2288
[2019-04-01 18:18:01,891] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 54.16666666666667, 0.0, 0.0, 26.0, 25.41161675310212, 10.80804429564231, 1.0, 1.0, 30.28087652811117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3347400.0000, 
sim time next is 3348000.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.50545086048756, 11.54778166934448, 1.0, 1.0, 26.85528070107964], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 1.0, 0.9293501229267943, 0.11547781669344481, 1.0, 1.0, 0.19182343357914028], 
reward next is 0.1891, 
noisyNet noise sample is [array([-0.22975628], dtype=float32), -0.3197208]. 
=============================================
[2019-04-01 18:18:01,894] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[50.292015]
 [49.794243]
 [49.708393]
 [49.10817 ]
 [48.445408]], R is [[50.19947052]
 [50.15796661]
 [50.2581749 ]
 [50.31346893]
 [50.31520081]].
[2019-04-01 18:18:04,084] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.6826218e-30 6.9267237e-37 1.5421062e-34 4.2801602e-31 3.1493470e-20
 1.0000000e+00 4.6198628e-30], sum to 1.0000
[2019-04-01 18:18:04,084] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0732
[2019-04-01 18:18:04,124] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8333333333333334, 71.16666666666667, 31.33333333333333, 204.3333333333333, 26.0, 25.32772541608146, 8.206021099285472, 1.0, 1.0, 32.95995685973343], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3484200.0000, 
sim time next is 3484800.0000, 
raw observation next is [-1.0, 71.0, 45.5, 253.0, 26.0, 25.24723248367274, 7.998968185672119, 1.0, 1.0, 30.06512095339785], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.15166666666666667, 0.2795580110497238, 1.0, 0.8924617833818199, 0.07998968185672119, 1.0, 1.0, 0.2147508639528418], 
reward next is 0.7852, 
noisyNet noise sample is [array([1.9406114], dtype=float32), 0.25436983]. 
=============================================
[2019-04-01 18:18:07,191] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:18:07,191] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5297
[2019-04-01 18:18:07,256] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.36745702288637, 8.534461846018255, 0.0, 1.0, 48.65075249857688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3482400.0000, 
sim time next is 3483000.0000, 
raw observation next is [-0.5, 71.5, 3.0, 107.0, 26.0, 25.46871421309613, 8.61264730748004, 1.0, 1.0, 42.62156633763383], 
processed observation next is [1.0, 0.30434782608695654, 0.44875346260387816, 0.715, 0.01, 0.11823204419889503, 1.0, 0.9241020304423044, 0.08612647307480041, 1.0, 1.0, 0.30443975955452734], 
reward next is 0.6956, 
noisyNet noise sample is [array([-0.11646788], dtype=float32), 0.9638348]. 
=============================================
[2019-04-01 18:18:07,276] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[65.40685]
 [65.44666]
 [65.96979]
 [66.14422]
 [66.33196]], R is [[58.16881561]
 [58.23962402]
 [58.1624794 ]
 [58.19437408]
 [58.26021576]].
[2019-04-01 18:18:07,550] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:18:07,552] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1115
[2019-04-01 18:18:07,567] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 69.5, 0.0, 0.0, 26.0, 25.20112078486843, 8.30210384311462, 0.0, 1.0, 42.95603346518207], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3472200.0000, 
sim time next is 3472800.0000, 
raw observation next is [0.3333333333333334, 70.33333333333334, 0.0, 0.0, 26.0, 25.1913385336418, 8.324840182653181, 0.0, 1.0, 41.9998369681648], 
processed observation next is [1.0, 0.17391304347826086, 0.4718374884579871, 0.7033333333333335, 0.0, 0.0, 1.0, 0.8844769333774002, 0.08324840182653181, 0.0, 1.0, 0.29999883548689144], 
reward next is 0.7000, 
noisyNet noise sample is [array([0.41912085], dtype=float32), -0.74057674]. 
=============================================
[2019-04-01 18:18:08,781] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9458593e-07 4.6178388e-21 1.7158051e-15 5.0367771e-10 4.6849994e-28
 9.9999976e-01 2.0289755e-22], sum to 1.0000
[2019-04-01 18:18:08,782] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5998
[2019-04-01 18:18:08,795] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 60.0, 110.0, 775.0, 26.0, 26.41292416116046, 12.66166350142656, 1.0, 1.0, 12.51688184954641], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3839400.0000, 
sim time next is 3840000.0000, 
raw observation next is [-1.333333333333333, 60.0, 111.1666666666667, 782.8333333333334, 26.0, 26.43467362843376, 12.93865792017156, 1.0, 1.0, 10.86831478022498], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.6, 0.3705555555555557, 0.8650092081031308, 1.0, 1.0620962326333943, 0.1293865792017156, 1.0, 1.0, 0.07763081985874985], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4384362], dtype=float32), 0.07783846]. 
=============================================
[2019-04-01 18:18:08,800] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[36.128075]
 [35.6451  ]
 [35.5373  ]
 [35.30869 ]
 [34.704903]], R is [[36.08733749]
 [35.72646332]
 [35.36919785]
 [35.0556488 ]
 [34.86826324]].
[2019-04-01 18:18:09,074] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.3981884e-38], sum to 1.0000
[2019-04-01 18:18:09,075] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6814
[2019-04-01 18:18:09,129] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.1666666666666667, 71.83333333333333, 0.0, 0.0, 26.0, 25.17655451240056, 8.240390203973055, 0.0, 1.0, 69.264781543863], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3481800.0000, 
sim time next is 3482400.0000, 
raw observation next is [-0.3333333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.36745702288637, 8.534461846018255, 0.0, 1.0, 48.65075249857688], 
processed observation next is [1.0, 0.30434782608695654, 0.4533702677747, 0.7166666666666667, 0.0, 0.0, 1.0, 0.9096367175551957, 0.08534461846018254, 0.0, 1.0, 0.34750537498983486], 
reward next is 0.6525, 
noisyNet noise sample is [array([0.19533853], dtype=float32), 1.115835]. 
=============================================
[2019-04-01 18:18:16,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 2.067904e-38 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:18:16,650] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3475
[2019-04-01 18:18:16,683] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 77.0, 94.5, 552.0, 26.0, 25.64474413671516, 9.04279188556712, 1.0, 1.0, 24.60404654878605], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3747600.0000, 
sim time next is 3748200.0000, 
raw observation next is [-3.833333333333333, 77.0, 96.33333333333333, 593.0, 26.0, 25.80327603879694, 9.626045862369004, 1.0, 1.0, 25.38627842926581], 
processed observation next is [1.0, 0.391304347826087, 0.3564173591874424, 0.77, 0.32111111111111107, 0.6552486187845303, 1.0, 0.9718965769709916, 0.09626045862369004, 1.0, 1.0, 0.1813305602090415], 
reward next is 0.8187, 
noisyNet noise sample is [array([0.1192764], dtype=float32), -0.7142449]. 
=============================================
[2019-04-01 18:18:20,607] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.0432251e-14], sum to 1.0000
[2019-04-01 18:18:20,608] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2987
[2019-04-01 18:18:20,627] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 45.0, 104.0, 776.0, 26.0, 25.47238606881967, 10.40123280561788, 0.0, 1.0, 3.816410703543824], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3681000.0000, 
sim time next is 3681600.0000, 
raw observation next is [6.0, 45.66666666666667, 101.1666666666667, 764.1666666666667, 26.0, 25.53425217027434, 10.48906594802539, 0.0, 1.0, 5.523368735989068], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.4566666666666667, 0.3372222222222223, 0.8443830570902395, 1.0, 0.933464595753477, 0.1048906594802539, 0.0, 1.0, 0.039452633828493344], 
reward next is 0.9605, 
noisyNet noise sample is [array([-1.4490088], dtype=float32), -1.4034692]. 
=============================================
[2019-04-01 18:18:23,413] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:18:23,414] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5396
[2019-04-01 18:18:23,473] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666667, 38.66666666666667, 0.0, 0.0, 26.0, 24.80865989166852, 7.351304700582927, 0.0, 1.0, 58.27061839945954], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4087200.0000, 
sim time next is 4087800.0000, 
raw observation next is [-4.5, 37.5, 0.0, 0.0, 26.0, 25.44666063283586, 7.655885721860322, 1.0, 1.0, 43.46117356131369], 
processed observation next is [1.0, 0.30434782608695654, 0.3379501385041552, 0.375, 0.0, 0.0, 1.0, 0.9209515189765517, 0.07655885721860323, 1.0, 1.0, 0.31043695400938354], 
reward next is 0.6896, 
noisyNet noise sample is [array([-0.2638736], dtype=float32), 0.8771606]. 
=============================================
[2019-04-01 18:18:24,652] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.4008425e-28
 1.0000000e+00 1.4013198e-36], sum to 1.0000
[2019-04-01 18:18:24,653] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7589
[2019-04-01 18:18:24,780] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.63739868207496, 6.252264735592625, 0.0, 1.0, 42.19322325330218], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3913200.0000, 
sim time next is 3913800.0000, 
raw observation next is [-7.166666666666667, 63.0, 0.0, 0.0, 26.0, 24.5610550905131, 7.630577889169456, 0.0, 1.0, 108.3854706598444], 
processed observation next is [1.0, 0.30434782608695654, 0.26408125577100644, 0.63, 0.0, 0.0, 1.0, 0.7944364415018715, 0.07630577889169456, 0.0, 1.0, 0.7741819332846028], 
reward next is 0.2258, 
noisyNet noise sample is [array([-1.3167152], dtype=float32), 0.90879077]. 
=============================================
[2019-04-01 18:18:33,580] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:18:33,582] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4243
[2019-04-01 18:18:33,611] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 34.0, 0.0, 0.0, 26.0, 25.36357715289597, 8.874387696871915, 0.0, 1.0, 40.14368674217358], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4055400.0000, 
sim time next is 4056000.0000, 
raw observation next is [-5.666666666666667, 35.0, 0.0, 0.0, 26.0, 25.3112073978755, 8.731360952181937, 0.0, 1.0, 40.72912522254651], 
processed observation next is [1.0, 0.9565217391304348, 0.30563250230840255, 0.35, 0.0, 0.0, 1.0, 0.901601056839357, 0.08731360952181938, 0.0, 1.0, 0.2909223230181893], 
reward next is 0.7091, 
noisyNet noise sample is [array([0.50070065], dtype=float32), 0.07176468]. 
=============================================
[2019-04-01 18:18:33,629] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.64636]
 [72.76355]
 [73.08264]
 [73.39112]
 [73.61274]], R is [[72.3972168 ]
 [72.38650513]
 [72.37025452]
 [72.33959198]
 [72.3687973 ]].
[2019-04-01 18:18:34,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.0153850e-21 8.4972758e-34 4.1600819e-27 9.3035330e-19 8.5516694e-21
 1.0000000e+00 7.6446497e-19], sum to 1.0000
[2019-04-01 18:18:34,369] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6539
[2019-04-01 18:18:34,384] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.66666666666667, 61.33333333333334, 0.0, 0.0, 26.0, 24.60496308133776, 6.3443582943946, 0.0, 1.0, 43.20796512694962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3980400.0000, 
sim time next is 3981000.0000, 
raw observation next is [-11.83333333333333, 62.16666666666666, 0.0, 0.0, 26.0, 24.53014503144951, 6.188029466618197, 0.0, 1.0, 43.21900648385719], 
processed observation next is [1.0, 0.043478260869565216, 0.13481071098799638, 0.6216666666666666, 0.0, 0.0, 1.0, 0.7900207187785016, 0.061880294666181966, 0.0, 1.0, 0.3087071891704085], 
reward next is 0.6913, 
noisyNet noise sample is [array([0.35155395], dtype=float32), -0.944557]. 
=============================================
[2019-04-01 18:18:34,391] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[56.12147 ]
 [56.44285 ]
 [56.77692 ]
 [57.1535  ]
 [57.523838]], R is [[56.00233078]
 [56.13367844]
 [56.26385498]
 [56.3927536 ]
 [56.52002335]].
[2019-04-01 18:18:34,664] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6400651e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.4549030e-37], sum to 1.0000
[2019-04-01 18:18:34,666] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8371
[2019-04-01 18:18:34,679] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 39.5, 0.0, 0.0, 26.0, 24.62347002633528, 5.876759329798577, 0.0, 1.0, 39.58155153476631], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4084200.0000, 
sim time next is 4084800.0000, 
raw observation next is [-4.666666666666666, 40.0, 0.0, 0.0, 26.0, 24.62301837418923, 5.851409032243244, 0.0, 1.0, 39.52157148171752], 
processed observation next is [1.0, 0.2608695652173913, 0.33333333333333337, 0.4, 0.0, 0.0, 1.0, 0.8032883391698898, 0.05851409032243244, 0.0, 1.0, 0.28229693915512516], 
reward next is 0.7177, 
noisyNet noise sample is [array([-0.96404433], dtype=float32), 0.8181338]. 
=============================================
[2019-04-01 18:18:37,241] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.587885e-36 0.000000e+00 0.000000e+00 0.000000e+00 9.487969e-19
 1.000000e+00 3.662724e-21], sum to 1.0000
[2019-04-01 18:18:37,245] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1145
[2019-04-01 18:18:37,255] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.27030387183024, 8.52256474648184, 0.0, 1.0, 40.49269647215781], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4057200.0000, 
sim time next is 4057800.0000, 
raw observation next is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.24594884622354, 8.402251790869421, 0.0, 1.0, 40.34908178137587], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.37, 0.0, 0.0, 1.0, 0.892278406603363, 0.08402251790869421, 0.0, 1.0, 0.28820772700982766], 
reward next is 0.7118, 
noisyNet noise sample is [array([-1.0598941], dtype=float32), -0.9405223]. 
=============================================
[2019-04-01 18:18:40,064] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:18:40,067] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8529
[2019-04-01 18:18:40,116] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 36.0, 0.0, 0.0, 26.0, 24.86938576771266, 9.73011312257151, 1.0, 1.0, 37.15559355031569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4128000.0000, 
sim time next is 4128600.0000, 
raw observation next is [3.0, 36.5, 0.0, 0.0, 26.0, 25.10121936956201, 10.77995044466407, 1.0, 1.0, 19.18646649996005], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.365, 0.0, 0.0, 1.0, 0.8716027670802872, 0.1077995044466407, 1.0, 1.0, 0.13704618928542894], 
reward next is 0.5510, 
noisyNet noise sample is [array([-1.0966526], dtype=float32), -0.21669476]. 
=============================================
[2019-04-01 18:18:41,683] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9614114e-26 6.4094671e-31 6.6561305e-30 9.0328872e-27 1.7888118e-28
 1.0000000e+00 4.1033873e-28], sum to 1.0000
[2019-04-01 18:18:41,685] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2311
[2019-04-01 18:18:41,699] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.5, 33.0, 106.0, 794.0, 26.0, 27.01760739566149, 16.82092970038407, 1.0, 1.0, 1.973028116771682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4113000.0000, 
sim time next is 4113600.0000, 
raw observation next is [3.666666666666667, 33.66666666666666, 104.0, 777.5, 26.0, 27.02002392218382, 17.09453584107963, 1.0, 1.0, 1.898176921098851], 
processed observation next is [1.0, 0.6086956521739131, 0.564173591874423, 0.33666666666666656, 0.3466666666666667, 0.8591160220994475, 1.0, 1.1457177031691173, 0.1709453584107963, 1.0, 1.0, 0.013558406579277508], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0644251], dtype=float32), 1.0412879]. 
=============================================
[2019-04-01 18:18:41,954] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:18:41,955] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0170
[2019-04-01 18:18:41,970] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.666666666666667, 46.0, 0.0, 0.0, 26.0, 25.44610228080599, 7.725168534959313, 0.0, 1.0, 36.35905697961387], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4239600.0000, 
sim time next is 4240200.0000, 
raw observation next is [2.833333333333333, 45.5, 0.0, 0.0, 26.0, 25.44582828929281, 7.662329880017947, 0.0, 1.0, 35.71351880145984], 
processed observation next is [0.0, 0.043478260869565216, 0.541089566020314, 0.455, 0.0, 0.0, 1.0, 0.9208326127561156, 0.07662329880017947, 0.0, 1.0, 0.2550965628675703], 
reward next is 0.7449, 
noisyNet noise sample is [array([-0.25065166], dtype=float32), 0.34824088]. 
=============================================
[2019-04-01 18:18:49,474] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-01 18:18:49,475] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 18:18:49,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:18:49,476] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 18:18:49,477] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:18:49,477] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 18:18:49,478] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:18:49,484] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run28
[2019-04-01 18:18:49,514] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run28
[2019-04-01 18:18:49,537] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run28
[2019-04-01 18:18:57,324] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.53307617], dtype=float32), -0.94774425]
[2019-04-01 18:18:57,324] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [6.8, 93.0, 144.5, 17.5, 26.0, 24.99104325597895, 6.367181093439925, 1.0, 1.0, 7.538616981595378]
[2019-04-01 18:18:57,324] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 18:18:57,325] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.4684511011318767
[2019-04-01 18:19:36,657] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.53307617], dtype=float32), -0.94774425]
[2019-04-01 18:19:36,658] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [9.95, 71.0, 0.0, 0.0, 26.0, 25.60301669424103, 11.7155248516279, 0.0, 1.0, 34.36676528277192]
[2019-04-01 18:19:36,658] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 18:19:36,658] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.6655709e-34], sampled 0.8679273202630674
[2019-04-01 18:20:08,413] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.53307617], dtype=float32), -0.94774425]
[2019-04-01 18:20:08,413] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.7860928711666668, 40.76331766833333, 74.81296381333333, 824.9527786000001, 26.0, 25.62211536386068, 8.109611597319004, 1.0, 1.0, 22.66572801174608]
[2019-04-01 18:20:08,414] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 18:20:08,415] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.6358561e-37 0.0000000e+00 0.0000000e+00 1.0812627e-36 0.0000000e+00
 1.0000000e+00 2.1244104e-30], sampled 0.7510641963344634
[2019-04-01 18:20:31,007] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 18:20:42,053] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.53307617], dtype=float32), -0.94774425]
[2019-04-01 18:20:42,054] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.833333333333333, 52.83333333333334, 0.0, 0.0, 26.0, 25.83380397969362, 12.7024487316536, 0.0, 1.0, 4.909123220469112]
[2019-04-01 18:20:42,055] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 18:20:42,056] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.3421530133048808
[2019-04-01 18:20:50,061] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 18:20:53,707] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 18:20:54,730] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 2700000, evaluation results [2700000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 18:20:59,956] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:20:59,957] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5039
[2019-04-01 18:20:59,978] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.9, 39.0, 19.0, 0.0, 26.0, 27.98671156499051, 28.5976720955181, 1.0, 1.0, 4.96358724340782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4381800.0000, 
sim time next is 4382400.0000, 
raw observation next is [12.8, 40.0, 14.0, 0.0, 26.0, 28.24117310613421, 28.16753656716861, 1.0, 1.0, 5.683768930207165], 
processed observation next is [1.0, 0.7391304347826086, 0.8171745152354571, 0.4, 0.04666666666666667, 0.0, 1.0, 1.3201675865906013, 0.2816753656716861, 1.0, 1.0, 0.04059834950147975], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1267782], dtype=float32), -0.6201082]. 
=============================================
[2019-04-01 18:21:02,541] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 6.573609e-25], sum to 1.0000
[2019-04-01 18:21:02,542] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0442
[2019-04-01 18:21:02,551] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 114.0, 732.0, 26.0, 25.2071699186375, 9.772529571176698, 0.0, 1.0, 5.16901229531254], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4806600.0000, 
sim time next is 4807200.0000, 
raw observation next is [3.0, 37.0, 105.5, 729.5, 26.0, 25.21217972131393, 9.760928795228411, 0.0, 1.0, 5.044051106943196], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.3516666666666667, 0.8060773480662984, 1.0, 0.8874542459019901, 0.09760928795228412, 0.0, 1.0, 0.036028936478165685], 
reward next is 0.9640, 
noisyNet noise sample is [array([-1.7057593], dtype=float32), 0.8337517]. 
=============================================
[2019-04-01 18:21:05,664] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:21:05,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9842
[2019-04-01 18:21:05,691] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.50476881774068, 7.432585677102352, 0.0, 1.0, 32.90565726985815], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4933200.0000, 
sim time next is 4933800.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.48214125308656, 7.235439528857488, 0.0, 1.0, 27.60311251075502], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 1.0, 0.9260201790123658, 0.07235439528857487, 0.0, 1.0, 0.19716508936253588], 
reward next is 0.8028, 
noisyNet noise sample is [array([-0.797755], dtype=float32), -0.075574964]. 
=============================================
[2019-04-01 18:21:05,992] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4408898e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.3373328e-33
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:21:05,996] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5203
[2019-04-01 18:21:06,008] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.45119261993853, 9.41542435034839, 1.0, 1.0, 10.44206798743754], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4560000.0000, 
sim time next is 4560600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.30542408736145, 9.231188737112106, 1.0, 1.0, 10.51170601421594], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.9007748696230645, 0.09231188737112106, 1.0, 1.0, 0.07508361438725672], 
reward next is 0.9249, 
noisyNet noise sample is [array([0.9617255], dtype=float32), -0.48738012]. 
=============================================
[2019-04-01 18:21:06,665] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:21:06,668] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3270
[2019-04-01 18:21:06,722] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 24.84598816863151, 8.912086796516975, 0.0, 1.0, 44.27693862879309], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4564200.0000, 
sim time next is 4564800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 24.91673694315713, 9.333731232668079, 1.0, 1.0, 33.3049477375988], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.8452481347367328, 0.09333731232668079, 1.0, 1.0, 0.23789248383999145], 
reward next is 0.7621, 
noisyNet noise sample is [array([0.87017465], dtype=float32), 0.5152256]. 
=============================================
[2019-04-01 18:21:07,782] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1259159e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.5588916e-32
 1.0000000e+00 3.6120274e-34], sum to 1.0000
[2019-04-01 18:21:07,787] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1194
[2019-04-01 18:21:07,798] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.916666666666667, 70.66666666666667, 0.0, 0.0, 26.0, 25.28134760415121, 8.487332984716195, 0.0, 1.0, 36.10359828279167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4596600.0000, 
sim time next is 4597200.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.29134004688441, 8.508371490991562, 0.0, 1.0, 35.9340045669274], 
processed observation next is [1.0, 0.21739130434782608, 0.40720221606648205, 0.71, 0.0, 0.0, 1.0, 0.8987628638406301, 0.08508371490991562, 0.0, 1.0, 0.2566714611923386], 
reward next is 0.7433, 
noisyNet noise sample is [array([1.8756112], dtype=float32), -0.1383441]. 
=============================================
[2019-04-01 18:21:08,989] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:21:08,993] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5389
[2019-04-01 18:21:08,999] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 26.0, 0.0, 0.0, 26.0, 25.80500490102198, 11.37814659069631, 0.0, 1.0, 6.100374116726349], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4998600.0000, 
sim time next is 4999200.0000, 
raw observation next is [4.666666666666666, 27.0, 0.0, 0.0, 26.0, 25.74312640754646, 11.07748137525838, 0.0, 1.0, 6.208466606581811], 
processed observation next is [1.0, 0.8695652173913043, 0.5918744228993538, 0.27, 0.0, 0.0, 1.0, 0.963303772506637, 0.1107748137525838, 0.0, 1.0, 0.044346190047012936], 
reward next is 0.9557, 
noisyNet noise sample is [array([0.06239862], dtype=float32), -0.17478064]. 
=============================================
[2019-04-01 18:21:09,365] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:21:09,365] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0552
[2019-04-01 18:21:09,375] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 57.00000000000001, 0.0, 0.0, 26.0, 25.83531817122945, 10.99221020709467, 0.0, 1.0, 17.44900259213676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4659600.0000, 
sim time next is 4660200.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 26.0, 25.65401063621976, 10.37565042852141, 0.0, 1.0, 15.36495556794184], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.57, 0.0, 0.0, 1.0, 0.9505729480313941, 0.1037565042852141, 0.0, 1.0, 0.109749682628156], 
reward next is 0.8903, 
noisyNet noise sample is [array([-1.038205], dtype=float32), 1.2893155]. 
=============================================
[2019-04-01 18:21:14,582] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:14,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:14,600] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run21
[2019-04-01 18:21:15,178] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:21:15,179] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2290
[2019-04-01 18:21:15,191] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.63123037478056, 10.27144268649404, 0.0, 1.0, 21.57091014427677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4667400.0000, 
sim time next is 4668000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.56798999927607, 10.0623278168669, 0.0, 1.0, 24.21085804727574], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.938284285610867, 0.10062327816866899, 0.0, 1.0, 0.17293470033768385], 
reward next is 0.8271, 
noisyNet noise sample is [array([1.8152527], dtype=float32), -1.5366538]. 
=============================================
[2019-04-01 18:21:15,201] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[75.80807 ]
 [76.336555]
 [76.96703 ]
 [77.65864 ]
 [78.422585]], R is [[75.29152679]
 [75.38453674]
 [75.46842957]
 [75.54267883]
 [75.60663605]].
[2019-04-01 18:21:17,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:17,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:17,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run21
[2019-04-01 18:21:18,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:18,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:18,244] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run21
[2019-04-01 18:21:23,434] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.3854504e-26 1.9960470e-32 1.3747314e-26 3.8886042e-17 2.3003124e-32
 1.0000000e+00 4.6825857e-29], sum to 1.0000
[2019-04-01 18:21:23,438] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6238
[2019-04-01 18:21:23,452] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 29.66666666666667, 115.5, 788.6666666666667, 26.0, 26.54465683288272, 11.99788599662161, 1.0, 1.0, 10.70121646106961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4962000.0000, 
sim time next is 4962600.0000, 
raw observation next is [2.0, 29.5, 117.0, 803.0, 26.0, 26.57855219084493, 12.40822468720624, 1.0, 1.0, 9.780026021033907], 
processed observation next is [1.0, 0.43478260869565216, 0.518005540166205, 0.295, 0.39, 0.887292817679558, 1.0, 1.0826503129778473, 0.12408224687206239, 1.0, 1.0, 0.06985732872167076], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29778922], dtype=float32), 0.37094852]. 
=============================================
[2019-04-01 18:21:23,843] A3C_AGENT_WORKER-Thread-18 INFO:Local step 170000, global step 2714847: loss 0.3438
[2019-04-01 18:21:23,845] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 170000, global step 2714847: learning rate 0.0010
[2019-04-01 18:21:26,606] A3C_AGENT_WORKER-Thread-17 INFO:Local step 170000, global step 2716291: loss 0.0531
[2019-04-01 18:21:26,607] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 170000, global step 2716292: learning rate 0.0010
[2019-04-01 18:21:27,371] A3C_AGENT_WORKER-Thread-4 INFO:Local step 170000, global step 2716730: loss 0.4409
[2019-04-01 18:21:27,372] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 170000, global step 2716730: learning rate 0.0010
[2019-04-01 18:21:29,097] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:21:29,097] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4001
[2019-04-01 18:21:29,144] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.9, 86.5, 0.0, 0.0, 26.0, 24.56722223089221, 7.020649027994793, 0.0, 1.0, 43.78164214602168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 67800.0000, 
sim time next is 68400.0000, 
raw observation next is [3.8, 86.0, 0.0, 0.0, 26.0, 24.59179742232404, 7.099464720252397, 0.0, 1.0, 43.75534467885407], 
processed observation next is [0.0, 0.8260869565217391, 0.5678670360110805, 0.86, 0.0, 0.0, 1.0, 0.7988282031891484, 0.07099464720252396, 0.0, 1.0, 0.3125381762775291], 
reward next is 0.6875, 
noisyNet noise sample is [array([1.4088628], dtype=float32), -0.15742332]. 
=============================================
[2019-04-01 18:21:29,252] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:29,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:29,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run21
[2019-04-01 18:21:29,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:29,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:29,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run21
[2019-04-01 18:21:30,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:30,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:30,080] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run21
[2019-04-01 18:21:30,838] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:30,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:30,854] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run21
[2019-04-01 18:21:31,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:31,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:31,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run21
[2019-04-01 18:21:32,632] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:32,632] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:32,638] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run21
[2019-04-01 18:21:32,983] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:32,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:32,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run21
[2019-04-01 18:21:33,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:33,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:33,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run21
[2019-04-01 18:21:33,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:33,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:33,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run21
[2019-04-01 18:21:34,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:34,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:34,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run21
[2019-04-01 18:21:35,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:35,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:35,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run21
[2019-04-01 18:21:36,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:36,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:36,072] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run21
[2019-04-01 18:21:36,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:21:36,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:21:36,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run21
[2019-04-01 18:21:39,526] A3C_AGENT_WORKER-Thread-11 INFO:Local step 170000, global step 2720115: loss 1.1962
[2019-04-01 18:21:39,527] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 170000, global step 2720115: learning rate 0.0010
[2019-04-01 18:21:40,446] A3C_AGENT_WORKER-Thread-20 INFO:Local step 170000, global step 2720237: loss 1.3029
[2019-04-01 18:21:40,447] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 170000, global step 2720237: learning rate 0.0010
[2019-04-01 18:21:40,730] A3C_AGENT_WORKER-Thread-2 INFO:Local step 170000, global step 2720301: loss 1.0292
[2019-04-01 18:21:40,732] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 170000, global step 2720301: learning rate 0.0010
[2019-04-01 18:21:41,937] A3C_AGENT_WORKER-Thread-16 INFO:Local step 170000, global step 2720600: loss 4.4486
[2019-04-01 18:21:41,937] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 170000, global step 2720600: learning rate 0.0010
[2019-04-01 18:21:41,949] A3C_AGENT_WORKER-Thread-14 INFO:Local step 170000, global step 2720602: loss 1.4149
[2019-04-01 18:21:41,950] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 170000, global step 2720602: learning rate 0.0010
[2019-04-01 18:21:43,976] A3C_AGENT_WORKER-Thread-19 INFO:Local step 170000, global step 2721179: loss 1.0476
[2019-04-01 18:21:43,976] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 170000, global step 2721179: learning rate 0.0010
[2019-04-01 18:21:43,992] A3C_AGENT_WORKER-Thread-3 INFO:Local step 170000, global step 2721187: loss 1.1350
[2019-04-01 18:21:43,994] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 170000, global step 2721188: learning rate 0.0010
[2019-04-01 18:21:44,277] A3C_AGENT_WORKER-Thread-9 INFO:Local step 170000, global step 2721302: loss 1.0760
[2019-04-01 18:21:44,277] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 170000, global step 2721302: learning rate 0.0010
[2019-04-01 18:21:44,287] A3C_AGENT_WORKER-Thread-13 INFO:Local step 170000, global step 2721305: loss 1.4469
[2019-04-01 18:21:44,287] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 170000, global step 2721305: learning rate 0.0010
[2019-04-01 18:21:45,030] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7105704e-29
 1.0000000e+00 5.3834998e-14], sum to 1.0000
[2019-04-01 18:21:45,030] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9613
[2019-04-01 18:21:45,045] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.04999999999999999, 95.0, 0.0, 0.0, 26.0, 24.32167381429744, 5.926651808986148, 0.0, 1.0, 39.58973994451284], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 85800.0000, 
sim time next is 86400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 24.29889063619081, 5.898019593113379, 0.0, 1.0, 39.60299653754293], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.7569843765986869, 0.058980195931133794, 0.0, 1.0, 0.2828785466967352], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.49210265], dtype=float32), 0.6561513]. 
=============================================
[2019-04-01 18:21:45,653] A3C_AGENT_WORKER-Thread-12 INFO:Local step 170000, global step 2721765: loss 1.7195
[2019-04-01 18:21:45,653] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 170000, global step 2721765: learning rate 0.0010
[2019-04-01 18:21:46,549] A3C_AGENT_WORKER-Thread-10 INFO:Local step 170000, global step 2722039: loss 1.0139
[2019-04-01 18:21:46,556] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 170000, global step 2722040: learning rate 0.0010
[2019-04-01 18:21:46,822] A3C_AGENT_WORKER-Thread-5 INFO:Local step 170000, global step 2722133: loss 1.1065
[2019-04-01 18:21:46,824] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 170000, global step 2722135: learning rate 0.0010
[2019-04-01 18:21:47,179] A3C_AGENT_WORKER-Thread-15 INFO:Local step 170000, global step 2722250: loss 0.6671
[2019-04-01 18:21:47,180] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 170000, global step 2722250: learning rate 0.0010
[2019-04-01 18:21:49,148] A3C_AGENT_WORKER-Thread-18 INFO:Local step 170500, global step 2722860: loss 2.5249
[2019-04-01 18:21:49,149] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 170500, global step 2722860: learning rate 0.0010
[2019-04-01 18:21:51,369] A3C_AGENT_WORKER-Thread-17 INFO:Local step 170500, global step 2723651: loss 0.3505
[2019-04-01 18:21:51,370] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 170500, global step 2723651: learning rate 0.0010
[2019-04-01 18:21:53,004] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.7328445e-34 3.7331253e-37 9.6469837e-34 6.2450435e-28 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:21:53,006] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1206
[2019-04-01 18:21:53,051] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.800000000000001, 69.66666666666667, 148.1666666666667, 0.0, 26.0, 25.3972620882683, 6.904959524806924, 1.0, 1.0, 33.92643080437063], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 213600.0000, 
sim time next is 214200.0000, 
raw observation next is [-5.6, 68.5, 153.0, 0.0, 26.0, 25.37146999677993, 6.927841667256186, 1.0, 1.0, 33.40785483491346], 
processed observation next is [1.0, 0.4782608695652174, 0.30747922437673136, 0.685, 0.51, 0.0, 1.0, 0.9102099995399898, 0.06927841667256186, 1.0, 1.0, 0.23862753453509614], 
reward next is 0.7614, 
noisyNet noise sample is [array([-1.2131584], dtype=float32), 1.1504424]. 
=============================================
[2019-04-01 18:21:53,158] A3C_AGENT_WORKER-Thread-4 INFO:Local step 170500, global step 2724184: loss 0.7321
[2019-04-01 18:21:53,159] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 170500, global step 2724184: learning rate 0.0010
[2019-04-01 18:21:58,103] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:21:58,105] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9822
[2019-04-01 18:21:58,120] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 79.0, 0.0, 0.0, 26.0, 24.13109910685991, 5.620442711886231, 0.0, 1.0, 43.75868979795113], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 259200.0000, 
sim time next is 259800.0000, 
raw observation next is [-4.866666666666667, 77.00000000000001, 0.0, 0.0, 26.0, 24.09360052418762, 5.583368549517307, 0.0, 1.0, 43.79641638575957], 
processed observation next is [1.0, 0.0, 0.3277931671283472, 0.7700000000000001, 0.0, 0.0, 1.0, 0.7276572177410888, 0.05583368549517307, 0.0, 1.0, 0.3128315456125684], 
reward next is 0.6872, 
noisyNet noise sample is [array([0.3057478], dtype=float32), -1.672158]. 
=============================================
[2019-04-01 18:22:01,624] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 1.628467e-25], sum to 1.0000
[2019-04-01 18:22:01,626] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6612
[2019-04-01 18:22:01,660] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.71953291092157, 6.68686669903384, 0.0, 1.0, 44.05891015983681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 248400.0000, 
sim time next is 249000.0000, 
raw observation next is [-3.483333333333333, 66.66666666666667, 0.0, 0.0, 26.0, 24.67197786667787, 6.579959569034787, 0.0, 1.0, 44.06406746465041], 
processed observation next is [1.0, 0.9130434782608695, 0.3661126500461681, 0.6666666666666667, 0.0, 0.0, 1.0, 0.810282552382553, 0.06579959569034786, 0.0, 1.0, 0.3147433390332172], 
reward next is 0.6853, 
noisyNet noise sample is [array([0.9679725], dtype=float32), 0.99068904]. 
=============================================
[2019-04-01 18:22:01,668] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[62.430107]
 [63.73017 ]
 [65.40954 ]
 [66.92138 ]
 [68.252754]], R is [[60.81772614]
 [60.89484406]
 [60.97138596]
 [61.04779434]
 [61.12532425]].
[2019-04-01 18:22:04,618] A3C_AGENT_WORKER-Thread-11 INFO:Local step 170500, global step 2727753: loss 0.1230
[2019-04-01 18:22:04,619] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 170500, global step 2727753: learning rate 0.0010
[2019-04-01 18:22:05,671] A3C_AGENT_WORKER-Thread-2 INFO:Local step 170500, global step 2728119: loss -0.6080
[2019-04-01 18:22:05,672] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 170500, global step 2728119: learning rate 0.0010
[2019-04-01 18:22:05,705] A3C_AGENT_WORKER-Thread-20 INFO:Local step 170500, global step 2728127: loss 0.2893
[2019-04-01 18:22:05,707] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 170500, global step 2728128: learning rate 0.0010
[2019-04-01 18:22:06,903] A3C_AGENT_WORKER-Thread-14 INFO:Local step 170500, global step 2728512: loss 1.9258
[2019-04-01 18:22:06,907] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 170500, global step 2728514: learning rate 0.0010
[2019-04-01 18:22:07,298] A3C_AGENT_WORKER-Thread-16 INFO:Local step 170500, global step 2728627: loss 0.4153
[2019-04-01 18:22:07,300] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 170500, global step 2728628: learning rate 0.0010
[2019-04-01 18:22:08,875] A3C_AGENT_WORKER-Thread-19 INFO:Local step 170500, global step 2729088: loss 1.5269
[2019-04-01 18:22:08,878] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 170500, global step 2729088: learning rate 0.0010
[2019-04-01 18:22:09,251] A3C_AGENT_WORKER-Thread-3 INFO:Local step 170500, global step 2729206: loss 2.1036
[2019-04-01 18:22:09,252] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 170500, global step 2729206: learning rate 0.0010
[2019-04-01 18:22:09,477] A3C_AGENT_WORKER-Thread-13 INFO:Local step 170500, global step 2729278: loss 0.8931
[2019-04-01 18:22:09,479] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 170500, global step 2729278: learning rate 0.0010
[2019-04-01 18:22:09,761] A3C_AGENT_WORKER-Thread-9 INFO:Local step 170500, global step 2729379: loss 1.7208
[2019-04-01 18:22:09,763] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 170500, global step 2729379: learning rate 0.0010
[2019-04-01 18:22:10,457] A3C_AGENT_WORKER-Thread-12 INFO:Local step 170500, global step 2729629: loss 1.0801
[2019-04-01 18:22:10,458] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 170500, global step 2729629: learning rate 0.0010
[2019-04-01 18:22:11,378] A3C_AGENT_WORKER-Thread-5 INFO:Local step 170500, global step 2729890: loss 0.0022
[2019-04-01 18:22:11,379] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 170500, global step 2729890: learning rate 0.0010
[2019-04-01 18:22:11,713] A3C_AGENT_WORKER-Thread-10 INFO:Local step 170500, global step 2729998: loss 0.6638
[2019-04-01 18:22:11,715] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 170500, global step 2729998: learning rate 0.0010
[2019-04-01 18:22:12,645] A3C_AGENT_WORKER-Thread-15 INFO:Local step 170500, global step 2730309: loss 0.5811
[2019-04-01 18:22:12,646] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 170500, global step 2730309: learning rate 0.0010
[2019-04-01 18:22:12,985] A3C_AGENT_WORKER-Thread-18 INFO:Local step 171000, global step 2730428: loss 1.5934
[2019-04-01 18:22:12,985] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 171000, global step 2730428: learning rate 0.0010
[2019-04-01 18:22:15,409] A3C_AGENT_WORKER-Thread-17 INFO:Local step 171000, global step 2731161: loss 0.3655
[2019-04-01 18:22:15,410] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 171000, global step 2731162: learning rate 0.0010
[2019-04-01 18:22:17,283] A3C_AGENT_WORKER-Thread-4 INFO:Local step 171000, global step 2731778: loss 0.1691
[2019-04-01 18:22:17,284] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 171000, global step 2731778: learning rate 0.0010
[2019-04-01 18:22:19,218] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:22:19,218] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0046
[2019-04-01 18:22:19,264] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.916666666666667, 32.83333333333334, 49.0, 0.0, 26.0, 25.35767151337366, 6.137595136695748, 1.0, 1.0, 35.38955811459881], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 465000.0000, 
sim time next is 465600.0000, 
raw observation next is [-5.633333333333334, 32.66666666666667, 55.50000000000001, 0.0, 26.0, 25.32792799378613, 6.122372081685846, 1.0, 1.0, 33.30911831946894], 
processed observation next is [1.0, 0.391304347826087, 0.30655586334256696, 0.3266666666666667, 0.18500000000000003, 0.0, 1.0, 0.9039897133980185, 0.06122372081685846, 1.0, 1.0, 0.23792227371049246], 
reward next is 0.7621, 
noisyNet noise sample is [array([3.1312253], dtype=float32), 0.12319448]. 
=============================================
[2019-04-01 18:22:19,707] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:22:19,707] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7228
[2019-04-01 18:22:19,749] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.766666666666667, 30.66666666666667, 92.0, 0.0, 26.0, 25.2904073409247, 6.053704516123489, 1.0, 1.0, 23.26585759273662], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 469200.0000, 
sim time next is 469800.0000, 
raw observation next is [-3.4, 30.0, 98.0, 0.0, 26.0, 25.24978096838727, 5.995608862576023, 1.0, 1.0, 23.70058238463783], 
processed observation next is [1.0, 0.43478260869565216, 0.368421052631579, 0.3, 0.32666666666666666, 0.0, 1.0, 0.8928258526267528, 0.059956088625760226, 1.0, 1.0, 0.1692898741759845], 
reward next is 0.8307, 
noisyNet noise sample is [array([0.50182647], dtype=float32), -0.31976467]. 
=============================================
[2019-04-01 18:22:27,112] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:22:27,112] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7019
[2019-04-01 18:22:27,126] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.05, 87.0, 0.0, 0.0, 26.0, 24.92175803135942, 6.604005070926992, 0.0, 1.0, 39.18830947177852], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 527400.0000, 
sim time next is 528000.0000, 
raw observation next is [3.966666666666667, 86.66666666666666, 0.0, 0.0, 26.0, 24.8995760077501, 6.566210347816049, 0.0, 1.0, 39.21870146659162], 
processed observation next is [0.0, 0.08695652173913043, 0.5724838411819021, 0.8666666666666666, 0.0, 0.0, 1.0, 0.8427965725357284, 0.06566210347816048, 0.0, 1.0, 0.2801335819042258], 
reward next is 0.7199, 
noisyNet noise sample is [array([-0.4416097], dtype=float32), -1.0840074]. 
=============================================
[2019-04-01 18:22:27,138] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[69.30055 ]
 [69.22157 ]
 [69.1582  ]
 [69.08251 ]
 [69.043686]], R is [[69.46759796]
 [69.49301147]
 [69.51837921]
 [69.54370117]
 [69.5689621 ]].
[2019-04-01 18:22:27,655] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.1197754e-16], sum to 1.0000
[2019-04-01 18:22:27,655] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6166
[2019-04-01 18:22:27,698] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 82.5, 118.0, 335.6666666666667, 26.0, 24.99260850999693, 8.100295451034043, 0.0, 1.0, 31.55199329317857], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 571800.0000, 
sim time next is 572400.0000, 
raw observation next is [-1.2, 83.0, 113.5, 270.0, 26.0, 24.9769782285637, 8.058850131339996, 0.0, 1.0, 31.99331179263962], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.37833333333333335, 0.2983425414364641, 1.0, 0.8538540326519569, 0.08058850131339997, 0.0, 1.0, 0.22852365566171157], 
reward next is 0.7715, 
noisyNet noise sample is [array([-0.5696545], dtype=float32), -0.14686194]. 
=============================================
[2019-04-01 18:22:28,216] A3C_AGENT_WORKER-Thread-11 INFO:Local step 171000, global step 2735738: loss 0.1372
[2019-04-01 18:22:28,218] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 171000, global step 2735738: learning rate 0.0010
[2019-04-01 18:22:28,643] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:22:28,643] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1637
[2019-04-01 18:22:28,684] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 9.5969815e-36], sum to 1.0000
[2019-04-01 18:22:28,684] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9110
[2019-04-01 18:22:28,694] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 25.07506926685492, 7.631395154434895, 0.0, 1.0, 43.12672180641782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 589800.0000, 
sim time next is 590400.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 25.12475261886854, 7.599927804297771, 0.0, 1.0, 42.95373396515738], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.87, 0.0, 0.0, 1.0, 0.874964659838363, 0.07599927804297771, 0.0, 1.0, 0.3068123854654099], 
reward next is 0.6932, 
noisyNet noise sample is [array([-0.6792523], dtype=float32), 0.79614246]. 
=============================================
[2019-04-01 18:22:28,722] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.93704656043428, 7.512332890446172, 0.0, 1.0, 46.03911840123789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 587400.0000, 
sim time next is 588000.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.94767125376682, 7.560213705236222, 0.0, 1.0, 47.45228948633745], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 1.0, 0.8496673219666887, 0.07560213705236223, 0.0, 1.0, 0.33894492490241035], 
reward next is 0.6611, 
noisyNet noise sample is [array([0.1590701], dtype=float32), 0.95639604]. 
=============================================
[2019-04-01 18:22:28,726] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[70.660645]
 [70.845764]
 [71.024086]
 [71.15314 ]
 [71.30631 ]], R is [[70.49680328]
 [70.46298981]
 [70.47534943]
 [70.4957962 ]
 [70.51481628]].
[2019-04-01 18:22:28,925] A3C_AGENT_WORKER-Thread-20 INFO:Local step 171000, global step 2735980: loss 0.1695
[2019-04-01 18:22:28,926] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 171000, global step 2735980: learning rate 0.0010
[2019-04-01 18:22:29,538] A3C_AGENT_WORKER-Thread-2 INFO:Local step 171000, global step 2736195: loss 0.0535
[2019-04-01 18:22:29,539] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 171000, global step 2736195: learning rate 0.0010
[2019-04-01 18:22:30,711] A3C_AGENT_WORKER-Thread-14 INFO:Local step 171000, global step 2736632: loss 0.0546
[2019-04-01 18:22:30,713] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 171000, global step 2736632: learning rate 0.0010
[2019-04-01 18:22:30,746] A3C_AGENT_WORKER-Thread-16 INFO:Local step 171000, global step 2736646: loss 0.2588
[2019-04-01 18:22:30,746] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 171000, global step 2736646: learning rate 0.0010
[2019-04-01 18:22:32,305] A3C_AGENT_WORKER-Thread-19 INFO:Local step 171000, global step 2737246: loss 0.0116
[2019-04-01 18:22:32,307] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 171000, global step 2737247: learning rate 0.0010
[2019-04-01 18:22:32,577] A3C_AGENT_WORKER-Thread-3 INFO:Local step 171000, global step 2737354: loss 0.0107
[2019-04-01 18:22:32,578] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 171000, global step 2737354: learning rate 0.0010
[2019-04-01 18:22:32,633] A3C_AGENT_WORKER-Thread-9 INFO:Local step 171000, global step 2737381: loss 0.0823
[2019-04-01 18:22:32,643] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 171000, global step 2737381: learning rate 0.0010
[2019-04-01 18:22:32,988] A3C_AGENT_WORKER-Thread-13 INFO:Local step 171000, global step 2737518: loss 0.0246
[2019-04-01 18:22:32,988] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 171000, global step 2737518: learning rate 0.0010
[2019-04-01 18:22:33,581] A3C_AGENT_WORKER-Thread-12 INFO:Local step 171000, global step 2737755: loss 0.0183
[2019-04-01 18:22:33,581] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 171000, global step 2737755: learning rate 0.0010
[2019-04-01 18:22:33,861] A3C_AGENT_WORKER-Thread-18 INFO:Local step 171500, global step 2737870: loss 1.6669
[2019-04-01 18:22:33,862] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 171500, global step 2737870: learning rate 0.0010
[2019-04-01 18:22:34,489] A3C_AGENT_WORKER-Thread-5 INFO:Local step 171000, global step 2738086: loss 0.0169
[2019-04-01 18:22:34,490] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 171000, global step 2738086: learning rate 0.0010
[2019-04-01 18:22:35,025] A3C_AGENT_WORKER-Thread-10 INFO:Local step 171000, global step 2738278: loss 0.0193
[2019-04-01 18:22:35,031] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 171000, global step 2738278: learning rate 0.0010
[2019-04-01 18:22:35,462] A3C_AGENT_WORKER-Thread-15 INFO:Local step 171000, global step 2738452: loss 0.1540
[2019-04-01 18:22:35,462] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 171000, global step 2738452: learning rate 0.0010
[2019-04-01 18:22:36,316] A3C_AGENT_WORKER-Thread-17 INFO:Local step 171500, global step 2738805: loss 1.5898
[2019-04-01 18:22:36,321] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 171500, global step 2738806: learning rate 0.0010
[2019-04-01 18:22:36,932] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:22:36,933] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8102
[2019-04-01 18:22:36,960] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.09999999999999999, 46.66666666666667, 83.33333333333333, 258.6666666666666, 26.0, 25.86577998118054, 8.488059075177297, 1.0, 1.0, 8.605596200204722], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 745800.0000, 
sim time next is 746400.0000, 
raw observation next is [-0.2, 46.33333333333334, 84.16666666666667, 144.8333333333333, 26.0, 25.69112476025266, 8.596592003409391, 1.0, 1.0, 19.18000043810744], 
processed observation next is [1.0, 0.6521739130434783, 0.4570637119113574, 0.46333333333333343, 0.28055555555555556, 0.16003683241252298, 1.0, 0.9558749657503799, 0.08596592003409391, 1.0, 1.0, 0.13700000312933888], 
reward next is 0.8630, 
noisyNet noise sample is [array([0.08236349], dtype=float32), 2.8200102]. 
=============================================
[2019-04-01 18:22:37,836] A3C_AGENT_WORKER-Thread-4 INFO:Local step 171500, global step 2739504: loss 0.1618
[2019-04-01 18:22:37,836] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 171500, global step 2739504: learning rate 0.0010
[2019-04-01 18:22:38,158] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:22:38,158] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2085
[2019-04-01 18:22:38,200] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 56.0, 0.0, 0.0, 26.0, 25.4757328345349, 9.212415297442305, 1.0, 1.0, 29.75669113952523], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 756000.0000, 
sim time next is 756600.0000, 
raw observation next is [-3.9, 55.5, 0.0, 0.0, 26.0, 25.65117592208924, 9.004179868342803, 1.0, 1.0, 31.0451235778715], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.555, 0.0, 0.0, 1.0, 0.9501679888698915, 0.09004179868342803, 1.0, 1.0, 0.22175088269908214], 
reward next is 0.7782, 
noisyNet noise sample is [array([-0.40379247], dtype=float32), -0.5461199]. 
=============================================
[2019-04-01 18:22:41,665] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:22:41,666] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1014
[2019-04-01 18:22:41,717] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 83.33333333333334, 0.0, 0.0, 26.0, 25.4115568767942, 7.882364676487259, 1.0, 1.0, 25.83557279530625], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 841200.0000, 
sim time next is 841800.0000, 
raw observation next is [-3.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.19374787413183, 7.654435635961588, 1.0, 1.0, 27.66651821794536], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8266666666666667, 0.0, 0.0, 1.0, 0.8848211248759758, 0.07654435635961587, 1.0, 1.0, 0.19761798727103827], 
reward next is 0.8024, 
noisyNet noise sample is [array([0.00369373], dtype=float32), 0.19502631]. 
=============================================
[2019-04-01 18:22:44,466] A3C_AGENT_WORKER-Thread-18 INFO:Local step 172000, global step 2742676: loss 2.4633
[2019-04-01 18:22:44,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 172000, global step 2742676: learning rate 0.0010
[2019-04-01 18:22:46,234] A3C_AGENT_WORKER-Thread-17 INFO:Local step 172000, global step 2743437: loss 1.3754
[2019-04-01 18:22:46,234] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 172000, global step 2743437: learning rate 0.0010
[2019-04-01 18:22:46,482] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:22:46,482] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6605
[2019-04-01 18:22:46,509] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.500000000000001, 100.0, 0.0, 0.0, 26.0, 25.3924999797044, 12.62765192333808, 0.0, 1.0, 39.2098683449317], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1289400.0000, 
sim time next is 1290000.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.4243318057281, 12.92707326859435, 0.0, 1.0, 38.24824372668333], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 1.0, 0.9177616865325859, 0.1292707326859435, 0.0, 1.0, 0.2732017409048809], 
reward next is 0.7268, 
noisyNet noise sample is [array([0.20731835], dtype=float32), 0.88136286]. 
=============================================
[2019-04-01 18:22:46,515] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.73646 ]
 [83.58373 ]
 [83.42514 ]
 [83.244995]
 [83.05442 ]], R is [[83.72301483]
 [83.60572052]
 [83.4935379 ]
 [83.39742279]
 [83.32457733]].
[2019-04-01 18:22:46,960] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:22:46,960] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1023
[2019-04-01 18:22:47,049] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 84.66666666666666, 24.16666666666667, 0.0, 26.0, 24.70874311166364, 7.761007052002131, 1.0, 1.0, 80.68532655752514], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 837600.0000, 
sim time next is 838200.0000, 
raw observation next is [-3.9, 85.33333333333334, 19.33333333333334, 0.0, 26.0, 25.26080397564569, 9.409897496901534, 1.0, 1.0, 44.35597325551915], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.8533333333333334, 0.06444444444444447, 0.0, 1.0, 0.8944005679493843, 0.09409897496901534, 1.0, 1.0, 0.31682838039656536], 
reward next is 0.6832, 
noisyNet noise sample is [array([-0.8508804], dtype=float32), 1.9752858]. 
=============================================
[2019-04-01 18:22:48,153] A3C_AGENT_WORKER-Thread-4 INFO:Local step 172000, global step 2744284: loss 0.7483
[2019-04-01 18:22:48,155] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 172000, global step 2744284: learning rate 0.0010
[2019-04-01 18:22:48,278] A3C_AGENT_WORKER-Thread-11 INFO:Local step 171500, global step 2744342: loss 0.4442
[2019-04-01 18:22:48,280] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 171500, global step 2744343: learning rate 0.0010
[2019-04-01 18:22:49,091] A3C_AGENT_WORKER-Thread-20 INFO:Local step 171500, global step 2744725: loss 0.2250
[2019-04-01 18:22:49,091] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 171500, global step 2744725: learning rate 0.0010
[2019-04-01 18:22:49,534] A3C_AGENT_WORKER-Thread-2 INFO:Local step 171500, global step 2744935: loss 0.0447
[2019-04-01 18:22:49,536] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 171500, global step 2744935: learning rate 0.0010
[2019-04-01 18:22:49,923] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:22:49,924] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2320
[2019-04-01 18:22:49,960] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.600000000000001, 99.33333333333334, 0.0, 0.0, 26.0, 25.58478427569398, 13.12046095630579, 0.0, 1.0, 28.58028855337976], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1284600.0000, 
sim time next is 1285200.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.58259034614844, 12.92008983559312, 0.0, 1.0, 27.66695506484747], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 1.0, 0.9403700494497771, 0.1292008983559312, 0.0, 1.0, 0.19762110760605336], 
reward next is 0.8024, 
noisyNet noise sample is [array([0.7609024], dtype=float32), -0.6016399]. 
=============================================
[2019-04-01 18:22:50,819] A3C_AGENT_WORKER-Thread-16 INFO:Local step 171500, global step 2745581: loss 0.0553
[2019-04-01 18:22:50,821] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 171500, global step 2745581: learning rate 0.0010
[2019-04-01 18:22:50,821] A3C_AGENT_WORKER-Thread-14 INFO:Local step 171500, global step 2745581: loss 0.1120
[2019-04-01 18:22:50,824] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 171500, global step 2745583: learning rate 0.0010
[2019-04-01 18:22:52,067] A3C_AGENT_WORKER-Thread-19 INFO:Local step 171500, global step 2746207: loss 0.0856
[2019-04-01 18:22:52,068] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 171500, global step 2746207: learning rate 0.0010
[2019-04-01 18:22:52,403] A3C_AGENT_WORKER-Thread-9 INFO:Local step 171500, global step 2746393: loss 0.2744
[2019-04-01 18:22:52,405] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 171500, global step 2746394: learning rate 0.0010
[2019-04-01 18:22:52,719] A3C_AGENT_WORKER-Thread-3 INFO:Local step 171500, global step 2746573: loss 0.3412
[2019-04-01 18:22:52,721] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 171500, global step 2746573: learning rate 0.0010
[2019-04-01 18:22:53,002] A3C_AGENT_WORKER-Thread-13 INFO:Local step 171500, global step 2746749: loss 0.0111
[2019-04-01 18:22:53,004] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 171500, global step 2746749: learning rate 0.0010
[2019-04-01 18:22:53,816] A3C_AGENT_WORKER-Thread-12 INFO:Local step 171500, global step 2747207: loss 0.2592
[2019-04-01 18:22:53,821] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 171500, global step 2747207: learning rate 0.0010
[2019-04-01 18:22:54,365] A3C_AGENT_WORKER-Thread-5 INFO:Local step 171500, global step 2747528: loss 0.0301
[2019-04-01 18:22:54,367] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 171500, global step 2747528: learning rate 0.0010
[2019-04-01 18:22:54,728] A3C_AGENT_WORKER-Thread-10 INFO:Local step 171500, global step 2747773: loss 0.0097
[2019-04-01 18:22:54,729] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 171500, global step 2747773: learning rate 0.0010
[2019-04-01 18:22:54,936] A3C_AGENT_WORKER-Thread-15 INFO:Local step 171500, global step 2747907: loss 0.1376
[2019-04-01 18:22:54,937] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 171500, global step 2747907: learning rate 0.0010
[2019-04-01 18:22:59,107] A3C_AGENT_WORKER-Thread-11 INFO:Local step 172000, global step 2750635: loss 0.0754
[2019-04-01 18:22:59,108] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 172000, global step 2750635: learning rate 0.0010
[2019-04-01 18:22:59,794] A3C_AGENT_WORKER-Thread-18 INFO:Local step 172500, global step 2751096: loss 0.5379
[2019-04-01 18:22:59,795] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 172500, global step 2751096: learning rate 0.0010
[2019-04-01 18:22:59,844] A3C_AGENT_WORKER-Thread-20 INFO:Local step 172000, global step 2751130: loss 0.0659
[2019-04-01 18:22:59,847] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 172000, global step 2751131: learning rate 0.0010
[2019-04-01 18:22:59,971] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 9.4926276e-29], sum to 1.0000
[2019-04-01 18:22:59,973] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4804
[2019-04-01 18:22:59,987] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.05, 67.5, 254.0, 215.0, 26.0, 27.38487749663539, 17.85384165222788, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1078200.0000, 
sim time next is 1078800.0000, 
raw observation next is [16.23333333333333, 66.66666666666666, 241.8333333333333, 232.0, 26.0, 27.32430821655421, 22.29609456610664, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.912280701754386, 0.6666666666666665, 0.806111111111111, 0.256353591160221, 1.0, 1.189186888079173, 0.2229609456610664, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3952279], dtype=float32), 0.30082452]. 
=============================================
[2019-04-01 18:23:00,451] A3C_AGENT_WORKER-Thread-2 INFO:Local step 172000, global step 2751545: loss 0.1220
[2019-04-01 18:23:00,453] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 172000, global step 2751545: learning rate 0.0010
[2019-04-01 18:23:01,032] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 6.245706e-22], sum to 1.0000
[2019-04-01 18:23:01,032] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0157
[2019-04-01 18:23:01,057] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.39001853242776, 12.2703005273101, 0.0, 1.0, 36.56472565422365], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1288200.0000, 
sim time next is 1288800.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.37804379398707, 12.43751109414208, 0.0, 1.0, 38.6572002816526], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 1.0, 0.9111491134267246, 0.1243751109414208, 0.0, 1.0, 0.27612285915466145], 
reward next is 0.7239, 
noisyNet noise sample is [array([-2.3357973], dtype=float32), 1.6795689]. 
=============================================
[2019-04-01 18:23:01,218] A3C_AGENT_WORKER-Thread-17 INFO:Local step 172500, global step 2752054: loss 0.4888
[2019-04-01 18:23:01,221] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 172500, global step 2752054: learning rate 0.0010
[2019-04-01 18:23:01,229] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:23:01,235] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4563
[2019-04-01 18:23:01,238] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.83333333333333, 49.33333333333334, 44.5, 0.0, 26.0, 27.82018562383774, 26.62764030290524, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1095600.0000, 
sim time next is 1096200.0000, 
raw observation next is [18.55, 49.5, 35.0, 0.0, 26.0, 27.86425867687166, 26.90442733117889, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.976454293628809, 0.495, 0.11666666666666667, 0.0, 1.0, 1.266322668124523, 0.26904427331178893, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.30381605], dtype=float32), -1.2284796]. 
=============================================
[2019-04-01 18:23:01,742] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0041868e-33 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.5587831e-27
 9.9999988e-01 9.1651820e-08], sum to 1.0000
[2019-04-01 18:23:01,745] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0773
[2019-04-01 18:23:01,749] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.4516858e-26 8.1904040e-36 4.7548333e-31 5.8925284e-23 1.9271353e-26
 1.0000000e+00 1.8782263e-25], sum to 1.0000
[2019-04-01 18:23:01,749] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5266
[2019-04-01 18:23:01,754] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.41666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.72085123697109, 13.79693372146281, 0.0, 1.0, 15.95732095893178], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1127400.0000, 
sim time next is 1128000.0000, 
raw observation next is [10.33333333333333, 77.66666666666667, 0.0, 0.0, 26.0, 25.68192577099785, 13.52927529909963, 0.0, 1.0, 15.23516335502085], 
processed observation next is [0.0, 0.043478260869565216, 0.7488457987072946, 0.7766666666666667, 0.0, 0.0, 1.0, 0.9545608244282643, 0.1352927529909963, 0.0, 1.0, 0.10882259539300607], 
reward next is 0.8912, 
noisyNet noise sample is [array([-1.043754], dtype=float32), 0.6237544]. 
=============================================
[2019-04-01 18:23:01,770] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.9, 100.0, 47.0, 0.0, 26.0, 25.74768948354812, 9.988144785381849, 1.0, 1.0, 13.20248339794204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1503000.0000, 
sim time next is 1503600.0000, 
raw observation next is [2.0, 100.0, 51.33333333333334, 0.0, 26.0, 25.83594006523216, 10.16440386325347, 1.0, 1.0, 12.57288280040977], 
processed observation next is [1.0, 0.391304347826087, 0.518005540166205, 1.0, 0.17111111111111113, 0.0, 1.0, 0.9765628664617368, 0.10164403863253471, 1.0, 1.0, 0.08980630571721264], 
reward next is 0.8444, 
noisyNet noise sample is [array([1.0964957], dtype=float32), 0.8336159]. 
=============================================
[2019-04-01 18:23:01,773] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[88.218636]
 [87.916565]
 [87.515335]
 [87.08647 ]
 [86.66235 ]], R is [[88.57459259]
 [88.57486725]
 [88.56956482]
 [88.55833435]
 [88.54184723]].
[2019-04-01 18:23:01,980] A3C_AGENT_WORKER-Thread-14 INFO:Local step 172000, global step 2752569: loss 0.3179
[2019-04-01 18:23:01,981] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 172000, global step 2752570: learning rate 0.0010
[2019-04-01 18:23:02,109] A3C_AGENT_WORKER-Thread-16 INFO:Local step 172000, global step 2752649: loss 0.7381
[2019-04-01 18:23:02,111] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 172000, global step 2752649: learning rate 0.0010
[2019-04-01 18:23:02,837] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.8153364e-29 3.5069232e-35 0.0000000e+00 1.6836415e-28 8.7266990e-27
 1.0000000e+00 1.3223108e-30], sum to 1.0000
[2019-04-01 18:23:02,841] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8628
[2019-04-01 18:23:02,849] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.56666666666667, 53.0, 41.83333333333334, 30.83333333333334, 26.0, 27.48929305022097, 21.27140224221695, 1.0, 1.0, 6.435688369743142], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1615200.0000, 
sim time next is 1615800.0000, 
raw observation next is [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 26.0, 27.51776556390263, 21.03668731190351, 1.0, 1.0, 6.558505232959135], 
processed observation next is [1.0, 0.6956521739130435, 0.8056325023084026, 0.535, 0.11222222222222224, 0.027255985267034995, 1.0, 1.2168236519860898, 0.2103668731190351, 1.0, 1.0, 0.04684646594970811], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.036031], dtype=float32), -1.3639317]. 
=============================================
[2019-04-01 18:23:03,368] A3C_AGENT_WORKER-Thread-19 INFO:Local step 172000, global step 2753502: loss 0.1672
[2019-04-01 18:23:03,373] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 172000, global step 2753504: learning rate 0.0010
[2019-04-01 18:23:03,625] A3C_AGENT_WORKER-Thread-4 INFO:Local step 172500, global step 2753671: loss 3.8667
[2019-04-01 18:23:03,629] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 172500, global step 2753672: learning rate 0.0010
[2019-04-01 18:23:03,827] A3C_AGENT_WORKER-Thread-9 INFO:Local step 172000, global step 2753808: loss 0.5183
[2019-04-01 18:23:03,834] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 172000, global step 2753810: learning rate 0.0010
[2019-04-01 18:23:03,854] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1333999e-21 6.8189821e-26 1.1235237e-23 2.6105715e-20 1.5769339e-30
 1.4511393e-07 9.9999988e-01], sum to 1.0000
[2019-04-01 18:23:03,855] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1873
[2019-04-01 18:23:03,863] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 106.0, 0.0, 26.0, 25.69940992386467, 11.11875269755706, 1.0, 1.0, 15.73053567715892], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1344600.0000, 
sim time next is 1345200.0000, 
raw observation next is [1.1, 92.0, 100.1666666666667, 0.0, 26.0, 25.67955275668443, 11.25060457447849, 1.0, 1.0, 15.03243164761957], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.333888888888889, 0.0, 1.0, 0.95422182238349, 0.1125060457447849, 1.0, 1.0, 0.10737451176871121], 
reward next is 0.3924, 
noisyNet noise sample is [array([1.6349379], dtype=float32), 0.23379445]. 
=============================================
[2019-04-01 18:23:04,218] A3C_AGENT_WORKER-Thread-3 INFO:Local step 172000, global step 2753994: loss 1.0012
[2019-04-01 18:23:04,220] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 172000, global step 2753994: learning rate 0.0010
[2019-04-01 18:23:04,659] A3C_AGENT_WORKER-Thread-13 INFO:Local step 172000, global step 2754282: loss 0.8731
[2019-04-01 18:23:04,661] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 172000, global step 2754283: learning rate 0.0010
[2019-04-01 18:23:05,451] A3C_AGENT_WORKER-Thread-12 INFO:Local step 172000, global step 2754801: loss 0.6109
[2019-04-01 18:23:05,453] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 172000, global step 2754801: learning rate 0.0010
[2019-04-01 18:23:05,873] A3C_AGENT_WORKER-Thread-5 INFO:Local step 172000, global step 2755032: loss 0.7641
[2019-04-01 18:23:05,878] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 172000, global step 2755035: learning rate 0.0010
[2019-04-01 18:23:06,237] A3C_AGENT_WORKER-Thread-10 INFO:Local step 172000, global step 2755218: loss 1.4900
[2019-04-01 18:23:06,238] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 172000, global step 2755218: learning rate 0.0010
[2019-04-01 18:23:06,581] A3C_AGENT_WORKER-Thread-15 INFO:Local step 172000, global step 2755406: loss 0.9680
[2019-04-01 18:23:06,584] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 172000, global step 2755406: learning rate 0.0010
[2019-04-01 18:23:13,214] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 3.0303087e-32], sum to 1.0000
[2019-04-01 18:23:13,215] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7356
[2019-04-01 18:23:13,269] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.633333333333333, 83.0, 124.8333333333333, 0.0, 26.0, 24.89639684083084, 7.981466150526397, 0.0, 1.0, 43.72438092966048], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1773600.0000, 
sim time next is 1774200.0000, 
raw observation next is [-2.716666666666667, 83.0, 123.6666666666667, 0.0, 26.0, 24.9048365194633, 8.179638598355742, 0.0, 1.0, 50.23071167649117], 
processed observation next is [0.0, 0.5217391304347826, 0.3873499538319483, 0.83, 0.4122222222222223, 0.0, 1.0, 0.8435480742090427, 0.08179638598355743, 0.0, 1.0, 0.3587907976892226], 
reward next is 0.6412, 
noisyNet noise sample is [array([-0.5993956], dtype=float32), -0.763307]. 
=============================================
[2019-04-01 18:23:14,158] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6373712e-23 5.1552777e-30 6.1912032e-24 1.6921617e-16 4.5249115e-27
 8.4158880e-01 1.5841120e-01], sum to 1.0000
[2019-04-01 18:23:14,158] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5508
[2019-04-01 18:23:14,204] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 12.0, 0.0, 26.0, 25.33487052226586, 8.945282741460204, 1.0, 1.0, 23.0602843336919], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1411800.0000, 
sim time next is 1412400.0000, 
raw observation next is [-0.6, 100.0, 15.0, 0.0, 26.0, 25.26144650487496, 9.685643711589334, 1.0, 1.0, 23.41207378773254], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.05, 0.0, 1.0, 0.8944923578392802, 0.09685643711589334, 1.0, 1.0, 0.16722909848380388], 
reward next is 0.8328, 
noisyNet noise sample is [array([0.1636868], dtype=float32), -0.7983279]. 
=============================================
[2019-04-01 18:23:14,805] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.5766909e-31 1.6413882e-34
 1.0000000e+00 9.8139949e-38], sum to 1.0000
[2019-04-01 18:23:14,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8714
[2019-04-01 18:23:14,849] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 27.33333333333333, 0.0, 26.0, 25.77639620713865, 10.41938245942868, 1.0, 1.0, 18.2046825998838], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1414200.0000, 
sim time next is 1414800.0000, 
raw observation next is [-0.6, 100.0, 32.0, 0.0, 26.0, 25.8530316552334, 10.55037191056969, 1.0, 1.0, 17.25594827688531], 
processed observation next is [1.0, 0.391304347826087, 0.44598337950138506, 1.0, 0.10666666666666667, 0.0, 1.0, 0.9790045221762002, 0.10550371910569689, 1.0, 1.0, 0.12325677340632364], 
reward next is 0.6566, 
noisyNet noise sample is [array([-1.3953598], dtype=float32), -0.6688009]. 
=============================================
[2019-04-01 18:23:15,003] A3C_AGENT_WORKER-Thread-11 INFO:Local step 172500, global step 2759457: loss 6.2946
[2019-04-01 18:23:15,005] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 172500, global step 2759457: learning rate 0.0010
[2019-04-01 18:23:15,317] A3C_AGENT_WORKER-Thread-20 INFO:Local step 172500, global step 2759612: loss 5.0690
[2019-04-01 18:23:15,320] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 172500, global step 2759614: learning rate 0.0010
[2019-04-01 18:23:16,197] A3C_AGENT_WORKER-Thread-2 INFO:Local step 172500, global step 2760075: loss 7.2086
[2019-04-01 18:23:16,198] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 172500, global step 2760075: learning rate 0.0010
[2019-04-01 18:23:16,589] A3C_AGENT_WORKER-Thread-18 INFO:Local step 173000, global step 2760271: loss 1.3929
[2019-04-01 18:23:16,590] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 173000, global step 2760271: learning rate 0.0010
[2019-04-01 18:23:17,181] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.3681682e-23 2.5930808e-34 7.8145555e-27 3.0138872e-17 1.9806527e-33
 9.9992967e-01 7.0372487e-05], sum to 1.0000
[2019-04-01 18:23:17,188] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7947
[2019-04-01 18:23:17,201] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.86666666666667, 58.33333333333334, 204.8333333333333, 228.1666666666667, 26.0, 26.94742855639155, 18.18786976819879, 1.0, 1.0, 5.072544632711549], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1597200.0000, 
sim time next is 1597800.0000, 
raw observation next is [11.23333333333333, 57.66666666666666, 193.6666666666667, 207.3333333333333, 26.0, 27.01472723894102, 18.52513095365588, 1.0, 1.0, 4.831345171224618], 
processed observation next is [1.0, 0.4782608695652174, 0.7737765466297323, 0.5766666666666665, 0.6455555555555557, 0.22909760589318595, 1.0, 1.1449610341344314, 0.18525130953655877, 1.0, 1.0, 0.03450960836589013], 
reward next is 0.0000, 
noisyNet noise sample is [array([3.1467814], dtype=float32), 0.8445002]. 
=============================================
[2019-04-01 18:23:17,870] A3C_AGENT_WORKER-Thread-16 INFO:Local step 172500, global step 2760955: loss 4.7259
[2019-04-01 18:23:17,873] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 172500, global step 2760955: learning rate 0.0010
[2019-04-01 18:23:17,875] A3C_AGENT_WORKER-Thread-14 INFO:Local step 172500, global step 2760957: loss 2.3427
[2019-04-01 18:23:17,881] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 172500, global step 2760960: learning rate 0.0010
[2019-04-01 18:23:18,061] A3C_AGENT_WORKER-Thread-17 INFO:Local step 173000, global step 2761080: loss 0.8290
[2019-04-01 18:23:18,062] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 173000, global step 2761080: learning rate 0.0010
[2019-04-01 18:23:18,795] A3C_AGENT_WORKER-Thread-19 INFO:Local step 172500, global step 2761527: loss 1.2947
[2019-04-01 18:23:18,796] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 172500, global step 2761527: learning rate 0.0010
[2019-04-01 18:23:19,028] A3C_AGENT_WORKER-Thread-9 INFO:Local step 172500, global step 2761672: loss 1.6497
[2019-04-01 18:23:19,030] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 172500, global step 2761673: learning rate 0.0010
[2019-04-01 18:23:19,284] A3C_AGENT_WORKER-Thread-3 INFO:Local step 172500, global step 2761830: loss 1.4558
[2019-04-01 18:23:19,287] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 172500, global step 2761832: learning rate 0.0010
[2019-04-01 18:23:19,805] A3C_AGENT_WORKER-Thread-13 INFO:Local step 172500, global step 2762130: loss 1.7655
[2019-04-01 18:23:19,806] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 172500, global step 2762130: learning rate 0.0010
[2019-04-01 18:23:20,227] A3C_AGENT_WORKER-Thread-4 INFO:Local step 173000, global step 2762394: loss 0.0708
[2019-04-01 18:23:20,227] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 173000, global step 2762394: learning rate 0.0010
[2019-04-01 18:23:20,380] A3C_AGENT_WORKER-Thread-12 INFO:Local step 172500, global step 2762488: loss 0.5157
[2019-04-01 18:23:20,381] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 172500, global step 2762488: learning rate 0.0010
[2019-04-01 18:23:20,821] A3C_AGENT_WORKER-Thread-5 INFO:Local step 172500, global step 2762742: loss 1.5593
[2019-04-01 18:23:20,823] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 172500, global step 2762742: learning rate 0.0010
[2019-04-01 18:23:21,138] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.3433166e-15 6.0556870e-26 1.1946727e-16 2.0494415e-17 3.7706627e-07
 9.9713647e-01 2.8631443e-03], sum to 1.0000
[2019-04-01 18:23:21,141] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7425
[2019-04-01 18:23:21,195] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.1, 71.66666666666667, 162.6666666666667, 54.00000000000001, 26.0, 24.90835571599052, 7.336971244528468, 0.0, 1.0, 54.64000532727753], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1857000.0000, 
sim time next is 1857600.0000, 
raw observation next is [-5.0, 71.0, 152.0, 40.5, 26.0, 25.02837768556335, 7.520896081502182, 0.0, 1.0, 50.58713411894147], 
processed observation next is [0.0, 0.5217391304347826, 0.32409972299168976, 0.71, 0.5066666666666667, 0.044751381215469614, 1.0, 0.8611968122233356, 0.07520896081502182, 0.0, 1.0, 0.36133667227815336], 
reward next is 0.6387, 
noisyNet noise sample is [array([-0.07101362], dtype=float32), -0.3778636]. 
=============================================
[2019-04-01 18:23:21,418] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8656043e-22 3.8743335e-38 1.1637800e-28 0.0000000e+00 9.4749442e-35
 1.0000000e+00 1.0838319e-20], sum to 1.0000
[2019-04-01 18:23:21,421] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1376
[2019-04-01 18:23:21,432] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.9, 84.0, 0.0, 0.0, 26.0, 25.70724809454834, 12.28738770783099, 0.0, 1.0, 16.85946705811819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1636200.0000, 
sim time next is 1636800.0000, 
raw observation next is [7.0, 83.33333333333334, 0.0, 0.0, 26.0, 25.58834740020468, 11.9688170975745, 0.0, 1.0, 15.91818660238893], 
processed observation next is [1.0, 0.9565217391304348, 0.6565096952908588, 0.8333333333333335, 0.0, 0.0, 1.0, 0.9411924857435255, 0.119688170975745, 0.0, 1.0, 0.11370133287420664], 
reward next is 0.8863, 
noisyNet noise sample is [array([-0.45975965], dtype=float32), -1.2673215]. 
=============================================
[2019-04-01 18:23:21,718] A3C_AGENT_WORKER-Thread-10 INFO:Local step 172500, global step 2763264: loss 0.0290
[2019-04-01 18:23:21,721] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 172500, global step 2763264: learning rate 0.0010
[2019-04-01 18:23:21,979] A3C_AGENT_WORKER-Thread-15 INFO:Local step 172500, global step 2763410: loss 1.4412
[2019-04-01 18:23:21,982] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 172500, global step 2763411: learning rate 0.0010
[2019-04-01 18:23:22,179] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.4838924e-36 0.0000000e+00 0.0000000e+00 2.1307139e-33 5.1201940e-34
 1.4853729e-05 9.9998510e-01], sum to 1.0000
[2019-04-01 18:23:22,182] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2690
[2019-04-01 18:23:22,190] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.5, 61.0, 0.0, 0.0, 26.0, 26.33524424568041, 16.37106519792964, 1.0, 1.0, 6.063923699076229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1620000.0000, 
sim time next is 1620600.0000, 
raw observation next is [10.31666666666667, 61.83333333333334, 0.0, 0.0, 26.0, 26.40384525329747, 16.35396231024165, 1.0, 1.0, 6.19320054936038], 
processed observation next is [1.0, 0.782608695652174, 0.7483841181902126, 0.6183333333333334, 0.0, 0.0, 1.0, 1.0576921790424956, 0.1635396231024165, 1.0, 1.0, 0.04423714678114558], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.45936248], dtype=float32), -0.37088192]. 
=============================================
[2019-04-01 18:23:26,338] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.5520174e-31], sum to 1.0000
[2019-04-01 18:23:26,338] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0257
[2019-04-01 18:23:26,362] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.23807606672176, 9.662985917054236, 0.0, 1.0, 42.50267830498386], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1729800.0000, 
sim time next is 1730400.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.23035541364514, 9.723493594079036, 0.0, 1.0, 42.45982922531173], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 1.0, 0.890050773377877, 0.09723493594079036, 0.0, 1.0, 0.30328449446651234], 
reward next is 0.6967, 
noisyNet noise sample is [array([1.5491315], dtype=float32), -1.3110056]. 
=============================================
[2019-04-01 18:23:27,043] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.6065499e-34 0.0000000e+00 8.7082179e-30 7.7796851e-38 8.2644957e-28
 1.6531989e-05 9.9998343e-01], sum to 1.0000
[2019-04-01 18:23:27,045] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0332
[2019-04-01 18:23:27,104] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 84.33333333333334, 102.3333333333333, 0.0, 26.0, 24.93152835911271, 8.02345438348039, 0.0, 1.0, 42.3901357769116], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1779600.0000, 
sim time next is 1780200.0000, 
raw observation next is [-2.8, 85.0, 99.0, 0.0, 26.0, 24.90926920623133, 8.121490901812349, 0.0, 1.0, 46.18384230506884], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.85, 0.33, 0.0, 1.0, 0.8441813151759041, 0.08121490901812349, 0.0, 1.0, 0.3298845878933489], 
reward next is 0.6701, 
noisyNet noise sample is [array([2.4805946], dtype=float32), 0.503732]. 
=============================================
[2019-04-01 18:23:29,661] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:23:29,663] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9735
[2019-04-01 18:23:29,757] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.866666666666667, 75.0, 0.0, 0.0, 26.0, 25.02701731289549, 7.520955563090169, 1.0, 1.0, 39.32905673502485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1970400.0000, 
sim time next is 1971000.0000, 
raw observation next is [-5.05, 77.0, 0.0, 0.0, 26.0, 24.94421503389961, 7.767780257273533, 1.0, 1.0, 59.1792372922278], 
processed observation next is [1.0, 0.8260869565217391, 0.32271468144044324, 0.77, 0.0, 0.0, 1.0, 0.8491735762713729, 0.07767780257273534, 1.0, 1.0, 0.42270883780162716], 
reward next is 0.5773, 
noisyNet noise sample is [array([-0.48283172], dtype=float32), 0.33071935]. 
=============================================
[2019-04-01 18:23:29,760] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[71.7536  ]
 [71.781364]
 [71.58115 ]
 [71.39379 ]
 [71.22752 ]], R is [[71.41796875]
 [71.42286682]
 [71.48744202]
 [71.54936218]
 [71.61977386]].
[2019-04-01 18:23:31,424] A3C_AGENT_WORKER-Thread-11 INFO:Local step 173000, global step 2767650: loss 0.2197
[2019-04-01 18:23:31,425] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 173000, global step 2767650: learning rate 0.0010
[2019-04-01 18:23:31,532] A3C_AGENT_WORKER-Thread-20 INFO:Local step 173000, global step 2767682: loss -0.4487
[2019-04-01 18:23:31,534] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 173000, global step 2767682: learning rate 0.0010
[2019-04-01 18:23:32,343] A3C_AGENT_WORKER-Thread-2 INFO:Local step 173000, global step 2767911: loss 0.1518
[2019-04-01 18:23:32,343] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 173000, global step 2767911: learning rate 0.0010
[2019-04-01 18:23:32,809] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:23:32,811] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2191
[2019-04-01 18:23:32,891] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 77.0, 124.6666666666667, 58.16666666666666, 26.0, 24.86164183596352, 6.904997388123571, 0.0, 1.0, 58.83810956079766], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1851600.0000, 
sim time next is 1852200.0000, 
raw observation next is [-5.6, 76.5, 120.0, 51.0, 26.0, 24.98734806926836, 7.076709837756771, 0.0, 1.0, 54.94493617234347], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.765, 0.4, 0.056353591160221, 1.0, 0.8553354384669086, 0.07076709837756771, 0.0, 1.0, 0.39246382980245337], 
reward next is 0.6075, 
noisyNet noise sample is [array([2.8742025], dtype=float32), -0.86446106]. 
=============================================
[2019-04-01 18:23:34,667] A3C_AGENT_WORKER-Thread-16 INFO:Local step 173000, global step 2768737: loss 0.0613
[2019-04-01 18:23:34,668] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 173000, global step 2768737: learning rate 0.0010
[2019-04-01 18:23:34,684] A3C_AGENT_WORKER-Thread-14 INFO:Local step 173000, global step 2768741: loss 0.0602
[2019-04-01 18:23:34,684] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 173000, global step 2768741: learning rate 0.0010
[2019-04-01 18:23:35,102] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:23:35,103] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6582
[2019-04-01 18:23:35,148] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 87.0, 71.5, 0.0, 26.0, 25.06397470047214, 8.359854849701437, 0.0, 1.0, 31.12178370544048], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1783200.0000, 
sim time next is 1783800.0000, 
raw observation next is [-3.1, 87.0, 66.0, 0.0, 26.0, 25.05868017598551, 8.243567876240563, 0.0, 1.0, 34.74503018626133], 
processed observation next is [0.0, 0.6521739130434783, 0.37673130193905824, 0.87, 0.22, 0.0, 1.0, 0.8655257394265016, 0.08243567876240564, 0.0, 1.0, 0.2481787870447238], 
reward next is 0.7518, 
noisyNet noise sample is [array([1.3173282], dtype=float32), 0.05869678]. 
=============================================
[2019-04-01 18:23:35,276] A3C_AGENT_WORKER-Thread-19 INFO:Local step 173000, global step 2768934: loss 0.1133
[2019-04-01 18:23:35,277] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 173000, global step 2768934: learning rate 0.0010
[2019-04-01 18:23:35,427] A3C_AGENT_WORKER-Thread-9 INFO:Local step 173000, global step 2768988: loss 0.0814
[2019-04-01 18:23:35,434] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 173000, global step 2768989: learning rate 0.0010
[2019-04-01 18:23:35,768] A3C_AGENT_WORKER-Thread-3 INFO:Local step 173000, global step 2769118: loss 0.0732
[2019-04-01 18:23:35,769] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 173000, global step 2769118: learning rate 0.0010
[2019-04-01 18:23:36,627] A3C_AGENT_WORKER-Thread-13 INFO:Local step 173000, global step 2769454: loss 0.2531
[2019-04-01 18:23:36,631] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 173000, global step 2769454: learning rate 0.0010
[2019-04-01 18:23:37,371] A3C_AGENT_WORKER-Thread-12 INFO:Local step 173000, global step 2769731: loss 0.2842
[2019-04-01 18:23:37,371] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 173000, global step 2769731: learning rate 0.0010
[2019-04-01 18:23:37,772] A3C_AGENT_WORKER-Thread-5 INFO:Local step 173000, global step 2769858: loss 0.2085
[2019-04-01 18:23:37,773] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 173000, global step 2769858: learning rate 0.0010
[2019-04-01 18:23:38,260] A3C_AGENT_WORKER-Thread-10 INFO:Local step 173000, global step 2770010: loss 0.2330
[2019-04-01 18:23:38,261] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 173000, global step 2770010: learning rate 0.0010
[2019-04-01 18:23:39,386] A3C_AGENT_WORKER-Thread-15 INFO:Local step 173000, global step 2770341: loss 0.6669
[2019-04-01 18:23:39,386] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 173000, global step 2770341: learning rate 0.0010
[2019-04-01 18:23:39,510] A3C_AGENT_WORKER-Thread-18 INFO:Local step 173500, global step 2770386: loss 1.2581
[2019-04-01 18:23:39,515] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 173500, global step 2770390: learning rate 0.0010
[2019-04-01 18:23:41,097] A3C_AGENT_WORKER-Thread-17 INFO:Local step 173500, global step 2771003: loss 3.4276
[2019-04-01 18:23:41,099] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 173500, global step 2771003: learning rate 0.0010
[2019-04-01 18:23:42,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.7665626e-36 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:23:42,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5554
[2019-04-01 18:23:42,815] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 79.0, 128.0, 392.5, 26.0, 25.616442814091, 7.610481487667042, 1.0, 1.0, 35.24699257921835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1936800.0000, 
sim time next is 1937400.0000, 
raw observation next is [-7.016666666666667, 78.33333333333334, 142.3333333333333, 340.3333333333333, 26.0, 25.70309392823587, 7.817479856820135, 1.0, 1.0, 34.32886840573779], 
processed observation next is [1.0, 0.43478260869565216, 0.2682363804247461, 0.7833333333333334, 0.4744444444444443, 0.3760589318600368, 1.0, 0.9575848468908384, 0.07817479856820135, 1.0, 1.0, 0.24520620289812708], 
reward next is 0.7548, 
noisyNet noise sample is [array([0.8867959], dtype=float32), -0.4722587]. 
=============================================
[2019-04-01 18:23:43,319] A3C_AGENT_WORKER-Thread-4 INFO:Local step 173500, global step 2771835: loss 6.5124
[2019-04-01 18:23:43,322] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 173500, global step 2771835: learning rate 0.0010
[2019-04-01 18:23:47,775] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:23:47,777] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0664
[2019-04-01 18:23:47,822] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 79.0, 0.0, 0.0, 26.0, 26.0023222159006, 9.766889898104687, 1.0, 1.0, 31.14962528428859], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1965600.0000, 
sim time next is 1966200.0000, 
raw observation next is [-4.916666666666667, 77.66666666666667, 0.0, 0.0, 26.0, 25.96171802371627, 9.542043144062497, 1.0, 1.0, 27.33988864107002], 
processed observation next is [1.0, 0.782608695652174, 0.32640812557710064, 0.7766666666666667, 0.0, 0.0, 1.0, 0.9945311462451814, 0.09542043144062497, 1.0, 1.0, 0.19528491886478586], 
reward next is 0.8047, 
noisyNet noise sample is [array([-0.9462523], dtype=float32), 0.87685925]. 
=============================================
[2019-04-01 18:23:48,679] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6313165e-21 2.8193480e-32 3.1327901e-26 4.6459141e-22 1.2555135e-34
 1.0000000e+00 3.4241142e-22], sum to 1.0000
[2019-04-01 18:23:48,680] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6069
[2019-04-01 18:23:48,810] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.4, 79.00000000000001, 150.6666666666667, 0.0, 26.0, 25.33468037646573, 7.191815182710435, 1.0, 1.0, 30.86317715845639], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2034600.0000, 
sim time next is 2035200.0000, 
raw observation next is [-4.300000000000001, 79.0, 149.3333333333333, 0.0, 26.0, 24.73227831065201, 7.397053501819566, 1.0, 1.0, 80.85135140379863], 
processed observation next is [1.0, 0.5652173913043478, 0.34349030470914127, 0.79, 0.4977777777777776, 0.0, 1.0, 0.8188969015217157, 0.07397053501819566, 1.0, 1.0, 0.5775096528842759], 
reward next is 0.4225, 
noisyNet noise sample is [array([0.95961565], dtype=float32), -0.33308014]. 
=============================================
[2019-04-01 18:23:49,603] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:23:49,603] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3083
[2019-04-01 18:23:49,664] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 0.0, 0.0, 26.0, 25.22482901839293, 7.661023433654847, 1.0, 1.0, 31.24885658441154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1969200.0000, 
sim time next is 1969800.0000, 
raw observation next is [-4.683333333333334, 73.0, 0.0, 0.0, 26.0, 25.09269296319546, 7.594341940634616, 1.0, 1.0, 30.9674118754692], 
processed observation next is [1.0, 0.8260869565217391, 0.3328716528162512, 0.73, 0.0, 0.0, 1.0, 0.870384709027923, 0.07594341940634616, 1.0, 1.0, 0.22119579911049428], 
reward next is 0.7788, 
noisyNet noise sample is [array([-0.16336763], dtype=float32), -0.2731044]. 
=============================================
[2019-04-01 18:23:55,467] A3C_AGENT_WORKER-Thread-20 INFO:Local step 173500, global step 2775987: loss 0.1223
[2019-04-01 18:23:55,468] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 173500, global step 2775987: learning rate 0.0010
[2019-04-01 18:23:55,503] A3C_AGENT_WORKER-Thread-11 INFO:Local step 173500, global step 2775999: loss 0.2180
[2019-04-01 18:23:55,505] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 173500, global step 2776000: learning rate 0.0010
[2019-04-01 18:23:55,836] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7171277e-38 0.0000000e+00 1.3722266e-34 2.7698481e-35 5.1340039e-25
 1.0000000e+00 2.9054597e-18], sum to 1.0000
[2019-04-01 18:23:55,838] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8540
[2019-04-01 18:23:55,893] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 62.0, 153.0, 378.0, 26.0, 24.85457469616281, 7.535929281004449, 0.0, 1.0, 48.14979655832088], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2372400.0000, 
sim time next is 2373000.0000, 
raw observation next is [-2.116666666666667, 59.50000000000001, 157.6666666666667, 354.0, 26.0, 24.90008363613996, 7.94816804382063, 0.0, 1.0, 41.70421725671331], 
processed observation next is [0.0, 0.4782608695652174, 0.40397045244690677, 0.5950000000000001, 0.5255555555555557, 0.3911602209944751, 1.0, 0.8428690908771372, 0.0794816804382063, 0.0, 1.0, 0.29788726611938077], 
reward next is 0.7021, 
noisyNet noise sample is [array([0.532333], dtype=float32), 0.4125626]. 
=============================================
[2019-04-01 18:23:55,896] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[73.79582 ]
 [73.7534  ]
 [73.75333 ]
 [73.72845 ]
 [73.732086]], R is [[73.72097778]
 [73.6398468 ]
 [73.64199066]
 [73.69528198]
 [73.78994751]].
[2019-04-01 18:23:56,473] A3C_AGENT_WORKER-Thread-2 INFO:Local step 173500, global step 2776402: loss 0.1870
[2019-04-01 18:23:56,473] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 173500, global step 2776402: learning rate 0.0010
[2019-04-01 18:23:58,296] A3C_AGENT_WORKER-Thread-16 INFO:Local step 173500, global step 2777020: loss 0.2707
[2019-04-01 18:23:58,297] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 173500, global step 2777020: learning rate 0.0010
[2019-04-01 18:23:58,483] A3C_AGENT_WORKER-Thread-14 INFO:Local step 173500, global step 2777077: loss 0.6181
[2019-04-01 18:23:58,484] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 173500, global step 2777077: learning rate 0.0010
[2019-04-01 18:23:58,668] A3C_AGENT_WORKER-Thread-19 INFO:Local step 173500, global step 2777137: loss 0.7470
[2019-04-01 18:23:58,670] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 173500, global step 2777137: learning rate 0.0010
[2019-04-01 18:23:59,418] A3C_AGENT_WORKER-Thread-9 INFO:Local step 173500, global step 2777411: loss 1.7831
[2019-04-01 18:23:59,418] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 173500, global step 2777411: learning rate 0.0010
[2019-04-01 18:23:59,619] A3C_AGENT_WORKER-Thread-3 INFO:Local step 173500, global step 2777489: loss 0.9886
[2019-04-01 18:23:59,619] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:23:59,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 173500, global step 2777489: learning rate 0.0010
[2019-04-01 18:23:59,625] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7570
[2019-04-01 18:23:59,639] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.933333333333334, 51.33333333333333, 0.0, 0.0, 26.0, 24.23313986418737, 5.38751881790041, 0.0, 1.0, 42.92472069740601], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2425200.0000, 
sim time next is 2425800.0000, 
raw observation next is [-7.116666666666666, 52.16666666666667, 0.0, 0.0, 26.0, 24.18365017360141, 5.355482644938448, 0.0, 1.0, 42.95287796979054], 
processed observation next is [0.0, 0.043478260869565216, 0.265466297322253, 0.5216666666666667, 0.0, 0.0, 1.0, 0.7405214533716301, 0.05355482644938448, 0.0, 1.0, 0.3068062712127896], 
reward next is 0.6932, 
noisyNet noise sample is [array([0.40621305], dtype=float32), 0.8919794]. 
=============================================
[2019-04-01 18:23:59,686] A3C_AGENT_WORKER-Thread-13 INFO:Local step 173500, global step 2777515: loss 0.4865
[2019-04-01 18:23:59,687] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 173500, global step 2777515: learning rate 0.0010
[2019-04-01 18:24:00,608] A3C_AGENT_WORKER-Thread-18 INFO:Local step 174000, global step 2777888: loss 0.1589
[2019-04-01 18:24:00,609] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 174000, global step 2777888: learning rate 0.0010
[2019-04-01 18:24:01,183] A3C_AGENT_WORKER-Thread-12 INFO:Local step 173500, global step 2778079: loss 0.8247
[2019-04-01 18:24:01,183] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 173500, global step 2778079: learning rate 0.0010
[2019-04-01 18:24:01,331] A3C_AGENT_WORKER-Thread-5 INFO:Local step 173500, global step 2778127: loss 0.8319
[2019-04-01 18:24:01,332] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 173500, global step 2778127: learning rate 0.0010
[2019-04-01 18:24:01,962] A3C_AGENT_WORKER-Thread-10 INFO:Local step 173500, global step 2778336: loss 0.2445
[2019-04-01 18:24:01,963] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 173500, global step 2778336: learning rate 0.0010
[2019-04-01 18:24:02,244] A3C_AGENT_WORKER-Thread-17 INFO:Local step 174000, global step 2778443: loss 0.0982
[2019-04-01 18:24:02,245] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 174000, global step 2778443: learning rate 0.0010
[2019-04-01 18:24:03,626] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:24:03,632] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7118
[2019-04-01 18:24:03,668] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 70.0, 124.0, 0.0, 26.0, 26.04134900851454, 9.5543008170294, 1.0, 1.0, 24.31789824761255], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2211600.0000, 
sim time next is 2212200.0000, 
raw observation next is [-3.9, 69.5, 120.0, 0.0, 26.0, 26.06036467592358, 9.463278810551522, 1.0, 1.0, 22.95533016100926], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.695, 0.4, 0.0, 1.0, 1.00862352513194, 0.09463278810551522, 1.0, 1.0, 0.163966644007209], 
reward next is 0.8360, 
noisyNet noise sample is [array([-0.5204517], dtype=float32), -1.0209912]. 
=============================================
[2019-04-01 18:24:03,752] A3C_AGENT_WORKER-Thread-15 INFO:Local step 173500, global step 2779017: loss 0.0369
[2019-04-01 18:24:03,753] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 173500, global step 2779017: learning rate 0.0010
[2019-04-01 18:24:04,097] A3C_AGENT_WORKER-Thread-4 INFO:Local step 174000, global step 2779131: loss 0.0712
[2019-04-01 18:24:04,099] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 174000, global step 2779131: learning rate 0.0010
[2019-04-01 18:24:07,992] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:24:07,992] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5694
[2019-04-01 18:24:08,028] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 55.0, 0.0, 0.0, 26.0, 25.18535815308102, 9.335803850966398, 0.0, 1.0, 42.48500424757164], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2320200.0000, 
sim time next is 2320800.0000, 
raw observation next is [-1.7, 54.66666666666667, 0.0, 0.0, 26.0, 25.30460346286378, 9.58195869322177, 0.0, 1.0, 41.04357334354666], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.5466666666666667, 0.0, 0.0, 1.0, 0.9006576375519687, 0.0958195869322177, 0.0, 1.0, 0.2931683810253333], 
reward next is 0.7068, 
noisyNet noise sample is [array([0.5507722], dtype=float32), -1.1022221]. 
=============================================
[2019-04-01 18:24:15,832] A3C_AGENT_WORKER-Thread-20 INFO:Local step 174000, global step 2783716: loss 0.0102
[2019-04-01 18:24:15,836] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 174000, global step 2783719: learning rate 0.0010
[2019-04-01 18:24:16,072] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.8011842e-27], sum to 1.0000
[2019-04-01 18:24:16,074] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7164
[2019-04-01 18:24:16,132] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4, 45.66666666666667, 66.83333333333334, 51.0, 26.0, 24.90706225152487, 7.399243628588121, 0.0, 1.0, 35.87552670199589], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2392800.0000, 
sim time next is 2393400.0000, 
raw observation next is [-0.5, 45.33333333333333, 54.66666666666667, 44.0, 26.0, 24.91600603014834, 7.47072821592382, 0.0, 1.0, 37.29834667547558], 
processed observation next is [0.0, 0.6956521739130435, 0.44875346260387816, 0.4533333333333333, 0.18222222222222223, 0.04861878453038674, 1.0, 0.8451437185926197, 0.0747072821592382, 0.0, 1.0, 0.2664167619676827], 
reward next is 0.7336, 
noisyNet noise sample is [array([0.12320515], dtype=float32), -0.02026397]. 
=============================================
[2019-04-01 18:24:16,560] A3C_AGENT_WORKER-Thread-11 INFO:Local step 174000, global step 2783983: loss 0.0149
[2019-04-01 18:24:16,561] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 174000, global step 2783983: learning rate 0.0010
[2019-04-01 18:24:16,919] A3C_AGENT_WORKER-Thread-2 INFO:Local step 174000, global step 2784108: loss 0.1847
[2019-04-01 18:24:16,922] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 174000, global step 2784108: learning rate 0.0010
[2019-04-01 18:24:16,954] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:24:16,955] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8856
[2019-04-01 18:24:16,993] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 63.5, 0.0, 0.0, 26.0, 24.92968016319843, 7.098067338135434, 0.0, 1.0, 37.98527062040557], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2334600.0000, 
sim time next is 2335200.0000, 
raw observation next is [-2.3, 63.0, 0.0, 0.0, 26.0, 24.88585447761464, 6.985303198936239, 0.0, 1.0, 38.08057151808013], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.63, 0.0, 0.0, 1.0, 0.8408363539449485, 0.0698530319893624, 0.0, 1.0, 0.2720040822720009], 
reward next is 0.7280, 
noisyNet noise sample is [array([-2.1192508], dtype=float32), -0.4254588]. 
=============================================
[2019-04-01 18:24:18,587] A3C_AGENT_WORKER-Thread-14 INFO:Local step 174000, global step 2784785: loss 0.0274
[2019-04-01 18:24:18,590] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 174000, global step 2784786: learning rate 0.0010
[2019-04-01 18:24:18,853] A3C_AGENT_WORKER-Thread-16 INFO:Local step 174000, global step 2784900: loss 0.0643
[2019-04-01 18:24:18,854] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 174000, global step 2784900: learning rate 0.0010
[2019-04-01 18:24:19,098] A3C_AGENT_WORKER-Thread-19 INFO:Local step 174000, global step 2784999: loss 0.0374
[2019-04-01 18:24:19,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 174000, global step 2784999: learning rate 0.0010
[2019-04-01 18:24:19,725] A3C_AGENT_WORKER-Thread-9 INFO:Local step 174000, global step 2785268: loss 0.0190
[2019-04-01 18:24:19,726] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 174000, global step 2785268: learning rate 0.0010
[2019-04-01 18:24:20,027] A3C_AGENT_WORKER-Thread-13 INFO:Local step 174000, global step 2785384: loss 0.0168
[2019-04-01 18:24:20,028] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 174000, global step 2785384: learning rate 0.0010
[2019-04-01 18:24:20,166] A3C_AGENT_WORKER-Thread-3 INFO:Local step 174000, global step 2785437: loss 0.0153
[2019-04-01 18:24:20,167] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 174000, global step 2785437: learning rate 0.0010
[2019-04-01 18:24:20,217] A3C_AGENT_WORKER-Thread-18 INFO:Local step 174500, global step 2785454: loss 0.0713
[2019-04-01 18:24:20,217] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 174500, global step 2785454: learning rate 0.0010
[2019-04-01 18:24:21,396] A3C_AGENT_WORKER-Thread-12 INFO:Local step 174000, global step 2785985: loss 0.3075
[2019-04-01 18:24:21,396] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 174000, global step 2785985: learning rate 0.0010
[2019-04-01 18:24:21,420] A3C_AGENT_WORKER-Thread-17 INFO:Local step 174500, global step 2785999: loss 0.4156
[2019-04-01 18:24:21,424] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 174500, global step 2785999: learning rate 0.0010
[2019-04-01 18:24:21,429] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:24:21,431] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3596
[2019-04-01 18:24:21,445] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 58.5, 15.33333333333333, 165.3333333333333, 26.0, 22.82242096719227, 6.31133854841201, 0.0, 1.0, 43.52564531160774], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2447400.0000, 
sim time next is 2448000.0000, 
raw observation next is [-9.5, 58.0, 21.5, 228.0, 26.0, 22.83383911720776, 6.282502190065604, 0.0, 1.0, 43.39373781513505], 
processed observation next is [0.0, 0.34782608695652173, 0.1994459833795014, 0.58, 0.07166666666666667, 0.25193370165745854, 1.0, 0.5476913024582514, 0.06282502190065604, 0.0, 1.0, 0.3099552701081075], 
reward next is 0.6900, 
noisyNet noise sample is [array([0.1748565], dtype=float32), 2.0712256]. 
=============================================
[2019-04-01 18:24:21,469] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[66.17553]
 [66.20553]
 [66.3771 ]
 [66.37732]
 [66.38661]], R is [[66.38000488]
 [66.40530396]
 [66.42976379]
 [66.45376587]
 [66.47790527]].
[2019-04-01 18:24:21,469] A3C_AGENT_WORKER-Thread-5 INFO:Local step 174000, global step 2786018: loss 0.0247
[2019-04-01 18:24:21,472] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 174000, global step 2786018: learning rate 0.0010
[2019-04-01 18:24:21,631] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.0110302e-25], sum to 1.0000
[2019-04-01 18:24:21,631] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0351
[2019-04-01 18:24:21,671] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 30.0, 0.0, 0.0, 26.0, 24.9273371016395, 6.577555987131535, 0.0, 1.0, 26.27379886444765], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2487600.0000, 
sim time next is 2488200.0000, 
raw observation next is [-0.1166666666666667, 29.83333333333333, 0.0, 0.0, 26.0, 24.86500285506716, 6.477827240141242, 0.0, 1.0, 27.09538109485771], 
processed observation next is [0.0, 0.8260869565217391, 0.4593721144967683, 0.2983333333333333, 0.0, 0.0, 1.0, 0.8378575507238801, 0.06477827240141242, 0.0, 1.0, 0.1935384363918408], 
reward next is 0.8065, 
noisyNet noise sample is [array([0.27580413], dtype=float32), 0.075361766]. 
=============================================
[2019-04-01 18:24:22,022] A3C_AGENT_WORKER-Thread-10 INFO:Local step 174000, global step 2786224: loss 0.0206
[2019-04-01 18:24:22,023] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 174000, global step 2786224: learning rate 0.0010
[2019-04-01 18:24:23,569] A3C_AGENT_WORKER-Thread-4 INFO:Local step 174500, global step 2786991: loss 0.1129
[2019-04-01 18:24:23,570] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 174500, global step 2786992: learning rate 0.0010
[2019-04-01 18:24:23,722] A3C_AGENT_WORKER-Thread-15 INFO:Local step 174000, global step 2787074: loss 0.0165
[2019-04-01 18:24:23,723] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 174000, global step 2787074: learning rate 0.0010
[2019-04-01 18:24:26,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:24:26,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2014
[2019-04-01 18:24:26,910] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.716666666666667, 54.5, 0.0, 0.0, 26.0, 24.77089481824787, 5.553208062787976, 0.0, 1.0, 38.04132341323615], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2530200.0000, 
sim time next is 2530800.0000, 
raw observation next is [-2.8, 54.0, 0.0, 0.0, 26.0, 24.79026132931752, 5.524898849579396, 0.0, 1.0, 38.35302333697467], 
processed observation next is [1.0, 0.30434782608695654, 0.38504155124653744, 0.54, 0.0, 0.0, 1.0, 0.8271801899025029, 0.05524898849579395, 0.0, 1.0, 0.27395016669267624], 
reward next is 0.7260, 
noisyNet noise sample is [array([-0.3480803], dtype=float32), -0.20086116]. 
=============================================
[2019-04-01 18:24:30,672] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.0222127e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.1998307e-37
 1.0000000e+00 1.4541683e-25], sum to 1.0000
[2019-04-01 18:24:30,672] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6980
[2019-04-01 18:24:30,711] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 79.16666666666667, 0.0, 0.0, 26.0, 24.9296017389267, 8.19328285607209, 1.0, 1.0, 26.08486143177777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2923800.0000, 
sim time next is 2924400.0000, 
raw observation next is [-1.0, 80.33333333333334, 0.0, 0.0, 26.0, 24.79980343892456, 8.3173055412415, 0.0, 1.0, 44.19344862361928], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.8033333333333335, 0.0, 0.0, 1.0, 0.8285433484177942, 0.083173055412415, 0.0, 1.0, 0.31566749016870915], 
reward next is 0.6843, 
noisyNet noise sample is [array([1.2940917], dtype=float32), 1.6201535]. 
=============================================
[2019-04-01 18:24:30,712] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:24:30,715] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0499
[2019-04-01 18:24:30,749] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 44.33333333333334, 207.3333333333333, 143.5, 26.0, 26.81124153411561, 13.16098291381609, 1.0, 1.0, 19.67773982163939], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2643600.0000, 
sim time next is 2644200.0000, 
raw observation next is [0.5, 45.0, 216.0, 130.0, 26.0, 26.75510541760136, 11.41838669344376, 1.0, 1.0, 20.12155369749597], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.45, 0.72, 0.143646408839779, 1.0, 1.10787220251448, 0.11418386693443759, 1.0, 1.0, 0.14372538355354264], 
reward next is 0.2889, 
noisyNet noise sample is [array([0.30605686], dtype=float32), -0.80798984]. 
=============================================
[2019-04-01 18:24:32,166] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:24:32,167] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9499
[2019-04-01 18:24:32,218] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.566666666666666, 68.33333333333333, 102.8333333333333, 121.6666666666667, 26.0, 25.73540920448136, 7.944668209596131, 1.0, 1.0, 33.94475816702762], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2626800.0000, 
sim time next is 2627400.0000, 
raw observation next is [-5.283333333333333, 66.66666666666667, 112.6666666666667, 152.3333333333333, 26.0, 25.76493326381479, 8.058714555928598, 1.0, 1.0, 27.017741684981], 
processed observation next is [1.0, 0.391304347826087, 0.31625115420129274, 0.6666666666666667, 0.37555555555555564, 0.16832412523020251, 1.0, 0.9664190376878271, 0.08058714555928598, 1.0, 1.0, 0.1929838691784357], 
reward next is 0.8070, 
noisyNet noise sample is [array([0.7306774], dtype=float32), 1.2976787]. 
=============================================
[2019-04-01 18:24:33,030] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:24:33,030] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1731
[2019-04-01 18:24:33,041] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.0, 69.0, 0.0, 0.0, 26.0, 24.54054368082158, 6.39372732674187, 0.0, 1.0, 43.77905168941097], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2682000.0000, 
sim time next is 2682600.0000, 
raw observation next is [-9.333333333333334, 70.16666666666667, 0.0, 0.0, 26.0, 24.54828294048647, 6.257490471693127, 0.0, 1.0, 43.76110531404102], 
processed observation next is [1.0, 0.043478260869565216, 0.20406278855032317, 0.7016666666666667, 0.0, 0.0, 1.0, 0.7926118486409243, 0.06257490471693126, 0.0, 1.0, 0.31257932367172153], 
reward next is 0.6874, 
noisyNet noise sample is [array([-0.37264735], dtype=float32), 0.973551]. 
=============================================
[2019-04-01 18:24:34,831] A3C_AGENT_WORKER-Thread-20 INFO:Local step 174500, global step 2792009: loss 3.2202
[2019-04-01 18:24:34,832] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 174500, global step 2792009: learning rate 0.0010
[2019-04-01 18:24:35,525] A3C_AGENT_WORKER-Thread-11 INFO:Local step 174500, global step 2792269: loss 3.5677
[2019-04-01 18:24:35,526] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 174500, global step 2792269: learning rate 0.0010
[2019-04-01 18:24:35,551] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:24:35,551] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3743
[2019-04-01 18:24:35,612] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-14.16666666666667, 89.66666666666667, 85.66666666666667, 424.0, 26.0, 25.98799728695489, 9.218913073507153, 1.0, 1.0, 39.31289619969231], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2710200.0000, 
sim time next is 2710800.0000, 
raw observation next is [-14.0, 91.0, 88.5, 471.0, 26.0, 26.03359798659054, 9.09364053442975, 1.0, 1.0, 36.6821128245162], 
processed observation next is [1.0, 0.391304347826087, 0.07479224376731301, 0.91, 0.295, 0.5204419889502763, 1.0, 1.0047997123700771, 0.0909364053442975, 1.0, 1.0, 0.2620150916036872], 
reward next is 0.7380, 
noisyNet noise sample is [array([0.68498415], dtype=float32), -1.1044571]. 
=============================================
[2019-04-01 18:24:35,667] A3C_AGENT_WORKER-Thread-2 INFO:Local step 174500, global step 2792322: loss 2.9717
[2019-04-01 18:24:35,668] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 174500, global step 2792322: learning rate 0.0010
[2019-04-01 18:24:37,360] A3C_AGENT_WORKER-Thread-19 INFO:Local step 174500, global step 2793008: loss 5.2579
[2019-04-01 18:24:37,360] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 174500, global step 2793008: learning rate 0.0010
[2019-04-01 18:24:37,673] A3C_AGENT_WORKER-Thread-14 INFO:Local step 174500, global step 2793111: loss 3.4803
[2019-04-01 18:24:37,674] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 174500, global step 2793111: learning rate 0.0010
[2019-04-01 18:24:37,936] A3C_AGENT_WORKER-Thread-16 INFO:Local step 174500, global step 2793196: loss 3.4392
[2019-04-01 18:24:37,937] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 174500, global step 2793196: learning rate 0.0010
[2019-04-01 18:24:37,945] A3C_AGENT_WORKER-Thread-18 INFO:Local step 175000, global step 2793198: loss 0.0200
[2019-04-01 18:24:37,946] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 175000, global step 2793198: learning rate 0.0010
[2019-04-01 18:24:38,683] A3C_AGENT_WORKER-Thread-9 INFO:Local step 174500, global step 2793496: loss 4.1330
[2019-04-01 18:24:38,684] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 174500, global step 2793496: learning rate 0.0010
[2019-04-01 18:24:38,734] A3C_AGENT_WORKER-Thread-13 INFO:Local step 174500, global step 2793516: loss 2.3363
[2019-04-01 18:24:38,735] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 174500, global step 2793516: learning rate 0.0010
[2019-04-01 18:24:39,326] A3C_AGENT_WORKER-Thread-3 INFO:Local step 174500, global step 2793779: loss 1.4183
[2019-04-01 18:24:39,326] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 174500, global step 2793779: learning rate 0.0010
[2019-04-01 18:24:39,734] A3C_AGENT_WORKER-Thread-17 INFO:Local step 175000, global step 2793954: loss 0.0302
[2019-04-01 18:24:39,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 175000, global step 2793954: learning rate 0.0010
[2019-04-01 18:24:40,284] A3C_AGENT_WORKER-Thread-12 INFO:Local step 174500, global step 2794190: loss 2.2613
[2019-04-01 18:24:40,286] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 174500, global step 2794191: learning rate 0.0010
[2019-04-01 18:24:40,502] A3C_AGENT_WORKER-Thread-5 INFO:Local step 174500, global step 2794282: loss 1.0157
[2019-04-01 18:24:40,503] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 174500, global step 2794282: learning rate 0.0010
[2019-04-01 18:24:40,817] A3C_AGENT_WORKER-Thread-10 INFO:Local step 174500, global step 2794403: loss 0.0639
[2019-04-01 18:24:40,822] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 174500, global step 2794404: learning rate 0.0010
[2019-04-01 18:24:41,665] A3C_AGENT_WORKER-Thread-4 INFO:Local step 175000, global step 2794750: loss 0.0596
[2019-04-01 18:24:41,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 175000, global step 2794750: learning rate 0.0010
[2019-04-01 18:24:42,115] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:24:42,117] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1057
[2019-04-01 18:24:42,127] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 50.0, 149.5, 635.5, 26.0, 26.10332759852614, 10.3868659884266, 1.0, 1.0, 12.9394532013021], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2804400.0000, 
sim time next is 2805000.0000, 
raw observation next is [-0.5, 49.00000000000001, 141.3333333333333, 678.0, 26.0, 26.13656004784695, 10.40052320004585, 1.0, 1.0, 11.47885474991102], 
processed observation next is [1.0, 0.4782608695652174, 0.44875346260387816, 0.49000000000000005, 0.471111111111111, 0.7491712707182321, 1.0, 1.0195085782638498, 0.1040052320004585, 1.0, 1.0, 0.08199181964222158], 
reward next is 0.7578, 
noisyNet noise sample is [array([-2.6219828], dtype=float32), 0.4869349]. 
=============================================
[2019-04-01 18:24:42,135] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[64.41842]
 [64.52781]
 [64.59219]
 [64.64038]
 [64.22156]], R is [[64.4966507 ]
 [64.60451508]
 [64.79289246]
 [65.02862549]
 [65.26446533]].
[2019-04-01 18:24:42,843] A3C_AGENT_WORKER-Thread-15 INFO:Local step 174500, global step 2795315: loss 0.1439
[2019-04-01 18:24:42,844] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 174500, global step 2795315: learning rate 0.0010
[2019-04-01 18:24:46,070] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 2.525011e-25], sum to 1.0000
[2019-04-01 18:24:46,071] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7496
[2019-04-01 18:24:46,089] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.333333333333333, 80.33333333333334, 0.0, 0.0, 26.0, 25.33937275985513, 9.082031655977753, 0.0, 1.0, 43.7770044372644], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2931600.0000, 
sim time next is 2932200.0000, 
raw observation next is [-1.5, 81.5, 0.0, 0.0, 26.0, 25.25438600016424, 8.86187503199975, 0.0, 1.0, 42.99732040969736], 
processed observation next is [1.0, 0.9565217391304348, 0.4210526315789474, 0.815, 0.0, 0.0, 1.0, 0.893483714309177, 0.0886187503199975, 0.0, 1.0, 0.307123717212124], 
reward next is 0.6929, 
noisyNet noise sample is [array([1.5301014], dtype=float32), 0.8483939]. 
=============================================
[2019-04-01 18:24:50,966] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:24:50,966] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9182
[2019-04-01 18:24:50,984] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.33701390205474, 10.41257905853367, 0.0, 1.0, 43.3086070965431], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2930400.0000, 
sim time next is 2931000.0000, 
raw observation next is [-1.166666666666667, 79.16666666666667, 0.0, 0.0, 26.0, 25.34622777675145, 9.330469818952436, 0.0, 1.0, 44.03337537821017], 
processed observation next is [1.0, 0.9565217391304348, 0.43028624192059095, 0.7916666666666667, 0.0, 0.0, 1.0, 0.9066039681073502, 0.09330469818952436, 0.0, 1.0, 0.31452410984435836], 
reward next is 0.6855, 
noisyNet noise sample is [array([1.8422241], dtype=float32), -0.1222422]. 
=============================================
[2019-04-01 18:24:50,994] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[67.875336]
 [68.047714]
 [68.15893 ]
 [68.35079 ]
 [68.61602 ]], R is [[67.76631927]
 [67.7793045 ]
 [67.79136658]
 [67.80232239]
 [67.81195831]].
[2019-04-01 18:24:52,717] A3C_AGENT_WORKER-Thread-20 INFO:Local step 175000, global step 2799968: loss 0.0273
[2019-04-01 18:24:52,718] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 175000, global step 2799968: learning rate 0.0010
[2019-04-01 18:24:52,805] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-01 18:24:52,806] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 18:24:52,808] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 18:24:52,808] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:24:52,809] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 18:24:52,809] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:24:52,811] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:24:52,818] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run29
[2019-04-01 18:24:52,838] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run29
[2019-04-01 18:24:52,857] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run29
[2019-04-01 18:26:25,344] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.6035919], dtype=float32), -1.0770586]
[2019-04-01 18:26:25,345] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.0, 49.0, 37.5, 338.0, 26.0, 26.64147728060064, 15.86775125874499, 1.0, 1.0, 8.573967078058363]
[2019-04-01 18:26:25,345] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 18:26:25,346] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.8696124525896092
[2019-04-01 18:26:34,211] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 18:26:36,603] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.6035919], dtype=float32), -1.0770586]
[2019-04-01 18:26:36,603] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-11.77359212, 59.21666768, 0.0, 0.0, 26.0, 24.18578638729803, 5.630167767235018, 0.0, 1.0, 43.58565653348968]
[2019-04-01 18:26:36,604] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 18:26:36,605] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 6.1836204e-36], sampled 0.35619442370294063
[2019-04-01 18:26:53,993] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 18:26:57,138] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 18:26:58,162] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 2800000, evaluation results [2800000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 18:26:58,610] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7339694e-23 5.0165802e-32 4.4676096e-26 5.2177265e-34 3.7677103e-16
 9.9994135e-01 5.8705642e-05], sum to 1.0000
[2019-04-01 18:26:58,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6047
[2019-04-01 18:26:58,642] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 23.79553484524578, 5.225010217814344, 0.0, 1.0, 39.70110624235411], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3045600.0000, 
sim time next is 3046200.0000, 
raw observation next is [-6.0, 71.16666666666667, 0.0, 0.0, 26.0, 23.76805403009658, 5.225676112225322, 0.0, 1.0, 39.73626149707629], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7116666666666667, 0.0, 0.0, 1.0, 0.6811505757280827, 0.05225676112225322, 0.0, 1.0, 0.28383043926483065], 
reward next is 0.7162, 
noisyNet noise sample is [array([-1.1981375], dtype=float32), -0.6992191]. 
=============================================
[2019-04-01 18:26:58,707] A3C_AGENT_WORKER-Thread-2 INFO:Local step 175000, global step 2800200: loss 0.0610
[2019-04-01 18:26:58,708] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 175000, global step 2800200: learning rate 0.0010
[2019-04-01 18:26:58,774] A3C_AGENT_WORKER-Thread-18 INFO:Local step 175500, global step 2800229: loss 1.7135
[2019-04-01 18:26:58,775] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 175500, global step 2800229: learning rate 0.0010
[2019-04-01 18:26:58,853] A3C_AGENT_WORKER-Thread-11 INFO:Local step 175000, global step 2800255: loss 0.0788
[2019-04-01 18:26:58,855] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 175000, global step 2800256: learning rate 0.0010
[2019-04-01 18:27:00,459] A3C_AGENT_WORKER-Thread-19 INFO:Local step 175000, global step 2800939: loss 0.0707
[2019-04-01 18:27:00,461] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 175000, global step 2800940: learning rate 0.0010
[2019-04-01 18:27:00,496] A3C_AGENT_WORKER-Thread-17 INFO:Local step 175500, global step 2800959: loss 0.3920
[2019-04-01 18:27:00,502] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 175500, global step 2800960: learning rate 0.0010
[2019-04-01 18:27:00,810] A3C_AGENT_WORKER-Thread-14 INFO:Local step 175000, global step 2801106: loss 0.0012
[2019-04-01 18:27:00,815] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 175000, global step 2801109: learning rate 0.0010
[2019-04-01 18:27:01,384] A3C_AGENT_WORKER-Thread-16 INFO:Local step 175000, global step 2801359: loss 0.0041
[2019-04-01 18:27:01,385] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 175000, global step 2801361: learning rate 0.0010
[2019-04-01 18:27:01,626] A3C_AGENT_WORKER-Thread-13 INFO:Local step 175000, global step 2801465: loss 0.0013
[2019-04-01 18:27:01,626] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 175000, global step 2801465: learning rate 0.0010
[2019-04-01 18:27:02,055] A3C_AGENT_WORKER-Thread-9 INFO:Local step 175000, global step 2801645: loss 0.0354
[2019-04-01 18:27:02,055] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 175000, global step 2801645: learning rate 0.0010
[2019-04-01 18:27:02,346] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:27:02,349] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7163
[2019-04-01 18:27:02,361] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.833333333333334, 70.0, 0.0, 0.0, 26.0, 24.66917860726615, 6.176732073446362, 0.0, 1.0, 37.49168290787422], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3027000.0000, 
sim time next is 3027600.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.62772352958344, 6.096502724854306, 0.0, 1.0, 37.55786881514668], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 1.0, 0.8039605042262059, 0.060965027248543066, 0.0, 1.0, 0.268270491536762], 
reward next is 0.7317, 
noisyNet noise sample is [array([0.33155844], dtype=float32), -0.16920361]. 
=============================================
[2019-04-01 18:27:02,619] A3C_AGENT_WORKER-Thread-3 INFO:Local step 175000, global step 2801934: loss 0.0455
[2019-04-01 18:27:02,619] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 175000, global step 2801934: learning rate 0.0010
[2019-04-01 18:27:02,839] A3C_AGENT_WORKER-Thread-4 INFO:Local step 175500, global step 2802055: loss 0.9303
[2019-04-01 18:27:02,839] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 175500, global step 2802055: learning rate 0.0010
[2019-04-01 18:27:03,172] A3C_AGENT_WORKER-Thread-12 INFO:Local step 175000, global step 2802221: loss 0.0410
[2019-04-01 18:27:03,180] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 175000, global step 2802226: learning rate 0.0010
[2019-04-01 18:27:03,513] A3C_AGENT_WORKER-Thread-5 INFO:Local step 175000, global step 2802379: loss 0.0283
[2019-04-01 18:27:03,514] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 175000, global step 2802379: learning rate 0.0010
[2019-04-01 18:27:03,800] A3C_AGENT_WORKER-Thread-10 INFO:Local step 175000, global step 2802514: loss 0.0035
[2019-04-01 18:27:03,800] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 175000, global step 2802514: learning rate 0.0010
[2019-04-01 18:27:05,365] A3C_AGENT_WORKER-Thread-15 INFO:Local step 175000, global step 2803359: loss 0.0045
[2019-04-01 18:27:05,366] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 175000, global step 2803359: learning rate 0.0010
[2019-04-01 18:27:07,563] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.9128870e-26 1.5517616e-25 7.2047506e-27 2.8331154e-17 1.1738465e-30
 1.0000000e+00 1.5850808e-24], sum to 1.0000
[2019-04-01 18:27:07,564] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9930
[2019-04-01 18:27:07,572] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.3, 99.5, 58.0, 499.0, 26.0, 26.03110842658459, 18.73606294548065, 1.0, 1.0, 1.128164225112409], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3169800.0000, 
sim time next is 3170400.0000, 
raw observation next is [6.2, 99.66666666666666, 49.66666666666667, 435.1666666666667, 26.0, 26.56971671479065, 20.27769703202955, 1.0, 1.0, 1.106693582435776], 
processed observation next is [1.0, 0.6956521739130435, 0.6343490304709142, 0.9966666666666666, 0.16555555555555557, 0.4808471454880295, 1.0, 1.0813881021129501, 0.2027769703202955, 1.0, 1.0, 0.007904954160255542], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.543851], dtype=float32), 0.64152354]. 
=============================================
[2019-04-01 18:27:08,199] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5206800e-25 3.2478368e-24 1.5606509e-22 7.3093905e-20 5.2673841e-29
 1.0000000e+00 6.5678606e-20], sum to 1.0000
[2019-04-01 18:27:08,201] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2249
[2019-04-01 18:27:08,209] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.8, 99.5, 84.0, 679.0, 26.0, 27.23391844266039, 22.28834357817262, 1.0, 1.0, 1.322693976445136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3166200.0000, 
sim time next is 3166800.0000, 
raw observation next is [6.733333333333333, 99.33333333333334, 79.66666666666666, 649.0, 26.0, 27.28146560233779, 22.80713646093459, 1.0, 1.0, 1.259692833362935], 
processed observation next is [1.0, 0.6521739130434783, 0.649122807017544, 0.9933333333333334, 0.26555555555555554, 0.7171270718232045, 1.0, 1.1830665146196842, 0.2280713646093459, 1.0, 1.0, 0.008997805952592391], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.90396655], dtype=float32), 0.057458483]. 
=============================================
[2019-04-01 18:27:13,630] A3C_AGENT_WORKER-Thread-18 INFO:Local step 176000, global step 2807893: loss 0.2851
[2019-04-01 18:27:13,631] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 176000, global step 2807893: learning rate 0.0010
[2019-04-01 18:27:13,857] A3C_AGENT_WORKER-Thread-20 INFO:Local step 175500, global step 2808002: loss 0.1954
[2019-04-01 18:27:13,859] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 175500, global step 2808004: learning rate 0.0010
[2019-04-01 18:27:14,566] A3C_AGENT_WORKER-Thread-2 INFO:Local step 175500, global step 2808314: loss 0.1765
[2019-04-01 18:27:14,566] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 175500, global step 2808314: learning rate 0.0010
[2019-04-01 18:27:14,664] A3C_AGENT_WORKER-Thread-11 INFO:Local step 175500, global step 2808362: loss 0.1873
[2019-04-01 18:27:14,666] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 175500, global step 2808362: learning rate 0.0010
[2019-04-01 18:27:15,518] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7400691e-37
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:27:15,519] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7165
[2019-04-01 18:27:15,541] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.1, 28.0, 0.0, 0.0, 26.0, 25.53094921790507, 7.966642816327467, 0.0, 1.0, 29.45536666143638], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3641400.0000, 
sim time next is 3642000.0000, 
raw observation next is [8.066666666666666, 28.33333333333334, 0.0, 0.0, 26.0, 25.55563082767548, 7.878642782671726, 0.0, 1.0, 27.81467798580312], 
processed observation next is [0.0, 0.13043478260869565, 0.6860572483841183, 0.2833333333333334, 0.0, 0.0, 1.0, 0.9365186896679256, 0.07878642782671726, 0.0, 1.0, 0.19867627132716514], 
reward next is 0.8013, 
noisyNet noise sample is [array([-1.8675877], dtype=float32), 0.071527414]. 
=============================================
[2019-04-01 18:27:15,552] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[84.17071]
 [84.20512]
 [84.28797]
 [84.49495]
 [84.55158]], R is [[84.13104248]
 [84.07933807]
 [84.01142883]
 [83.9569931 ]
 [83.9148407 ]].
[2019-04-01 18:27:15,775] A3C_AGENT_WORKER-Thread-17 INFO:Local step 176000, global step 2808891: loss 0.4624
[2019-04-01 18:27:15,777] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 176000, global step 2808891: learning rate 0.0010
[2019-04-01 18:27:16,001] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:27:16,002] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9757
[2019-04-01 18:27:16,015] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.3, 77.0, 0.0, 0.0, 26.0, 24.65609921729061, 6.939474908608982, 0.0, 1.0, 43.38505304762081], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3295200.0000, 
sim time next is 3295800.0000, 
raw observation next is [-8.45, 77.0, 0.0, 0.0, 26.0, 24.65419444001623, 6.941096472384622, 0.0, 1.0, 43.34622340905378], 
processed observation next is [1.0, 0.13043478260869565, 0.2285318559556787, 0.77, 0.0, 0.0, 1.0, 0.8077420628594615, 0.06941096472384622, 0.0, 1.0, 0.30961588149324126], 
reward next is 0.6904, 
noisyNet noise sample is [array([0.79848534], dtype=float32), 0.012288694]. 
=============================================
[2019-04-01 18:27:16,402] A3C_AGENT_WORKER-Thread-14 INFO:Local step 175500, global step 2809203: loss 0.6988
[2019-04-01 18:27:16,412] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 175500, global step 2809204: learning rate 0.0010
[2019-04-01 18:27:16,519] A3C_AGENT_WORKER-Thread-19 INFO:Local step 175500, global step 2809265: loss 0.7250
[2019-04-01 18:27:16,520] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 175500, global step 2809266: learning rate 0.0010
[2019-04-01 18:27:17,576] A3C_AGENT_WORKER-Thread-16 INFO:Local step 175500, global step 2809721: loss 0.3821
[2019-04-01 18:27:17,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 175500, global step 2809721: learning rate 0.0010
[2019-04-01 18:27:17,740] A3C_AGENT_WORKER-Thread-13 INFO:Local step 175500, global step 2809800: loss 0.4033
[2019-04-01 18:27:17,741] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 175500, global step 2809800: learning rate 0.0010
[2019-04-01 18:27:17,906] A3C_AGENT_WORKER-Thread-9 INFO:Local step 175500, global step 2809873: loss 0.3284
[2019-04-01 18:27:17,907] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 175500, global step 2809873: learning rate 0.0010
[2019-04-01 18:27:17,985] A3C_AGENT_WORKER-Thread-4 INFO:Local step 176000, global step 2809905: loss 0.8977
[2019-04-01 18:27:17,985] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 176000, global step 2809905: learning rate 0.0010
[2019-04-01 18:27:18,868] A3C_AGENT_WORKER-Thread-3 INFO:Local step 175500, global step 2810410: loss 0.9371
[2019-04-01 18:27:18,872] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 175500, global step 2810410: learning rate 0.0010
[2019-04-01 18:27:19,311] A3C_AGENT_WORKER-Thread-12 INFO:Local step 175500, global step 2810627: loss 1.2543
[2019-04-01 18:27:19,314] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 175500, global step 2810630: learning rate 0.0010
[2019-04-01 18:27:19,736] A3C_AGENT_WORKER-Thread-10 INFO:Local step 175500, global step 2810837: loss 0.3172
[2019-04-01 18:27:19,737] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 175500, global step 2810837: learning rate 0.0010
[2019-04-01 18:27:19,846] A3C_AGENT_WORKER-Thread-5 INFO:Local step 175500, global step 2810891: loss 0.4831
[2019-04-01 18:27:19,849] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 175500, global step 2810892: learning rate 0.0010
[2019-04-01 18:27:21,191] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.9301591e-18 6.2058917e-26 2.5433376e-20 1.8307273e-01 1.3571781e-25
 8.1692725e-01 0.0000000e+00], sum to 1.0000
[2019-04-01 18:27:21,192] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5605
[2019-04-01 18:27:21,218] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 50.0, 102.3333333333333, 693.3333333333334, 26.0, 26.53636266158577, 12.72841367720258, 1.0, 1.0, 16.95972677196882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3405000.0000, 
sim time next is 3405600.0000, 
raw observation next is [2.0, 48.0, 104.0, 711.0, 26.0, 26.58570781157299, 13.15909649436604, 1.0, 1.0, 15.85861073349175], 
processed observation next is [1.0, 0.43478260869565216, 0.518005540166205, 0.48, 0.3466666666666667, 0.7856353591160221, 1.0, 1.0836725445104274, 0.1315909649436604, 1.0, 1.0, 0.11327579095351249], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5369152], dtype=float32), -0.6170127]. 
=============================================
[2019-04-01 18:27:21,272] A3C_AGENT_WORKER-Thread-15 INFO:Local step 175500, global step 2811642: loss 3.3376
[2019-04-01 18:27:21,273] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 175500, global step 2811643: learning rate 0.0010
[2019-04-01 18:27:21,620] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.02980072e-26 1.92757152e-31 1.02071045e-25 1.97131037e-17
 3.49372197e-25 1.00000000e+00 6.64393730e-20], sum to 1.0000
[2019-04-01 18:27:21,620] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2270
[2019-04-01 18:27:21,637] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8333333333333334, 69.16666666666667, 95.33333333333334, 579.6666666666667, 26.0, 25.55460036612422, 9.714381610074001, 1.0, 1.0, 20.73034676095907], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3489000.0000, 
sim time next is 3489600.0000, 
raw observation next is [-0.6666666666666667, 67.33333333333334, 97.16666666666666, 624.8333333333334, 26.0, 25.63245552268863, 9.904970937490866, 1.0, 1.0, 20.48040152599054], 
processed observation next is [1.0, 0.391304347826087, 0.44413665743305636, 0.6733333333333335, 0.32388888888888884, 0.6904235727440148, 1.0, 0.9474936460983757, 0.09904970937490866, 1.0, 1.0, 0.14628858232850386], 
reward next is 0.8537, 
noisyNet noise sample is [array([1.7655646], dtype=float32), -0.63414806]. 
=============================================
[2019-04-01 18:27:28,308] A3C_AGENT_WORKER-Thread-18 INFO:Local step 176500, global step 2815402: loss 0.4824
[2019-04-01 18:27:28,310] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 176500, global step 2815402: learning rate 0.0010
[2019-04-01 18:27:29,433] A3C_AGENT_WORKER-Thread-20 INFO:Local step 176000, global step 2815956: loss 0.5698
[2019-04-01 18:27:29,445] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 176000, global step 2815957: learning rate 0.0010
[2019-04-01 18:27:29,853] A3C_AGENT_WORKER-Thread-2 INFO:Local step 176000, global step 2816168: loss 0.7924
[2019-04-01 18:27:29,854] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 176000, global step 2816168: learning rate 0.0010
[2019-04-01 18:27:30,097] A3C_AGENT_WORKER-Thread-11 INFO:Local step 176000, global step 2816295: loss 0.5885
[2019-04-01 18:27:30,100] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 176000, global step 2816296: learning rate 0.0010
[2019-04-01 18:27:30,286] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.3118288e-31], sum to 1.0000
[2019-04-01 18:27:30,289] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2998
[2019-04-01 18:27:30,302] A3C_AGENT_WORKER-Thread-17 INFO:Local step 176500, global step 2816395: loss 0.0420
[2019-04-01 18:27:30,304] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.333333333333334, 26.33333333333334, 0.0, 0.0, 26.0, 25.51915893267072, 7.725463204789459, 0.0, 1.0, 27.43054891203805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3652800.0000, 
sim time next is 3653400.0000, 
raw observation next is [9.166666666666666, 26.66666666666666, 0.0, 0.0, 26.0, 25.49418684880364, 7.739553924718922, 0.0, 1.0, 28.27279348821509], 
processed observation next is [0.0, 0.2608695652173913, 0.7165281625115422, 0.2666666666666666, 0.0, 0.0, 1.0, 0.9277409784005199, 0.07739553924718921, 0.0, 1.0, 0.20194852491582208], 
reward next is 0.7981, 
noisyNet noise sample is [array([0.57177824], dtype=float32), -2.169882]. 
=============================================
[2019-04-01 18:27:30,308] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 176500, global step 2816397: learning rate 0.0010
[2019-04-01 18:27:31,480] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.293038e-36
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:27:31,480] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2796
[2019-04-01 18:27:31,524] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.333333333333333, 66.66666666666666, 101.8333333333333, 689.6666666666666, 26.0, 25.93055513659114, 11.33148312859574, 0.0, 1.0, 31.47923589507295], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3577200.0000, 
sim time next is 3577800.0000, 
raw observation next is [-5.166666666666667, 65.83333333333334, 103.6666666666667, 703.3333333333334, 26.0, 25.8778362927853, 11.15974100466191, 0.0, 1.0, 29.5284505702314], 
processed observation next is [0.0, 0.391304347826087, 0.31948291782086796, 0.6583333333333334, 0.34555555555555567, 0.7771639042357275, 1.0, 0.9825480418264715, 0.1115974100466191, 0.0, 1.0, 0.21091750407308144], 
reward next is 0.7891, 
noisyNet noise sample is [array([0.42403686], dtype=float32), 0.1715008]. 
=============================================
[2019-04-01 18:27:31,871] A3C_AGENT_WORKER-Thread-19 INFO:Local step 176000, global step 2817185: loss 0.3185
[2019-04-01 18:27:31,879] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 176000, global step 2817187: learning rate 0.0010
[2019-04-01 18:27:32,082] A3C_AGENT_WORKER-Thread-4 INFO:Local step 176500, global step 2817310: loss 2.1670
[2019-04-01 18:27:32,084] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 176500, global step 2817311: learning rate 0.0010
[2019-04-01 18:27:32,342] A3C_AGENT_WORKER-Thread-14 INFO:Local step 176000, global step 2817452: loss 0.4292
[2019-04-01 18:27:32,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 176000, global step 2817452: learning rate 0.0010
[2019-04-01 18:27:32,871] A3C_AGENT_WORKER-Thread-16 INFO:Local step 176000, global step 2817740: loss 0.5263
[2019-04-01 18:27:32,872] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 176000, global step 2817740: learning rate 0.0010
[2019-04-01 18:27:32,899] A3C_AGENT_WORKER-Thread-13 INFO:Local step 176000, global step 2817756: loss 0.4368
[2019-04-01 18:27:32,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 176000, global step 2817756: learning rate 0.0010
[2019-04-01 18:27:33,615] A3C_AGENT_WORKER-Thread-9 INFO:Local step 176000, global step 2818126: loss 0.5566
[2019-04-01 18:27:33,616] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 176000, global step 2818127: learning rate 0.0010
[2019-04-01 18:27:34,351] A3C_AGENT_WORKER-Thread-12 INFO:Local step 176000, global step 2818523: loss 0.3102
[2019-04-01 18:27:34,352] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 176000, global step 2818525: learning rate 0.0010
[2019-04-01 18:27:34,522] A3C_AGENT_WORKER-Thread-3 INFO:Local step 176000, global step 2818630: loss 0.2893
[2019-04-01 18:27:34,524] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 176000, global step 2818630: learning rate 0.0010
[2019-04-01 18:27:35,061] A3C_AGENT_WORKER-Thread-5 INFO:Local step 176000, global step 2818956: loss 0.4854
[2019-04-01 18:27:35,062] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 176000, global step 2818956: learning rate 0.0010
[2019-04-01 18:27:35,134] A3C_AGENT_WORKER-Thread-10 INFO:Local step 176000, global step 2818990: loss 0.4903
[2019-04-01 18:27:35,135] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 176000, global step 2818990: learning rate 0.0010
[2019-04-01 18:27:36,567] A3C_AGENT_WORKER-Thread-15 INFO:Local step 176000, global step 2819819: loss 0.2025
[2019-04-01 18:27:36,567] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 176000, global step 2819819: learning rate 0.0010
[2019-04-01 18:27:38,298] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.6094054e-07 1.1815638e-07 1.2470889e-07 9.9999416e-01 4.8778661e-06
 1.8498970e-10 7.0159625e-09], sum to 1.0000
[2019-04-01 18:27:38,300] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1000
[2019-04-01 18:27:38,310] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6666666666666667, 60.00000000000001, 115.6666666666667, 819.8333333333334, 26.0, 26.14019387652997, 12.90843387943437, 1.0, 1.0, 7.068657324041333], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3763200.0000, 
sim time next is 3763800.0000, 
raw observation next is [-0.5, 60.0, 115.0, 818.0, 26.0, 26.27217628762829, 13.46892002126067, 1.0, 1.0, 7.185360459965895], 
processed observation next is [1.0, 0.5652173913043478, 0.44875346260387816, 0.6, 0.38333333333333336, 0.9038674033149171, 1.0, 1.0388823268040415, 0.1346892002126067, 1.0, 1.0, 0.051324003285470675], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.4762676], dtype=float32), -0.06680254]. 
=============================================
[2019-04-01 18:27:40,091] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0637341e-34 0.0000000e+00 0.0000000e+00 5.3046143e-34 1.6643235e-22
 1.0000000e+00 5.9687690e-23], sum to 1.0000
[2019-04-01 18:27:40,091] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9710
[2019-04-01 18:27:40,134] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 37.5, 0.0, 0.0, 26.0, 25.44666063283586, 7.655885721860322, 1.0, 1.0, 43.46117356131369], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4087800.0000, 
sim time next is 4088400.0000, 
raw observation next is [-4.333333333333334, 36.33333333333333, 15.33333333333333, 78.16666666666664, 26.0, 25.5648678664182, 7.798855570367439, 1.0, 1.0, 38.10526287495188], 
processed observation next is [1.0, 0.30434782608695654, 0.3425669436749769, 0.3633333333333333, 0.0511111111111111, 0.08637200736648248, 1.0, 0.9378382666311715, 0.0779885557036744, 1.0, 1.0, 0.27218044910679917], 
reward next is 0.7278, 
noisyNet noise sample is [array([-2.8201833], dtype=float32), -0.9663764]. 
=============================================
[2019-04-01 18:27:43,184] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1865977e-28 2.1130632e-37 3.2753486e-30 2.8104320e-12 1.6707538e-23
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:27:43,185] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8332
[2019-04-01 18:27:43,248] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.333333333333334, 73.0, 92.66666666666667, 485.8333333333333, 26.0, 25.71612101601286, 9.586801655757546, 1.0, 1.0, 24.55577005962171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3832800.0000, 
sim time next is 3833400.0000, 
raw observation next is [-4.166666666666667, 72.0, 94.33333333333333, 524.6666666666667, 26.0, 25.86369569297483, 9.783573708543235, 1.0, 1.0, 22.9561449307856], 
processed observation next is [1.0, 0.34782608695652173, 0.3471837488457987, 0.72, 0.3144444444444444, 0.5797421731123389, 1.0, 0.9805279561392614, 0.09783573708543235, 1.0, 1.0, 0.1639724637913257], 
reward next is 0.8360, 
noisyNet noise sample is [array([2.5787418], dtype=float32), -0.7112802]. 
=============================================
[2019-04-01 18:27:44,506] A3C_AGENT_WORKER-Thread-20 INFO:Local step 176500, global step 2823928: loss 0.0052
[2019-04-01 18:27:44,508] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 176500, global step 2823930: learning rate 0.0010
[2019-04-01 18:27:44,747] A3C_AGENT_WORKER-Thread-18 INFO:Local step 177000, global step 2824055: loss 0.0366
[2019-04-01 18:27:44,748] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 177000, global step 2824056: learning rate 0.0010
[2019-04-01 18:27:44,751] A3C_AGENT_WORKER-Thread-2 INFO:Local step 176500, global step 2824059: loss 0.0357
[2019-04-01 18:27:44,753] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 176500, global step 2824059: learning rate 0.0010
[2019-04-01 18:27:45,230] A3C_AGENT_WORKER-Thread-11 INFO:Local step 176500, global step 2824288: loss 0.0530
[2019-04-01 18:27:45,231] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 176500, global step 2824288: learning rate 0.0010
[2019-04-01 18:27:46,749] A3C_AGENT_WORKER-Thread-17 INFO:Local step 177000, global step 2825034: loss 0.0055
[2019-04-01 18:27:46,751] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 177000, global step 2825035: learning rate 0.0010
[2019-04-01 18:27:46,964] A3C_AGENT_WORKER-Thread-19 INFO:Local step 176500, global step 2825151: loss -0.2142
[2019-04-01 18:27:46,968] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 176500, global step 2825152: learning rate 0.0010
[2019-04-01 18:27:47,511] A3C_AGENT_WORKER-Thread-14 INFO:Local step 176500, global step 2825411: loss 0.0071
[2019-04-01 18:27:47,513] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 176500, global step 2825412: learning rate 0.0010
[2019-04-01 18:27:47,944] A3C_AGENT_WORKER-Thread-16 INFO:Local step 176500, global step 2825589: loss 0.1913
[2019-04-01 18:27:47,945] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 176500, global step 2825589: learning rate 0.0010
[2019-04-01 18:27:48,240] A3C_AGENT_WORKER-Thread-13 INFO:Local step 176500, global step 2825709: loss 0.0120
[2019-04-01 18:27:48,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 176500, global step 2825709: learning rate 0.0010
[2019-04-01 18:27:48,253] A3C_AGENT_WORKER-Thread-4 INFO:Local step 177000, global step 2825716: loss 0.0148
[2019-04-01 18:27:48,254] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 177000, global step 2825716: learning rate 0.0010
[2019-04-01 18:27:48,591] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:27:48,599] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8508
[2019-04-01 18:27:48,617] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.36722003040222, 7.293911814389086, 0.0, 1.0, 38.78760944811228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4255200.0000, 
sim time next is 4255800.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.35703614842416, 7.287231662413561, 0.0, 1.0, 39.12724067687465], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 1.0, 0.9081480212034515, 0.07287231662413561, 0.0, 1.0, 0.2794802905491046], 
reward next is 0.7205, 
noisyNet noise sample is [array([-1.2445197], dtype=float32), -0.36700842]. 
=============================================
[2019-04-01 18:27:48,634] A3C_AGENT_WORKER-Thread-9 INFO:Local step 176500, global step 2825888: loss 0.4076
[2019-04-01 18:27:48,635] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 176500, global step 2825889: learning rate 0.0010
[2019-04-01 18:27:49,439] A3C_AGENT_WORKER-Thread-12 INFO:Local step 176500, global step 2826274: loss 0.3632
[2019-04-01 18:27:49,441] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 176500, global step 2826275: learning rate 0.0010
[2019-04-01 18:27:49,548] A3C_AGENT_WORKER-Thread-3 INFO:Local step 176500, global step 2826323: loss 0.0973
[2019-04-01 18:27:49,550] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 176500, global step 2826323: learning rate 0.0010
[2019-04-01 18:27:50,285] A3C_AGENT_WORKER-Thread-5 INFO:Local step 176500, global step 2826707: loss 0.0033
[2019-04-01 18:27:50,286] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 176500, global step 2826708: learning rate 0.0010
[2019-04-01 18:27:50,359] A3C_AGENT_WORKER-Thread-10 INFO:Local step 176500, global step 2826748: loss 0.0030
[2019-04-01 18:27:50,361] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 176500, global step 2826749: learning rate 0.0010
[2019-04-01 18:27:51,603] A3C_AGENT_WORKER-Thread-15 INFO:Local step 176500, global step 2827365: loss 0.8607
[2019-04-01 18:27:51,604] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 176500, global step 2827365: learning rate 0.0010
[2019-04-01 18:27:51,893] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0944162e-17 5.4970870e-25 9.8625891e-24 3.0912639e-26 1.3478284e-25
 9.9999976e-01 2.7180383e-07], sum to 1.0000
[2019-04-01 18:27:51,894] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2343
[2019-04-01 18:27:51,902] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.666666666666667, 35.66666666666667, 118.3333333333333, 832.6666666666667, 26.0, 26.44749461343268, 12.82658342828756, 1.0, 1.0, 10.2119221596803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4018200.0000, 
sim time next is 4018800.0000, 
raw observation next is [-5.333333333333333, 34.33333333333334, 118.1666666666667, 836.8333333333334, 26.0, 26.46579474962969, 12.83391956772267, 1.0, 1.0, 9.519662474083558], 
processed observation next is [1.0, 0.5217391304347826, 0.3148661126500462, 0.34333333333333343, 0.393888888888889, 0.9246777163904236, 1.0, 1.066542107089956, 0.1283391956772267, 1.0, 1.0, 0.06799758910059685], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.38893917], dtype=float32), -0.5999831]. 
=============================================
[2019-04-01 18:27:52,603] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:27:52,603] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7796
[2019-04-01 18:27:52,609] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.8, 40.0, 14.0, 0.0, 26.0, 28.24117310613421, 28.16753656716861, 1.0, 1.0, 5.683768930207165], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4382400.0000, 
sim time next is 4383000.0000, 
raw observation next is [12.7, 41.0, 9.0, 0.0, 26.0, 28.22381170440325, 27.96699346009173, 1.0, 1.0, 5.8798831087168], 
processed observation next is [1.0, 0.7391304347826086, 0.8144044321329641, 0.41, 0.03, 0.0, 1.0, 1.3176873863433214, 0.27966993460091727, 1.0, 1.0, 0.04199916506226286], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.16092327], dtype=float32), 0.47281122]. 
=============================================
[2019-04-01 18:27:52,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[56.44384 ]
 [56.69768 ]
 [56.85671 ]
 [56.928864]
 [56.786514]], R is [[55.46401596]
 [54.90937805]
 [54.3602829 ]
 [53.81668091]
 [53.27851486]].
[2019-04-01 18:27:54,474] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4755255e-29 4.6609256e-32 1.6756800e-32 2.7685987e-37 1.1956804e-10
 1.0000000e+00 4.4151230e-21], sum to 1.0000
[2019-04-01 18:27:54,477] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5480
[2019-04-01 18:27:54,500] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.666666666666667, 35.66666666666667, 93.16666666666666, 480.0, 26.0, 27.31842701292241, 19.09276784206848, 1.0, 1.0, 1.414445286898547], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4119600.0000, 
sim time next is 4120200.0000, 
raw observation next is [3.5, 36.0, 93.0, 437.0, 26.0, 27.4331868552168, 15.7218388332553, 1.0, 1.0, 1.382231865888288], 
processed observation next is [1.0, 0.6956521739130435, 0.5595567867036012, 0.36, 0.31, 0.48287292817679556, 1.0, 1.2047409793166857, 0.157218388332553, 1.0, 1.0, 0.009873084756344915], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9424766], dtype=float32), 0.6497872]. 
=============================================
[2019-04-01 18:27:58,209] A3C_AGENT_WORKER-Thread-18 INFO:Local step 177500, global step 2830842: loss 0.3289
[2019-04-01 18:27:58,211] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 177500, global step 2830843: learning rate 0.0010
[2019-04-01 18:28:00,100] A3C_AGENT_WORKER-Thread-2 INFO:Local step 177000, global step 2831938: loss 0.0233
[2019-04-01 18:28:00,101] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 177000, global step 2831938: learning rate 0.0010
[2019-04-01 18:28:00,106] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 4.7653254e-10 1.0000000e+00], sum to 1.0000
[2019-04-01 18:28:00,108] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3355
[2019-04-01 18:28:00,119] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 37.0, 214.0, 669.0, 26.0, 25.11412909922704, 8.865526073657712, 0.0, 1.0, 9.057619360244418], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4195800.0000, 
sim time next is 4196400.0000, 
raw observation next is [2.0, 38.0, 209.5, 572.3333333333334, 26.0, 25.1197267284314, 8.868804978136824, 0.0, 1.0, 9.02489424884428], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.38, 0.6983333333333334, 0.6324125230202579, 1.0, 0.8742466754901999, 0.08868804978136824, 0.0, 1.0, 0.06446353034888772], 
reward next is 0.9355, 
noisyNet noise sample is [array([0.444664], dtype=float32), -0.66576356]. 
=============================================
[2019-04-01 18:28:00,184] A3C_AGENT_WORKER-Thread-17 INFO:Local step 177500, global step 2831984: loss 3.0772
[2019-04-01 18:28:00,185] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 177500, global step 2831984: learning rate 0.0010
[2019-04-01 18:28:00,190] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:28:00,191] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5047
[2019-04-01 18:28:00,212] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.666666666666667, 46.0, 171.5, 28.83333333333333, 26.0, 26.07987925810638, 11.826765960851, 1.0, 1.0, 10.73681030397331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4548000.0000, 
sim time next is 4548600.0000, 
raw observation next is [2.5, 46.5, 153.0, 12.0, 26.0, 25.41791190989301, 10.43036963941991, 1.0, 1.0, 9.90132563238368], 
processed observation next is [1.0, 0.6521739130434783, 0.5318559556786704, 0.465, 0.51, 0.013259668508287293, 1.0, 0.9168445585561444, 0.10430369639419909, 1.0, 1.0, 0.07072375451702628], 
reward next is 0.7571, 
noisyNet noise sample is [array([-0.00049873], dtype=float32), -1.9353169]. 
=============================================
[2019-04-01 18:28:00,327] A3C_AGENT_WORKER-Thread-20 INFO:Local step 177000, global step 2832061: loss 0.0504
[2019-04-01 18:28:00,329] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 177000, global step 2832061: learning rate 0.0010
[2019-04-01 18:28:00,865] A3C_AGENT_WORKER-Thread-11 INFO:Local step 177000, global step 2832360: loss 0.0157
[2019-04-01 18:28:00,866] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 177000, global step 2832360: learning rate 0.0010
[2019-04-01 18:28:02,124] A3C_AGENT_WORKER-Thread-4 INFO:Local step 177500, global step 2833061: loss 3.0360
[2019-04-01 18:28:02,126] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 177500, global step 2833062: learning rate 0.0010
[2019-04-01 18:28:02,319] A3C_AGENT_WORKER-Thread-19 INFO:Local step 177000, global step 2833179: loss 0.0079
[2019-04-01 18:28:02,320] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 177000, global step 2833180: learning rate 0.0010
[2019-04-01 18:28:03,008] A3C_AGENT_WORKER-Thread-14 INFO:Local step 177000, global step 2833613: loss 0.0134
[2019-04-01 18:28:03,017] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 177000, global step 2833613: learning rate 0.0010
[2019-04-01 18:28:03,220] A3C_AGENT_WORKER-Thread-16 INFO:Local step 177000, global step 2833742: loss 0.0089
[2019-04-01 18:28:03,223] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 177000, global step 2833742: learning rate 0.0010
[2019-04-01 18:28:03,549] A3C_AGENT_WORKER-Thread-13 INFO:Local step 177000, global step 2833961: loss 0.0059
[2019-04-01 18:28:03,551] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 177000, global step 2833961: learning rate 0.0010
[2019-04-01 18:28:03,793] A3C_AGENT_WORKER-Thread-9 INFO:Local step 177000, global step 2834102: loss 0.0047
[2019-04-01 18:28:03,806] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 177000, global step 2834108: learning rate 0.0010
[2019-04-01 18:28:04,490] A3C_AGENT_WORKER-Thread-12 INFO:Local step 177000, global step 2834468: loss 0.0393
[2019-04-01 18:28:04,490] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 177000, global step 2834468: learning rate 0.0010
[2019-04-01 18:28:04,690] A3C_AGENT_WORKER-Thread-3 INFO:Local step 177000, global step 2834560: loss 0.0245
[2019-04-01 18:28:04,692] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 177000, global step 2834560: learning rate 0.0010
[2019-04-01 18:28:05,230] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 3.026604e-22], sum to 1.0000
[2019-04-01 18:28:05,232] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6618
[2019-04-01 18:28:05,244] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.833333333333333, 38.16666666666667, 129.3333333333333, 542.0, 26.0, 25.3527309973422, 9.438671613407566, 0.0, 1.0, 9.817634557027034], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4204200.0000, 
sim time next is 4204800.0000, 
raw observation next is [3.0, 37.0, 114.0, 544.0, 26.0, 25.35201496910447, 9.488872187727452, 0.0, 1.0, 9.136974268406872], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.37, 0.38, 0.6011049723756906, 1.0, 0.9074307098720672, 0.09488872187727453, 0.0, 1.0, 0.06526410191719194], 
reward next is 0.9347, 
noisyNet noise sample is [array([-2.0714436], dtype=float32), 0.72876334]. 
=============================================
[2019-04-01 18:28:05,517] A3C_AGENT_WORKER-Thread-5 INFO:Local step 177000, global step 2835021: loss 0.0061
[2019-04-01 18:28:05,518] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 177000, global step 2835021: learning rate 0.0010
[2019-04-01 18:28:05,737] A3C_AGENT_WORKER-Thread-10 INFO:Local step 177000, global step 2835143: loss 0.0262
[2019-04-01 18:28:05,739] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 177000, global step 2835144: learning rate 0.0010
[2019-04-01 18:28:06,754] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:28:06,757] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9050
[2019-04-01 18:28:06,793] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.333333333333333, 53.33333333333334, 170.0, 118.0, 26.0, 25.60313800894699, 8.4180650950734, 0.0, 1.0, 21.30279892368785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4267200.0000, 
sim time next is 4267800.0000, 
raw observation next is [3.5, 53.5, 182.0, 131.0, 26.0, 25.61720896951617, 8.43966382288641, 0.0, 1.0, 20.07684016402432], 
processed observation next is [0.0, 0.391304347826087, 0.5595567867036012, 0.535, 0.6066666666666667, 0.14475138121546963, 1.0, 0.9453155670737386, 0.08439663822886409, 0.0, 1.0, 0.14340600117160227], 
reward next is 0.8566, 
noisyNet noise sample is [array([-0.28783908], dtype=float32), 0.43152854]. 
=============================================
[2019-04-01 18:28:07,236] A3C_AGENT_WORKER-Thread-15 INFO:Local step 177000, global step 2836021: loss 0.0862
[2019-04-01 18:28:07,237] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 177000, global step 2836023: learning rate 0.0010
[2019-04-01 18:28:08,464] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.2617382e-26], sum to 1.0000
[2019-04-01 18:28:08,464] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1248
[2019-04-01 18:28:08,477] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.916666666666666, 67.83333333333334, 0.0, 0.0, 26.0, 25.78992109008638, 11.244893739832, 0.0, 1.0, 18.46353558145228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4423800.0000, 
sim time next is 4424400.0000, 
raw observation next is [3.8, 68.0, 0.0, 0.0, 26.0, 25.69285889141393, 10.79856233250819, 0.0, 1.0, 17.69149856245102], 
processed observation next is [1.0, 0.21739130434782608, 0.5678670360110805, 0.68, 0.0, 0.0, 1.0, 0.9561226987734186, 0.10798562332508191, 0.0, 1.0, 0.12636784687465014], 
reward next is 0.8736, 
noisyNet noise sample is [array([-0.41935366], dtype=float32), -1.3402753]. 
=============================================
[2019-04-01 18:28:13,098] A3C_AGENT_WORKER-Thread-18 INFO:Local step 178000, global step 2839382: loss 0.6502
[2019-04-01 18:28:13,099] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 178000, global step 2839382: learning rate 0.0010
[2019-04-01 18:28:14,406] A3C_AGENT_WORKER-Thread-2 INFO:Local step 177500, global step 2840012: loss 1.1320
[2019-04-01 18:28:14,407] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 177500, global step 2840012: learning rate 0.0010
[2019-04-01 18:28:14,639] A3C_AGENT_WORKER-Thread-20 INFO:Local step 177500, global step 2840127: loss 0.3490
[2019-04-01 18:28:14,641] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 177500, global step 2840127: learning rate 0.0010
[2019-04-01 18:28:14,977] A3C_AGENT_WORKER-Thread-17 INFO:Local step 178000, global step 2840292: loss 1.0307
[2019-04-01 18:28:14,981] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 178000, global step 2840292: learning rate 0.0010
[2019-04-01 18:28:15,138] A3C_AGENT_WORKER-Thread-11 INFO:Local step 177500, global step 2840369: loss 1.1731
[2019-04-01 18:28:15,139] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 177500, global step 2840369: learning rate 0.0010
[2019-04-01 18:28:15,568] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:28:15,572] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1868
[2019-04-01 18:28:15,613] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4666666666666667, 72.83333333333333, 0.0, 0.0, 26.0, 25.39350189137114, 10.26865126414642, 0.0, 1.0, 42.69796404719815], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4492200.0000, 
sim time next is 4492800.0000, 
raw observation next is [-0.5, 73.0, 0.0, 0.0, 26.0, 25.45923988571442, 10.23176655572875, 0.0, 1.0, 41.74643323681957], 
processed observation next is [1.0, 0.0, 0.44875346260387816, 0.73, 0.0, 0.0, 1.0, 0.9227485551020601, 0.10231766555728751, 0.0, 1.0, 0.2981888088344255], 
reward next is 0.7018, 
noisyNet noise sample is [array([1.4259055], dtype=float32), 0.45588347]. 
=============================================
[2019-04-01 18:28:17,103] A3C_AGENT_WORKER-Thread-19 INFO:Local step 177500, global step 2841362: loss 0.9552
[2019-04-01 18:28:17,105] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 177500, global step 2841363: learning rate 0.0010
[2019-04-01 18:28:17,287] A3C_AGENT_WORKER-Thread-14 INFO:Local step 177500, global step 2841458: loss 0.7782
[2019-04-01 18:28:17,290] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 177500, global step 2841460: learning rate 0.0010
[2019-04-01 18:28:17,462] A3C_AGENT_WORKER-Thread-4 INFO:Local step 178000, global step 2841546: loss 0.1583
[2019-04-01 18:28:17,464] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 178000, global step 2841549: learning rate 0.0010
[2019-04-01 18:28:17,606] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:28:17,609] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2689
[2019-04-01 18:28:17,622] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.5193353999358, 9.617523796725672, 1.0, 1.0, 9.428729308309286], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4559400.0000, 
sim time next is 4560000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.40261415915725, 9.212285584603784, 1.0, 1.0, 10.05561179154119], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.9146591655938927, 0.09212285584603784, 1.0, 1.0, 0.07182579851100851], 
reward next is 0.9282, 
noisyNet noise sample is [array([-0.38660702], dtype=float32), -0.41229668]. 
=============================================
[2019-04-01 18:28:17,629] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[27.89463 ]
 [28.277521]
 [28.700981]
 [29.147041]
 [29.751608]], R is [[28.12313843]
 [28.77455902]
 [29.42270279]
 [29.99597168]
 [30.5705452 ]].
[2019-04-01 18:28:17,649] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0425935e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.4435135e-25
 1.0000000e+00 3.9032469e-30], sum to 1.0000
[2019-04-01 18:28:17,649] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5081
[2019-04-01 18:28:17,653] A3C_AGENT_WORKER-Thread-16 INFO:Local step 177500, global step 2841645: loss 2.1045
[2019-04-01 18:28:17,654] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 177500, global step 2841645: learning rate 0.0010
[2019-04-01 18:28:17,657] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.21591584347392, 8.90445990486912, 1.0, 1.0, 10.47368906700552], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4561200.0000, 
sim time next is 4561800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.16150347429053, 8.761967398778829, 1.0, 1.0, 11.43586761658704], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.8802147820415043, 0.08761967398778829, 1.0, 1.0, 0.08168476868990743], 
reward next is 0.9183, 
noisyNet noise sample is [array([0.8431648], dtype=float32), 0.11978081]. 
=============================================
[2019-04-01 18:28:18,094] A3C_AGENT_WORKER-Thread-13 INFO:Local step 177500, global step 2841861: loss 2.4748
[2019-04-01 18:28:18,097] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 177500, global step 2841861: learning rate 0.0010
[2019-04-01 18:28:18,554] A3C_AGENT_WORKER-Thread-9 INFO:Local step 177500, global step 2842096: loss 1.5571
[2019-04-01 18:28:18,555] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 177500, global step 2842096: learning rate 0.0010
[2019-04-01 18:28:19,084] A3C_AGENT_WORKER-Thread-3 INFO:Local step 177500, global step 2842369: loss 2.9889
[2019-04-01 18:28:19,085] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 177500, global step 2842370: learning rate 0.0010
[2019-04-01 18:28:19,621] A3C_AGENT_WORKER-Thread-12 INFO:Local step 177500, global step 2842675: loss 4.2221
[2019-04-01 18:28:19,621] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 177500, global step 2842675: learning rate 0.0010
[2019-04-01 18:28:19,964] A3C_AGENT_WORKER-Thread-5 INFO:Local step 177500, global step 2842867: loss 3.7653
[2019-04-01 18:28:19,964] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 177500, global step 2842867: learning rate 0.0010
[2019-04-01 18:28:20,270] A3C_AGENT_WORKER-Thread-10 INFO:Local step 177500, global step 2843037: loss 5.0430
[2019-04-01 18:28:20,270] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 177500, global step 2843037: learning rate 0.0010
[2019-04-01 18:28:21,649] A3C_AGENT_WORKER-Thread-15 INFO:Local step 177500, global step 2843748: loss 4.2033
[2019-04-01 18:28:21,650] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 177500, global step 2843749: learning rate 0.0010
[2019-04-01 18:28:26,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:26,886] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:26,924] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run22
[2019-04-01 18:28:27,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:27,991] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:27,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run22
[2019-04-01 18:28:29,685] A3C_AGENT_WORKER-Thread-2 INFO:Local step 178000, global step 2847802: loss 0.2340
[2019-04-01 18:28:29,686] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 178000, global step 2847802: learning rate 0.0010
[2019-04-01 18:28:29,942] A3C_AGENT_WORKER-Thread-20 INFO:Local step 178000, global step 2847928: loss 0.3139
[2019-04-01 18:28:29,945] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 178000, global step 2847928: learning rate 0.0010
[2019-04-01 18:28:30,231] A3C_AGENT_WORKER-Thread-11 INFO:Local step 178000, global step 2848071: loss 0.0353
[2019-04-01 18:28:30,233] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 178000, global step 2848071: learning rate 0.0010
[2019-04-01 18:28:31,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:31,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:31,198] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run22
[2019-04-01 18:28:31,591] A3C_AGENT_WORKER-Thread-19 INFO:Local step 178000, global step 2848586: loss 0.0076
[2019-04-01 18:28:31,593] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 178000, global step 2848586: learning rate 0.0010
[2019-04-01 18:28:31,605] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:28:31,606] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2309
[2019-04-01 18:28:31,615] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 36.5, 87.0, 608.3333333333334, 26.0, 25.21979672213538, 9.59484424687465, 0.0, 1.0, 3.878892902945058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4810200.0000, 
sim time next is 4810800.0000, 
raw observation next is [3.0, 36.0, 84.5, 578.6666666666666, 26.0, 25.21965429920726, 9.595672734693357, 0.0, 1.0, 7.360617223286725], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.36, 0.2816666666666667, 0.6394106813996316, 1.0, 0.8885220427438943, 0.09595672734693357, 0.0, 1.0, 0.05257583730919089], 
reward next is 0.9474, 
noisyNet noise sample is [array([-0.9865175], dtype=float32), -1.8576758]. 
=============================================
[2019-04-01 18:28:32,156] A3C_AGENT_WORKER-Thread-14 INFO:Local step 178000, global step 2848795: loss 0.0579
[2019-04-01 18:28:32,160] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 178000, global step 2848797: learning rate 0.0010
[2019-04-01 18:28:32,836] A3C_AGENT_WORKER-Thread-16 INFO:Local step 178000, global step 2849133: loss 0.0514
[2019-04-01 18:28:32,837] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 178000, global step 2849134: learning rate 0.0010
[2019-04-01 18:28:33,439] A3C_AGENT_WORKER-Thread-13 INFO:Local step 178000, global step 2849411: loss 0.0448
[2019-04-01 18:28:33,440] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 178000, global step 2849411: learning rate 0.0010
[2019-04-01 18:28:33,579] A3C_AGENT_WORKER-Thread-9 INFO:Local step 178000, global step 2849476: loss 0.0062
[2019-04-01 18:28:33,581] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 178000, global step 2849477: learning rate 0.0010
[2019-04-01 18:28:34,195] A3C_AGENT_WORKER-Thread-3 INFO:Local step 178000, global step 2849785: loss 0.0246
[2019-04-01 18:28:34,196] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 178000, global step 2849786: learning rate 0.0010
[2019-04-01 18:28:34,371] A3C_AGENT_WORKER-Thread-5 INFO:Local step 178000, global step 2849886: loss 0.0317
[2019-04-01 18:28:34,382] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 178000, global step 2849891: learning rate 0.0010
[2019-04-01 18:28:34,682] A3C_AGENT_WORKER-Thread-12 INFO:Local step 178000, global step 2850052: loss 0.0822
[2019-04-01 18:28:34,683] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 178000, global step 2850052: learning rate 0.0010
[2019-04-01 18:28:35,079] A3C_AGENT_WORKER-Thread-10 INFO:Local step 178000, global step 2850228: loss 0.0598
[2019-04-01 18:28:35,080] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 178000, global step 2850228: learning rate 0.0010
[2019-04-01 18:28:36,230] A3C_AGENT_WORKER-Thread-15 INFO:Local step 178000, global step 2850719: loss 0.0257
[2019-04-01 18:28:36,230] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 178000, global step 2850719: learning rate 0.0010
[2019-04-01 18:28:42,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:42,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:42,454] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run22
[2019-04-01 18:28:42,534] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:42,534] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:42,566] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run22
[2019-04-01 18:28:42,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:42,830] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:42,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run22
[2019-04-01 18:28:44,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:44,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:44,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run22
[2019-04-01 18:28:44,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:44,322] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:44,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run22
[2019-04-01 18:28:45,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:45,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:45,364] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run22
[2019-04-01 18:28:45,910] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:45,910] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:45,916] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run22
[2019-04-01 18:28:45,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:45,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:45,959] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run22
[2019-04-01 18:28:46,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:46,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:46,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run22
[2019-04-01 18:28:46,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:46,574] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:46,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run22
[2019-04-01 18:28:46,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:46,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:46,643] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run22
[2019-04-01 18:28:46,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:46,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:46,952] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run22
[2019-04-01 18:28:47,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:28:47,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:28:47,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run22
[2019-04-01 18:28:54,837] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:28:54,838] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0923
[2019-04-01 18:28:54,883] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.416666666666667, 82.66666666666667, 28.66666666666666, 0.0, 26.0, 24.62330758056298, 6.974910784442943, 0.0, 1.0, 32.27639897667802], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 58200.0000, 
sim time next is 58800.0000, 
raw observation next is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 26.0, 24.64926585584534, 6.960151228304355, 0.0, 1.0, 27.24225466812769], 
processed observation next is [0.0, 0.6956521739130435, 0.6352723915050786, 0.8333333333333335, 0.07777777777777777, 0.0, 1.0, 0.807037979406477, 0.06960151228304355, 0.0, 1.0, 0.19458753334376921], 
reward next is 0.8054, 
noisyNet noise sample is [array([1.3526253], dtype=float32), -1.6461637]. 
=============================================
[2019-04-01 18:28:55,174] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:28:55,175] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6457
[2019-04-01 18:28:55,206] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.699999999999999, 93.0, 0.0, 0.0, 26.0, 21.30095065384896, 11.73163789190723, 0.0, 1.0, 39.76458577349467], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 24000.0000, 
sim time next is 24600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.31962531434127, 11.60323359092858, 0.0, 1.0, 39.76471870200786], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 1.0, 0.3313750449058957, 0.1160323359092858, 0.0, 1.0, 0.2840337050143419], 
reward next is 0.7160, 
noisyNet noise sample is [array([-1.382978], dtype=float32), 0.29125375]. 
=============================================
[2019-04-01 18:28:56,826] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:28:56,826] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1314
[2019-04-01 18:28:56,906] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 49.0, 0.0, 26.0, 23.58106053666539, 5.600336310420571, 0.0, 1.0, 55.60594447759507], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 36000.0000, 
sim time next is 36600.0000, 
raw observation next is [7.7, 93.0, 52.66666666666667, 0.0, 26.0, 23.661361928885, 5.584743564539576, 0.0, 1.0, 55.81063131037418], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.17555555555555558, 0.0, 1.0, 0.6659088469835714, 0.055847435645395765, 0.0, 1.0, 0.3986473665026727], 
reward next is 0.6014, 
noisyNet noise sample is [array([-0.3484959], dtype=float32), 1.0450042]. 
=============================================
[2019-04-01 18:28:59,228] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:28:59,232] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4988
[2019-04-01 18:28:59,277] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.766666666666667, 88.0, 0.0, 0.0, 26.0, 24.58619189830052, 7.022516844801345, 0.0, 1.0, 41.32102990206715], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 63600.0000, 
sim time next is 64200.0000, 
raw observation next is [4.583333333333334, 88.5, 0.0, 0.0, 26.0, 24.63207820609738, 7.063238376539125, 0.0, 1.0, 38.21997581446006], 
processed observation next is [0.0, 0.7391304347826086, 0.5895660203139428, 0.885, 0.0, 0.0, 1.0, 0.8045826008710542, 0.07063238376539126, 0.0, 1.0, 0.27299982724614325], 
reward next is 0.7270, 
noisyNet noise sample is [array([-0.1284446], dtype=float32), -0.236061]. 
=============================================
[2019-04-01 18:28:59,339] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0309937e-34 2.7743288e-37 1.7387191e-33 3.4454111e-34 5.1319740e-30
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:28:59,339] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8603
[2019-04-01 18:28:59,378] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 42.0, 72.0, 501.3333333333333, 26.0, 25.94369641369802, 11.86166776547009, 1.0, 1.0, 35.597163106982], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 314400.0000, 
sim time next is 315000.0000, 
raw observation next is [-9.5, 42.0, 70.0, 477.0, 26.0, 26.28092752048856, 12.45953731379609, 1.0, 1.0, 26.4284024486578], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.23333333333333334, 0.5270718232044199, 1.0, 1.0401325029269373, 0.1245953731379609, 1.0, 1.0, 0.18877430320469857], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.9892853], dtype=float32), 1.0574708]. 
=============================================
[2019-04-01 18:28:59,380] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[53.592335]
 [53.019035]
 [53.397747]
 [53.043144]
 [52.54678 ]], R is [[53.71191025]
 [53.17586136]
 [53.05106354]
 [52.65065765]
 [52.12415314]].
[2019-04-01 18:29:10,744] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:29:10,744] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7076
[2019-04-01 18:29:10,801] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 65.0, 141.0, 0.0, 26.0, 25.33397026061224, 6.849343766709308, 1.0, 1.0, 31.98745592084951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 216000.0000, 
sim time next is 216600.0000, 
raw observation next is [-4.916666666666667, 65.0, 137.0, 0.0, 26.0, 25.28423374438492, 6.853756949495843, 1.0, 1.0, 31.53724575165226], 
processed observation next is [1.0, 0.5217391304347826, 0.32640812557710064, 0.65, 0.45666666666666667, 0.0, 1.0, 0.8977476777692743, 0.06853756949495843, 1.0, 1.0, 0.22526604108323042], 
reward next is 0.7747, 
noisyNet noise sample is [array([-0.20498256], dtype=float32), -0.53264964]. 
=============================================
[2019-04-01 18:29:14,159] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:29:14,159] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8554
[2019-04-01 18:29:14,223] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.8695177630639, 7.246127407360156, 1.0, 1.0, 55.35557224512132], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 241800.0000, 
sim time next is 242400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.95297529865974, 7.374199952461105, 0.0, 1.0, 47.48560049910613], 
processed observation next is [1.0, 0.8260869565217391, 0.368421052631579, 0.65, 0.0, 0.0, 1.0, 0.8504250426656773, 0.07374199952461105, 0.0, 1.0, 0.3391828607079009], 
reward next is 0.6608, 
noisyNet noise sample is [array([1.9279677], dtype=float32), -0.6891421]. 
=============================================
[2019-04-01 18:29:14,616] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:29:14,616] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8976
[2019-04-01 18:29:14,646] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.23333333333333, 47.33333333333334, 86.83333333333334, 741.3333333333334, 26.0, 25.80714392577055, 9.737173946035911, 1.0, 1.0, 38.11226141359239], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 303600.0000, 
sim time next is 304200.0000, 
raw observation next is [-10.05, 46.5, 83.0, 758.0, 26.0, 25.95965441651768, 9.868795907016322, 1.0, 1.0, 29.32787579390218], 
processed observation next is [1.0, 0.5217391304347826, 0.18421052631578946, 0.465, 0.27666666666666667, 0.8375690607734807, 1.0, 0.9942363452168114, 0.09868795907016321, 1.0, 1.0, 0.20948482709930127], 
reward next is 0.7905, 
noisyNet noise sample is [array([-0.72507906], dtype=float32), 0.1605635]. 
=============================================
[2019-04-01 18:29:17,468] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2860087e-33 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:29:17,468] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3370
[2019-04-01 18:29:17,537] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.33333333333333, 62.00000000000001, 88.33333333333333, 490.5, 26.0, 25.91954159300919, 8.793688340614791, 1.0, 1.0, 33.01747909062719], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 296400.0000, 
sim time next is 297000.0000, 
raw observation next is [-11.15, 61.5, 87.0, 512.0, 26.0, 25.94921607847397, 8.847453284438785, 1.0, 1.0, 31.2024871826648], 
processed observation next is [1.0, 0.43478260869565216, 0.15373961218836565, 0.615, 0.29, 0.5657458563535912, 1.0, 0.99274515406771, 0.08847453284438785, 1.0, 1.0, 0.2228749084476057], 
reward next is 0.7771, 
noisyNet noise sample is [array([-2.088532], dtype=float32), -0.07270524]. 
=============================================
[2019-04-01 18:29:17,546] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[60.868793]
 [61.561268]
 [62.15736 ]
 [62.87663 ]
 [63.968487]], R is [[60.32820129]
 [60.48907852]
 [60.63830948]
 [60.74380493]
 [60.77312088]].
[2019-04-01 18:29:23,206] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.8199856e-24 5.2055867e-34 2.2856170e-25 1.8251032e-12 1.6496288e-28
 1.0000000e+00 2.9593408e-34], sum to 1.0000
[2019-04-01 18:29:23,207] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8793
[2019-04-01 18:29:23,264] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.23333333333333, 82.0, 36.66666666666666, 694.5, 26.0, 25.51119216451427, 7.045414766815616, 1.0, 1.0, 59.2042107089153], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 379200.0000, 
sim time next is 379800.0000, 
raw observation next is [-15.05, 78.0, 39.0, 738.0, 26.0, 25.52462425307006, 7.336495523677563, 1.0, 1.0, 60.40093431973803], 
processed observation next is [1.0, 0.391304347826087, 0.0457063711911357, 0.78, 0.13, 0.8154696132596685, 1.0, 0.9320891790100083, 0.07336495523677562, 1.0, 1.0, 0.4314352451409859], 
reward next is 0.5686, 
noisyNet noise sample is [array([0.20018612], dtype=float32), -1.2858374]. 
=============================================
[2019-04-01 18:29:24,816] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:29:24,816] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1227
[2019-04-01 18:29:24,909] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.61666666666667, 80.5, 8.333333333333332, 192.3333333333333, 26.0, 23.57480119465383, 5.894081849146573, 1.0, 1.0, 83.39884954873565], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 373800.0000, 
sim time next is 374400.0000, 
raw observation next is [-16.7, 81.0, 12.5, 263.5, 26.0, 23.77141824459211, 5.750002285801209, 0.0, 1.0, 82.62117542993352], 
processed observation next is [1.0, 0.34782608695652173, 0.0, 0.81, 0.041666666666666664, 0.29116022099447514, 1.0, 0.6816311777988727, 0.05750002285801209, 0.0, 1.0, 0.5901512530709537], 
reward next is 0.4098, 
noisyNet noise sample is [array([-1.2206078], dtype=float32), -0.46720126]. 
=============================================
[2019-04-01 18:29:26,202] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:29:26,202] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2962
[2019-04-01 18:29:26,231] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.43442163871865, 5.713325100854536, 0.0, 1.0, 45.06044708230839], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 438600.0000, 
sim time next is 439200.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.35630358592486, 5.728369095669092, 0.0, 1.0, 45.12373632249966], 
processed observation next is [1.0, 0.08695652173913043, 0.15235457063711913, 0.55, 0.0, 0.0, 1.0, 0.6223290837035513, 0.05728369095669092, 0.0, 1.0, 0.322312402303569], 
reward next is 0.6777, 
noisyNet noise sample is [array([0.5274235], dtype=float32), -1.7308148]. 
=============================================
[2019-04-01 18:29:29,541] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:29:29,541] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1460
[2019-04-01 18:29:29,562] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.95, 43.5, 0.0, 0.0, 26.0, 22.47899242504971, 7.396804744545904, 0.0, 1.0, 46.36091868527006], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 455400.0000, 
sim time next is 456000.0000, 
raw observation next is [-8.766666666666666, 43.33333333333333, 0.0, 0.0, 26.0, 22.5835846058712, 7.413318934457273, 0.0, 1.0, 45.20252734094706], 
processed observation next is [1.0, 0.2608695652173913, 0.2197599261311173, 0.4333333333333333, 0.0, 0.0, 1.0, 0.5119406579815998, 0.07413318934457273, 0.0, 1.0, 0.322875195292479], 
reward next is 0.6771, 
noisyNet noise sample is [array([-0.5603915], dtype=float32), 0.8212457]. 
=============================================
[2019-04-01 18:29:29,580] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[68.57625 ]
 [68.52014 ]
 [68.46156 ]
 [68.405136]
 [68.3418  ]], R is [[68.7042923 ]
 [68.68610382]
 [68.67249298]
 [68.65852356]
 [68.64407349]].
[2019-04-01 18:29:36,814] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:29:36,815] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3922
[2019-04-01 18:29:36,824] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.08333333333333331, 46.66666666666667, 81.66666666666667, 486.3333333333334, 26.0, 25.97365928088422, 9.576906145759864, 1.0, 1.0, 9.082289371181343], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 744600.0000, 
sim time next is 745200.0000, 
raw observation next is [0.0, 47.0, 82.5, 372.5, 26.0, 25.94378195368732, 9.297530255089294, 1.0, 1.0, 8.736625304543988], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.275, 0.4116022099447514, 1.0, 0.9919688505267601, 0.09297530255089294, 1.0, 1.0, 0.062404466461028485], 
reward next is 0.9376, 
noisyNet noise sample is [array([0.13617967], dtype=float32), 2.7457232]. 
=============================================
[2019-04-01 18:29:49,982] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:29:49,983] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5903
[2019-04-01 18:29:50,021] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 68.0, 120.0, 58.5, 26.0, 25.9351708737778, 7.781288857528675, 1.0, 1.0, 19.6638836172504], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 727200.0000, 
sim time next is 727800.0000, 
raw observation next is [-1.516666666666667, 67.66666666666667, 126.3333333333333, 61.66666666666666, 26.0, 25.92906810401777, 7.712242179542727, 1.0, 1.0, 18.68093542283449], 
processed observation next is [1.0, 0.43478260869565216, 0.4205909510618652, 0.6766666666666667, 0.421111111111111, 0.06813996316758747, 1.0, 0.9898668720025385, 0.07712242179542728, 1.0, 1.0, 0.13343525302024636], 
reward next is 0.8666, 
noisyNet noise sample is [array([0.46343255], dtype=float32), 1.4680434]. 
=============================================
[2019-04-01 18:29:51,020] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:29:51,023] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8290
[2019-04-01 18:29:51,028] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.2, 77.33333333333333, 0.0, 0.0, 26.0, 25.72044068648351, 11.81277963908466, 0.0, 1.0, 10.34626267199364], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1046400.0000, 
sim time next is 1047000.0000, 
raw observation next is [14.3, 77.16666666666667, 0.0, 0.0, 26.0, 25.60335390038455, 11.51485630205184, 0.0, 1.0, 10.05497468897143], 
processed observation next is [1.0, 0.08695652173913043, 0.8587257617728533, 0.7716666666666667, 0.0, 0.0, 1.0, 0.9433362714835073, 0.1151485630205184, 0.0, 1.0, 0.07182124777836736], 
reward next is 0.9282, 
noisyNet noise sample is [array([0.81801754], dtype=float32), -1.1112022]. 
=============================================
[2019-04-01 18:29:51,031] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[85.384384]
 [85.42934 ]
 [85.504684]
 [85.63613 ]
 [85.81339 ]], R is [[85.38764954]
 [85.45987701]
 [85.52325439]
 [85.58894348]
 [85.6499939 ]].
[2019-04-01 18:29:51,140] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:29:51,141] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7338
[2019-04-01 18:29:51,175] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 69.0, 0.0, 0.0, 26.0, 24.44426647162616, 6.034258011008379, 0.0, 1.0, 41.58791784685753], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 775800.0000, 
sim time next is 776400.0000, 
raw observation next is [-7.1, 69.66666666666666, 0.0, 0.0, 26.0, 24.41075256431706, 5.968060711920846, 0.0, 1.0, 41.48488995482843], 
processed observation next is [1.0, 1.0, 0.2659279778393352, 0.6966666666666665, 0.0, 0.0, 1.0, 0.7729646520452944, 0.05968060711920846, 0.0, 1.0, 0.2963206425344888], 
reward next is 0.7037, 
noisyNet noise sample is [array([0.28908953], dtype=float32), -0.16994005]. 
=============================================
[2019-04-01 18:29:52,541] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:29:52,542] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3951
[2019-04-01 18:29:52,579] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.5, 91.33333333333333, 8.999999999999998, 0.0, 26.0, 25.72613988003954, 10.43308964957449, 1.0, 1.0, 22.32357865471857], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 978600.0000, 
sim time next is 979200.0000, 
raw observation next is [9.4, 93.0, 13.5, 0.0, 26.0, 25.7120220408728, 10.39879662043764, 1.0, 1.0, 21.10604424457324], 
processed observation next is [1.0, 0.34782608695652173, 0.7229916897506927, 0.93, 0.045, 0.0, 1.0, 0.958860291553257, 0.10398796620437639, 1.0, 1.0, 0.15075745888980885], 
reward next is 0.6897, 
noisyNet noise sample is [array([-2.10521], dtype=float32), 0.2973275]. 
=============================================
[2019-04-01 18:30:03,574] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:30:03,575] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8126
[2019-04-01 18:30:03,614] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 80.0, 38.5, 0.0, 26.0, 25.42692985820501, 7.162896560423126, 1.0, 1.0, 21.12068206845633], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 896400.0000, 
sim time next is 897000.0000, 
raw observation next is [1.1, 80.66666666666667, 41.66666666666666, 0.0, 26.0, 25.45215226726436, 7.232080211211607, 1.0, 1.0, 20.08932432967373], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8066666666666668, 0.13888888888888887, 0.0, 1.0, 0.9217360381806229, 0.07232080211211607, 1.0, 1.0, 0.14349517378338378], 
reward next is 0.8565, 
noisyNet noise sample is [array([1.0654056], dtype=float32), -0.23707548]. 
=============================================
[2019-04-01 18:30:03,619] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[75.87961]
 [75.89225]
 [75.80702]
 [75.49362]
 [75.36211]], R is [[76.06552124]
 [76.15399933]
 [76.23580933]
 [76.29379272]
 [76.3168335 ]].
[2019-04-01 18:30:08,184] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:30:08,185] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1656
[2019-04-01 18:30:08,220] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.4, 93.0, 13.5, 0.0, 26.0, 25.7120220408728, 10.39879662043764, 1.0, 1.0, 21.10604424457324], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 979200.0000, 
sim time next is 979800.0000, 
raw observation next is [9.5, 92.83333333333333, 18.0, 0.0, 26.0, 25.70544335001741, 10.18102349551107, 1.0, 1.0, 19.99829185827922], 
processed observation next is [1.0, 0.34782608695652173, 0.7257617728531857, 0.9283333333333332, 0.06, 0.0, 1.0, 0.9579204785739159, 0.1018102349551107, 1.0, 1.0, 0.14284494184485158], 
reward next is 0.7847, 
noisyNet noise sample is [array([0.35509974], dtype=float32), 0.18601114]. 
=============================================
[2019-04-01 18:30:13,051] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:30:13,053] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0275
[2019-04-01 18:30:13,059] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.0, 96.0, 14.0, 0.0, 26.0, 23.4328851002625, 5.839634012152111, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1238400.0000, 
sim time next is 1239000.0000, 
raw observation next is [15.0, 96.66666666666666, 18.66666666666667, 0.0, 26.0, 23.41183882929219, 5.82387846652509, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.9666666666666666, 0.06222222222222224, 0.0, 1.0, 0.6302626898988842, 0.058238784665250905, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5775694], dtype=float32), 0.23455827]. 
=============================================
[2019-04-01 18:30:13,070] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[94.80344 ]
 [94.75614 ]
 [94.66155 ]
 [94.64543 ]
 [94.628784]], R is [[94.84441376]
 [94.89597321]
 [94.94701385]
 [94.99754333]
 [95.04756927]].
[2019-04-01 18:30:25,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 4.961194e-30], sum to 1.0000
[2019-04-01 18:30:25,643] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0107
[2019-04-01 18:30:25,702] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.65, 84.5, 28.0, 0.0, 26.0, 24.97380827206025, 8.365904779088773, 0.0, 1.0, 48.98190225316878], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1787400.0000, 
sim time next is 1788000.0000, 
raw observation next is [-3.733333333333333, 83.66666666666667, 23.5, 0.0, 26.0, 25.03560686060847, 8.446973558276644, 0.0, 1.0, 45.65424218236676], 
processed observation next is [0.0, 0.6956521739130435, 0.35918744228993543, 0.8366666666666667, 0.07833333333333334, 0.0, 1.0, 0.8622295515154959, 0.08446973558276644, 0.0, 1.0, 0.3261017298740483], 
reward next is 0.6739, 
noisyNet noise sample is [array([-0.7707485], dtype=float32), 1.8141822]. 
=============================================
[2019-04-01 18:30:25,718] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[76.28424 ]
 [76.4463  ]
 [76.754425]
 [77.025185]
 [77.25832 ]], R is [[76.03580475]
 [75.92557526]
 [75.79505157]
 [75.69965363]
 [75.63696289]].
[2019-04-01 18:30:32,456] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.0992748e-38 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:30:32,456] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3653
[2019-04-01 18:30:32,472] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.066666666666666, 66.33333333333334, 83.33333333333334, 694.6666666666667, 26.0, 26.34895672771929, 14.82484413687233, 1.0, 1.0, 2.001981126394702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1518000.0000, 
sim time next is 1518600.0000, 
raw observation next is [9.533333333333333, 64.66666666666666, 81.66666666666666, 688.3333333333333, 26.0, 26.46248224318279, 15.48470476716313, 1.0, 1.0, 1.884755270595118], 
processed observation next is [1.0, 0.5652173913043478, 0.7266851338873501, 0.6466666666666666, 0.2722222222222222, 0.7605893186003683, 1.0, 1.066068891883256, 0.1548470476716313, 1.0, 1.0, 0.013462537647107985], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2306645], dtype=float32), 2.0434864]. 
=============================================
[2019-04-01 18:30:35,869] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-01 18:30:35,870] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 18:30:35,871] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:30:35,872] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 18:30:35,873] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:30:35,874] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 18:30:35,874] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:30:35,888] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run30
[2019-04-01 18:30:35,910] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run30
[2019-04-01 18:30:35,933] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run30
[2019-04-01 18:30:48,108] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.8758417], dtype=float32), -1.1201155]
[2019-04-01 18:30:48,109] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.870080555, 66.018621185, 0.0, 0.0, 26.0, 24.88033254053129, 6.933500837531905, 0.0, 1.0, 45.18717092553258]
[2019-04-01 18:30:48,109] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 18:30:48,109] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.3639168692596554
[2019-04-01 18:31:04,106] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.8758417], dtype=float32), -1.1201155]
[2019-04-01 18:31:04,106] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.6, 45.0, 76.5, 17.0, 26.0, 25.67584802612067, 7.995063242747892, 1.0, 1.0, 19.64322495691992]
[2019-04-01 18:31:04,106] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 18:31:04,107] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.292895586225408
[2019-04-01 18:31:29,520] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.8758417], dtype=float32), -1.1201155]
[2019-04-01 18:31:29,520] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.9, 73.5, 0.0, 0.0, 26.0, 25.22959886704356, 8.601296974629966, 0.0, 1.0, 55.40841128264041]
[2019-04-01 18:31:29,520] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 18:31:29,521] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.5761491792511094
[2019-04-01 18:32:16,385] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 18:32:28,252] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.8758417], dtype=float32), -1.1201155]
[2019-04-01 18:32:28,252] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.0, 61.0, 0.0, 0.0, 26.0, 25.70949870843782, 10.0505229303613, 0.0, 1.0, 22.01344794574212]
[2019-04-01 18:32:28,252] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 18:32:28,253] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.5073315751697908
[2019-04-01 18:32:37,684] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 18:32:40,438] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 18:32:41,463] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2900000, evaluation results [2900000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 18:32:44,938] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:32:44,938] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4645
[2019-04-01 18:32:44,960] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.24455702921782, 9.722696862402904, 0.0, 1.0, 42.53986554296652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1729200.0000, 
sim time next is 1729800.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.23807606672176, 9.662985917054236, 0.0, 1.0, 42.50267830498386], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 1.0, 0.8911537238173943, 0.09662985917054236, 0.0, 1.0, 0.3035905593213133], 
reward next is 0.6964, 
noisyNet noise sample is [array([-0.20236097], dtype=float32), -0.7486852]. 
=============================================
[2019-04-01 18:32:45,426] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:32:45,426] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4024
[2019-04-01 18:32:45,445] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.2255665425769, 5.404953442716121, 0.0, 1.0, 40.54946372824783], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2006400.0000, 
sim time next is 2007000.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.23701243868904, 5.3765807110349, 0.0, 1.0, 40.54355218151809], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 1.0, 0.7481446340984341, 0.053765807110349, 0.0, 1.0, 0.2895968012965578], 
reward next is 0.7104, 
noisyNet noise sample is [array([-0.7013976], dtype=float32), -0.5106379]. 
=============================================
[2019-04-01 18:32:45,454] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[65.19797]
 [65.21424]
 [65.20632]
 [65.22114]
 [65.23825]], R is [[65.26093292]
 [65.31867981]
 [65.37583923]
 [65.43214417]
 [65.48763275]].
[2019-04-01 18:32:46,978] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:32:46,978] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7553
[2019-04-01 18:32:46,993] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 85.66666666666667, 0.0, 0.0, 26.0, 24.94544918852102, 8.155564478902088, 0.0, 1.0, 42.82133781949474], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1743600.0000, 
sim time next is 1744200.0000, 
raw observation next is [-0.6, 85.0, 0.0, 0.0, 26.0, 24.92540609548085, 8.084149060831562, 0.0, 1.0, 42.86715266520306], 
processed observation next is [0.0, 0.17391304347826086, 0.44598337950138506, 0.85, 0.0, 0.0, 1.0, 0.8464865850686927, 0.08084149060831562, 0.0, 1.0, 0.30619394760859325], 
reward next is 0.6938, 
noisyNet noise sample is [array([0.10698769], dtype=float32), 1.3433938]. 
=============================================
[2019-04-01 18:32:54,332] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:32:54,333] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1818
[2019-04-01 18:32:54,366] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 83.33333333333334, 0.0, 0.0, 26.0, 24.72178658243966, 6.5837368040958, 0.0, 1.0, 45.05670663225055], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1809600.0000, 
sim time next is 1810200.0000, 
raw observation next is [-5.0, 82.66666666666667, 0.0, 0.0, 26.0, 24.69390471523669, 6.511357036658161, 0.0, 1.0, 44.99681273534938], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.8266666666666667, 0.0, 0.0, 1.0, 0.8134149593195273, 0.06511357036658161, 0.0, 1.0, 0.32140580525249557], 
reward next is 0.6786, 
noisyNet noise sample is [array([-0.6771866], dtype=float32), -0.21493241]. 
=============================================
[2019-04-01 18:33:07,063] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6357984e-35
 1.0000000e+00 9.2878077e-15], sum to 1.0000
[2019-04-01 18:33:07,065] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9365
[2019-04-01 18:33:07,081] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.30476009832392, 5.481268184911154, 0.0, 1.0, 40.72943372191291], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2001000.0000, 
sim time next is 2001600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.25530897903938, 5.423909017618651, 0.0, 1.0, 40.74399881465406], 
processed observation next is [1.0, 0.17391304347826086, 0.30747922437673136, 0.83, 0.0, 0.0, 1.0, 0.7507584255770544, 0.05423909017618651, 0.0, 1.0, 0.29102856296181473], 
reward next is 0.7090, 
noisyNet noise sample is [array([-1.011358], dtype=float32), -0.346713]. 
=============================================
[2019-04-01 18:33:24,717] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:33:24,717] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6196
[2019-04-01 18:33:24,736] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.733333333333334, 89.66666666666667, 0.0, 0.0, 26.0, 23.91681357514834, 5.298676397116135, 0.0, 1.0, 42.90877306982507], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2263200.0000, 
sim time next is 2263800.0000, 
raw observation next is [-8.816666666666666, 90.33333333333334, 0.0, 0.0, 26.0, 23.85020336932598, 5.287208635013734, 0.0, 1.0, 42.87834456067948], 
processed observation next is [1.0, 0.17391304347826086, 0.21837488457987075, 0.9033333333333334, 0.0, 0.0, 1.0, 0.6928861956179969, 0.052872086350137334, 0.0, 1.0, 0.30627388971913916], 
reward next is 0.6937, 
noisyNet noise sample is [array([1.3277684], dtype=float32), 0.46313113]. 
=============================================
[2019-04-01 18:33:24,747] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:33:24,748] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5587
[2019-04-01 18:33:24,790] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.833333333333334, 84.0, 91.66666666666667, 35.16666666666666, 26.0, 25.54090612871665, 7.284782900335803, 1.0, 1.0, 37.05653401952082], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2280000.0000, 
sim time next is 2280600.0000, 
raw observation next is [-7.550000000000001, 82.5, 101.0, 39.0, 26.0, 25.54308334263389, 7.379002387704648, 1.0, 1.0, 36.49888489665511], 
processed observation next is [1.0, 0.391304347826087, 0.25346260387811637, 0.825, 0.33666666666666667, 0.0430939226519337, 1.0, 0.9347261918048412, 0.07379002387704647, 1.0, 1.0, 0.2607063206903937], 
reward next is 0.7393, 
noisyNet noise sample is [array([2.1262264], dtype=float32), 0.5412237]. 
=============================================
[2019-04-01 18:33:30,954] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:33:30,954] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3147
[2019-04-01 18:33:30,981] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 64.5, 0.0, 0.0, 26.0, 24.99997271933319, 7.331841273821856, 0.0, 1.0, 37.99063292490095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2333400.0000, 
sim time next is 2334000.0000, 
raw observation next is [-2.3, 64.0, 0.0, 0.0, 26.0, 24.98352979292443, 7.221425444117767, 0.0, 1.0, 37.93719918859738], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.64, 0.0, 0.0, 1.0, 0.8547899704177756, 0.07221425444117767, 0.0, 1.0, 0.270979994204267], 
reward next is 0.7290, 
noisyNet noise sample is [array([-0.1374095], dtype=float32), 0.058268193]. 
=============================================
[2019-04-01 18:33:30,991] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[69.13058]
 [69.27481]
 [68.50691]
 [68.51587]
 [68.55214]], R is [[68.86631775]
 [68.90629578]
 [68.94551849]
 [68.98369598]
 [69.02185059]].
[2019-04-01 18:33:32,228] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:33:32,228] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7146
[2019-04-01 18:33:32,256] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 65.0, 0.0, 0.0, 26.0, 24.46134397022768, 5.960749844745169, 0.0, 1.0, 39.77020990449478], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2347200.0000, 
sim time next is 2347800.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.47931049884322, 5.915407570651406, 0.0, 1.0, 39.85471176751805], 
processed observation next is [0.0, 0.17391304347826086, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 1.0, 0.7827586426918884, 0.05915407570651406, 0.0, 1.0, 0.2846765126251289], 
reward next is 0.7153, 
noisyNet noise sample is [array([-0.5105191], dtype=float32), 1.2495565]. 
=============================================
[2019-04-01 18:33:37,424] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:33:37,425] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4697
[2019-04-01 18:33:37,456] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.733333333333333, 42.66666666666667, 0.0, 0.0, 26.0, 24.96756359023004, 7.301286256268057, 0.0, 1.0, 48.14989719343674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2402400.0000, 
sim time next is 2403000.0000, 
raw observation next is [-2.9, 42.5, 0.0, 0.0, 26.0, 25.01175435797646, 7.347035036663617, 0.0, 1.0, 44.56857564578604], 
processed observation next is [0.0, 0.8260869565217391, 0.38227146814404434, 0.425, 0.0, 0.0, 1.0, 0.8588220511394944, 0.07347035036663617, 0.0, 1.0, 0.3183469688984717], 
reward next is 0.6817, 
noisyNet noise sample is [array([0.50530565], dtype=float32), -0.63262933]. 
=============================================
[2019-04-01 18:33:37,462] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[69.25544 ]
 [69.186066]
 [69.21823 ]
 [69.2095  ]
 [69.18045 ]], R is [[69.35405731]
 [69.31658936]
 [69.29764557]
 [69.35101318]
 [69.39476013]].
[2019-04-01 18:33:37,645] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:33:37,646] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6272
[2019-04-01 18:33:37,661] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.4, 60.83333333333334, 0.0, 0.0, 26.0, 23.03363110870561, 6.07198331884842, 0.0, 1.0, 43.46255244477633], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2443800.0000, 
sim time next is 2444400.0000, 
raw observation next is [-9.5, 61.0, 0.0, 0.0, 26.0, 22.9900438824534, 6.152071703423331, 0.0, 1.0, 43.50077247198127], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.61, 0.0, 0.0, 1.0, 0.5700062689219142, 0.06152071703423331, 0.0, 1.0, 0.3107198033712948], 
reward next is 0.6893, 
noisyNet noise sample is [array([0.79913366], dtype=float32), 2.0910246]. 
=============================================
[2019-04-01 18:33:45,240] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:33:45,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7849
[2019-04-01 18:33:45,256] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.8, 26.0, 165.0, 378.5, 26.0, 25.76447512142555, 8.33562558119223, 1.0, 1.0, 5.726857845732196], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2556000.0000, 
sim time next is 2556600.0000, 
raw observation next is [3.716666666666666, 26.5, 163.0, 344.0, 26.0, 25.7693621254661, 8.483977893712273, 1.0, 1.0, 5.727588156825869], 
processed observation next is [1.0, 0.6086956521739131, 0.5655586334256695, 0.265, 0.5433333333333333, 0.38011049723756907, 1.0, 0.9670517322094427, 0.08483977893712273, 1.0, 1.0, 0.04091134397732763], 
reward next is 0.9591, 
noisyNet noise sample is [array([-1.6553526], dtype=float32), 0.54481304]. 
=============================================
[2019-04-01 18:33:49,264] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:33:49,278] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5509
[2019-04-01 18:33:49,300] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 77.83333333333334, 0.0, 0.0, 26.0, 25.32188377875306, 7.818058325651779, 0.0, 1.0, 54.97701475015148], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2857800.0000, 
sim time next is 2858400.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.34676023362282, 7.69614251252167, 0.0, 1.0, 55.10320229499445], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 1.0, 0.9066800333746886, 0.0769614251252167, 0.0, 1.0, 0.3935943021071032], 
reward next is 0.6064, 
noisyNet noise sample is [array([2.0590467], dtype=float32), 0.43104288]. 
=============================================
[2019-04-01 18:33:49,590] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:33:49,591] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7702
[2019-04-01 18:33:49,626] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.266666666666667, 63.0, 164.0, 257.6666666666667, 26.0, 25.76291906743346, 8.26232959052841, 1.0, 1.0, 19.23692324363826], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2630400.0000, 
sim time next is 2631000.0000, 
raw observation next is [-4.083333333333333, 62.5, 176.0, 240.3333333333333, 26.0, 25.7580084685897, 8.293360515918083, 1.0, 1.0, 18.24854724579078], 
processed observation next is [1.0, 0.43478260869565216, 0.34949215143120965, 0.625, 0.5866666666666667, 0.265561694290976, 1.0, 0.9654297812271001, 0.08293360515918083, 1.0, 1.0, 0.1303467660413627], 
reward next is 0.8697, 
noisyNet noise sample is [array([0.6464342], dtype=float32), 0.6247268]. 
=============================================
[2019-04-01 18:33:49,631] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[64.72471 ]
 [64.98739 ]
 [65.34179 ]
 [65.64133 ]
 [66.029396]], R is [[64.72341156]
 [64.93877411]
 [65.14437866]
 [65.33943176]
 [65.52394867]].
[2019-04-01 18:33:49,907] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 3.963964e-12], sum to 1.0000
[2019-04-01 18:33:49,907] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0106
[2019-04-01 18:33:49,925] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 24.12117971525251, 5.589190658940413, 0.0, 1.0, 52.42794062861547], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2959800.0000, 
sim time next is 2960400.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 24.04320768033235, 5.521812587551267, 0.0, 1.0, 56.78121063763918], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 1.0, 0.7204582400474787, 0.05521812587551267, 0.0, 1.0, 0.405580075983137], 
reward next is 0.5944, 
noisyNet noise sample is [array([-0.00643475], dtype=float32), 0.2516256]. 
=============================================
[2019-04-01 18:33:50,506] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:33:50,510] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7450
[2019-04-01 18:33:50,522] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.04999999999999999, 52.0, 7.0, 82.0, 26.0, 25.87508247304894, 9.280558877614057, 1.0, 1.0, 12.82659309969867], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2655000.0000, 
sim time next is 2655600.0000, 
raw observation next is [-0.2333333333333333, 52.66666666666667, 0.0, 0.0, 26.0, 25.73006581523612, 8.327095350728149, 1.0, 1.0, 12.38865364711498], 
processed observation next is [1.0, 0.7391304347826086, 0.456140350877193, 0.5266666666666667, 0.0, 0.0, 1.0, 0.96143797360516, 0.08327095350728149, 1.0, 1.0, 0.08849038319367843], 
reward next is 0.9115, 
noisyNet noise sample is [array([-0.56603223], dtype=float32), -1.5614282]. 
=============================================
[2019-04-01 18:33:50,898] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:33:50,900] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7202
[2019-04-01 18:33:50,941] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 60.5, 0.0, 0.0, 26.0, 25.08329482675332, 8.547785014923164, 1.0, 1.0, 33.08883784443012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2661000.0000, 
sim time next is 2661600.0000, 
raw observation next is [-1.2, 61.00000000000001, 0.0, 0.0, 26.0, 25.11739141842262, 8.449018662471063, 0.0, 1.0, 28.73575960902699], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.6100000000000001, 0.0, 0.0, 1.0, 0.87391305977466, 0.08449018662471064, 0.0, 1.0, 0.2052554257787642], 
reward next is 0.7947, 
noisyNet noise sample is [array([-1.0753785], dtype=float32), 0.18208216]. 
=============================================
[2019-04-01 18:34:02,723] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:34:02,723] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1285
[2019-04-01 18:34:02,740] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.66443389368361, 10.04886281589798, 0.0, 1.0, 32.85170915964854], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2841600.0000, 
sim time next is 2842200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.64600860385464, 9.828502868148533, 0.0, 1.0, 30.70208489946398], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44, 0.0, 0.0, 1.0, 0.9494298005506627, 0.09828502868148532, 0.0, 1.0, 0.21930060642474272], 
reward next is 0.7807, 
noisyNet noise sample is [array([-1.3853096], dtype=float32), 0.66149193]. 
=============================================
[2019-04-01 18:34:03,976] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:34:03,976] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3592
[2019-04-01 18:34:04,001] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 81.33333333333334, 0.0, 0.0, 26.0, 25.28934745567737, 7.590752398942617, 0.0, 1.0, 55.09593425938873], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2859600.0000, 
sim time next is 2860200.0000, 
raw observation next is [1.0, 82.5, 0.0, 0.0, 26.0, 25.27659182483593, 7.56691710041301, 0.0, 1.0, 55.04824970224431], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.825, 0.0, 0.0, 1.0, 0.8966559749765614, 0.07566917100413009, 0.0, 1.0, 0.3932017835874594], 
reward next is 0.6068, 
noisyNet noise sample is [array([-0.79641044], dtype=float32), -0.03240923]. 
=============================================
[2019-04-01 18:34:07,157] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:34:07,157] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8287
[2019-04-01 18:34:07,183] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 67.0, 0.0, 0.0, 26.0, 25.37852952669915, 8.464501290478516, 0.0, 1.0, 41.69872044695452], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2849400.0000, 
sim time next is 2850000.0000, 
raw observation next is [1.333333333333333, 68.66666666666667, 0.0, 0.0, 26.0, 25.37269415427325, 8.39855287974582, 0.0, 1.0, 41.34360254167908], 
processed observation next is [1.0, 1.0, 0.4995383194829178, 0.6866666666666668, 0.0, 0.0, 1.0, 0.910384879181893, 0.0839855287974582, 0.0, 1.0, 0.2953114467262792], 
reward next is 0.7047, 
noisyNet noise sample is [array([-0.05389086], dtype=float32), -0.9561241]. 
=============================================
[2019-04-01 18:34:07,192] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[65.13502 ]
 [65.245445]
 [65.3396  ]
 [65.380516]
 [65.41598 ]], R is [[65.09327698]
 [65.1444931 ]
 [65.19108582]
 [65.23394775]
 [65.27282715]].
[2019-04-01 18:34:15,717] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:34:15,718] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7827
[2019-04-01 18:34:15,756] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8, 87.0, 0.0, 0.0, 26.0, 24.9745055255421, 7.450009005457403, 0.0, 1.0, 35.02398330907985], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3090600.0000, 
sim time next is 3091200.0000, 
raw observation next is [-0.8666666666666667, 88.66666666666666, 0.0, 0.0, 26.0, 24.99831747930423, 7.478572441384865, 0.0, 1.0, 33.83145509627081], 
processed observation next is [0.0, 0.782608695652174, 0.4385964912280702, 0.8866666666666666, 0.0, 0.0, 1.0, 0.8569024970434614, 0.07478572441384865, 0.0, 1.0, 0.24165325068764862], 
reward next is 0.7583, 
noisyNet noise sample is [array([-0.00072471], dtype=float32), -0.56321687]. 
=============================================
[2019-04-01 18:34:17,848] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 7.6457595e-26], sum to 1.0000
[2019-04-01 18:34:17,850] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6740
[2019-04-01 18:34:17,866] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.87113374593915, 7.010778879954306, 0.0, 1.0, 40.66455712763331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3376800.0000, 
sim time next is 3377400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.81370272340709, 6.948227737273527, 0.0, 1.0, 40.66813986270292], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.8305289604867271, 0.06948227737273527, 0.0, 1.0, 0.29048671330502085], 
reward next is 0.7095, 
noisyNet noise sample is [array([-0.09516268], dtype=float32), -0.6493463]. 
=============================================
[2019-04-01 18:34:19,448] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.49821076e-19 6.28009355e-28 1.07950056e-19 4.56449682e-28
 1.06799225e-09 9.99836564e-01 1.63455974e-04], sum to 1.0000
[2019-04-01 18:34:19,449] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3420
[2019-04-01 18:34:19,472] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666666, 100.0, 0.0, 0.0, 26.0, 25.30037825773036, 7.380581566308699, 0.0, 1.0, 38.75478545662079], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3112800.0000, 
sim time next is 3113400.0000, 
raw observation next is [0.8333333333333334, 100.0, 0.0, 0.0, 26.0, 25.27233159434224, 7.589522988344899, 0.0, 1.0, 39.31622363929446], 
processed observation next is [1.0, 0.0, 0.4856879039704525, 1.0, 0.0, 0.0, 1.0, 0.8960473706203201, 0.07589522988344899, 0.0, 1.0, 0.2808301688521033], 
reward next is 0.7192, 
noisyNet noise sample is [array([-1.7142133], dtype=float32), -0.55860704]. 
=============================================
[2019-04-01 18:34:20,013] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.1970611e-23], sum to 1.0000
[2019-04-01 18:34:20,013] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6322
[2019-04-01 18:34:20,029] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 100.0, 0.0, 0.0, 26.0, 25.46043782786018, 7.565551097704284, 0.0, 1.0, 47.06257363926281], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3136200.0000, 
sim time next is 3136800.0000, 
raw observation next is [6.0, 100.0, 0.0, 0.0, 26.0, 25.4621716735192, 7.531034816752848, 1.0, 1.0, 38.02743292744697], 
processed observation next is [1.0, 0.30434782608695654, 0.6288088642659281, 1.0, 0.0, 0.0, 1.0, 0.9231673819313144, 0.07531034816752848, 1.0, 1.0, 0.27162452091033545], 
reward next is 0.7284, 
noisyNet noise sample is [array([0.7675143], dtype=float32), -1.7216278]. 
=============================================
[2019-04-01 18:34:24,341] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:34:24,343] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0449
[2019-04-01 18:34:24,385] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 43.0, 226.0, 26.0, 25.48559466010209, 10.35010529314189, 1.0, 1.0, 25.62140933935177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3225600.0000, 
sim time next is 3226200.0000, 
raw observation next is [-3.0, 92.0, 57.00000000000001, 274.0, 26.0, 25.47068859486787, 10.26803732509035, 1.0, 1.0, 24.0873320161394], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.19000000000000003, 0.3027624309392265, 1.0, 0.9243840849811242, 0.10268037325090351, 1.0, 1.0, 0.17205237154385286], 
reward next is 0.7207, 
noisyNet noise sample is [array([1.7398322], dtype=float32), 1.1999948]. 
=============================================
[2019-04-01 18:34:35,757] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5893860e-10 2.6272307e-10 1.7147608e-10 9.9807608e-01 2.1288735e-15
 4.4288614e-04 1.4810775e-03], sum to 1.0000
[2019-04-01 18:34:35,758] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9248
[2019-04-01 18:34:35,796] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 77.0, 94.5, 552.0, 26.0, 25.64474413671516, 9.04279188556712, 1.0, 1.0, 24.60404654878605], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3747600.0000, 
sim time next is 3748200.0000, 
raw observation next is [-3.833333333333333, 77.0, 96.33333333333333, 593.0, 26.0, 25.80327603879694, 9.626045862369004, 1.0, 1.0, 25.38627842926581], 
processed observation next is [1.0, 0.391304347826087, 0.3564173591874424, 0.77, 0.32111111111111107, 0.6552486187845303, 1.0, 0.9718965769709916, 0.09626045862369004, 1.0, 1.0, 0.1813305602090415], 
reward next is 0.8187, 
noisyNet noise sample is [array([-1.6708218], dtype=float32), 0.012805045]. 
=============================================
[2019-04-01 18:34:37,283] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.7827023e-37], sum to 1.0000
[2019-04-01 18:34:37,286] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6964
[2019-04-01 18:34:37,352] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.1666666666666667, 71.83333333333333, 0.0, 0.0, 26.0, 25.17655451240056, 8.240390203973055, 0.0, 1.0, 69.264781543863], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3481800.0000, 
sim time next is 3482400.0000, 
raw observation next is [-0.3333333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.36745702288637, 8.534461846018255, 0.0, 1.0, 48.65075249857688], 
processed observation next is [1.0, 0.30434782608695654, 0.4533702677747, 0.7166666666666667, 0.0, 0.0, 1.0, 0.9096367175551957, 0.08534461846018254, 0.0, 1.0, 0.34750537498983486], 
reward next is 0.6525, 
noisyNet noise sample is [array([-1.9909014], dtype=float32), -0.8403995]. 
=============================================
[2019-04-01 18:34:38,678] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 3.0921215e-21], sum to 1.0000
[2019-04-01 18:34:38,680] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0042
[2019-04-01 18:34:38,695] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.34111927906645, 9.228763859639601, 0.0, 1.0, 41.38488288136664], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3546000.0000, 
sim time next is 3546600.0000, 
raw observation next is [-2.166666666666667, 61.83333333333333, 0.0, 0.0, 26.0, 25.32301832046266, 9.116257159855401, 0.0, 1.0, 41.21983073292278], 
processed observation next is [0.0, 0.043478260869565216, 0.4025854108956602, 0.6183333333333333, 0.0, 0.0, 1.0, 0.9032883314946655, 0.09116257159855401, 0.0, 1.0, 0.29442736237801986], 
reward next is 0.7056, 
noisyNet noise sample is [array([-0.9400371], dtype=float32), -1.5427608]. 
=============================================
[2019-04-01 18:34:46,651] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:34:46,652] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1874
[2019-04-01 18:34:46,667] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.0, 25.0, 0.0, 0.0, 26.0, 25.48391750367027, 7.836788191371109, 0.0, 1.0, 30.51603452171059], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3650400.0000, 
sim time next is 3651000.0000, 
raw observation next is [9.833333333333334, 25.33333333333334, 0.0, 0.0, 26.0, 25.5146524397761, 7.908554917283408, 0.0, 1.0, 30.73460688756661], 
processed observation next is [0.0, 0.2608695652173913, 0.7349953831948293, 0.2533333333333334, 0.0, 0.0, 1.0, 0.9306646342537286, 0.07908554917283409, 0.0, 1.0, 0.2195329063397615], 
reward next is 0.7805, 
noisyNet noise sample is [array([-0.5751932], dtype=float32), -0.01684357]. 
=============================================
[2019-04-01 18:34:46,676] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[76.80207]
 [76.77133]
 [76.71702]
 [76.60787]
 [76.49558]], R is [[76.74828339]
 [76.76283264]
 [76.78775787]
 [76.82302094]
 [76.86621857]].
[2019-04-01 18:34:46,920] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:34:46,921] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0978
[2019-04-01 18:34:46,933] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 24.96334272882751, 7.49101315241568, 0.0, 1.0, 17.18301334635609], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3696000.0000, 
sim time next is 3696600.0000, 
raw observation next is [4.0, 59.0, 0.0, 0.0, 26.0, 24.95530102060563, 7.485554157637523, 0.0, 1.0, 21.67218844748144], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.59, 0.0, 0.0, 1.0, 0.8507572886579473, 0.07485554157637524, 0.0, 1.0, 0.15480134605343884], 
reward next is 0.8452, 
noisyNet noise sample is [array([2.1278114], dtype=float32), -0.03648617]. 
=============================================
[2019-04-01 18:34:50,709] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:34:50,709] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0788
[2019-04-01 18:34:50,730] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 24.93566452260399, 7.476440847967251, 0.0, 1.0, 23.29950595576806], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3697200.0000, 
sim time next is 3697800.0000, 
raw observation next is [3.833333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 24.92119760585301, 7.442433465138246, 0.0, 1.0, 19.4941240008265], 
processed observation next is [0.0, 0.8260869565217391, 0.5687903970452447, 0.5966666666666667, 0.0, 0.0, 1.0, 0.8458853722647157, 0.07442433465138247, 0.0, 1.0, 0.13924374286304642], 
reward next is 0.8608, 
noisyNet noise sample is [array([-0.42002276], dtype=float32), -1.3310281]. 
=============================================
[2019-04-01 18:34:50,993] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 4.3502428e-27], sum to 1.0000
[2019-04-01 18:34:50,993] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0294
[2019-04-01 18:34:51,021] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.333333333333333, 62.33333333333333, 0.0, 0.0, 26.0, 25.82621251907763, 9.857317198047147, 0.0, 1.0, 26.80611753118933], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3703200.0000, 
sim time next is 3703800.0000, 
raw observation next is [2.166666666666667, 62.16666666666667, 0.0, 0.0, 26.0, 25.7942197779856, 9.61404562483616, 0.0, 1.0, 25.39586792358303], 
processed observation next is [0.0, 0.8695652173913043, 0.5226223453370269, 0.6216666666666667, 0.0, 0.0, 1.0, 0.9706028254265142, 0.09614045624836161, 0.0, 1.0, 0.18139905659702163], 
reward next is 0.8186, 
noisyNet noise sample is [array([0.14804201], dtype=float32), -0.12474179]. 
=============================================
[2019-04-01 18:34:55,422] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:34:55,423] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2863
[2019-04-01 18:34:55,430] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 26.0, 26.15283468716875, 13.66845601937613, 1.0, 1.0, 6.930859760713779], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3866400.0000, 
sim time next is 3867000.0000, 
raw observation next is [1.833333333333333, 48.5, 0.0, 0.0, 26.0, 26.19096723122227, 13.87367247108654, 1.0, 1.0, 7.382806090515589], 
processed observation next is [1.0, 0.782608695652174, 0.5133887349953832, 0.485, 0.0, 0.0, 1.0, 1.027281033031753, 0.1387367247108654, 1.0, 1.0, 0.052734329217968494], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1954741], dtype=float32), 0.58484036]. 
=============================================
[2019-04-01 18:34:55,451] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[47.411087]
 [46.67507 ]
 [45.47639 ]
 [44.91221 ]
 [44.229885]], R is [[47.3177681 ]
 [46.84458923]
 [46.37614441]
 [45.91238403]
 [45.45326233]].
[2019-04-01 18:34:57,042] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 9.141618e-37], sum to 1.0000
[2019-04-01 18:34:57,043] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1233
[2019-04-01 18:34:57,082] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 38.0, 110.5, 806.0, 26.0, 26.95261995599118, 15.06301120678142, 1.0, 1.0, 14.71342351947815], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3938400.0000, 
sim time next is 3939000.0000, 
raw observation next is [-4.833333333333334, 38.0, 108.6666666666667, 800.0, 26.0, 26.25311653660342, 15.04240821335659, 1.0, 1.0, 13.54864910680616], 
processed observation next is [1.0, 0.6086956521739131, 0.32871652816251157, 0.38, 0.36222222222222233, 0.8839779005524862, 1.0, 1.0361595052290602, 0.1504240821335659, 1.0, 1.0, 0.09677606504861543], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.9215786], dtype=float32), 1.1357642]. 
=============================================
[2019-04-01 18:34:57,088] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[32.197422]
 [31.812023]
 [31.408998]
 [31.17953 ]
 [30.96349 ]], R is [[32.00759506]
 [31.68751907]
 [31.37064362]
 [31.05693817]
 [30.74636841]].
[2019-04-01 18:34:58,676] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.4833311e-34 0.0000000e+00 0.0000000e+00 1.6460517e-36 1.7158757e-32
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:34:58,677] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3486
[2019-04-01 18:34:58,692] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.23394462273429, 8.124135355715971, 0.0, 1.0, 40.14044674812922], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3901800.0000, 
sim time next is 3902400.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.21795622688316, 8.274232592133105, 0.0, 1.0, 40.04057929836518], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.71, 0.0, 0.0, 1.0, 0.8882794609833086, 0.08274232592133104, 0.0, 1.0, 0.2860041378454656], 
reward next is 0.7140, 
noisyNet noise sample is [array([-1.37224], dtype=float32), 0.9228746]. 
=============================================
[2019-04-01 18:35:00,746] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 4.8318325e-30], sum to 1.0000
[2019-04-01 18:35:00,746] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5328
[2019-04-01 18:35:00,774] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.833333333333333, 43.83333333333334, 117.0, 827.6666666666666, 26.0, 26.73846018349096, 16.84952162056423, 1.0, 1.0, 18.13812608726348], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3935400.0000, 
sim time next is 3936000.0000, 
raw observation next is [-5.666666666666666, 42.66666666666667, 116.5, 825.8333333333334, 26.0, 26.82078597192103, 17.20822054608728, 1.0, 1.0, 17.1322824563496], 
processed observation next is [1.0, 0.5652173913043478, 0.3056325023084026, 0.4266666666666667, 0.3883333333333333, 0.912523020257827, 1.0, 1.1172551388458614, 0.1720822054608728, 1.0, 1.0, 0.12237344611678286], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.14793412], dtype=float32), -0.6697508]. 
=============================================
[2019-04-01 18:35:00,782] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[40.26811 ]
 [40.130417]
 [39.99128 ]
 [39.97253 ]
 [39.876797]], R is [[40.01494598]
 [39.61479568]
 [39.218647  ]
 [38.82646179]
 [38.43819809]].
[2019-04-01 18:35:03,561] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0004907e-33 0.0000000e+00 4.5547229e-34 0.0000000e+00 7.4520273e-33
 1.0000000e+00 2.7438256e-16], sum to 1.0000
[2019-04-01 18:35:03,563] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6937
[2019-04-01 18:35:03,587] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.33333333333333, 65.0, 0.0, 0.0, 26.0, 23.48687196128928, 5.311742935863864, 0.0, 1.0, 42.76239812943936], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3997200.0000, 
sim time next is 3997800.0000, 
raw observation next is [-13.5, 66.0, 0.0, 0.0, 26.0, 23.47683530565106, 5.320183864664433, 0.0, 1.0, 42.6620084513722], 
processed observation next is [1.0, 0.2608695652173913, 0.0886426592797784, 0.66, 0.0, 0.0, 1.0, 0.6395479008072945, 0.05320183864664432, 0.0, 1.0, 0.30472863179551574], 
reward next is 0.6953, 
noisyNet noise sample is [array([-0.89836717], dtype=float32), 0.09414156]. 
=============================================
[2019-04-01 18:35:07,354] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1768858e-18 1.5839429e-26 4.4637410e-22 7.4684615e-22 4.2460493e-20
 1.0000000e+00 2.6117645e-22], sum to 1.0000
[2019-04-01 18:35:07,356] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2800
[2019-04-01 18:35:07,401] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 34.0, 46.0, 234.5, 26.0, 25.54223450404544, 7.973174197648973, 1.0, 1.0, 32.79618388642923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4089600.0000, 
sim time next is 4090200.0000, 
raw observation next is [-3.833333333333333, 34.66666666666667, 61.33333333333334, 312.6666666666667, 26.0, 25.53801295684144, 8.103185299604156, 1.0, 1.0, 30.50727741685133], 
processed observation next is [1.0, 0.34782608695652173, 0.3564173591874424, 0.34666666666666673, 0.20444444444444448, 0.34548802946593005, 1.0, 0.9340018509773483, 0.08103185299604157, 1.0, 1.0, 0.21790912440608093], 
reward next is 0.7821, 
noisyNet noise sample is [array([-0.7156589], dtype=float32), -0.7636521]. 
=============================================
[2019-04-01 18:35:20,747] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.5434574e-24], sum to 1.0000
[2019-04-01 18:35:20,749] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0744
[2019-04-01 18:35:20,757] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.9, 58.0, 116.0, 655.0, 26.0, 25.42776234369542, 9.759568264408806, 0.0, 1.0, 3.420127142702583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4289400.0000, 
sim time next is 4290000.0000, 
raw observation next is [6.933333333333334, 57.33333333333333, 108.3333333333333, 638.5, 26.0, 25.4356570831365, 9.789832006559875, 0.0, 1.0, 3.299412918163322], 
processed observation next is [0.0, 0.6521739130434783, 0.6546629732225301, 0.5733333333333333, 0.361111111111111, 0.7055248618784531, 1.0, 0.9193795833052144, 0.09789832006559875, 0.0, 1.0, 0.023567235129738014], 
reward next is 0.9764, 
noisyNet noise sample is [array([-0.09757149], dtype=float32), 0.71269864]. 
=============================================
[2019-04-01 18:35:20,769] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[73.46669]
 [73.71216]
 [73.93074]
 [74.24074]
 [74.72203]], R is [[73.83238983]
 [74.06964111]
 [74.30360413]
 [74.53437805]
 [74.76213074]].
[2019-04-01 18:35:22,391] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8180835e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.3648256e-25
 1.0000000e+00 1.2839424e-22], sum to 1.0000
[2019-04-01 18:35:22,397] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4005
[2019-04-01 18:35:22,413] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.666666666666667, 72.0, 0.0, 0.0, 26.0, 25.533015009715, 11.39942392770345, 0.0, 1.0, 40.39688394005463], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4429200.0000, 
sim time next is 4429800.0000, 
raw observation next is [2.5, 74.0, 0.0, 0.0, 26.0, 25.61399538050899, 11.64803604948312, 0.0, 1.0, 31.23826243467548], 
processed observation next is [1.0, 0.2608695652173913, 0.5318559556786704, 0.74, 0.0, 0.0, 1.0, 0.9448564829298556, 0.1164803604948312, 0.0, 1.0, 0.2231304459619677], 
reward next is 0.7769, 
noisyNet noise sample is [array([-1.9003048], dtype=float32), -0.5107807]. 
=============================================
[2019-04-01 18:35:24,924] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.6007429e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 3.6578379e-28], sum to 1.0000
[2019-04-01 18:35:24,927] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7413
[2019-04-01 18:35:24,935] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666667, 88.0, 108.6666666666667, 0.0, 26.0, 26.5562558148217, 14.05256751956521, 1.0, 1.0, 10.20005200132575], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4450800.0000, 
sim time next is 4451400.0000, 
raw observation next is [0.5, 89.0, 102.0, 0.0, 26.0, 26.50292623291551, 13.61603213689947, 1.0, 1.0, 10.36626702694452], 
processed observation next is [1.0, 0.5217391304347826, 0.4764542936288089, 0.89, 0.34, 0.0, 1.0, 1.0718466047022157, 0.1361603213689947, 1.0, 1.0, 0.07404476447817514], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7873541], dtype=float32), -1.7480458]. 
=============================================
[2019-04-01 18:35:32,170] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2095410e-26 2.0623686e-28 2.5335485e-26 9.8076726e-30 1.5368858e-26
 1.0000000e+00 5.1386910e-31], sum to 1.0000
[2019-04-01 18:35:32,171] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9476
[2019-04-01 18:35:32,195] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1666666666666667, 70.16666666666667, 119.0, 22.0, 26.0, 26.16817605185553, 10.88545403574477, 1.0, 1.0, 16.16946001811677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4525800.0000, 
sim time next is 4526400.0000, 
raw observation next is [0.3333333333333333, 68.33333333333334, 121.0, 11.0, 26.0, 26.19709210487349, 10.82703382583833, 1.0, 1.0, 15.3870106508779], 
processed observation next is [1.0, 0.391304347826087, 0.4718374884579871, 0.6833333333333335, 0.4033333333333333, 0.012154696132596685, 1.0, 1.0281560149819273, 0.1082703382583833, 1.0, 1.0, 0.10990721893484214], 
reward next is 0.5593, 
noisyNet noise sample is [array([-0.902034], dtype=float32), 0.059388082]. 
=============================================
[2019-04-01 18:35:32,677] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:35:32,677] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5354
[2019-04-01 18:35:32,697] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 51.33333333333334, 167.0, 16.0, 26.0, 25.83177683691959, 9.834257556133219, 1.0, 1.0, 7.407983182738354], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4539000.0000, 
sim time next is 4539600.0000, 
raw observation next is [2.0, 52.0, 187.0, 24.0, 26.0, 25.74647744815849, 9.85977772378063, 1.0, 1.0, 7.465394520411902], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 0.52, 0.6233333333333333, 0.026519337016574586, 1.0, 0.96378249259407, 0.0985977772378063, 1.0, 1.0, 0.05332424657437073], 
reward next is 0.9467, 
noisyNet noise sample is [array([-0.9692176], dtype=float32), -0.38701]. 
=============================================
[2019-04-01 18:35:38,328] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.621133e-27
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:35:38,333] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5739
[2019-04-01 18:35:38,346] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.0, 17.33333333333334, 62.66666666666667, 487.3333333333334, 26.0, 29.0384127643015, 35.26749513238244, 1.0, 1.0, 0.6237501127923023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5071800.0000, 
sim time next is 5072400.0000, 
raw observation next is [12.0, 17.0, 56.0, 438.5, 26.0, 29.17556096373134, 29.00635964285925, 1.0, 1.0, 0.6278682316008353], 
processed observation next is [1.0, 0.7391304347826086, 0.7950138504155125, 0.17, 0.18666666666666668, 0.4845303867403315, 1.0, 1.4536515662473342, 0.2900635964285925, 1.0, 1.0, 0.004484773082863109], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.59372693], dtype=float32), -0.33689618]. 
=============================================
[2019-04-01 18:35:40,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:35:40,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:35:40,342] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run23
[2019-04-01 18:35:41,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:35:41,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:35:41,197] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run23
[2019-04-01 18:35:46,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:35:46,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:35:46,234] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run23
[2019-04-01 18:35:55,615] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:35:55,619] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9899
[2019-04-01 18:35:55,633] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.833333333333333, 35.0, 0.0, 0.0, 26.0, 25.420219727935, 9.867349533808095, 0.0, 1.0, 39.80320190526449], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5008200.0000, 
sim time next is 5008800.0000, 
raw observation next is [2.666666666666667, 36.0, 0.0, 0.0, 26.0, 25.44095871995674, 10.08435637674449, 0.0, 1.0, 39.90034387418065], 
processed observation next is [1.0, 1.0, 0.5364727608494922, 0.36, 0.0, 0.0, 1.0, 0.92013695999382, 0.1008435637674449, 0.0, 1.0, 0.2850024562441475], 
reward next is 0.7150, 
noisyNet noise sample is [array([-1.0323702], dtype=float32), 0.5287271]. 
=============================================
[2019-04-01 18:35:55,994] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:35:55,994] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:35:55,999] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run23
[2019-04-01 18:35:56,379] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:35:56,379] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:35:56,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run23
[2019-04-01 18:35:56,458] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:35:56,459] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7924
[2019-04-01 18:35:56,471] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.4379393035653, 8.473800390396145, 0.0, 1.0, 31.340735585258], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5023200.0000, 
sim time next is 5023800.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.43199039753186, 8.417976007674227, 0.0, 1.0, 34.24505095953156], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.55, 0.0, 0.0, 1.0, 0.9188557710759798, 0.08417976007674227, 0.0, 1.0, 0.24460750685379687], 
reward next is 0.7554, 
noisyNet noise sample is [array([1.6058758], dtype=float32), -0.9488257]. 
=============================================
[2019-04-01 18:35:56,573] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:35:56,576] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5910
[2019-04-01 18:35:56,604] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.333333333333333, 43.0, 112.6666666666667, 716.5, 26.0, 26.64936124896465, 14.59217225426592, 1.0, 1.0, 11.68064223803522], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5046000.0000, 
sim time next is 5046600.0000, 
raw observation next is [2.666666666666667, 42.0, 113.3333333333333, 735.0, 26.0, 26.83898360712868, 14.87331754894182, 1.0, 1.0, 10.7888529007449], 
processed observation next is [1.0, 0.391304347826087, 0.5364727608494922, 0.42, 0.37777777777777766, 0.8121546961325967, 1.0, 1.1198548010183826, 0.1487331754894182, 1.0, 1.0, 0.07706323500532071], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.2828865], dtype=float32), -2.3253298]. 
=============================================
[2019-04-01 18:35:57,380] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:35:57,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:35:57,391] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run23
[2019-04-01 18:35:57,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:35:57,859] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:35:57,876] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run23
[2019-04-01 18:35:58,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:35:58,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:35:58,736] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run23
[2019-04-01 18:35:59,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:35:59,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:35:59,282] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run23
[2019-04-01 18:35:59,402] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:35:59,402] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:35:59,416] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run23
[2019-04-01 18:35:59,453] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:35:59,454] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:35:59,467] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run23
[2019-04-01 18:35:59,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:35:59,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:36:00,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run23
[2019-04-01 18:36:00,250] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:36:00,251] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:36:00,254] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run23
[2019-04-01 18:36:00,665] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:36:00,665] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:36:00,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run23
[2019-04-01 18:36:01,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:36:01,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:36:01,382] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run23
[2019-04-01 18:36:03,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:36:03,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:36:03,071] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run23
[2019-04-01 18:36:12,625] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:36:12,626] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7725
[2019-04-01 18:36:12,682] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.7, 89.0, 0.0, 0.0, 26.0, 24.8056013876297, 7.062927695475149, 0.0, 1.0, 40.77255798413881], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 72000.0000, 
sim time next is 72600.0000, 
raw observation next is [2.516666666666667, 88.33333333333334, 0.0, 0.0, 26.0, 24.78844229428941, 7.000993898601455, 0.0, 1.0, 40.69958456262905], 
processed observation next is [0.0, 0.8695652173913043, 0.5323176361957526, 0.8833333333333334, 0.0, 0.0, 1.0, 0.8269203277556301, 0.07000993898601454, 0.0, 1.0, 0.2907113183044932], 
reward next is 0.7093, 
noisyNet noise sample is [array([0.38842613], dtype=float32), -0.056219716]. 
=============================================
[2019-04-01 18:36:17,503] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:36:17,504] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3111
[2019-04-01 18:36:17,574] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.1, 66.0, 0.0, 0.0, 26.0, 24.9014403999734, 8.05759509318226, 1.0, 1.0, 72.94357400251138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 156600.0000, 
sim time next is 157200.0000, 
raw observation next is [-8.2, 66.66666666666666, 0.0, 0.0, 26.0, 24.94198549835978, 8.425200701739776, 1.0, 1.0, 66.9397003478627], 
processed observation next is [1.0, 0.8260869565217391, 0.23545706371191139, 0.6666666666666665, 0.0, 0.0, 1.0, 0.8488550711942544, 0.08425200701739775, 1.0, 1.0, 0.47814071677044784], 
reward next is 0.5219, 
noisyNet noise sample is [array([0.5972228], dtype=float32), -0.6189966]. 
=============================================
[2019-04-01 18:36:18,879] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 4.922442e-35], sum to 1.0000
[2019-04-01 18:36:18,879] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3705
[2019-04-01 18:36:18,896] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 74.0, 0.0, 0.0, 26.0, 23.67427025161145, 5.443778312634905, 0.0, 1.0, 43.82416185545342], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 177000.0000, 
sim time next is 177600.0000, 
raw observation next is [-8.900000000000002, 74.0, 0.0, 0.0, 26.0, 23.65071702428297, 5.436285399474168, 0.0, 1.0, 43.80436547332231], 
processed observation next is [1.0, 0.043478260869565216, 0.2160664819944598, 0.74, 0.0, 0.0, 1.0, 0.6643881463261384, 0.05436285399474168, 0.0, 1.0, 0.31288832480944506], 
reward next is 0.6871, 
noisyNet noise sample is [array([0.5401281], dtype=float32), -0.8165049]. 
=============================================
[2019-04-01 18:36:32,005] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-01 18:36:32,005] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 18:36:32,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:36:32,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run31
[2019-04-01 18:36:32,035] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 18:36:32,036] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:36:32,038] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 18:36:32,039] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:36:32,350] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run31
[2019-04-01 18:36:32,638] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run31
[2019-04-01 18:36:50,762] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.823586], dtype=float32), -1.2339575]
[2019-04-01 18:36:50,762] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-10.26758918266667, 40.93761413666667, 0.0, 0.0, 26.0, 25.13907773421621, 7.968698282474167, 1.0, 1.0, 58.53938042964367]
[2019-04-01 18:36:50,762] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 18:36:50,763] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.3576061556947264
[2019-04-01 18:37:32,717] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.823586], dtype=float32), -1.2339575]
[2019-04-01 18:37:32,718] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-8.911451544666667, 74.19137756333333, 140.4379789333333, 0.0, 26.0, 25.68850598304069, 8.977776852752662, 1.0, 1.0, 26.07286532664465]
[2019-04-01 18:37:32,718] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 18:37:32,718] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.6733977593268603
[2019-04-01 18:38:10,853] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.823586], dtype=float32), -1.2339575]
[2019-04-01 18:38:10,853] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [9.885384287, 28.34397408333334, 0.0, 0.0, 26.0, 25.59255322779917, 7.868350230216571, 0.0, 1.0, 22.05882376978537]
[2019-04-01 18:38:10,854] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 18:38:10,854] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.40439451946610927
[2019-04-01 18:38:15,120] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 18:38:33,613] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 18:38:38,122] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 18:38:39,146] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 3000000, evaluation results [3000000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 18:38:46,653] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:38:46,726] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8238
[2019-04-01 18:38:46,796] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.517497301893, 8.916479460198232, 1.0, 1.0, 45.9578351100005], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 411000.0000, 
sim time next is 411600.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.53478380127111, 8.819498909094163, 1.0, 1.0, 46.31610061296826], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 1.0, 0.9335405430387301, 0.08819498909094163, 1.0, 1.0, 0.33082929009263046], 
reward next is 0.6692, 
noisyNet noise sample is [array([-0.8034978], dtype=float32), -0.16996792]. 
=============================================
[2019-04-01 18:38:50,745] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:38:50,746] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6277
[2019-04-01 18:38:50,770] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 49.0, 0.0, 0.0, 26.0, 24.36407771691413, 6.097339471203955, 0.0, 1.0, 44.27429164142895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 424800.0000, 
sim time next is 425400.0000, 
raw observation next is [-10.78333333333333, 49.83333333333334, 0.0, 0.0, 26.0, 24.31658126604298, 6.020752366060631, 0.0, 1.0, 44.20422245377812], 
processed observation next is [1.0, 0.9565217391304348, 0.1638965835641737, 0.4983333333333334, 0.0, 0.0, 1.0, 0.7595116094347115, 0.060207523660606305, 0.0, 1.0, 0.3157444460984152], 
reward next is 0.6843, 
noisyNet noise sample is [array([0.3862611], dtype=float32), -0.090668626]. 
=============================================
[2019-04-01 18:38:59,737] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 7.5803103e-31], sum to 1.0000
[2019-04-01 18:38:59,738] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9846
[2019-04-01 18:38:59,779] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8666666666666667, 80.83333333333333, 116.3333333333333, 309.0000000000001, 26.0, 24.99458237001079, 7.842182409663512, 0.0, 1.0, 34.97547072353854], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 562200.0000, 
sim time next is 562800.0000, 
raw observation next is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 26.0, 25.033002932473, 7.877333520382007, 0.0, 1.0, 29.46964076647922], 
processed observation next is [0.0, 0.5217391304347826, 0.4367497691597415, 0.8066666666666668, 0.4105555555555557, 0.38950276243093923, 1.0, 0.8618575617818571, 0.07877333520382007, 0.0, 1.0, 0.21049743404628013], 
reward next is 0.7895, 
noisyNet noise sample is [array([0.7550517], dtype=float32), 1.7450105]. 
=============================================
[2019-04-01 18:39:01,474] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:01,475] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7536
[2019-04-01 18:39:01,490] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.30306326310927, 5.783415231289503, 0.0, 1.0, 41.25975499347008], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 778200.0000, 
sim time next is 778800.0000, 
raw observation next is [-7.300000000000001, 71.0, 0.0, 0.0, 26.0, 24.26668353999686, 5.728874374758942, 0.0, 1.0, 41.21104207854722], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.71, 0.0, 0.0, 1.0, 0.752383362856694, 0.057288743747589424, 0.0, 1.0, 0.2943645862753373], 
reward next is 0.7056, 
noisyNet noise sample is [array([-1.0680093], dtype=float32), 0.24779296]. 
=============================================
[2019-04-01 18:39:05,697] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:05,706] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5334
[2019-04-01 18:39:05,720] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.783333333333333, 81.66666666666667, 0.0, 0.0, 26.0, 25.57412562174094, 9.729374997878267, 0.0, 1.0, 26.64018719780482], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 958200.0000, 
sim time next is 958800.0000, 
raw observation next is [6.966666666666667, 81.33333333333334, 0.0, 0.0, 26.0, 25.57015575969058, 9.618777266593503, 0.0, 1.0, 25.73228414160277], 
processed observation next is [1.0, 0.08695652173913043, 0.6555863342566944, 0.8133333333333335, 0.0, 0.0, 1.0, 0.9385936799557969, 0.09618777266593502, 0.0, 1.0, 0.18380202958287692], 
reward next is 0.8162, 
noisyNet noise sample is [array([0.68403715], dtype=float32), -1.9777107]. 
=============================================
[2019-04-01 18:39:06,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:06,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5689
[2019-04-01 18:39:06,488] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.6, 83.0, 0.0, 0.0, 26.0, 25.51306895348813, 9.080092137282202, 0.0, 1.0, 25.22938388532856], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 974400.0000, 
sim time next is 975000.0000, 
raw observation next is [9.8, 83.0, 0.0, 0.0, 26.0, 25.41522007992589, 9.139808473974098, 0.0, 1.0, 30.11502859714143], 
processed observation next is [1.0, 0.2608695652173913, 0.7340720221606649, 0.83, 0.0, 0.0, 1.0, 0.9164600114179845, 0.09139808473974098, 0.0, 1.0, 0.21510734712243879], 
reward next is 0.7849, 
noisyNet noise sample is [array([-0.61277133], dtype=float32), 0.35593164]. 
=============================================
[2019-04-01 18:39:06,495] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[75.81453 ]
 [75.82832 ]
 [75.83529 ]
 [75.85429 ]
 [75.865234]], R is [[75.86325836]
 [75.92442322]
 [76.01625061]
 [76.10215759]
 [76.17318726]].
[2019-04-01 18:39:10,918] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:10,918] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8565
[2019-04-01 18:39:10,933] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.383333333333333, 74.83333333333333, 0.0, 0.0, 26.0, 23.84114861327182, 5.269548689988146, 0.0, 1.0, 40.76977765631462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 791400.0000, 
sim time next is 792000.0000, 
raw observation next is [-7.3, 75.0, 0.0, 0.0, 26.0, 23.87619927360952, 5.255571656149795, 0.0, 1.0, 40.7873642523077], 
processed observation next is [1.0, 0.17391304347826086, 0.26038781163434904, 0.75, 0.0, 0.0, 1.0, 0.6965998962299312, 0.05255571656149795, 0.0, 1.0, 0.2913383160879121], 
reward next is 0.7087, 
noisyNet noise sample is [array([0.83033764], dtype=float32), 0.86074376]. 
=============================================
[2019-04-01 18:39:10,954] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[69.18283]
 [69.12721]
 [69.06269]
 [68.98514]
 [68.9241 ]], R is [[69.25891113]
 [69.27510834]
 [69.29141235]
 [69.30783081]
 [69.32415009]].
[2019-04-01 18:39:13,900] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:13,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9014
[2019-04-01 18:39:13,926] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.2, 46.33333333333334, 84.16666666666667, 144.8333333333333, 26.0, 25.69112476025266, 8.596592003409391, 1.0, 1.0, 19.18000043810744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 746400.0000, 
sim time next is 747000.0000, 
raw observation next is [-0.3, 46.0, 85.0, 31.0, 26.0, 25.37060078453076, 8.071587497927348, 1.0, 1.0, 20.19674205499486], 
processed observation next is [1.0, 0.6521739130434783, 0.4542936288088643, 0.46, 0.2833333333333333, 0.03425414364640884, 1.0, 0.9100858263615373, 0.08071587497927349, 1.0, 1.0, 0.1442624432499633], 
reward next is 0.8557, 
noisyNet noise sample is [array([-0.16910818], dtype=float32), -1.6204219]. 
=============================================
[2019-04-01 18:39:13,935] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[77.78703 ]
 [77.84658 ]
 [77.745224]
 [77.54384 ]
 [77.246056]], R is [[77.94952393]
 [78.03302765]
 [78.19123077]
 [78.3469162 ]
 [78.4985733 ]].
[2019-04-01 18:39:17,064] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 3.45968e-31 1.00000e+00
 0.00000e+00], sum to 1.0000
[2019-04-01 18:39:17,065] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0314
[2019-04-01 18:39:17,092] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.8, 79.16666666666667, 0.0, 0.0, 26.0, 24.69386054414277, 6.089306837947876, 0.0, 1.0, 38.9960869921306], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 870600.0000, 
sim time next is 871200.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 26.0, 24.63206714098287, 6.088372710043143, 0.0, 1.0, 39.00542726968148], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.79, 0.0, 0.0, 1.0, 0.8045810201404099, 0.06088372710043143, 0.0, 1.0, 0.2786101947834392], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.31077296], dtype=float32), -0.9034351]. 
=============================================
[2019-04-01 18:39:21,191] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3754175e-35
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:39:21,191] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1764
[2019-04-01 18:39:21,257] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 26.316112202981, 10.71540631194812, 1.0, 1.0, 30.7947106853804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 839400.0000, 
sim time next is 840000.0000, 
raw observation next is [-3.9, 84.66666666666667, 0.0, 0.0, 26.0, 26.39356590335213, 9.863495826896282, 1.0, 1.0, 29.38500018078838], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8466666666666667, 0.0, 0.0, 1.0, 1.0562237004788757, 0.09863495826896282, 1.0, 1.0, 0.20989285843420272], 
reward next is 0.7901, 
noisyNet noise sample is [array([0.56060696], dtype=float32), 1.9223874]. 
=============================================
[2019-04-01 18:39:21,263] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[62.84157 ]
 [62.807774]
 [62.57864 ]
 [62.272324]
 [62.923298]], R is [[63.01211929]
 [62.87587357]
 [62.79969788]
 [62.85487366]
 [62.65000153]].
[2019-04-01 18:39:23,120] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:23,120] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0332
[2019-04-01 18:39:23,125] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.0, 100.0, 74.66666666666667, 0.0, 26.0, 23.3089229688855, 5.771133921051347, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1244400.0000, 
sim time next is 1245000.0000, 
raw observation next is [15.0, 100.0, 75.33333333333333, 0.0, 26.0, 23.30393391737881, 5.769857934928265, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.8781163434903049, 1.0, 0.2511111111111111, 0.0, 1.0, 0.6148477024826872, 0.057698579349282644, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2988247], dtype=float32), -1.0658591]. 
=============================================
[2019-04-01 18:39:23,133] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[90.64546 ]
 [90.769714]
 [90.81593 ]
 [90.84261 ]
 [90.823235]], R is [[90.62982941]
 [90.72353363]
 [90.81629944]
 [90.90813446]
 [90.99905396]].
[2019-04-01 18:39:26,803] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:26,806] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7939
[2019-04-01 18:39:26,814] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 96.0, 0.0, 0.0, 26.0, 25.23578023742223, 8.051065396689133, 1.0, 1.0, 8.447281450100263], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 928800.0000, 
sim time next is 929400.0000, 
raw observation next is [4.4, 96.66666666666666, 0.0, 0.0, 26.0, 25.17375024613025, 7.695826029014943, 1.0, 1.0, 8.873959458703185], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.9666666666666666, 0.0, 0.0, 1.0, 0.88196432087575, 0.07695826029014943, 1.0, 1.0, 0.06338542470502274], 
reward next is 0.9366, 
noisyNet noise sample is [array([0.7357996], dtype=float32), -1.1519314]. 
=============================================
[2019-04-01 18:39:29,465] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6500610e-38
 1.0000000e+00 2.3075145e-36], sum to 1.0000
[2019-04-01 18:39:29,474] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1422
[2019-04-01 18:39:29,496] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.30267883808115, 9.156603361474097, 0.0, 1.0, 38.18934219721975], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 948000.0000, 
sim time next is 948600.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.30645146447527, 9.163073705331206, 0.0, 1.0, 38.08643495367979], 
processed observation next is [1.0, 1.0, 0.6011080332409973, 0.96, 0.0, 0.0, 1.0, 0.9009216377821814, 0.09163073705331205, 0.0, 1.0, 0.27204596395485564], 
reward next is 0.7280, 
noisyNet noise sample is [array([0.68730694], dtype=float32), -0.64753604]. 
=============================================
[2019-04-01 18:39:30,253] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:30,256] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2044
[2019-04-01 18:39:30,265] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 26.05650279431002, 13.89292328087536, 0.0, 1.0, 18.93067044176017], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1030800.0000, 
sim time next is 1031400.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 26.05251211024114, 13.68191926945809, 0.0, 1.0, 17.93058002340593], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 1.0, 1.0075017300344484, 0.1368191926945809, 0.0, 1.0, 0.12807557159575664], 
reward next is 0.8719, 
noisyNet noise sample is [array([-0.8838973], dtype=float32), 0.16313207]. 
=============================================
[2019-04-01 18:39:34,142] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:34,145] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8181
[2019-04-01 18:39:34,152] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.9, 51.5, 0.0, 0.0, 26.0, 26.93471133282161, 20.34866065226016, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1099800.0000, 
sim time next is 1100400.0000, 
raw observation next is [16.63333333333333, 52.0, 0.0, 0.0, 26.0, 26.9889761618761, 20.62694272124536, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.9233610341643583, 0.52, 0.0, 0.0, 1.0, 1.1412823088394428, 0.2062694272124536, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.9392753], dtype=float32), 1.0023774]. 
=============================================
[2019-04-01 18:39:36,860] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:36,861] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2329
[2019-04-01 18:39:36,876] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.7, 83.33333333333333, 11.0, 0.6666666666666667, 26.0, 25.6940251468567, 13.02911425184399, 0.0, 1.0, 20.85627642408646], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1151400.0000, 
sim time next is 1152000.0000, 
raw observation next is [12.7, 84.0, 16.0, 0.5, 26.0, 25.67812562633928, 12.95816251077599, 0.0, 1.0, 20.66888012796012], 
processed observation next is [0.0, 0.34782608695652173, 0.8144044321329641, 0.84, 0.05333333333333334, 0.0005524861878453039, 1.0, 0.9540179466198973, 0.1295816251077599, 0.0, 1.0, 0.147634858056858], 
reward next is 0.8524, 
noisyNet noise sample is [array([0.13579114], dtype=float32), 0.34835005]. 
=============================================
[2019-04-01 18:39:36,892] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[88.34822]
 [88.28636]
 [88.27909]
 [88.26053]
 [88.21394]], R is [[88.3261261 ]
 [88.29389191]
 [88.25850677]
 [88.22678375]
 [88.19485474]].
[2019-04-01 18:39:38,978] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:38,980] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0531
[2019-04-01 18:39:38,991] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.0, 84.66666666666667, 0.0, 0.0, 26.0, 23.95545373829711, 6.716622846880976, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1217400.0000, 
sim time next is 1218000.0000, 
raw observation next is [15.9, 86.33333333333334, 0.0, 0.0, 26.0, 24.01153647143794, 6.718987122364173, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9030470914127425, 0.8633333333333334, 0.0, 0.0, 1.0, 0.7159337816339912, 0.06718987122364173, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8745862], dtype=float32), 0.044360407]. 
=============================================
[2019-04-01 18:39:39,004] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[95.32967]
 [95.12747]
 [94.97812]
 [94.8783 ]
 [94.85981]], R is [[95.54869843]
 [95.59321594]
 [95.63728333]
 [95.6809082 ]
 [95.72409821]].
[2019-04-01 18:39:41,392] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:41,396] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2818
[2019-04-01 18:39:41,414] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.516666666666667, 92.0, 0.0, 0.0, 26.0, 25.37471781395935, 10.49738293032172, 0.0, 1.0, 36.22509352659493], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1318200.0000, 
sim time next is 1318800.0000, 
raw observation next is [1.433333333333334, 92.0, 0.0, 0.0, 26.0, 25.30307684907947, 10.47535112300256, 0.0, 1.0, 40.2687255452119], 
processed observation next is [1.0, 0.2608695652173913, 0.502308402585411, 0.92, 0.0, 0.0, 1.0, 0.9004395498684958, 0.1047535112300256, 0.0, 1.0, 0.28763375389437074], 
reward next is 0.7124, 
noisyNet noise sample is [array([-1.0073944], dtype=float32), 0.27894026]. 
=============================================
[2019-04-01 18:39:42,949] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:42,950] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9801
[2019-04-01 18:39:42,965] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.133333333333334, 98.66666666666667, 0.0, 0.0, 26.0, 25.51629808567169, 12.19230998711185, 0.0, 1.0, 29.42928485543951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1293600.0000, 
sim time next is 1294200.0000, 
raw observation next is [4.95, 98.0, 0.0, 0.0, 26.0, 25.45654797073655, 12.00363306260695, 0.0, 1.0, 31.97915227642387], 
processed observation next is [0.0, 1.0, 0.5997229916897507, 0.98, 0.0, 0.0, 1.0, 0.9223639958195073, 0.1200363306260695, 0.0, 1.0, 0.22842251626017052], 
reward next is 0.7716, 
noisyNet noise sample is [array([0.32467088], dtype=float32), -1.460023]. 
=============================================
[2019-04-01 18:39:51,502] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4191667e-37
 1.0000000e+00 5.1796698e-22], sum to 1.0000
[2019-04-01 18:39:51,503] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6132
[2019-04-01 18:39:51,515] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.12844916473033, 9.647341542167473, 0.0, 1.0, 39.4306956720702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1465200.0000, 
sim time next is 1465800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.19098541364547, 9.76248978935917, 0.0, 1.0, 38.35076095927654], 
processed observation next is [1.0, 1.0, 0.5069252077562327, 0.92, 0.0, 0.0, 1.0, 0.8844264876636387, 0.0976248978935917, 0.0, 1.0, 0.2739340068519753], 
reward next is 0.7261, 
noisyNet noise sample is [array([0.26945767], dtype=float32), 1.778378]. 
=============================================
[2019-04-01 18:39:52,541] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:52,542] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7610
[2019-04-01 18:39:52,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 8.509805e-17], sum to 1.0000
[2019-04-01 18:39:52,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2698
[2019-04-01 18:39:52,551] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.616666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 25.59445022451502, 12.42232028520633, 1.0, 1.0, 5.808047986217054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1627800.0000, 
sim time next is 1628400.0000, 
raw observation next is [7.533333333333333, 74.66666666666667, 0.0, 0.0, 26.0, 25.51132877619983, 12.77276038966605, 0.0, 1.0, 27.99757096910532], 
processed observation next is [1.0, 0.8695652173913043, 0.6712834718374886, 0.7466666666666667, 0.0, 0.0, 1.0, 0.9301898251714041, 0.1277276038966605, 0.0, 1.0, 0.1999826497793237], 
reward next is 0.8000, 
noisyNet noise sample is [array([-1.8033596], dtype=float32), 0.41660744]. 
=============================================
[2019-04-01 18:39:52,566] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.47977803397137, 10.91155179616227, 0.0, 1.0, 25.67762376373662], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1653600.0000, 
sim time next is 1654200.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.45266252202641, 11.14056767911342, 0.0, 1.0, 35.34539020205521], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 1.0, 0.9218089317180588, 0.11140567679113421, 0.0, 1.0, 0.25246707287182296], 
reward next is 0.7475, 
noisyNet noise sample is [array([-0.2798029], dtype=float32), -0.86246866]. 
=============================================
[2019-04-01 18:39:53,113] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:53,115] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6216
[2019-04-01 18:39:53,125] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.49943778619404, 11.78130403316245, 0.0, 1.0, 35.1198739656619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1557600.0000, 
sim time next is 1558200.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.55067566316252, 12.12245314088702, 0.0, 1.0, 32.84678511740476], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 1.0, 0.935810809023217, 0.1212245314088702, 0.0, 1.0, 0.23461989369574826], 
reward next is 0.7654, 
noisyNet noise sample is [array([-1.5635102], dtype=float32), 0.8836619]. 
=============================================
[2019-04-01 18:39:54,448] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:39:54,449] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5656
[2019-04-01 18:39:54,468] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.516666666666667, 69.33333333333334, 143.3333333333333, 120.0, 26.0, 26.3367952098745, 13.72251121203739, 1.0, 1.0, 9.641559822675697], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1590600.0000, 
sim time next is 1591200.0000, 
raw observation next is [7.7, 68.0, 157.5, 112.0, 26.0, 26.41060472107306, 14.17147857440405, 1.0, 1.0, 8.971444672572918], 
processed observation next is [1.0, 0.43478260869565216, 0.6759002770083103, 0.68, 0.525, 0.12375690607734807, 1.0, 1.0586578172961514, 0.1417147857440405, 1.0, 1.0, 0.06408174766123513], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0644388], dtype=float32), -0.1888566]. 
=============================================
[2019-04-01 18:40:02,747] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:40:02,748] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6551
[2019-04-01 18:40:02,776] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.35, 84.5, 53.0, 0.0, 26.0, 25.52132336527103, 11.39867250228164, 1.0, 1.0, 19.78136107701812], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1697400.0000, 
sim time next is 1698000.0000, 
raw observation next is [1.433333333333333, 83.33333333333334, 49.16666666666667, 0.0, 26.0, 25.76749131835683, 11.86387981114954, 1.0, 1.0, 17.52415086049668], 
processed observation next is [1.0, 0.6521739130434783, 0.502308402585411, 0.8333333333333335, 0.16388888888888892, 0.0, 1.0, 0.9667844740509756, 0.1186387981114954, 1.0, 1.0, 0.12517250614640485], 
reward next is 0.1293, 
noisyNet noise sample is [array([0.21667688], dtype=float32), 0.23224744]. 
=============================================
[2019-04-01 18:40:02,779] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[64.462395]
 [64.141655]
 [64.03205 ]
 [63.70314 ]
 [63.479954]], R is [[64.45116425]
 [64.10588837]
 [64.18118286]
 [63.53937149]
 [62.90398026]].
[2019-04-01 18:40:06,832] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0273537e-27 0.0000000e+00 2.0260130e-37 0.0000000e+00 4.4101647e-17
 9.9999964e-01 3.4338845e-07], sum to 1.0000
[2019-04-01 18:40:06,842] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3106
[2019-04-01 18:40:06,862] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.616666666666667, 78.16666666666666, 0.0, 0.0, 26.0, 23.37937606314491, 5.339191038770586, 0.0, 1.0, 46.45347999366135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1839000.0000, 
sim time next is 1839600.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 23.34591030508214, 5.36210152991881, 0.0, 1.0, 46.49046998405801], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 1.0, 0.6208443292974485, 0.0536210152991881, 0.0, 1.0, 0.33207478560041437], 
reward next is 0.6679, 
noisyNet noise sample is [array([-0.7332177], dtype=float32), 0.0715833]. 
=============================================
[2019-04-01 18:40:07,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:40:07,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7893
[2019-04-01 18:40:07,262] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 83.0, 109.0, 0.0, 26.0, 25.02786648608945, 8.116940920719017, 0.0, 1.0, 37.1800624120525], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1778400.0000, 
sim time next is 1779000.0000, 
raw observation next is [-2.8, 83.66666666666667, 105.6666666666667, 0.0, 26.0, 24.97701027130733, 8.02329586302398, 0.0, 1.0, 38.60305722903673], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8366666666666667, 0.3522222222222223, 0.0, 1.0, 0.8538586101867612, 0.0802329586302398, 0.0, 1.0, 0.2757361230645481], 
reward next is 0.7243, 
noisyNet noise sample is [array([-0.80338365], dtype=float32), -1.6563394]. 
=============================================
[2019-04-01 18:40:07,267] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[65.69783 ]
 [65.62294 ]
 [65.59678 ]
 [65.63044 ]
 [65.562065]], R is [[65.89972687]
 [65.97515869]
 [66.05960846]
 [66.16197968]
 [66.28665924]].
[2019-04-01 18:40:14,032] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:40:14,034] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6406
[2019-04-01 18:40:14,086] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 71.0, 109.5, 225.5, 26.0, 25.73331845557789, 8.168864232998013, 1.0, 1.0, 20.36294083174532], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2196000.0000, 
sim time next is 2196600.0000, 
raw observation next is [-4.916666666666667, 71.0, 112.0, 150.3333333333333, 26.0, 25.74843240050721, 8.036583091309657, 1.0, 1.0, 19.74470981953677], 
processed observation next is [1.0, 0.43478260869565216, 0.32640812557710064, 0.71, 0.37333333333333335, 0.16611418047882132, 1.0, 0.9640617715010299, 0.08036583091309657, 1.0, 1.0, 0.1410336415681198], 
reward next is 0.8590, 
noisyNet noise sample is [array([-0.3189359], dtype=float32), -0.2377029]. 
=============================================
[2019-04-01 18:40:18,208] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8445940e-25 3.1026743e-35 1.9104651e-22 9.8811822e-19 8.2656965e-15
 1.0000000e+00 4.0654961e-20], sum to 1.0000
[2019-04-01 18:40:18,209] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6341
[2019-04-01 18:40:18,262] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.199999999999999, 87.0, 0.0, 0.0, 26.0, 25.44588040215277, 7.418602738515598, 1.0, 1.0, 47.37768907200511], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2014800.0000, 
sim time next is 2015400.0000, 
raw observation next is [-6.2, 87.0, 10.33333333333333, 0.0, 26.0, 25.54317133739874, 7.401918345321353, 1.0, 1.0, 44.6190256917519], 
processed observation next is [1.0, 0.30434782608695654, 0.2908587257617729, 0.87, 0.03444444444444444, 0.0, 1.0, 0.9347387624855342, 0.07401918345321352, 1.0, 1.0, 0.31870732636965643], 
reward next is 0.6813, 
noisyNet noise sample is [array([-1.2125448], dtype=float32), -0.80634737]. 
=============================================
[2019-04-01 18:40:31,887] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:40:31,889] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8189
[2019-04-01 18:40:31,938] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.39505537652474, 5.792396797327283, 0.0, 1.0, 41.96622599628738], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2169600.0000, 
sim time next is 2170200.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.44193353314582, 5.782480835079276, 0.0, 1.0, 41.86045847063169], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 1.0, 0.7774190761636888, 0.057824808350792754, 0.0, 1.0, 0.29900327479022637], 
reward next is 0.7010, 
noisyNet noise sample is [array([-0.2880495], dtype=float32), -1.4206223]. 
=============================================
[2019-04-01 18:40:35,699] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:40:35,700] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2706
[2019-04-01 18:40:35,726] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.7, 29.0, 0.0, 0.0, 26.0, 25.12605595783669, 6.939041235666735, 0.0, 1.0, 40.29322841495899], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2491200.0000, 
sim time next is 2491800.0000, 
raw observation next is [-0.7833333333333333, 30.33333333333334, 0.0, 0.0, 26.0, 25.16965265123324, 6.956215992846713, 0.0, 1.0, 40.08641080633554], 
processed observation next is [0.0, 0.8695652173913043, 0.44090489381348114, 0.3033333333333334, 0.0, 0.0, 1.0, 0.881378950176177, 0.06956215992846712, 0.0, 1.0, 0.2863315057595396], 
reward next is 0.7137, 
noisyNet noise sample is [array([-0.40341666], dtype=float32), 1.2632048]. 
=============================================
[2019-04-01 18:40:44,331] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.0465667e-30], sum to 1.0000
[2019-04-01 18:40:44,332] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1140
[2019-04-01 18:40:44,403] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.1, 67.0, 121.0, 360.0, 26.0, 25.43554783231932, 8.076922516515282, 0.0, 1.0, 34.37727031885241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2367000.0000, 
sim time next is 2367600.0000, 
raw observation next is [-3.0, 66.33333333333333, 124.0, 375.0, 26.0, 25.38037881226803, 7.96840074243174, 0.0, 1.0, 32.17091027512686], 
processed observation next is [0.0, 0.391304347826087, 0.3795013850415513, 0.6633333333333333, 0.41333333333333333, 0.4143646408839779, 1.0, 0.9114826874668616, 0.0796840074243174, 0.0, 1.0, 0.22979221625090612], 
reward next is 0.7702, 
noisyNet noise sample is [array([0.71479756], dtype=float32), -1.0601742]. 
=============================================
[2019-04-01 18:40:49,957] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:40:49,957] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6938
[2019-04-01 18:40:49,993] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 26.0, 24.45429901996039, 5.853688094374948, 0.0, 1.0, 39.97992158969673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2348400.0000, 
sim time next is 2349000.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 26.0, 24.41903987798079, 5.796411384907482, 0.0, 1.0, 40.09594926035573], 
processed observation next is [0.0, 0.17391304347826086, 0.37673130193905824, 0.67, 0.0, 0.0, 1.0, 0.7741485539972557, 0.05796411384907482, 0.0, 1.0, 0.2863996375739695], 
reward next is 0.7136, 
noisyNet noise sample is [array([-0.3268194], dtype=float32), -1.951622]. 
=============================================
[2019-04-01 18:40:50,011] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[69.38185]
 [69.43304]
 [69.4689 ]
 [69.51756]
 [69.57445]], R is [[69.36082458]
 [69.3816452 ]
 [69.40315247]
 [69.42504883]
 [69.44747925]].
[2019-04-01 18:40:57,375] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:40:57,376] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5171
[2019-04-01 18:40:57,392] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 56.0, 0.0, 0.0, 26.0, 25.35617237830856, 8.56506026931841, 0.0, 1.0, 44.35442753344492], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2846400.0000, 
sim time next is 2847000.0000, 
raw observation next is [2.0, 59.0, 0.0, 0.0, 26.0, 25.35525307002188, 8.574098896851423, 0.0, 1.0, 43.90028662355181], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.59, 0.0, 0.0, 1.0, 0.9078932957174116, 0.08574098896851423, 0.0, 1.0, 0.3135734758825129], 
reward next is 0.6864, 
noisyNet noise sample is [array([0.05378325], dtype=float32), -0.644008]. 
=============================================
[2019-04-01 18:40:57,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[79.1952  ]
 [79.09378 ]
 [79.10734 ]
 [79.09012 ]
 [79.039215]], R is [[78.91233063]
 [78.80638885]
 [78.70171356]
 [78.59189606]
 [78.46779633]].
[2019-04-01 18:40:58,092] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:40:58,092] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2414
[2019-04-01 18:40:58,098] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.466666666666667, 28.0, 151.5, 287.6666666666667, 26.0, 25.88510619180035, 8.62281071674895, 1.0, 1.0, 6.037328760502418], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2558400.0000, 
sim time next is 2559000.0000, 
raw observation next is [3.383333333333333, 28.5, 144.0, 300.3333333333333, 26.0, 25.86428836453453, 8.58165972657568, 1.0, 1.0, 6.064035474532193], 
processed observation next is [1.0, 0.6086956521739131, 0.5563250230840259, 0.285, 0.48, 0.3318600368324125, 1.0, 0.980612623504933, 0.0858165972657568, 1.0, 1.0, 0.043314539103801376], 
reward next is 0.9567, 
noisyNet noise sample is [array([0.701964], dtype=float32), -0.40125757]. 
=============================================
[2019-04-01 18:40:58,111] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.43065 ]
 [79.004135]
 [79.392555]
 [79.70572 ]
 [79.85405 ]], R is [[78.04416656]
 [78.22060394]
 [78.39640808]
 [78.57144165]
 [78.74481964]].
[2019-04-01 18:40:58,553] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:40:58,557] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0208
[2019-04-01 18:40:58,584] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 62.0, 0.0, 0.0, 26.0, 24.78787185562146, 6.616194306575778, 0.0, 1.0, 41.3733533852574], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2592000.0000, 
sim time next is 2592600.0000, 
raw observation next is [-4.583333333333333, 63.0, 0.0, 0.0, 26.0, 24.7527857413864, 6.523280914854229, 0.0, 1.0, 41.37637464288872], 
processed observation next is [1.0, 0.0, 0.3356417359187443, 0.63, 0.0, 0.0, 1.0, 0.8218265344837715, 0.06523280914854229, 0.0, 1.0, 0.29554553316349086], 
reward next is 0.7045, 
noisyNet noise sample is [array([-0.63331574], dtype=float32), -1.1101367]. 
=============================================
[2019-04-01 18:41:05,155] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 7.0126647e-26], sum to 1.0000
[2019-04-01 18:41:05,159] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9942
[2019-04-01 18:41:05,174] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.64600860385464, 9.828502868148533, 0.0, 1.0, 30.70208489946398], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2842200.0000, 
sim time next is 2842800.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.58370184445493, 9.533859925398316, 0.0, 1.0, 28.77015924832596], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44, 0.0, 0.0, 1.0, 0.9405288349221327, 0.09533859925398316, 0.0, 1.0, 0.20550113748804258], 
reward next is 0.7945, 
noisyNet noise sample is [array([-1.220333], dtype=float32), 0.58929443]. 
=============================================
[2019-04-01 18:41:16,751] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0066092e-31 0.0000000e+00 4.9141117e-35 0.0000000e+00 4.0958241e-14
 1.0000000e+00 1.3774251e-25], sum to 1.0000
[2019-04-01 18:41:16,752] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7082
[2019-04-01 18:41:16,776] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.29041985946289, 12.48069292545994, 0.0, 1.0, 42.62794736610741], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3193800.0000, 
sim time next is 3194400.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.31690589277016, 12.81357500446724, 0.0, 1.0, 41.47564214476059], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 1.0, 0.9024151275385941, 0.1281357500446724, 0.0, 1.0, 0.2962545867482899], 
reward next is 0.7037, 
noisyNet noise sample is [array([1.3031975], dtype=float32), -1.2211332]. 
=============================================
[2019-04-01 18:41:24,141] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:41:24,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0796
[2019-04-01 18:41:24,155] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 80.5, 0.0, 0.0, 26.0, 24.22375009161866, 5.909204394151395, 0.0, 1.0, 42.123201912117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2957400.0000, 
sim time next is 2958000.0000, 
raw observation next is [-3.666666666666667, 79.33333333333333, 0.0, 0.0, 26.0, 24.21712671929794, 5.860088318993533, 0.0, 1.0, 42.08043737915456], 
processed observation next is [0.0, 0.21739130434782608, 0.3610341643582641, 0.7933333333333333, 0.0, 0.0, 1.0, 0.745303817042563, 0.05860088318993533, 0.0, 1.0, 0.30057455270824684], 
reward next is 0.6994, 
noisyNet noise sample is [array([-0.10837706], dtype=float32), 0.8292713]. 
=============================================
[2019-04-01 18:41:24,160] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.30992]
 [71.41162]
 [71.48204]
 [71.49599]
 [71.4684 ]], R is [[71.18421936]
 [71.17150116]
 [71.15862274]
 [71.1455307 ]
 [71.13201904]].
[2019-04-01 18:41:26,276] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:41:26,276] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6741
[2019-04-01 18:41:26,289] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.03695972377248, 7.122191757578785, 0.0, 1.0, 37.88291308183561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3019800.0000, 
sim time next is 3020400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.9979169480612, 7.023804176368858, 0.0, 1.0, 37.81552059730338], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 1.0, 0.856845278294457, 0.07023804176368859, 0.0, 1.0, 0.2701108614093099], 
reward next is 0.7299, 
noisyNet noise sample is [array([-0.12451538], dtype=float32), -1.8065826]. 
=============================================
[2019-04-01 18:41:30,444] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:41:30,444] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5567
[2019-04-01 18:41:30,459] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 26.0, 25.40704126583783, 7.685884076396192, 0.0, 1.0, 38.29039955580002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3114000.0000, 
sim time next is 3114600.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 26.0, 25.47670740986487, 7.809559731809237, 0.0, 1.0, 36.86903659820997], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 1.0, 0.0, 0.0, 1.0, 0.9252439156949812, 0.07809559731809237, 0.0, 1.0, 0.26335026141578555], 
reward next is 0.7366, 
noisyNet noise sample is [array([-1.6973271], dtype=float32), -0.35084546]. 
=============================================
[2019-04-01 18:41:34,229] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:41:34,229] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2076
[2019-04-01 18:41:34,243] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 100.0, 0.0, 0.0, 26.0, 25.15768912660545, 6.843002239724392, 0.0, 1.0, 53.49155773918617], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3124200.0000, 
sim time next is 3124800.0000, 
raw observation next is [2.6, 100.0, 0.0, 0.0, 26.0, 25.16261853727389, 6.904066917098305, 0.0, 1.0, 53.1920351587466], 
processed observation next is [1.0, 0.17391304347826086, 0.5346260387811635, 1.0, 0.0, 0.0, 1.0, 0.8803740767534128, 0.06904066917098305, 0.0, 1.0, 0.3799431082767614], 
reward next is 0.6201, 
noisyNet noise sample is [array([-1.3154832], dtype=float32), -1.2057644]. 
=============================================
[2019-04-01 18:41:37,808] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:41:37,808] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7681
[2019-04-01 18:41:37,832] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.12276996162094, 8.154544313290641, 0.0, 1.0, 20.91269782302191], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3609000.0000, 
sim time next is 3609600.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.09843070673508, 8.01353995493586, 0.0, 1.0, 21.87939635193844], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 1.0, 0.87120438667644, 0.0801353995493586, 0.0, 1.0, 0.156281402513846], 
reward next is 0.8437, 
noisyNet noise sample is [array([0.37562293], dtype=float32), 1.3572973]. 
=============================================
[2019-04-01 18:41:37,890] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 2.8956387e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 18:41:37,894] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1448
[2019-04-01 18:41:37,910] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 100.0, 0.0, 0.0, 26.0, 25.17151642822511, 9.780770269034734, 0.0, 1.0, 40.03303477733664], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3216600.0000, 
sim time next is 3217200.0000, 
raw observation next is [-2.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.18060604946724, 9.676353098416003, 0.0, 1.0, 40.04971772954183], 
processed observation next is [1.0, 0.21739130434782608, 0.38873499538319484, 1.0, 0.0, 0.0, 1.0, 0.8829437213524629, 0.09676353098416003, 0.0, 1.0, 0.28606941235387023], 
reward next is 0.7139, 
noisyNet noise sample is [array([1.1735657], dtype=float32), 1.4095082]. 
=============================================
[2019-04-01 18:41:40,542] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5645397e-28 5.6338891e-38 9.3850589e-30 1.0767863e-37 6.4032008e-31
 9.9803966e-01 1.9603055e-03], sum to 1.0000
[2019-04-01 18:41:40,543] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6402
[2019-04-01 18:41:40,555] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.083333333333334, 76.83333333333334, 0.0, 0.0, 26.0, 24.59891491426735, 6.672461969235989, 0.0, 1.0, 43.02994136237265], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3298200.0000, 
sim time next is 3298800.0000, 
raw observation next is [-9.266666666666667, 76.66666666666667, 0.0, 0.0, 26.0, 24.59885512303654, 6.588126106808389, 0.0, 1.0, 43.18306700887435], 
processed observation next is [1.0, 0.17391304347826086, 0.20590951061865187, 0.7666666666666667, 0.0, 0.0, 1.0, 0.799836446148077, 0.06588126106808388, 0.0, 1.0, 0.3084504786348168], 
reward next is 0.6915, 
noisyNet noise sample is [array([-0.60958], dtype=float32), -1.3127216]. 
=============================================
[2019-04-01 18:41:41,609] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5484948e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 6.1738534e-09], sum to 1.0000
[2019-04-01 18:41:41,617] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5273
[2019-04-01 18:41:41,635] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.27214184192687, 9.955259383098527, 0.0, 1.0, 43.5896201759425], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3277800.0000, 
sim time next is 3278400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.24117774900171, 9.911211971298535, 0.0, 1.0, 43.28782009748021], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 1.0, 0.8915968212859587, 0.09911211971298535, 0.0, 1.0, 0.30919871498200147], 
reward next is 0.6908, 
noisyNet noise sample is [array([0.94101065], dtype=float32), 1.1578273]. 
=============================================
[2019-04-01 18:41:49,050] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:41:49,050] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1213
[2019-04-01 18:41:49,079] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 48.0, 99.66666666666666, 760.3333333333333, 26.0, 27.13106758680054, 15.58619737549502, 1.0, 1.0, 6.000044666759262], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3855000.0000, 
sim time next is 3855600.0000, 
raw observation next is [2.0, 48.0, 96.5, 749.5, 26.0, 26.5009059808753, 15.75877679148103, 1.0, 1.0, 5.740408390564404], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 0.48, 0.32166666666666666, 0.8281767955801105, 1.0, 1.0715579972679, 0.1575877679148103, 1.0, 1.0, 0.041002917075460034], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.06666759], dtype=float32), 1.0800658]. 
=============================================
[2019-04-01 18:41:50,346] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8001258e-22 1.2789121e-30 8.8815511e-28 8.8396789e-34 1.2097788e-22
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:41:50,350] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3110
[2019-04-01 18:41:50,366] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.79177536503811, 12.88360916057025, 0.0, 1.0, 31.28877987022202], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3445200.0000, 
sim time next is 3445800.0000, 
raw observation next is [1.0, 80.16666666666667, 0.0, 0.0, 26.0, 25.84323054253262, 12.78415475590472, 0.0, 1.0, 30.1897965215931], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.8016666666666667, 0.0, 0.0, 1.0, 0.9776043632189457, 0.1278415475590472, 0.0, 1.0, 0.21564140372566498], 
reward next is 0.7844, 
noisyNet noise sample is [array([-0.9746829], dtype=float32), 0.5787775]. 
=============================================
[2019-04-01 18:41:51,128] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4306894e-32 3.9990720e-23 9.5724406e-30 0.0000000e+00 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:41:51,130] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7763
[2019-04-01 18:41:51,159] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.66666666666666, 116.3333333333333, 812.6666666666667, 26.0, 26.61814256234494, 14.18992466754173, 1.0, 1.0, 13.93275037051427], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3413400.0000, 
sim time next is 3414000.0000, 
raw observation next is [3.0, 46.33333333333334, 116.6666666666667, 814.8333333333334, 26.0, 26.6417767229531, 12.08311233812519, 1.0, 1.0, 14.5463672809469], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.46333333333333343, 0.388888888888889, 0.9003683241252303, 1.0, 1.0916823889933, 0.1208311233812519, 1.0, 1.0, 0.103902623435335], 
reward next is 0.0629, 
noisyNet noise sample is [array([-0.5940056], dtype=float32), 1.5317543]. 
=============================================
[2019-04-01 18:41:51,170] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[33.422424]
 [33.345345]
 [33.25386 ]
 [33.254505]
 [33.086407]], R is [[33.34960556]
 [33.01610947]
 [32.68594742]
 [32.3590889 ]
 [32.03549957]].
[2019-04-01 18:41:51,607] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:41:51,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9752
[2019-04-01 18:41:51,624] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 26.0, 25.70088106136076, 11.14219193531011, 1.0, 1.0, 12.43481777957842], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3870600.0000, 
sim time next is 3871200.0000, 
raw observation next is [1.0, 51.00000000000001, 0.0, 0.0, 26.0, 25.60759012441839, 10.69606273856294, 1.0, 1.0, 11.65860970461848], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.5100000000000001, 0.0, 0.0, 1.0, 0.9439414463454844, 0.1069606273856294, 1.0, 1.0, 0.08327578360441772], 
reward next is 0.6383, 
noisyNet noise sample is [array([-1.2472746], dtype=float32), -1.2092285]. 
=============================================
[2019-04-01 18:41:54,816] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:41:54,824] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8471
[2019-04-01 18:41:54,842] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.1666666666666667, 73.0, 0.0, 0.0, 26.0, 25.81378754220321, 13.43190579256282, 0.0, 1.0, 35.77625485278632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3532200.0000, 
sim time next is 3532800.0000, 
raw observation next is [-0.3333333333333333, 74.0, 0.0, 0.0, 26.0, 25.91527031305229, 13.32862492810293, 0.0, 1.0, 31.38309766560895], 
processed observation next is [1.0, 0.9130434782608695, 0.4533702677747, 0.74, 0.0, 0.0, 1.0, 0.98789575900747, 0.13328624928102928, 0.0, 1.0, 0.22416498332577822], 
reward next is 0.7758, 
noisyNet noise sample is [array([-0.83736503], dtype=float32), 0.868012]. 
=============================================
[2019-04-01 18:41:56,759] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.6997980e-31
 9.9999702e-01 3.0116246e-06], sum to 1.0000
[2019-04-01 18:41:56,760] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4787
[2019-04-01 18:41:56,824] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.166666666666667, 65.83333333333334, 103.6666666666667, 703.3333333333334, 26.0, 25.87956725808117, 11.17193642117936, 0.0, 1.0, 29.52809784668874], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3577800.0000, 
sim time next is 3578400.0000, 
raw observation next is [-5.0, 65.0, 105.5, 717.0, 26.0, 25.82024291832521, 11.01132683953445, 0.0, 1.0, 27.73761683387109], 
processed observation next is [0.0, 0.43478260869565216, 0.32409972299168976, 0.65, 0.3516666666666667, 0.7922651933701658, 1.0, 0.9743204169036014, 0.1101132683953445, 0.0, 1.0, 0.19812583452765065], 
reward next is 0.8019, 
noisyNet noise sample is [array([0.35233498], dtype=float32), -1.2623621]. 
=============================================
[2019-04-01 18:42:04,084] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6053101e-36 0.0000000e+00 2.3282546e-31 3.0252072e-37 1.8518083e-20
 1.0000000e+00 1.9525978e-19], sum to 1.0000
[2019-04-01 18:42:04,098] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7142
[2019-04-01 18:42:04,112] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.56441548804219, 8.36855651576006, 0.0, 1.0, 33.00763010666051], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3718800.0000, 
sim time next is 3719400.0000, 
raw observation next is [-3.0, 70.0, 0.0, 0.0, 26.0, 25.54143166251174, 8.2748148141743, 0.0, 1.0, 29.26244292395159], 
processed observation next is [1.0, 0.043478260869565216, 0.3795013850415513, 0.7, 0.0, 0.0, 1.0, 0.9344902375016773, 0.082748148141743, 0.0, 1.0, 0.20901744945679707], 
reward next is 0.7910, 
noisyNet noise sample is [array([-0.09278264], dtype=float32), 0.79345775]. 
=============================================
[2019-04-01 18:42:09,854] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0621386e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0099331e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 18:42:09,858] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0139
[2019-04-01 18:42:09,871] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 75.0, 0.0, 0.0, 26.0, 25.28740148451861, 8.724733439979195, 0.0, 1.0, 43.28026674231505], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3804000.0000, 
sim time next is 3804600.0000, 
raw observation next is [-3.833333333333333, 76.0, 0.0, 0.0, 26.0, 25.27231741021337, 8.559269409294922, 0.0, 1.0, 43.403550076665], 
processed observation next is [1.0, 0.0, 0.3564173591874424, 0.76, 0.0, 0.0, 1.0, 0.8960453443161954, 0.08559269409294921, 0.0, 1.0, 0.3100253576904643], 
reward next is 0.6900, 
noisyNet noise sample is [array([1.109745], dtype=float32), -0.49297905]. 
=============================================
[2019-04-01 18:42:10,659] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.6579403e-32 7.0686080e-34 0.0000000e+00 5.2976333e-28
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:42:10,660] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5504
[2019-04-01 18:42:10,668] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 46.5, 87.0, 717.0, 26.0, 27.03117321015715, 18.55552259784282, 1.0, 1.0, 4.671011701271444], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3857400.0000, 
sim time next is 3858000.0000, 
raw observation next is [2.666666666666667, 46.0, 83.16666666666666, 689.3333333333333, 26.0, 27.1291850673508, 18.81887412260346, 1.0, 1.0, 5.039809144251964], 
processed observation next is [1.0, 0.6521739130434783, 0.5364727608494922, 0.46, 0.2772222222222222, 0.7616942909760589, 1.0, 1.1613121524786858, 0.1881887412260346, 1.0, 1.0, 0.035998636744656885], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.09036538], dtype=float32), 0.36041975]. 
=============================================
[2019-04-01 18:42:10,678] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[27.050133]
 [26.59696 ]
 [26.212564]
 [25.834051]
 [25.544167]], R is [[27.31683159]
 [27.04366302]
 [26.77322578]
 [26.50549316]
 [26.24043846]].
[2019-04-01 18:42:12,364] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3672020e-26 0.0000000e+00 4.3878167e-31 1.2230346e-24 5.6597453e-31
 1.0000000e+00 2.8928019e-22], sum to 1.0000
[2019-04-01 18:42:12,366] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4929
[2019-04-01 18:42:12,380] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 72.0, 0.0, 0.0, 26.0, 25.12445001393834, 7.533725952344885, 0.0, 1.0, 42.9087297099807], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3811800.0000, 
sim time next is 3812400.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.04593623992704, 7.55142476382554, 0.0, 1.0, 43.7574335409479], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 1.0, 0.8637051771324344, 0.0755142476382554, 0.0, 1.0, 0.3125530967210564], 
reward next is 0.6874, 
noisyNet noise sample is [array([0.5041814], dtype=float32), 0.71739227]. 
=============================================
[2019-04-01 18:42:19,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 9.5523766e-34 9.6875347e-33 0.0000000e+00
 5.0135169e-02 9.4986486e-01], sum to 1.0000
[2019-04-01 18:42:19,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9351
[2019-04-01 18:42:19,625] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.66666666666667, 67.0, 0.0, 0.0, 26.0, 23.98095149199093, 5.35746204872252, 0.0, 1.0, 43.18382997649002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3991200.0000, 
sim time next is 3991800.0000, 
raw observation next is [-12.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.91811025256014, 5.321803134862155, 0.0, 1.0, 43.20806145918312], 
processed observation next is [1.0, 0.17391304347826086, 0.10710987996306563, 0.68, 0.0, 0.0, 1.0, 0.7025871789371629, 0.053218031348621554, 0.0, 1.0, 0.30862901042273655], 
reward next is 0.6914, 
noisyNet noise sample is [array([1.0403311], dtype=float32), 0.112219416]. 
=============================================
[2019-04-01 18:42:19,936] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-01 18:42:19,947] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 18:42:19,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:42:19,948] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 18:42:19,949] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 18:42:19,949] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:42:19,949] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:42:19,953] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run32
[2019-04-01 18:42:19,971] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run32
[2019-04-01 18:42:19,992] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run32
[2019-04-01 18:43:55,520] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.1520382], dtype=float32), -1.2366127]
[2019-04-01 18:43:55,520] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [12.18333333333333, 40.50000000000001, 134.0, 650.3333333333333, 26.0, 26.84961413705526, 19.6198251448041, 1.0, 1.0, 5.609922280102848]
[2019-04-01 18:43:55,521] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 18:43:55,521] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 9.333379e-37], sampled 0.14956633401263764
[2019-04-01 18:43:58,158] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.1520382], dtype=float32), -1.2366127]
[2019-04-01 18:43:58,158] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [8.233333333333334, 65.5, 287.6666666666667, 221.3333333333333, 26.0, 26.42794081077528, 23.04799430816199, 0.0, 1.0, 6.092955422793668]
[2019-04-01 18:43:58,159] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 18:43:58,160] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.4233069032561957
[2019-04-01 18:44:02,503] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 18:44:22,085] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 18:44:24,720] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 18:44:25,744] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 3100000, evaluation results [3100000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 18:44:26,222] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4730278e-27 5.0866244e-32 1.2212018e-21 8.7598836e-31 2.9577157e-30
 1.9407229e-01 8.0592769e-01], sum to 1.0000
[2019-04-01 18:44:26,224] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9136
[2019-04-01 18:44:26,239] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 38.0, 0.0, 0.0, 26.0, 24.69155398166254, 5.960230437451568, 0.0, 1.0, 39.68403229875958], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4082400.0000, 
sim time next is 4083000.0000, 
raw observation next is [-4.166666666666667, 38.5, 0.0, 0.0, 26.0, 24.666617714809, 5.942931442346601, 0.0, 1.0, 39.64485462845109], 
processed observation next is [1.0, 0.2608695652173913, 0.3471837488457987, 0.385, 0.0, 0.0, 1.0, 0.8095168164012857, 0.05942931442346601, 0.0, 1.0, 0.28317753306036497], 
reward next is 0.7168, 
noisyNet noise sample is [array([0.71028346], dtype=float32), -0.0152076045]. 
=============================================
[2019-04-01 18:44:26,257] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[61.35671 ]
 [61.416344]
 [61.4651  ]
 [61.47793 ]
 [61.494232]], R is [[61.34601593]
 [61.44909668]
 [61.55113602]
 [61.6521759 ]
 [61.75201035]].
[2019-04-01 18:44:28,373] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:44:28,374] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9447
[2019-04-01 18:44:28,396] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 35.0, 98.0, 728.0, 26.0, 27.20684908000888, 18.44749544797467, 1.0, 1.0, 1.669945399097762], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4115400.0000, 
sim time next is 4116000.0000, 
raw observation next is [4.0, 35.0, 96.0, 711.5, 26.0, 27.27607083679796, 15.98732404847437, 1.0, 1.0, 1.701959070574543], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.32, 0.7861878453038674, 1.0, 1.1822958338282799, 0.15987324048474372, 1.0, 1.0, 0.012156850504103879], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.74754804], dtype=float32), 0.73731047]. 
=============================================
[2019-04-01 18:44:28,405] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[50.190517]
 [49.97851 ]
 [49.62538 ]
 [49.387295]
 [49.218365]], R is [[50.04479218]
 [49.54434586]
 [49.04890442]
 [48.55841446]
 [48.0728302 ]].
[2019-04-01 18:44:29,453] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:44:29,454] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4763
[2019-04-01 18:44:29,493] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 41.66666666666667, 105.3333333333333, 631.3333333333333, 26.0, 25.64794910978048, 9.45741969003891, 0.0, 1.0, 26.53516978105202], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4180800.0000, 
sim time next is 4181400.0000, 
raw observation next is [-3.0, 40.0, 108.0, 660.0, 26.0, 25.65277149013072, 9.413924131126722, 0.0, 1.0, 24.73448741880105], 
processed observation next is [0.0, 0.391304347826087, 0.3795013850415513, 0.4, 0.36, 0.7292817679558011, 1.0, 0.9503959271615317, 0.09413924131126722, 0.0, 1.0, 0.1766749101342932], 
reward next is 0.8233, 
noisyNet noise sample is [array([0.9728195], dtype=float32), -0.25776628]. 
=============================================
[2019-04-01 18:44:29,740] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 9.211724e-17 1.000000e+00], sum to 1.0000
[2019-04-01 18:44:29,741] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9797
[2019-04-01 18:44:29,779] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4, 72.5, 111.0, 66.0, 26.0, 25.58733327705223, 9.708996990328734, 1.0, 1.0, 22.01551912086507], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4523400.0000, 
sim time next is 4524000.0000, 
raw observation next is [-0.2666666666666667, 72.33333333333334, 113.0, 55.0, 26.0, 25.80382268318834, 9.88654699926852, 1.0, 1.0, 19.47412188602586], 
processed observation next is [1.0, 0.34782608695652173, 0.4552169898430287, 0.7233333333333334, 0.37666666666666665, 0.06077348066298342, 1.0, 0.9719746690269057, 0.0988654699926852, 1.0, 1.0, 0.13910087061447043], 
reward next is 0.8609, 
noisyNet noise sample is [array([-0.40426484], dtype=float32), 0.20153813]. 
=============================================
[2019-04-01 18:44:29,783] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[44.66396 ]
 [44.925434]
 [46.07901 ]
 [39.087673]
 [39.631294]], R is [[44.22615433]
 [44.62664032]
 [45.00879288]
 [45.3889389 ]
 [45.75483322]].
[2019-04-01 18:44:37,960] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:44:37,966] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3928
[2019-04-01 18:44:37,975] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.5, 31.0, 155.0, 735.5, 26.0, 28.54358678558621, 32.4461842994887, 1.0, 1.0, 1.256836082846225], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4370400.0000, 
sim time next is 4371000.0000, 
raw observation next is [14.4, 31.16666666666667, 168.3333333333333, 700.0, 26.0, 28.60840716243091, 33.28188101941335, 1.0, 1.0, 1.201889302102324], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.3116666666666667, 0.561111111111111, 0.7734806629834254, 1.0, 1.3726295946329874, 0.33281881019413345, 1.0, 1.0, 0.008584923586445171], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.0121442], dtype=float32), -1.6122694]. 
=============================================
[2019-04-01 18:44:37,983] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.23734 ]
 [77.62303 ]
 [76.988205]
 [76.4284  ]
 [76.22551 ]], R is [[77.99837494]
 [77.21839142]
 [76.44620514]
 [75.68174744]
 [74.92493439]].
[2019-04-01 18:44:41,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.0875001e-35 3.3768268e-33 1.9324096e-30 2.8828310e-23 5.4305490e-21
 1.0000000e+00 6.8493389e-38], sum to 1.0000
[2019-04-01 18:44:41,427] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2886
[2019-04-01 18:44:41,451] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 178.3333333333333, 48.66666666666666, 26.0, 26.44435215421583, 13.80958476430158, 1.0, 1.0, 7.953519372883866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4446600.0000, 
sim time next is 4447200.0000, 
raw observation next is [1.0, 86.0, 160.1666666666667, 24.33333333333333, 26.0, 26.46233345333298, 13.59103852928821, 1.0, 1.0, 7.824294144215991], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.5338888888888891, 0.026887661141804783, 1.0, 1.0660476361904259, 0.1359103852928821, 1.0, 1.0, 0.05588781531582851], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.0081017], dtype=float32), -1.4368367]. 
=============================================
[2019-04-01 18:44:42,476] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5251375e-38 0.0000000e+00
 1.0000000e+00 4.5434917e-28], sum to 1.0000
[2019-04-01 18:44:42,478] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1427
[2019-04-01 18:44:42,489] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.45, 72.5, 0.0, 0.0, 26.0, 25.72755161788526, 8.635173323515842, 0.0, 1.0, 21.84331015777771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4326600.0000, 
sim time next is 4327200.0000, 
raw observation next is [4.5, 72.0, 0.0, 0.0, 26.0, 25.69962032747215, 8.399655787112993, 0.0, 1.0, 20.59068721760098], 
processed observation next is [1.0, 0.08695652173913043, 0.5872576177285319, 0.72, 0.0, 0.0, 1.0, 0.9570886182103072, 0.08399655787112993, 0.0, 1.0, 0.14707633726857844], 
reward next is 0.8529, 
noisyNet noise sample is [array([-0.13830972], dtype=float32), 0.1333696]. 
=============================================
[2019-04-01 18:44:44,067] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9855240e-37 1.7096761e-31 2.2002430e-35 6.3591664e-34 0.0000000e+00
 1.0000000e+00 2.8264761e-25], sum to 1.0000
[2019-04-01 18:44:44,071] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0185
[2019-04-01 18:44:44,096] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.4, 44.0, 0.0, 0.0, 26.0, 28.13346529289906, 29.80195963438981, 1.0, 1.0, 5.794052580266681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4384800.0000, 
sim time next is 4385400.0000, 
raw observation next is [12.33333333333333, 45.0, 0.0, 0.0, 26.0, 28.11934458813771, 25.40787049485224, 1.0, 1.0, 6.210724060816199], 
processed observation next is [1.0, 0.782608695652174, 0.8042474607571561, 0.45, 0.0, 0.0, 1.0, 1.3027635125911015, 0.2540787049485224, 1.0, 1.0, 0.04436231472011571], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.923155], dtype=float32), -0.17863387]. 
=============================================
[2019-04-01 18:44:44,894] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 5.1904087e-30 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:44:44,896] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0380
[2019-04-01 18:44:44,921] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.15, 85.0, 165.0, 31.0, 26.0, 26.22938862709929, 13.18119308857971, 1.0, 1.0, 11.68654952659917], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4440600.0000, 
sim time next is 4441200.0000, 
raw observation next is [1.1, 85.33333333333333, 179.3333333333333, 50.16666666666666, 26.0, 26.33139246262188, 13.44430148335917, 1.0, 1.0, 11.37184652217363], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8533333333333333, 0.5977777777777776, 0.05543278084714548, 1.0, 1.0473417803745544, 0.1344430148335917, 1.0, 1.0, 0.08122747515838308], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.41929135], dtype=float32), -0.32552335]. 
=============================================
[2019-04-01 18:44:46,613] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8836867e-18 1.4575052e-12 2.8673672e-16 9.9925691e-01 1.0051744e-24
 7.4301846e-04 8.9001421e-28], sum to 1.0000
[2019-04-01 18:44:46,615] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7638
[2019-04-01 18:44:46,622] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.15, 85.0, 165.0, 31.0, 26.0, 26.21801202566989, 12.97614812588524, 1.0, 1.0, 11.96145453666995], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4440600.0000, 
sim time next is 4441200.0000, 
raw observation next is [1.1, 85.33333333333333, 179.3333333333333, 50.16666666666666, 26.0, 26.32070015017438, 13.24247889767439, 1.0, 1.0, 11.62583120389017], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8533333333333333, 0.5977777777777776, 0.05543278084714548, 1.0, 1.0458143071677684, 0.1324247889767439, 1.0, 1.0, 0.08304165145635836], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5459957], dtype=float32), -2.4112475]. 
=============================================
[2019-04-01 18:44:56,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:44:56,216] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4082
[2019-04-01 18:44:56,233] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 74.5, 0.0, 0.0, 26.0, 25.25104826179949, 7.973373224071129, 0.0, 1.0, 35.82281292430286], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4601400.0000, 
sim time next is 4602000.0000, 
raw observation next is [-2.733333333333333, 75.0, 0.0, 0.0, 26.0, 25.19533489890151, 7.767541913133191, 0.0, 1.0, 35.87303041687546], 
processed observation next is [1.0, 0.2608695652173913, 0.3868882733148662, 0.75, 0.0, 0.0, 1.0, 0.8850478427002157, 0.07767541913133191, 0.0, 1.0, 0.2562359315491104], 
reward next is 0.7438, 
noisyNet noise sample is [array([0.8719747], dtype=float32), 0.50373316]. 
=============================================
[2019-04-01 18:44:56,245] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[60.238655]
 [60.28782 ]
 [60.34268 ]
 [60.420525]
 [60.5215  ]], R is [[60.31459808]
 [60.45557404]
 [60.59516907]
 [60.73321152]
 [60.86954498]].
[2019-04-01 18:44:56,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:44:56,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:44:56,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run24
[2019-04-01 18:44:56,953] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.0526450e-27
 1.0000000e+00 2.3791546e-37], sum to 1.0000
[2019-04-01 18:44:56,963] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5510
[2019-04-01 18:44:56,975] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 62.66666666666666, 0.0, 0.0, 26.0, 25.50216049810087, 9.198715106832509, 0.0, 1.0, 28.38710387080982], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4582200.0000, 
sim time next is 4582800.0000, 
raw observation next is [0.4, 63.0, 0.0, 0.0, 26.0, 25.42319470715969, 9.043925624589482, 0.0, 1.0, 29.79699772990364], 
processed observation next is [1.0, 0.043478260869565216, 0.4736842105263158, 0.63, 0.0, 0.0, 1.0, 0.917599243879956, 0.09043925624589483, 0.0, 1.0, 0.2128356980707403], 
reward next is 0.7872, 
noisyNet noise sample is [array([0.8272094], dtype=float32), 1.1682909]. 
=============================================
[2019-04-01 18:44:57,633] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.3429081e-20 5.6131022e-34 8.6649249e-22 2.7346573e-12 1.0140782e-02
 9.8985916e-01 6.3313081e-19], sum to 1.0000
[2019-04-01 18:44:57,633] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0291
[2019-04-01 18:44:57,648] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.95298532231583, 7.857614684849142, 0.0, 1.0, 40.57244798200529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4755600.0000, 
sim time next is 4756200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.92092918766719, 7.857697178892674, 0.0, 1.0, 40.48548248460499], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 1.0, 0.8458470268095988, 0.07857697178892674, 0.0, 1.0, 0.2891820177471785], 
reward next is 0.7108, 
noisyNet noise sample is [array([0.28627288], dtype=float32), 1.0689992]. 
=============================================
[2019-04-01 18:44:59,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:44:59,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:44:59,228] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run24
[2019-04-01 18:45:04,248] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 7.0502043e-37 0.0000000e+00 1.5448787e-35
 3.2216549e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 18:45:04,249] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7917
[2019-04-01 18:45:04,293] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 46.0, 137.5, 784.5, 26.0, 25.30957996662753, 9.544528134536863, 0.0, 1.0, 18.11687977513238], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4791600.0000, 
sim time next is 4792200.0000, 
raw observation next is [-1.5, 45.5, 132.3333333333333, 802.6666666666666, 26.0, 25.26000405773538, 9.424614066379549, 0.0, 1.0, 17.0459785746105], 
processed observation next is [0.0, 0.4782608695652174, 0.4210526315789474, 0.455, 0.44111111111111095, 0.8869244935543278, 1.0, 0.894286293962197, 0.0942461406637955, 0.0, 1.0, 0.12175698981864644], 
reward next is 0.8782, 
noisyNet noise sample is [array([0.44522047], dtype=float32), 0.92225415]. 
=============================================
[2019-04-01 18:45:04,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:04,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:04,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run24
[2019-04-01 18:45:08,832] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.8492573e-22
 1.0000000e+00 2.0725134e-33], sum to 1.0000
[2019-04-01 18:45:08,833] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2388
[2019-04-01 18:45:08,870] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 44.33333333333334, 0.0, 0.0, 26.0, 24.98619564145928, 8.015761374777421, 0.0, 1.0, 38.37296352376701], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4821600.0000, 
sim time next is 4822200.0000, 
raw observation next is [1.0, 45.0, 0.0, 0.0, 26.0, 25.02652711358559, 8.521734216067768, 0.0, 1.0, 50.73920698182698], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.45, 0.0, 0.0, 1.0, 0.8609324447979415, 0.08521734216067768, 0.0, 1.0, 0.36242290701304986], 
reward next is 0.6376, 
noisyNet noise sample is [array([0.6304356], dtype=float32), -2.1310282]. 
=============================================
[2019-04-01 18:45:09,737] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.         0.         0.         0.         0.         0.04265828
 0.9573417 ], sum to 1.0000
[2019-04-01 18:45:09,737] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6586
[2019-04-01 18:45:09,759] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 44.16666666666667, 22.0, 124.6666666666667, 26.0, 24.97472311982811, 7.555328842077931, 0.0, 1.0, 21.54964239132068], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4902600.0000, 
sim time next is 4903200.0000, 
raw observation next is [2.0, 44.0, 16.5, 93.5, 26.0, 24.97114482545803, 7.475709841229396, 0.0, 1.0, 23.71673287022038], 
processed observation next is [0.0, 0.782608695652174, 0.518005540166205, 0.44, 0.055, 0.10331491712707182, 1.0, 0.853020689351147, 0.07475709841229396, 0.0, 1.0, 0.16940523478728842], 
reward next is 0.8306, 
noisyNet noise sample is [array([1.4677643], dtype=float32), -0.055128854]. 
=============================================
[2019-04-01 18:45:11,845] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4832791e-38 0.0000000e+00 1.3963921e-32 4.0287053e-28 2.6334615e-24
 1.0000000e+00 5.1432640e-22], sum to 1.0000
[2019-04-01 18:45:11,847] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5554
[2019-04-01 18:45:11,896] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 48.66666666666667, 0.0, 0.0, 26.0, 25.04937963931669, 6.564169939937631, 1.0, 1.0, 36.81040500125157], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4951200.0000, 
sim time next is 4951800.0000, 
raw observation next is [-2.5, 48.0, 0.0, 0.0, 26.0, 25.22335986630816, 6.684648358218865, 1.0, 1.0, 32.09342822585297], 
processed observation next is [1.0, 0.30434782608695654, 0.39335180055401664, 0.48, 0.0, 0.0, 1.0, 0.8890514094725945, 0.06684648358218866, 1.0, 1.0, 0.22923877304180693], 
reward next is 0.7708, 
noisyNet noise sample is [array([0.47547907], dtype=float32), -1.452108]. 
=============================================
[2019-04-01 18:45:14,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:14,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:14,556] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run24
[2019-04-01 18:45:14,655] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:14,655] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:14,663] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run24
[2019-04-01 18:45:15,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:15,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:15,199] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run24
[2019-04-01 18:45:16,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:16,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:16,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run24
[2019-04-01 18:45:17,627] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:17,628] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:17,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run24
[2019-04-01 18:45:17,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:17,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:17,750] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run24
[2019-04-01 18:45:18,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:18,114] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:18,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run24
[2019-04-01 18:45:18,542] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:18,542] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:18,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run24
[2019-04-01 18:45:18,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:18,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:18,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run24
[2019-04-01 18:45:19,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:19,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:19,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run24
[2019-04-01 18:45:19,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:19,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:19,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run24
[2019-04-01 18:45:20,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:20,045] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:20,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run24
[2019-04-01 18:45:22,390] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:45:22,390] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:45:22,410] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run24
[2019-04-01 18:45:30,863] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.2983454e-34
 1.0000000e+00 3.3530944e-17], sum to 1.0000
[2019-04-01 18:45:30,864] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7670
[2019-04-01 18:45:30,930] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.699999999999999, 93.0, 23.83333333333333, 0.0, 26.0, 22.58938655461505, 6.753261531977536, 0.0, 1.0, 58.60546206520611], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 31200.0000, 
sim time next is 31800.0000, 
raw observation next is [7.7, 93.0, 26.66666666666666, 0.0, 26.0, 22.76056745944424, 6.47605374874388, 0.0, 1.0, 57.97020794674206], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.08888888888888886, 0.0, 1.0, 0.5372239227777484, 0.06476053748743879, 0.0, 1.0, 0.41407291390530043], 
reward next is 0.5859, 
noisyNet noise sample is [array([-0.10535829], dtype=float32), -0.26279458]. 
=============================================
[2019-04-01 18:45:34,855] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.3543363e-35 0.0000000e+00 3.3908210e-32 4.4279452e-27 1.1271367e-26
 9.9997330e-01 2.6659225e-05], sum to 1.0000
[2019-04-01 18:45:34,855] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4876
[2019-04-01 18:45:34,870] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 74.66666666666667, 0.0, 0.0, 26.0, 23.50408473087136, 5.478645076114383, 0.0, 1.0, 43.60512822013529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 180600.0000, 
sim time next is 181200.0000, 
raw observation next is [-8.900000000000002, 75.33333333333334, 0.0, 0.0, 26.0, 23.45395253657038, 5.44979040781736, 0.0, 1.0, 43.56176949922121], 
processed observation next is [1.0, 0.08695652173913043, 0.2160664819944598, 0.7533333333333334, 0.0, 0.0, 1.0, 0.6362789337957686, 0.05449790407817359, 0.0, 1.0, 0.3111554964230086], 
reward next is 0.6888, 
noisyNet noise sample is [array([-0.06355014], dtype=float32), 1.3014821]. 
=============================================
[2019-04-01 18:45:35,681] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 2.1997805e-36 0.0000000e+00 1.0087080e-31 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:45:35,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1932
[2019-04-01 18:45:35,719] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 61.0, 119.0, 85.66666666666667, 26.0, 25.84350381204023, 9.20269701329716, 1.0, 1.0, 20.54563777162578], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 139800.0000, 
sim time next is 140400.0000, 
raw observation next is [-6.7, 61.0, 104.5, 75.5, 26.0, 25.71992580715392, 8.641943412229104, 1.0, 1.0, 19.72750603827111], 
processed observation next is [1.0, 0.6521739130434783, 0.2770083102493075, 0.61, 0.34833333333333333, 0.08342541436464089, 1.0, 0.9599894010219886, 0.08641943412229104, 1.0, 1.0, 0.1409107574162222], 
reward next is 0.8591, 
noisyNet noise sample is [array([-0.22973262], dtype=float32), 0.19961667]. 
=============================================
[2019-04-01 18:45:47,309] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 5.593692e-34 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:45:47,309] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6428
[2019-04-01 18:45:47,394] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 24.86117467785289, 7.29544018253723, 1.0, 1.0, 40.53801598314302], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 503400.0000, 
sim time next is 504000.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 25.04983839353326, 7.440937259584217, 1.0, 1.0, 32.45054608423897], 
processed observation next is [1.0, 0.8695652173913043, 0.49307479224376743, 0.96, 0.0, 0.0, 1.0, 0.8642626276476086, 0.07440937259584217, 1.0, 1.0, 0.23178961488742122], 
reward next is 0.7682, 
noisyNet noise sample is [array([0.58281577], dtype=float32), -0.39788747]. 
=============================================
[2019-04-01 18:45:47,406] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[61.080803]
 [64.428604]
 [68.2342  ]
 [75.19023 ]
 [73.30396 ]], R is [[59.26210785]
 [59.37992859]
 [59.36454391]
 [59.30773926]
 [59.43602371]].
[2019-04-01 18:45:48,369] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:45:48,369] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6025
[2019-04-01 18:45:48,417] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 44.0, 92.83333333333334, 629.6666666666667, 26.0, 26.58798247878021, 14.17501771775594, 1.0, 1.0, 45.53407434335024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 308400.0000, 
sim time next is 309000.0000, 
raw observation next is [-9.5, 44.0, 90.66666666666667, 628.3333333333333, 26.0, 26.92583802275778, 14.97799047749347, 1.0, 1.0, 41.39850935549142], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.3022222222222222, 0.6942909760589318, 1.0, 1.1322625746796828, 0.14977990477493472, 1.0, 1.0, 0.29570363825351015], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5591052], dtype=float32), 0.5655082]. 
=============================================
[2019-04-01 18:45:48,433] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[56.749043]
 [56.38798 ]
 [55.62145 ]
 [56.16767 ]
 [56.524563]], R is [[56.39320374]
 [55.82927322]
 [55.27098083]
 [55.00966644]
 [55.02353668]].
[2019-04-01 18:45:50,655] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7938557e-25 2.5609962e-28 2.0752906e-25 1.8908352e-18 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:45:50,658] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0926
[2019-04-01 18:45:50,732] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.2, 66.33333333333334, 73.33333333333334, 384.0, 26.0, 25.5582628555687, 7.310829987588892, 1.0, 1.0, 49.52043263479879], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 292200.0000, 
sim time next is 292800.0000, 
raw observation next is [-12.1, 65.66666666666667, 84.16666666666667, 383.5, 26.0, 25.51889011094502, 7.544365116091392, 1.0, 1.0, 52.23834229775014], 
processed observation next is [1.0, 0.391304347826087, 0.12742382271468145, 0.6566666666666667, 0.28055555555555556, 0.42375690607734806, 1.0, 0.9312700158492885, 0.07544365116091392, 1.0, 1.0, 0.37313101641250096], 
reward next is 0.6269, 
noisyNet noise sample is [array([-2.1238697], dtype=float32), -0.48628694]. 
=============================================
[2019-04-01 18:45:57,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2140583e-13 2.2609494e-29 7.4569745e-18 1.0923979e-07 2.2804791e-13
 9.9999988e-01 1.0381863e-08], sum to 1.0000
[2019-04-01 18:45:57,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7781
[2019-04-01 18:45:57,314] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 26.0, 23.80619811176555, 5.593451202025657, 0.0, 1.0, 44.3893691966312], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 430800.0000, 
sim time next is 431400.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 26.0, 23.79458939668848, 5.59193014885481, 0.0, 1.0, 44.43376669649449], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 1.0, 0.6849413423840686, 0.055919301488548095, 0.0, 1.0, 0.3173840478321035], 
reward next is 0.6826, 
noisyNet noise sample is [array([-0.11902483], dtype=float32), 0.78849703]. 
=============================================
[2019-04-01 18:46:03,923] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:46:03,923] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2166
[2019-04-01 18:46:03,989] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.533333333333333, 38.83333333333334, 15.33333333333334, 0.0, 26.0, 24.91036535417671, 5.867941250104404, 0.0, 1.0, 59.47183985790826], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 461400.0000, 
sim time next is 462000.0000, 
raw observation next is [-7.266666666666667, 37.66666666666667, 19.16666666666667, 0.0, 26.0, 24.93608710918357, 5.929961745044298, 1.0, 1.0, 54.37272762376828], 
processed observation next is [1.0, 0.34782608695652173, 0.26131117266851345, 0.3766666666666667, 0.0638888888888889, 0.0, 1.0, 0.8480124441690816, 0.05929961745044298, 1.0, 1.0, 0.38837662588405913], 
reward next is 0.6116, 
noisyNet noise sample is [array([0.20960467], dtype=float32), -1.6593816]. 
=============================================
[2019-04-01 18:46:04,000] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[76.90053 ]
 [70.19932 ]
 [67.524574]
 [66.004196]
 [66.40965 ]], R is [[77.61971283]
 [77.41871643]
 [77.18580627]
 [76.84823608]
 [76.50337982]].
[2019-04-01 18:46:10,259] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:46:10,259] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2913
[2019-04-01 18:46:10,327] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.2, 79.0, 91.0, 0.0, 26.0, 25.47761614555357, 7.242321962330318, 1.0, 1.0, 27.05923438479298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 826200.0000, 
sim time next is 826800.0000, 
raw observation next is [-4.1, 79.0, 85.66666666666667, 0.0, 26.0, 25.13724511213966, 6.979723065167974, 1.0, 1.0, 36.71329928049699], 
processed observation next is [1.0, 0.5652173913043478, 0.3490304709141275, 0.79, 0.28555555555555556, 0.0, 1.0, 0.876749301734237, 0.06979723065167974, 1.0, 1.0, 0.26223785200354993], 
reward next is 0.7378, 
noisyNet noise sample is [array([2.186229], dtype=float32), 0.114915915]. 
=============================================
[2019-04-01 18:46:13,242] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:46:13,243] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8381
[2019-04-01 18:46:13,290] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.2, 87.0, 0.0, 0.0, 26.0, 24.95325905938788, 7.558662815402815, 0.0, 1.0, 37.85216097867016], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 582600.0000, 
sim time next is 583200.0000, 
raw observation next is [-2.3, 87.0, 0.0, 0.0, 26.0, 24.93372811967053, 7.537866264921699, 0.0, 1.0, 39.36922636199242], 
processed observation next is [0.0, 0.782608695652174, 0.3988919667590028, 0.87, 0.0, 0.0, 1.0, 0.8476754456672185, 0.07537866264921698, 0.0, 1.0, 0.2812087597285173], 
reward next is 0.7188, 
noisyNet noise sample is [array([-0.45798558], dtype=float32), -1.1369287]. 
=============================================
[2019-04-01 18:46:15,152] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.3269812e-35 1.1353811e-30 2.2022841e-27 9.4152952e-09 2.6568814e-24
 3.0505281e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 18:46:15,157] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1367
[2019-04-01 18:46:15,198] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.55, 76.0, 29.0, 0.0, 26.0, 25.08471133475434, 6.834865241725583, 1.0, 1.0, 29.96272547382501], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 894600.0000, 
sim time next is 895200.0000, 
raw observation next is [0.7333333333333334, 77.33333333333333, 32.16666666666666, 0.0, 26.0, 25.27292906343324, 7.075606398295716, 1.0, 1.0, 25.15207727868229], 
processed observation next is [1.0, 0.34782608695652173, 0.4829178208679595, 0.7733333333333333, 0.10722222222222219, 0.0, 1.0, 0.8961327233476055, 0.07075606398295715, 1.0, 1.0, 0.17965769484773064], 
reward next is 0.8203, 
noisyNet noise sample is [array([-0.39204198], dtype=float32), -0.7678469]. 
=============================================
[2019-04-01 18:46:18,092] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:46:18,093] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5787
[2019-04-01 18:46:18,146] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.933333333333334, 62.33333333333333, 92.83333333333334, 48.33333333333333, 26.0, 24.7861962290119, 6.539234772070887, 0.0, 1.0, 51.18936036465197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 646800.0000, 
sim time next is 647400.0000, 
raw observation next is [-2.816666666666667, 61.66666666666666, 96.66666666666667, 58.66666666666666, 26.0, 24.78480408408101, 6.728769026263566, 0.0, 1.0, 56.61962407258758], 
processed observation next is [0.0, 0.4782608695652174, 0.38457987072945526, 0.6166666666666666, 0.32222222222222224, 0.06482504604051564, 1.0, 0.8264005834401443, 0.06728769026263566, 0.0, 1.0, 0.40442588623276843], 
reward next is 0.5956, 
noisyNet noise sample is [array([0.49597225], dtype=float32), 0.079971686]. 
=============================================
[2019-04-01 18:46:25,053] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.789636e-36
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:46:25,054] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2547
[2019-04-01 18:46:25,110] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 54.0, 34.0, 2.5, 26.0, 26.54869255462905, 11.64063280749284, 1.0, 1.0, 23.33665373786904], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 752400.0000, 
sim time next is 753000.0000, 
raw observation next is [-2.983333333333333, 54.33333333333333, 0.0, 0.0, 26.0, 26.52763207253398, 10.0938091747084, 1.0, 1.0, 23.79692933215114], 
processed observation next is [1.0, 0.7391304347826086, 0.37996306555863346, 0.5433333333333333, 0.0, 0.0, 1.0, 1.0753760103619971, 0.10093809174708399, 1.0, 1.0, 0.16997806665822243], 
reward next is 0.7925, 
noisyNet noise sample is [array([-0.05567168], dtype=float32), 0.16655537]. 
=============================================
[2019-04-01 18:46:25,142] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[77.179436]
 [77.9857  ]
 [78.623116]
 [78.98681 ]
 [78.73806 ]], R is [[76.12269592]
 [75.53852844]
 [74.96419525]
 [74.49671936]
 [74.47132111]].
[2019-04-01 18:46:31,357] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 7.1146109e-30 0.0000000e+00
 1.0000000e+00 2.1209846e-23], sum to 1.0000
[2019-04-01 18:46:31,358] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3378
[2019-04-01 18:46:31,377] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.65455046903917, 6.306324400030976, 0.0, 1.0, 39.16036786664187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 866400.0000, 
sim time next is 867000.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.71264840562596, 6.282552358335889, 0.0, 1.0, 39.09607099098705], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 1.0, 0.816092629375137, 0.06282552358335888, 0.0, 1.0, 0.2792576499356218], 
reward next is 0.7207, 
noisyNet noise sample is [array([1.0055492], dtype=float32), 0.46057618]. 
=============================================
[2019-04-01 18:46:31,395] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.07128]
 [73.12471]
 [73.28097]
 [73.48614]
 [73.75977]], R is [[73.063797  ]
 [73.05344391]
 [73.04224396]
 [73.03018188]
 [73.01739502]].
[2019-04-01 18:46:33,308] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.6435228e-16
 1.0000000e+00 1.5073457e-31], sum to 1.0000
[2019-04-01 18:46:33,308] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2368
[2019-04-01 18:46:33,313] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.0, 100.0, 58.66666666666667, 0.0, 26.0, 23.33032278146347, 5.773063001517694, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1242600.0000, 
sim time next is 1243200.0000, 
raw observation next is [15.0, 100.0, 66.33333333333334, 0.0, 26.0, 23.32177905323346, 5.772649870619124, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.8781163434903049, 1.0, 0.22111111111111115, 0.0, 1.0, 0.61739700760478, 0.05772649870619124, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31587848], dtype=float32), 0.15516593]. 
=============================================
[2019-04-01 18:46:35,246] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:46:35,249] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7558
[2019-04-01 18:46:35,261] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 24.9674841579766, 8.654142411877096, 0.0, 1.0, 40.70268191063762], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 939600.0000, 
sim time next is 940200.0000, 
raw observation next is [5.0, 99.33333333333334, 0.0, 0.0, 26.0, 25.04093658697788, 8.789835532557857, 0.0, 1.0, 40.32550045591503], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.9933333333333334, 0.0, 0.0, 1.0, 0.8629909409968403, 0.08789835532557858, 0.0, 1.0, 0.2880392889708216], 
reward next is 0.7120, 
noisyNet noise sample is [array([0.17511839], dtype=float32), 0.59780663]. 
=============================================
[2019-04-01 18:46:40,125] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0869992e-28 9.4775255e-32 3.8305189e-34 2.5703628e-13 1.1056522e-26
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:46:40,127] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8848
[2019-04-01 18:46:40,142] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 81.0, 87.83333333333334, 0.0, 26.0, 25.66136153305773, 13.3853862430882, 1.0, 1.0, 14.06226713631571], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1003200.0000, 
sim time next is 1003800.0000, 
raw observation next is [14.4, 81.0, 81.66666666666667, 0.0, 26.0, 26.24135509270195, 14.52373489855581, 1.0, 1.0, 8.670060933206885], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.27222222222222225, 0.0, 1.0, 1.0344792989574216, 0.1452373489855581, 1.0, 1.0, 0.06192900666576347], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.8485644], dtype=float32), -0.39537326]. 
=============================================
[2019-04-01 18:46:40,891] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4718994e-17 2.7249048e-23 6.6808330e-22 3.3231267e-20 1.8815154e-20
 1.0000000e+00 6.0211691e-28], sum to 1.0000
[2019-04-01 18:46:40,891] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6863
[2019-04-01 18:46:40,900] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.95, 78.0, 57.0, 0.0, 26.0, 26.59101030721638, 16.12189224754377, 1.0, 1.0, 9.116039281663378], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1006200.0000, 
sim time next is 1006800.0000, 
raw observation next is [15.13333333333333, 77.0, 51.66666666666666, 0.0, 26.0, 26.85952698423723, 16.94839558469427, 1.0, 1.0, 8.112333157394513], 
processed observation next is [1.0, 0.6521739130434783, 0.8818097876269622, 0.77, 0.1722222222222222, 0.0, 1.0, 1.1227895691767473, 0.1694839558469427, 1.0, 1.0, 0.057945236838532234], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.01759988], dtype=float32), 1.0308148]. 
=============================================
[2019-04-01 18:46:42,234] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:46:42,237] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5703
[2019-04-01 18:46:42,250] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.1, 53.0, 0.0, 0.0, 26.0, 26.97137805702956, 19.88969367868817, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1101600.0000, 
sim time next is 1102200.0000, 
raw observation next is [15.91666666666667, 53.66666666666667, 0.0, 0.0, 26.0, 26.84259328479545, 19.51458704158955, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.9035087719298247, 0.5366666666666667, 0.0, 0.0, 1.0, 1.1203704692564926, 0.1951458704158955, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.37227362], dtype=float32), -0.55722654]. 
=============================================
[2019-04-01 18:46:46,112] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:46:46,116] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0449
[2019-04-01 18:46:46,124] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [19.4, 49.0, 63.5, 0.0, 26.0, 27.67866220168509, 25.83016903563812, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1094400.0000, 
sim time next is 1095000.0000, 
raw observation next is [19.11666666666667, 49.16666666666667, 54.0, 0.0, 26.0, 27.75044270178604, 26.28636625408613, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.9921514312096033, 0.4916666666666667, 0.18, 0.0, 1.0, 1.2500632431122913, 0.2628636625408613, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3991442], dtype=float32), -0.2209134]. 
=============================================
[2019-04-01 18:46:46,131] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[63.24405 ]
 [62.997223]
 [62.879025]
 [62.622257]
 [62.24671 ]], R is [[62.82190323]
 [62.19368362]
 [61.57174683]
 [60.9560318 ]
 [60.34647369]].
[2019-04-01 18:46:48,240] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:46:48,242] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0622
[2019-04-01 18:46:48,251] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.34746030683441, 7.624353733403223, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1204800.0000, 
sim time next is 1205400.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 26.0, 24.34623087059323, 7.572282413122117, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 1.0, 0.7637472672276046, 0.07572282413122117, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1691566], dtype=float32), 1.3299357]. 
=============================================
[2019-04-01 18:46:54,373] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:46:54,375] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7915
[2019-04-01 18:46:54,385] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 66.5, 0.0, 26.0, 25.85830760464277, 11.31848931782874, 1.0, 1.0, 9.091720991330867], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1348800.0000, 
sim time next is 1349400.0000, 
raw observation next is [1.1, 92.0, 62.0, 0.0, 26.0, 25.81544433028537, 11.03362152268045, 1.0, 1.0, 8.893474631536202], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.20666666666666667, 0.0, 1.0, 0.9736349043264815, 0.11033621522680451, 1.0, 1.0, 0.06352481879668716], 
reward next is 0.5230, 
noisyNet noise sample is [array([-0.8619008], dtype=float32), -0.77014464]. 
=============================================
[2019-04-01 18:46:55,022] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6917716e-31 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:46:55,022] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3586
[2019-04-01 18:46:55,063] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.63807435152926, 11.56585990319435, 1.0, 1.0, 20.80386113715671], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1363200.0000, 
sim time next is 1363800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.440560973233, 11.22171564501706, 1.0, 1.0, 19.56645215088191], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 1.0, 0.9200801390332859, 0.1122171564501706, 1.0, 1.0, 0.13976037250629936], 
reward next is 0.3716, 
noisyNet noise sample is [array([0.82322216], dtype=float32), -1.0704787]. 
=============================================
[2019-04-01 18:46:59,057] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:46:59,059] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1315
[2019-04-01 18:46:59,081] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.35, 90.5, 0.0, 0.0, 26.0, 25.5105333004967, 12.01312452622237, 0.0, 1.0, 38.43443241675999], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1459800.0000, 
sim time next is 1460400.0000, 
raw observation next is [1.266666666666667, 91.0, 0.0, 0.0, 26.0, 25.56623730221284, 11.99926543709451, 0.0, 1.0, 31.11107643026563], 
processed observation next is [1.0, 0.9130434782608695, 0.4976915974145891, 0.91, 0.0, 0.0, 1.0, 0.9380339003161201, 0.1199926543709451, 0.0, 1.0, 0.22222197450189735], 
reward next is 0.7778, 
noisyNet noise sample is [array([0.02190563], dtype=float32), 0.38607717]. 
=============================================
[2019-04-01 18:46:59,415] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:46:59,415] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5238
[2019-04-01 18:46:59,432] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.133333333333333, 85.66666666666667, 0.0, 0.0, 26.0, 24.01342415710574, 5.392525358641973, 0.0, 1.0, 46.01035059427117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1824000.0000, 
sim time next is 1824600.0000, 
raw observation next is [-6.166666666666667, 86.33333333333333, 0.0, 0.0, 26.0, 23.97533373957225, 5.365031847478382, 0.0, 1.0, 46.06486934778154], 
processed observation next is [0.0, 0.08695652173913043, 0.2917820867959372, 0.8633333333333333, 0.0, 0.0, 1.0, 0.7107619627960357, 0.05365031847478382, 0.0, 1.0, 0.3290347810555824], 
reward next is 0.6710, 
noisyNet noise sample is [array([-0.38032466], dtype=float32), 0.36330572]. 
=============================================
[2019-04-01 18:47:07,588] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:47:07,589] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8212
[2019-04-01 18:47:07,597] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.516666666666667, 69.33333333333334, 143.3333333333333, 120.0, 26.0, 26.33315089078031, 13.74105377912067, 1.0, 1.0, 9.593600455953803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1590600.0000, 
sim time next is 1591200.0000, 
raw observation next is [7.7, 68.0, 157.5, 112.0, 26.0, 26.40726852126492, 14.20365316684457, 1.0, 1.0, 8.93061516782957], 
processed observation next is [1.0, 0.43478260869565216, 0.6759002770083103, 0.68, 0.525, 0.12375690607734807, 1.0, 1.05818121732356, 0.14203653166844568, 1.0, 1.0, 0.06379010834163978], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5477363], dtype=float32), -0.3454983]. 
=============================================
[2019-04-01 18:47:10,025] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:47:10,026] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6409
[2019-04-01 18:47:10,034] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.266666666666666, 71.33333333333333, 0.0, 0.0, 26.0, 25.79917764952689, 13.40574735534778, 1.0, 1.0, 6.535399729011501], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1626000.0000, 
sim time next is 1626600.0000, 
raw observation next is [7.983333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.73936241033183, 13.14010254560693, 1.0, 1.0, 6.554260585800152], 
processed observation next is [1.0, 0.8260869565217391, 0.6837488457987074, 0.7266666666666667, 0.0, 0.0, 1.0, 0.9627660586188327, 0.1314010254560693, 1.0, 1.0, 0.046816147041429654], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1171685], dtype=float32), -1.3359826]. 
=============================================
[2019-04-01 18:47:11,206] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:47:11,207] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9121
[2019-04-01 18:47:11,224] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 26.0, 23.61733839462865, 5.46279649173762, 0.0, 1.0, 44.58379043049113], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1918800.0000, 
sim time next is 1919400.0000, 
raw observation next is [-8.9, 82.00000000000001, 0.0, 0.0, 26.0, 23.57768527954434, 5.498738782270952, 0.0, 1.0, 44.55426363115505], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.8200000000000002, 0.0, 0.0, 1.0, 0.6539550399349058, 0.054987387822709524, 0.0, 1.0, 0.31824474022253607], 
reward next is 0.6818, 
noisyNet noise sample is [array([1.2460984], dtype=float32), 2.380269]. 
=============================================
[2019-04-01 18:47:12,362] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:47:12,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0311
[2019-04-01 18:47:12,374] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 86.66666666666667, 105.8333333333333, 0.0, 26.0, 25.52360705502602, 9.539397363421168, 1.0, 1.0, 14.98394077318167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1687200.0000, 
sim time next is 1687800.0000, 
raw observation next is [1.1, 87.33333333333334, 104.6666666666667, 0.0, 26.0, 25.40427374794099, 9.456987850850945, 1.0, 1.0, 14.92576833994511], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.8733333333333334, 0.348888888888889, 0.0, 1.0, 0.9148962497058558, 0.09456987850850945, 1.0, 1.0, 0.10661263099960792], 
reward next is 0.8934, 
noisyNet noise sample is [array([-0.2782219], dtype=float32), -1.1689421]. 
=============================================
[2019-04-01 18:47:15,054] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.5339574e-27
 1.0000000e+00 6.0413116e-32], sum to 1.0000
[2019-04-01 18:47:15,059] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7395
[2019-04-01 18:47:15,083] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.25, 93.5, 0.0, 0.0, 26.0, 25.26946261425346, 9.988315273582954, 0.0, 1.0, 42.7955780624674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1726200.0000, 
sim time next is 1726800.0000, 
raw observation next is [0.3333333333333333, 93.0, 0.0, 0.0, 26.0, 25.26494454298869, 9.930118141392933, 0.0, 1.0, 42.73881502801507], 
processed observation next is [1.0, 1.0, 0.4718374884579871, 0.93, 0.0, 0.0, 1.0, 0.8949920775698129, 0.09930118141392934, 0.0, 1.0, 0.30527725020010765], 
reward next is 0.6947, 
noisyNet noise sample is [array([-1.4284511], dtype=float32), 0.702428]. 
=============================================
[2019-04-01 18:47:37,336] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:47:37,336] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5707
[2019-04-01 18:47:37,366] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.0, 38.5, 0.0, 26.0, 25.89796330600536, 9.375174725889343, 1.0, 1.0, 19.40608653352334], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2046000.0000, 
sim time next is 2046600.0000, 
raw observation next is [-3.9, 82.0, 32.0, 0.0, 26.0, 25.7879136068595, 8.730366154219597, 1.0, 1.0, 18.56926341477751], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.10666666666666667, 0.0, 1.0, 0.9697019438370715, 0.08730366154219597, 1.0, 1.0, 0.13263759581983936], 
reward next is 0.8674, 
noisyNet noise sample is [array([0.9921785], dtype=float32), -0.6326598]. 
=============================================
[2019-04-01 18:47:45,165] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:47:45,167] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3573
[2019-04-01 18:47:45,231] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.12880719984425, 8.216178876220885, 1.0, 1.0, 41.46409229081709], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2137200.0000, 
sim time next is 2137800.0000, 
raw observation next is [-4.916666666666667, 70.5, 0.0, 0.0, 26.0, 24.99029439910934, 8.763512768678153, 1.0, 1.0, 53.79752158314851], 
processed observation next is [1.0, 0.7391304347826086, 0.32640812557710064, 0.705, 0.0, 0.0, 1.0, 0.855756342729906, 0.08763512768678153, 1.0, 1.0, 0.38426801130820365], 
reward next is 0.6157, 
noisyNet noise sample is [array([0.17563792], dtype=float32), 1.0179026]. 
=============================================
[2019-04-01 18:48:09,208] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:48:09,214] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8560
[2019-04-01 18:48:09,252] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 56.0, 93.5, 25.5, 26.0, 25.66732723726205, 7.037056392839318, 1.0, 1.0, 22.34319084909033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2538000.0000, 
sim time next is 2538600.0000, 
raw observation next is [-2.533333333333334, 54.83333333333334, 107.6666666666667, 28.0, 26.0, 25.72328496587322, 7.110523289996777, 1.0, 1.0, 21.07837338682151], 
processed observation next is [1.0, 0.391304347826087, 0.39242843951985223, 0.5483333333333335, 0.358888888888889, 0.030939226519337018, 1.0, 0.9604692808390313, 0.07110523289996777, 1.0, 1.0, 0.15055980990586793], 
reward next is 0.8494, 
noisyNet noise sample is [array([-0.10257061], dtype=float32), -0.50089276]. 
=============================================
[2019-04-01 18:48:13,156] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:48:13,156] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4855
[2019-04-01 18:48:13,164] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 7.214281e-38 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:48:13,165] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2309
[2019-04-01 18:48:13,180] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.2333333333333333, 35.66666666666667, 0.0, 0.0, 26.0, 25.11988680098164, 7.533898946107001, 1.0, 1.0, 16.74375454744285], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2572800.0000, 
sim time next is 2573400.0000, 
raw observation next is [-0.4166666666666667, 35.83333333333333, 0.0, 0.0, 26.0, 25.03089221587036, 7.37008188039641, 1.0, 1.0, 22.98614382473117], 
processed observation next is [1.0, 0.782608695652174, 0.45106186518928904, 0.3583333333333333, 0.0, 0.0, 1.0, 0.8615560308386228, 0.0737008188039641, 1.0, 1.0, 0.16418674160522265], 
reward next is 0.8358, 
noisyNet noise sample is [array([-0.39006647], dtype=float32), 0.3563228]. 
=============================================
[2019-04-01 18:48:13,203] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8333333333333334, 94.16666666666666, 67.66666666666666, 51.99999999999999, 26.0, 25.44674242877264, 7.518460635726768, 1.0, 1.0, 26.13628238429087], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2884200.0000, 
sim time next is 2884800.0000, 
raw observation next is [0.6666666666666667, 95.33333333333334, 58.33333333333333, 25.99999999999999, 26.0, 25.44061104783807, 7.500296829871679, 1.0, 1.0, 23.95865186744549], 
processed observation next is [1.0, 0.391304347826087, 0.4810710987996307, 0.9533333333333335, 0.19444444444444442, 0.028729281767955788, 1.0, 0.9200872925482955, 0.07500296829871679, 1.0, 1.0, 0.17113322762461064], 
reward next is 0.8289, 
noisyNet noise sample is [array([-1.961024], dtype=float32), 0.5860577]. 
=============================================
[2019-04-01 18:48:17,563] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:48:17,566] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9140
[2019-04-01 18:48:17,596] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 49.5, 128.3333333333333, 178.6666666666667, 26.0, 26.30032230604948, 11.0082100728461, 1.0, 1.0, 12.55956316883246], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2649000.0000, 
sim time next is 2649600.0000, 
raw observation next is [0.5, 50.0, 115.0, 165.0, 26.0, 26.19142586086988, 10.92172124203917, 1.0, 1.0, 11.35193103163893], 
processed observation next is [1.0, 0.6956521739130435, 0.4764542936288089, 0.5, 0.38333333333333336, 0.18232044198895028, 1.0, 1.0273465515528402, 0.1092172124203917, 1.0, 1.0, 0.08108522165456379], 
reward next is 0.5502, 
noisyNet noise sample is [array([-0.02383267], dtype=float32), -0.7157434]. 
=============================================
[2019-04-01 18:48:20,554] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-01 18:48:20,555] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 18:48:20,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:48:20,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run33
[2019-04-01 18:48:20,577] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 18:48:20,581] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 18:48:20,583] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:48:20,584] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:48:20,588] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run33
[2019-04-01 18:48:20,618] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run33
[2019-04-01 18:50:02,881] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 18:50:20,433] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 18:50:25,034] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 18:50:26,058] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 3200000, evaluation results [3200000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 18:50:27,739] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 1.111911e-28], sum to 1.0000
[2019-04-01 18:50:27,741] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7662
[2019-04-01 18:50:27,754] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.31666666666667, 77.16666666666667, 0.0, 0.0, 26.0, 24.15188223168932, 5.727716215269463, 0.0, 1.0, 43.80183735218916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2686200.0000, 
sim time next is 2686800.0000, 
raw observation next is [-11.63333333333333, 78.33333333333334, 0.0, 0.0, 26.0, 24.1497811451682, 5.678497204023493, 0.0, 1.0, 43.79604233081761], 
processed observation next is [1.0, 0.08695652173913043, 0.14035087719298256, 0.7833333333333334, 0.0, 0.0, 1.0, 0.735683020738314, 0.056784972040234936, 0.0, 1.0, 0.3128288737915544], 
reward next is 0.6872, 
noisyNet noise sample is [array([1.6674255], dtype=float32), -0.8494223]. 
=============================================
[2019-04-01 18:50:44,272] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:50:44,275] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1510
[2019-04-01 18:50:44,299] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.07682947507054, 7.226762650632551, 0.0, 1.0, 37.96523515016096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3019200.0000, 
sim time next is 3019800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.03695972377248, 7.122191757578785, 0.0, 1.0, 37.88291308183561], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 1.0, 0.8624228176817829, 0.07122191757578786, 0.0, 1.0, 0.2705922362988258], 
reward next is 0.7294, 
noisyNet noise sample is [array([-0.768419], dtype=float32), 0.4735196]. 
=============================================
[2019-04-01 18:50:47,905] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6810034e-36
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:50:47,906] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3722
[2019-04-01 18:50:47,921] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 72.33333333333333, 0.0, 0.0, 26.0, 23.8492915717759, 5.231598649229052, 0.0, 1.0, 39.6558564237436], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3044400.0000, 
sim time next is 3045000.0000, 
raw observation next is [-6.0, 71.16666666666667, 0.0, 0.0, 26.0, 23.82286940518889, 5.22760474618975, 0.0, 1.0, 39.67727782680653], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.7116666666666667, 0.0, 0.0, 1.0, 0.6889813435984128, 0.052276047461897505, 0.0, 1.0, 0.2834091273343324], 
reward next is 0.7166, 
noisyNet noise sample is [array([-0.00120934], dtype=float32), -1.0507784]. 
=============================================
[2019-04-01 18:50:47,932] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[68.764404]
 [68.80059 ]
 [68.82266 ]
 [68.84889 ]
 [68.85586 ]], R is [[68.76313782]
 [68.79225159]
 [68.82118988]
 [68.84972382]
 [68.87786102]].
[2019-04-01 18:50:48,166] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:50:48,166] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5453
[2019-04-01 18:50:48,212] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.833333333333333, 59.5, 90.33333333333334, 727.6666666666666, 26.0, 26.44341549084721, 15.02402356415118, 1.0, 1.0, 11.42134994238685], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3424200.0000, 
sim time next is 3424800.0000, 
raw observation next is [2.666666666666667, 61.0, 87.16666666666666, 715.8333333333334, 26.0, 26.62878872147653, 15.6935947159486, 1.0, 1.0, 11.24166159813934], 
processed observation next is [1.0, 0.6521739130434783, 0.5364727608494922, 0.61, 0.2905555555555555, 0.7909760589318601, 1.0, 1.0898269602109327, 0.156935947159486, 1.0, 1.0, 0.08029758284385242], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7069135], dtype=float32), -0.6617367]. 
=============================================
[2019-04-01 18:50:56,633] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:50:56,636] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7994
[2019-04-01 18:50:56,721] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.0, 78.66666666666667, 0.0, 0.0, 26.0, 24.07803864364571, 7.242248363291101, 1.0, 1.0, 94.65136301927855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3309600.0000, 
sim time next is 3310200.0000, 
raw observation next is [-11.0, 80.0, 2.0, 94.0, 26.0, 24.90436748160916, 8.418168612748694, 1.0, 1.0, 71.32365373626175], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.8, 0.006666666666666667, 0.10386740331491713, 1.0, 0.8434810688013086, 0.08418168612748694, 1.0, 1.0, 0.5094546695447267], 
reward next is 0.4905, 
noisyNet noise sample is [array([-0.3628941], dtype=float32), 0.7931276]. 
=============================================
[2019-04-01 18:50:59,840] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:50:59,843] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4780
[2019-04-01 18:50:59,868] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.333333333333333, 66.0, 111.8333333333333, 749.6666666666667, 26.0, 26.31705170804103, 12.67601834305393, 1.0, 1.0, 15.85735778929522], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3321600.0000, 
sim time next is 3322200.0000, 
raw observation next is [-7.166666666666667, 65.0, 112.6666666666667, 759.3333333333333, 26.0, 26.34111369384928, 12.8564901249404, 1.0, 1.0, 14.94050495700625], 
processed observation next is [1.0, 0.43478260869565216, 0.26408125577100644, 0.65, 0.37555555555555564, 0.8390423572744014, 1.0, 1.0487305276927543, 0.128564901249404, 1.0, 1.0, 0.10671789255004464], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.11158968], dtype=float32), -0.17045744]. 
=============================================
[2019-04-01 18:51:05,907] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.2183481e-33], sum to 1.0000
[2019-04-01 18:51:05,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5725
[2019-04-01 18:51:05,924] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.52898027133818, 9.983959051755171, 0.0, 1.0, 29.60479681544053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3455400.0000, 
sim time next is 3456000.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.50227385342282, 9.811171949482327, 0.0, 1.0, 28.74187775955269], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.86, 0.0, 0.0, 1.0, 0.9288962647746887, 0.09811171949482328, 0.0, 1.0, 0.20529912685394777], 
reward next is 0.7947, 
noisyNet noise sample is [array([0.49184608], dtype=float32), -1.942667]. 
=============================================
[2019-04-01 18:51:05,930] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[64.5149  ]
 [64.51582 ]
 [64.50116 ]
 [64.780945]
 [65.073616]], R is [[64.05027008]
 [64.19830322]
 [64.32373047]
 [64.39417267]
 [64.45250702]].
[2019-04-01 18:51:09,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:51:09,232] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5823
[2019-04-01 18:51:09,246] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 81.33333333333334, 0.0, 0.0, 26.0, 25.841840568428, 12.56694028887621, 0.0, 1.0, 28.56716325311097], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3446400.0000, 
sim time next is 3447000.0000, 
raw observation next is [1.0, 82.5, 0.0, 0.0, 26.0, 25.8211153416514, 12.3077428659977, 0.0, 1.0, 26.997266751124], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.825, 0.0, 0.0, 1.0, 0.9744450488073427, 0.12307742865997699, 0.0, 1.0, 0.1928376196508857], 
reward next is 0.8072, 
noisyNet noise sample is [array([-0.56099105], dtype=float32), -0.47478828]. 
=============================================
[2019-04-01 18:51:09,253] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[67.96382 ]
 [68.017204]
 [67.50937 ]
 [67.00676 ]
 [66.048805]], R is [[68.10909271]
 [68.22395325]
 [68.32611084]
 [68.41936493]
 [68.47871399]].
[2019-04-01 18:51:11,725] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8387744e-32 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.9443489e-30
 1.0000000e+00 8.8215359e-31], sum to 1.0000
[2019-04-01 18:51:11,726] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1429
[2019-04-01 18:51:11,738] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 69.0, 0.0, 0.0, 26.0, 25.22265313444304, 8.557222652139018, 0.0, 1.0, 40.23934281547135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3550800.0000, 
sim time next is 3551400.0000, 
raw observation next is [-3.0, 68.0, 0.0, 0.0, 26.0, 25.18978628798963, 8.420327411182894, 0.0, 1.0, 40.23966264462522], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.68, 0.0, 0.0, 1.0, 0.8842551839985185, 0.08420327411182894, 0.0, 1.0, 0.287426161747323], 
reward next is 0.7126, 
noisyNet noise sample is [array([-0.07892983], dtype=float32), -1.475788]. 
=============================================
[2019-04-01 18:51:25,527] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 5.3585223e-37 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:51:25,529] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5556
[2019-04-01 18:51:25,551] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.22067810408181, 8.282126462274114, 0.0, 1.0, 40.2874179627416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4058400.0000, 
sim time next is 4059000.0000, 
raw observation next is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.19505582342709, 8.164103804992237, 0.0, 1.0, 40.25082524998406], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.37, 0.0, 0.0, 1.0, 0.8850079747752986, 0.08164103804992237, 0.0, 1.0, 0.2875058946427433], 
reward next is 0.7125, 
noisyNet noise sample is [array([-0.8228706], dtype=float32), -1.0310912]. 
=============================================
[2019-04-01 18:51:25,561] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[67.35973 ]
 [67.495285]
 [67.54135 ]
 [67.06232 ]
 [67.47975 ]], R is [[68.04214478]
 [68.07395935]
 [68.10501099]
 [68.13472748]
 [68.16218567]].
[2019-04-01 18:51:28,077] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4633130e-15 1.4664736e-33 5.6247551e-21 1.2003555e-10 4.1128812e-22
 1.0000000e+00 1.2947015e-18], sum to 1.0000
[2019-04-01 18:51:28,081] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8029
[2019-04-01 18:51:28,156] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.26730275374057, 6.500977900373736, 1.0, 1.0, 94.14545509320479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3828000.0000, 
sim time next is 3828600.0000, 
raw observation next is [-5.0, 77.0, 5.0, 149.0, 26.0, 24.98785587779801, 7.392338234514651, 1.0, 1.0, 61.34763607910684], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.77, 0.016666666666666666, 0.16464088397790055, 1.0, 0.8554079825425731, 0.0739233823451465, 1.0, 1.0, 0.43819740056504886], 
reward next is 0.5618, 
noisyNet noise sample is [array([0.29777876], dtype=float32), -0.24069765]. 
=============================================
[2019-04-01 18:51:28,222] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:51:28,224] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7250
[2019-04-01 18:51:28,275] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 77.0, 19.33333333333333, 198.6666666666667, 26.0, 25.35156411914073, 7.980217847675813, 1.0, 1.0, 37.16908483185489], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3829200.0000, 
sim time next is 3829800.0000, 
raw observation next is [-5.0, 77.0, 33.66666666666666, 248.3333333333333, 26.0, 25.33011963078189, 7.909048362894389, 1.0, 1.0, 34.67089014892685], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.77, 0.1122222222222222, 0.2744014732965009, 1.0, 0.904302804397413, 0.07909048362894389, 1.0, 1.0, 0.2476492153494775], 
reward next is 0.7524, 
noisyNet noise sample is [array([-2.2551203], dtype=float32), 0.84398687]. 
=============================================
[2019-04-01 18:51:28,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:51:28,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1821
[2019-04-01 18:51:28,659] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 26.0, 25.42753933079539, 7.725833972607205, 0.0, 1.0, 34.69061225909751], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4234800.0000, 
sim time next is 4235400.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.39265129674711, 7.677908918363056, 0.0, 1.0, 36.74834290733627], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 1.0, 0.9132358995353016, 0.07677908918363056, 0.0, 1.0, 0.26248816362383054], 
reward next is 0.7375, 
noisyNet noise sample is [array([-0.94312394], dtype=float32), -1.5475607]. 
=============================================
[2019-04-01 18:51:34,740] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:51:34,744] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8906
[2019-04-01 18:51:34,777] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.73333333333333, 28.66666666666666, 117.5, 851.1666666666667, 26.0, 28.27900819953048, 24.27188315730562, 1.0, 1.0, 1.679975241041416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4365600.0000, 
sim time next is 4366200.0000, 
raw observation next is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 26.0, 27.72194546096762, 24.81431716457412, 1.0, 1.0, 1.654359741494354], 
processed observation next is [1.0, 0.5217391304347826, 0.8688827331486613, 0.2883333333333334, 0.39, 0.9384898710865562, 1.0, 1.2459922087096602, 0.2481431716457412, 1.0, 1.0, 0.011816855296388243], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.50910425], dtype=float32), -0.15840697]. 
=============================================
[2019-04-01 18:51:35,448] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.0407833e-26 3.1398529e-16 4.1876645e-32 1.0000000e+00 0.0000000e+00
 0.0000000e+00 1.0922852e-30], sum to 1.0000
[2019-04-01 18:51:35,449] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9503
[2019-04-01 18:51:35,463] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.333333333333333, 22.0, 101.5, 780.3333333333334, 19.0, 21.64099048084119, 11.44236010094697, 1.0, 1.0, 2.287436318056909], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4027200.0000, 
sim time next is 4027800.0000, 
raw observation next is [-2.166666666666667, 21.0, 99.0, 766.6666666666666, 19.0, 21.72405001818976, 11.21975586664872, 1.0, 1.0, 2.215694836658499], 
processed observation next is [1.0, 0.6086956521739131, 0.4025854108956602, 0.21, 0.33, 0.8471454880294659, 0.0, 0.38915000259853727, 0.1121975586664872, 1.0, 1.0, 0.01582639169041785], 
reward next is 0.4963, 
noisyNet noise sample is [array([-0.6505355], dtype=float32), -1.0435324]. 
=============================================
[2019-04-01 18:51:36,170] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3025289e-27 0.0000000e+00 2.5808620e-31 2.7037765e-27 2.2256382e-25
 1.0000000e+00 3.3174042e-26], sum to 1.0000
[2019-04-01 18:51:36,170] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8534
[2019-04-01 18:51:36,185] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 63.00000000000001, 0.0, 0.0, 26.0, 24.36933336161964, 6.006021891225044, 0.0, 1.0, 43.20538957189741], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3982800.0000, 
sim time next is 3983400.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.3657461399025, 5.894080937101168, 0.0, 1.0, 43.17513503526329], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.63, 0.0, 0.0, 1.0, 0.7665351628432145, 0.058940809371011676, 0.0, 1.0, 0.30839382168045204], 
reward next is 0.6916, 
noisyNet noise sample is [array([-1.0535892], dtype=float32), 0.57387733]. 
=============================================
[2019-04-01 18:51:41,234] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.45024364e-36 1.00000000e+00 1.85089485e-30], sum to 1.0000
[2019-04-01 18:51:41,236] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3746
[2019-04-01 18:51:41,252] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333333, 42.66666666666667, 0.0, 0.0, 26.0, 25.34772578854341, 9.651615814566085, 0.0, 1.0, 40.15009893138116], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4144800.0000, 
sim time next is 4145400.0000, 
raw observation next is [-0.5, 42.5, 0.0, 0.0, 26.0, 25.41495501995479, 9.756393571288042, 0.0, 1.0, 39.66264778330023], 
processed observation next is [1.0, 1.0, 0.44875346260387816, 0.425, 0.0, 0.0, 1.0, 0.916422145707827, 0.09756393571288041, 0.0, 1.0, 0.28330462702357306], 
reward next is 0.7167, 
noisyNet noise sample is [array([-0.40974304], dtype=float32), 0.5874219]. 
=============================================
[2019-04-01 18:51:42,189] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2156934e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.2665141e-35], sum to 1.0000
[2019-04-01 18:51:42,190] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1446
[2019-04-01 18:51:42,202] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 36.0, 0.0, 0.0, 26.0, 24.79486814371509, 6.153711181935175, 0.0, 1.0, 39.7191333815475], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4080600.0000, 
sim time next is 4081200.0000, 
raw observation next is [-4.0, 36.66666666666666, 0.0, 0.0, 26.0, 24.78427525187515, 6.073445747016429, 0.0, 1.0, 39.69222038353409], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.3666666666666666, 0.0, 0.0, 1.0, 0.8263250359821644, 0.06073445747016429, 0.0, 1.0, 0.28351585988238637], 
reward next is 0.7165, 
noisyNet noise sample is [array([-1.3333077], dtype=float32), 0.32166618]. 
=============================================
[2019-04-01 18:51:44,898] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 1.00000000e+00 1.02229046e-29], sum to 1.0000
[2019-04-01 18:51:44,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2010
[2019-04-01 18:51:44,927] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 29.66666666666667, 116.6666666666667, 831.8333333333334, 26.0, 26.49844159351097, 14.32943159273061, 1.0, 1.0, 2.31534681916518], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4108800.0000, 
sim time next is 4109400.0000, 
raw observation next is [3.0, 30.0, 116.0, 830.0, 26.0, 26.57026088803983, 12.69089375781367, 1.0, 1.0, 2.4196307084822], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.3, 0.38666666666666666, 0.9171270718232044, 1.0, 1.0814658411485472, 0.1269089375781367, 1.0, 1.0, 0.017283076489158573], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.902063], dtype=float32), -0.73539495]. 
=============================================
[2019-04-01 18:51:48,549] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:51:48,550] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0898
[2019-04-01 18:51:48,561] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 50.5, 247.0, 48.0, 26.0, 25.86604003594577, 10.70132248270318, 1.0, 1.0, 7.498966191504371], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4541400.0000, 
sim time next is 4542000.0000, 
raw observation next is [2.666666666666667, 50.0, 249.8333333333333, 58.83333333333333, 26.0, 25.92364615141325, 11.05190966234436, 1.0, 1.0, 7.62766185077609], 
processed observation next is [1.0, 0.5652173913043478, 0.5364727608494922, 0.5, 0.8327777777777776, 0.06500920810313075, 1.0, 0.9890923073447502, 0.11051909662344359, 1.0, 1.0, 0.05448329893411493], 
reward next is 0.5248, 
noisyNet noise sample is [array([-0.08428274], dtype=float32), -0.4993349]. 
=============================================
[2019-04-01 18:51:48,582] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[77.80187 ]
 [77.76066 ]
 [77.65615 ]
 [77.507286]
 [77.22377 ]], R is [[77.32920837]
 [77.22182465]
 [77.23085022]
 [77.37501526]
 [77.54794312]].
[2019-04-01 18:51:51,048] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.1923893e-23], sum to 1.0000
[2019-04-01 18:51:51,052] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0552
[2019-04-01 18:51:51,113] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.66666666666667, 73.33333333333334, 35.33333333333334, 26.0, 25.31697263376582, 7.943483848957247, 0.0, 1.0, 54.46371550953221], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4263000.0000, 
sim time next is 4263600.0000, 
raw observation next is [3.0, 50.33333333333334, 91.66666666666667, 44.16666666666667, 26.0, 25.53551606626886, 8.275713782538345, 0.0, 1.0, 36.27137691604414], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.5033333333333334, 0.3055555555555556, 0.04880294659300185, 1.0, 0.9336451523241226, 0.08275713782538345, 0.0, 1.0, 0.2590812636860296], 
reward next is 0.7409, 
noisyNet noise sample is [array([-0.04005759], dtype=float32), -0.22552308]. 
=============================================
[2019-04-01 18:51:52,547] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:51:52,548] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3061
[2019-04-01 18:51:52,573] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.6, 71.66666666666667, 0.0, 0.0, 26.0, 25.07343579241399, 8.842642480147061, 0.0, 1.0, 46.61243851162745], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4304400.0000, 
sim time next is 4305000.0000, 
raw observation next is [5.500000000000001, 72.33333333333333, 0.0, 0.0, 26.0, 25.37120600744088, 9.463077005448982, 0.0, 1.0, 42.52881760330519], 
processed observation next is [0.0, 0.8260869565217391, 0.6149584487534627, 0.7233333333333333, 0.0, 0.0, 1.0, 0.9101722867772685, 0.09463077005448982, 0.0, 1.0, 0.30377726859503706], 
reward next is 0.6962, 
noisyNet noise sample is [array([-0.29354766], dtype=float32), 0.014250631]. 
=============================================
[2019-04-01 18:51:52,579] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[68.81691]
 [69.6124 ]
 [70.76084]
 [71.82338]
 [72.3079 ]], R is [[68.07121277]
 [68.05755615]
 [67.96924591]
 [67.98360443]
 [68.18417358]].
[2019-04-01 18:51:54,242] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:51:54,242] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9169
[2019-04-01 18:51:54,251] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.95, 61.5, 0.0, 0.0, 26.0, 26.38443324472447, 15.62846242921639, 0.0, 1.0, 6.852551499548274], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4401000.0000, 
sim time next is 4401600.0000, 
raw observation next is [8.8, 61.66666666666666, 0.0, 0.0, 26.0, 26.23824918433222, 15.00134756361167, 0.0, 1.0, 6.826950624676911], 
processed observation next is [1.0, 0.9565217391304348, 0.7063711911357342, 0.6166666666666666, 0.0, 0.0, 1.0, 1.0340355977617455, 0.1500134756361167, 0.0, 1.0, 0.04876393303340651], 
reward next is 0.9512, 
noisyNet noise sample is [array([-0.573671], dtype=float32), -0.52358425]. 
=============================================
[2019-04-01 18:51:56,439] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:51:56,440] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5465
[2019-04-01 18:51:56,456] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.833333333333333, 70.0, 0.0, 0.0, 26.0, 25.51561268278828, 11.17182034342624, 0.0, 1.0, 38.01440439535855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4428600.0000, 
sim time next is 4429200.0000, 
raw observation next is [2.666666666666667, 72.0, 0.0, 0.0, 26.0, 25.56389904548907, 11.14885735244404, 0.0, 1.0, 35.15616795048642], 
processed observation next is [1.0, 0.2608695652173913, 0.5364727608494922, 0.72, 0.0, 0.0, 1.0, 0.9376998636412957, 0.1114885735244404, 0.0, 1.0, 0.2511154853606173], 
reward next is 0.7489, 
noisyNet noise sample is [array([2.2411628], dtype=float32), -0.80191034]. 
=============================================
[2019-04-01 18:51:58,179] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:51:58,179] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0523
[2019-04-01 18:51:58,204] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.4, 44.0, 0.0, 0.0, 26.0, 28.13112047693082, 29.77567864443099, 1.0, 1.0, 5.792555787427032], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4384800.0000, 
sim time next is 4385400.0000, 
raw observation next is [12.33333333333333, 45.0, 0.0, 0.0, 26.0, 28.11698437493393, 25.38406952752239, 1.0, 1.0, 6.209200607781242], 
processed observation next is [1.0, 0.782608695652174, 0.8042474607571561, 0.45, 0.0, 0.0, 1.0, 1.3024263392762758, 0.2538406952752239, 1.0, 1.0, 0.04435143291272316], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.49823645], dtype=float32), 0.27708292]. 
=============================================
[2019-04-01 18:52:00,715] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2130949e-31 0.0000000e+00 0.0000000e+00 3.5091822e-29 4.3636554e-27
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:52:00,717] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1721
[2019-04-01 18:52:00,779] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.92381501313528, 9.153934179206688, 1.0, 1.0, 34.12429386252794], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4475400.0000, 
sim time next is 4476000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.8754862669934, 9.718173659460376, 1.0, 1.0, 53.72985800971604], 
processed observation next is [1.0, 0.8260869565217391, 0.46260387811634357, 0.72, 0.0, 0.0, 1.0, 0.8393551809990569, 0.09718173659460376, 1.0, 1.0, 0.3837847000694003], 
reward next is 0.6162, 
noisyNet noise sample is [array([0.96298456], dtype=float32), -0.60451806]. 
=============================================
[2019-04-01 18:52:00,784] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[39.76889 ]
 [41.758156]
 [43.53389 ]
 [38.168175]
 [31.962921]], R is [[37.80688477]
 [38.18507385]
 [38.63957214]
 [39.1090889 ]
 [39.57269287]].
[2019-04-01 18:52:03,824] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5014041e-26 0.0000000e+00 5.7566168e-32 2.8418361e-12 1.8121163e-27
 1.0000000e+00 2.5050603e-21], sum to 1.0000
[2019-04-01 18:52:03,827] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4744
[2019-04-01 18:52:03,838] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.866666666666667, 76.0, 0.0, 0.0, 26.0, 25.03219956806888, 7.434391755126299, 0.0, 1.0, 35.8498913156832], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4603200.0000, 
sim time next is 4603800.0000, 
raw observation next is [-2.933333333333333, 76.5, 0.0, 0.0, 26.0, 24.99779923853835, 7.428005877654601, 0.0, 1.0, 35.86448788382798], 
processed observation next is [1.0, 0.2608695652173913, 0.38134810710988, 0.765, 0.0, 0.0, 1.0, 0.8568284626483356, 0.07428005877654602, 0.0, 1.0, 0.25617491345591414], 
reward next is 0.7438, 
noisyNet noise sample is [array([0.7228757], dtype=float32), -1.5454928]. 
=============================================
[2019-04-01 18:52:04,737] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 7.3217065e-25 1.2057097e-31
 1.0000000e+00 1.2960044e-28], sum to 1.0000
[2019-04-01 18:52:04,740] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3112
[2019-04-01 18:52:04,784] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 74.0, 0.0, 0.0, 26.0, 25.20835936691785, 7.98992940970957, 1.0, 1.0, 30.42979834777274], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4606200.0000, 
sim time next is 4606800.0000, 
raw observation next is [-2.333333333333333, 73.0, 20.5, 28.49999999999999, 26.0, 25.24164941810417, 8.153709228685237, 1.0, 1.0, 28.17556843608838], 
processed observation next is [1.0, 0.30434782608695654, 0.3979686057248385, 0.73, 0.06833333333333333, 0.03149171270718231, 1.0, 0.8916642025863101, 0.08153709228685237, 1.0, 1.0, 0.20125406025777415], 
reward next is 0.7987, 
noisyNet noise sample is [array([-2.036755], dtype=float32), -0.12790765]. 
=============================================
[2019-04-01 18:52:05,392] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:52:05,393] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2421
[2019-04-01 18:52:05,399] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 52.5, 0.0, 0.0, 26.0, 25.73512274930822, 12.03742037147187, 1.0, 1.0, 5.725629472022446], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4649400.0000, 
sim time next is 4650000.0000, 
raw observation next is [2.333333333333333, 52.33333333333333, 0.0, 0.0, 26.0, 25.6746334232954, 11.74309668145846, 1.0, 1.0, 5.997383298417559], 
processed observation next is [1.0, 0.8260869565217391, 0.5272391505078486, 0.5233333333333333, 0.0, 0.0, 1.0, 0.9535190604707715, 0.1174309668145846, 1.0, 1.0, 0.042838452131553996], 
reward next is 0.2599, 
noisyNet noise sample is [array([-0.6288444], dtype=float32), 0.93411154]. 
=============================================
[2019-04-01 18:52:05,410] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[63.648067]
 [66.45074 ]
 [61.82996 ]
 [55.930447]
 [56.16975 ]], R is [[60.7062149 ]
 [60.24328613]
 [60.60529327]
 [60.96417618]
 [60.35453415]].
[2019-04-01 18:52:07,209] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:52:07,209] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2261
[2019-04-01 18:52:07,228] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.2, 65.0, 0.0, 0.0, 26.0, 25.03064906889805, 7.181568632061446, 0.0, 1.0, 35.25188482176685], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4586400.0000, 
sim time next is 4587000.0000, 
raw observation next is [-0.35, 65.33333333333334, 0.0, 0.0, 26.0, 25.05826976153013, 7.151023036913425, 0.0, 1.0, 35.23642993420127], 
processed observation next is [1.0, 0.08695652173913043, 0.45290858725761773, 0.6533333333333334, 0.0, 0.0, 1.0, 0.8654671087900186, 0.07151023036913425, 0.0, 1.0, 0.2516887852442948], 
reward next is 0.7483, 
noisyNet noise sample is [array([0.14008401], dtype=float32), -1.4776958]. 
=============================================
[2019-04-01 18:52:07,242] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[64.59601]
 [64.81016]
 [65.00148]
 [65.1662 ]
 [65.33704]], R is [[64.52444458]
 [64.62740326]
 [64.72923279]
 [64.82969666]
 [64.92910004]].
[2019-04-01 18:52:07,797] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:52:07,797] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1213
[2019-04-01 18:52:07,811] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666666, 97.33333333333333, 0.0, 0.0, 26.0, 25.56983845521225, 9.34829289085789, 0.0, 1.0, 24.02681334990774], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4682400.0000, 
sim time next is 4683000.0000, 
raw observation next is [-0.8333333333333334, 98.66666666666667, 0.0, 0.0, 26.0, 25.58410404958131, 9.259478960210132, 0.0, 1.0, 24.10884464569659], 
processed observation next is [1.0, 0.17391304347826086, 0.43951985226223456, 0.9866666666666667, 0.0, 0.0, 1.0, 0.94058629279733, 0.09259478960210131, 0.0, 1.0, 0.17220603318354707], 
reward next is 0.8278, 
noisyNet noise sample is [array([-3.590249], dtype=float32), -0.28487772]. 
=============================================
[2019-04-01 18:52:07,830] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[67.53741 ]
 [67.66502 ]
 [67.789825]
 [67.99365 ]
 [68.331726]], R is [[67.61180878]
 [67.7640686 ]
 [67.91330719]
 [68.02858734]
 [68.08145142]].
[2019-04-01 18:52:09,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:09,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:09,107] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run25
[2019-04-01 18:52:10,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:10,866] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:10,883] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run25
[2019-04-01 18:52:16,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:16,774] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:16,785] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run25
[2019-04-01 18:52:17,642] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:52:17,643] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9276
[2019-04-01 18:52:17,658] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 38.5, 220.0, 568.0, 26.0, 25.17996351325747, 9.529670740879498, 0.0, 1.0, 9.043888720430717], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4800600.0000, 
sim time next is 4801200.0000, 
raw observation next is [2.666666666666667, 38.0, 208.0, 597.0, 26.0, 25.17017318713927, 9.54243279613567, 0.0, 1.0, 8.475705880985982], 
processed observation next is [0.0, 0.5652173913043478, 0.5364727608494922, 0.38, 0.6933333333333334, 0.6596685082872928, 1.0, 0.881453312448467, 0.09542432796135669, 0.0, 1.0, 0.060540756292757014], 
reward next is 0.9395, 
noisyNet noise sample is [array([1.0993683], dtype=float32), 1.0605395]. 
=============================================
[2019-04-01 18:52:20,452] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:52:20,455] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7698
[2019-04-01 18:52:20,470] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 69.0, 23.5, 52.16666666666666, 26.0, 24.38605784808983, 5.60121257613542, 0.0, 1.0, 39.06801205433442], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4866000.0000, 
sim time next is 4866600.0000, 
raw observation next is [-4.0, 70.0, 46.99999999999999, 104.3333333333333, 26.0, 24.36573687515565, 5.647843676936115, 0.0, 1.0, 38.92771343065895], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.7, 0.15666666666666665, 0.11528545119705337, 1.0, 0.7665338393079502, 0.05647843676936115, 0.0, 1.0, 0.2780550959332782], 
reward next is 0.7219, 
noisyNet noise sample is [array([-0.68154037], dtype=float32), -1.2152226]. 
=============================================
[2019-04-01 18:52:22,106] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:52:22,108] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2064
[2019-04-01 18:52:22,131] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666667, 37.0, 0.0, 0.0, 26.0, 25.46346892598388, 8.156351549409818, 0.0, 1.0, 33.6111408132704], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4918800.0000, 
sim time next is 4919400.0000, 
raw observation next is [0.5, 37.5, 0.0, 0.0, 26.0, 25.50233581419004, 8.049201212381712, 0.0, 1.0, 33.04123988313725], 
processed observation next is [0.0, 0.9565217391304348, 0.4764542936288089, 0.375, 0.0, 0.0, 1.0, 0.9289051163128629, 0.08049201212381712, 0.0, 1.0, 0.2360088563081232], 
reward next is 0.7640, 
noisyNet noise sample is [array([2.3149507], dtype=float32), -0.22852702]. 
=============================================
[2019-04-01 18:52:27,238] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:27,239] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:27,278] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run25
[2019-04-01 18:52:27,932] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:27,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:27,946] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run25
[2019-04-01 18:52:28,350] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2588241e-24
 1.0000000e+00 3.3492295e-27], sum to 1.0000
[2019-04-01 18:52:28,351] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9670
[2019-04-01 18:52:28,363] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.833333333333333, 40.0, 0.0, 0.0, 26.0, 25.56055933971426, 9.702008363909387, 0.0, 1.0, 28.2981027587993], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5011800.0000, 
sim time next is 5012400.0000, 
raw observation next is [1.666666666666667, 40.0, 0.0, 0.0, 26.0, 25.52857238860124, 9.707926085207111, 0.0, 1.0, 29.30160346550921], 
processed observation next is [1.0, 0.0, 0.5087719298245615, 0.4, 0.0, 0.0, 1.0, 0.9326531983716058, 0.09707926085207111, 0.0, 1.0, 0.20929716761078007], 
reward next is 0.7907, 
noisyNet noise sample is [array([-0.86245763], dtype=float32), -1.0376711]. 
=============================================
[2019-04-01 18:52:28,798] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:28,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:28,808] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run25
[2019-04-01 18:52:28,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:28,870] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:28,889] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run25
[2019-04-01 18:52:29,167] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:29,167] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:29,170] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run25
[2019-04-01 18:52:30,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:30,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:30,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run25
[2019-04-01 18:52:30,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:30,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:30,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run25
[2019-04-01 18:52:31,248] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:31,248] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:31,252] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run25
[2019-04-01 18:52:31,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:31,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:31,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run25
[2019-04-01 18:52:31,733] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:31,733] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:31,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run25
[2019-04-01 18:52:32,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:32,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:32,103] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run25
[2019-04-01 18:52:32,951] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:32,951] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:32,954] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run25
[2019-04-01 18:52:33,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:52:33,018] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:52:33,022] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run25
[2019-04-01 18:52:41,142] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:52:41,142] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2890
[2019-04-01 18:52:41,155] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 26.0, 20.09084448904801, 19.68168796735439, 0.0, 1.0, 43.72001242351597], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5400.0000, 
sim time next is 6000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 26.0, 20.17761501908349, 19.08038811052853, 0.0, 1.0, 43.38839473399693], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 1.0, 0.1682307170119271, 0.19080388110528532, 0.0, 1.0, 0.3099171052428352], 
reward next is 0.6901, 
noisyNet noise sample is [array([0.7399034], dtype=float32), 0.040352795]. 
=============================================
[2019-04-01 18:52:41,158] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[66.636986]
 [63.35993 ]
 [59.570236]
 [54.638496]
 [48.962696]], R is [[69.94747925]
 [69.93572235]
 [69.92155457]
 [69.90445709]
 [69.88386536]].
[2019-04-01 18:52:50,937] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6308940e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8039895e-29
 1.0000000e+00 2.9722934e-35], sum to 1.0000
[2019-04-01 18:52:50,939] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6884
[2019-04-01 18:52:50,957] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.816666666666666, 73.5, 0.0, 0.0, 26.0, 23.73736703159102, 5.459451822270822, 0.0, 1.0, 43.80073027079493], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 175800.0000, 
sim time next is 176400.0000, 
raw observation next is [-8.9, 74.0, 0.0, 0.0, 26.0, 23.72428071062136, 5.454709039415334, 0.0, 1.0, 43.80678539265317], 
processed observation next is [1.0, 0.043478260869565216, 0.21606648199445982, 0.74, 0.0, 0.0, 1.0, 0.6748972443744802, 0.05454709039415334, 0.0, 1.0, 0.31290560994752264], 
reward next is 0.6871, 
noisyNet noise sample is [array([-0.5220961], dtype=float32), -0.37594312]. 
=============================================
[2019-04-01 18:53:01,854] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.7368681e-26 0.0000000e+00 1.2462060e-31 5.2797371e-36 0.0000000e+00
 1.0000000e+00 2.7799803e-26], sum to 1.0000
[2019-04-01 18:53:01,856] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5898
[2019-04-01 18:53:01,894] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 26.0, 24.10061122877415, 5.47820462335604, 0.0, 1.0, 44.50494729413943], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 263400.0000, 
sim time next is 264000.0000, 
raw observation next is [-6.9, 68.33333333333334, 0.0, 0.0, 26.0, 24.0578322357135, 5.449693569549964, 0.0, 1.0, 44.65515440513589], 
processed observation next is [1.0, 0.043478260869565216, 0.27146814404432135, 0.6833333333333335, 0.0, 0.0, 1.0, 0.7225474622447859, 0.05449693569549965, 0.0, 1.0, 0.31896538860811346], 
reward next is 0.6810, 
noisyNet noise sample is [array([1.16939], dtype=float32), -0.46183437]. 
=============================================
[2019-04-01 18:53:01,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[57.04483 ]
 [57.20565 ]
 [57.397667]
 [57.58992 ]
 [57.79988 ]], R is [[57.0090065 ]
 [57.12102127]
 [57.23313522]
 [57.34544754]
 [57.45764542]].
[2019-04-01 18:53:08,894] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:53:08,896] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7215
[2019-04-01 18:53:08,913] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.11522700981324, 8.083615828627808, 0.0, 1.0, 48.99204793283494], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 363000.0000, 
sim time next is 363600.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.21235129416974, 8.206124990156148, 0.0, 1.0, 47.69200509281462], 
processed observation next is [1.0, 0.21739130434782608, 0.030470914127423816, 0.73, 0.0, 0.0, 1.0, 0.45890732773853415, 0.08206124990156148, 0.0, 1.0, 0.34065717923439015], 
reward next is 0.6593, 
noisyNet noise sample is [array([0.41557685], dtype=float32), -0.52491724]. 
=============================================
[2019-04-01 18:53:10,886] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:53:10,886] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1689
[2019-04-01 18:53:10,944] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.916666666666668, 41.66666666666666, 0.0, 0.0, 26.0, 25.11347578778942, 8.339636131232822, 1.0, 1.0, 56.36495369792212], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 417000.0000, 
sim time next is 417600.0000, 
raw observation next is [-10.0, 42.0, 0.0, 0.0, 26.0, 25.1801466171002, 8.36357600085706, 0.0, 1.0, 53.97720279235784], 
processed observation next is [1.0, 0.8695652173913043, 0.18559556786703602, 0.42, 0.0, 0.0, 1.0, 0.8828780881571713, 0.0836357600085706, 0.0, 1.0, 0.38555144851684175], 
reward next is 0.6144, 
noisyNet noise sample is [array([-0.67391753], dtype=float32), 0.8280529]. 
=============================================
[2019-04-01 18:53:21,460] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.679209e-38
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:53:21,461] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9431
[2019-04-01 18:53:21,477] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 24.65309031161776, 5.919349922427152, 0.0, 1.0, 40.36581334343253], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 543600.0000, 
sim time next is 544200.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 24.58230849759465, 5.841446543617143, 0.0, 1.0, 40.44166901251563], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 1.0, 0.7974726425135213, 0.05841446543617143, 0.0, 1.0, 0.2888690643751117], 
reward next is 0.7111, 
noisyNet noise sample is [array([-0.46159744], dtype=float32), 0.51088303]. 
=============================================
[2019-04-01 18:53:25,179] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:53:25,179] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4428
[2019-04-01 18:53:25,193] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.39277933400794, 5.679850123505576, 0.0, 1.0, 41.54843505039366], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 610200.0000, 
sim time next is 610800.0000, 
raw observation next is [-3.899999999999999, 86.0, 0.0, 0.0, 26.0, 24.36694364173738, 5.636995727225925, 0.0, 1.0, 41.58630042901219], 
processed observation next is [0.0, 0.043478260869565216, 0.35457063711911363, 0.86, 0.0, 0.0, 1.0, 0.7667062345339113, 0.05636995727225925, 0.0, 1.0, 0.2970450030643728], 
reward next is 0.7030, 
noisyNet noise sample is [array([-0.5763736], dtype=float32), 0.26121354]. 
=============================================
[2019-04-01 18:53:30,005] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 8.128499e-35], sum to 1.0000
[2019-04-01 18:53:30,005] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5822
[2019-04-01 18:53:30,077] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 71.0, 77.0, 25.5, 26.0, 25.26181328391319, 6.853027236790029, 0.0, 1.0, 46.45922896662152], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 637200.0000, 
sim time next is 637800.0000, 
raw observation next is [-3.9, 70.0, 96.33333333333334, 34.00000000000001, 26.0, 25.24132919398767, 6.7957108753624, 0.0, 1.0, 44.16954035548119], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.7, 0.3211111111111111, 0.03756906077348067, 1.0, 0.891618456283953, 0.067957108753624, 0.0, 1.0, 0.31549671682486563], 
reward next is 0.6845, 
noisyNet noise sample is [array([-0.08187388], dtype=float32), 0.7176237]. 
=============================================
[2019-04-01 18:53:31,051] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:53:31,051] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6198
[2019-04-01 18:53:31,069] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.00557107968426, 9.078729213772776, 1.0, 1.0, 3.420816043915154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1023000.0000, 
sim time next is 1023600.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 24.94890929747654, 9.38989183935105, 0.0, 1.0, 65.41772718843063], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 1.0, 0.8498441853537914, 0.0938989183935105, 0.0, 1.0, 0.4672694799173617], 
reward next is 0.5327, 
noisyNet noise sample is [array([-0.18012209], dtype=float32), 0.63525546]. 
=============================================
[2019-04-01 18:53:33,939] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:53:33,939] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0655
[2019-04-01 18:53:33,982] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 55.0, 0.0, 0.0, 26.0, 25.52173452158665, 8.470699061476607, 1.0, 1.0, 29.0605945480767], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 757200.0000, 
sim time next is 757800.0000, 
raw observation next is [-3.9, 54.5, 0.0, 0.0, 26.0, 25.28147710465402, 7.979689345753471, 1.0, 1.0, 25.61746850246612], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.545, 0.0, 0.0, 1.0, 0.8973538720934313, 0.07979689345753471, 1.0, 1.0, 0.182981917874758], 
reward next is 0.8170, 
noisyNet noise sample is [array([-2.8044121], dtype=float32), 1.1069722]. 
=============================================
[2019-04-01 18:53:41,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:53:41,554] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3911
[2019-04-01 18:53:41,570] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.55, 79.5, 0.0, 0.0, 26.0, 24.62309916792946, 6.332664073976606, 0.0, 1.0, 40.16220769278964], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 862200.0000, 
sim time next is 862800.0000, 
raw observation next is [-2.466666666666667, 79.66666666666666, 0.0, 0.0, 26.0, 24.59698629025763, 6.281727403255762, 0.0, 1.0, 40.0155797899592], 
processed observation next is [1.0, 1.0, 0.39427516158818104, 0.7966666666666665, 0.0, 0.0, 1.0, 0.7995694700368041, 0.06281727403255762, 0.0, 1.0, 0.28582556992827995], 
reward next is 0.7142, 
noisyNet noise sample is [array([-0.16571249], dtype=float32), -1.5890087]. 
=============================================
[2019-04-01 18:53:42,199] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:53:42,205] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9875
[2019-04-01 18:53:42,238] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.19641928313206, 9.795267222877532, 0.0, 1.0, 39.33324289349618], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1390200.0000, 
sim time next is 1390800.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.22072639513022, 9.754070610888848, 0.0, 1.0, 39.24745849624395], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.8886751993043173, 0.09754070610888847, 0.0, 1.0, 0.28033898925888534], 
reward next is 0.7197, 
noisyNet noise sample is [array([-0.16376674], dtype=float32), -1.2621044]. 
=============================================
[2019-04-01 18:53:52,465] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:53:52,468] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7773
[2019-04-01 18:53:52,479] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.15, 81.0, 0.0, 0.0, 26.0, 25.5581510208485, 9.407453189304725, 0.0, 1.0, 24.49767362097455], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 959400.0000, 
sim time next is 960000.0000, 
raw observation next is [7.333333333333333, 80.66666666666666, 0.0, 0.0, 26.0, 25.5047072134908, 9.183981878979386, 0.0, 1.0, 27.0817555649879], 
processed observation next is [1.0, 0.08695652173913043, 0.6657433056325024, 0.8066666666666665, 0.0, 0.0, 1.0, 0.9292438876415429, 0.09183981878979386, 0.0, 1.0, 0.193441111178485], 
reward next is 0.8066, 
noisyNet noise sample is [array([0.96259224], dtype=float32), -1.0489097]. 
=============================================
[2019-04-01 18:53:52,492] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[68.12922 ]
 [68.22301 ]
 [68.34717 ]
 [68.561005]
 [68.73472 ]], R is [[68.22345734]
 [68.36624146]
 [68.4987793 ]
 [68.62350464]
 [68.725914  ]].
[2019-04-01 18:53:54,020] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1035266e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 9.5029829e-38], sum to 1.0000
[2019-04-01 18:53:54,021] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9254
[2019-04-01 18:53:54,038] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 79.0, 0.0, 0.0, 26.0, 25.5070190867829, 10.50449753568566, 0.0, 1.0, 23.45218012530896], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1562400.0000, 
sim time next is 1563000.0000, 
raw observation next is [4.9, 80.16666666666667, 0.0, 0.0, 26.0, 25.44931662118302, 10.613206672123, 0.0, 1.0, 30.86878961245793], 
processed observation next is [1.0, 0.08695652173913043, 0.5983379501385043, 0.8016666666666667, 0.0, 0.0, 1.0, 0.9213309458832885, 0.10613206672123, 0.0, 1.0, 0.2204913543746995], 
reward next is 0.7795, 
noisyNet noise sample is [array([0.18493357], dtype=float32), 2.5756829]. 
=============================================
[2019-04-01 18:53:54,058] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[73.363846]
 [73.80845 ]
 [74.33303 ]
 [74.82654 ]
 [75.41466 ]], R is [[73.04489899]
 [73.14693451]
 [73.27185822]
 [73.37714386]
 [73.48603058]].
[2019-04-01 18:54:05,190] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-01 18:54:05,192] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 18:54:05,193] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 18:54:05,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:54:05,194] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 18:54:05,194] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:54:05,195] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:54:05,201] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run34
[2019-04-01 18:54:05,225] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run34
[2019-04-01 18:54:05,249] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run34
[2019-04-01 18:55:41,938] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.3956267], dtype=float32), -0.8737495]
[2019-04-01 18:55:41,938] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.0, 66.0, 0.0, 0.0, 26.0, 25.25211369694625, 7.392807383705829, 0.0, 1.0, 39.35731648937368]
[2019-04-01 18:55:41,938] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 18:55:41,939] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.27413712820239167
[2019-04-01 18:55:47,826] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 18:56:07,311] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 18:56:10,539] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 18:56:11,563] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 3300000, evaluation results [3300000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 18:56:13,768] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9077366e-27 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:56:13,768] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7486
[2019-04-01 18:56:13,777] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.933333333333334, 92.0, 25.16666666666667, 0.0, 26.0, 25.41514766080889, 10.44053812220487, 1.0, 1.0, 18.68523997970831], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1671600.0000, 
sim time next is 1672200.0000, 
raw observation next is [2.75, 92.0, 30.0, 0.0, 26.0, 25.51584979178796, 10.34419186562286, 1.0, 1.0, 16.69399409409108], 
processed observation next is [1.0, 0.34782608695652173, 0.5387811634349031, 0.92, 0.1, 0.0, 1.0, 0.9308356845411373, 0.1034419186562286, 1.0, 1.0, 0.11924281495779344], 
reward next is 0.7431, 
noisyNet noise sample is [array([-1.1849607], dtype=float32), -0.16465141]. 
=============================================
[2019-04-01 18:56:15,019] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1008112e-28 1.4402662e-34 0.0000000e+00 5.3029566e-26 2.2325231e-35
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:56:15,019] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6215
[2019-04-01 18:56:15,030] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 93.0, 31.0, 0.0, 26.0, 25.71674881733396, 10.76446265935943, 1.0, 1.0, 10.92890599113433], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1353600.0000, 
sim time next is 1354200.0000, 
raw observation next is [1.0, 93.5, 26.66666666666666, 0.0, 26.0, 25.71306252340447, 10.67199893635054, 1.0, 1.0, 11.01565263001835], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.935, 0.08888888888888886, 0.0, 1.0, 0.9590089319149244, 0.10671998936350541, 1.0, 1.0, 0.07868323307155964], 
reward next is 0.6525, 
noisyNet noise sample is [array([-1.3116732], dtype=float32), -0.32707396]. 
=============================================
[2019-04-01 18:56:22,737] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:56:22,741] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9452
[2019-04-01 18:56:22,748] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.7, 51.66666666666667, 76.33333333333333, 539.6666666666666, 26.0, 25.94895066543424, 14.80889837727048, 1.0, 1.0, 1.430784380196429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1523400.0000, 
sim time next is 1524000.0000, 
raw observation next is [11.8, 51.33333333333334, 76.66666666666667, 508.8333333333334, 26.0, 26.38547068186845, 15.686707849099, 1.0, 1.0, 1.378138205737925], 
processed observation next is [1.0, 0.6521739130434783, 0.7894736842105264, 0.5133333333333334, 0.2555555555555556, 0.5622467771639044, 1.0, 1.0550672402669217, 0.15686707849099, 1.0, 1.0, 0.009843844326699464], 
reward next is 0.0000, 
noisyNet noise sample is [array([3.547544], dtype=float32), -0.09465205]. 
=============================================
[2019-04-01 18:56:22,767] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[48.706425]
 [48.30322 ]
 [47.73797 ]
 [47.1506  ]
 [46.69501 ]], R is [[48.82513046]
 [48.33687973]
 [47.85351181]
 [47.37497711]
 [46.90122604]].
[2019-04-01 18:56:23,269] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:56:23,270] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7022
[2019-04-01 18:56:23,280] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.86378452101836, 12.86064493733134, 0.0, 1.0, 25.12054476493712], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1657800.0000, 
sim time next is 1658400.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.87694028930348, 12.47533041607293, 0.0, 1.0, 23.83764761406021], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 1.0, 0.9824200413290686, 0.1247533041607293, 0.0, 1.0, 0.1702689115290015], 
reward next is 0.8297, 
noisyNet noise sample is [array([1.2628685], dtype=float32), -0.07703831]. 
=============================================
[2019-04-01 18:56:28,505] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:56:28,509] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2924
[2019-04-01 18:56:28,531] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 92.0, 41.5, 0.0, 26.0, 25.55909758265036, 10.41212813481487, 1.0, 1.0, 14.78877925864038], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1674000.0000, 
sim time next is 1674600.0000, 
raw observation next is [2.083333333333333, 92.0, 45.33333333333334, 0.0, 26.0, 25.59707394223725, 10.47900796421818, 1.0, 1.0, 14.3403738951059], 
processed observation next is [1.0, 0.391304347826087, 0.5203139427516159, 0.92, 0.15111111111111114, 0.0, 1.0, 0.9424391346053217, 0.1047900796421818, 1.0, 1.0, 0.10243124210789929], 
reward next is 0.7060, 
noisyNet noise sample is [array([1.1400211], dtype=float32), -0.16324636]. 
=============================================
[2019-04-01 18:56:29,558] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:56:29,560] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5896
[2019-04-01 18:56:29,570] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 85.33333333333334, 103.0, 0.0, 26.0, 25.69596312637449, 10.1116181902714, 1.0, 1.0, 15.01193156448413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1686000.0000, 
sim time next is 1686600.0000, 
raw observation next is [1.1, 86.0, 107.0, 0.0, 26.0, 25.61497153338816, 9.861544327346998, 1.0, 1.0, 14.98345499817234], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.86, 0.3566666666666667, 0.0, 1.0, 0.9449959333411658, 0.09861544327346998, 1.0, 1.0, 0.10702467855837386], 
reward next is 0.8930, 
noisyNet noise sample is [array([-0.13374776], dtype=float32), -0.73699516]. 
=============================================
[2019-04-01 18:56:38,923] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.3373586e-34
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:56:38,923] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3780
[2019-04-01 18:56:38,942] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.56150756338787, 5.256191723848626, 0.0, 1.0, 46.40047620812857], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1836000.0000, 
sim time next is 1836600.0000, 
raw observation next is [-6.283333333333333, 78.83333333333334, 0.0, 0.0, 26.0, 23.52251507687862, 5.267463236076137, 0.0, 1.0, 46.3909618090895], 
processed observation next is [0.0, 0.2608695652173913, 0.288550323176362, 0.7883333333333334, 0.0, 0.0, 1.0, 0.6460735824112314, 0.052674632360761364, 0.0, 1.0, 0.33136401292206785], 
reward next is 0.6686, 
noisyNet noise sample is [array([1.9549177], dtype=float32), -0.52622426]. 
=============================================
[2019-04-01 18:56:42,727] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.0787611e-25
 1.0000000e+00 6.5880655e-11], sum to 1.0000
[2019-04-01 18:56:42,731] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2747
[2019-04-01 18:56:42,791] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 76.33333333333333, 43.33333333333333, 0.0, 26.0, 24.91901422167275, 7.275095836980086, 0.0, 1.0, 45.6662525820541], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1872600.0000, 
sim time next is 1873200.0000, 
raw observation next is [-4.5, 77.66666666666667, 36.16666666666666, 0.0, 26.0, 24.91395241265338, 7.477768695381741, 0.0, 1.0, 55.20404184689747], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.7766666666666667, 0.12055555555555553, 0.0, 1.0, 0.8448503446647686, 0.07477768695381741, 0.0, 1.0, 0.3943145846206962], 
reward next is 0.6057, 
noisyNet noise sample is [array([-0.876361], dtype=float32), 0.10707175]. 
=============================================
[2019-04-01 18:56:43,381] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:56:43,381] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7431
[2019-04-01 18:56:43,438] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.916666666666667, 85.5, 0.0, 0.0, 26.0, 24.96783622038697, 7.459867773286663, 0.0, 1.0, 50.56128706289658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1878600.0000, 
sim time next is 1879200.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 26.0, 25.0296740189802, 7.569462628273165, 0.0, 1.0, 47.15826105434003], 
processed observation next is [0.0, 0.782608695652174, 0.32409972299168976, 0.86, 0.0, 0.0, 1.0, 0.8613820027114569, 0.07569462628273164, 0.0, 1.0, 0.3368447218167145], 
reward next is 0.6632, 
noisyNet noise sample is [array([-0.3771975], dtype=float32), 0.69277126]. 
=============================================
[2019-04-01 18:56:44,255] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:56:44,261] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5646
[2019-04-01 18:56:44,294] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 85.0, 0.0, 0.0, 26.0, 24.88533685322405, 6.676257617905859, 0.0, 1.0, 43.73457367230316], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1887600.0000, 
sim time next is 1888200.0000, 
raw observation next is [-5.6, 84.5, 0.0, 0.0, 26.0, 24.89109215657301, 6.622962336336781, 0.0, 1.0, 44.06923467998171], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.845, 0.0, 0.0, 1.0, 0.8415845937961441, 0.06622962336336781, 0.0, 1.0, 0.3147802477141551], 
reward next is 0.6852, 
noisyNet noise sample is [array([-0.18647464], dtype=float32), -0.16604637]. 
=============================================
[2019-04-01 18:56:46,620] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.7613682e-26
 1.0000000e+00 7.7528800e-24], sum to 1.0000
[2019-04-01 18:56:46,621] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0454
[2019-04-01 18:56:46,637] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.69399428940756, 6.521393024426689, 0.0, 1.0, 42.40472084356855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1983600.0000, 
sim time next is 1984200.0000, 
raw observation next is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.64510129916989, 6.428083602037463, 0.0, 1.0, 42.38304168130252], 
processed observation next is [1.0, 1.0, 0.3074792243767313, 0.83, 0.0, 0.0, 1.0, 0.8064430427385558, 0.06428083602037464, 0.0, 1.0, 0.3027360120093037], 
reward next is 0.6973, 
noisyNet noise sample is [array([0.14256585], dtype=float32), 0.25431007]. 
=============================================
[2019-04-01 18:56:46,803] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8705222e-36
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:56:46,808] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7631
[2019-04-01 18:56:46,854] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.933333333333334, 77.66666666666667, 0.0, 0.0, 26.0, 24.43713917519659, 5.769932871236932, 0.0, 1.0, 44.33077110442426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1896000.0000, 
sim time next is 1896600.0000, 
raw observation next is [-7.116666666666666, 78.33333333333334, 0.0, 0.0, 26.0, 24.39657238447504, 5.720704680632927, 0.0, 1.0, 44.33240860912498], 
processed observation next is [0.0, 0.9565217391304348, 0.265466297322253, 0.7833333333333334, 0.0, 0.0, 1.0, 0.7709389120678628, 0.05720704680632927, 0.0, 1.0, 0.31666006149374987], 
reward next is 0.6833, 
noisyNet noise sample is [array([-0.13793787], dtype=float32), 0.49530202]. 
=============================================
[2019-04-01 18:56:55,831] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.2524449e-36], sum to 1.0000
[2019-04-01 18:56:55,832] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7658
[2019-04-01 18:56:55,909] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.1, 78.66666666666667, 0.0, 0.0, 26.0, 25.41094943390623, 7.733197994705328, 1.0, 1.0, 48.23722887780565], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2101200.0000, 
sim time next is 2101800.0000, 
raw observation next is [-7.199999999999999, 78.83333333333334, 24.66666666666666, 12.33333333333333, 26.0, 25.49280872787567, 7.814851210104199, 1.0, 1.0, 45.25593186675039], 
processed observation next is [1.0, 0.30434782608695654, 0.26315789473684215, 0.7883333333333334, 0.0822222222222222, 0.013627992633517492, 1.0, 0.9275441039822384, 0.07814851210104198, 1.0, 1.0, 0.3232566561910742], 
reward next is 0.6767, 
noisyNet noise sample is [array([0.28220466], dtype=float32), -0.6984047]. 
=============================================
[2019-04-01 18:56:58,990] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:56:58,990] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1822
[2019-04-01 18:56:59,051] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 86.0, 0.0, 0.0, 26.0, 25.5861671824291, 9.527159593901068, 0.0, 1.0, 30.89377432486877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2061600.0000, 
sim time next is 2062200.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.43181248856331, 9.348437728963924, 0.0, 1.0, 41.77798952974727], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 1.0, 0.9188303555090442, 0.09348437728963925, 0.0, 1.0, 0.29841421092676623], 
reward next is 0.7016, 
noisyNet noise sample is [array([1.0468332], dtype=float32), 0.10130613]. 
=============================================
[2019-04-01 18:57:03,783] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:57:03,784] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4141
[2019-04-01 18:57:03,833] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.05358689659843, 8.874709509587944, 0.0, 1.0, 43.45183152427639], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2147400.0000, 
sim time next is 2148000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.12687345207167, 8.93262817796676, 0.0, 1.0, 42.92829808433977], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 1.0, 0.8752676360102386, 0.0893262817796676, 0.0, 1.0, 0.3066307006024269], 
reward next is 0.6934, 
noisyNet noise sample is [array([-0.37165838], dtype=float32), 0.19549145]. 
=============================================
[2019-04-01 18:57:03,840] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[58.649242]
 [58.654236]
 [58.92134 ]
 [59.28084 ]
 [59.144417]], R is [[58.34094238]
 [58.44716263]
 [58.5383873 ]
 [58.61740112]
 [58.59857941]].
[2019-04-01 18:57:14,097] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:57:14,098] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2644
[2019-04-01 18:57:14,127] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.199999999999999, 80.83333333333334, 0.0, 0.0, 26.0, 24.20086335245191, 5.677547441626906, 0.0, 1.0, 43.403148596077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2253000.0000, 
sim time next is 2253600.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 26.0, 24.18966831299251, 5.615181233193105, 0.0, 1.0, 43.39655333523998], 
processed observation next is [1.0, 0.08695652173913043, 0.26038781163434904, 0.82, 0.0, 0.0, 1.0, 0.7413811875703585, 0.05615181233193105, 0.0, 1.0, 0.30997538096599986], 
reward next is 0.6900, 
noisyNet noise sample is [array([0.7003147], dtype=float32), 1.4011517]. 
=============================================
[2019-04-01 18:57:21,207] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:57:21,208] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4438
[2019-04-01 18:57:21,275] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 54.0, 40.0, 416.0, 26.0, 24.37009583494086, 5.989618338601178, 0.0, 1.0, 82.04738195175979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2449800.0000, 
sim time next is 2450400.0000, 
raw observation next is [-8.033333333333333, 52.66666666666666, 43.5, 457.5, 26.0, 24.77099532817459, 6.337977439338403, 0.0, 1.0, 67.36107879120307], 
processed observation next is [0.0, 0.34782608695652173, 0.24007386888273316, 0.5266666666666666, 0.145, 0.505524861878453, 1.0, 0.8244279040249413, 0.06337977439338403, 0.0, 1.0, 0.4811505627943076], 
reward next is 0.5188, 
noisyNet noise sample is [array([0.24545833], dtype=float32), 0.21380965]. 
=============================================
[2019-04-01 18:57:22,096] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:57:22,097] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5522
[2019-04-01 18:57:22,150] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 25.82537457191723, 11.31160440750474, 1.0, 1.0, 53.16889577336155], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2833200.0000, 
sim time next is 2833800.0000, 
raw observation next is [2.833333333333333, 38.16666666666667, 0.0, 0.0, 26.0, 26.00776859647497, 11.60829192582743, 1.0, 1.0, 47.73623374214562], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.3816666666666667, 0.0, 0.0, 1.0, 1.0011097994964244, 0.1160829192582743, 1.0, 1.0, 0.340973098158183], 
reward next is 0.0157, 
noisyNet noise sample is [array([-1.0508567], dtype=float32), -0.6619577]. 
=============================================
[2019-04-01 18:57:25,365] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:57:25,366] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4571
[2019-04-01 18:57:25,384] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.566666666666667, 27.0, 79.5, 792.0, 26.0, 24.96915921478701, 7.266133981829538, 0.0, 1.0, 16.97213762876852], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2470800.0000, 
sim time next is 2471400.0000, 
raw observation next is [2.75, 27.0, 78.0, 784.0, 26.0, 24.9494259571843, 7.265319387828856, 0.0, 1.0, 18.6131002016158], 
processed observation next is [0.0, 0.6086956521739131, 0.5387811634349031, 0.27, 0.26, 0.8662983425414365, 1.0, 0.8499179938834713, 0.07265319387828856, 0.0, 1.0, 0.13295071572582715], 
reward next is 0.8670, 
noisyNet noise sample is [array([1.1496147], dtype=float32), -0.27932]. 
=============================================
[2019-04-01 18:57:25,735] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:57:25,737] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8107
[2019-04-01 18:57:25,759] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.9666666666666667, 36.0, 0.0, 0.0, 26.0, 25.08174885419452, 6.359060273220225, 0.0, 1.0, 39.34444405911885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2503200.0000, 
sim time next is 2503800.0000, 
raw observation next is [-1.15, 36.5, 0.0, 0.0, 26.0, 25.06608676009751, 6.312451123324685, 0.0, 1.0, 39.25356093858303], 
processed observation next is [0.0, 1.0, 0.4307479224376732, 0.365, 0.0, 0.0, 1.0, 0.8665838228710727, 0.06312451123324685, 0.0, 1.0, 0.2803825781327359], 
reward next is 0.7196, 
noisyNet noise sample is [array([-0.00945019], dtype=float32), -0.7607009]. 
=============================================
[2019-04-01 18:57:27,004] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:57:27,006] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2590
[2019-04-01 18:57:27,031] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.466666666666667, 29.33333333333333, 186.6666666666667, 328.6666666666667, 26.0, 25.47448815991542, 7.319761211759335, 1.0, 1.0, 7.036797831203373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2553000.0000, 
sim time next is 2553600.0000, 
raw observation next is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 26.0, 25.34426338087923, 7.547142032114643, 1.0, 1.0, 6.638093488567154], 
processed observation next is [1.0, 0.5652173913043478, 0.538319482917821, 0.28666666666666674, 0.5961111111111109, 0.44788213627992646, 1.0, 0.9063233401256043, 0.07547142032114644, 1.0, 1.0, 0.04741495348976539], 
reward next is 0.9526, 
noisyNet noise sample is [array([0.55461675], dtype=float32), 1.0972742]. 
=============================================
[2019-04-01 18:57:29,881] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6255305e-26
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:57:29,881] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9940
[2019-04-01 18:57:29,897] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 44.83333333333334, 0.0, 0.0, 26.0, 24.90173869950313, 5.869166925382373, 0.0, 1.0, 38.1205157719671], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2517000.0000, 
sim time next is 2517600.0000, 
raw observation next is [-1.7, 45.66666666666667, 0.0, 0.0, 26.0, 24.95659762707474, 5.894612737104342, 0.0, 1.0, 38.04628029300015], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4566666666666667, 0.0, 0.0, 1.0, 0.8509425181535342, 0.058946127371043426, 0.0, 1.0, 0.2717591449500011], 
reward next is 0.7282, 
noisyNet noise sample is [array([0.01174442], dtype=float32), 0.67599976]. 
=============================================
[2019-04-01 18:57:33,556] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:57:33,556] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0246
[2019-04-01 18:57:33,594] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.04999999999999999, 35.5, 0.0, 0.0, 26.0, 25.2522888253471, 7.778382013275508, 1.0, 1.0, 17.45352814638933], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2572200.0000, 
sim time next is 2572800.0000, 
raw observation next is [-0.2333333333333333, 35.66666666666667, 0.0, 0.0, 26.0, 25.11988680098164, 7.533898946107001, 1.0, 1.0, 16.74375454744285], 
processed observation next is [1.0, 0.782608695652174, 0.456140350877193, 0.3566666666666667, 0.0, 0.0, 1.0, 0.874269542997377, 0.07533898946107001, 1.0, 1.0, 0.11959824676744894], 
reward next is 0.8804, 
noisyNet noise sample is [array([0.06069476], dtype=float32), -1.2646142]. 
=============================================
[2019-04-01 18:57:33,915] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:57:33,915] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6223
[2019-04-01 18:57:33,933] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.166666666666667, 57.0, 0.0, 0.0, 26.0, 25.40539910079541, 7.962420593817619, 0.0, 1.0, 40.55899330558461], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2586000.0000, 
sim time next is 2586600.0000, 
raw observation next is [-3.35, 57.5, 0.0, 0.0, 26.0, 25.31485299437156, 7.708681598567082, 0.0, 1.0, 39.09700073380663], 
processed observation next is [1.0, 0.9565217391304348, 0.3698060941828255, 0.575, 0.0, 0.0, 1.0, 0.9021218563387942, 0.07708681598567081, 0.0, 1.0, 0.27926429095576166], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.1395487], dtype=float32), 0.8345544]. 
=============================================
[2019-04-01 18:57:36,234] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:57:36,235] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6858
[2019-04-01 18:57:36,272] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 53.33333333333334, 113.5, 815.0, 26.0, 25.12914941826132, 8.325966925930919, 0.0, 1.0, 19.08248737157388], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3068400.0000, 
sim time next is 3069000.0000, 
raw observation next is [-2.5, 52.5, 114.0, 817.0, 26.0, 25.13455475977909, 8.327652296209598, 0.0, 1.0, 18.1872934393875], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.525, 0.38, 0.9027624309392265, 1.0, 0.876364965682727, 0.08327652296209598, 0.0, 1.0, 0.12990923885276784], 
reward next is 0.8701, 
noisyNet noise sample is [array([-0.06654453], dtype=float32), 0.046515055]. 
=============================================
[2019-04-01 18:57:36,289] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[61.842052]
 [62.07734 ]
 [62.307842]
 [62.54332 ]
 [62.80954 ]], R is [[61.90262222]
 [62.14729309]
 [62.3623085 ]
 [62.54819107]
 [62.72516251]].
[2019-04-01 18:57:38,564] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:57:38,566] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1949
[2019-04-01 18:57:38,600] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 85.0, 0.0, 0.0, 26.0, 25.34578973531243, 9.571556643223303, 1.0, 1.0, 25.15045088298753], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2921400.0000, 
sim time next is 2922000.0000, 
raw observation next is [-1.0, 82.66666666666667, 0.0, 0.0, 26.0, 25.29381795169458, 9.219883338440518, 1.0, 1.0, 23.91679703263501], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.8266666666666667, 0.0, 0.0, 1.0, 0.8991168502420831, 0.09219883338440518, 1.0, 1.0, 0.1708342645188215], 
reward next is 0.8292, 
noisyNet noise sample is [array([0.33706945], dtype=float32), 0.9092064]. 
=============================================
[2019-04-01 18:57:38,608] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[67.41787 ]
 [66.97789 ]
 [68.35547 ]
 [68.17908 ]
 [67.947624]], R is [[68.32585144]
 [68.46294403]
 [68.5925293 ]
 [68.68166351]
 [68.66984558]].
[2019-04-01 18:57:40,296] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:57:40,296] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4334
[2019-04-01 18:57:40,308] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.07981886112486, 7.7423611455322, 0.0, 1.0, 45.15475255245543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2760600.0000, 
sim time next is 2761200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.0296806842132, 7.587365127569534, 0.0, 1.0, 44.83430382021522], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 1.0, 0.8613829548876001, 0.07587365127569534, 0.0, 1.0, 0.32024502728725157], 
reward next is 0.6798, 
noisyNet noise sample is [array([1.3768743], dtype=float32), 0.42567435]. 
=============================================
[2019-04-01 18:57:44,850] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.6130277e-37
 1.0000000e+00 2.0189621e-37], sum to 1.0000
[2019-04-01 18:57:44,851] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3292
[2019-04-01 18:57:44,866] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 26.0, 25.36359805556702, 8.575104269392098, 0.0, 1.0, 43.22972927459968], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2847600.0000, 
sim time next is 2848200.0000, 
raw observation next is [1.833333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 25.3732864441235, 8.55517383694955, 0.0, 1.0, 42.73237473586242], 
processed observation next is [1.0, 1.0, 0.5133887349953832, 0.6366666666666667, 0.0, 0.0, 1.0, 0.9104694920176429, 0.08555173836949549, 0.0, 1.0, 0.305231248113303], 
reward next is 0.6948, 
noisyNet noise sample is [array([1.0278201], dtype=float32), 2.981798]. 
=============================================
[2019-04-01 18:57:47,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5713897e-34 0.0000000e+00 0.0000000e+00 1.8287558e-26 1.0000000e+00
 1.2257950e-09 0.0000000e+00], sum to 1.0000
[2019-04-01 18:57:47,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7923
[2019-04-01 18:57:47,777] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.32007592764149, 5.382029438498665, 0.0, 1.0, 41.36819459243952], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2787000.0000, 
sim time next is 2787600.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.20954099584677, 5.367760234035399, 0.0, 1.0, 52.98547792681482], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 1.0, 0.7442201422638242, 0.053677602340353986, 0.0, 1.0, 0.37846769947724873], 
reward next is 0.6215, 
noisyNet noise sample is [array([-1.4518878], dtype=float32), 0.4943209]. 
=============================================
[2019-04-01 18:57:49,377] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5989219e-31 0.0000000e+00 0.0000000e+00 5.9214719e-27 4.7029683e-32
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:57:49,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9368
[2019-04-01 18:57:49,406] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 93.0, 0.0, 0.0, 26.0, 25.09083676306273, 8.484919657424426, 1.0, 1.0, 25.29470608384106], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2914800.0000, 
sim time next is 2915400.0000, 
raw observation next is [1.166666666666667, 93.0, 0.0, 0.0, 26.0, 24.97978372463363, 8.722451745220779, 1.0, 1.0, 36.14874508129565], 
processed observation next is [1.0, 0.7391304347826086, 0.49492151431209613, 0.93, 0.0, 0.0, 1.0, 0.8542548178048044, 0.08722451745220779, 1.0, 1.0, 0.25820532200925467], 
reward next is 0.7418, 
noisyNet noise sample is [array([0.11781982], dtype=float32), -0.06420273]. 
=============================================
[2019-04-01 18:57:53,184] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 9.999995e-01 4.887445e-07], sum to 1.0000
[2019-04-01 18:57:53,184] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8768
[2019-04-01 18:57:53,212] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 62.5, 110.0, 800.0, 26.0, 25.17684721055488, 9.186828474047184, 0.0, 1.0, 19.82753024718843], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2986200.0000, 
sim time next is 2986800.0000, 
raw observation next is [-2.333333333333333, 61.66666666666667, 108.5, 791.8333333333334, 26.0, 25.14907632296296, 9.076760160226366, 0.0, 1.0, 21.37113260722006], 
processed observation next is [0.0, 0.5652173913043478, 0.3979686057248385, 0.6166666666666667, 0.3616666666666667, 0.8749539594843463, 1.0, 0.8784394747089941, 0.09076760160226366, 0.0, 1.0, 0.15265094719442898], 
reward next is 0.8473, 
noisyNet noise sample is [array([-0.5226461], dtype=float32), -0.7754914]. 
=============================================
[2019-04-01 18:57:59,907] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.8959526e-16
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:57:59,908] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0265
[2019-04-01 18:57:59,934] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 74.66666666666667, 0.0, 0.0, 26.0, 23.86729887429903, 5.243178652794683, 0.0, 1.0, 39.65625873628467], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3043200.0000, 
sim time next is 3043800.0000, 
raw observation next is [-6.0, 73.5, 0.0, 0.0, 26.0, 23.87011546855055, 5.235283613727707, 0.0, 1.0, 39.63983770767035], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.735, 0.0, 0.0, 1.0, 0.6957307812215073, 0.052352836137277065, 0.0, 1.0, 0.2831416979119311], 
reward next is 0.7169, 
noisyNet noise sample is [array([1.6659956], dtype=float32), 0.4902073]. 
=============================================
[2019-04-01 18:58:03,499] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.6144135e-26
 1.0000000e+00 8.2439493e-27], sum to 1.0000
[2019-04-01 18:58:03,500] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8790
[2019-04-01 18:58:03,534] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 71.16666666666667, 0.0, 0.0, 26.0, 23.76805403009658, 5.225676112225322, 0.0, 1.0, 39.73626149707629], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3046200.0000, 
sim time next is 3046800.0000, 
raw observation next is [-6.0, 72.33333333333334, 0.0, 0.0, 26.0, 23.74119387367089, 5.228252961178967, 0.0, 1.0, 39.78410592903761], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7233333333333334, 0.0, 0.0, 1.0, 0.6773134105244131, 0.05228252961178967, 0.0, 1.0, 0.2841721852074115], 
reward next is 0.7158, 
noisyNet noise sample is [array([-0.11896751], dtype=float32), -1.9849277]. 
=============================================
[2019-04-01 18:58:07,913] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.3487764e-23 2.3013633e-34
 1.0000000e+00 9.4687921e-37], sum to 1.0000
[2019-04-01 18:58:07,914] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7437
[2019-04-01 18:58:07,927] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 96.5, 0.0, 0.0, 26.0, 25.53466468816313, 12.34067273679713, 0.0, 1.0, 25.16704896058749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3198600.0000, 
sim time next is 3199200.0000, 
raw observation next is [1.333333333333333, 97.66666666666666, 0.0, 0.0, 26.0, 25.54414880924443, 12.22445850897092, 0.0, 1.0, 29.33316321682426], 
processed observation next is [1.0, 0.0, 0.4995383194829178, 0.9766666666666666, 0.0, 0.0, 1.0, 0.9348784013206328, 0.1222445850897092, 0.0, 1.0, 0.20952259440588758], 
reward next is 0.7905, 
noisyNet noise sample is [array([-0.24528827], dtype=float32), -0.8667875]. 
=============================================
[2019-04-01 18:58:15,756] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:58:15,760] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7688
[2019-04-01 18:58:15,768] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 111.3333333333333, 800.8333333333334, 26.0, 26.4248346110533, 14.13613321275664, 1.0, 1.0, 16.3780840150763], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3418800.0000, 
sim time next is 3419400.0000, 
raw observation next is [3.0, 49.0, 109.6666666666667, 795.6666666666666, 26.0, 26.5587525855243, 14.70446078732933, 1.0, 1.0, 16.25975322601634], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.3655555555555557, 0.8791896869244935, 1.0, 1.0798217979320428, 0.1470446078732933, 1.0, 1.0, 0.11614109447154529], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4746997], dtype=float32), 1.0003657]. 
=============================================
[2019-04-01 18:58:17,053] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6820057e-37 0.0000000e+00 0.0000000e+00 2.4374783e-31 4.6069769e-24
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:58:17,053] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4265
[2019-04-01 18:58:17,067] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.54342344379875, 9.699781988558433, 0.0, 1.0, 35.79098471639327], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3450000.0000, 
sim time next is 3450600.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.36453421996332, 9.537863699432243, 0.0, 1.0, 35.70106700763552], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 1.0, 0.9092191742804745, 0.09537863699432243, 0.0, 1.0, 0.2550076214831109], 
reward next is 0.7450, 
noisyNet noise sample is [array([-0.10543808], dtype=float32), 1.527651]. 
=============================================
[2019-04-01 18:58:22,174] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:58:22,174] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7197
[2019-04-01 18:58:22,190] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.16095536259159, 8.214268499320442, 0.0, 1.0, 41.48235529662462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3474600.0000, 
sim time next is 3475200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.1717922361574, 8.27211503730643, 0.0, 1.0, 41.47982554919567], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 1.0, 0.881684605165343, 0.0827211503730643, 0.0, 1.0, 0.2962844682085405], 
reward next is 0.7037, 
noisyNet noise sample is [array([0.01740063], dtype=float32), -1.2818097]. 
=============================================
[2019-04-01 18:58:22,386] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:58:22,386] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4666
[2019-04-01 18:58:22,406] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 108.0, 790.5, 26.0, 26.64841777879791, 15.15375985953034, 1.0, 1.0, 15.92049059717695], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3420000.0000, 
sim time next is 3420600.0000, 
raw observation next is [3.0, 50.5, 106.3333333333333, 785.3333333333334, 26.0, 26.72263685548031, 15.5870121319426, 1.0, 1.0, 15.32063289792757], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.505, 0.35444444444444434, 0.8677716390423573, 1.0, 1.1032338364971872, 0.155870121319426, 1.0, 1.0, 0.10943309212805408], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.83746284], dtype=float32), 0.99716717]. 
=============================================
[2019-04-01 18:58:26,028] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:58:26,030] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3906
[2019-04-01 18:58:26,054] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 50.5, 13.0, 151.0, 26.0, 26.56984197660017, 13.4969020895732, 1.0, 1.0, 9.244795514730328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3519000.0000, 
sim time next is 3519600.0000, 
raw observation next is [2.333333333333333, 51.0, 10.83333333333333, 125.8333333333333, 26.0, 25.90247048997134, 13.17426280715947, 1.0, 1.0, 7.762641986080074], 
processed observation next is [1.0, 0.7391304347826086, 0.5272391505078486, 0.51, 0.0361111111111111, 0.13904235727440142, 1.0, 0.9860672128530484, 0.1317426280715947, 1.0, 1.0, 0.055447442757714814], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.29816037], dtype=float32), 0.9832989]. 
=============================================
[2019-04-01 18:58:33,341] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:58:33,343] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2795
[2019-04-01 18:58:33,352] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.0, 24.0, 113.5, 789.5, 26.0, 25.76710846554216, 10.81300549332284, 0.0, 1.0, 9.523162504676085], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3668400.0000, 
sim time next is 3669000.0000, 
raw observation next is [10.66666666666667, 27.5, 114.3333333333333, 798.3333333333334, 26.0, 25.77847964680286, 10.82087651813544, 0.0, 1.0, 8.532642830769575], 
processed observation next is [0.0, 0.4782608695652174, 0.7580794090489382, 0.275, 0.381111111111111, 0.8821362799263353, 1.0, 0.9683542352575516, 0.1082087651813544, 0.0, 1.0, 0.06094744879121125], 
reward next is 0.9391, 
noisyNet noise sample is [array([2.476017], dtype=float32), 0.8141495]. 
=============================================
[2019-04-01 18:58:33,363] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.20956]
 [72.73024]
 [72.20388]
 [71.64231]
 [71.20062]], R is [[73.48137665]
 [73.67854309]
 [73.86940765]
 [74.05473328]
 [74.23344421]].
[2019-04-01 18:58:33,890] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0242675e-18 3.7123821e-27 4.3124180e-29 9.8570297e-23 1.9001333e-26
 1.0000000e+00 8.0497477e-21], sum to 1.0000
[2019-04-01 18:58:33,890] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9048
[2019-04-01 18:58:33,912] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333333, 61.83333333333333, 32.66666666666666, 277.6666666666666, 26.0, 26.97598535402828, 13.43990424555595, 1.0, 1.0, 9.775693134594276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3777000.0000, 
sim time next is 3777600.0000, 
raw observation next is [-0.6666666666666666, 63.66666666666667, 24.83333333333333, 212.3333333333333, 26.0, 26.83502933779585, 13.44764296885405, 1.0, 1.0, 9.828018509515474], 
processed observation next is [1.0, 0.7391304347826086, 0.44413665743305636, 0.6366666666666667, 0.08277777777777776, 0.23462246777163898, 1.0, 1.119289905399407, 0.13447642968854048, 1.0, 1.0, 0.07020013221082481], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.32333556], dtype=float32), 0.22651574]. 
=============================================
[2019-04-01 18:58:34,161] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:58:34,162] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5236
[2019-04-01 18:58:34,183] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.166666666666667, 66.0, 0.0, 0.0, 26.0, 25.12728988867709, 9.417901155748625, 0.0, 1.0, 25.49897318275004], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3787800.0000, 
sim time next is 3788400.0000, 
raw observation next is [-2.333333333333333, 67.0, 0.0, 0.0, 26.0, 25.07170549415494, 9.743482190790422, 0.0, 1.0, 43.40879527408872], 
processed observation next is [1.0, 0.8695652173913043, 0.3979686057248385, 0.67, 0.0, 0.0, 1.0, 0.8673864991649916, 0.09743482190790421, 0.0, 1.0, 0.310062823386348], 
reward next is 0.6899, 
noisyNet noise sample is [array([0.95727426], dtype=float32), -0.28707507]. 
=============================================
[2019-04-01 18:58:40,166] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 2.305978e-33 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:58:40,166] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7076
[2019-04-01 18:58:40,180] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.16666666666667, 35.33333333333334, 116.6666666666667, 796.0, 26.0, 27.68175141830246, 21.98378686840884, 1.0, 1.0, 5.225017036807952], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4359000.0000, 
sim time next is 4359600.0000, 
raw observation next is [12.6, 34.0, 117.5, 804.0, 26.0, 27.79975980302036, 22.3813039577409, 1.0, 1.0, 4.239182813184132], 
processed observation next is [1.0, 0.4782608695652174, 0.8116343490304709, 0.34, 0.39166666666666666, 0.8883977900552487, 1.0, 1.257108543288623, 0.223813039577409, 1.0, 1.0, 0.030279877237029514], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.91768193], dtype=float32), 0.16395096]. 
=============================================
[2019-04-01 18:58:41,269] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.8591325e-38
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:58:41,270] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0895
[2019-04-01 18:58:41,286] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.166666666666667, 72.0, 0.0, 0.0, 26.0, 25.26186782629603, 8.766361258990065, 0.0, 1.0, 43.12400701342227], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3802200.0000, 
sim time next is 3802800.0000, 
raw observation next is [-3.333333333333333, 73.0, 0.0, 0.0, 26.0, 25.2385447792338, 8.68259246033106, 0.0, 1.0, 43.18198988957258], 
processed observation next is [1.0, 0.0, 0.37026777469990774, 0.73, 0.0, 0.0, 1.0, 0.8912206827476855, 0.0868259246033106, 0.0, 1.0, 0.3084427849255184], 
reward next is 0.6916, 
noisyNet noise sample is [array([-0.41143605], dtype=float32), -1.8470228]. 
=============================================
[2019-04-01 18:58:43,297] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.1536134e-36 4.1159868e-23
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:58:43,298] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4603
[2019-04-01 18:58:43,321] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.39321615818578, 9.650344881992966, 0.0, 1.0, 39.26180197331611], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3888000.0000, 
sim time next is 3888600.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.39654627380629, 9.557673142572016, 0.0, 1.0, 38.44668125728797], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 1.0, 0.91379232482947, 0.09557673142572015, 0.0, 1.0, 0.2746191518377712], 
reward next is 0.7254, 
noisyNet noise sample is [array([0.44013086], dtype=float32), -0.6289883]. 
=============================================
[2019-04-01 18:58:43,407] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4648962e-29 1.8543715e-23
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:58:43,409] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0261
[2019-04-01 18:58:43,426] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 60.0, 0.0, 0.0, 26.0, 25.69939454508562, 13.07552334818043, 0.0, 1.0, 34.79548111259781], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3877200.0000, 
sim time next is 3877800.0000, 
raw observation next is [-1.0, 59.16666666666666, 0.0, 0.0, 26.0, 25.79551026230715, 13.05362120714926, 0.0, 1.0, 30.20017207004272], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.5916666666666666, 0.0, 0.0, 1.0, 0.9707871803295929, 0.1305362120714926, 0.0, 1.0, 0.2157155147860194], 
reward next is 0.7843, 
noisyNet noise sample is [array([0.56027305], dtype=float32), 0.2108051]. 
=============================================
[2019-04-01 18:58:43,534] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3237151e-28 0.0000000e+00
 1.0000000e+00 1.7736770e-31], sum to 1.0000
[2019-04-01 18:58:43,534] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6730
[2019-04-01 18:58:43,588] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 77.0, 33.66666666666666, 248.3333333333333, 26.0, 25.33095905973197, 7.910890644982445, 1.0, 1.0, 34.67613210783351], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3829800.0000, 
sim time next is 3830400.0000, 
raw observation next is [-5.0, 77.0, 48.0, 298.0, 26.0, 25.27923735130189, 7.797258344850957, 1.0, 1.0, 32.47412834256944], 
processed observation next is [1.0, 0.34782608695652173, 0.32409972299168976, 0.77, 0.16, 0.3292817679558011, 1.0, 0.8970339073288416, 0.07797258344850957, 1.0, 1.0, 0.23195805958978175], 
reward next is 0.7680, 
noisyNet noise sample is [array([1.8996346], dtype=float32), 0.62604153]. 
=============================================
[2019-04-01 18:58:44,828] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:58:44,829] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2619
[2019-04-01 18:58:44,857] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.4976052723347, 11.27590876740186, 0.0, 1.0, 38.39664639323071], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3962400.0000, 
sim time next is 3963000.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.54361937877272, 11.14220131441655, 0.0, 1.0, 34.08231339646225], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 1.0, 0.934802768396103, 0.1114220131441655, 0.0, 1.0, 0.24344509568901604], 
reward next is 0.7566, 
noisyNet noise sample is [array([-0.79630554], dtype=float32), -1.513313]. 
=============================================
[2019-04-01 18:58:44,864] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[61.791286]
 [61.006542]
 [59.90539 ]
 [58.479908]
 [56.42541 ]], R is [[62.45226288]
 [62.55347824]
 [62.62577057]
 [62.71273422]
 [62.85948944]].
[2019-04-01 18:58:46,307] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:58:46,307] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6954
[2019-04-01 18:58:46,346] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 45.66666666666666, 118.0, 831.3333333333334, 26.0, 26.31231738990316, 15.64455037333648, 1.0, 1.0, 20.88865133561872], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3934200.0000, 
sim time next is 3934800.0000, 
raw observation next is [-6.0, 45.0, 117.5, 829.5, 26.0, 26.59171730925626, 16.43899163360433, 1.0, 1.0, 19.15334302089437], 
processed observation next is [1.0, 0.5652173913043478, 0.296398891966759, 0.45, 0.39166666666666666, 0.9165745856353591, 1.0, 1.0845310441794658, 0.1643899163360433, 1.0, 1.0, 0.13680959300638834], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.24001533], dtype=float32), -1.8369608]. 
=============================================
[2019-04-01 18:58:53,756] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:58:53,768] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2212
[2019-04-01 18:58:53,790] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.0, 36.0, 49.66666666666666, 0.0, 26.0, 28.67232234330133, 32.29796887777346, 1.0, 1.0, 3.938769698320669], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4378800.0000, 
sim time next is 4379400.0000, 
raw observation next is [13.0, 36.5, 39.0, 0.0, 26.0, 28.73542745231371, 28.03967459017802, 1.0, 1.0, 5.381912575091051], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.365, 0.13, 0.0, 1.0, 1.3907753503305302, 0.2803967459017802, 1.0, 1.0, 0.03844223267922179], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0253915], dtype=float32), -0.51889104]. 
=============================================
[2019-04-01 18:58:54,922] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:58:54,924] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1706
[2019-04-01 18:58:54,932] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 31.0, 111.0, 812.0, 26.0, 26.63248799368081, 14.84785074873662, 1.0, 1.0, 2.285245110688463], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4111200.0000, 
sim time next is 4111800.0000, 
raw observation next is [3.166666666666667, 31.66666666666667, 109.3333333333333, 806.0, 26.0, 26.75987436090796, 15.34051680904234, 1.0, 1.0, 2.231448013875078], 
processed observation next is [1.0, 0.6086956521739131, 0.5503231763619576, 0.3166666666666667, 0.36444444444444435, 0.8906077348066298, 1.0, 1.1085534801297088, 0.1534051680904234, 1.0, 1.0, 0.015938914384821985], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.24002582], dtype=float32), 1.259844]. 
=============================================
[2019-04-01 18:59:00,823] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.1548618e-38
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:59:00,827] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5703
[2019-04-01 18:59:00,845] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.666666666666666, 59.33333333333333, 54.66666666666667, 446.6666666666667, 26.0, 25.51743848915855, 9.654514185744993, 0.0, 1.0, 3.620937107198594], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4294200.0000, 
sim time next is 4294800.0000, 
raw observation next is [6.6, 60.0, 47.0, 392.0, 26.0, 25.55100296564971, 9.512474386310688, 0.0, 1.0, 3.641009062513653], 
processed observation next is [0.0, 0.7391304347826086, 0.6454293628808865, 0.6, 0.15666666666666668, 0.4331491712707182, 1.0, 0.9358575665213874, 0.09512474386310688, 0.0, 1.0, 0.026007207589383234], 
reward next is 0.9740, 
noisyNet noise sample is [array([-1.877253], dtype=float32), -0.27678946]. 
=============================================
[2019-04-01 18:59:04,179] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:59:04,179] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1920
[2019-04-01 18:59:04,194] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 47.33333333333334, 0.0, 0.0, 26.0, 25.34238359310894, 7.899452586233451, 0.0, 1.0, 40.63585031004084], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4231200.0000, 
sim time next is 4231800.0000, 
raw observation next is [1.5, 47.5, 0.0, 0.0, 26.0, 25.37808313186602, 7.984771293896753, 0.0, 1.0, 40.11126877795689], 
processed observation next is [0.0, 1.0, 0.5041551246537397, 0.475, 0.0, 0.0, 1.0, 0.9111547331237172, 0.07984771293896753, 0.0, 1.0, 0.28650906269969206], 
reward next is 0.7135, 
noisyNet noise sample is [array([-1.6104423], dtype=float32), 0.6779096]. 
=============================================
[2019-04-01 18:59:06,018] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:59:06,021] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4606
[2019-04-01 18:59:06,029] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.86666666666667, 58.16666666666667, 0.0, 0.0, 26.0, 27.17239692529296, 22.64710899885999, 1.0, 1.0, 6.196296798911681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4392600.0000, 
sim time next is 4393200.0000, 
raw observation next is [10.73333333333333, 58.33333333333334, 0.0, 0.0, 26.0, 27.11194705865022, 22.21285481963514, 0.0, 1.0, 6.221778654280045], 
processed observation next is [1.0, 0.8695652173913043, 0.7599261311172669, 0.5833333333333335, 0.0, 0.0, 1.0, 1.1588495798071743, 0.22212854819635142, 0.0, 1.0, 0.04444127610200032], 
reward next is 0.9556, 
noisyNet noise sample is [array([-0.6549459], dtype=float32), 0.31687605]. 
=============================================
[2019-04-01 18:59:16,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:16,830] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:16,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run26
[2019-04-01 18:59:20,338] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2771434e-30 0.0000000e+00 0.0000000e+00 2.4760941e-20 5.1950502e-35
 1.0000000e+00 1.4175580e-12], sum to 1.0000
[2019-04-01 18:59:20,338] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7426
[2019-04-01 18:59:20,351] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 26.0, 25.40241304655177, 10.14914785764506, 0.0, 1.0, 33.79923271570789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4661400.0000, 
sim time next is 4662000.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 26.0, 25.35580256018008, 10.46772189039915, 0.0, 1.0, 40.37174455177768], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.57, 0.0, 0.0, 1.0, 0.9079717943114402, 0.1046772189039915, 0.0, 1.0, 0.28836960394126915], 
reward next is 0.7116, 
noisyNet noise sample is [array([1.7746179], dtype=float32), -0.5679164]. 
=============================================
[2019-04-01 18:59:20,360] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[76.21788]
 [76.64315]
 [76.82871]
 [76.74156]
 [76.84524]], R is [[75.7070694 ]
 [75.70857239]
 [75.78582001]
 [75.91643524]
 [76.03105164]].
[2019-04-01 18:59:20,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:20,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:20,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run26
[2019-04-01 18:59:22,894] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:59:22,895] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4828
[2019-04-01 18:59:22,915] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.46026210433612, 8.857131464891246, 0.0, 1.0, 29.52364483463827], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4689600.0000, 
sim time next is 4690200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.45249945159744, 8.820724324763205, 0.0, 1.0, 32.27386904434931], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 1.0, 0.9217856359424914, 0.08820724324763204, 0.0, 1.0, 0.23052763603106652], 
reward next is 0.7695, 
noisyNet noise sample is [array([0.21391721], dtype=float32), 0.53502303]. 
=============================================
[2019-04-01 18:59:25,500] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.31584527e-25 0.00000000e+00 2.79170575e-31 4.64818895e-12
 2.71220798e-18 1.00000000e+00 1.03463764e-35], sum to 1.0000
[2019-04-01 18:59:25,504] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9959
[2019-04-01 18:59:25,525] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.60636084092392, 9.44611967693286, 0.0, 1.0, 26.51407145724197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4687800.0000, 
sim time next is 4688400.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.57511503044075, 9.083756294071003, 0.0, 1.0, 25.83027898585301], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 1.0, 0.9393021472058213, 0.09083756294071003, 0.0, 1.0, 0.18450199275609291], 
reward next is 0.8155, 
noisyNet noise sample is [array([1.8659506], dtype=float32), 0.07494172]. 
=============================================
[2019-04-01 18:59:25,686] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:59:25,686] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2515
[2019-04-01 18:59:25,696] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 77.0, 0.0, 0.0, 26.0, 25.66743499487429, 9.345260528733741, 0.0, 1.0, 20.04135330944621], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4678200.0000, 
sim time next is 4678800.0000, 
raw observation next is [0.6666666666666667, 82.0, 0.0, 0.0, 26.0, 25.56532643128615, 9.127099311022377, 0.0, 1.0, 19.49540998486726], 
processed observation next is [1.0, 0.13043478260869565, 0.4810710987996307, 0.82, 0.0, 0.0, 1.0, 0.9379037758980212, 0.09127099311022377, 0.0, 1.0, 0.13925292846333756], 
reward next is 0.8607, 
noisyNet noise sample is [array([-0.10749973], dtype=float32), -0.73356736]. 
=============================================
[2019-04-01 18:59:26,772] A3C_AGENT_WORKER-Thread-17 INFO:Local step 212500, global step 3391243: loss 0.0304
[2019-04-01 18:59:26,773] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 212500, global step 3391243: learning rate 0.0010
[2019-04-01 18:59:29,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:29,098] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:29,115] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run26
[2019-04-01 18:59:30,167] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 4.529543e-28
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 18:59:30,168] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0477
[2019-04-01 18:59:30,195] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.15130853938542, 7.416112097425715, 0.0, 1.0, 39.00985855899514], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4842600.0000, 
sim time next is 4843200.0000, 
raw observation next is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.140738940385, 7.368159115982448, 0.0, 1.0, 38.91852910293413], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6000000000000001, 0.0, 0.0, 1.0, 0.8772484200549998, 0.07368159115982448, 0.0, 1.0, 0.2779894935923866], 
reward next is 0.7220, 
noisyNet noise sample is [array([-2.7642334], dtype=float32), 2.5397277]. 
=============================================
[2019-04-01 18:59:31,199] A3C_AGENT_WORKER-Thread-18 INFO:Local step 212500, global step 3393403: loss 0.0262
[2019-04-01 18:59:31,200] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 212500, global step 3393403: learning rate 0.0010
[2019-04-01 18:59:32,402] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.3662983e-30
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 18:59:32,402] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1219
[2019-04-01 18:59:32,420] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.39905167429567, 6.982125557625868, 0.0, 1.0, 31.10283107385094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4934400.0000, 
sim time next is 4935000.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.29820089952182, 6.929431764433616, 0.0, 1.0, 35.48592691147483], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 1.0, 0.8997429856459743, 0.06929431764433616, 0.0, 1.0, 0.2534709065105345], 
reward next is 0.7465, 
noisyNet noise sample is [array([-0.2618835], dtype=float32), -0.260408]. 
=============================================
[2019-04-01 18:59:32,427] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[65.02572 ]
 [64.96537 ]
 [64.79716 ]
 [64.70788 ]
 [64.687996]], R is [[65.1067276 ]
 [65.23349762]
 [65.38399506]
 [65.49511719]
 [65.56481934]].
[2019-04-01 18:59:38,594] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:59:38,595] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3347
[2019-04-01 18:59:38,614] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 29.5, 117.0, 803.0, 26.0, 26.57943242387744, 12.41840928083922, 1.0, 1.0, 9.777060190598508], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4962600.0000, 
sim time next is 4963200.0000, 
raw observation next is [2.333333333333333, 29.33333333333334, 117.8333333333333, 810.0, 26.0, 26.61604727879299, 12.82787582095165, 1.0, 1.0, 8.969849070223619], 
processed observation next is [1.0, 0.43478260869565216, 0.5272391505078486, 0.2933333333333334, 0.39277777777777767, 0.8950276243093923, 1.0, 1.0880067541132843, 0.1282787582095165, 1.0, 1.0, 0.06407035050159728], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.70324117], dtype=float32), -1.1678451]. 
=============================================
[2019-04-01 18:59:39,250] A3C_AGENT_WORKER-Thread-4 INFO:Local step 212500, global step 3397583: loss 0.2820
[2019-04-01 18:59:39,251] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 212500, global step 3397583: learning rate 0.0010
[2019-04-01 18:59:39,567] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:39,568] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:39,574] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run26
[2019-04-01 18:59:39,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:39,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:39,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run26
[2019-04-01 18:59:40,354] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:40,354] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:40,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run26
[2019-04-01 18:59:40,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:40,462] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:40,475] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run26
[2019-04-01 18:59:41,341] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:59:41,341] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7427
[2019-04-01 18:59:41,361] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.3, 80.33333333333334, 0.0, 0.0, 26.0, 23.97062249627173, 5.349017118545376, 0.0, 1.0, 42.99262026053618], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 100200.0000, 
sim time next is 100800.0000, 
raw observation next is [-3.4, 79.0, 0.0, 0.0, 26.0, 23.92378328560668, 5.335445358874445, 0.0, 1.0, 43.05563168223746], 
processed observation next is [1.0, 0.17391304347826086, 0.368421052631579, 0.79, 0.0, 0.0, 1.0, 0.7033976122295259, 0.053354453588744445, 0.0, 1.0, 0.30754022630169614], 
reward next is 0.6925, 
noisyNet noise sample is [array([-0.4710792], dtype=float32), -0.5979475]. 
=============================================
[2019-04-01 18:59:42,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:42,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:42,016] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run26
[2019-04-01 18:59:42,258] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 18:59:42,258] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8441
[2019-04-01 18:59:42,269] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 49.33333333333334, 0.0, 0.0, 26.0, 25.4620137817064, 8.580266010730801, 0.0, 1.0, 34.26672023272474], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5029800.0000, 
sim time next is 5030400.0000, 
raw observation next is [-1.0, 48.66666666666667, 0.0, 0.0, 26.0, 25.53356394462411, 8.499506962792715, 0.0, 1.0, 32.83631799541966], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 0.4866666666666667, 0.0, 0.0, 1.0, 0.9333662778034443, 0.08499506962792715, 0.0, 1.0, 0.23454512853871187], 
reward next is 0.7655, 
noisyNet noise sample is [array([0.63267106], dtype=float32), -0.3459853]. 
=============================================
[2019-04-01 18:59:42,593] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:42,593] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:42,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run26
[2019-04-01 18:59:43,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:43,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:43,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run26
[2019-04-01 18:59:44,932] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:44,932] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:44,935] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run26
[2019-04-01 18:59:45,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:45,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:45,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run26
[2019-04-01 18:59:45,257] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:45,257] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:45,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run26
[2019-04-01 18:59:46,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:46,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:46,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run26
[2019-04-01 18:59:46,487] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:46,487] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:46,490] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run26
[2019-04-01 18:59:46,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 18:59:46,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:46,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run26
[2019-04-01 18:59:48,734] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-01 18:59:48,737] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 18:59:48,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:48,739] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run35
[2019-04-01 18:59:48,806] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 18:59:48,807] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 18:59:48,807] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:48,809] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 18:59:48,815] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run35
[2019-04-01 18:59:48,850] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run35
[2019-04-01 19:00:24,080] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.4937252], dtype=float32), -0.75016403]
[2019-04-01 19:00:24,080] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.982185796, 88.69160864, 0.0, 0.0, 26.0, 24.52944948738533, 6.934916792506503, 0.0, 1.0, 44.49689414413017]
[2019-04-01 19:00:24,080] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:00:24,081] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.5374677e-32 3.2758549e-26
 1.0000000e+00 0.0000000e+00], sampled 0.07381530789832003
[2019-04-01 19:01:06,668] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.4937252], dtype=float32), -0.75016403]
[2019-04-01 19:01:06,668] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-4.15, 80.0, 0.0, 0.0, 26.0, 24.71492997602634, 7.716708837573347, 0.0, 1.0, 44.72588290816825]
[2019-04-01 19:01:06,668] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 19:01:06,669] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.791683e-38
 1.000000e+00 0.000000e+00], sampled 0.5718722909640349
[2019-04-01 19:01:31,214] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 19:01:51,557] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 19:01:55,358] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 19:01:56,380] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 3400000, evaluation results [3400000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 19:01:57,283] A3C_AGENT_WORKER-Thread-17 INFO:Local step 213000, global step 3400456: loss 0.0821
[2019-04-01 19:01:57,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 213000, global step 3400456: learning rate 0.0010
[2019-04-01 19:01:58,166] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:01:58,166] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9823
[2019-04-01 19:01:58,244] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 32.33333333333334, 0.0, 26.0, 23.07906373740434, 6.016510705671084, 0.0, 1.0, 57.3603840436241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 33000.0000, 
sim time next is 33600.0000, 
raw observation next is [7.7, 93.0, 35.16666666666666, 0.0, 26.0, 23.21055739353488, 5.883502161512898, 0.0, 1.0, 57.18986165187906], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.11722222222222219, 0.0, 1.0, 0.6015081990764115, 0.058835021615128984, 0.0, 1.0, 0.40849901179913617], 
reward next is 0.5915, 
noisyNet noise sample is [array([0.10344109], dtype=float32), 1.5157067]. 
=============================================
[2019-04-01 19:01:58,380] A3C_AGENT_WORKER-Thread-11 INFO:Local step 212500, global step 3400768: loss 0.0333
[2019-04-01 19:01:58,381] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 212500, global step 3400768: learning rate 0.0010
[2019-04-01 19:01:58,689] A3C_AGENT_WORKER-Thread-20 INFO:Local step 212500, global step 3400835: loss 0.0079
[2019-04-01 19:01:58,690] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 212500, global step 3400835: learning rate 0.0010
[2019-04-01 19:01:59,147] A3C_AGENT_WORKER-Thread-2 INFO:Local step 212500, global step 3400937: loss 0.0154
[2019-04-01 19:01:59,165] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 212500, global step 3400937: learning rate 0.0010
[2019-04-01 19:01:59,515] A3C_AGENT_WORKER-Thread-14 INFO:Local step 212500, global step 3401023: loss 0.0146
[2019-04-01 19:01:59,516] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 212500, global step 3401023: learning rate 0.0010
[2019-04-01 19:02:00,153] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:02:00,153] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4618
[2019-04-01 19:02:00,198] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.0, 86.0, 87.0, 0.0, 26.0, 24.45121447508909, 6.332035739686073, 0.0, 1.0, 29.80799809365152], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 48600.0000, 
sim time next is 49200.0000, 
raw observation next is [7.9, 86.0, 85.66666666666666, 0.0, 26.0, 24.41280361494912, 6.304572619661748, 0.0, 1.0, 32.54122137794904], 
processed observation next is [0.0, 0.5652173913043478, 0.6814404432132966, 0.86, 0.2855555555555555, 0.0, 1.0, 0.7732576592784456, 0.06304572619661748, 0.0, 1.0, 0.2324372955567789], 
reward next is 0.7676, 
noisyNet noise sample is [array([0.8434287], dtype=float32), 1.4058397]. 
=============================================
[2019-04-01 19:02:01,051] A3C_AGENT_WORKER-Thread-12 INFO:Local step 212500, global step 3401494: loss 0.3969
[2019-04-01 19:02:01,054] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 212500, global step 3401495: learning rate 0.0010
[2019-04-01 19:02:01,136] A3C_AGENT_WORKER-Thread-13 INFO:Local step 212500, global step 3401525: loss 0.2591
[2019-04-01 19:02:01,137] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 212500, global step 3401526: learning rate 0.0010
[2019-04-01 19:02:01,185] A3C_AGENT_WORKER-Thread-19 INFO:Local step 212500, global step 3401540: loss 0.4216
[2019-04-01 19:02:01,186] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 212500, global step 3401540: learning rate 0.0010
[2019-04-01 19:02:01,196] A3C_AGENT_WORKER-Thread-15 INFO:Local step 212500, global step 3401548: loss 0.1310
[2019-04-01 19:02:01,199] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 212500, global step 3401548: learning rate 0.0010
[2019-04-01 19:02:01,240] A3C_AGENT_WORKER-Thread-9 INFO:Local step 212500, global step 3401562: loss 0.0796
[2019-04-01 19:02:01,240] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 212500, global step 3401562: learning rate 0.0010
[2019-04-01 19:02:01,258] A3C_AGENT_WORKER-Thread-10 INFO:Local step 212500, global step 3401570: loss 0.0661
[2019-04-01 19:02:01,259] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 212500, global step 3401570: learning rate 0.0010
[2019-04-01 19:02:01,276] A3C_AGENT_WORKER-Thread-5 INFO:Local step 212500, global step 3401579: loss 0.1477
[2019-04-01 19:02:01,276] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 212500, global step 3401579: learning rate 0.0010
[2019-04-01 19:02:01,360] A3C_AGENT_WORKER-Thread-16 INFO:Local step 212500, global step 3401608: loss 0.0661
[2019-04-01 19:02:01,360] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 212500, global step 3401608: learning rate 0.0010
[2019-04-01 19:02:01,364] A3C_AGENT_WORKER-Thread-3 INFO:Local step 212500, global step 3401609: loss 0.1201
[2019-04-01 19:02:01,364] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 212500, global step 3401609: learning rate 0.0010
[2019-04-01 19:02:01,795] A3C_AGENT_WORKER-Thread-18 INFO:Local step 213000, global step 3401755: loss 0.0065
[2019-04-01 19:02:01,795] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 213000, global step 3401755: learning rate 0.0010
[2019-04-01 19:02:10,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.2195244e-33
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:02:10,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8323
[2019-04-01 19:02:10,307] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 74.0, 0.0, 0.0, 26.0, 23.62679828905739, 5.452460436598375, 0.0, 1.0, 43.78511353077744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 178200.0000, 
sim time next is 178800.0000, 
raw observation next is [-8.9, 74.0, 0.0, 0.0, 26.0, 23.56261673495719, 5.437153320480363, 0.0, 1.0, 43.76326939562022], 
processed observation next is [1.0, 0.043478260869565216, 0.21606648199445982, 0.74, 0.0, 0.0, 1.0, 0.6518023907081698, 0.054371533204803625, 0.0, 1.0, 0.3125947813972873], 
reward next is 0.6874, 
noisyNet noise sample is [array([-0.72882724], dtype=float32), -0.3000147]. 
=============================================
[2019-04-01 19:02:11,076] A3C_AGENT_WORKER-Thread-4 INFO:Local step 213000, global step 3404961: loss 0.0036
[2019-04-01 19:02:11,078] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 213000, global step 3404961: learning rate 0.0010
[2019-04-01 19:02:13,696] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:02:13,696] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0393
[2019-04-01 19:02:13,736] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.75, 65.0, 129.0, 0.0, 26.0, 25.25290181838715, 6.73783208951635, 1.0, 1.0, 31.55482834632256], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 217800.0000, 
sim time next is 218400.0000, 
raw observation next is [-4.666666666666667, 65.0, 132.3333333333333, 0.0, 26.0, 25.16907507015992, 6.702185076313948, 1.0, 1.0, 31.55065241637164], 
processed observation next is [1.0, 0.5217391304347826, 0.3333333333333333, 0.65, 0.44111111111111095, 0.0, 1.0, 0.8812964385942743, 0.06702185076313948, 1.0, 1.0, 0.22536180297408315], 
reward next is 0.7746, 
noisyNet noise sample is [array([-0.34286726], dtype=float32), 0.35840467]. 
=============================================
[2019-04-01 19:02:21,828] A3C_AGENT_WORKER-Thread-17 INFO:Local step 213500, global step 3408149: loss 1.6178
[2019-04-01 19:02:21,829] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 213500, global step 3408149: learning rate 0.0010
[2019-04-01 19:02:23,074] A3C_AGENT_WORKER-Thread-20 INFO:Local step 213000, global step 3408541: loss 0.0398
[2019-04-01 19:02:23,074] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 213000, global step 3408541: learning rate 0.0010
[2019-04-01 19:02:23,397] A3C_AGENT_WORKER-Thread-11 INFO:Local step 213000, global step 3408648: loss 0.0035
[2019-04-01 19:02:23,398] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 213000, global step 3408648: learning rate 0.0010
[2019-04-01 19:02:23,531] A3C_AGENT_WORKER-Thread-2 INFO:Local step 213000, global step 3408690: loss 0.0151
[2019-04-01 19:02:23,531] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 213000, global step 3408690: learning rate 0.0010
[2019-04-01 19:02:23,994] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:02:23,995] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8512
[2019-04-01 19:02:24,014] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.78329158742214, 10.07118656441663, 0.0, 1.0, 48.31034906454488], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 369000.0000, 
sim time next is 369600.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.71051096014225, 10.45218233804103, 0.0, 1.0, 48.46999966816091], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 1.0, 0.38721585144889303, 0.1045218233804103, 0.0, 1.0, 0.3462142833440065], 
reward next is 0.6538, 
noisyNet noise sample is [array([-1.135465], dtype=float32), 0.40760407]. 
=============================================
[2019-04-01 19:02:24,094] A3C_AGENT_WORKER-Thread-14 INFO:Local step 213000, global step 3408863: loss 0.0583
[2019-04-01 19:02:24,100] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 213000, global step 3408863: learning rate 0.0010
[2019-04-01 19:02:25,285] A3C_AGENT_WORKER-Thread-10 INFO:Local step 213000, global step 3409353: loss 0.0056
[2019-04-01 19:02:25,288] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 213000, global step 3409355: learning rate 0.0010
[2019-04-01 19:02:25,306] A3C_AGENT_WORKER-Thread-9 INFO:Local step 213000, global step 3409361: loss 0.0022
[2019-04-01 19:02:25,309] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 213000, global step 3409364: learning rate 0.0010
[2019-04-01 19:02:25,347] A3C_AGENT_WORKER-Thread-15 INFO:Local step 213000, global step 3409385: loss 0.0071
[2019-04-01 19:02:25,347] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 213000, global step 3409385: learning rate 0.0010
[2019-04-01 19:02:25,518] A3C_AGENT_WORKER-Thread-12 INFO:Local step 213000, global step 3409466: loss 0.0001
[2019-04-01 19:02:25,519] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 213000, global step 3409466: learning rate 0.0010
[2019-04-01 19:02:25,819] A3C_AGENT_WORKER-Thread-13 INFO:Local step 213000, global step 3409604: loss 0.0480
[2019-04-01 19:02:25,823] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 213000, global step 3409605: learning rate 0.0010
[2019-04-01 19:02:25,845] A3C_AGENT_WORKER-Thread-5 INFO:Local step 213000, global step 3409612: loss 0.0353
[2019-04-01 19:02:25,846] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 213000, global step 3409612: learning rate 0.0010
[2019-04-01 19:02:25,938] A3C_AGENT_WORKER-Thread-16 INFO:Local step 213000, global step 3409646: loss 0.0261
[2019-04-01 19:02:25,939] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 213000, global step 3409646: learning rate 0.0010
[2019-04-01 19:02:26,059] A3C_AGENT_WORKER-Thread-3 INFO:Local step 213000, global step 3409687: loss 0.0165
[2019-04-01 19:02:26,060] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 213000, global step 3409687: learning rate 0.0010
[2019-04-01 19:02:26,106] A3C_AGENT_WORKER-Thread-19 INFO:Local step 213000, global step 3409701: loss 0.0037
[2019-04-01 19:02:26,107] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 213000, global step 3409701: learning rate 0.0010
[2019-04-01 19:02:26,147] A3C_AGENT_WORKER-Thread-18 INFO:Local step 213500, global step 3409718: loss 1.2507
[2019-04-01 19:02:26,147] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 213500, global step 3409718: learning rate 0.0010
[2019-04-01 19:02:26,602] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.3470424e-21 1.9601962e-18 1.0413608e-28 1.3900124e-20 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:02:26,602] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7796
[2019-04-01 19:02:26,668] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.1, 55.5, 58.0, 764.0, 26.0, 25.632671275074, 8.872530601748123, 1.0, 1.0, 65.74346043541287], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 387000.0000, 
sim time next is 387600.0000, 
raw observation next is [-13.0, 54.0, 58.0, 787.5, 26.0, 25.85277695412499, 9.502033394438826, 1.0, 1.0, 42.96450987209733], 
processed observation next is [1.0, 0.4782608695652174, 0.10249307479224376, 0.54, 0.19333333333333333, 0.8701657458563536, 1.0, 0.9789681363035702, 0.09502033394438826, 1.0, 1.0, 0.30688935622926666], 
reward next is 0.6931, 
noisyNet noise sample is [array([-0.6895194], dtype=float32), -1.0522339]. 
=============================================
[2019-04-01 19:02:31,971] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:02:31,972] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2651
[2019-04-01 19:02:32,015] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.4, 45.33333333333334, 0.0, 0.0, 26.0, 24.94321457351348, 7.245301709199339, 0.0, 1.0, 45.58247950157167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 420000.0000, 
sim time next is 420600.0000, 
raw observation next is [-10.5, 46.16666666666667, 0.0, 0.0, 26.0, 24.81965291927406, 6.988150938188251, 0.0, 1.0, 45.49185863350731], 
processed observation next is [1.0, 0.8695652173913043, 0.17174515235457063, 0.4616666666666667, 0.0, 0.0, 1.0, 0.8313789884677227, 0.0698815093818825, 0.0, 1.0, 0.3249418473821951], 
reward next is 0.6751, 
noisyNet noise sample is [array([-1.0231676], dtype=float32), -2.6201172]. 
=============================================
[2019-04-01 19:02:34,539] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0591268e-13 4.2740690e-16 5.0516796e-12 2.9967055e-08 9.9981183e-01
 1.8822715e-04 5.6867134e-35], sum to 1.0000
[2019-04-01 19:02:34,546] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5536
[2019-04-01 19:02:34,610] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.533333333333333, 38.83333333333334, 15.33333333333334, 0.0, 26.0, 24.94044580002631, 5.907488628722436, 0.0, 1.0, 52.02881623052885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 461400.0000, 
sim time next is 462000.0000, 
raw observation next is [-7.266666666666667, 37.66666666666667, 19.16666666666667, 0.0, 26.0, 24.96655314809967, 5.961634920156183, 1.0, 1.0, 53.20422019835453], 
processed observation next is [1.0, 0.34782608695652173, 0.26131117266851345, 0.3766666666666667, 0.0638888888888889, 0.0, 1.0, 0.8523647354428101, 0.059616349201561836, 1.0, 1.0, 0.3800301442739609], 
reward next is 0.6200, 
noisyNet noise sample is [array([1.0395073], dtype=float32), 0.3774221]. 
=============================================
[2019-04-01 19:02:34,636] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.158455]
 [76.79799 ]
 [73.90317 ]
 [72.12282 ]
 [71.58602 ]], R is [[82.86010742]
 [82.65987396]
 [82.37473297]
 [81.98545837]
 [81.58947754]].
[2019-04-01 19:02:35,588] A3C_AGENT_WORKER-Thread-4 INFO:Local step 213500, global step 3412632: loss 1.1909
[2019-04-01 19:02:35,589] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 213500, global step 3412632: learning rate 0.0010
[2019-04-01 19:02:42,025] A3C_AGENT_WORKER-Thread-17 INFO:Local step 214000, global step 3414983: loss 0.9391
[2019-04-01 19:02:42,026] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 214000, global step 3414983: learning rate 0.0010
[2019-04-01 19:02:43,235] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 6.6369136e-37 0.0000000e+00 1.6622306e-36
 1.0000000e+00 2.2980846e-17], sum to 1.0000
[2019-04-01 19:02:43,235] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2686
[2019-04-01 19:02:43,276] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 80.5, 130.0, 396.0, 26.0, 25.02330260014329, 7.847840512235019, 0.0, 1.0, 25.90891899481941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 563400.0000, 
sim time next is 564000.0000, 
raw observation next is [-1.066666666666667, 80.33333333333333, 131.3333333333333, 429.1666666666666, 26.0, 24.99799849030774, 7.854608576883211, 0.0, 1.0, 24.72700180964864], 
processed observation next is [0.0, 0.5217391304347826, 0.43305632502308405, 0.8033333333333332, 0.4377777777777776, 0.4742173112338857, 1.0, 0.85685692718682, 0.07854608576883211, 0.0, 1.0, 0.1766214414974903], 
reward next is 0.8234, 
noisyNet noise sample is [array([0.10873723], dtype=float32), 0.04053687]. 
=============================================
[2019-04-01 19:02:43,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[69.16502 ]
 [69.37982 ]
 [69.45123 ]
 [69.466644]
 [69.356636]], R is [[68.94789124]
 [69.073349  ]
 [69.17211914]
 [69.23057556]
 [69.27740479]].
[2019-04-01 19:02:45,396] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.1392527e-29
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:02:45,397] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0104
[2019-04-01 19:02:45,409] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.10966033707084, 11.95829109423195, 0.0, 1.0, 45.75959537903682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1024800.0000, 
sim time next is 1025400.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 25.43535733030578, 12.96351271588227, 0.0, 1.0, 40.25977741771784], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 1.0, 0.9193367614722544, 0.1296351271588227, 0.0, 1.0, 0.2875698386979846], 
reward next is 0.7124, 
noisyNet noise sample is [array([-0.23845153], dtype=float32), 0.7154296]. 
=============================================
[2019-04-01 19:02:46,230] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 9.4782478e-32 3.8761830e-37 1.3350297e-04
 4.2028052e-05 9.9982446e-01], sum to 1.0000
[2019-04-01 19:02:46,230] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9317
[2019-04-01 19:02:46,248] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 23.60175050846035, 5.282303653070997, 0.0, 1.0, 43.23806062661362], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 630000.0000, 
sim time next is 630600.0000, 
raw observation next is [-4.5, 69.83333333333334, 0.0, 0.0, 26.0, 23.56983940682752, 5.295125332788264, 0.0, 1.0, 43.28440953527986], 
processed observation next is [0.0, 0.30434782608695654, 0.3379501385041552, 0.6983333333333335, 0.0, 0.0, 1.0, 0.6528342009753599, 0.05295125332788264, 0.0, 1.0, 0.3091743538234276], 
reward next is 0.6908, 
noisyNet noise sample is [array([2.0080984], dtype=float32), -0.25406465]. 
=============================================
[2019-04-01 19:02:46,584] A3C_AGENT_WORKER-Thread-18 INFO:Local step 214000, global step 3416839: loss 0.0016
[2019-04-01 19:02:46,585] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 214000, global step 3416840: learning rate 0.0010
[2019-04-01 19:02:46,915] A3C_AGENT_WORKER-Thread-20 INFO:Local step 213500, global step 3416983: loss 1.6843
[2019-04-01 19:02:46,917] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 213500, global step 3416983: learning rate 0.0010
[2019-04-01 19:02:47,477] A3C_AGENT_WORKER-Thread-2 INFO:Local step 213500, global step 3417160: loss 0.1929
[2019-04-01 19:02:47,477] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 213500, global step 3417160: learning rate 0.0010
[2019-04-01 19:02:47,635] A3C_AGENT_WORKER-Thread-11 INFO:Local step 213500, global step 3417201: loss 0.9467
[2019-04-01 19:02:47,637] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 213500, global step 3417201: learning rate 0.0010
[2019-04-01 19:02:48,383] A3C_AGENT_WORKER-Thread-14 INFO:Local step 213500, global step 3417417: loss 0.3239
[2019-04-01 19:02:48,384] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 213500, global step 3417417: learning rate 0.0010
[2019-04-01 19:02:48,964] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.1805877e-30], sum to 1.0000
[2019-04-01 19:02:48,964] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4363
[2019-04-01 19:02:48,980] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 26.0, 25.02421792341254, 6.708095414867081, 0.0, 1.0, 41.80549089326758], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 678000.0000, 
sim time next is 678600.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 26.0, 25.02283874282804, 6.656038963878136, 0.0, 1.0, 41.97384314589656], 
processed observation next is [0.0, 0.8695652173913043, 0.37673130193905824, 0.67, 0.0, 0.0, 1.0, 0.8604055346897199, 0.06656038963878136, 0.0, 1.0, 0.2998131653278326], 
reward next is 0.7002, 
noisyNet noise sample is [array([1.7390058], dtype=float32), 1.7844836]. 
=============================================
[2019-04-01 19:02:49,255] A3C_AGENT_WORKER-Thread-10 INFO:Local step 213500, global step 3417708: loss 1.0158
[2019-04-01 19:02:49,263] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 213500, global step 3417709: learning rate 0.0010
[2019-04-01 19:02:49,403] A3C_AGENT_WORKER-Thread-9 INFO:Local step 213500, global step 3417765: loss 0.7466
[2019-04-01 19:02:49,404] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 213500, global step 3417765: learning rate 0.0010
[2019-04-01 19:02:49,416] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:02:49,416] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6488
[2019-04-01 19:02:49,427] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 25.62989205187513, 11.23512621780004, 0.0, 1.0, 4.195080258348222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1018800.0000, 
sim time next is 1019400.0000, 
raw observation next is [14.4, 80.33333333333333, 0.0, 0.0, 26.0, 25.65308634470041, 10.89468348522298, 1.0, 1.0, 4.315259381094256], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.8033333333333332, 0.0, 0.0, 1.0, 0.9504409063857727, 0.1089468348522298, 1.0, 1.0, 0.0308232812935304], 
reward next is 0.6113, 
noisyNet noise sample is [array([-0.07554763], dtype=float32), -0.15032576]. 
=============================================
[2019-04-01 19:02:49,705] A3C_AGENT_WORKER-Thread-12 INFO:Local step 213500, global step 3417891: loss 0.7895
[2019-04-01 19:02:49,705] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 213500, global step 3417891: learning rate 0.0010
[2019-04-01 19:02:49,944] A3C_AGENT_WORKER-Thread-15 INFO:Local step 213500, global step 3417993: loss 0.5720
[2019-04-01 19:02:49,946] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 213500, global step 3417994: learning rate 0.0010
[2019-04-01 19:02:50,004] A3C_AGENT_WORKER-Thread-13 INFO:Local step 213500, global step 3418022: loss 0.4467
[2019-04-01 19:02:50,005] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 213500, global step 3418022: learning rate 0.0010
[2019-04-01 19:02:50,047] A3C_AGENT_WORKER-Thread-5 INFO:Local step 213500, global step 3418041: loss 0.0675
[2019-04-01 19:02:50,048] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 213500, global step 3418041: learning rate 0.0010
[2019-04-01 19:02:50,091] A3C_AGENT_WORKER-Thread-3 INFO:Local step 213500, global step 3418056: loss 0.6168
[2019-04-01 19:02:50,096] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 213500, global step 3418056: learning rate 0.0010
[2019-04-01 19:02:50,106] A3C_AGENT_WORKER-Thread-16 INFO:Local step 213500, global step 3418067: loss 0.1843
[2019-04-01 19:02:50,107] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 213500, global step 3418067: learning rate 0.0010
[2019-04-01 19:02:50,483] A3C_AGENT_WORKER-Thread-19 INFO:Local step 213500, global step 3418219: loss 0.2221
[2019-04-01 19:02:50,484] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 213500, global step 3418219: learning rate 0.0010
[2019-04-01 19:02:53,527] A3C_AGENT_WORKER-Thread-17 INFO:Local step 214500, global step 3419760: loss 0.2700
[2019-04-01 19:02:53,527] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 214500, global step 3419760: learning rate 0.0010
[2019-04-01 19:02:55,627] A3C_AGENT_WORKER-Thread-4 INFO:Local step 214000, global step 3420774: loss 0.7290
[2019-04-01 19:02:55,631] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 214000, global step 3420774: learning rate 0.0010
[2019-04-01 19:02:57,074] A3C_AGENT_WORKER-Thread-18 INFO:Local step 214500, global step 3421357: loss 0.0901
[2019-04-01 19:02:57,076] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 214500, global step 3421357: learning rate 0.0010
[2019-04-01 19:03:01,752] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:03:01,752] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7228
[2019-04-01 19:03:01,768] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.59975049990361, 6.255603528707762, 0.0, 1.0, 39.67202216467308], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 864000.0000, 
sim time next is 864600.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.58609093833697, 6.204894776248914, 0.0, 1.0, 39.54788306605298], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 1.0, 0.7980129911909957, 0.06204894776248914, 0.0, 1.0, 0.2824848790432356], 
reward next is 0.7175, 
noisyNet noise sample is [array([-1.1115168], dtype=float32), -0.57185215]. 
=============================================
[2019-04-01 19:03:05,465] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:03:05,467] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0799
[2019-04-01 19:03:05,479] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.44040549784899, 5.635682163221198, 0.0, 1.0, 38.35460789398723], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 885600.0000, 
sim time next is 886200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.43841134074798, 5.607310259934581, 0.0, 1.0, 38.27214364554366], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 1.0, 0.7769159058211399, 0.056073102599345816, 0.0, 1.0, 0.2733724546110261], 
reward next is 0.7266, 
noisyNet noise sample is [array([0.09937347], dtype=float32), 0.96689785]. 
=============================================
[2019-04-01 19:03:06,230] A3C_AGENT_WORKER-Thread-20 INFO:Local step 214000, global step 3425206: loss 0.5882
[2019-04-01 19:03:06,231] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 214000, global step 3425206: learning rate 0.0010
[2019-04-01 19:03:06,366] A3C_AGENT_WORKER-Thread-4 INFO:Local step 214500, global step 3425262: loss 0.0031
[2019-04-01 19:03:06,366] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 214500, global step 3425262: learning rate 0.0010
[2019-04-01 19:03:06,807] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.5697294e-25
 1.0000000e+00 2.1942445e-10], sum to 1.0000
[2019-04-01 19:03:06,808] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2375
[2019-04-01 19:03:06,828] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.9, 98.66666666666667, 0.0, 0.0, 26.0, 24.56755646060164, 9.20610292322129, 0.0, 1.0, 19.56179504673917], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1272000.0000, 
sim time next is 1272600.0000, 
raw observation next is [10.25, 98.0, 0.0, 0.0, 26.0, 24.56732436389722, 9.276818357201629, 0.0, 1.0, 22.42189635417521], 
processed observation next is [0.0, 0.7391304347826086, 0.7465373961218837, 0.98, 0.0, 0.0, 1.0, 0.7953320519853173, 0.09276818357201629, 0.0, 1.0, 0.16015640252982294], 
reward next is 0.8398, 
noisyNet noise sample is [array([0.03208034], dtype=float32), 1.0501106]. 
=============================================
[2019-04-01 19:03:07,285] A3C_AGENT_WORKER-Thread-2 INFO:Local step 214000, global step 3425679: loss 1.2876
[2019-04-01 19:03:07,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 214000, global step 3425682: learning rate 0.0010
[2019-04-01 19:03:07,679] A3C_AGENT_WORKER-Thread-11 INFO:Local step 214000, global step 3425861: loss 0.5313
[2019-04-01 19:03:07,680] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 214000, global step 3425861: learning rate 0.0010
[2019-04-01 19:03:07,875] A3C_AGENT_WORKER-Thread-14 INFO:Local step 214000, global step 3425960: loss 2.0057
[2019-04-01 19:03:07,876] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 214000, global step 3425961: learning rate 0.0010
[2019-04-01 19:03:08,667] A3C_AGENT_WORKER-Thread-10 INFO:Local step 214000, global step 3426395: loss 0.0512
[2019-04-01 19:03:08,679] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 214000, global step 3426403: learning rate 0.0010
[2019-04-01 19:03:08,723] A3C_AGENT_WORKER-Thread-17 INFO:Local step 215000, global step 3426431: loss 0.9076
[2019-04-01 19:03:08,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 215000, global step 3426431: learning rate 0.0010
[2019-04-01 19:03:08,839] A3C_AGENT_WORKER-Thread-12 INFO:Local step 214000, global step 3426504: loss 0.1352
[2019-04-01 19:03:08,843] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 214000, global step 3426505: learning rate 0.0010
[2019-04-01 19:03:09,032] A3C_AGENT_WORKER-Thread-13 INFO:Local step 214000, global step 3426615: loss 0.0046
[2019-04-01 19:03:09,034] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 214000, global step 3426617: learning rate 0.0010
[2019-04-01 19:03:09,169] A3C_AGENT_WORKER-Thread-9 INFO:Local step 214000, global step 3426707: loss 1.0410
[2019-04-01 19:03:09,170] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 214000, global step 3426707: learning rate 0.0010
[2019-04-01 19:03:09,385] A3C_AGENT_WORKER-Thread-5 INFO:Local step 214000, global step 3426848: loss 0.0756
[2019-04-01 19:03:09,387] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 214000, global step 3426851: learning rate 0.0010
[2019-04-01 19:03:09,494] A3C_AGENT_WORKER-Thread-15 INFO:Local step 214000, global step 3426916: loss 0.0042
[2019-04-01 19:03:09,501] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 214000, global step 3426916: learning rate 0.0010
[2019-04-01 19:03:09,508] A3C_AGENT_WORKER-Thread-3 INFO:Local step 214000, global step 3426924: loss 0.4465
[2019-04-01 19:03:09,509] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 214000, global step 3426924: learning rate 0.0010
[2019-04-01 19:03:10,019] A3C_AGENT_WORKER-Thread-16 INFO:Local step 214000, global step 3427208: loss 0.6511
[2019-04-01 19:03:10,020] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 214000, global step 3427208: learning rate 0.0010
[2019-04-01 19:03:10,039] A3C_AGENT_WORKER-Thread-19 INFO:Local step 214000, global step 3427215: loss 1.1818
[2019-04-01 19:03:10,041] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 214000, global step 3427217: learning rate 0.0010
[2019-04-01 19:03:12,772] A3C_AGENT_WORKER-Thread-18 INFO:Local step 215000, global step 3428953: loss 2.5242
[2019-04-01 19:03:12,776] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 215000, global step 3428953: learning rate 0.0010
[2019-04-01 19:03:15,682] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:03:15,684] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6684
[2019-04-01 19:03:15,696] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.73333333333333, 77.66666666666666, 0.0, 0.0, 26.0, 25.67465800300832, 13.83660423318999, 0.0, 1.0, 21.27492066674634], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1132800.0000, 
sim time next is 1133400.0000, 
raw observation next is [10.91666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.68627483647292, 13.72878905454883, 0.0, 1.0, 19.76087207527646], 
processed observation next is [0.0, 0.08695652173913043, 0.765004616805171, 0.7733333333333334, 0.0, 0.0, 1.0, 0.9551821194961312, 0.1372878905454883, 0.0, 1.0, 0.14114908625197473], 
reward next is 0.8589, 
noisyNet noise sample is [array([-0.6941937], dtype=float32), 0.23217268]. 
=============================================
[2019-04-01 19:03:17,065] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:03:17,066] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2948
[2019-04-01 19:03:17,076] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.44747483437341, 7.824497335188449, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1203000.0000, 
sim time next is 1203600.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 26.0, 24.41353861935771, 7.738634324432826, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 1.0, 0.7733626599082442, 0.07738634324432826, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19734791], dtype=float32), 0.79831046]. 
=============================================
[2019-04-01 19:03:17,380] A3C_AGENT_WORKER-Thread-20 INFO:Local step 214500, global step 3432047: loss 1.8309
[2019-04-01 19:03:17,381] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 214500, global step 3432048: learning rate 0.0010
[2019-04-01 19:03:18,579] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.8134184e-26
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:03:18,580] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7526
[2019-04-01 19:03:18,598] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.27591037965324, 9.136542965093303, 0.0, 1.0, 36.50395402091247], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1489200.0000, 
sim time next is 1489800.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.27825234116218, 9.204959163021554, 0.0, 1.0, 36.62273542373693], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 1.0, 0.8968931915945971, 0.09204959163021555, 0.0, 1.0, 0.2615909673124066], 
reward next is 0.7384, 
noisyNet noise sample is [array([-0.44312808], dtype=float32), -0.41507444]. 
=============================================
[2019-04-01 19:03:18,642] A3C_AGENT_WORKER-Thread-2 INFO:Local step 214500, global step 3432886: loss 1.2253
[2019-04-01 19:03:18,648] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 214500, global step 3432886: learning rate 0.0010
[2019-04-01 19:03:19,051] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:03:19,054] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8719
[2019-04-01 19:03:19,060] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.51666666666667, 75.5, 0.0, 0.0, 26.0, 24.31618734062176, 7.474404836526752, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1206600.0000, 
sim time next is 1207200.0000, 
raw observation next is [16.43333333333334, 76.0, 0.0, 0.0, 26.0, 24.29479995785032, 7.433021629887207, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 1.0, 0.9178208679593725, 0.76, 0.0, 0.0, 1.0, 0.756399993978617, 0.07433021629887207, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.342394], dtype=float32), 0.43450865]. 
=============================================
[2019-04-01 19:03:19,323] A3C_AGENT_WORKER-Thread-11 INFO:Local step 214500, global step 3433348: loss 1.5364
[2019-04-01 19:03:19,325] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 214500, global step 3433348: learning rate 0.0010
[2019-04-01 19:03:19,560] A3C_AGENT_WORKER-Thread-14 INFO:Local step 214500, global step 3433507: loss 1.5316
[2019-04-01 19:03:19,564] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 214500, global step 3433508: learning rate 0.0010
[2019-04-01 19:03:20,373] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 7.2239744e-38], sum to 1.0000
[2019-04-01 19:03:20,375] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9474
[2019-04-01 19:03:20,391] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.283333333333333, 96.0, 0.0, 0.0, 26.0, 25.0226402809756, 11.74272706783869, 0.0, 1.0, 40.8398860112748], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1281000.0000, 
sim time next is 1281600.0000, 
raw observation next is [6.1, 96.0, 0.0, 0.0, 26.0, 25.16038911381345, 12.12967307444453, 0.0, 1.0, 40.23084065837494], 
processed observation next is [0.0, 0.8695652173913043, 0.6315789473684211, 0.96, 0.0, 0.0, 1.0, 0.8800555876876359, 0.12129673074444529, 0.0, 1.0, 0.287363147559821], 
reward next is 0.7126, 
noisyNet noise sample is [array([-0.3721701], dtype=float32), -0.2558344]. 
=============================================
[2019-04-01 19:03:20,414] A3C_AGENT_WORKER-Thread-10 INFO:Local step 214500, global step 3434053: loss 0.5996
[2019-04-01 19:03:20,416] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 214500, global step 3434053: learning rate 0.0010
[2019-04-01 19:03:20,533] A3C_AGENT_WORKER-Thread-12 INFO:Local step 214500, global step 3434118: loss 1.0161
[2019-04-01 19:03:20,535] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 214500, global step 3434118: learning rate 0.0010
[2019-04-01 19:03:20,895] A3C_AGENT_WORKER-Thread-13 INFO:Local step 214500, global step 3434328: loss 0.7391
[2019-04-01 19:03:20,897] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 214500, global step 3434329: learning rate 0.0010
[2019-04-01 19:03:20,902] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.6809562e-31
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:03:20,904] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4836
[2019-04-01 19:03:20,924] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 96.66666666666666, 97.0, 0.0, 26.0, 25.04352851623817, 10.2460313128509, 0.0, 1.0, 15.75988638472516], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1252200.0000, 
sim time next is 1252800.0000, 
raw observation next is [14.4, 96.0, 98.0, 0.0, 26.0, 25.04598605609031, 10.24431461988767, 0.0, 1.0, 14.73122420976702], 
processed observation next is [0.0, 0.5217391304347826, 0.8614958448753465, 0.96, 0.32666666666666666, 0.0, 1.0, 0.8637122937271872, 0.10244314619887669, 0.0, 1.0, 0.10522303006976444], 
reward next is 0.8948, 
noisyNet noise sample is [array([0.578287], dtype=float32), -0.105192415]. 
=============================================
[2019-04-01 19:03:21,036] A3C_AGENT_WORKER-Thread-9 INFO:Local step 214500, global step 3434406: loss 1.6445
[2019-04-01 19:03:21,036] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 214500, global step 3434406: learning rate 0.0010
[2019-04-01 19:03:21,196] A3C_AGENT_WORKER-Thread-5 INFO:Local step 214500, global step 3434498: loss 1.6096
[2019-04-01 19:03:21,200] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 214500, global step 3434498: learning rate 0.0010
[2019-04-01 19:03:21,269] A3C_AGENT_WORKER-Thread-3 INFO:Local step 214500, global step 3434539: loss 1.5530
[2019-04-01 19:03:21,271] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 214500, global step 3434539: learning rate 0.0010
[2019-04-01 19:03:21,349] A3C_AGENT_WORKER-Thread-15 INFO:Local step 214500, global step 3434581: loss 1.0044
[2019-04-01 19:03:21,352] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 214500, global step 3434581: learning rate 0.0010
[2019-04-01 19:03:21,761] A3C_AGENT_WORKER-Thread-16 INFO:Local step 214500, global step 3434817: loss 1.7977
[2019-04-01 19:03:21,763] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 214500, global step 3434820: learning rate 0.0010
[2019-04-01 19:03:21,812] A3C_AGENT_WORKER-Thread-4 INFO:Local step 215000, global step 3434844: loss 1.9707
[2019-04-01 19:03:21,813] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 215000, global step 3434844: learning rate 0.0010
[2019-04-01 19:03:21,846] A3C_AGENT_WORKER-Thread-19 INFO:Local step 214500, global step 3434866: loss 0.8643
[2019-04-01 19:03:21,850] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 214500, global step 3434868: learning rate 0.0010
[2019-04-01 19:03:26,249] A3C_AGENT_WORKER-Thread-17 INFO:Local step 215500, global step 3437095: loss 3.6669
[2019-04-01 19:03:26,251] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 215500, global step 3437095: learning rate 0.0010
[2019-04-01 19:03:30,947] A3C_AGENT_WORKER-Thread-18 INFO:Local step 215500, global step 3439187: loss 0.8843
[2019-04-01 19:03:30,950] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 215500, global step 3439187: learning rate 0.0010
[2019-04-01 19:03:31,818] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.8559462e-33 0.0000000e+00 0.0000000e+00 9.9602461e-01 3.4529838e-36
 3.9754123e-03 0.0000000e+00], sum to 1.0000
[2019-04-01 19:03:31,820] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0509
[2019-04-01 19:03:31,840] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 24.94346416503237, 8.684474217390083, 1.0, 1.0, 20.70142991877894], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1454400.0000, 
sim time next is 1455000.0000, 
raw observation next is [1.183333333333333, 91.5, 0.0, 0.0, 26.0, 24.85067865290866, 8.618597545963034, 0.0, 1.0, 24.92665719663259], 
processed observation next is [1.0, 0.8695652173913043, 0.49538319482917825, 0.915, 0.0, 0.0, 1.0, 0.8358112361298085, 0.08618597545963035, 0.0, 1.0, 0.1780475514045185], 
reward next is 0.8220, 
noisyNet noise sample is [array([-0.51451325], dtype=float32), -1.1382667]. 
=============================================
[2019-04-01 19:03:31,843] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[61.541283]
 [63.173557]
 [64.92553 ]
 [61.602654]
 [57.08369 ]], R is [[64.62453461]
 [64.83042145]
 [65.07437134]
 [65.31062317]
 [65.53916168]].
[2019-04-01 19:03:32,485] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.1845548e-19 0.0000000e+00
 1.0000000e+00 3.4911011e-38], sum to 1.0000
[2019-04-01 19:03:32,488] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3146
[2019-04-01 19:03:32,503] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.48335595703506, 10.38990550106635, 0.0, 1.0, 36.02372459195247], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1471200.0000, 
sim time next is 1471800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.53295808380548, 10.62398348219398, 0.0, 1.0, 30.06117542142653], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 1.0, 0.9332797262579255, 0.1062398348219398, 0.0, 1.0, 0.21472268158161806], 
reward next is 0.7853, 
noisyNet noise sample is [array([-0.22793312], dtype=float32), -0.58483684]. 
=============================================
[2019-04-01 19:03:32,834] A3C_AGENT_WORKER-Thread-20 INFO:Local step 215000, global step 3440129: loss 2.5198
[2019-04-01 19:03:32,837] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 215000, global step 3440129: learning rate 0.0010
[2019-04-01 19:03:33,476] A3C_AGENT_WORKER-Thread-2 INFO:Local step 215000, global step 3440480: loss 0.0262
[2019-04-01 19:03:33,476] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 215000, global step 3440480: learning rate 0.0010
[2019-04-01 19:03:34,622] A3C_AGENT_WORKER-Thread-11 INFO:Local step 215000, global step 3441119: loss 0.0497
[2019-04-01 19:03:34,623] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 215000, global step 3441119: learning rate 0.0010
[2019-04-01 19:03:34,792] A3C_AGENT_WORKER-Thread-14 INFO:Local step 215000, global step 3441229: loss 0.0130
[2019-04-01 19:03:34,793] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 215000, global step 3441229: learning rate 0.0010
[2019-04-01 19:03:35,417] A3C_AGENT_WORKER-Thread-12 INFO:Local step 215000, global step 3441639: loss 1.8096
[2019-04-01 19:03:35,419] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 215000, global step 3441640: learning rate 0.0010
[2019-04-01 19:03:35,590] A3C_AGENT_WORKER-Thread-10 INFO:Local step 215000, global step 3441750: loss 0.7864
[2019-04-01 19:03:35,592] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 215000, global step 3441750: learning rate 0.0010
[2019-04-01 19:03:36,000] A3C_AGENT_WORKER-Thread-9 INFO:Local step 215000, global step 3441990: loss 0.4952
[2019-04-01 19:03:36,001] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 215000, global step 3441991: learning rate 0.0010
[2019-04-01 19:03:36,088] A3C_AGENT_WORKER-Thread-13 INFO:Local step 215000, global step 3442049: loss 1.8495
[2019-04-01 19:03:36,089] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 215000, global step 3442050: learning rate 0.0010
[2019-04-01 19:03:36,365] A3C_AGENT_WORKER-Thread-3 INFO:Local step 215000, global step 3442215: loss 1.2816
[2019-04-01 19:03:36,369] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 215000, global step 3442217: learning rate 0.0010
[2019-04-01 19:03:36,495] A3C_AGENT_WORKER-Thread-15 INFO:Local step 215000, global step 3442286: loss 0.9156
[2019-04-01 19:03:36,497] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 215000, global step 3442286: learning rate 0.0010
[2019-04-01 19:03:36,593] A3C_AGENT_WORKER-Thread-5 INFO:Local step 215000, global step 3442339: loss 0.1786
[2019-04-01 19:03:36,594] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 215000, global step 3442339: learning rate 0.0010
[2019-04-01 19:03:36,981] A3C_AGENT_WORKER-Thread-16 INFO:Local step 215000, global step 3442560: loss 0.0903
[2019-04-01 19:03:36,983] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 215000, global step 3442560: learning rate 0.0010
[2019-04-01 19:03:37,358] A3C_AGENT_WORKER-Thread-19 INFO:Local step 215000, global step 3442757: loss 0.3178
[2019-04-01 19:03:37,360] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 215000, global step 3442757: learning rate 0.0010
[2019-04-01 19:03:38,878] A3C_AGENT_WORKER-Thread-4 INFO:Local step 215500, global step 3443573: loss 4.2750
[2019-04-01 19:03:38,880] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 215500, global step 3443574: learning rate 0.0010
[2019-04-01 19:03:48,421] A3C_AGENT_WORKER-Thread-17 INFO:Local step 216000, global step 3447617: loss 0.0876
[2019-04-01 19:03:48,431] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 216000, global step 3447617: learning rate 0.0010
[2019-04-01 19:03:50,223] A3C_AGENT_WORKER-Thread-20 INFO:Local step 215500, global step 3448270: loss 2.1996
[2019-04-01 19:03:50,224] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 215500, global step 3448270: learning rate 0.0010
[2019-04-01 19:03:51,226] A3C_AGENT_WORKER-Thread-2 INFO:Local step 215500, global step 3448690: loss 1.0618
[2019-04-01 19:03:51,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 215500, global step 3448691: learning rate 0.0010
[2019-04-01 19:03:52,093] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 5.4428168e-32 2.9810498e-34 1.0000000e+00
 2.5035204e-15 1.3665898e-27], sum to 1.0000
[2019-04-01 19:03:52,093] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8215
[2019-04-01 19:03:52,113] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 82.66666666666667, 0.0, 0.0, 26.0, 24.69390471523669, 6.511357036658161, 0.0, 1.0, 44.99681273534938], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1810200.0000, 
sim time next is 1810800.0000, 
raw observation next is [-5.0, 82.0, 0.0, 0.0, 26.0, 24.66142428125973, 6.438610132622292, 0.0, 1.0, 44.94001009478541], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.82, 0.0, 0.0, 1.0, 0.8087748973228186, 0.06438610132622292, 0.0, 1.0, 0.32100007210561005], 
reward next is 0.6790, 
noisyNet noise sample is [array([-0.27410284], dtype=float32), -0.43345475]. 
=============================================
[2019-04-01 19:03:52,230] A3C_AGENT_WORKER-Thread-11 INFO:Local step 215500, global step 3449068: loss 1.0044
[2019-04-01 19:03:52,230] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 215500, global step 3449068: learning rate 0.0010
[2019-04-01 19:03:52,308] A3C_AGENT_WORKER-Thread-14 INFO:Local step 215500, global step 3449091: loss 0.3897
[2019-04-01 19:03:52,309] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 215500, global step 3449091: learning rate 0.0010
[2019-04-01 19:03:53,374] A3C_AGENT_WORKER-Thread-18 INFO:Local step 216000, global step 3449411: loss 0.0139
[2019-04-01 19:03:53,375] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 216000, global step 3449411: learning rate 0.0010
[2019-04-01 19:03:53,380] A3C_AGENT_WORKER-Thread-10 INFO:Local step 215500, global step 3449415: loss 1.0151
[2019-04-01 19:03:53,381] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 215500, global step 3449415: learning rate 0.0010
[2019-04-01 19:03:53,751] A3C_AGENT_WORKER-Thread-13 INFO:Local step 215500, global step 3449524: loss 0.7578
[2019-04-01 19:03:53,753] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 215500, global step 3449525: learning rate 0.0010
[2019-04-01 19:03:53,887] A3C_AGENT_WORKER-Thread-15 INFO:Local step 215500, global step 3449562: loss 0.8589
[2019-04-01 19:03:53,888] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 215500, global step 3449562: learning rate 0.0010
[2019-04-01 19:03:53,904] A3C_AGENT_WORKER-Thread-9 INFO:Local step 215500, global step 3449567: loss 0.6619
[2019-04-01 19:03:53,905] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 215500, global step 3449567: learning rate 0.0010
[2019-04-01 19:03:54,001] A3C_AGENT_WORKER-Thread-12 INFO:Local step 215500, global step 3449600: loss 1.3577
[2019-04-01 19:03:54,002] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 215500, global step 3449600: learning rate 0.0010
[2019-04-01 19:03:54,121] A3C_AGENT_WORKER-Thread-3 INFO:Local step 215500, global step 3449634: loss 1.0153
[2019-04-01 19:03:54,121] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 215500, global step 3449634: learning rate 0.0010
[2019-04-01 19:03:54,773] A3C_AGENT_WORKER-Thread-16 INFO:Local step 215500, global step 3449827: loss 0.4726
[2019-04-01 19:03:54,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 215500, global step 3449827: learning rate 0.0010
[2019-04-01 19:03:55,310] A3C_AGENT_WORKER-Thread-5 INFO:Local step 215500, global step 3450001: loss 0.6478
[2019-04-01 19:03:55,311] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 215500, global step 3450001: learning rate 0.0010
[2019-04-01 19:03:55,361] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:03:55,361] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4031
[2019-04-01 19:03:55,409] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 81.66666666666667, 19.66666666666667, 0.0, 26.0, 25.14961928630906, 7.665976084534471, 0.0, 1.0, 33.90499742321472], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1875000.0000, 
sim time next is 1875600.0000, 
raw observation next is [-4.5, 83.0, 15.0, 0.0, 26.0, 25.13106740912202, 7.529826652535831, 0.0, 1.0, 29.8933219076189], 
processed observation next is [0.0, 0.7391304347826086, 0.3379501385041552, 0.83, 0.05, 0.0, 1.0, 0.8758667727317173, 0.0752982665253583, 0.0, 1.0, 0.21352372791156357], 
reward next is 0.7865, 
noisyNet noise sample is [array([-0.3296127], dtype=float32), 0.4088218]. 
=============================================
[2019-04-01 19:03:55,663] A3C_AGENT_WORKER-Thread-19 INFO:Local step 215500, global step 3450131: loss 0.7311
[2019-04-01 19:03:55,665] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 215500, global step 3450131: learning rate 0.0010
[2019-04-01 19:03:55,723] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:03:55,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2562
[2019-04-01 19:03:55,737] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.466666666666667, 83.33333333333334, 0.0, 0.0, 26.0, 24.14691487493069, 5.712856932730159, 0.0, 1.0, 43.98999343141673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2254800.0000, 
sim time next is 2255400.0000, 
raw observation next is [-7.55, 84.0, 0.0, 0.0, 26.0, 24.26327809517905, 5.710792288496681, 0.0, 1.0, 42.93794498391501], 
processed observation next is [1.0, 0.08695652173913043, 0.25346260387811637, 0.84, 0.0, 0.0, 1.0, 0.7518968707398643, 0.057107922884966814, 0.0, 1.0, 0.30669960702796434], 
reward next is 0.6933, 
noisyNet noise sample is [array([0.4835501], dtype=float32), -0.05957046]. 
=============================================
[2019-04-01 19:04:00,135] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0379830e-24 0.0000000e+00 4.0387052e-29 7.7797313e-14 7.2266060e-12
 1.0000000e+00 1.2557597e-31], sum to 1.0000
[2019-04-01 19:04:00,135] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5895
[2019-04-01 19:04:00,150] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.45, 76.5, 0.0, 0.0, 26.0, 24.59350151926674, 6.476260086082934, 0.0, 1.0, 43.51775306576614], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2244600.0000, 
sim time next is 2245200.0000, 
raw observation next is [-6.533333333333333, 77.0, 0.0, 0.0, 26.0, 24.54072380980858, 6.369249234252382, 0.0, 1.0, 43.55307784294936], 
processed observation next is [1.0, 1.0, 0.2816251154201293, 0.77, 0.0, 0.0, 1.0, 0.791531972829797, 0.06369249234252382, 0.0, 1.0, 0.31109341316392397], 
reward next is 0.6889, 
noisyNet noise sample is [array([1.7234885], dtype=float32), 0.5486383]. 
=============================================
[2019-04-01 19:04:01,998] A3C_AGENT_WORKER-Thread-4 INFO:Local step 216000, global step 3452455: loss 0.3503
[2019-04-01 19:04:01,999] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 216000, global step 3452455: learning rate 0.0010
[2019-04-01 19:04:10,282] A3C_AGENT_WORKER-Thread-17 INFO:Local step 216500, global step 3455320: loss 0.0283
[2019-04-01 19:04:10,284] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 216500, global step 3455320: learning rate 0.0010
[2019-04-01 19:04:13,685] A3C_AGENT_WORKER-Thread-2 INFO:Local step 216000, global step 3456602: loss 0.1178
[2019-04-01 19:04:13,686] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 216000, global step 3456602: learning rate 0.0010
[2019-04-01 19:04:13,848] A3C_AGENT_WORKER-Thread-20 INFO:Local step 216000, global step 3456652: loss 0.0067
[2019-04-01 19:04:13,859] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 216000, global step 3456654: learning rate 0.0010
[2019-04-01 19:04:14,846] A3C_AGENT_WORKER-Thread-11 INFO:Local step 216000, global step 3456979: loss 0.0028
[2019-04-01 19:04:14,847] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 216000, global step 3456979: learning rate 0.0010
[2019-04-01 19:04:15,009] A3C_AGENT_WORKER-Thread-18 INFO:Local step 216500, global step 3457039: loss 2.5544
[2019-04-01 19:04:15,009] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 216500, global step 3457039: learning rate 0.0010
[2019-04-01 19:04:15,536] A3C_AGENT_WORKER-Thread-14 INFO:Local step 216000, global step 3457224: loss 0.1014
[2019-04-01 19:04:15,538] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 216000, global step 3457224: learning rate 0.0010
[2019-04-01 19:04:15,703] A3C_AGENT_WORKER-Thread-10 INFO:Local step 216000, global step 3457288: loss 0.0006
[2019-04-01 19:04:15,704] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 216000, global step 3457288: learning rate 0.0010
[2019-04-01 19:04:16,374] A3C_AGENT_WORKER-Thread-3 INFO:Local step 216000, global step 3457591: loss 0.0396
[2019-04-01 19:04:16,374] A3C_AGENT_WORKER-Thread-9 INFO:Local step 216000, global step 3457591: loss 0.0017
[2019-04-01 19:04:16,374] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 216000, global step 3457591: learning rate 0.0010
[2019-04-01 19:04:16,374] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 216000, global step 3457591: learning rate 0.0010
[2019-04-01 19:04:16,549] A3C_AGENT_WORKER-Thread-15 INFO:Local step 216000, global step 3457656: loss 0.0149
[2019-04-01 19:04:16,550] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 216000, global step 3457656: learning rate 0.0010
[2019-04-01 19:04:16,863] A3C_AGENT_WORKER-Thread-12 INFO:Local step 216000, global step 3457778: loss 0.0283
[2019-04-01 19:04:16,864] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 216000, global step 3457778: learning rate 0.0010
[2019-04-01 19:04:17,208] A3C_AGENT_WORKER-Thread-13 INFO:Local step 216000, global step 3457916: loss 0.0091
[2019-04-01 19:04:17,209] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 216000, global step 3457916: learning rate 0.0010
[2019-04-01 19:04:17,956] A3C_AGENT_WORKER-Thread-5 INFO:Local step 216000, global step 3458211: loss 0.0981
[2019-04-01 19:04:17,957] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 216000, global step 3458211: learning rate 0.0010
[2019-04-01 19:04:18,052] A3C_AGENT_WORKER-Thread-16 INFO:Local step 216000, global step 3458248: loss 0.0206
[2019-04-01 19:04:18,053] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 216000, global step 3458248: learning rate 0.0010
[2019-04-01 19:04:19,051] A3C_AGENT_WORKER-Thread-19 INFO:Local step 216000, global step 3458622: loss 0.0029
[2019-04-01 19:04:19,052] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 216000, global step 3458622: learning rate 0.0010
[2019-04-01 19:04:22,742] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 6.9626303e-34 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:04:22,742] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5075
[2019-04-01 19:04:22,814] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 61.00000000000001, 0.0, 0.0, 26.0, 25.11739141842262, 8.449018662471063, 0.0, 1.0, 28.73575960902699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2661600.0000, 
sim time next is 2662200.0000, 
raw observation next is [-1.2, 61.5, 0.0, 0.0, 26.0, 25.06621949902886, 8.213378007256948, 0.0, 1.0, 28.18140072153223], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.615, 0.0, 0.0, 1.0, 0.8666027855755516, 0.08213378007256948, 0.0, 1.0, 0.20129571943951594], 
reward next is 0.7987, 
noisyNet noise sample is [array([0.14746043], dtype=float32), -0.01561548]. 
=============================================
[2019-04-01 19:04:24,081] A3C_AGENT_WORKER-Thread-4 INFO:Local step 216500, global step 3460415: loss 4.1242
[2019-04-01 19:04:24,083] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 216500, global step 3460415: learning rate 0.0010
[2019-04-01 19:04:26,929] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1144307e-35 2.0182912e-38 0.0000000e+00 1.2255921e-27 5.6130020e-31
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:04:26,929] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3941
[2019-04-01 19:04:26,991] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8, 50.0, 11.0, 0.0, 26.0, 24.9385838874182, 7.733538857612037, 1.0, 1.0, 44.73725258778702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2308800.0000, 
sim time next is 2309400.0000, 
raw observation next is [-0.8999999999999999, 50.5, 0.0, 0.0, 26.0, 25.23925872104554, 8.262953127554008, 1.0, 1.0, 26.35926938582028], 
processed observation next is [1.0, 0.7391304347826086, 0.43767313019390586, 0.505, 0.0, 0.0, 1.0, 0.8913226744350773, 0.08262953127554008, 1.0, 1.0, 0.188280495613002], 
reward next is 0.8117, 
noisyNet noise sample is [array([0.72877324], dtype=float32), 0.21329701]. 
=============================================
[2019-04-01 19:04:29,365] A3C_AGENT_WORKER-Thread-17 INFO:Local step 217000, global step 3462459: loss 1.0104
[2019-04-01 19:04:29,366] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 217000, global step 3462459: learning rate 0.0010
[2019-04-01 19:04:34,303] A3C_AGENT_WORKER-Thread-18 INFO:Local step 217000, global step 3464329: loss 1.4047
[2019-04-01 19:04:34,305] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 217000, global step 3464332: learning rate 0.0010
[2019-04-01 19:04:34,605] A3C_AGENT_WORKER-Thread-2 INFO:Local step 216500, global step 3464453: loss 1.2810
[2019-04-01 19:04:34,605] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 216500, global step 3464453: learning rate 0.0010
[2019-04-01 19:04:35,376] A3C_AGENT_WORKER-Thread-20 INFO:Local step 216500, global step 3464798: loss 3.1151
[2019-04-01 19:04:35,378] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 216500, global step 3464798: learning rate 0.0010
[2019-04-01 19:04:35,803] A3C_AGENT_WORKER-Thread-11 INFO:Local step 216500, global step 3465021: loss 2.7831
[2019-04-01 19:04:35,804] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 216500, global step 3465021: learning rate 0.0010
[2019-04-01 19:04:37,019] A3C_AGENT_WORKER-Thread-14 INFO:Local step 216500, global step 3465509: loss 2.5929
[2019-04-01 19:04:37,026] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 216500, global step 3465509: learning rate 0.0010
[2019-04-01 19:04:37,219] A3C_AGENT_WORKER-Thread-10 INFO:Local step 216500, global step 3465595: loss 2.4151
[2019-04-01 19:04:37,220] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 216500, global step 3465596: learning rate 0.0010
[2019-04-01 19:04:37,237] A3C_AGENT_WORKER-Thread-9 INFO:Local step 216500, global step 3465605: loss 3.0527
[2019-04-01 19:04:37,237] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 216500, global step 3465605: learning rate 0.0010
[2019-04-01 19:04:37,543] A3C_AGENT_WORKER-Thread-15 INFO:Local step 216500, global step 3465734: loss 2.8898
[2019-04-01 19:04:37,546] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 216500, global step 3465735: learning rate 0.0010
[2019-04-01 19:04:37,718] A3C_AGENT_WORKER-Thread-3 INFO:Local step 216500, global step 3465818: loss 3.1595
[2019-04-01 19:04:37,719] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 216500, global step 3465818: learning rate 0.0010
[2019-04-01 19:04:38,432] A3C_AGENT_WORKER-Thread-12 INFO:Local step 216500, global step 3466132: loss 2.4895
[2019-04-01 19:04:38,432] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 216500, global step 3466132: learning rate 0.0010
[2019-04-01 19:04:38,583] A3C_AGENT_WORKER-Thread-13 INFO:Local step 216500, global step 3466198: loss 1.9871
[2019-04-01 19:04:38,584] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 216500, global step 3466198: learning rate 0.0010
[2019-04-01 19:04:38,914] A3C_AGENT_WORKER-Thread-16 INFO:Local step 216500, global step 3466326: loss 1.3467
[2019-04-01 19:04:38,915] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 216500, global step 3466326: learning rate 0.0010
[2019-04-01 19:04:39,672] A3C_AGENT_WORKER-Thread-5 INFO:Local step 216500, global step 3466655: loss 0.4218
[2019-04-01 19:04:39,673] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 216500, global step 3466655: learning rate 0.0010
[2019-04-01 19:04:40,207] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:04:40,212] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7807
[2019-04-01 19:04:40,252] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.85, 37.16666666666666, 79.33333333333333, 794.3333333333334, 26.0, 25.04876897575308, 7.085439747321831, 0.0, 1.0, 31.85687186996927], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2458200.0000, 
sim time next is 2458800.0000, 
raw observation next is [-2.3, 36.0, 81.0, 803.0, 26.0, 25.09154725526873, 7.124529296625958, 0.0, 1.0, 27.69960938838201], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.36, 0.27, 0.887292817679558, 1.0, 0.8702210364669613, 0.07124529296625959, 0.0, 1.0, 0.19785435277415722], 
reward next is 0.8021, 
noisyNet noise sample is [array([-3.9803782], dtype=float32), 1.2951437]. 
=============================================
[2019-04-01 19:04:40,362] A3C_AGENT_WORKER-Thread-19 INFO:Local step 216500, global step 3467001: loss 0.2248
[2019-04-01 19:04:40,364] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 216500, global step 3467002: learning rate 0.0010
[2019-04-01 19:04:42,644] A3C_AGENT_WORKER-Thread-4 INFO:Local step 217000, global step 3468142: loss 0.0618
[2019-04-01 19:04:42,644] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 217000, global step 3468142: learning rate 0.0010
[2019-04-01 19:04:44,712] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8350159e-04 5.0380338e-29
 9.9981648e-01 0.0000000e+00], sum to 1.0000
[2019-04-01 19:04:44,712] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4224
[2019-04-01 19:04:44,764] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.516666666666667, 42.66666666666667, 0.0, 0.0, 26.0, 24.98258680040857, 7.780334036630393, 0.0, 1.0, 36.07233080809777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2577000.0000, 
sim time next is 2577600.0000, 
raw observation next is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.9671757161984, 7.807907916374042, 0.0, 1.0, 37.62244297558836], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.44, 0.0, 0.0, 1.0, 0.8524536737426287, 0.07807907916374042, 0.0, 1.0, 0.26873173553991686], 
reward next is 0.7313, 
noisyNet noise sample is [array([1.1304214], dtype=float32), 0.51605177]. 
=============================================
[2019-04-01 19:04:47,653] A3C_AGENT_WORKER-Thread-17 INFO:Local step 217500, global step 3470360: loss 1.3385
[2019-04-01 19:04:47,654] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 217500, global step 3470360: learning rate 0.0010
[2019-04-01 19:04:52,005] A3C_AGENT_WORKER-Thread-18 INFO:Local step 217500, global step 3472204: loss 1.2046
[2019-04-01 19:04:52,006] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 217500, global step 3472204: learning rate 0.0010
[2019-04-01 19:04:52,976] A3C_AGENT_WORKER-Thread-2 INFO:Local step 217000, global step 3472581: loss 0.0601
[2019-04-01 19:04:52,977] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 217000, global step 3472582: learning rate 0.0010
[2019-04-01 19:04:53,834] A3C_AGENT_WORKER-Thread-20 INFO:Local step 217000, global step 3472944: loss 0.2012
[2019-04-01 19:04:53,835] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 217000, global step 3472945: learning rate 0.0010
[2019-04-01 19:04:54,041] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:04:54,045] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5898
[2019-04-01 19:04:54,068] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.29446111562628, 5.484742863894266, 0.0, 1.0, 40.73906973003211], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2785200.0000, 
sim time next is 2785800.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.2755202202641, 5.495294594486151, 0.0, 1.0, 40.73194234828862], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 1.0, 0.7536457457520142, 0.05495294594486151, 0.0, 1.0, 0.2909424453449187], 
reward next is 0.7091, 
noisyNet noise sample is [array([-1.3850695], dtype=float32), 1.5080537]. 
=============================================
[2019-04-01 19:04:54,439] A3C_AGENT_WORKER-Thread-11 INFO:Local step 217000, global step 3473207: loss 0.0685
[2019-04-01 19:04:54,439] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 217000, global step 3473207: learning rate 0.0010
[2019-04-01 19:04:55,123] A3C_AGENT_WORKER-Thread-14 INFO:Local step 217000, global step 3473485: loss 0.0193
[2019-04-01 19:04:55,130] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 217000, global step 3473486: learning rate 0.0010
[2019-04-01 19:04:55,363] A3C_AGENT_WORKER-Thread-12 INFO:Local step 217000, global step 3473590: loss 1.7636
[2019-04-01 19:04:55,364] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 217000, global step 3473590: learning rate 0.0010
[2019-04-01 19:04:55,426] A3C_AGENT_WORKER-Thread-10 INFO:Local step 217000, global step 3473617: loss 0.2183
[2019-04-01 19:04:55,427] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 217000, global step 3473617: learning rate 0.0010
[2019-04-01 19:04:55,664] A3C_AGENT_WORKER-Thread-15 INFO:Local step 217000, global step 3473728: loss 0.2611
[2019-04-01 19:04:55,666] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:04:55,667] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 217000, global step 3473731: learning rate 0.0010
[2019-04-01 19:04:55,668] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9791
[2019-04-01 19:04:55,683] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 47.0, 125.0, 763.0, 26.0, 26.08419196810914, 9.994770391145783, 1.0, 1.0, 20.53524174205574], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2806200.0000, 
sim time next is 2806800.0000, 
raw observation next is [0.9999999999999999, 46.0, 133.8333333333333, 750.0, 26.0, 26.03101940309207, 9.943989621759021, 1.0, 1.0, 20.19326131239026], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.46, 0.44611111111111096, 0.8287292817679558, 1.0, 1.0044313432988672, 0.09943989621759021, 1.0, 1.0, 0.14423758080278756], 
reward next is 0.8558, 
noisyNet noise sample is [array([-0.7161479], dtype=float32), -0.010213336]. 
=============================================
[2019-04-01 19:04:55,797] A3C_AGENT_WORKER-Thread-9 INFO:Local step 217000, global step 3473793: loss 0.6960
[2019-04-01 19:04:55,803] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 217000, global step 3473793: learning rate 0.0010
[2019-04-01 19:04:56,060] A3C_AGENT_WORKER-Thread-3 INFO:Local step 217000, global step 3473915: loss 0.7480
[2019-04-01 19:04:56,060] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 217000, global step 3473915: learning rate 0.0010
[2019-04-01 19:04:56,966] A3C_AGENT_WORKER-Thread-13 INFO:Local step 217000, global step 3474320: loss 0.2271
[2019-04-01 19:04:56,967] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 217000, global step 3474320: learning rate 0.0010
[2019-04-01 19:04:57,057] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:04:57,057] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6591
[2019-04-01 19:04:57,064] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.666666666666667, 95.33333333333334, 113.1666666666667, 820.0, 26.0, 26.98073338690067, 18.06603506318243, 1.0, 1.0, 4.170810135722771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3154800.0000, 
sim time next is 3155400.0000, 
raw observation next is [7.5, 96.5, 113.0, 823.0, 26.0, 26.99066255639843, 18.13000173129022, 1.0, 1.0, 3.772600497317435], 
processed observation next is [1.0, 0.5217391304347826, 0.6703601108033241, 0.965, 0.37666666666666665, 0.9093922651933701, 1.0, 1.1415232223426328, 0.1813000173129022, 1.0, 1.0, 0.02694714640941025], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.2074149], dtype=float32), 2.1261392]. 
=============================================
[2019-04-01 19:04:57,470] A3C_AGENT_WORKER-Thread-16 INFO:Local step 217000, global step 3474534: loss 1.2747
[2019-04-01 19:04:57,471] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 217000, global step 3474534: learning rate 0.0010
[2019-04-01 19:04:58,423] A3C_AGENT_WORKER-Thread-5 INFO:Local step 217000, global step 3474943: loss 3.5476
[2019-04-01 19:04:58,424] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 217000, global step 3474943: learning rate 0.0010
[2019-04-01 19:04:58,810] A3C_AGENT_WORKER-Thread-19 INFO:Local step 217000, global step 3475141: loss 3.5145
[2019-04-01 19:04:58,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 217000, global step 3475142: learning rate 0.0010
[2019-04-01 19:05:01,182] A3C_AGENT_WORKER-Thread-4 INFO:Local step 217500, global step 3476223: loss 0.8862
[2019-04-01 19:05:01,182] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 217500, global step 3476223: learning rate 0.0010
[2019-04-01 19:05:03,025] A3C_AGENT_WORKER-Thread-17 INFO:Local step 218000, global step 3477096: loss 13.7860
[2019-04-01 19:05:03,026] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 218000, global step 3477096: learning rate 0.0010
[2019-04-01 19:05:07,734] A3C_AGENT_WORKER-Thread-18 INFO:Local step 218000, global step 3479270: loss 0.1222
[2019-04-01 19:05:07,736] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 218000, global step 3479270: learning rate 0.0010
[2019-04-01 19:05:09,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.5110997e-35
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:05:09,192] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2944
[2019-04-01 19:05:09,242] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 206.0, 555.3333333333334, 26.0, 24.95867010392577, 8.497519100403096, 0.0, 1.0, 44.32289552843558], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2981400.0000, 
sim time next is 2982000.0000, 
raw observation next is [-3.0, 65.0, 193.5, 623.1666666666667, 26.0, 24.96425108824372, 8.876817250683155, 0.0, 1.0, 43.5727756752414], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.645, 0.6885819521178638, 1.0, 0.852035869749103, 0.08876817250683156, 0.0, 1.0, 0.31123411196601003], 
reward next is 0.6888, 
noisyNet noise sample is [array([1.3213942], dtype=float32), -0.29109094]. 
=============================================
[2019-04-01 19:05:09,244] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[48.48372 ]
 [49.028954]
 [49.749252]
 [50.524593]
 [51.263012]], R is [[48.25374603]
 [48.45461655]
 [48.74817276]
 [49.08065033]
 [49.39723587]].
[2019-04-01 19:05:10,945] A3C_AGENT_WORKER-Thread-2 INFO:Local step 217500, global step 3480605: loss 4.0447
[2019-04-01 19:05:10,954] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 217500, global step 3480606: learning rate 0.0010
[2019-04-01 19:05:11,681] A3C_AGENT_WORKER-Thread-20 INFO:Local step 217500, global step 3480961: loss 4.0934
[2019-04-01 19:05:11,683] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 217500, global step 3480962: learning rate 0.0010
[2019-04-01 19:05:11,836] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.0650793e-30], sum to 1.0000
[2019-04-01 19:05:11,837] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6772
[2019-04-01 19:05:11,887] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 24.99307838371599, 7.94460836931921, 0.0, 1.0, 35.64179579542429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3005400.0000, 
sim time next is 3006000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.0064431696559, 7.955083608107223, 0.0, 1.0, 34.90005300147496], 
processed observation next is [0.0, 0.8260869565217391, 0.40720221606648205, 0.6, 0.0, 0.0, 1.0, 0.8580633099508427, 0.07955083608107223, 0.0, 1.0, 0.24928609286767828], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.43063232], dtype=float32), -0.008339392]. 
=============================================
[2019-04-01 19:05:11,904] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[68.11047]
 [67.5795 ]
 [66.9558 ]
 [66.32669]
 [65.74846]], R is [[68.62281799]
 [68.68200684]
 [68.74165344]
 [68.80999756]
 [68.8950882 ]].
[2019-04-01 19:05:12,924] A3C_AGENT_WORKER-Thread-11 INFO:Local step 217500, global step 3481571: loss 0.6114
[2019-04-01 19:05:12,925] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 217500, global step 3481571: learning rate 0.0010
[2019-04-01 19:05:13,137] A3C_AGENT_WORKER-Thread-10 INFO:Local step 217500, global step 3481662: loss 0.0575
[2019-04-01 19:05:13,139] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 217500, global step 3481662: learning rate 0.0010
[2019-04-01 19:05:13,169] A3C_AGENT_WORKER-Thread-14 INFO:Local step 217500, global step 3481676: loss 0.2855
[2019-04-01 19:05:13,171] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 217500, global step 3481677: learning rate 0.0010
[2019-04-01 19:05:13,535] A3C_AGENT_WORKER-Thread-15 INFO:Local step 217500, global step 3481799: loss 1.3026
[2019-04-01 19:05:13,539] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 217500, global step 3481799: learning rate 0.0010
[2019-04-01 19:05:13,607] A3C_AGENT_WORKER-Thread-12 INFO:Local step 217500, global step 3481822: loss 0.8451
[2019-04-01 19:05:13,608] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 217500, global step 3481822: learning rate 0.0010
[2019-04-01 19:05:13,763] A3C_AGENT_WORKER-Thread-9 INFO:Local step 217500, global step 3481876: loss 0.0551
[2019-04-01 19:05:13,763] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 217500, global step 3481876: learning rate 0.0010
[2019-04-01 19:05:14,019] A3C_AGENT_WORKER-Thread-3 INFO:Local step 217500, global step 3481983: loss 1.4551
[2019-04-01 19:05:14,020] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 217500, global step 3481983: learning rate 0.0010
[2019-04-01 19:05:14,367] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:05:14,367] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6178
[2019-04-01 19:05:14,382] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93849717952379, 6.840935875396575, 0.0, 1.0, 37.5957813742539], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3022200.0000, 
sim time next is 3022800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.91115320250459, 6.747348056829307, 0.0, 1.0, 37.55532671274752], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 1.0, 0.8444504575006556, 0.06747348056829307, 0.0, 1.0, 0.2682523336624823], 
reward next is 0.7317, 
noisyNet noise sample is [array([0.40510613], dtype=float32), 0.12022501]. 
=============================================
[2019-04-01 19:05:14,880] A3C_AGENT_WORKER-Thread-13 INFO:Local step 217500, global step 3482381: loss 10.1170
[2019-04-01 19:05:14,881] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 217500, global step 3482381: learning rate 0.0010
[2019-04-01 19:05:15,622] A3C_AGENT_WORKER-Thread-16 INFO:Local step 217500, global step 3482778: loss 10.7340
[2019-04-01 19:05:15,625] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 217500, global step 3482778: learning rate 0.0010
[2019-04-01 19:05:16,351] A3C_AGENT_WORKER-Thread-4 INFO:Local step 218000, global step 3483181: loss 13.3673
[2019-04-01 19:05:16,352] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 218000, global step 3483181: learning rate 0.0010
[2019-04-01 19:05:16,515] A3C_AGENT_WORKER-Thread-5 INFO:Local step 217500, global step 3483281: loss 4.2430
[2019-04-01 19:05:16,517] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 217500, global step 3483281: learning rate 0.0010
[2019-04-01 19:05:16,716] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.9021485e-27 3.8414888e-29 5.8997126e-35 9.5972235e-09 6.3704870e-32
 1.0000000e+00 2.7077305e-19], sum to 1.0000
[2019-04-01 19:05:16,718] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0826
[2019-04-01 19:05:16,752] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 104.6666666666667, 777.3333333333334, 26.0, 27.30475219347213, 17.41858120951797, 1.0, 1.0, 1.910869536098324], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3161400.0000, 
sim time next is 3162000.0000, 
raw observation next is [7.0, 100.0, 102.8333333333333, 770.1666666666667, 26.0, 26.61658276423643, 17.59483612469215, 1.0, 1.0, 1.889833425291047], 
processed observation next is [1.0, 0.6086956521739131, 0.6565096952908588, 1.0, 0.3427777777777777, 0.8510128913443832, 1.0, 1.0880832520337758, 0.1759483612469215, 1.0, 1.0, 0.013498810180650335], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.38536808], dtype=float32), 0.82515717]. 
=============================================
[2019-04-01 19:05:16,762] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[50.48767 ]
 [50.33919 ]
 [50.305267]
 [50.285225]
 [50.27796 ]], R is [[49.95193863]
 [49.45241928]
 [48.95789719]
 [48.46831894]
 [47.98363495]].
[2019-04-01 19:05:16,945] A3C_AGENT_WORKER-Thread-19 INFO:Local step 217500, global step 3483533: loss 3.1697
[2019-04-01 19:05:16,947] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 217500, global step 3483533: learning rate 0.0010
[2019-04-01 19:05:17,649] A3C_AGENT_WORKER-Thread-17 INFO:Local step 218500, global step 3483969: loss 0.0003
[2019-04-01 19:05:17,653] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 218500, global step 3483969: learning rate 0.0010
[2019-04-01 19:05:21,976] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.4947165e-30 0.0000000e+00 1.8260051e-38 9.0310339e-18 8.2114147e-09
 1.0000000e+00 7.3899954e-20], sum to 1.0000
[2019-04-01 19:05:21,977] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1840
[2019-04-01 19:05:22,000] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 97.66666666666666, 0.0, 0.0, 26.0, 25.54414880924443, 12.22445850897092, 0.0, 1.0, 29.33316321682426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3199200.0000, 
sim time next is 3199800.0000, 
raw observation next is [1.166666666666667, 98.83333333333334, 0.0, 0.0, 26.0, 25.53161427118497, 12.45399466624885, 0.0, 1.0, 30.5152806191801], 
processed observation next is [1.0, 0.0, 0.49492151431209613, 0.9883333333333334, 0.0, 0.0, 1.0, 0.9330877530264244, 0.1245399466624885, 0.0, 1.0, 0.21796629013700072], 
reward next is 0.7820, 
noisyNet noise sample is [array([-0.542911], dtype=float32), -0.27448717]. 
=============================================
[2019-04-01 19:05:22,590] A3C_AGENT_WORKER-Thread-18 INFO:Local step 218500, global step 3486870: loss 4.1073
[2019-04-01 19:05:22,592] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 218500, global step 3486870: learning rate 0.0010
[2019-04-01 19:05:24,627] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.0229288e-33], sum to 1.0000
[2019-04-01 19:05:24,628] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8176
[2019-04-01 19:05:24,645] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.166666666666667, 71.0, 90.33333333333333, 713.6666666666666, 26.0, 26.63187138780437, 18.48938543832101, 1.0, 1.0, 10.5647587136309], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3251400.0000, 
sim time next is 3252000.0000, 
raw observation next is [-2.333333333333333, 71.0, 87.66666666666667, 699.8333333333334, 26.0, 26.73708805022786, 18.83961659724911, 1.0, 1.0, 10.05616140146289], 
processed observation next is [1.0, 0.6521739130434783, 0.3979686057248385, 0.71, 0.2922222222222222, 0.7732965009208104, 1.0, 1.1052982928896944, 0.1883961659724911, 1.0, 1.0, 0.07182972429616351], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3053886], dtype=float32), -0.6947979]. 
=============================================
[2019-04-01 19:05:24,658] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[50.09383 ]
 [49.722485]
 [49.28094 ]
 [49.066586]
 [48.735153]], R is [[49.89745331]
 [49.39847946]
 [48.90449524]
 [48.41545105]
 [47.9312973 ]].
[2019-04-01 19:05:26,653] A3C_AGENT_WORKER-Thread-2 INFO:Local step 218000, global step 3488759: loss 43.1440
[2019-04-01 19:05:26,655] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 218000, global step 3488759: learning rate 0.0010
[2019-04-01 19:05:26,948] A3C_AGENT_WORKER-Thread-20 INFO:Local step 218000, global step 3488883: loss 17.0547
[2019-04-01 19:05:26,950] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 218000, global step 3488883: learning rate 0.0010
[2019-04-01 19:05:27,069] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:05:27,070] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9901
[2019-04-01 19:05:27,104] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.75, 77.0, 0.0, 0.0, 26.0, 24.65705491193307, 6.91508376623316, 0.0, 1.0, 43.29947145738861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3297000.0000, 
sim time next is 3297600.0000, 
raw observation next is [-8.9, 77.0, 0.0, 0.0, 26.0, 24.70008637674698, 6.624455229245783, 0.0, 1.0, 43.96804688088656], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.77, 0.0, 0.0, 1.0, 0.8142980538209973, 0.06624455229245782, 0.0, 1.0, 0.3140574777206183], 
reward next is 0.6859, 
noisyNet noise sample is [array([0.57773834], dtype=float32), 0.62079316]. 
=============================================
[2019-04-01 19:05:28,202] A3C_AGENT_WORKER-Thread-11 INFO:Local step 218000, global step 3489504: loss 18.5843
[2019-04-01 19:05:28,203] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 218000, global step 3489506: learning rate 0.0010
[2019-04-01 19:05:28,330] A3C_AGENT_WORKER-Thread-10 INFO:Local step 218000, global step 3489569: loss 22.2796
[2019-04-01 19:05:28,331] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 218000, global step 3489569: learning rate 0.0010
[2019-04-01 19:05:28,588] A3C_AGENT_WORKER-Thread-14 INFO:Local step 218000, global step 3489698: loss 20.5184
[2019-04-01 19:05:28,606] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 218000, global step 3489709: learning rate 0.0010
[2019-04-01 19:05:28,984] A3C_AGENT_WORKER-Thread-15 INFO:Local step 218000, global step 3489896: loss 16.9759
[2019-04-01 19:05:28,984] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 218000, global step 3489896: learning rate 0.0010
[2019-04-01 19:05:29,569] A3C_AGENT_WORKER-Thread-9 INFO:Local step 218000, global step 3490200: loss 22.2100
[2019-04-01 19:05:29,570] A3C_AGENT_WORKER-Thread-3 INFO:Local step 218000, global step 3490202: loss 14.1260
[2019-04-01 19:05:29,571] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 218000, global step 3490203: learning rate 0.0010
[2019-04-01 19:05:29,575] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 218000, global step 3490203: learning rate 0.0010
[2019-04-01 19:05:29,808] A3C_AGENT_WORKER-Thread-13 INFO:Local step 218000, global step 3490338: loss 126.6066
[2019-04-01 19:05:29,808] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 218000, global step 3490338: learning rate 0.0010
[2019-04-01 19:05:29,911] A3C_AGENT_WORKER-Thread-12 INFO:Local step 218000, global step 3490390: loss 15.0694
[2019-04-01 19:05:29,912] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 218000, global step 3490390: learning rate 0.0010
[2019-04-01 19:05:30,902] A3C_AGENT_WORKER-Thread-16 INFO:Local step 218000, global step 3490878: loss 15.7003
[2019-04-01 19:05:30,906] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 218000, global step 3490879: learning rate 0.0010
[2019-04-01 19:05:31,412] A3C_AGENT_WORKER-Thread-4 INFO:Local step 218500, global step 3491143: loss 6.9818
[2019-04-01 19:05:31,414] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 218500, global step 3491143: learning rate 0.0010
[2019-04-01 19:05:32,198] A3C_AGENT_WORKER-Thread-17 INFO:Local step 219000, global step 3491559: loss 15.4910
[2019-04-01 19:05:32,198] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 219000, global step 3491559: learning rate 0.0010
[2019-04-01 19:05:32,385] A3C_AGENT_WORKER-Thread-5 INFO:Local step 218000, global step 3491656: loss 6.9485
[2019-04-01 19:05:32,387] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 218000, global step 3491656: learning rate 0.0010
[2019-04-01 19:05:32,667] A3C_AGENT_WORKER-Thread-19 INFO:Local step 218000, global step 3491817: loss 19.0973
[2019-04-01 19:05:32,669] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 218000, global step 3491817: learning rate 0.0010
[2019-04-01 19:05:37,333] A3C_AGENT_WORKER-Thread-18 INFO:Local step 219000, global step 3494382: loss 23.5479
[2019-04-01 19:05:37,334] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 219000, global step 3494384: learning rate 0.0010
[2019-04-01 19:05:41,304] A3C_AGENT_WORKER-Thread-2 INFO:Local step 218500, global step 3496401: loss 1.5964
[2019-04-01 19:05:41,305] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 218500, global step 3496401: learning rate 0.0010
[2019-04-01 19:05:41,580] A3C_AGENT_WORKER-Thread-20 INFO:Local step 218500, global step 3496561: loss 0.2623
[2019-04-01 19:05:41,581] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 218500, global step 3496561: learning rate 0.0010
[2019-04-01 19:05:42,606] A3C_AGENT_WORKER-Thread-10 INFO:Local step 218500, global step 3497223: loss 0.0606
[2019-04-01 19:05:42,608] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 218500, global step 3497224: learning rate 0.0010
[2019-04-01 19:05:43,090] A3C_AGENT_WORKER-Thread-14 INFO:Local step 218500, global step 3497494: loss 1.6154
[2019-04-01 19:05:43,091] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 218500, global step 3497494: learning rate 0.0010
[2019-04-01 19:05:43,251] A3C_AGENT_WORKER-Thread-11 INFO:Local step 218500, global step 3497582: loss 0.0738
[2019-04-01 19:05:43,255] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 218500, global step 3497583: learning rate 0.0010
[2019-04-01 19:05:43,464] A3C_AGENT_WORKER-Thread-15 INFO:Local step 218500, global step 3497696: loss 1.3049
[2019-04-01 19:05:43,466] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 218500, global step 3497697: learning rate 0.0010
[2019-04-01 19:05:44,060] A3C_AGENT_WORKER-Thread-9 INFO:Local step 218500, global step 3498040: loss 2.1242
[2019-04-01 19:05:44,061] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 218500, global step 3498040: learning rate 0.0010
[2019-04-01 19:05:44,440] A3C_AGENT_WORKER-Thread-12 INFO:Local step 218500, global step 3498267: loss 2.4814
[2019-04-01 19:05:44,443] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 218500, global step 3498267: learning rate 0.0010
[2019-04-01 19:05:44,526] A3C_AGENT_WORKER-Thread-3 INFO:Local step 218500, global step 3498323: loss 2.6404
[2019-04-01 19:05:44,533] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 218500, global step 3498324: learning rate 0.0010
[2019-04-01 19:05:44,795] A3C_AGENT_WORKER-Thread-13 INFO:Local step 218500, global step 3498480: loss 4.7587
[2019-04-01 19:05:44,797] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 218500, global step 3498484: learning rate 0.0010
[2019-04-01 19:05:45,470] A3C_AGENT_WORKER-Thread-16 INFO:Local step 218500, global step 3498865: loss 8.9282
[2019-04-01 19:05:45,472] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 218500, global step 3498865: learning rate 0.0010
[2019-04-01 19:05:45,971] A3C_AGENT_WORKER-Thread-4 INFO:Local step 219000, global step 3499173: loss 9.8786
[2019-04-01 19:05:45,972] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 219000, global step 3499173: learning rate 0.0010
[2019-04-01 19:05:47,198] A3C_AGENT_WORKER-Thread-5 INFO:Local step 218500, global step 3499862: loss 2.1899
[2019-04-01 19:05:47,199] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 218500, global step 3499862: learning rate 0.0010
[2019-04-01 19:05:47,447] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-01 19:05:47,449] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 19:05:47,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:05:47,449] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 19:05:47,450] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 19:05:47,450] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:05:47,452] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:05:48,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run36
[2019-04-01 19:05:48,158] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run36
[2019-04-01 19:05:48,205] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run36
[2019-04-01 19:07:25,149] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.5623627], dtype=float32), -0.6603585]
[2019-04-01 19:07:25,150] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [13.1, 86.16666666666667, 77.0, 212.3333333333333, 26.0, 26.16255254117408, 18.16926481847834, 1.0, 1.0, 12.38607766327366]
[2019-04-01 19:07:25,150] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 19:07:25,151] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 7.3655705e-34], sampled 0.3407115249570756
[2019-04-01 19:07:30,833] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 19:07:37,987] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.5623627], dtype=float32), -0.6603585]
[2019-04-01 19:07:37,987] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [6.447419289833332, 60.59064944666666, 127.3020195666667, 932.6234127, 26.0, 25.15454917777017, 8.69509488613028, 0.0, 1.0, 5.27655169038128]
[2019-04-01 19:07:37,987] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:07:37,988] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 4.651474e-18], sampled 0.4484482903906425
[2019-04-01 19:07:49,944] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 19:07:52,287] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 19:07:53,311] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 3500000, evaluation results [3500000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 19:07:53,346] A3C_AGENT_WORKER-Thread-19 INFO:Local step 218500, global step 3500018: loss 1.9557
[2019-04-01 19:07:53,349] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 218500, global step 3500018: learning rate 0.0010
[2019-04-01 19:07:53,494] A3C_AGENT_WORKER-Thread-17 INFO:Local step 219500, global step 3500110: loss 0.0761
[2019-04-01 19:07:53,496] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 219500, global step 3500110: learning rate 0.0010
[2019-04-01 19:07:53,634] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00
 5.296494e-16 0.000000e+00], sum to 1.0000
[2019-04-01 19:07:53,638] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2380
[2019-04-01 19:07:53,657] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 73.0, 0.0, 0.0, 26.0, 25.23899781073385, 8.685266930584495, 0.0, 1.0, 43.18246345048177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3802800.0000, 
sim time next is 3803400.0000, 
raw observation next is [-3.5, 74.0, 0.0, 0.0, 26.0, 25.22537960217413, 8.82230410318779, 0.0, 1.0, 43.22747550262966], 
processed observation next is [1.0, 0.0, 0.36565096952908593, 0.74, 0.0, 0.0, 1.0, 0.8893399431677328, 0.08822304103187789, 0.0, 1.0, 0.3087676821616404], 
reward next is 0.6912, 
noisyNet noise sample is [array([0.35758758], dtype=float32), -1.106644]. 
=============================================
[2019-04-01 19:07:59,224] A3C_AGENT_WORKER-Thread-18 INFO:Local step 219500, global step 3503114: loss 0.0277
[2019-04-01 19:07:59,227] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 219500, global step 3503114: learning rate 0.0010
[2019-04-01 19:08:01,491] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 2.1043044e-36 1.0000000e+00], sum to 1.0000
[2019-04-01 19:08:01,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5484
[2019-04-01 19:08:01,521] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.166666666666667, 34.66666666666667, 73.33333333333334, 598.6666666666666, 26.0, 27.03228956469788, 18.41539997184779, 1.0, 1.0, 9.253570614409481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3946200.0000, 
sim time next is 3946800.0000, 
raw observation next is [-4.333333333333334, 35.33333333333334, 69.66666666666666, 567.3333333333334, 26.0, 27.13344907526729, 14.98909263806846, 1.0, 1.0, 9.318351598489004], 
processed observation next is [1.0, 0.6956521739130435, 0.3425669436749769, 0.35333333333333344, 0.2322222222222222, 0.6268876611418048, 1.0, 1.1619212964667558, 0.1498909263806846, 1.0, 1.0, 0.06655965427492146], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.32339853], dtype=float32), 0.23171303]. 
=============================================
[2019-04-01 19:08:02,435] A3C_AGENT_WORKER-Thread-20 INFO:Local step 219000, global step 3504751: loss 51.8526
[2019-04-01 19:08:02,435] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 219000, global step 3504751: learning rate 0.0010
[2019-04-01 19:08:02,530] A3C_AGENT_WORKER-Thread-2 INFO:Local step 219000, global step 3504787: loss 32.1005
[2019-04-01 19:08:02,531] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 219000, global step 3504788: learning rate 0.0010
[2019-04-01 19:08:03,843] A3C_AGENT_WORKER-Thread-10 INFO:Local step 219000, global step 3505421: loss 55.8349
[2019-04-01 19:08:03,845] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 219000, global step 3505421: learning rate 0.0010
[2019-04-01 19:08:04,050] A3C_AGENT_WORKER-Thread-14 INFO:Local step 219000, global step 3505529: loss 60.7728
[2019-04-01 19:08:04,057] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 219000, global step 3505529: learning rate 0.0010
[2019-04-01 19:08:04,403] A3C_AGENT_WORKER-Thread-15 INFO:Local step 219000, global step 3505694: loss 43.2251
[2019-04-01 19:08:04,404] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 219000, global step 3505694: learning rate 0.0010
[2019-04-01 19:08:04,462] A3C_AGENT_WORKER-Thread-9 INFO:Local step 219000, global step 3505722: loss 54.6937
[2019-04-01 19:08:04,463] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 219000, global step 3505723: learning rate 0.0010
[2019-04-01 19:08:04,675] A3C_AGENT_WORKER-Thread-11 INFO:Local step 219000, global step 3505832: loss 46.7058
[2019-04-01 19:08:04,676] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 219000, global step 3505832: learning rate 0.0010
[2019-04-01 19:08:04,860] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.0592305e-24 1.6921863e-20 1.9335200e-18 2.4147668e-16 2.1253214e-12
 1.7681341e-01 8.2318658e-01], sum to 1.0000
[2019-04-01 19:08:04,860] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4439
[2019-04-01 19:08:04,917] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.0, 63.0, 46.5, 222.0, 26.0, 25.48741683313381, 8.14068465359548, 1.0, 1.0, 49.70236462335909], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4003200.0000, 
sim time next is 4003800.0000, 
raw observation next is [-12.66666666666667, 61.33333333333333, 62.00000000000001, 296.0000000000001, 26.0, 25.61582857223362, 8.351654787268105, 1.0, 1.0, 46.74859738369982], 
processed observation next is [1.0, 0.34782608695652173, 0.11172668513388727, 0.6133333333333333, 0.2066666666666667, 0.32707182320442, 1.0, 0.9451183674619459, 0.08351654787268105, 1.0, 1.0, 0.33391855274071297], 
reward next is 0.6661, 
noisyNet noise sample is [array([-1.0557929], dtype=float32), 0.60633695]. 
=============================================
[2019-04-01 19:08:05,474] A3C_AGENT_WORKER-Thread-3 INFO:Local step 219000, global step 3506268: loss 48.3701
[2019-04-01 19:08:05,477] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 219000, global step 3506269: learning rate 0.0010
[2019-04-01 19:08:05,590] A3C_AGENT_WORKER-Thread-13 INFO:Local step 219000, global step 3506320: loss 43.5990
[2019-04-01 19:08:05,595] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 219000, global step 3506324: learning rate 0.0010
[2019-04-01 19:08:05,770] A3C_AGENT_WORKER-Thread-12 INFO:Local step 219000, global step 3506402: loss 72.6608
[2019-04-01 19:08:05,772] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 219000, global step 3506403: learning rate 0.0010
[2019-04-01 19:08:06,538] A3C_AGENT_WORKER-Thread-16 INFO:Local step 219000, global step 3506767: loss 45.3701
[2019-04-01 19:08:06,540] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 219000, global step 3506768: learning rate 0.0010
[2019-04-01 19:08:07,450] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9905481e-12 7.8147561e-10 2.6263025e-21 7.5192588e-11 1.0000000e+00
 0.0000000e+00 2.0343655e-19], sum to 1.0000
[2019-04-01 19:08:07,452] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9775
[2019-04-01 19:08:07,458] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 21.5, 24.94046716139699, 8.204962045763224, 1.0, 1.0, 7.277964301614481], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 4557600.0000, 
sim time next is 4558200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.0, 25.07635074001334, 8.290261402113066, 1.0, 1.0, 7.894025694790181], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.42857142857142855, 0.8680501057161913, 0.08290261402113065, 1.0, 1.0, 0.056385897819929864], 
reward next is 0.9436, 
noisyNet noise sample is [array([-0.69200367], dtype=float32), -0.39428937]. 
=============================================
[2019-04-01 19:08:07,520] A3C_AGENT_WORKER-Thread-17 INFO:Local step 220000, global step 3507242: loss 53.5985
[2019-04-01 19:08:07,520] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 220000, global step 3507242: learning rate 0.0010
[2019-04-01 19:08:07,780] A3C_AGENT_WORKER-Thread-4 INFO:Local step 219500, global step 3507367: loss 0.0002
[2019-04-01 19:08:07,788] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 219500, global step 3507368: learning rate 0.0010
[2019-04-01 19:08:08,403] A3C_AGENT_WORKER-Thread-5 INFO:Local step 219000, global step 3507657: loss 49.7233
[2019-04-01 19:08:08,404] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 219000, global step 3507657: learning rate 0.0010
[2019-04-01 19:08:08,469] A3C_AGENT_WORKER-Thread-19 INFO:Local step 219000, global step 3507687: loss 48.3679
[2019-04-01 19:08:08,470] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 219000, global step 3507687: learning rate 0.0010
[2019-04-01 19:08:11,028] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:08:11,029] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5615
[2019-04-01 19:08:11,043] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 37.66666666666667, 0.0, 0.0, 26.0, 25.06720993484418, 7.608642984141452, 0.0, 1.0, 40.19770214444682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4061400.0000, 
sim time next is 4062000.0000, 
raw observation next is [-6.0, 38.33333333333334, 0.0, 0.0, 26.0, 25.04269838833661, 7.505604071426295, 0.0, 1.0, 40.20153712711861], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.3833333333333334, 0.0, 0.0, 1.0, 0.8632426269052301, 0.07505604071426294, 0.0, 1.0, 0.2871538366222758], 
reward next is 0.7128, 
noisyNet noise sample is [array([0.18444616], dtype=float32), -0.25321305]. 
=============================================
[2019-04-01 19:08:11,055] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[63.598965]
 [63.66759 ]
 [64.12041 ]
 [64.21982 ]
 [64.31108 ]], R is [[63.72809219]
 [63.80368805]
 [63.87844086]
 [63.95227051]
 [64.02516174]].
[2019-04-01 19:08:12,942] A3C_AGENT_WORKER-Thread-18 INFO:Local step 220000, global step 3510070: loss 0.9059
[2019-04-01 19:08:12,944] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 220000, global step 3510071: learning rate 0.0010
[2019-04-01 19:08:17,494] A3C_AGENT_WORKER-Thread-2 INFO:Local step 219500, global step 3512463: loss 0.0002
[2019-04-01 19:08:17,494] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 219500, global step 3512463: learning rate 0.0010
[2019-04-01 19:08:17,732] A3C_AGENT_WORKER-Thread-20 INFO:Local step 219500, global step 3512593: loss 0.0023
[2019-04-01 19:08:17,735] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 219500, global step 3512594: learning rate 0.0010
[2019-04-01 19:08:18,747] A3C_AGENT_WORKER-Thread-10 INFO:Local step 219500, global step 3513137: loss 0.0055
[2019-04-01 19:08:18,750] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 219500, global step 3513138: learning rate 0.0010
[2019-04-01 19:08:19,172] A3C_AGENT_WORKER-Thread-14 INFO:Local step 219500, global step 3513364: loss 0.0060
[2019-04-01 19:08:19,175] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 219500, global step 3513366: learning rate 0.0010
[2019-04-01 19:08:19,932] A3C_AGENT_WORKER-Thread-9 INFO:Local step 219500, global step 3513788: loss 0.0005
[2019-04-01 19:08:19,933] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 219500, global step 3513788: learning rate 0.0010
[2019-04-01 19:08:20,095] A3C_AGENT_WORKER-Thread-15 INFO:Local step 219500, global step 3513887: loss -0.1082
[2019-04-01 19:08:20,096] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 219500, global step 3513887: learning rate 0.0010
[2019-04-01 19:08:20,153] A3C_AGENT_WORKER-Thread-11 INFO:Local step 219500, global step 3513912: loss 0.0095
[2019-04-01 19:08:20,156] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 219500, global step 3513913: learning rate 0.0010
[2019-04-01 19:08:20,713] A3C_AGENT_WORKER-Thread-3 INFO:Local step 219500, global step 3514192: loss 0.0736
[2019-04-01 19:08:20,715] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 219500, global step 3514192: learning rate 0.0010
[2019-04-01 19:08:20,897] A3C_AGENT_WORKER-Thread-13 INFO:Local step 219500, global step 3514287: loss 0.0250
[2019-04-01 19:08:20,904] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 219500, global step 3514290: learning rate 0.0010
[2019-04-01 19:08:21,243] A3C_AGENT_WORKER-Thread-12 INFO:Local step 219500, global step 3514467: loss 0.1298
[2019-04-01 19:08:21,244] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 219500, global step 3514468: learning rate 0.0010
[2019-04-01 19:08:21,974] A3C_AGENT_WORKER-Thread-4 INFO:Local step 220000, global step 3514874: loss 2.5077
[2019-04-01 19:08:21,983] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 220000, global step 3514875: learning rate 0.0010
[2019-04-01 19:08:22,016] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:08:22,020] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2069
[2019-04-01 19:08:22,041] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.15, 49.5, 107.0, 677.0, 26.0, 26.74936040161272, 15.07331073632041, 1.0, 1.0, 12.51917768655271], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4354200.0000, 
sim time next is 4354800.0000, 
raw observation next is [8.766666666666666, 47.0, 108.3333333333333, 694.1666666666667, 26.0, 26.98933544714478, 16.16359209928294, 1.0, 1.0, 11.38518125673615], 
processed observation next is [1.0, 0.391304347826087, 0.7054478301015698, 0.47, 0.361111111111111, 0.767034990791897, 1.0, 1.141333635306397, 0.16163592099282942, 1.0, 1.0, 0.08132272326240107], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29707837], dtype=float32), 0.961744]. 
=============================================
[2019-04-01 19:08:22,376] A3C_AGENT_WORKER-Thread-16 INFO:Local step 219500, global step 3515112: loss -0.0142
[2019-04-01 19:08:22,380] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 219500, global step 3515113: learning rate 0.0010
[2019-04-01 19:08:22,492] A3C_AGENT_WORKER-Thread-17 INFO:Local step 220500, global step 3515185: loss 0.0459
[2019-04-01 19:08:22,492] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 220500, global step 3515185: learning rate 0.0010
[2019-04-01 19:08:24,049] A3C_AGENT_WORKER-Thread-19 INFO:Local step 219500, global step 3516099: loss 0.0287
[2019-04-01 19:08:24,051] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 219500, global step 3516099: learning rate 0.0010
[2019-04-01 19:08:24,079] A3C_AGENT_WORKER-Thread-5 INFO:Local step 219500, global step 3516111: loss 0.0318
[2019-04-01 19:08:24,081] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 219500, global step 3516111: learning rate 0.0010
[2019-04-01 19:08:24,344] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:08:24,344] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8525
[2019-04-01 19:08:24,366] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.749999999999999, 62.83333333333333, 0.0, 0.0, 26.0, 25.73239794583632, 12.5641574954131, 0.0, 1.0, 6.911698270389429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4405800.0000, 
sim time next is 4406400.0000, 
raw observation next is [7.6, 63.0, 0.0, 0.0, 26.0, 25.68396905578467, 12.23403581445727, 0.0, 1.0, 6.960825313780144], 
processed observation next is [1.0, 0.0, 0.6731301939058172, 0.63, 0.0, 0.0, 1.0, 0.9548527222549527, 0.1223403581445727, 0.0, 1.0, 0.049720180812715314], 
reward next is 0.9503, 
noisyNet noise sample is [array([0.450656], dtype=float32), -0.3590557]. 
=============================================
[2019-04-01 19:08:24,529] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:08:24,530] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0009
[2019-04-01 19:08:24,574] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.55, 71.16666666666667, 61.33333333333334, 327.3333333333334, 26.0, 25.67770391815068, 9.101025839603585, 1.0, 1.0, 21.86411350112428], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4349400.0000, 
sim time next is 4350000.0000, 
raw observation next is [4.1, 68.33333333333334, 76.66666666666667, 409.1666666666667, 26.0, 25.65995315424494, 10.14204312847585, 1.0, 1.0, 21.92672286527236], 
processed observation next is [1.0, 0.34782608695652173, 0.5761772853185596, 0.6833333333333335, 0.2555555555555556, 0.4521178637200737, 1.0, 0.9514218791778484, 0.1014204312847585, 1.0, 1.0, 0.1566194490376597], 
reward next is 0.7866, 
noisyNet noise sample is [array([0.52846557], dtype=float32), 2.3722088]. 
=============================================
[2019-04-01 19:08:24,578] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[55.18123 ]
 [55.051258]
 [56.122715]
 [58.56855 ]
 [62.278748]], R is [[55.82384491]
 [56.10943222]
 [56.38158417]
 [56.63980103]
 [56.88158035]].
[2019-04-01 19:08:26,956] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:08:26,957] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7039
[2019-04-01 19:08:26,979] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 76.0, 0.0, 0.0, 26.0, 25.42779126299166, 8.359086193627965, 0.0, 1.0, 34.12241473662858], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4320000.0000, 
sim time next is 4320600.0000, 
raw observation next is [4.45, 75.83333333333334, 0.0, 0.0, 26.0, 25.43452684236179, 8.533322375426076, 0.0, 1.0, 38.00035509879083], 
processed observation next is [1.0, 0.0, 0.5858725761772854, 0.7583333333333334, 0.0, 0.0, 1.0, 0.9192181203373987, 0.08533322375426076, 0.0, 1.0, 0.2714311078485059], 
reward next is 0.7286, 
noisyNet noise sample is [array([-1.8373333], dtype=float32), -0.63937175]. 
=============================================
[2019-04-01 19:08:28,086] A3C_AGENT_WORKER-Thread-18 INFO:Local step 220500, global step 3518494: loss 0.1264
[2019-04-01 19:08:28,090] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 220500, global step 3518494: learning rate 0.0010
[2019-04-01 19:08:31,551] A3C_AGENT_WORKER-Thread-2 INFO:Local step 220000, global step 3520276: loss 2.2924
[2019-04-01 19:08:31,558] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 220000, global step 3520279: learning rate 0.0010
[2019-04-01 19:08:31,736] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:08:31,747] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9459
[2019-04-01 19:08:31,763] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 74.0, 0.0, 0.0, 26.0, 25.60492465553827, 11.22477077872229, 0.0, 1.0, 27.75477974803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4429800.0000, 
sim time next is 4430400.0000, 
raw observation next is [2.333333333333333, 76.0, 0.0, 0.0, 26.0, 25.6506704627906, 11.259664276575, 0.0, 1.0, 25.97080479876179], 
processed observation next is [1.0, 0.2608695652173913, 0.5272391505078486, 0.76, 0.0, 0.0, 1.0, 0.9500957803986572, 0.11259664276575, 0.0, 1.0, 0.18550574856258423], 
reward next is 0.8145, 
noisyNet noise sample is [array([1.3895237], dtype=float32), -0.39445215]. 
=============================================
[2019-04-01 19:08:32,403] A3C_AGENT_WORKER-Thread-20 INFO:Local step 220000, global step 3520705: loss 4.3992
[2019-04-01 19:08:32,405] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 220000, global step 3520705: learning rate 0.0010
[2019-04-01 19:08:32,980] A3C_AGENT_WORKER-Thread-10 INFO:Local step 220000, global step 3520999: loss 1.3627
[2019-04-01 19:08:32,983] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 220000, global step 3520999: learning rate 0.0010
[2019-04-01 19:08:33,740] A3C_AGENT_WORKER-Thread-14 INFO:Local step 220000, global step 3521362: loss 0.3307
[2019-04-01 19:08:33,741] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 220000, global step 3521362: learning rate 0.0010
[2019-04-01 19:08:34,178] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.8980141e-30 1.0000000e+00], sum to 1.0000
[2019-04-01 19:08:34,179] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8106
[2019-04-01 19:08:34,194] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 24.0, 120.0, 862.5, 26.0, 27.07513563814318, 16.87026641552623, 1.0, 1.0, 2.037051157749253], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4971600.0000, 
sim time next is 4972200.0000, 
raw observation next is [7.166666666666667, 24.33333333333333, 119.0, 861.6666666666666, 26.0, 27.10937474408869, 17.39531176127159, 1.0, 1.0, 1.999289394392479], 
processed observation next is [1.0, 0.5652173913043478, 0.6611265004616806, 0.2433333333333333, 0.39666666666666667, 0.9521178637200737, 1.0, 1.1584821062983843, 0.1739531176127159, 1.0, 1.0, 0.01428063853137485], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.580805], dtype=float32), 0.28495264]. 
=============================================
[2019-04-01 19:08:34,286] A3C_AGENT_WORKER-Thread-9 INFO:Local step 220000, global step 3521637: loss 0.2181
[2019-04-01 19:08:34,288] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 220000, global step 3521638: learning rate 0.0010
[2019-04-01 19:08:34,738] A3C_AGENT_WORKER-Thread-11 INFO:Local step 220000, global step 3521866: loss 0.6187
[2019-04-01 19:08:34,739] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 220000, global step 3521866: learning rate 0.0010
[2019-04-01 19:08:34,839] A3C_AGENT_WORKER-Thread-15 INFO:Local step 220000, global step 3521922: loss 1.7855
[2019-04-01 19:08:34,840] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 220000, global step 3521922: learning rate 0.0010
[2019-04-01 19:08:35,132] A3C_AGENT_WORKER-Thread-3 INFO:Local step 220000, global step 3522071: loss 1.1055
[2019-04-01 19:08:35,134] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 220000, global step 3522072: learning rate 0.0010
[2019-04-01 19:08:35,276] A3C_AGENT_WORKER-Thread-13 INFO:Local step 220000, global step 3522147: loss 16.0384
[2019-04-01 19:08:35,278] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 220000, global step 3522147: learning rate 0.0010
[2019-04-01 19:08:35,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:08:35,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:08:35,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run27
[2019-04-01 19:08:35,622] A3C_AGENT_WORKER-Thread-12 INFO:Local step 220000, global step 3522322: loss 0.3365
[2019-04-01 19:08:35,623] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 220000, global step 3522322: learning rate 0.0010
[2019-04-01 19:08:36,800] A3C_AGENT_WORKER-Thread-16 INFO:Local step 220000, global step 3522980: loss 0.1115
[2019-04-01 19:08:36,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 220000, global step 3522981: learning rate 0.0010
[2019-04-01 19:08:37,122] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.2497454e-24 0.0000000e+00
 1.0000000e+00 2.0070310e-35], sum to 1.0000
[2019-04-01 19:08:37,130] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5733
[2019-04-01 19:08:37,150] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.866666666666667, 76.0, 0.0, 0.0, 26.0, 25.08159022862429, 7.58053343722226, 0.0, 1.0, 35.87478095505899], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4603200.0000, 
sim time next is 4603800.0000, 
raw observation next is [-2.933333333333333, 76.5, 0.0, 0.0, 26.0, 25.0491015012885, 7.577637755944383, 0.0, 1.0, 35.88013304044838], 
processed observation next is [1.0, 0.2608695652173913, 0.38134810710988, 0.765, 0.0, 0.0, 1.0, 0.8641573573269287, 0.07577637755944383, 0.0, 1.0, 0.2562866645746313], 
reward next is 0.7437, 
noisyNet noise sample is [array([0.2470513], dtype=float32), 0.25271457]. 
=============================================
[2019-04-01 19:08:37,542] A3C_AGENT_WORKER-Thread-4 INFO:Local step 220500, global step 3523348: loss 0.0747
[2019-04-01 19:08:37,545] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 220500, global step 3523348: learning rate 0.0010
[2019-04-01 19:08:38,467] A3C_AGENT_WORKER-Thread-5 INFO:Local step 220000, global step 3523835: loss 0.7118
[2019-04-01 19:08:38,468] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 220000, global step 3523836: learning rate 0.0010
[2019-04-01 19:08:38,593] A3C_AGENT_WORKER-Thread-19 INFO:Local step 220000, global step 3523913: loss 73.4507
[2019-04-01 19:08:38,595] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 220000, global step 3523913: learning rate 0.0010
[2019-04-01 19:08:40,710] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.8827266e-36 1.2293766e-12
 2.1955164e-05 9.9997807e-01], sum to 1.0000
[2019-04-01 19:08:40,711] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4995
[2019-04-01 19:08:40,738] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 78.16666666666667, 0.0, 0.0, 26.0, 25.14939938237528, 8.739369402944957, 0.0, 1.0, 41.30210056422222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4749000.0000, 
sim time next is 4749600.0000, 
raw observation next is [-3.333333333333333, 79.33333333333334, 0.0, 0.0, 26.0, 25.11858446599522, 8.599880082796126, 0.0, 1.0, 41.2201364431556], 
processed observation next is [1.0, 1.0, 0.37026777469990774, 0.7933333333333334, 0.0, 0.0, 1.0, 0.8740834951421742, 0.08599880082796126, 0.0, 1.0, 0.29442954602254], 
reward next is 0.7056, 
noisyNet noise sample is [array([0.7819096], dtype=float32), -1.2436565]. 
=============================================
[2019-04-01 19:08:40,786] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:08:40,786] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:08:40,800] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run27
[2019-04-01 19:08:43,686] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.4190307e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 19:08:43,687] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3734
[2019-04-01 19:08:43,694] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 131.0, 737.0, 26.0, 25.21995349051138, 9.834641873567877, 0.0, 1.0, 5.799642613567277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4805400.0000, 
sim time next is 4806000.0000, 
raw observation next is [3.0, 37.0, 122.5, 734.5, 26.0, 25.21324455667045, 9.933759160022513, 0.0, 1.0, 5.453472113303785], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.4083333333333333, 0.8116022099447514, 1.0, 0.8876063652386357, 0.09933759160022512, 0.0, 1.0, 0.03895337223788418], 
reward next is 0.9610, 
noisyNet noise sample is [array([-0.29305777], dtype=float32), -0.4790681]. 
=============================================
[2019-04-01 19:08:43,702] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[66.55049]
 [66.49959]
 [66.42532]
 [66.4965 ]
 [66.50236]], R is [[66.82467651]
 [67.11500549]
 [67.40271759]
 [67.67776489]
 [67.94766235]].
[2019-04-01 19:08:45,366] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 4.399352e-29], sum to 1.0000
[2019-04-01 19:08:45,367] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3403
[2019-04-01 19:08:45,421] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.333333333333333, 52.33333333333333, 147.8333333333333, 748.1666666666667, 26.0, 25.43303669453558, 9.806427240077808, 0.0, 1.0, 20.66500452703735], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4790400.0000, 
sim time next is 4791000.0000, 
raw observation next is [-2.166666666666667, 49.16666666666666, 142.6666666666667, 766.3333333333334, 26.0, 25.36298450052131, 9.664614577025795, 0.0, 1.0, 19.26267984637407], 
processed observation next is [0.0, 0.43478260869565216, 0.4025854108956602, 0.4916666666666666, 0.47555555555555573, 0.8467771639042357, 1.0, 0.9089977857887587, 0.09664614577025794, 0.0, 1.0, 0.13759057033124336], 
reward next is 0.8624, 
noisyNet noise sample is [array([1.2097534], dtype=float32), 0.42759004]. 
=============================================
[2019-04-01 19:08:45,440] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[64.92621 ]
 [64.642426]
 [64.53444 ]
 [64.495186]
 [64.3325  ]], R is [[65.43644714]
 [65.63447571]
 [65.80815887]
 [65.98144531]
 [66.14141083]].
[2019-04-01 19:08:45,898] A3C_AGENT_WORKER-Thread-2 INFO:Local step 220500, global step 3527591: loss 0.0376
[2019-04-01 19:08:45,910] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 220500, global step 3527592: learning rate 0.0010
[2019-04-01 19:08:47,140] A3C_AGENT_WORKER-Thread-20 INFO:Local step 220500, global step 3528174: loss 0.0268
[2019-04-01 19:08:47,141] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 220500, global step 3528174: learning rate 0.0010
[2019-04-01 19:08:47,313] A3C_AGENT_WORKER-Thread-10 INFO:Local step 220500, global step 3528283: loss 0.2452
[2019-04-01 19:08:47,316] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 220500, global step 3528283: learning rate 0.0010
[2019-04-01 19:08:48,347] A3C_AGENT_WORKER-Thread-14 INFO:Local step 220500, global step 3528875: loss 0.0907
[2019-04-01 19:08:48,348] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 220500, global step 3528875: learning rate 0.0010
[2019-04-01 19:08:48,378] A3C_AGENT_WORKER-Thread-9 INFO:Local step 220500, global step 3528892: loss 0.0937
[2019-04-01 19:08:48,381] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 220500, global step 3528894: learning rate 0.0010
[2019-04-01 19:08:49,171] A3C_AGENT_WORKER-Thread-15 INFO:Local step 220500, global step 3529304: loss 0.0720
[2019-04-01 19:08:49,176] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 220500, global step 3529304: learning rate 0.0010
[2019-04-01 19:08:49,295] A3C_AGENT_WORKER-Thread-11 INFO:Local step 220500, global step 3529362: loss 0.0885
[2019-04-01 19:08:49,306] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 220500, global step 3529366: learning rate 0.0010
[2019-04-01 19:08:49,564] A3C_AGENT_WORKER-Thread-13 INFO:Local step 220500, global step 3529503: loss 0.0602
[2019-04-01 19:08:49,564] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 220500, global step 3529503: learning rate 0.0010
[2019-04-01 19:08:49,576] A3C_AGENT_WORKER-Thread-3 INFO:Local step 220500, global step 3529511: loss 0.0998
[2019-04-01 19:08:49,579] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 220500, global step 3529513: learning rate 0.0010
[2019-04-01 19:08:49,798] A3C_AGENT_WORKER-Thread-12 INFO:Local step 220500, global step 3529590: loss 0.0564
[2019-04-01 19:08:49,799] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 220500, global step 3529590: learning rate 0.0010
[2019-04-01 19:08:49,822] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:08:49,822] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:08:49,832] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7036994e-31 0.0000000e+00
 1.0000000e+00 2.2533445e-16], sum to 1.0000
[2019-04-01 19:08:49,832] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3838
[2019-04-01 19:08:49,867] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.166666666666667, 60.83333333333333, 0.0, 0.0, 26.0, 24.62174662524265, 5.886691395345302, 0.0, 1.0, 39.03915425041588], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4860600.0000, 
sim time next is 4861200.0000, 
raw observation next is [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 26.0, 24.59741755182072, 5.843449486239945, 0.0, 1.0, 39.02485745948004], 
processed observation next is [0.0, 0.2608695652173913, 0.37026777469990774, 0.6166666666666667, 0.0, 0.0, 1.0, 0.7996310788315313, 0.05843449486239945, 0.0, 1.0, 0.27874898185342883], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.01050373], dtype=float32), -0.6162666]. 
=============================================
[2019-04-01 19:08:49,920] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run27
[2019-04-01 19:08:50,707] A3C_AGENT_WORKER-Thread-16 INFO:Local step 220500, global step 3529978: loss 0.0453
[2019-04-01 19:08:50,709] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 220500, global step 3529978: learning rate 0.0010
[2019-04-01 19:08:52,851] A3C_AGENT_WORKER-Thread-5 INFO:Local step 220500, global step 3531071: loss 0.0664
[2019-04-01 19:08:52,853] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 220500, global step 3531071: learning rate 0.0010
[2019-04-01 19:08:53,052] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 2.7918805e-13 1.0000000e+00], sum to 1.0000
[2019-04-01 19:08:53,052] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5906
[2019-04-01 19:08:53,072] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.5, 24.5, 123.0, 865.0, 26.0, 27.09328734188225, 16.47945116243683, 1.0, 1.0, 3.657329543176324], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4969800.0000, 
sim time next is 4970400.0000, 
raw observation next is [6.666666666666666, 24.33333333333334, 122.0, 864.1666666666667, 26.0, 27.08648353555321, 16.55116347847841, 1.0, 1.0, 2.048899987826085], 
processed observation next is [1.0, 0.5217391304347826, 0.6472760849492153, 0.2433333333333334, 0.4066666666666667, 0.9548802946593002, 1.0, 1.1552119336504585, 0.16551163478478412, 1.0, 1.0, 0.014634999913043465], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.04063298], dtype=float32), 0.10829366]. 
=============================================
[2019-04-01 19:08:53,087] A3C_AGENT_WORKER-Thread-19 INFO:Local step 220500, global step 3531206: loss 0.0388
[2019-04-01 19:08:53,092] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 220500, global step 3531208: learning rate 0.0010
[2019-04-01 19:08:54,954] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:08:54,957] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2389
[2019-04-01 19:08:54,974] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 26.0, 25.41397468377347, 8.934936998915205, 0.0, 1.0, 37.14037585541327], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5018400.0000, 
sim time next is 5019000.0000, 
raw observation next is [0.6666666666666667, 42.5, 0.0, 0.0, 26.0, 25.39987021487559, 9.058523170849183, 0.0, 1.0, 37.40817205223427], 
processed observation next is [1.0, 0.08695652173913043, 0.4810710987996307, 0.425, 0.0, 0.0, 1.0, 0.9142671735536558, 0.09058523170849182, 0.0, 1.0, 0.2672012289445305], 
reward next is 0.7328, 
noisyNet noise sample is [array([-1.5149355], dtype=float32), -1.1893735]. 
=============================================
[2019-04-01 19:08:54,988] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[67.52798]
 [67.65484]
 [67.84735]
 [67.98273]
 [68.21203]], R is [[67.45417786]
 [67.51435089]
 [67.59127808]
 [67.68631744]
 [67.78618622]].
[2019-04-01 19:08:58,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:08:58,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:08:58,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run27
[2019-04-01 19:08:59,023] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:08:59,024] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:08:59,034] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run27
[2019-04-01 19:08:59,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:08:59,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:08:59,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run27
[2019-04-01 19:09:00,003] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:09:00,003] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:09:00,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run27
[2019-04-01 19:09:00,198] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:09:00,198] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:09:00,218] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run27
[2019-04-01 19:09:00,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:09:00,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:09:00,806] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run27
[2019-04-01 19:09:01,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:09:01,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:09:01,150] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run27
[2019-04-01 19:09:01,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:09:01,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:09:01,257] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run27
[2019-04-01 19:09:01,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:09:01,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:09:01,490] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run27
[2019-04-01 19:09:01,730] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:09:01,731] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:09:01,734] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run27
[2019-04-01 19:09:02,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:09:02,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:09:02,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run27
[2019-04-01 19:09:04,598] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:09:04,598] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:09:04,602] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run27
[2019-04-01 19:09:04,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:09:04,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:09:04,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run27
[2019-04-01 19:09:19,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.1013510e-24 3.4507241e-27 3.4553012e-31 6.2804525e-27 5.0715180e-26
 1.0000000e+00 3.7172976e-28], sum to 1.0000
[2019-04-01 19:09:19,771] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0949
[2019-04-01 19:09:19,821] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666666, 28.66666666666667, 106.0, 0.0, 26.0, 25.1028362939538, 5.931383080791768, 1.0, 1.0, 29.85435533368432], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 471000.0000, 
sim time next is 471600.0000, 
raw observation next is [-2.3, 28.0, 110.0, 0.0, 26.0, 25.09328458127536, 5.990851674587272, 1.0, 1.0, 33.83764684216846], 
processed observation next is [1.0, 0.4782608695652174, 0.3988919667590028, 0.28, 0.36666666666666664, 0.0, 1.0, 0.8704692258964799, 0.05990851674587273, 1.0, 1.0, 0.24169747744406042], 
reward next is 0.7583, 
noisyNet noise sample is [array([1.5140553], dtype=float32), 1.0830305]. 
=============================================
[2019-04-01 19:09:26,346] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2583590e-38 0.0000000e+00
 1.0000000e+00 3.0800726e-32], sum to 1.0000
[2019-04-01 19:09:26,346] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9332
[2019-04-01 19:09:26,385] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.916666666666667, 65.0, 137.0, 0.0, 26.0, 25.28423374438492, 6.853756949495843, 1.0, 1.0, 31.53724575165226], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 216600.0000, 
sim time next is 217200.0000, 
raw observation next is [-4.833333333333334, 65.0, 133.0, 0.0, 26.0, 25.30490626281637, 6.825354550986863, 1.0, 1.0, 31.41949934517089], 
processed observation next is [1.0, 0.5217391304347826, 0.32871652816251157, 0.65, 0.44333333333333336, 0.0, 1.0, 0.9007008946880529, 0.06825354550986863, 1.0, 1.0, 0.22442499532264923], 
reward next is 0.7756, 
noisyNet noise sample is [array([-0.9390711], dtype=float32), -1.5401896]. 
=============================================
[2019-04-01 19:09:36,068] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.18243845e-20 5.63761449e-27 4.82564141e-19 1.58465188e-08
 9.99917746e-01 8.22623551e-05 1.57506062e-08], sum to 1.0000
[2019-04-01 19:09:36,068] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2671
[2019-04-01 19:09:36,086] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.07652367748752, 6.152430817443729, 0.0, 1.0, 48.20872700981778], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 354000.0000, 
sim time next is 354600.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 22.99676305121515, 6.206257377900902, 0.0, 1.0, 48.31153091144426], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 1.0, 0.570966150173593, 0.06206257377900901, 0.0, 1.0, 0.3450823636531733], 
reward next is 0.6549, 
noisyNet noise sample is [array([-0.64864033], dtype=float32), -1.5956367]. 
=============================================
[2019-04-01 19:09:52,871] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:09:52,871] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2572
[2019-04-01 19:09:52,911] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.93704656043428, 7.512332890446172, 0.0, 1.0, 46.03911840123789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 587400.0000, 
sim time next is 588000.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.94767125376682, 7.560213705236222, 0.0, 1.0, 47.45228948633745], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 1.0, 0.8496673219666887, 0.07560213705236223, 0.0, 1.0, 0.33894492490241035], 
reward next is 0.6611, 
noisyNet noise sample is [array([0.33925638], dtype=float32), 0.5504359]. 
=============================================
[2019-04-01 19:09:52,919] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[69.43324 ]
 [69.686676]
 [69.893845]
 [70.04787 ]
 [70.1586  ]], R is [[69.24656677]
 [69.22525024]
 [69.24998474]
 [69.28268433]
 [69.31383514]].
[2019-04-01 19:09:56,668] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.1693050e-35 1.9612004e-31
 1.0000000e+00 6.0234219e-20], sum to 1.0000
[2019-04-01 19:09:56,668] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7465
[2019-04-01 19:09:56,689] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 69.16666666666667, 0.0, 0.0, 26.0, 23.82155935990506, 5.243328754518913, 0.0, 1.0, 43.81745588833865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 622200.0000, 
sim time next is 622800.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 26.0, 23.79264720728316, 5.240538196637904, 0.0, 1.0, 43.74730536281402], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.68, 0.0, 0.0, 1.0, 0.6846638867547371, 0.05240538196637904, 0.0, 1.0, 0.3124807525915287], 
reward next is 0.6875, 
noisyNet noise sample is [array([-0.698739], dtype=float32), -0.88447994]. 
=============================================
[2019-04-01 19:09:57,439] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:09:57,441] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5992
[2019-04-01 19:09:57,482] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 80.0, 134.3333333333333, 552.3333333333333, 26.0, 25.06252938569035, 8.464506351124514, 0.0, 1.0, 26.95387481430556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 568200.0000, 
sim time next is 568800.0000, 
raw observation next is [-1.2, 80.0, 132.5, 531.0, 26.0, 25.07656751279784, 8.417755095327466, 0.0, 1.0, 23.32787783157783], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.8, 0.44166666666666665, 0.5867403314917127, 1.0, 0.8680810732568344, 0.08417755095327467, 0.0, 1.0, 0.1666276987969845], 
reward next is 0.8334, 
noisyNet noise sample is [array([-0.38824975], dtype=float32), 1.12261]. 
=============================================
[2019-04-01 19:10:08,782] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:10:08,782] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3434
[2019-04-01 19:10:08,828] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 9.0, 0.0, 26.0, 25.73894228066405, 11.53029912825768, 1.0, 1.0, 25.72422203891722], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1324800.0000, 
sim time next is 1325400.0000, 
raw observation next is [1.0, 92.0, 12.0, 0.0, 26.0, 25.66336265520324, 11.16752641017924, 1.0, 1.0, 24.32172965560233], 
processed observation next is [1.0, 0.34782608695652173, 0.4903047091412743, 0.92, 0.04, 0.0, 1.0, 0.95190895074332, 0.1116752641017924, 1.0, 1.0, 0.1737266403971595], 
reward next is 0.3593, 
noisyNet noise sample is [array([0.27025825], dtype=float32), 0.94183695]. 
=============================================
[2019-04-01 19:10:10,277] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1597135e-10 2.6197797e-35
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:10:10,277] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6024
[2019-04-01 19:10:10,329] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 79.0, 75.0, 0.0, 26.0, 25.33762745587928, 9.314888018012264, 1.0, 1.0, 48.549944908103], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 828000.0000, 
sim time next is 828600.0000, 
raw observation next is [-3.9, 80.16666666666667, 69.66666666666666, 0.0, 26.0, 26.32977731541466, 10.84942352922305, 1.0, 1.0, 34.80708948476379], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.8016666666666667, 0.2322222222222222, 0.0, 1.0, 1.0471110450592371, 0.1084942352922305, 1.0, 1.0, 0.2486220677483128], 
reward next is 0.4116, 
noisyNet noise sample is [array([-0.7232535], dtype=float32), 0.020393359]. 
=============================================
[2019-04-01 19:10:11,294] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.2951124e-22 5.5109385e-35
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:10:11,294] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9674
[2019-04-01 19:10:11,339] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.4, 60.0, 0.0, 0.0, 26.0, 24.83922394163444, 7.311229634731873, 0.0, 1.0, 44.51005992991553], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 765600.0000, 
sim time next is 766200.0000, 
raw observation next is [-5.5, 60.5, 0.0, 0.0, 26.0, 24.84476795750942, 7.245690397253408, 0.0, 1.0, 43.9580409156595], 
processed observation next is [1.0, 0.8695652173913043, 0.3102493074792244, 0.605, 0.0, 0.0, 1.0, 0.834966851072774, 0.07245690397253407, 0.0, 1.0, 0.313986006540425], 
reward next is 0.6860, 
noisyNet noise sample is [array([0.7458092], dtype=float32), 0.06430668]. 
=============================================
[2019-04-01 19:10:18,305] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7586593e-16 0.0000000e+00
 1.0000000e+00 1.0353552e-37], sum to 1.0000
[2019-04-01 19:10:18,305] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3448
[2019-04-01 19:10:18,330] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.55, 79.5, 0.0, 0.0, 26.0, 24.62309916792946, 6.332664073976606, 0.0, 1.0, 40.16220769278964], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 862200.0000, 
sim time next is 862800.0000, 
raw observation next is [-2.466666666666667, 79.66666666666666, 0.0, 0.0, 26.0, 24.59698629025763, 6.281727403255762, 0.0, 1.0, 40.0155797899592], 
processed observation next is [1.0, 1.0, 0.39427516158818104, 0.7966666666666665, 0.0, 0.0, 1.0, 0.7995694700368041, 0.06281727403255762, 0.0, 1.0, 0.28582556992827995], 
reward next is 0.7142, 
noisyNet noise sample is [array([0.6361357], dtype=float32), 0.90269566]. 
=============================================
[2019-04-01 19:10:18,764] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3299813e-36 1.7178689e-37 0.0000000e+00 4.2997870e-21 1.8383279e-37
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:10:18,765] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9148
[2019-04-01 19:10:18,788] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 84.0, 80.33333333333334, 0.0, 26.0, 25.48812335527359, 7.213673887629099, 1.0, 1.0, 12.99379558510993], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 902400.0000, 
sim time next is 903000.0000, 
raw observation next is [1.1, 84.0, 83.66666666666666, 0.0, 26.0, 25.47209050190828, 7.200126715958395, 1.0, 1.0, 12.39948713456627], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.27888888888888885, 0.0, 1.0, 0.9245843574154685, 0.07200126715958395, 1.0, 1.0, 0.08856776524690192], 
reward next is 0.9114, 
noisyNet noise sample is [array([-0.8133105], dtype=float32), -0.49148288]. 
=============================================
[2019-04-01 19:10:18,792] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[70.81147 ]
 [70.819145]
 [70.76523 ]
 [70.56855 ]
 [70.43113 ]], R is [[71.0266571 ]
 [71.22357941]
 [71.41428375]
 [71.5987854 ]
 [71.77662659]].
[2019-04-01 19:10:27,173] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:10:27,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9214
[2019-04-01 19:10:27,189] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.2, 76.0, 0.0, 0.0, 26.0, 26.10150183557206, 13.77236901924081, 0.0, 1.0, 14.58770654146703], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1041600.0000, 
sim time next is 1042200.0000, 
raw observation next is [14.1, 76.5, 0.0, 0.0, 26.0, 26.06679896665481, 13.48848620586866, 0.0, 1.0, 13.85752606530032], 
processed observation next is [1.0, 0.043478260869565216, 0.8531855955678671, 0.765, 0.0, 0.0, 1.0, 1.0095427095221159, 0.1348848620586866, 0.0, 1.0, 0.09898232903785943], 
reward next is 0.9010, 
noisyNet noise sample is [array([-0.50328106], dtype=float32), 1.2767465]. 
=============================================
[2019-04-01 19:10:27,495] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0043286e-14 7.6902367e-22 9.5198907e-23 7.2984478e-18 8.5847689e-16
 1.0000000e+00 3.7548321e-23], sum to 1.0000
[2019-04-01 19:10:27,500] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8000
[2019-04-01 19:10:27,507] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.46666666666667, 55.33333333333334, 172.6666666666667, 52.83333333333332, 26.0, 27.16161680358366, 23.8038632042874, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1084800.0000, 
sim time next is 1085400.0000, 
raw observation next is [18.55, 55.0, 171.0, 0.0, 26.0, 27.34665982361583, 24.67554709760741, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.976454293628809, 0.55, 0.57, 0.0, 1.0, 1.1923799748022614, 0.2467554709760741, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4663419], dtype=float32), -0.9995228]. 
=============================================
[2019-04-01 19:10:28,740] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:10:28,747] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7641
[2019-04-01 19:10:28,756] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.1, 66.83333333333333, 0.0, 0.0, 26.0, 25.83614078620199, 15.13680820925764, 0.0, 1.0, 12.91214900425403], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1120200.0000, 
sim time next is 1120800.0000, 
raw observation next is [12.0, 67.66666666666667, 0.0, 0.0, 26.0, 25.78473260001675, 14.79601984675753, 0.0, 1.0, 12.31288618978535], 
processed observation next is [1.0, 1.0, 0.7950138504155125, 0.6766666666666667, 0.0, 0.0, 1.0, 0.9692475142881071, 0.1479601984675753, 0.0, 1.0, 0.08794918706989537], 
reward next is 0.9121, 
noisyNet noise sample is [array([0.95462364], dtype=float32), 0.77223784]. 
=============================================
[2019-04-01 19:10:30,502] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:10:30,506] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2503
[2019-04-01 19:10:30,515] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.8, 63.0, 165.5, 0.0, 26.0, 25.10710704157617, 10.84847583535789, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1166400.0000, 
sim time next is 1167000.0000, 
raw observation next is [18.71666666666667, 63.33333333333333, 167.3333333333333, 0.0, 26.0, 25.09701233277691, 10.82994161333808, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.981071098799631, 0.6333333333333333, 0.5577777777777776, 0.0, 1.0, 0.8710017618252728, 0.1082994161333808, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18768878], dtype=float32), -0.40139818]. 
=============================================
[2019-04-01 19:10:30,530] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[106.63625 ]
 [106.19996 ]
 [105.744965]
 [105.17001 ]
 [104.35806 ]], R is [[106.97133636]
 [106.90162659]
 [106.83261108]
 [106.76428223]
 [106.69664001]].
[2019-04-01 19:10:35,287] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1473481e-28 3.6692443e-34 9.3599245e-33 2.7281445e-12 8.8641964e-26
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:10:35,288] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8017
[2019-04-01 19:10:35,297] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6000000000000001, 92.0, 83.00000000000001, 0.0, 26.0, 26.04889607573564, 12.37004601668809, 1.0, 1.0, 12.90394464540816], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1332600.0000, 
sim time next is 1333200.0000, 
raw observation next is [0.7000000000000001, 92.0, 92.5, 0.0, 26.0, 26.0662468979812, 12.40013708854707, 1.0, 1.0, 12.21366830605329], 
processed observation next is [1.0, 0.43478260869565216, 0.4819944598337951, 0.92, 0.30833333333333335, 0.0, 1.0, 1.0094638425687426, 0.1240013708854707, 1.0, 1.0, 0.08724048790038064], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.01031454], dtype=float32), -0.08961274]. 
=============================================
[2019-04-01 19:10:36,262] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 5.2509294e-38], sum to 1.0000
[2019-04-01 19:10:36,263] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0756
[2019-04-01 19:10:36,289] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 100.0, 51.0, 0.0, 26.0, 24.65599970240774, 9.313715049373357, 0.0, 1.0, 4.981907479063783], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1265400.0000, 
sim time next is 1266000.0000, 
raw observation next is [13.8, 100.0, 45.66666666666666, 0.0, 26.0, 24.63974429145132, 9.262158900558559, 0.0, 1.0, 5.636694975363261], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.1522222222222222, 0.0, 1.0, 0.805677755921617, 0.09262158900558559, 0.0, 1.0, 0.040262106966880436], 
reward next is 0.9597, 
noisyNet noise sample is [array([-0.546323], dtype=float32), -0.8132326]. 
=============================================
[2019-04-01 19:10:36,299] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[80.13801 ]
 [80.21003 ]
 [80.27391 ]
 [80.32381 ]
 [80.347626]], R is [[80.19039154]
 [80.35290527]
 [80.51264191]
 [80.66956329]
 [80.82366943]].
[2019-04-01 19:10:48,928] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4712224e-29 1.3616023e-30 1.5983920e-29 9.9999976e-01 6.1344651e-22
 2.7542615e-07 0.0000000e+00], sum to 1.0000
[2019-04-01 19:10:48,928] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6420
[2019-04-01 19:10:48,989] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.1, 86.5, 29.0, 0.0, 26.0, 25.62627856615731, 7.666264318243321, 1.0, 1.0, 34.92704503731522], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2017800.0000, 
sim time next is 2018400.0000, 
raw observation next is [-6.066666666666666, 86.33333333333333, 35.66666666666666, 0.0, 26.0, 25.76644796258098, 7.849248568372967, 1.0, 1.0, 32.65383692814596], 
processed observation next is [1.0, 0.34782608695652173, 0.2945521698984303, 0.8633333333333333, 0.11888888888888886, 0.0, 1.0, 0.9666354232258543, 0.07849248568372967, 1.0, 1.0, 0.2332416923438997], 
reward next is 0.7668, 
noisyNet noise sample is [array([1.2294999], dtype=float32), -0.030666675]. 
=============================================
[2019-04-01 19:10:49,130] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9515486e-01 2.0367088e-34
 4.8451847e-03 0.0000000e+00], sum to 1.0000
[2019-04-01 19:10:49,133] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4164
[2019-04-01 19:10:49,141] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.583333333333334, 65.16666666666666, 0.0, 0.0, 26.0, 26.23231065366116, 15.38335299596931, 1.0, 1.0, 6.610632072510788], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1623000.0000, 
sim time next is 1623600.0000, 
raw observation next is [9.4, 66.0, 0.0, 0.0, 26.0, 26.19727977466417, 15.04611196810371, 1.0, 1.0, 6.514425531553137], 
processed observation next is [1.0, 0.8260869565217391, 0.7229916897506927, 0.66, 0.0, 0.0, 1.0, 1.0281828249520244, 0.15046111968103712, 1.0, 1.0, 0.04653161093966526], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.88307273], dtype=float32), -0.67230445]. 
=============================================
[2019-04-01 19:10:52,392] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4559426e-26 6.3109785e-34 4.3795845e-33 2.7809266e-17 4.7039598e-15
 1.0000000e+00 1.1585037e-22], sum to 1.0000
[2019-04-01 19:10:52,393] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0683
[2019-04-01 19:10:52,401] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 49.0, 155.3333333333333, 0.0, 26.0, 27.14545779952612, 19.48603669967423, 1.0, 1.0, 5.551678384438255], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1606200.0000, 
sim time next is 1606800.0000, 
raw observation next is [13.8, 49.0, 150.1666666666667, 0.0, 26.0, 27.22020297950672, 19.97009989184287, 1.0, 1.0, 5.533800503553598], 
processed observation next is [1.0, 0.6086956521739131, 0.844875346260388, 0.49, 0.5005555555555558, 0.0, 1.0, 1.1743147113581027, 0.1997009989184287, 1.0, 1.0, 0.039527146453954276], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6019458], dtype=float32), 1.6302114]. 
=============================================
[2019-04-01 19:11:30,388] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4791641e-24 0.0000000e+00 1.4288046e-32 2.8646609e-06 5.4577585e-14
 9.9999714e-01 4.9933277e-36], sum to 1.0000
[2019-04-01 19:11:30,388] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1424
[2019-04-01 19:11:30,404] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.9, 77.33333333333334, 0.0, 0.0, 26.0, 24.23012765621221, 5.696793252902985, 0.0, 1.0, 43.49221885742727], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2251200.0000, 
sim time next is 2251800.0000, 
raw observation next is [-7.0, 78.5, 0.0, 0.0, 26.0, 24.20749611455864, 5.671307850418562, 0.0, 1.0, 43.440367918133], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.785, 0.0, 0.0, 1.0, 0.7439280163655201, 0.05671307850418562, 0.0, 1.0, 0.31028834227237856], 
reward next is 0.6897, 
noisyNet noise sample is [array([-0.47193378], dtype=float32), 1.6814826]. 
=============================================
[2019-04-01 19:11:45,008] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-01 19:11:45,009] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 19:11:45,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:11:45,011] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 19:11:45,012] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 19:11:45,012] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:11:45,012] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:11:45,019] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run37
[2019-04-01 19:11:45,041] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run37
[2019-04-01 19:11:45,062] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run37
[2019-04-01 19:11:49,124] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.6182455], dtype=float32), -0.74507827]
[2019-04-01 19:11:49,140] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [7.7, 93.0, 14.0, 0.0, 26.0, 21.55392175035404, 9.138957056250062, 0.0, 1.0, 82.93628543040838]
[2019-04-01 19:11:49,140] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 19:11:49,140] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.3963985e-36], sampled 0.5196248311899848
[2019-04-01 19:12:27,581] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.6182455], dtype=float32), -0.74507827]
[2019-04-01 19:12:27,581] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.966666666666667, 85.0, 0.0, 0.0, 26.0, 25.39897711510228, 10.6413159006071, 0.0, 1.0, 42.40359860139587]
[2019-04-01 19:12:27,581] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 19:12:27,582] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.8001806904133515
[2019-04-01 19:12:48,903] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.6182455], dtype=float32), -0.74507827]
[2019-04-01 19:12:48,903] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.2, 65.0, 0.0, 0.0, 26.0, 25.0079962741516, 7.15328692340798, 0.0, 1.0, 41.90830993417946]
[2019-04-01 19:12:48,904] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 19:12:48,904] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.7630229663414848
[2019-04-01 19:12:52,403] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.6182455], dtype=float32), -0.74507827]
[2019-04-01 19:12:52,403] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.4, 51.0, 0.0, 0.0, 26.0, 25.16084281585355, 8.229811481476508, 0.0, 1.0, 40.22374189786899]
[2019-04-01 19:12:52,403] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 19:12:52,403] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.031354755637241216
[2019-04-01 19:13:10,676] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.6182455], dtype=float32), -0.74507827]
[2019-04-01 19:13:10,677] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [4.0, 100.0, 0.0, 0.0, 26.0, 25.38802734871331, 7.124383208988777, 0.0, 1.0, 50.78436899383391]
[2019-04-01 19:13:10,678] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 19:13:10,679] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4394510e-38 0.0000000e+00
 1.0000000e+00 6.5057144e-37], sampled 0.10813958477394936
[2019-04-01 19:13:26,972] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 19:13:37,974] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.6182455], dtype=float32), -0.74507827]
[2019-04-01 19:13:37,975] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [6.763449917000001, 67.81086337666667, 0.0, 0.0, 26.0, 25.89529128084214, 12.19704241062643, 0.0, 1.0, 14.38600194533589]
[2019-04-01 19:13:37,975] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:13:37,975] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.11111533285007225
[2019-04-01 19:13:46,443] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 19:13:50,467] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 19:13:51,491] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 3600000, evaluation results [3600000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 19:14:00,370] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.9200134e-38
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:14:00,373] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7342
[2019-04-01 19:14:00,389] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5833333333333334, 29.16666666666667, 0.0, 0.0, 26.0, 25.04369115597608, 6.881797161681217, 0.0, 1.0, 40.50676008463787], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2490600.0000, 
sim time next is 2491200.0000, 
raw observation next is [-0.7, 29.0, 0.0, 0.0, 26.0, 25.13257598243182, 6.945581636672924, 0.0, 1.0, 40.2854963459738], 
processed observation next is [0.0, 0.8695652173913043, 0.443213296398892, 0.29, 0.0, 0.0, 1.0, 0.8760822832045457, 0.06945581636672923, 0.0, 1.0, 0.28775354532838426], 
reward next is 0.7122, 
noisyNet noise sample is [array([0.49935222], dtype=float32), -0.87227756]. 
=============================================
[2019-04-01 19:14:04,186] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4119736e-18 4.7506085e-26 6.3503470e-20 9.8911129e-02 8.7726349e-20
 1.0689070e-22 9.0108883e-01], sum to 1.0000
[2019-04-01 19:14:04,189] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9275
[2019-04-01 19:14:04,206] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.1, 82.16666666666667, 0.0, 0.0, 26.0, 24.34873792080205, 5.51085640857679, 0.0, 1.0, 42.38375368127873], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2609400.0000, 
sim time next is 2610000.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 26.0, 24.29510302438329, 5.435530661875241, 0.0, 1.0, 42.49926725191655], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.83, 0.0, 0.0, 1.0, 0.7564432891976131, 0.05435530661875241, 0.0, 1.0, 0.3035661946565468], 
reward next is 0.6964, 
noisyNet noise sample is [array([0.6704954], dtype=float32), -0.29551542]. 
=============================================
[2019-04-01 19:14:04,209] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[69.96211 ]
 [70.029015]
 [70.10353 ]
 [70.18762 ]
 [70.297516]], R is [[69.90068817]
 [69.89894104]
 [69.89801025]
 [69.89759064]
 [69.89749146]].
[2019-04-01 19:14:12,413] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4657394e-24 4.7913569e-24
 1.0000000e+00 3.3969025e-19], sum to 1.0000
[2019-04-01 19:14:12,418] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2779
[2019-04-01 19:14:12,444] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.66666666666667, 83.0, 0.0, 0.0, 26.0, 23.1924943051784, 5.501284478900371, 0.0, 1.0, 42.8497476969733], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2698800.0000, 
sim time next is 2699400.0000, 
raw observation next is [-15.83333333333333, 83.0, 0.0, 0.0, 26.0, 23.10083013819797, 5.546695235880904, 0.0, 1.0, 42.79293562333108], 
processed observation next is [1.0, 0.21739130434782608, 0.02400738688827338, 0.83, 0.0, 0.0, 1.0, 0.5858328768854244, 0.05546695235880904, 0.0, 1.0, 0.3056638258809363], 
reward next is 0.6943, 
noisyNet noise sample is [array([-0.07305501], dtype=float32), -0.6916412]. 
=============================================
[2019-04-01 19:14:16,659] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 4.0277188e-22
 2.2970271e-22 9.4104644e-28], sum to 1.0000
[2019-04-01 19:14:16,660] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2088
[2019-04-01 19:14:16,670] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 25.08288992966011, 7.055011353298322, 0.0, 1.0, 54.65068965606986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2865600.0000, 
sim time next is 2866200.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 25.08224033177875, 7.03269361267485, 0.0, 1.0, 54.6350571491764], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 1.0, 0.8688914759683927, 0.0703269361267485, 0.0, 1.0, 0.39025040820840284], 
reward next is 0.6097, 
noisyNet noise sample is [array([1.6304139], dtype=float32), 0.6896479]. 
=============================================
[2019-04-01 19:14:21,700] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1397061e-33 0.0000000e+00 3.6704604e-35 1.9291203e-12 6.1744484e-22
 1.0000000e+00 1.4645843e-10], sum to 1.0000
[2019-04-01 19:14:21,703] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8298
[2019-04-01 19:14:21,728] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 63.33333333333333, 0.0, 0.0, 26.0, 24.60928890178694, 5.963007783220289, 0.0, 1.0, 42.26775558604415], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3393600.0000, 
sim time next is 3394200.0000, 
raw observation next is [-3.0, 64.16666666666667, 0.0, 0.0, 26.0, 24.5619526523002, 5.877434605351152, 0.0, 1.0, 42.21849132232767], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6416666666666667, 0.0, 0.0, 1.0, 0.7945646646143142, 0.05877434605351153, 0.0, 1.0, 0.3015606523023405], 
reward next is 0.6984, 
noisyNet noise sample is [array([1.5819223], dtype=float32), -0.5292246]. 
=============================================
[2019-04-01 19:14:31,812] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 19:14:31,813] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8679
[2019-04-01 19:14:31,837] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 70.66666666666666, 579.0, 26.0, 26.91303791239273, 18.08278136220058, 1.0, 1.0, 8.656496348177967], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3514200.0000, 
sim time next is 3514800.0000, 
raw observation next is [3.0, 49.0, 66.33333333333334, 552.0, 26.0, 27.07478250995315, 14.43148368387632, 1.0, 1.0, 9.00861412186851], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.22111111111111115, 0.6099447513812155, 1.0, 1.1535403585647355, 0.1443148368387632, 1.0, 1.0, 0.06434724372763222], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1270537], dtype=float32), -0.23385513]. 
=============================================
[2019-04-01 19:14:39,961] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 8.3444659e-35 0.0000000e+00 7.5517272e-26 1.1128607e-37
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 19:14:39,964] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3015
[2019-04-01 19:14:39,981] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 101.0, 763.0, 26.0, 25.58929475469395, 16.14157591338723, 1.0, 1.0, 1.790210766751556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3162600.0000, 
sim time next is 3163200.0000, 
raw observation next is [7.0, 100.0, 98.16666666666667, 749.0, 26.0, 26.15529935548634, 18.05563233316628, 1.0, 1.0, 1.712330941101199], 
processed observation next is [1.0, 0.6086956521739131, 0.6565096952908588, 1.0, 0.32722222222222225, 0.8276243093922652, 1.0, 1.022185622212334, 0.1805563233316628, 1.0, 1.0, 0.012230935293579994], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.63909364], dtype=float32), 0.7974717]. 
=============================================
[2019-04-01 19:14:41,238] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 8.946424e-37 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 19:14:41,239] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8749
[2019-04-01 19:14:41,254] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 73.0, 0.0, 0.0, 26.0, 25.27661858930225, 7.73341350965472, 0.0, 1.0, 41.06013183484721], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3811200.0000, 
sim time next is 3811800.0000, 
raw observation next is [-4.0, 72.0, 0.0, 0.0, 26.0, 25.12444559827598, 7.533584810402378, 0.0, 1.0, 42.90883448058079], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.72, 0.0, 0.0, 1.0, 0.8749207997537114, 0.07533584810402379, 0.0, 1.0, 0.30649167486129136], 
reward next is 0.6935, 
noisyNet noise sample is [array([1.3406591], dtype=float32), 0.59548044]. 
=============================================
[2019-04-01 19:14:47,974] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 7.10022e-36
 1.00000e+00], sum to 1.0000
[2019-04-01 19:14:47,974] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8604
[2019-04-01 19:14:47,984] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 51.0, 115.0, 829.5, 26.0, 25.93535520885288, 13.13350398959101, 1.0, 1.0, 10.06202384231551], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3848400.0000, 
sim time next is 3849000.0000, 
raw observation next is [1.166666666666667, 50.5, 114.3333333333333, 827.6666666666666, 26.0, 26.15029363435918, 14.22105838532953, 1.0, 1.0, 11.09765498032237], 
processed observation next is [1.0, 0.5652173913043478, 0.49492151431209613, 0.505, 0.381111111111111, 0.9145488029465929, 1.0, 1.0214705191941686, 0.1422105838532953, 1.0, 1.0, 0.07926896414515978], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0562512], dtype=float32), -1.0719721]. 
=============================================
[2019-04-01 19:14:47,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[53.797737]
 [53.274773]
 [52.54592 ]
 [52.4289  ]
 [52.075737]], R is [[53.69480515]
 [53.15785599]
 [52.62627792]
 [52.47655487]
 [52.33381271]].
[2019-04-01 19:14:49,692] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4262961e-35 2.0154153e-29
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:14:49,692] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4141
[2019-04-01 19:14:49,705] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 49.0, 0.0, 0.0, 26.0, 25.35647294741797, 10.55151035526705, 0.0, 1.0, 43.98879574885063], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3967200.0000, 
sim time next is 3967800.0000, 
raw observation next is [-8.166666666666668, 49.66666666666667, 0.0, 0.0, 26.0, 25.38215146508421, 9.582639763473146, 0.0, 1.0, 44.46022762005768], 
processed observation next is [1.0, 0.9565217391304348, 0.2363804247460757, 0.4966666666666667, 0.0, 0.0, 1.0, 0.9117359235834586, 0.09582639763473147, 0.0, 1.0, 0.3175730544289834], 
reward next is 0.6824, 
noisyNet noise sample is [array([-0.5326338], dtype=float32), 1.5867368]. 
=============================================
[2019-04-01 19:15:12,246] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.98111808e-32 3.99552734e-34 0.00000000e+00 0.00000000e+00
 8.52092512e-37 1.28694965e-20 1.00000000e+00], sum to 1.0000
[2019-04-01 19:15:12,247] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3137
[2019-04-01 19:15:12,256] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 60.0, 114.6666666666667, 806.3333333333334, 26.0, 26.46758531100301, 12.90508687561272, 1.0, 1.0, 9.346577136440004], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3841800.0000, 
sim time next is 3842400.0000, 
raw observation next is [-1.0, 60.00000000000001, 115.8333333333333, 814.1666666666666, 26.0, 26.36854226155428, 12.88480816934501, 1.0, 1.0, 8.338862023977567], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6000000000000001, 0.386111111111111, 0.8996316758747698, 1.0, 1.0526488945077541, 0.1288480816934501, 1.0, 1.0, 0.05956330017126834], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.21620193], dtype=float32), 0.8569802]. 
=============================================
[2019-04-01 19:15:20,825] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.3365445e-03 9.9466348e-01 3.4096625e-15 1.2012201e-18 1.1098585e-33
 0.0000000e+00 1.8298569e-15], sum to 1.0000
[2019-04-01 19:15:20,826] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3867
[2019-04-01 19:15:20,838] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 37.0, 118.5, 828.5, 19.0, 26.41870996923643, 12.68324948220813, 1.0, 1.0, 10.9124598193865], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4017600.0000, 
sim time next is 4018200.0000, 
raw observation next is [-5.666666666666667, 35.66666666666667, 118.3333333333333, 832.6666666666667, 19.0, 26.44799498666863, 12.82960433249797, 1.0, 1.0, 10.20582150326021], 
processed observation next is [1.0, 0.5217391304347826, 0.30563250230840255, 0.3566666666666667, 0.3944444444444443, 0.9200736648250462, 0.0, 1.0639992838098042, 0.1282960433249797, 1.0, 1.0, 0.07289872502328722], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.18554913], dtype=float32), -0.02352426]. 
=============================================
[2019-04-01 19:15:27,031] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.1898294e-06 1.1522486e-07 2.6352128e-03 1.5464368e-05 5.4019381e-21
 2.5675556e-07 9.9734181e-01], sum to 1.0000
[2019-04-01 19:15:27,032] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0423
[2019-04-01 19:15:27,069] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 36.66666666666666, 94.0, 504.0, 26.0, 25.99379231137709, 10.15745145552677, 1.0, 1.0, 24.26819003410478], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4092000.0000, 
sim time next is 4092600.0000, 
raw observation next is [-3.166666666666667, 37.33333333333334, 96.0, 539.0, 26.0, 26.21828720049454, 10.05042795487399, 1.0, 1.0, 24.40290423349457], 
processed observation next is [1.0, 0.34782608695652173, 0.3748845798707295, 0.3733333333333334, 0.32, 0.5955801104972376, 1.0, 1.0311838857849343, 0.1005042795487399, 1.0, 1.0, 0.1743064588106755], 
reward next is 0.8055, 
noisyNet noise sample is [array([-0.8962861], dtype=float32), 1.122417]. 
=============================================
[2019-04-01 19:15:27,121] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.31669661e-09 3.74794640e-09 7.97954200e-08 3.66738556e-10
 1.20493565e-26 2.74001852e-08 9.99999881e-01], sum to 1.0000
[2019-04-01 19:15:27,124] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4742
[2019-04-01 19:15:27,169] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 38.0, 98.0, 574.0, 26.0, 26.16161662015715, 10.21155045515134, 1.0, 1.0, 21.25339646758817], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4093200.0000, 
sim time next is 4093800.0000, 
raw observation next is [-2.833333333333333, 37.5, 100.0, 609.0, 26.0, 26.17628678421148, 10.36187156483, 1.0, 1.0, 19.78840828072969], 
processed observation next is [1.0, 0.391304347826087, 0.3841181902123731, 0.375, 0.3333333333333333, 0.6729281767955801, 1.0, 1.0251838263159259, 0.1036187156483, 1.0, 1.0, 0.14134577343378352], 
reward next is 0.7139, 
noisyNet noise sample is [array([-0.8962861], dtype=float32), 1.122417]. 
=============================================
[2019-04-01 19:15:30,556] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0162226e-26 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:15:30,559] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6914
[2019-04-01 19:15:30,574] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.416666666666667, 71.83333333333333, 0.0, 0.0, 26.0, 25.63164419370103, 8.16749676134656, 0.0, 1.0, 22.87665162860687], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4327800.0000, 
sim time next is 4328400.0000, 
raw observation next is [4.333333333333334, 71.66666666666667, 0.0, 0.0, 26.0, 25.55745475731099, 7.977945717444946, 0.0, 1.0, 26.39291006065415], 
processed observation next is [1.0, 0.08695652173913043, 0.58264081255771, 0.7166666666666667, 0.0, 0.0, 1.0, 0.9367792510444269, 0.07977945717444947, 0.0, 1.0, 0.18852078614752962], 
reward next is 0.8115, 
noisyNet noise sample is [array([-0.5931233], dtype=float32), -0.28904608]. 
=============================================
[2019-04-01 19:15:31,041] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:15:31,042] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7341
[2019-04-01 19:15:31,061] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.69126368939733, 12.55367326491395, 0.0, 1.0, 29.15575951548432], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4483800.0000, 
sim time next is 4484400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.69124703159377, 12.32111276647866, 0.0, 1.0, 28.03877694655482], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 1.0, 0.9558924330848245, 0.12321112766478659, 0.0, 1.0, 0.20027697818967727], 
reward next is 0.7997, 
noisyNet noise sample is [array([-1.1243565], dtype=float32), -0.14566123]. 
=============================================
[2019-04-01 19:15:31,949] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:15:31,950] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3399
[2019-04-01 19:15:32,039] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 55.0, 26.5, 26.0, 25.31949068581572, 7.35565980349256, 0.0, 1.0, 38.50524250940236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4262400.0000, 
sim time next is 4263000.0000, 
raw observation next is [3.0, 49.66666666666667, 73.33333333333334, 35.33333333333334, 26.0, 25.32282797427033, 7.916696023753663, 0.0, 1.0, 54.45243596193637], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.4966666666666667, 0.24444444444444446, 0.03904235727440148, 1.0, 0.9032611391814755, 0.07916696023753662, 0.0, 1.0, 0.38894597115668833], 
reward next is 0.6111, 
noisyNet noise sample is [array([-1.8579555], dtype=float32), 3.0190535]. 
=============================================
[2019-04-01 19:15:32,052] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[71.155655]
 [71.011566]
 [70.87228 ]
 [70.76369 ]
 [70.80347 ]], R is [[70.72679138]
 [70.74448395]
 [70.76137543]
 [70.77766418]
 [70.79349518]].
[2019-04-01 19:15:32,318] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 6.1792743e-31], sum to 1.0000
[2019-04-01 19:15:32,320] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6098
[2019-04-01 19:15:32,357] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.266666666666667, 42.33333333333334, 0.0, 0.0, 26.0, 24.99906098387903, 7.703521513500526, 0.0, 1.0, 46.78323994408695], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4216800.0000, 
sim time next is 4217400.0000, 
raw observation next is [1.2, 42.5, 0.0, 0.0, 26.0, 25.07907491834817, 8.255567303878683, 0.0, 1.0, 50.36219130541596], 
processed observation next is [0.0, 0.8260869565217391, 0.4958448753462604, 0.425, 0.0, 0.0, 1.0, 0.8684392740497385, 0.08255567303878683, 0.0, 1.0, 0.3597299378958283], 
reward next is 0.6403, 
noisyNet noise sample is [array([-1.4680177], dtype=float32), -0.011210755]. 
=============================================
[2019-04-01 19:15:32,893] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 2.0337147e-33 3.3856021e-38 0.0000000e+00
 1.0000000e+00 2.1207752e-22], sum to 1.0000
[2019-04-01 19:15:32,893] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0712
[2019-04-01 19:15:32,944] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 50.33333333333334, 91.66666666666667, 44.16666666666667, 26.0, 25.54293450055872, 8.26293970732077, 0.0, 1.0, 36.27176879862344], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4263600.0000, 
sim time next is 4264200.0000, 
raw observation next is [3.0, 51.0, 110.0, 53.0, 26.0, 25.65062778282065, 8.478520395547603, 0.0, 1.0, 31.75294946929317], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.51, 0.36666666666666664, 0.05856353591160221, 1.0, 0.9500896832600928, 0.08478520395547604, 0.0, 1.0, 0.22680678192352266], 
reward next is 0.7732, 
noisyNet noise sample is [array([0.23934247], dtype=float32), -0.0956224]. 
=============================================
[2019-04-01 19:15:36,553] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.0019258e-38 0.0000000e+00 7.9235118e-31 0.0000000e+00 5.5771244e-07
 9.9999940e-01 7.9413365e-23], sum to 1.0000
[2019-04-01 19:15:36,553] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1092
[2019-04-01 19:15:36,562] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 54.5, 188.0, 717.0, 26.0, 25.30099574421251, 9.26730526819527, 0.0, 1.0, 4.330119441726286], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4282200.0000, 
sim time next is 4282800.0000, 
raw observation next is [7.0, 55.33333333333334, 194.8333333333333, 661.6666666666666, 26.0, 25.30647480382207, 9.282830774079287, 0.0, 1.0, 3.580776023848651], 
processed observation next is [0.0, 0.5652173913043478, 0.6565096952908588, 0.5533333333333335, 0.6494444444444443, 0.731123388581952, 1.0, 0.9009249719745814, 0.09282830774079287, 0.0, 1.0, 0.025576971598918936], 
reward next is 0.9744, 
noisyNet noise sample is [array([0.16877337], dtype=float32), 0.3352664]. 
=============================================
[2019-04-01 19:15:38,342] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 9.9999869e-01 1.2971745e-06], sum to 1.0000
[2019-04-01 19:15:38,344] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0217
[2019-04-01 19:15:38,358] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.966666666666667, 56.66666666666667, 100.6666666666667, 622.0, 26.0, 25.44511860800787, 9.874075178244832, 0.0, 1.0, 2.483215228506826], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4290600.0000, 
sim time next is 4291200.0000, 
raw observation next is [7.0, 56.0, 93.0, 605.5, 26.0, 25.48418575142578, 9.906106117923828, 0.0, 1.0, 2.535231722268177], 
processed observation next is [0.0, 0.6956521739130435, 0.6565096952908588, 0.56, 0.31, 0.669060773480663, 1.0, 0.9263122502036829, 0.09906106117923828, 0.0, 1.0, 0.01810879801620126], 
reward next is 0.9819, 
noisyNet noise sample is [array([-0.23259667], dtype=float32), 0.8161367]. 
=============================================
[2019-04-01 19:15:39,835] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:15:39,839] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1787
[2019-04-01 19:15:39,858] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.75, 67.0, 0.0, 0.0, 26.0, 25.48655094079791, 12.06548586101142, 0.0, 1.0, 40.52312113398854], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4419000.0000, 
sim time next is 4419600.0000, 
raw observation next is [4.666666666666667, 67.0, 0.0, 0.0, 26.0, 25.60598953863678, 12.71482238185343, 0.0, 1.0, 29.687301581301], 
processed observation next is [1.0, 0.13043478260869565, 0.5918744228993538, 0.67, 0.0, 0.0, 1.0, 0.9437127912338258, 0.1271482238185343, 0.0, 1.0, 0.21205215415215], 
reward next is 0.7879, 
noisyNet noise sample is [array([1.9879779], dtype=float32), -0.2869665]. 
=============================================
[2019-04-01 19:15:40,987] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2958044e-10 9.9997771e-01 5.1071384e-14 2.8100746e-23 4.8614860e-17
 0.0000000e+00 2.2234983e-05], sum to 1.0000
[2019-04-01 19:15:40,993] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7168
[2019-04-01 19:15:41,002] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 24.89351019383703, 8.060909310890223, 1.0, 1.0, 12.03681274354406], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4471800.0000, 
sim time next is 4472400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 24.86490060552717, 7.905748503238409, 1.0, 1.0, 12.26387382223564], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.0, 0.8378429436467384, 0.07905748503238409, 1.0, 1.0, 0.08759909873025457], 
reward next is 0.9124, 
noisyNet noise sample is [array([-1.1320528], dtype=float32), -0.28078836]. 
=============================================
[2019-04-01 19:15:43,000] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.1963141e-37 1.6981570e-10 7.3329391e-26
 1.0000000e+00 6.3160568e-35], sum to 1.0000
[2019-04-01 19:15:43,004] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9067
[2019-04-01 19:15:43,018] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 26.0, 24.81695818976885, 6.976677419795426, 0.0, 1.0, 40.40937581877613], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4515600.0000, 
sim time next is 4516200.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 24.77475928403454, 6.970330186369469, 0.0, 1.0, 40.35442301191855], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 1.0, 0.8249656120049345, 0.0697033018636947, 0.0, 1.0, 0.28824587865656104], 
reward next is 0.7118, 
noisyNet noise sample is [array([-0.2501786], dtype=float32), -0.48337185]. 
=============================================
[2019-04-01 19:15:44,025] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.4578770e-32 9.9259338e-33 6.1206701e-27 1.4997953e-11 9.3672352e-05
 0.0000000e+00 9.9990630e-01], sum to 1.0000
[2019-04-01 19:15:44,025] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3582
[2019-04-01 19:15:44,039] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.27372447623197, 8.477397050028323, 0.0, 1.0, 40.13466318450993], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4516800.0000, 
sim time next is 4517400.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.31807127021447, 8.494286143149568, 0.0, 1.0, 40.03306813050544], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 1.0, 0.9025816100306387, 0.08494286143149568, 0.0, 1.0, 0.28595048664646744], 
reward next is 0.7140, 
noisyNet noise sample is [array([0.7915196], dtype=float32), -0.08640865]. 
=============================================
[2019-04-01 19:15:44,781] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.5018310e-27 5.9976552e-26
 1.0000000e+00 1.8223798e-23], sum to 1.0000
[2019-04-01 19:15:44,782] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2352
[2019-04-01 19:15:44,807] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.27347579590507, 8.476465333064638, 0.0, 1.0, 40.13448855571506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4516800.0000, 
sim time next is 4517400.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.31781231768937, 8.4933257767426, 0.0, 1.0, 40.03292193791275], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 1.0, 0.9025446168127671, 0.084933257767426, 0.0, 1.0, 0.2859494424136625], 
reward next is 0.7141, 
noisyNet noise sample is [array([0.19122946], dtype=float32), -2.042382]. 
=============================================
[2019-04-01 19:15:45,778] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.9588493e-27 1.0000000e+00], sum to 1.0000
[2019-04-01 19:15:45,781] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4380
[2019-04-01 19:15:45,794] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.833333333333333, 49.0, 126.3333333333333, 843.6666666666667, 26.0, 25.95697304596491, 13.96707438335009, 1.0, 1.0, 2.119634532694849], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4625400.0000, 
sim time next is 4626000.0000, 
raw observation next is [4.0, 49.0, 129.5, 836.0, 26.0, 26.24927490795389, 15.54864371674076, 1.0, 1.0, 2.093493348884112], 
processed observation next is [1.0, 0.5652173913043478, 0.5734072022160666, 0.49, 0.43166666666666664, 0.9237569060773481, 1.0, 1.0356107011362699, 0.1554864371674076, 1.0, 1.0, 0.014953523920600799], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0216278], dtype=float32), 0.4416847]. 
=============================================
[2019-04-01 19:15:45,802] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[40.86237 ]
 [40.464966]
 [40.146675]
 [39.966644]
 [39.57708 ]], R is [[40.82308197]
 [40.41485214]
 [40.01070404]
 [39.6105957 ]
 [39.21448898]].
[2019-04-01 19:15:45,892] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6613029e-31 1.0342304e-29 2.3188393e-33 1.4050761e-23 4.3676555e-18
 1.5160745e-26 1.0000000e+00], sum to 1.0000
[2019-04-01 19:15:45,892] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0363
[2019-04-01 19:15:45,897] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 55.5, 152.3333333333333, 5.999999999999998, 26.0, 26.26814676091678, 11.43550780400819, 1.0, 1.0, 8.361998428813013], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4533000.0000, 
sim time next is 4533600.0000, 
raw observation next is [2.0, 54.00000000000001, 135.1666666666667, 2.999999999999999, 26.0, 26.28583040489995, 11.44047433908778, 1.0, 1.0, 8.194210140557445], 
processed observation next is [1.0, 0.4782608695652174, 0.518005540166205, 0.54, 0.4505555555555557, 0.0033149171270718224, 1.0, 1.0408329149857072, 0.1144047433908778, 1.0, 1.0, 0.058530072432553176], 
reward next is 0.3653, 
noisyNet noise sample is [array([1.0055017], dtype=float32), -0.8280727]. 
=============================================
[2019-04-01 19:15:45,911] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:15:45,911] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:15:45,935] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run28
[2019-04-01 19:15:50,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:15:50,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:15:50,416] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run28
[2019-04-01 19:16:02,423] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:02,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:02,436] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run28
[2019-04-01 19:16:08,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:08,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:08,643] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run28
[2019-04-01 19:16:09,247] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:09,247] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:09,262] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run28
[2019-04-01 19:16:10,845] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 4.739801e-24 1.000000e+00], sum to 1.0000
[2019-04-01 19:16:10,845] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4714
[2019-04-01 19:16:10,860] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.666666666666667, 39.33333333333334, 115.3333333333333, 790.5, 26.0, 27.07272157223942, 17.19698934194507, 1.0, 1.0, 8.394169489481724], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5048400.0000, 
sim time next is 5049000.0000, 
raw observation next is [4.0, 38.5, 116.0, 809.0, 26.0, 27.18455140066408, 17.83838994080141, 1.0, 1.0, 7.614979397956136], 
processed observation next is [1.0, 0.43478260869565216, 0.5734072022160666, 0.385, 0.38666666666666666, 0.8939226519337017, 1.0, 1.1692216286662973, 0.1783838994080141, 1.0, 1.0, 0.054392709985400974], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5699983], dtype=float32), -1.2719799]. 
=============================================
[2019-04-01 19:16:10,864] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[15.541263]
 [15.707677]
 [16.082722]
 [16.54747 ]
 [17.211588]], R is [[15.43796253]
 [15.28358269]
 [15.13074684]
 [14.97943974]
 [14.82964516]].
[2019-04-01 19:16:12,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:12,489] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:12,498] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run28
[2019-04-01 19:16:12,870] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:12,871] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:12,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run28
[2019-04-01 19:16:12,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:12,946] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:12,951] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run28
[2019-04-01 19:16:13,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:13,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:13,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run28
[2019-04-01 19:16:13,366] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:13,367] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:13,376] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run28
[2019-04-01 19:16:13,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:13,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:13,536] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run28
[2019-04-01 19:16:14,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:14,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:14,136] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run28
[2019-04-01 19:16:14,570] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:14,570] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:14,574] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run28
[2019-04-01 19:16:15,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:15,002] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:15,006] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run28
[2019-04-01 19:16:15,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:15,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:15,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run28
[2019-04-01 19:16:16,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:16:16,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:16:16,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run28
[2019-04-01 19:16:25,224] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:16:25,225] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1889
[2019-04-01 19:16:25,241] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.51666666666667, 53.16666666666666, 0.0, 0.0, 26.0, 24.09943204679628, 5.769029815157093, 0.0, 1.0, 44.14660927143451], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 427800.0000, 
sim time next is 428400.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 26.0, 24.04015179218542, 5.723688438705271, 0.0, 1.0, 44.18013562885609], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 1.0, 0.720021684597917, 0.05723688438705271, 0.0, 1.0, 0.3155723973489721], 
reward next is 0.6844, 
noisyNet noise sample is [array([0.02425221], dtype=float32), -0.14778443]. 
=============================================
[2019-04-01 19:16:34,095] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.7130976e-32 1.1841394e-33 0.0000000e+00 0.0000000e+00 2.0814960e-35
 1.5894055e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 19:16:34,095] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5118
[2019-04-01 19:16:34,132] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.899999999999999, 61.0, 133.0, 563.3333333333334, 26.0, 25.53834987700818, 9.008424235702037, 1.0, 1.0, 43.64829911594011], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 132600.0000, 
sim time next is 133200.0000, 
raw observation next is [-7.8, 61.0, 134.5, 543.5, 26.0, 25.55163571408404, 9.266004697263241, 1.0, 1.0, 41.80587634213888], 
processed observation next is [1.0, 0.5652173913043478, 0.24653739612188366, 0.61, 0.4483333333333333, 0.6005524861878453, 1.0, 0.9359479591548627, 0.09266004697263241, 1.0, 1.0, 0.29861340244384915], 
reward next is 0.7014, 
noisyNet noise sample is [array([-0.10167994], dtype=float32), 1.016057]. 
=============================================
[2019-04-01 19:16:44,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 3.7538151e-38 7.5798016e-28 1.0296829e-23
 1.0000000e+00 2.5933390e-29], sum to 1.0000
[2019-04-01 19:16:44,230] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4457
[2019-04-01 19:16:44,246] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-14.91666666666667, 69.0, 0.0, 0.0, 26.0, 23.27218394869478, 5.796129226154785, 0.0, 1.0, 47.57658006214069], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 352200.0000, 
sim time next is 352800.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.26153076239245, 5.875157682968205, 0.0, 1.0, 47.78357124070722], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 1.0, 0.6087901089132072, 0.05875157682968205, 0.0, 1.0, 0.3413112231479087], 
reward next is 0.6587, 
noisyNet noise sample is [array([1.0733172], dtype=float32), -0.115529746]. 
=============================================
[2019-04-01 19:16:44,976] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1174123e-37 0.0000000e+00
 1.0000000e+00 4.1943458e-25], sum to 1.0000
[2019-04-01 19:16:44,977] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0777
[2019-04-01 19:16:44,998] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.51666666666667, 69.5, 0.0, 0.0, 26.0, 22.81600647431741, 6.311949873915156, 0.0, 1.0, 47.12932276332042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 280200.0000, 
sim time next is 280800.0000, 
raw observation next is [-11.7, 70.0, 0.0, 0.0, 26.0, 22.76041710607765, 6.407320764173669, 0.0, 1.0, 47.10951104330902], 
processed observation next is [1.0, 0.2608695652173913, 0.13850415512465375, 0.7, 0.0, 0.0, 1.0, 0.5372024437253786, 0.06407320764173668, 0.0, 1.0, 0.33649650745220727], 
reward next is 0.6635, 
noisyNet noise sample is [array([0.37381813], dtype=float32), 0.7069936]. 
=============================================
[2019-04-01 19:16:53,546] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 8.3743496e-19 3.1361302e-16
 1.0000000e+00 4.4374529e-10], sum to 1.0000
[2019-04-01 19:16:53,548] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8084
[2019-04-01 19:16:53,615] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 91.0, 89.0, 103.5, 26.0, 25.13991818768014, 7.203964077799891, 0.0, 1.0, 32.13835144554853], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 550800.0000, 
sim time next is 551400.0000, 
raw observation next is [-0.09999999999999999, 90.33333333333334, 107.3333333333333, 103.3333333333333, 26.0, 25.07731614527082, 7.091801939091283, 0.0, 1.0, 30.23175382682633], 
processed observation next is [0.0, 0.391304347826087, 0.4598337950138504, 0.9033333333333334, 0.3577777777777777, 0.11418047882136276, 1.0, 0.8681880207529744, 0.07091801939091283, 0.0, 1.0, 0.21594109876304524], 
reward next is 0.7841, 
noisyNet noise sample is [array([-0.5381848], dtype=float32), 0.72288483]. 
=============================================
[2019-04-01 19:17:15,071] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5406195e-33 0.0000000e+00 2.2312336e-31 2.3745288e-16 7.5703795e-36
 1.0000000e+00 8.1772256e-22], sum to 1.0000
[2019-04-01 19:17:15,072] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8347
[2019-04-01 19:17:15,089] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.566666666666667, 69.66666666666667, 0.0, 0.0, 26.0, 24.74983360906251, 6.028588154978483, 0.0, 1.0, 41.25404874497093], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 685200.0000, 
sim time next is 685800.0000, 
raw observation next is [-3.65, 70.0, 0.0, 0.0, 26.0, 24.70863041445211, 5.989541794664497, 0.0, 1.0, 41.18214321913346], 
processed observation next is [0.0, 0.9565217391304348, 0.3614958448753463, 0.7, 0.0, 0.0, 1.0, 0.8155186306360156, 0.05989541794664497, 0.0, 1.0, 0.2941581658509533], 
reward next is 0.7058, 
noisyNet noise sample is [array([-0.76119334], dtype=float32), -0.37521455]. 
=============================================
[2019-04-01 19:17:17,077] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.4669421e-33 9.9999976e-01
 2.6360163e-07 0.0000000e+00], sum to 1.0000
[2019-04-01 19:17:17,080] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1470
[2019-04-01 19:17:17,130] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.97028285483148, 7.022616815726392, 0.0, 1.0, 38.38114540229994], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 673200.0000, 
sim time next is 673800.0000, 
raw observation next is [-2.383333333333333, 62.5, 0.0, 0.0, 26.0, 24.95588621413022, 6.93741170803595, 0.0, 1.0, 45.14142414552416], 
processed observation next is [0.0, 0.8260869565217391, 0.3965835641735919, 0.625, 0.0, 0.0, 1.0, 0.8508408877328887, 0.0693741170803595, 0.0, 1.0, 0.32243874389660115], 
reward next is 0.6776, 
noisyNet noise sample is [array([0.55596125], dtype=float32), -0.7516997]. 
=============================================
[2019-04-01 19:17:23,696] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0648774e-37 7.1490553e-37 1.2902755e-38 5.7168640e-36 7.6373408e-32
 1.0000000e+00 1.3495955e-37], sum to 1.0000
[2019-04-01 19:17:23,696] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1499
[2019-04-01 19:17:23,751] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 80.16666666666667, 69.66666666666666, 0.0, 26.0, 26.32977731541466, 10.84942352922305, 1.0, 1.0, 34.80708948476379], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 828600.0000, 
sim time next is 829200.0000, 
raw observation next is [-3.9, 81.33333333333334, 64.33333333333333, 0.0, 26.0, 26.48069087277802, 11.04002947525565, 1.0, 1.0, 34.06480722881106], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.8133333333333335, 0.21444444444444444, 0.0, 1.0, 1.0686701246825743, 0.1104002947525565, 1.0, 1.0, 0.24332005163436474], 
reward next is 0.3407, 
noisyNet noise sample is [array([0.5411051], dtype=float32), -1.3863665]. 
=============================================
[2019-04-01 19:17:26,246] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2238454e-26 3.4406960e-19
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:17:26,247] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1550
[2019-04-01 19:17:26,271] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.81032234466055, 7.019650680751515, 0.0, 1.0, 41.400126686281], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 853800.0000, 
sim time next is 854400.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.7870019469291, 6.943801471460278, 0.0, 1.0, 41.55538627929374], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 1.0, 0.8267145638470141, 0.06943801471460277, 0.0, 1.0, 0.296824187709241], 
reward next is 0.7032, 
noisyNet noise sample is [array([-0.53101176], dtype=float32), 0.8951627]. 
=============================================
[2019-04-01 19:17:27,530] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.4495449e-32 0.0000000e+00 8.6447405e-33 9.9742150e-01 1.1064122e-04
 2.4678472e-03 6.3812161e-10], sum to 1.0000
[2019-04-01 19:17:27,531] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3607
[2019-04-01 19:17:27,548] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 98.0, 0.0, 0.0, 26.0, 25.09310080132659, 8.772388454182414, 0.0, 1.0, 39.95622438317078], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 941400.0000, 
sim time next is 942000.0000, 
raw observation next is [5.0, 97.33333333333334, 0.0, 0.0, 26.0, 25.13553654076933, 8.851011640119077, 0.0, 1.0, 39.73688974812577], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.9733333333333334, 0.0, 0.0, 1.0, 0.8765052201099043, 0.08851011640119077, 0.0, 1.0, 0.2838349267723269], 
reward next is 0.7162, 
noisyNet noise sample is [array([0.9175983], dtype=float32), -0.024389688]. 
=============================================
[2019-04-01 19:17:27,560] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[70.64266]
 [70.71044]
 [70.65994]
 [70.35643]
 [69.98617]], R is [[70.82342529]
 [70.82978821]
 [70.83441925]
 [70.83706665]
 [70.83690643]].
[2019-04-01 19:17:30,570] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00
 6.35193e-15], sum to 1.0000
[2019-04-01 19:17:30,573] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4299
[2019-04-01 19:17:30,580] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.71666666666667, 63.33333333333333, 46.0, 0.0, 26.0, 25.02124560653599, 10.14165097571785, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1181400.0000, 
sim time next is 1182000.0000, 
raw observation next is [18.63333333333333, 63.66666666666667, 37.5, 0.0, 26.0, 25.00368455835274, 10.04818961128724, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.9787626962142197, 0.6366666666666667, 0.125, 0.0, 1.0, 0.8576692226218202, 0.10048189611287241, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9056612], dtype=float32), -1.2343696]. 
=============================================
[2019-04-01 19:17:30,588] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[83.718254]
 [83.88663 ]
 [84.01744 ]
 [84.121445]
 [84.21253 ]], R is [[83.69232941]
 [83.85540771]
 [84.01685333]
 [84.17668915]
 [84.33492279]].
[2019-04-01 19:17:33,668] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1406542e-35 4.7286206e-32 0.0000000e+00 2.0486682e-22 2.8483101e-38
 6.2032946e-08 9.9999988e-01], sum to 1.0000
[2019-04-01 19:17:33,670] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5973
[2019-04-01 19:17:33,677] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.1, 86.0, 122.6666666666667, 0.0, 26.0, 26.73269842200393, 15.53871584989719, 1.0, 1.0, 6.188102180807534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 993000.0000, 
sim time next is 993600.0000, 
raw observation next is [12.2, 86.0, 124.0, 0.0, 26.0, 26.75065902428992, 15.81815008027959, 1.0, 1.0, 5.91995783419391], 
processed observation next is [1.0, 0.5217391304347826, 0.8005540166204987, 0.86, 0.41333333333333333, 0.0, 1.0, 1.1072370034699885, 0.15818150080279592, 1.0, 1.0, 0.04228541310138507], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5598149], dtype=float32), 0.4346757]. 
=============================================
[2019-04-01 19:17:34,189] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00
 8.8696875e-16 6.8044800e-27], sum to 1.0000
[2019-04-01 19:17:34,190] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3143
[2019-04-01 19:17:34,199] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.233333333333333, 78.0, 0.0, 0.0, 26.0, 25.96349389885369, 12.64910550731773, 0.0, 1.0, 19.0744922759818], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1549200.0000, 
sim time next is 1549800.0000, 
raw observation next is [6.05, 79.0, 0.0, 0.0, 26.0, 25.79913901574785, 11.96390499077521, 0.0, 1.0, 16.92151772123469], 
processed observation next is [1.0, 0.9565217391304348, 0.6301939058171746, 0.79, 0.0, 0.0, 1.0, 0.9713055736782644, 0.1196390499077521, 0.0, 1.0, 0.12086798372310492], 
reward next is 0.8791, 
noisyNet noise sample is [array([-0.49844638], dtype=float32), -0.6752696]. 
=============================================
[2019-04-01 19:17:35,633] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-01 19:17:35,634] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 19:17:35,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:17:35,636] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 19:17:35,637] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 19:17:35,637] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:17:35,638] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:17:35,642] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run38
[2019-04-01 19:17:35,666] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run38
[2019-04-01 19:17:35,666] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run38
[2019-04-01 19:18:24,976] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.5730652], dtype=float32), -0.70761925]
[2019-04-01 19:18:24,976] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.7, 78.0, 0.0, 0.0, 26.0, 23.34591030508214, 5.36210152991881, 0.0, 1.0, 46.49046998405801]
[2019-04-01 19:18:24,976] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 19:18:24,977] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.6896763129428947
[2019-04-01 19:18:45,821] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.5730652], dtype=float32), -0.70761925]
[2019-04-01 19:18:45,821] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.7, 47.33333333333333, 0.0, 0.0, 26.0, 25.00188745836813, 5.845291107549717, 0.0, 1.0, 37.99383199668102]
[2019-04-01 19:18:45,821] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 19:18:45,822] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.9139150404411849
[2019-04-01 19:19:14,988] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.5730652], dtype=float32), -0.70761925]
[2019-04-01 19:19:14,988] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.18662482168977, 7.634846783297796, 0.0, 1.0, 40.71619756525885]
[2019-04-01 19:19:14,988] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 19:19:14,988] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.41107248870155344
[2019-04-01 19:19:18,126] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 19:19:27,454] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.5730652], dtype=float32), -0.70761925]
[2019-04-01 19:19:27,454] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.869860702166667, 84.26249858333333, 211.6323793, 50.10717803333333, 26.0, 25.82173033926406, 11.44476223094122, 1.0, 1.0, 8.300371874860588]
[2019-04-01 19:19:27,455] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:19:27,455] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.6564286e-33], sampled 0.462820541422019
[2019-04-01 19:19:36,562] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 19:19:39,519] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 19:19:40,543] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 3700000, evaluation results [3700000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 19:19:41,275] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8165481e-32 0.0000000e+00
 1.0000000e+00 4.3152900e-25], sum to 1.0000
[2019-04-01 19:19:41,278] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7132
[2019-04-01 19:19:41,287] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 26.0, 25.80453883978753, 12.83584806767485, 0.0, 1.0, 16.1675333450773], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1060200.0000, 
sim time next is 1060800.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 26.0, 25.79944675247971, 12.60451452738889, 0.0, 1.0, 15.33933644567468], 
processed observation next is [1.0, 0.2608695652173913, 0.8310249307479226, 0.8, 0.0, 0.0, 1.0, 0.97134953606853, 0.1260451452738889, 0.0, 1.0, 0.10956668889767629], 
reward next is 0.8904, 
noisyNet noise sample is [array([1.0910614], dtype=float32), 0.39263588]. 
=============================================
[2019-04-01 19:19:47,859] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.8749422e-27
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:19:47,863] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4777
[2019-04-01 19:19:47,868] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.34623087059323, 7.572282413122117, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1205400.0000, 
sim time next is 1206000.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 26.0, 24.32173965628719, 7.505415151466527, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 1.0, 0.922437673130194, 0.75, 0.0, 0.0, 1.0, 0.7602485223267413, 0.07505415151466527, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6751671], dtype=float32), -0.16019154]. 
=============================================
[2019-04-01 19:19:47,878] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[98.579926]
 [98.59401 ]
 [98.63142 ]
 [98.67569 ]
 [98.72931 ]], R is [[98.62120819]
 [98.63499451]
 [98.64864349]
 [98.66215515]
 [98.67553711]].
[2019-04-01 19:19:49,414] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:19:49,417] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7548
[2019-04-01 19:19:49,422] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.3, 63.0, 0.0, 0.0, 26.0, 24.80179657434966, 9.199372206814493, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1188000.0000, 
sim time next is 1188600.0000, 
raw observation next is [18.2, 63.66666666666666, 0.0, 0.0, 26.0, 24.78378621286429, 9.127435729663496, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.9667590027700832, 0.6366666666666666, 0.0, 0.0, 1.0, 0.826255173266327, 0.09127435729663497, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4305571], dtype=float32), 1.6986963]. 
=============================================
[2019-04-01 19:19:55,122] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4997711e-26 6.3927113e-25 4.6148085e-22 1.0000000e+00 8.7128816e-15
 8.3086815e-32 0.0000000e+00], sum to 1.0000
[2019-04-01 19:19:55,123] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1038
[2019-04-01 19:19:55,130] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 94.33333333333334, 0.0, 0.0, 19.5, 21.86696876810129, 8.038879665245993, 0.0, 1.0, 6.725081770148083], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1480200.0000, 
sim time next is 1480800.0000, 
raw observation next is [2.2, 94.66666666666667, 0.0, 0.0, 19.5, 21.78669457702816, 7.97320261918671, 0.0, 1.0, 6.672278100542312], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9466666666666668, 0.0, 0.0, 0.07142857142857142, 0.3980992252897373, 0.07973202619186709, 0.0, 1.0, 0.04765912928958794], 
reward next is 0.9523, 
noisyNet noise sample is [array([-2.0582123], dtype=float32), -0.81035936]. 
=============================================
[2019-04-01 19:19:56,843] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:19:56,844] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3602
[2019-04-01 19:19:56,882] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 93.0, 0.0, 26.0, 25.25718558102206, 8.601824075079323, 1.0, 1.0, 13.64651695727233], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1429200.0000, 
sim time next is 1429800.0000, 
raw observation next is [0.6000000000000001, 92.0, 92.0, 0.0, 26.0, 24.92607809178448, 8.152812475381646, 1.0, 1.0, 41.23845676669496], 
processed observation next is [1.0, 0.5652173913043478, 0.479224376731302, 0.92, 0.30666666666666664, 0.0, 1.0, 0.8465825845406398, 0.08152812475381646, 1.0, 1.0, 0.29456040547639256], 
reward next is 0.7054, 
noisyNet noise sample is [array([-1.8081034], dtype=float32), -0.64740556]. 
=============================================
[2019-04-01 19:20:01,406] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1032427e-20 0.0000000e+00 0.0000000e+00 1.8278497e-18 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:20:01,408] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6375
[2019-04-01 19:20:01,441] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 90.0, 0.0, 0.0, 26.0, 25.73674152332424, 11.4625105152349, 1.0, 1.0, 22.85608161200717], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1449000.0000, 
sim time next is 1449600.0000, 
raw observation next is [1.1, 90.66666666666666, 0.0, 0.0, 26.0, 25.68217374306628, 11.37800328950426, 1.0, 1.0, 21.69471834355182], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.9066666666666666, 0.0, 0.0, 1.0, 0.9545962490094685, 0.1137800328950426, 1.0, 1.0, 0.154962273882513], 
reward next is 0.2938, 
noisyNet noise sample is [array([-1.8025091], dtype=float32), -0.12124036]. 
=============================================
[2019-04-01 19:20:01,714] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5570271e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.7765466e-37
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:20:01,715] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9145
[2019-04-01 19:20:01,745] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 9.0, 0.0, 26.0, 25.82957243044246, 10.66782197094615, 1.0, 1.0, 15.83540004445179], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1443600.0000, 
sim time next is 1444200.0000, 
raw observation next is [1.1, 91.33333333333334, 5.999999999999998, 0.0, 26.0, 25.90300660042018, 10.75272687596031, 1.0, 1.0, 15.0635111318358], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.9133333333333334, 0.019999999999999993, 0.0, 1.0, 0.9861438000600258, 0.1075272687596031, 1.0, 1.0, 0.10759650808454142], 
reward next is 0.5913, 
noisyNet noise sample is [array([0.25962257], dtype=float32), 1.6105229]. 
=============================================
[2019-04-01 19:20:02,058] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:20:02,059] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7571
[2019-04-01 19:20:02,074] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.216666666666667, 93.5, 92.0, 705.3333333333334, 26.0, 26.19384415783615, 13.19850215947659, 1.0, 1.0, 5.548454101898325], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1511400.0000, 
sim time next is 1512000.0000, 
raw observation next is [4.4, 93.0, 94.0, 704.0, 26.0, 25.94832627947444, 12.81149611494898, 1.0, 1.0, 4.987943698046204], 
processed observation next is [1.0, 0.5217391304347826, 0.5844875346260389, 0.93, 0.31333333333333335, 0.7779005524861878, 1.0, 0.9926180399249199, 0.1281149611494898, 1.0, 1.0, 0.0356281692717586], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3519465], dtype=float32), -2.0885665]. 
=============================================
[2019-04-01 19:20:02,090] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[47.717846]
 [48.410015]
 [49.00028 ]
 [50.174644]
 [51.15847 ]], R is [[46.27306747]
 [45.81033707]
 [46.02690887]
 [45.56663895]
 [45.1320076 ]].
[2019-04-01 19:20:02,607] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:20:02,612] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5428
[2019-04-01 19:20:02,626] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 92.0, 0.0, 0.0, 26.0, 25.28770029127155, 9.40877315590796, 0.0, 1.0, 37.20301950497462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1476000.0000, 
sim time next is 1476600.0000, 
raw observation next is [2.2, 92.33333333333334, 0.0, 0.0, 26.0, 25.2891729072838, 9.689878822024616, 0.0, 1.0, 37.12473380283421], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9233333333333335, 0.0, 0.0, 1.0, 0.8984532724691141, 0.09689878822024615, 0.0, 1.0, 0.2651766700202443], 
reward next is 0.7348, 
noisyNet noise sample is [array([-1.2459491], dtype=float32), -1.0185713]. 
=============================================
[2019-04-01 19:20:06,201] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:20:06,201] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8187
[2019-04-01 19:20:06,212] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.283333333333333, 73.16666666666667, 0.0, 0.0, 26.0, 25.45353079006219, 11.33973464539331, 0.0, 1.0, 4.464946788418107], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1541400.0000, 
sim time next is 1542000.0000, 
raw observation next is [7.366666666666667, 73.33333333333334, 0.0, 0.0, 26.0, 25.37390220953323, 11.74463049513756, 0.0, 1.0, 44.47597872973127], 
processed observation next is [1.0, 0.8695652173913043, 0.6666666666666667, 0.7333333333333334, 0.0, 0.0, 1.0, 0.9105574585047472, 0.1174463049513756, 0.0, 1.0, 0.3176855623552233], 
reward next is 0.6823, 
noisyNet noise sample is [array([1.0714257], dtype=float32), -1.1561251]. 
=============================================
[2019-04-01 19:20:06,217] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[80.20418 ]
 [78.540985]
 [75.60209 ]
 [77.46099 ]
 [74.33599 ]], R is [[81.14232635]
 [81.29901123]
 [81.45291901]
 [80.83071136]
 [80.98627472]].
[2019-04-01 19:20:06,737] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1502940e-37 0.0000000e+00 0.0000000e+00 9.8972988e-01 7.8546821e-17
 1.0270138e-02 1.6264338e-35], sum to 1.0000
[2019-04-01 19:20:06,740] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0594
[2019-04-01 19:20:06,777] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.83508980559717, 10.66735322778401, 1.0, 1.0, 18.36050458004952], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1704600.0000, 
sim time next is 1705200.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.70356044244372, 11.15548860235705, 1.0, 1.0, 17.45389829959805], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.0, 0.0, 1.0, 0.9576514917776743, 0.1115548860235705, 1.0, 1.0, 0.12467070213998606], 
reward next is 0.4131, 
noisyNet noise sample is [array([-0.5382902], dtype=float32), 0.3490114]. 
=============================================
[2019-04-01 19:20:12,786] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1991661e-28 0.0000000e+00 1.3789465e-37 1.0000000e+00 4.1621405e-22
 2.4555919e-17 8.9717714e-32], sum to 1.0000
[2019-04-01 19:20:12,786] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1285
[2019-04-01 19:20:12,799] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.17765944842142, 9.095853107689246, 1.0, 1.0, 13.63967576741179], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1708800.0000, 
sim time next is 1709400.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 24.9584001244091, 8.677148132996663, 1.0, 1.0, 17.58599251078697], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 1.0, 0.8512000177727285, 0.08677148132996663, 1.0, 1.0, 0.12561423221990692], 
reward next is 0.8744, 
noisyNet noise sample is [array([-0.24706982], dtype=float32), 1.5399551]. 
=============================================
[2019-04-01 19:20:14,586] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4918615e-37 1.1345325e-30
 1.0000000e+00 1.0207798e-34], sum to 1.0000
[2019-04-01 19:20:14,587] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2463
[2019-04-01 19:20:14,634] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 83.0, 45.5, 0.0, 26.0, 25.17327691943681, 8.0361386264031, 0.0, 1.0, 32.2203413785391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1760400.0000, 
sim time next is 1761000.0000, 
raw observation next is [-1.8, 83.66666666666667, 52.0, 0.0, 26.0, 25.09203979451331, 7.829406730758104, 0.0, 1.0, 30.40061759114631], 
processed observation next is [0.0, 0.391304347826087, 0.41274238227146814, 0.8366666666666667, 0.17333333333333334, 0.0, 1.0, 0.8702913992161869, 0.07829406730758104, 0.0, 1.0, 0.21714726850818794], 
reward next is 0.7829, 
noisyNet noise sample is [array([0.04763545], dtype=float32), 0.75451386]. 
=============================================
[2019-04-01 19:20:14,641] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[63.431187]
 [63.404   ]
 [63.452236]
 [63.410866]
 [63.457092]], R is [[63.45247269]
 [63.58780289]
 [63.7073555 ]
 [63.80795288]
 [63.87525177]].
[2019-04-01 19:20:17,728] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:20:17,729] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6808
[2019-04-01 19:20:17,778] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 83.0, 122.5, 0.0, 26.0, 25.07013033651686, 8.36836947873001, 0.0, 1.0, 32.68039661366684], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1771200.0000, 
sim time next is 1771800.0000, 
raw observation next is [-2.383333333333333, 83.0, 123.6666666666667, 0.0, 26.0, 25.04993087028494, 8.196595483658827, 0.0, 1.0, 29.39379178760785], 
processed observation next is [0.0, 0.5217391304347826, 0.3965835641735919, 0.83, 0.4122222222222223, 0.0, 1.0, 0.8642758386121342, 0.08196595483658826, 0.0, 1.0, 0.20995565562577034], 
reward next is 0.7900, 
noisyNet noise sample is [array([0.8614531], dtype=float32), -0.14108449]. 
=============================================
[2019-04-01 19:20:24,275] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1092555e-34 0.0000000e+00 0.0000000e+00 1.0293184e-14 1.2515993e-38
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:20:24,276] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9511
[2019-04-01 19:20:24,320] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 68.5, 0.0, 0.0, 26.0, 25.91580227405049, 11.1847072552111, 1.0, 1.0, 36.40117586927303], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2224200.0000, 
sim time next is 2224800.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 26.0, 26.0100521424369, 11.42939004115456, 1.0, 1.0, 34.31808988561529], 
processed observation next is [1.0, 0.782608695652174, 0.3379501385041552, 0.68, 0.0, 0.0, 1.0, 1.0014360203481287, 0.11429390041154561, 1.0, 1.0, 0.2451292134686806], 
reward next is 0.1831, 
noisyNet noise sample is [array([2.2943847], dtype=float32), 0.4650184]. 
=============================================
[2019-04-01 19:20:26,654] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2975068e-38 0.0000000e+00 1.2437780e-33 2.9310563e-01 0.0000000e+00
 7.0689434e-01 0.0000000e+00], sum to 1.0000
[2019-04-01 19:20:26,654] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3075
[2019-04-01 19:20:26,712] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666667, 84.0, 0.0, 0.0, 26.0, 25.0669078561371, 7.377787200596582, 0.0, 1.0, 38.14282073009628], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1881600.0000, 
sim time next is 1882200.0000, 
raw observation next is [-4.583333333333333, 83.5, 0.0, 0.0, 26.0, 25.03159744152227, 7.291117943259731, 0.0, 1.0, 39.20839963790779], 
processed observation next is [0.0, 0.782608695652174, 0.3356417359187443, 0.835, 0.0, 0.0, 1.0, 0.8616567773603242, 0.07291117943259731, 0.0, 1.0, 0.2800599974136271], 
reward next is 0.7199, 
noisyNet noise sample is [array([1.0923865], dtype=float32), 0.8103241]. 
=============================================
[2019-04-01 19:20:32,992] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.6831349e-27], sum to 1.0000
[2019-04-01 19:20:32,995] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1796
[2019-04-01 19:20:33,016] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.38100621139853, 5.665139002819281, 0.0, 1.0, 40.95466572351903], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1997400.0000, 
sim time next is 1998000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.38775470733426, 5.593509156752243, 0.0, 1.0, 40.90060838103716], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 1.0, 0.7696792439048943, 0.05593509156752243, 0.0, 1.0, 0.292147202721694], 
reward next is 0.7079, 
noisyNet noise sample is [array([-0.11442604], dtype=float32), -0.3873344]. 
=============================================
[2019-04-01 19:20:33,028] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[72.62772 ]
 [72.853065]
 [73.04446 ]
 [73.30692 ]
 [73.54978 ]], R is [[72.39296722]
 [72.37650299]
 [72.35977936]
 [72.34293365]
 [72.32566833]].
[2019-04-01 19:20:40,799] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:20:40,800] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5539
[2019-04-01 19:20:40,834] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.783333333333333, 83.0, 0.0, 0.0, 26.0, 25.19072496721221, 8.915951013834528, 0.0, 1.0, 42.5599685968327], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2149800.0000, 
sim time next is 2150400.0000, 
raw observation next is [-5.966666666666667, 83.0, 0.0, 0.0, 26.0, 25.19766527785403, 8.872085158899068, 0.0, 1.0, 41.8125232216616], 
processed observation next is [1.0, 0.9130434782608695, 0.2973222530009234, 0.83, 0.0, 0.0, 1.0, 0.885380753979147, 0.08872085158899068, 0.0, 1.0, 0.2986608801547257], 
reward next is 0.7013, 
noisyNet noise sample is [array([0.8406171], dtype=float32), -1.1879671]. 
=============================================
[2019-04-01 19:20:41,376] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.8873143e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.7446813e-25
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:20:41,378] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3729
[2019-04-01 19:20:41,441] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.00000000000001, 12.0, 0.0, 26.0, 25.63921244648114, 8.936278768718324, 1.0, 1.0, 29.53318450592962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2049000.0000, 
sim time next is 2049600.0000, 
raw observation next is [-3.9, 82.0, 6.999999999999999, 0.0, 26.0, 25.44630073290149, 8.175046728763535, 1.0, 1.0, 29.1905985823024], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.02333333333333333, 0.0, 1.0, 0.9209001047002131, 0.08175046728763535, 1.0, 1.0, 0.2085042755878743], 
reward next is 0.7915, 
noisyNet noise sample is [array([-1.9687406], dtype=float32), -0.93866205]. 
=============================================
[2019-04-01 19:20:47,850] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.5580975e-32 0.0000000e+00 7.4343022e-27 0.0000000e+00 1.4333236e-31
 3.3845825e-07 9.9999964e-01], sum to 1.0000
[2019-04-01 19:20:47,851] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1688
[2019-04-01 19:20:47,909] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 66.33333333333333, 124.0, 375.0, 26.0, 25.38022608600982, 7.967829882775722, 0.0, 1.0, 32.17079570777609], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2367600.0000, 
sim time next is 2368200.0000, 
raw observation next is [-2.9, 65.66666666666667, 127.0, 390.0, 26.0, 25.32090605399449, 7.848567433329194, 0.0, 1.0, 30.15127817654325], 
processed observation next is [0.0, 0.391304347826087, 0.38227146814404434, 0.6566666666666667, 0.42333333333333334, 0.430939226519337, 1.0, 0.9029865791420703, 0.07848567433329194, 0.0, 1.0, 0.21536627268959466], 
reward next is 0.7846, 
noisyNet noise sample is [array([0.6043272], dtype=float32), 0.7906323]. 
=============================================
[2019-04-01 19:20:53,505] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 6.171290e-07 0.000000e+00
 9.999994e-01 0.000000e+00], sum to 1.0000
[2019-04-01 19:20:53,505] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7014
[2019-04-01 19:20:53,528] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 56.0, 0.0, 0.0, 26.0, 25.32324392600492, 9.289660583104032, 0.0, 1.0, 39.29792822572787], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2325600.0000, 
sim time next is 2326200.0000, 
raw observation next is [-1.8, 56.5, 0.0, 0.0, 26.0, 25.31334843669704, 8.49310569785763, 0.0, 1.0, 40.70585712134145], 
processed observation next is [1.0, 0.9565217391304348, 0.41274238227146814, 0.565, 0.0, 0.0, 1.0, 0.9019069195281487, 0.0849310569785763, 0.0, 1.0, 0.29075612229529607], 
reward next is 0.7092, 
noisyNet noise sample is [array([0.15017928], dtype=float32), 1.18637]. 
=============================================
[2019-04-01 19:20:54,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 8.8463788e-37 0.0000000e+00
 1.0000000e+00 3.6303017e-27], sum to 1.0000
[2019-04-01 19:20:54,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8655
[2019-04-01 19:20:54,776] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.95, 87.0, 0.0, 0.0, 26.0, 23.70888987831593, 5.299275458142422, 0.0, 1.0, 43.89658191392863], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2691000.0000, 
sim time next is 2691600.0000, 
raw observation next is [-14.3, 88.33333333333334, 0.0, 0.0, 26.0, 23.61525071142896, 5.301684290949209, 0.0, 1.0, 43.90744220186915], 
processed observation next is [1.0, 0.13043478260869565, 0.06648199445983377, 0.8833333333333334, 0.0, 0.0, 1.0, 0.6593215302041371, 0.05301684290949209, 0.0, 1.0, 0.3136245871562082], 
reward next is 0.6864, 
noisyNet noise sample is [array([-0.31955847], dtype=float32), 0.784493]. 
=============================================
[2019-04-01 19:21:01,632] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.1975304e-38], sum to 1.0000
[2019-04-01 19:21:01,632] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8034
[2019-04-01 19:21:01,667] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.97006194654388, 6.882886105194473, 0.0, 1.0, 54.69871988667053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2868000.0000, 
sim time next is 2868600.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 25.01130824299813, 6.800464772813693, 0.0, 1.0, 54.66544276128469], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 1.0, 0.8587583204283042, 0.06800464772813693, 0.0, 1.0, 0.3904674482948906], 
reward next is 0.6095, 
noisyNet noise sample is [array([-1.4114169], dtype=float32), -0.22787224]. 
=============================================
[2019-04-01 19:21:04,996] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:21:05,021] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9433
[2019-04-01 19:21:05,035] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.583333333333333, 42.33333333333334, 0.0, 0.0, 26.0, 25.14298934796171, 7.034452702162644, 0.0, 1.0, 42.41134184067531], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2409000.0000, 
sim time next is 2409600.0000, 
raw observation next is [-3.766666666666667, 42.66666666666667, 0.0, 0.0, 26.0, 25.11861123349813, 6.928053262628836, 0.0, 1.0, 42.42209594345244], 
processed observation next is [0.0, 0.9130434782608695, 0.358264081255771, 0.4266666666666667, 0.0, 0.0, 1.0, 0.8740873190711612, 0.06928053262628836, 0.0, 1.0, 0.3030149710246603], 
reward next is 0.6970, 
noisyNet noise sample is [array([-0.29089472], dtype=float32), 1.997878]. 
=============================================
[2019-04-01 19:21:05,278] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:21:05,283] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6219
[2019-04-01 19:21:05,299] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 58.5, 15.33333333333333, 165.3333333333333, 26.0, 22.82242096719227, 6.31133854841201, 0.0, 1.0, 43.52564531160774], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2447400.0000, 
sim time next is 2448000.0000, 
raw observation next is [-9.5, 58.0, 21.5, 228.0, 26.0, 22.83383911720776, 6.282502190065604, 0.0, 1.0, 43.39373781513505], 
processed observation next is [0.0, 0.34782608695652173, 0.1994459833795014, 0.58, 0.07166666666666667, 0.25193370165745854, 1.0, 0.5476913024582514, 0.06282502190065604, 0.0, 1.0, 0.3099552701081075], 
reward next is 0.6900, 
noisyNet noise sample is [array([1.5277383], dtype=float32), -0.62063515]. 
=============================================
[2019-04-01 19:21:05,306] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[68.00186 ]
 [67.996025]
 [68.233116]
 [68.26757 ]
 [68.315704]], R is [[68.25402069]
 [68.26058197]
 [68.26649475]
 [68.27213287]
 [68.2780838 ]].
[2019-04-01 19:21:12,862] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:21:12,862] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2354
[2019-04-01 19:21:12,902] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 55.33333333333333, 65.16666666666666, 20.5, 26.0, 25.52310343895298, 6.847294888122484, 1.0, 1.0, 24.01349934266602], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2536800.0000, 
sim time next is 2537400.0000, 
raw observation next is [-2.8, 55.66666666666667, 79.33333333333333, 23.0, 26.0, 25.65931292260538, 6.946683675103827, 1.0, 1.0, 23.52314101288371], 
processed observation next is [1.0, 0.34782608695652173, 0.38504155124653744, 0.5566666666666668, 0.2644444444444444, 0.02541436464088398, 1.0, 0.9513304175150543, 0.06946683675103828, 1.0, 1.0, 0.1680224358063122], 
reward next is 0.8320, 
noisyNet noise sample is [array([-2.264703], dtype=float32), -1.0186886]. 
=============================================
[2019-04-01 19:21:25,532] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:21:25,534] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9706
[2019-04-01 19:21:25,552] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.84317882036191, 6.663348054231339, 0.0, 1.0, 41.31499498539034], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2770800.0000, 
sim time next is 2771400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.85977258690982, 6.554416584416248, 0.0, 1.0, 41.19079466339568], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 1.0, 0.8371103695585457, 0.06554416584416248, 0.0, 1.0, 0.2942199618813977], 
reward next is 0.7058, 
noisyNet noise sample is [array([-0.6542026], dtype=float32), 0.3328418]. 
=============================================
[2019-04-01 19:21:25,718] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 4.1839982e-14 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:21:25,718] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9407
[2019-04-01 19:21:25,741] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:21:25,741] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6454
[2019-04-01 19:21:25,759] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.13333049165832, 9.812149103889189, 0.0, 1.0, 40.15891485801696], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3211800.0000, 
sim time next is 3212400.0000, 
raw observation next is [-1.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.09336049684998, 9.79976687070842, 0.0, 1.0, 41.92955447425871], 
processed observation next is [1.0, 0.17391304347826086, 0.42566943674976926, 1.0, 0.0, 0.0, 1.0, 0.8704800709785684, 0.09799766870708419, 0.0, 1.0, 0.29949681767327646], 
reward next is 0.7005, 
noisyNet noise sample is [array([0.33954766], dtype=float32), 0.12089203]. 
=============================================
[2019-04-01 19:21:25,763] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 54.0, 0.0, 0.0, 26.0, 25.32263304515885, 10.42429290364656, 1.0, 1.0, 45.58757696248212], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2743200.0000, 
sim time next is 2743800.0000, 
raw observation next is [-4.166666666666667, 54.83333333333333, 0.0, 0.0, 26.0, 25.60053798959099, 10.81191842468142, 1.0, 1.0, 41.63278186567595], 
processed observation next is [1.0, 0.782608695652174, 0.3471837488457987, 0.5483333333333333, 0.0, 0.0, 1.0, 0.9429339985129985, 0.10811918424681419, 1.0, 1.0, 0.2973770133262568], 
reward next is 0.3779, 
noisyNet noise sample is [array([-0.2662437], dtype=float32), -0.3558139]. 
=============================================
[2019-04-01 19:21:28,471] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:21:28,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7005
[2019-04-01 19:21:28,504] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 52.5, 174.0, 508.0, 26.0, 26.07890637808038, 9.94873434194362, 1.0, 1.0, 15.94303866996939], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2802600.0000, 
sim time next is 2803200.0000, 
raw observation next is [-1.666666666666667, 51.66666666666666, 165.8333333333333, 550.5, 26.0, 26.08994949395176, 10.03105227452873, 1.0, 1.0, 14.54848410930719], 
processed observation next is [1.0, 0.43478260869565216, 0.4164358264081256, 0.5166666666666666, 0.5527777777777776, 0.6082872928176796, 1.0, 1.0128499277073943, 0.10031052274528729, 1.0, 1.0, 0.1039177436379085], 
reward next is 0.8837, 
noisyNet noise sample is [array([0.12741135], dtype=float32), -1.2624485]. 
=============================================
[2019-04-01 19:21:35,996] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 3.030438e-23 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 19:21:35,997] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1412
[2019-04-01 19:21:36,012] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.81031564424631, 7.328896152185631, 0.0, 1.0, 42.73516937596025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2942400.0000, 
sim time next is 2943000.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.77240406111898, 7.279030202115668, 0.0, 1.0, 42.70962885742772], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 1.0, 0.8246291515884258, 0.07279030202115667, 0.0, 1.0, 0.3050687775530551], 
reward next is 0.6949, 
noisyNet noise sample is [array([-0.9250351], dtype=float32), 0.40601143]. 
=============================================
[2019-04-01 19:21:36,023] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[68.57141]
 [68.38483]
 [68.25522]
 [68.0406 ]
 [67.81836]], R is [[68.7971344 ]
 [68.8039093 ]
 [68.81058502]
 [68.81730652]
 [68.82365417]].
[2019-04-01 19:21:37,364] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:21:37,364] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1318
[2019-04-01 19:21:37,421] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 157.3333333333333, 727.3333333333334, 26.0, 25.17454003871159, 9.42548604007913, 0.0, 1.0, 27.06686785913441], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2983800.0000, 
sim time next is 2984400.0000, 
raw observation next is [-3.0, 65.0, 145.5, 745.5, 26.0, 25.19286286248618, 9.403739023570123, 0.0, 1.0, 23.59045778993693], 
processed observation next is [0.0, 0.5652173913043478, 0.3795013850415513, 0.65, 0.485, 0.8237569060773481, 1.0, 0.8846946946408829, 0.09403739023570123, 0.0, 1.0, 0.16850326992812092], 
reward next is 0.8315, 
noisyNet noise sample is [array([2.289792], dtype=float32), -0.833506]. 
=============================================
[2019-04-01 19:21:45,644] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:21:45,644] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2025
[2019-04-01 19:21:45,663] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666667, 100.0, 0.0, 0.0, 26.0, 25.39978473682423, 7.775877657096122, 0.0, 1.0, 38.17730923196653], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3104400.0000, 
sim time next is 3105000.0000, 
raw observation next is [-0.5, 100.0, 0.0, 0.0, 26.0, 25.39818032414795, 7.77388690384057, 0.0, 1.0, 38.2486621711371], 
processed observation next is [0.0, 0.9565217391304348, 0.44875346260387816, 1.0, 0.0, 0.0, 1.0, 0.9140257605925645, 0.0777388690384057, 0.0, 1.0, 0.2732047297938364], 
reward next is 0.7268, 
noisyNet noise sample is [array([0.20151007], dtype=float32), -0.0034612613]. 
=============================================
[2019-04-01 19:21:45,677] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[66.570564]
 [66.55489 ]
 [66.54525 ]
 [66.63982 ]
 [66.742065]], R is [[66.70455933]
 [66.76481628]
 [66.81620026]
 [66.85852051]
 [66.8991394 ]].
[2019-04-01 19:21:49,398] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 3.29673e-32 1.00000e+00
 0.00000e+00], sum to 1.0000
[2019-04-01 19:21:49,399] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8224
[2019-04-01 19:21:49,414] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 96.0, 0.0, 0.0, 26.0, 25.49144812085687, 8.059682419172253, 0.0, 1.0, 32.74786802384513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3097800.0000, 
sim time next is 3098400.0000, 
raw observation next is [-1.0, 97.33333333333333, 0.0, 0.0, 26.0, 25.44096090166595, 7.909156760756002, 0.0, 1.0, 34.38502267153337], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9733333333333333, 0.0, 0.0, 1.0, 0.9201372716665642, 0.07909156760756002, 0.0, 1.0, 0.24560730479666693], 
reward next is 0.7544, 
noisyNet noise sample is [array([-1.3438486], dtype=float32), 0.17179598]. 
=============================================
[2019-04-01 19:21:55,860] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4794437e-38 4.9169779e-36
 1.0000000e+00 3.7824843e-12], sum to 1.0000
[2019-04-01 19:21:55,863] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2486
[2019-04-01 19:21:55,878] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.166666666666667, 70.0, 0.0, 0.0, 26.0, 24.91307735930507, 7.416368787692065, 0.0, 1.0, 40.51277101544295], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3557400.0000, 
sim time next is 3558000.0000, 
raw observation next is [-4.333333333333334, 69.0, 0.0, 0.0, 26.0, 24.87686095564409, 7.471271425319979, 0.0, 1.0, 40.44978237291357], 
processed observation next is [0.0, 0.17391304347826086, 0.3425669436749769, 0.69, 0.0, 0.0, 1.0, 0.839551565092013, 0.0747127142531998, 0.0, 1.0, 0.2889270169493826], 
reward next is 0.7111, 
noisyNet noise sample is [array([-0.0204789], dtype=float32), 0.27319118]. 
=============================================
[2019-04-01 19:21:55,890] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[73.295845]
 [73.41355 ]
 [73.497284]
 [73.616615]
 [73.697266]], R is [[73.11315918]
 [73.09265137]
 [73.07192993]
 [73.05097961]
 [73.03000641]].
[2019-04-01 19:21:59,871] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 9.1461837e-38 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:21:59,872] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6426
[2019-04-01 19:21:59,915] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.88395689769259, 12.26263668496665, 1.0, 1.0, 24.83478356563987], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3349200.0000, 
sim time next is 3349800.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.94413372763589, 11.69341760506813, 1.0, 1.0, 20.62458641046392], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 1.0, 0.9920191039479843, 0.11693417605068131, 1.0, 1.0, 0.14731847436045656], 
reward next is 0.1753, 
noisyNet noise sample is [array([-0.25166816], dtype=float32), 2.6955292]. 
=============================================
[2019-04-01 19:22:00,743] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 5.048055e-23
 1.000000e+00 5.840551e-35], sum to 1.0000
[2019-04-01 19:22:00,743] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1636
[2019-04-01 19:22:00,751] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 67.0, 0.0, 0.0, 26.0, 25.78889234365204, 12.09425914876434, 1.0, 1.0, 9.361759122842903], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3433800.0000, 
sim time next is 3434400.0000, 
raw observation next is [2.0, 67.0, 0.0, 0.0, 26.0, 25.97784996525288, 12.26070112080394, 1.0, 1.0, 9.702090703546009], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.67, 0.0, 0.0, 1.0, 0.99683570932184, 0.12260701120803941, 1.0, 1.0, 0.06930064788247149], 
reward next is 0.0264, 
noisyNet noise sample is [array([-0.42147082], dtype=float32), -0.8558059]. 
=============================================
[2019-04-01 19:22:02,539] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8746163e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.1900916e-37
 7.2730569e-15 1.0000000e+00], sum to 1.0000
[2019-04-01 19:22:02,539] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2282
[2019-04-01 19:22:02,564] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 46.33333333333334, 116.6666666666667, 814.8333333333334, 26.0, 26.6417767229531, 12.08311233812519, 1.0, 1.0, 14.5463672809469], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3414000.0000, 
sim time next is 3414600.0000, 
raw observation next is [3.0, 47.0, 117.0, 817.0, 26.0, 25.92344803723293, 11.14722100336467, 1.0, 1.0, 12.54404379276243], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.47, 0.39, 0.9027624309392265, 1.0, 0.98906400531899, 0.1114722100336467, 1.0, 1.0, 0.08960031280544593], 
reward next is 0.4515, 
noisyNet noise sample is [array([1.5025116], dtype=float32), 0.12697028]. 
=============================================
[2019-04-01 19:22:03,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:22:03,547] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4911
[2019-04-01 19:22:03,561] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 67.0, 0.0, 0.0, 26.0, 25.4638576525139, 8.570522795416984, 0.0, 1.0, 31.54568277846523], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3470400.0000, 
sim time next is 3471000.0000, 
raw observation next is [0.8333333333333334, 67.83333333333334, 0.0, 0.0, 26.0, 25.35024457317846, 8.384518536468086, 0.0, 1.0, 35.23416493949188], 
processed observation next is [1.0, 0.17391304347826086, 0.4856879039704525, 0.6783333333333335, 0.0, 0.0, 1.0, 0.9071777961683516, 0.08384518536468086, 0.0, 1.0, 0.25167260671065633], 
reward next is 0.7483, 
noisyNet noise sample is [array([0.8462353], dtype=float32), 1.0183362]. 
=============================================
[2019-04-01 19:22:03,575] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[61.67505 ]
 [61.580948]
 [61.434364]
 [61.38376 ]
 [61.32242 ]], R is [[61.82952881]
 [61.98590851]
 [62.15727997]
 [62.29587555]
 [62.44268799]].
[2019-04-01 19:22:11,081] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:22:11,081] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2833
[2019-04-01 19:22:11,097] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.833333333333334, 25.33333333333334, 0.0, 0.0, 26.0, 25.51535077763379, 7.918792903855862, 0.0, 1.0, 30.69657907343499], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3651000.0000, 
sim time next is 3651600.0000, 
raw observation next is [9.666666666666668, 25.66666666666667, 0.0, 0.0, 26.0, 25.53443165466237, 7.886927851337383, 0.0, 1.0, 29.02316495275586], 
processed observation next is [0.0, 0.2608695652173913, 0.7303785780240075, 0.2566666666666667, 0.0, 0.0, 1.0, 0.9334902363803386, 0.07886927851337383, 0.0, 1.0, 0.20730832109111327], 
reward next is 0.7927, 
noisyNet noise sample is [array([0.14587323], dtype=float32), -1.9045386]. 
=============================================
[2019-04-01 19:22:20,376] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.3229860e-19 9.5899730e-25 3.7834366e-27 5.4053854e-25 1.0758417e-18
 2.2108669e-11 1.0000000e+00], sum to 1.0000
[2019-04-01 19:22:20,377] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3650
[2019-04-01 19:22:20,387] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 60.0, 112.3333333333333, 790.6666666666667, 26.0, 26.50364461539677, 13.29999234694629, 1.0, 1.0, 11.2649511287183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3840600.0000, 
sim time next is 3841200.0000, 
raw observation next is [-1.0, 60.0, 113.5, 798.5, 26.0, 26.50555238057084, 13.5114259641623, 1.0, 1.0, 10.51836543669041], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.37833333333333335, 0.8823204419889503, 1.0, 1.0722217686529771, 0.135114259641623, 1.0, 1.0, 0.07513118169064578], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.30260634], dtype=float32), 0.20229277]. 
=============================================
[2019-04-01 19:22:43,524] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.1420942e-27 0.0000000e+00 0.0000000e+00
 2.0774606e-18 1.0000000e+00], sum to 1.0000
[2019-04-01 19:22:43,526] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7398
[2019-04-01 19:22:43,539] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 54.5, 148.6666666666667, 749.3333333333334, 26.0, 25.17095979515005, 8.370518373816829, 0.0, 1.0, 10.03061057804634], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4273800.0000, 
sim time next is 4274400.0000, 
raw observation next is [5.666666666666667, 54.0, 134.8333333333333, 785.6666666666666, 26.0, 25.15307358541126, 8.36850757497408, 0.0, 1.0, 9.113716019322702], 
processed observation next is [0.0, 0.4782608695652174, 0.6195752539242845, 0.54, 0.4494444444444443, 0.8681399631675875, 1.0, 0.8790105122016085, 0.0836850757497408, 0.0, 1.0, 0.06509797156659072], 
reward next is 0.9349, 
noisyNet noise sample is [array([-1.9297945], dtype=float32), 1.4093423]. 
=============================================
[2019-04-01 19:22:44,740] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0577328e-30 0.0000000e+00 2.7255751e-27 1.2973379e-16 5.0468971e-09
 1.0000000e+00 4.9418422e-19], sum to 1.0000
[2019-04-01 19:22:44,740] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7183
[2019-04-01 19:22:44,755] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.166666666666667, 47.5, 0.0, 0.0, 26.0, 25.39287018082396, 7.781338325790446, 0.0, 1.0, 40.2149142104607], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4237800.0000, 
sim time next is 4238400.0000, 
raw observation next is [2.333333333333333, 47.0, 0.0, 0.0, 26.0, 25.4154764062537, 7.797939746801663, 0.0, 1.0, 39.1084042213398], 
processed observation next is [0.0, 0.043478260869565216, 0.5272391505078486, 0.47, 0.0, 0.0, 1.0, 0.9164966294648141, 0.07797939746801663, 0.0, 1.0, 0.2793457444381415], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.09291814], dtype=float32), -0.6418231]. 
=============================================
[2019-04-01 19:22:47,917] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9747366e-24 3.5368943e-22 2.4640166e-34 2.6356386e-35 9.9993169e-01
 1.2240961e-14 6.8333014e-05], sum to 1.0000
[2019-04-01 19:22:47,918] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1645
[2019-04-01 19:22:47,938] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.58333333333333, 29.33333333333334, 116.0, 845.6666666666666, 26.0, 27.49169375739856, 24.76622116447253, 1.0, 1.0, 1.482170742646243], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4367400.0000, 
sim time next is 4368000.0000, 
raw observation next is [14.56666666666667, 29.66666666666667, 115.5, 843.8333333333334, 26.0, 27.69130193015491, 26.91037377339815, 1.0, 1.0, 1.506657888717305], 
processed observation next is [1.0, 0.5652173913043478, 0.8661126500461682, 0.2966666666666667, 0.385, 0.9324125230202579, 1.0, 1.2416145614507015, 0.2691037377339815, 1.0, 1.0, 0.010761842062266463], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2840189], dtype=float32), -0.21243264]. 
=============================================
[2019-04-01 19:22:47,947] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[12.202871]
 [12.205628]
 [12.172426]
 [12.252506]
 [12.311718]], R is [[12.05454636]
 [11.93400097]
 [11.81466103]
 [11.69651413]
 [11.57954884]].
[2019-04-01 19:22:48,023] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:22:48,024] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6899
[2019-04-01 19:22:48,043] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.25, 71.5, 0.0, 0.0, 26.0, 25.46591378597881, 7.906131116524477, 0.0, 1.0, 31.32648598152303], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4329000.0000, 
sim time next is 4329600.0000, 
raw observation next is [4.166666666666667, 71.33333333333333, 0.0, 0.0, 26.0, 25.38965899548779, 7.998212068962888, 0.0, 1.0, 36.38988202278321], 
processed observation next is [1.0, 0.08695652173913043, 0.5780240073868884, 0.7133333333333333, 0.0, 0.0, 1.0, 0.912808427926827, 0.07998212068962889, 0.0, 1.0, 0.2599277287341658], 
reward next is 0.7401, 
noisyNet noise sample is [array([0.33766803], dtype=float32), -1.1503241]. 
=============================================
[2019-04-01 19:22:51,597] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3927062e-31
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:22:51,599] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3285
[2019-04-01 19:22:51,622] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.6, 58.5, 0.0, 0.0, 26.0, 27.05390492525264, 21.95562010167493, 0.0, 1.0, 6.241041238269995], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4393800.0000, 
sim time next is 4394400.0000, 
raw observation next is [10.46666666666667, 58.66666666666666, 0.0, 0.0, 26.0, 27.01702684444395, 21.78593357132359, 0.0, 1.0, 6.243031019125202], 
processed observation next is [1.0, 0.8695652173913043, 0.7525392428439522, 0.5866666666666666, 0.0, 0.0, 1.0, 1.1452895492062787, 0.2178593357132359, 0.0, 1.0, 0.04459307870803716], 
reward next is 0.9554, 
noisyNet noise sample is [array([0.14564326], dtype=float32), 1.259057]. 
=============================================
[2019-04-01 19:22:52,142] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8811551e-20 7.4297528e-25 8.1813079e-32 8.3643769e-38 0.0000000e+00
 4.7335667e-03 9.9526644e-01], sum to 1.0000
[2019-04-01 19:22:52,143] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8186
[2019-04-01 19:22:52,151] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.5, 19.5, 111.0, 819.0, 26.0, 28.12991242642129, 27.6191947168824, 1.0, 1.0, 1.326925906934493], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5063400.0000, 
sim time next is 5064000.0000, 
raw observation next is [11.66666666666667, 19.33333333333334, 108.5, 806.6666666666666, 26.0, 28.31899826199045, 28.84189555770152, 1.0, 1.0, 1.269502506661951], 
processed observation next is [1.0, 0.6086956521739131, 0.785780240073869, 0.19333333333333338, 0.3616666666666667, 0.8913443830570902, 1.0, 1.331285465998636, 0.2884189555770152, 1.0, 1.0, 0.009067875047585364], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1444355], dtype=float32), -0.42374122]. 
=============================================
[2019-04-01 19:22:52,179] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[33.74728 ]
 [33.506535]
 [33.09893 ]
 [32.719822]
 [32.5445  ]], R is [[33.71194839]
 [33.37482834]
 [33.04108047]
 [32.71067047]
 [32.383564  ]].
[2019-04-01 19:22:54,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:22:54,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:22:54,434] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run29
[2019-04-01 19:22:54,821] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.3215799e-26 0.0000000e+00 1.1288180e-28 2.9167817e-14 2.3995239e-31
 1.0000000e+00 4.5277824e-35], sum to 1.0000
[2019-04-01 19:22:54,821] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6949
[2019-04-01 19:22:54,833] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 80.5, 0.0, 0.0, 26.0, 25.0854205419704, 8.463268483153968, 0.0, 1.0, 41.15258219782343], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4750200.0000, 
sim time next is 4750800.0000, 
raw observation next is [-3.666666666666667, 81.66666666666667, 0.0, 0.0, 26.0, 25.05066626712826, 8.333099457224264, 0.0, 1.0, 41.09252260787413], 
processed observation next is [1.0, 1.0, 0.3610341643582641, 0.8166666666666668, 0.0, 0.0, 1.0, 0.8643808953040372, 0.08333099457224265, 0.0, 1.0, 0.29351801862767235], 
reward next is 0.7065, 
noisyNet noise sample is [array([-0.18013865], dtype=float32), 0.08807767]. 
=============================================
[2019-04-01 19:22:57,924] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6309057e-24 1.1672025e-33 7.1750270e-36 1.9838856e-06 2.9985164e-34
 9.9999797e-01 1.3261837e-36], sum to 1.0000
[2019-04-01 19:22:57,925] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1651
[2019-04-01 19:22:57,954] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 47.66666666666667, 261.1666666666666, 102.1666666666667, 26.0, 25.20539627629315, 10.81451020971576, 1.0, 1.0, 21.99594861772018], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4544400.0000, 
sim time next is 4545000.0000, 
raw observation next is [3.0, 47.0, 264.0, 113.0, 26.0, 25.6991125471938, 11.9123161362003, 1.0, 1.0, 13.00434650627875], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.47, 0.88, 0.12486187845303867, 1.0, 0.9570160781705427, 0.119123161362003, 1.0, 1.0, 0.0928881893305625], 
reward next is 0.1422, 
noisyNet noise sample is [array([-0.3712248], dtype=float32), 1.0291004]. 
=============================================
[2019-04-01 19:22:57,958] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[66.12324]
 [66.06731]
 [67.22827]
 [67.20583]
 [67.25814]], R is [[65.64837646]
 [65.50897217]
 [65.5454483 ]
 [65.8355484 ]
 [66.11936951]].
[2019-04-01 19:23:00,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:23:00,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:23:00,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run29
[2019-04-01 19:23:08,470] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:23:08,472] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2924
[2019-04-01 19:23:08,488] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 130.5, 0.9999999999999998, 26.0, 26.17694813497963, 10.85691068204894, 1.0, 1.0, 12.06186876369913], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4700400.0000, 
sim time next is 4701000.0000, 
raw observation next is [0.0, 92.0, 146.0, 2.0, 26.0, 26.2005840565275, 11.02239155804063, 1.0, 1.0, 11.52458239630836], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.4866666666666667, 0.0022099447513812156, 1.0, 1.0286548652182144, 0.1102239155804063, 1.0, 1.0, 0.08231844568791685], 
reward next is 0.5087, 
noisyNet noise sample is [array([0.78775156], dtype=float32), -0.18681341]. 
=============================================
[2019-04-01 19:23:08,497] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[66.93611]
 [66.22546]
 [65.62949]
 [65.24132]
 [65.13756]], R is [[67.61533356]
 [67.51026154]
 [67.45441437]
 [67.50554657]
 [67.67623138]].
[2019-04-01 19:23:08,640] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:23:08,642] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8874
[2019-04-01 19:23:08,658] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 81.66666666666667, 0.0, 0.0, 26.0, 25.31160391491736, 9.449415368927559, 0.0, 1.0, 42.86637495633626], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4746000.0000, 
sim time next is 4746600.0000, 
raw observation next is [-3.0, 80.5, 0.0, 0.0, 26.0, 25.28004220701083, 9.327514945169304, 0.0, 1.0, 42.19563638596323], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.805, 0.0, 0.0, 1.0, 0.8971488867158328, 0.09327514945169303, 0.0, 1.0, 0.30139740275688026], 
reward next is 0.6986, 
noisyNet noise sample is [array([-0.8288467], dtype=float32), 0.28482082]. 
=============================================
[2019-04-01 19:23:08,874] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:23:08,874] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5901
[2019-04-01 19:23:08,889] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.133333333333334, 92.66666666666667, 0.0, 0.0, 26.0, 23.83054765559087, 5.362388509463384, 0.0, 1.0, 41.40761039203296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4778400.0000, 
sim time next is 4779000.0000, 
raw observation next is [-6.1, 92.5, 0.0, 0.0, 26.0, 23.80196006320368, 5.342721349855547, 0.0, 1.0, 41.42452638323734], 
processed observation next is [0.0, 0.30434782608695654, 0.29362880886426596, 0.925, 0.0, 0.0, 1.0, 0.6859942947433828, 0.053427213498555476, 0.0, 1.0, 0.295889474165981], 
reward next is 0.7041, 
noisyNet noise sample is [array([0.3015425], dtype=float32), 0.71250975]. 
=============================================
[2019-04-01 19:23:08,898] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.925575]
 [78.94545 ]
 [78.98127 ]
 [79.03556 ]
 [79.10307 ]], R is [[78.83201599]
 [78.7479248 ]
 [78.66484833]
 [78.58282471]
 [78.50187683]].
[2019-04-01 19:23:08,992] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-01 19:23:08,993] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 19:23:08,993] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 19:23:08,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:23:08,993] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 19:23:08,993] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:23:08,994] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:23:08,998] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run39
[2019-04-01 19:23:08,998] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run39
[2019-04-01 19:23:09,040] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run39
[2019-04-01 19:23:44,759] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.6150595], dtype=float32), -0.65352935]
[2019-04-01 19:23:44,759] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [13.91156541, 79.42003771666667, 21.59156973333333, 62.35638693333332, 26.0, 25.69496072829502, 12.14817290628317, 1.0, 1.0, 12.48679749116415]
[2019-04-01 19:23:44,759] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:23:44,760] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.5059763849105737
[2019-04-01 19:24:11,669] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.6150595], dtype=float32), -0.65352935]
[2019-04-01 19:24:11,669] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.383935403000001, 70.39864627166666, 112.13930056, 225.6896536, 26.0, 25.22065900306693, 6.900872584894763, 1.0, 1.0, 34.94352012499829]
[2019-04-01 19:24:11,669] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:24:11,670] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.9015704095188376
[2019-04-01 19:24:43,278] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.6150595], dtype=float32), -0.65352935]
[2019-04-01 19:24:43,278] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.193003949, 43.69640677666667, 50.845514535, 873.92737065, 26.0, 25.79475016246676, 9.849814341811237, 1.0, 1.0, 15.8386513814327]
[2019-04-01 19:24:43,279] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:24:43,279] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.8137969944727269
[2019-04-01 19:24:47,697] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.6150595], dtype=float32), -0.65352935]
[2019-04-01 19:24:47,698] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.0, 71.0, 0.0, 0.0, 26.0, 25.26524764359304, 9.241022153248418, 0.0, 1.0, 43.32084720862341]
[2019-04-01 19:24:47,698] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 19:24:47,698] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.2563964802105855
[2019-04-01 19:24:51,310] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 19:25:11,360] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 19:25:14,483] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 19:25:15,507] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 3800000, evaluation results [3800000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 19:25:19,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:19,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:19,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run29
[2019-04-01 19:25:23,096] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:25:23,098] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5929
[2019-04-01 19:25:23,107] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.166666666666667, 44.16666666666667, 248.0, 378.6666666666667, 26.0, 25.07432182835706, 8.317877014545402, 0.0, 1.0, 12.76837867205287], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4889400.0000, 
sim time next is 4890000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 242.0, 376.3333333333334, 26.0, 25.06695709750651, 8.337566634700748, 0.0, 1.0, 13.00643754923335], 
processed observation next is [0.0, 0.6086956521739131, 0.5272391505078486, 0.4433333333333334, 0.8066666666666666, 0.4158379373848988, 1.0, 0.8667081567866443, 0.08337566634700748, 0.0, 1.0, 0.09290312535166678], 
reward next is 0.9071, 
noisyNet noise sample is [array([0.08868858], dtype=float32), 0.22820751]. 
=============================================
[2019-04-01 19:25:23,111] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[68.64519 ]
 [68.78735 ]
 [68.962265]
 [69.18832 ]
 [69.4069  ]], R is [[68.76226044]
 [68.98343658]
 [69.20542145]
 [69.42808533]
 [69.65762329]].
[2019-04-01 19:25:25,387] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:25,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:25,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run29
[2019-04-01 19:25:25,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:25,591] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:25,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run29
[2019-04-01 19:25:28,420] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1830324e-34 0.0000000e+00 1.1296241e-19 2.7422477e-29 9.9999976e-01
 2.7102988e-07 2.9815327e-30], sum to 1.0000
[2019-04-01 19:25:28,425] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9797
[2019-04-01 19:25:28,437] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 26.0, 27.29004384424816, 22.42520451546778, 1.0, 1.0, 5.284985272093156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5079000.0000, 
sim time next is 5079600.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 26.0, 27.32361076893646, 22.76873024558164, 1.0, 1.0, 3.814808452392317], 
processed observation next is [1.0, 0.8260869565217391, 0.7673130193905818, 0.17, 0.0, 0.0, 1.0, 1.1890872527052088, 0.2276873024558164, 1.0, 1.0, 0.027248631802802264], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5535501], dtype=float32), -0.34313175]. 
=============================================
[2019-04-01 19:25:28,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:28,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:28,924] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run29
[2019-04-01 19:25:29,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:29,534] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:29,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run29
[2019-04-01 19:25:29,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:29,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:29,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run29
[2019-04-01 19:25:30,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:30,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:30,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run29
[2019-04-01 19:25:30,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:30,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:30,341] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run29
[2019-04-01 19:25:30,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:30,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:30,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run29
[2019-04-01 19:25:31,190] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:31,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:31,209] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run29
[2019-04-01 19:25:31,759] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:31,759] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:31,768] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run29
[2019-04-01 19:25:32,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:32,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:32,727] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run29
[2019-04-01 19:25:32,813] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:32,813] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:32,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run29
[2019-04-01 19:25:34,587] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:25:34,587] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:25:34,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run29
[2019-04-01 19:25:39,378] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 5.4607148e-35 5.5057314e-26 2.1603814e-11
 1.0000000e+00 8.7948746e-12], sum to 1.0000
[2019-04-01 19:25:39,379] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6347
[2019-04-01 19:25:39,446] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 29.5, 0.0, 26.0, 22.88248655115899, 6.175395742536717, 0.0, 1.0, 57.59108429332336], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 32400.0000, 
sim time next is 33000.0000, 
raw observation next is [7.7, 93.0, 32.33333333333334, 0.0, 26.0, 23.07906373740434, 6.016510705671084, 0.0, 1.0, 57.3603840436241], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.1077777777777778, 0.0, 1.0, 0.5827233910577628, 0.06016510705671083, 0.0, 1.0, 0.4097170288830293], 
reward next is 0.5903, 
noisyNet noise sample is [array([-1.2195114], dtype=float32), 0.72613966]. 
=============================================
[2019-04-01 19:25:39,456] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[65.49564]
 [65.1829 ]
 [64.91988]
 [64.53668]
 [64.20018]], R is [[65.53239441]
 [65.46570587]
 [65.39698029]
 [65.32440186]
 [65.23883057]].
[2019-04-01 19:25:44,094] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1969190e-33 0.0000000e+00
 1.0000000e+00 5.5315554e-36], sum to 1.0000
[2019-04-01 19:25:44,094] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6605
[2019-04-01 19:25:44,145] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.100000000000001, 86.0, 88.5, 0.0, 26.0, 24.47021906864107, 6.356683086638308, 0.0, 1.0, 27.5740620756966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 48000.0000, 
sim time next is 48600.0000, 
raw observation next is [8.0, 86.0, 87.0, 0.0, 26.0, 24.45121447508909, 6.332035739686073, 0.0, 1.0, 29.80799809365152], 
processed observation next is [0.0, 0.5652173913043478, 0.6842105263157896, 0.86, 0.29, 0.0, 1.0, 0.7787449250127269, 0.06332035739686073, 0.0, 1.0, 0.21291427209751085], 
reward next is 0.7871, 
noisyNet noise sample is [array([0.33280075], dtype=float32), -0.6735195]. 
=============================================
[2019-04-01 19:25:44,698] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00
 4.2468094e-28 0.0000000e+00], sum to 1.0000
[2019-04-01 19:25:44,700] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3136
[2019-04-01 19:25:44,724] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.04999999999999999, 95.0, 0.0, 0.0, 26.0, 24.32167381429744, 5.926651808986148, 0.0, 1.0, 39.58973994451284], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 85800.0000, 
sim time next is 86400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 24.29889063619081, 5.898019593113379, 0.0, 1.0, 39.60299653754293], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.7569843765986869, 0.058980195931133794, 0.0, 1.0, 0.2828785466967352], 
reward next is 0.7171, 
noisyNet noise sample is [array([-1.6818633], dtype=float32), 0.6294268]. 
=============================================
[2019-04-01 19:25:55,004] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5104699e-37 0.0000000e+00 2.0417278e-34 1.1175425e-33 5.4451555e-20
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:25:55,006] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0598
[2019-04-01 19:25:55,074] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.1, 60.5, 117.0, 0.0, 26.0, 25.9067430050667, 8.064758599318672, 1.0, 1.0, 28.31632022377373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 225000.0000, 
sim time next is 225600.0000, 
raw observation next is [-3.0, 60.0, 106.8333333333333, 0.0, 26.0, 25.45706161621774, 7.344087462912104, 1.0, 1.0, 26.45589591649948], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.6, 0.356111111111111, 0.0, 1.0, 0.9224373737453914, 0.07344087462912104, 1.0, 1.0, 0.18897068511785342], 
reward next is 0.8110, 
noisyNet noise sample is [array([-0.20994534], dtype=float32), -1.8975793]. 
=============================================
[2019-04-01 19:26:04,504] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8163994e-37 0.0000000e+00 0.0000000e+00 3.2504026e-09 7.3247573e-31
 1.0000000e+00 2.9485778e-35], sum to 1.0000
[2019-04-01 19:26:04,504] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5006
[2019-04-01 19:26:04,573] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.38333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 25.41457743724148, 9.0431415923034, 1.0, 1.0, 55.75787618506791], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 328200.0000, 
sim time next is 328800.0000, 
raw observation next is [-12.46666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.3961061111618, 8.951535040953395, 1.0, 1.0, 56.04196212289649], 
processed observation next is [1.0, 0.8260869565217391, 0.11726685133887339, 0.6533333333333334, 0.0, 0.0, 1.0, 0.9137294444516856, 0.08951535040953396, 1.0, 1.0, 0.4002997294492607], 
reward next is 0.5997, 
noisyNet noise sample is [array([-0.40332234], dtype=float32), 0.23996218]. 
=============================================
[2019-04-01 19:26:05,919] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.3256556e-22 3.2642323e-24 6.4130540e-26 4.7409125e-28 7.2941813e-09
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:26:05,919] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2907
[2019-04-01 19:26:06,023] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.41666666666667, 47.83333333333334, 16.0, 162.0, 26.0, 25.71128590905707, 8.780831185943876, 1.0, 1.0, 19.06977811356025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 319800.0000, 
sim time next is 320400.0000, 
raw observation next is [-10.6, 49.0, 12.0, 123.0, 26.0, 25.36981621291903, 8.816303219043183, 1.0, 1.0, 64.83640591592665], 
processed observation next is [1.0, 0.7391304347826086, 0.1689750692520776, 0.49, 0.04, 0.13591160220994475, 1.0, 0.9099737447027186, 0.08816303219043183, 1.0, 1.0, 0.46311718511376176], 
reward next is 0.5369, 
noisyNet noise sample is [array([-1.051168], dtype=float32), 0.13223235]. 
=============================================
[2019-04-01 19:26:09,301] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 3.3529007e-33 9.1143916e-14 3.7940171e-34
 1.0000000e+00 6.0353622e-34], sum to 1.0000
[2019-04-01 19:26:09,301] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5309
[2019-04-01 19:26:09,327] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.433333333333334, 84.66666666666667, 0.0, 0.0, 26.0, 24.82158625215842, 6.420816051756439, 0.0, 1.0, 39.40923965709624], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 530400.0000, 
sim time next is 531000.0000, 
raw observation next is [3.25, 84.0, 0.0, 0.0, 26.0, 24.80191238987113, 6.382538607371605, 0.0, 1.0, 39.46572170702644], 
processed observation next is [0.0, 0.13043478260869565, 0.5526315789473685, 0.84, 0.0, 0.0, 1.0, 0.8288446271244473, 0.06382538607371604, 0.0, 1.0, 0.281898012193046], 
reward next is 0.7181, 
noisyNet noise sample is [array([-0.9020822], dtype=float32), 1.0922639]. 
=============================================
[2019-04-01 19:26:09,337] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[68.6231 ]
 [68.72999]
 [68.47611]
 [68.22233]
 [67.9686 ]], R is [[68.53591156]
 [68.56906128]
 [68.60225677]
 [68.63547516]
 [68.66867828]].
[2019-04-01 19:26:15,168] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:26:15,194] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4783
[2019-04-01 19:26:15,276] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8, 32.66666666666666, 114.8333333333333, 0.0, 26.0, 25.35157404753735, 6.862430925299743, 1.0, 1.0, 28.59276338957079], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 481200.0000, 
sim time next is 481800.0000, 
raw observation next is [-0.7, 33.83333333333334, 110.6666666666667, 0.0, 26.0, 25.54647595848096, 7.043907232845837, 1.0, 1.0, 24.51488819315014], 
processed observation next is [1.0, 0.5652173913043478, 0.443213296398892, 0.33833333333333343, 0.368888888888889, 0.0, 1.0, 0.9352108512115659, 0.07043907232845836, 1.0, 1.0, 0.1751063442367867], 
reward next is 0.8249, 
noisyNet noise sample is [array([1.7383233], dtype=float32), -0.62922376]. 
=============================================
[2019-04-01 19:26:18,568] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:26:18,569] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0653
[2019-04-01 19:26:18,591] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.6, 91.66666666666666, 0.0, 0.0, 26.0, 24.91053995115281, 6.665548300368049, 0.0, 1.0, 39.09749170654595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 520800.0000, 
sim time next is 521400.0000, 
raw observation next is [4.8, 90.33333333333334, 0.0, 0.0, 26.0, 24.905949889914, 6.65356156574353, 0.0, 1.0, 39.08723383459264], 
processed observation next is [0.0, 0.0, 0.5955678670360112, 0.9033333333333334, 0.0, 0.0, 1.0, 0.8437071271305717, 0.0665356156574353, 0.0, 1.0, 0.27919452738994743], 
reward next is 0.7208, 
noisyNet noise sample is [array([1.9734174], dtype=float32), -0.5586176]. 
=============================================
[2019-04-01 19:26:33,869] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.2755254e-30
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:26:33,869] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5377
[2019-04-01 19:26:33,879] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.22496183108018, 5.243130902227001, 0.0, 1.0, 41.4254613523477], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 714600.0000, 
sim time next is 715200.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.24554845779092, 5.237030155997386, 0.0, 1.0, 41.41939092178127], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 1.0, 0.749364065398703, 0.05237030155997387, 0.0, 1.0, 0.29585279229843764], 
reward next is 0.7041, 
noisyNet noise sample is [array([-0.28445885], dtype=float32), -0.11461078]. 
=============================================
[2019-04-01 19:26:36,218] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:26:36,218] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9350
[2019-04-01 19:26:36,274] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.6, 100.0, 80.83333333333333, 0.0, 26.0, 24.41555646978674, 8.972573566609647, 0.0, 1.0, 29.38359405285143], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1248000.0000, 
sim time next is 1248600.0000, 
raw observation next is [14.5, 100.0, 83.66666666666667, 0.0, 26.0, 24.67433083444246, 9.426624074267844, 0.0, 1.0, 23.29811932961172], 
processed observation next is [0.0, 0.43478260869565216, 0.8642659279778394, 1.0, 0.2788888888888889, 0.0, 1.0, 0.8106186906346373, 0.09426624074267843, 0.0, 1.0, 0.16641513806865513], 
reward next is 0.8336, 
noisyNet noise sample is [array([1.4177693], dtype=float32), 1.2270627]. 
=============================================
[2019-04-01 19:26:46,097] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.8690334e-13 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:26:46,100] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3597
[2019-04-01 19:26:46,139] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.566666666666666, 84.0, 0.0, 0.0, 26.0, 25.15841423498958, 7.798313812590959, 1.0, 1.0, 29.15139168894314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 848400.0000, 
sim time next is 849000.0000, 
raw observation next is [-3.483333333333333, 83.5, 0.0, 0.0, 26.0, 25.04082541923385, 7.536619561418997, 1.0, 1.0, 27.49518294986958], 
processed observation next is [1.0, 0.8260869565217391, 0.3661126500461681, 0.835, 0.0, 0.0, 1.0, 0.8629750598905501, 0.07536619561418996, 1.0, 1.0, 0.19639416392763986], 
reward next is 0.8036, 
noisyNet noise sample is [array([-0.6979728], dtype=float32), 1.4444373]. 
=============================================
[2019-04-01 19:26:46,145] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[72.96609 ]
 [73.63704 ]
 [74.28439 ]
 [72.86173 ]
 [72.894554]], R is [[72.50610352]
 [72.57282257]
 [72.6244278 ]
 [72.64315033]
 [72.67060852]].
[2019-04-01 19:26:52,275] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3199834e-21 5.2489222e-25 1.8840922e-22 5.2482024e-02 1.0718918e-05
 9.4750732e-01 0.0000000e+00], sum to 1.0000
[2019-04-01 19:26:52,279] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6101
[2019-04-01 19:26:52,292] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.17502614830732, 9.089541894076334, 1.0, 1.0, 13.69705165492266], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1708800.0000, 
sim time next is 1709400.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 24.95618763441186, 8.67637162513414, 1.0, 1.0, 17.77877321709063], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 1.0, 0.850883947773123, 0.0867637162513414, 1.0, 1.0, 0.12699123726493308], 
reward next is 0.8730, 
noisyNet noise sample is [array([0.5627775], dtype=float32), -0.3592121]. 
=============================================
[2019-04-01 19:26:53,231] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.62653211e-30
 2.28300511e-23 1.00000000e+00 1.27432465e-30], sum to 1.0000
[2019-04-01 19:26:53,238] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1889
[2019-04-01 19:26:53,269] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.783333333333333, 74.66666666666667, 89.0, 102.3333333333333, 26.0, 25.87248644451368, 11.56400324577864, 1.0, 1.0, 12.75972910247654], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1588200.0000, 
sim time next is 1588800.0000, 
raw observation next is [6.966666666666667, 73.33333333333334, 102.0, 119.1666666666667, 26.0, 25.93943628441034, 12.25696920530977, 1.0, 1.0, 12.84898242097035], 
processed observation next is [1.0, 0.391304347826087, 0.6555863342566944, 0.7333333333333334, 0.34, 0.13167587476979745, 1.0, 0.9913480406300488, 0.1225696920530977, 1.0, 1.0, 0.09177844586407392], 
reward next is 0.0054, 
noisyNet noise sample is [array([-0.46219587], dtype=float32), 0.15170485]. 
=============================================
[2019-04-01 19:26:57,592] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:26:57,593] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1771
[2019-04-01 19:26:57,600] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.8, 88.0, 0.0, 0.0, 26.0, 23.99746875561257, 6.679288398377142, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1218600.0000, 
sim time next is 1219200.0000, 
raw observation next is [15.7, 89.66666666666667, 0.0, 0.0, 26.0, 23.9691600854516, 6.624505983573236, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.8975069252077563, 0.8966666666666667, 0.0, 0.0, 1.0, 0.7098800122073712, 0.06624505983573237, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.57390785], dtype=float32), 0.27359977]. 
=============================================
[2019-04-01 19:27:01,116] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6342783e-27 8.6403208e-32 2.2560096e-32 8.9890804e-15 3.7540080e-12
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:27:01,118] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5809
[2019-04-01 19:27:01,129] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.9000000000000001, 92.0, 106.1666666666667, 0.0, 26.0, 26.07559431210849, 12.42949068974871, 1.0, 1.0, 10.80178008176386], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1334400.0000, 
sim time next is 1335000.0000, 
raw observation next is [1.0, 92.0, 110.3333333333333, 0.0, 26.0, 26.08525815646549, 12.47193113433713, 1.0, 1.0, 10.19739325300331], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.92, 0.36777777777777765, 0.0, 1.0, 1.012179736637927, 0.12471931134337129, 1.0, 1.0, 0.07283852323573793], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.37978432], dtype=float32), -0.055707045]. 
=============================================
[2019-04-01 19:27:01,140] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[48.07688 ]
 [47.976036]
 [46.86342 ]
 [45.501663]
 [44.050743]], R is [[47.72071838]
 [47.2435112 ]
 [46.7710762 ]
 [46.30336761]
 [45.84033585]].
[2019-04-01 19:27:07,132] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 1.272372e-30 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 19:27:07,132] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2635
[2019-04-01 19:27:07,176] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 92.0, 12.0, 0.0, 26.0, 25.65969835723619, 11.17657896497243, 1.0, 1.0, 24.32262173760902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1325400.0000, 
sim time next is 1326000.0000, 
raw observation next is [0.9000000000000001, 92.0, 15.0, 0.0, 26.0, 25.56817827638475, 11.65835174303132, 1.0, 1.0, 24.71870220238684], 
processed observation next is [1.0, 0.34782608695652173, 0.48753462603878117, 0.92, 0.05, 0.0, 1.0, 0.9383111823406785, 0.1165835174303132, 1.0, 1.0, 0.17656215858847743], 
reward next is 0.1601, 
noisyNet noise sample is [array([-0.8517072], dtype=float32), -1.1765983]. 
=============================================
[2019-04-01 19:27:07,183] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[51.823456]
 [54.45611 ]
 [57.010685]
 [59.892025]
 [62.945023]], R is [[49.13083649]
 [48.99516678]
 [48.70562363]
 [48.26935577]
 [47.78666306]].
[2019-04-01 19:27:11,739] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:27:11,739] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8235
[2019-04-01 19:27:11,754] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.8, 60.33333333333334, 0.0, 0.0, 26.0, 26.32761854042494, 15.06088915735486, 1.0, 1.0, 5.259255076183035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1534800.0000, 
sim time next is 1535400.0000, 
raw observation next is [9.7, 60.5, 0.0, 0.0, 26.0, 26.25690478493187, 14.75239396836549, 1.0, 1.0, 4.077190068068389], 
processed observation next is [1.0, 0.782608695652174, 0.7313019390581719, 0.605, 0.0, 0.0, 1.0, 1.0367006835616956, 0.1475239396836549, 1.0, 1.0, 0.02912278620048849], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1678237], dtype=float32), 1.5204087]. 
=============================================
[2019-04-01 19:27:13,801] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 9.4167059e-37 9.9176939e-20 0.0000000e+00
 1.0000000e+00 5.0918437e-37], sum to 1.0000
[2019-04-01 19:27:13,802] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8098
[2019-04-01 19:27:13,850] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6000000000000001, 92.0, 92.0, 0.0, 26.0, 24.92607809178448, 8.152812475381646, 1.0, 1.0, 41.23845676669496], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1429800.0000, 
sim time next is 1430400.0000, 
raw observation next is [0.7000000000000001, 92.0, 91.0, 0.0, 26.0, 24.91268109999911, 9.596075315341132, 1.0, 1.0, 42.70222093796009], 
processed observation next is [1.0, 0.5652173913043478, 0.4819944598337951, 0.92, 0.30333333333333334, 0.0, 1.0, 0.8446687285713013, 0.09596075315341132, 1.0, 1.0, 0.30501586384257207], 
reward next is 0.6950, 
noisyNet noise sample is [array([-1.215976], dtype=float32), -1.0384064]. 
=============================================
[2019-04-01 19:27:27,621] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 6.7545682e-19 1.7753923e-28
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:27:27,621] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8614
[2019-04-01 19:27:27,682] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 83.66666666666667, 105.6666666666667, 0.0, 26.0, 24.97708833145463, 7.9991674335656, 0.0, 1.0, 38.61103922395348], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1779000.0000, 
sim time next is 1779600.0000, 
raw observation next is [-2.8, 84.33333333333334, 102.3333333333333, 0.0, 26.0, 24.93178450154349, 8.000294449586326, 0.0, 1.0, 42.39324239729558], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8433333333333334, 0.341111111111111, 0.0, 1.0, 0.8473977859347842, 0.08000294449586326, 0.0, 1.0, 0.302808874266397], 
reward next is 0.6972, 
noisyNet noise sample is [array([-0.27713493], dtype=float32), 0.09898795]. 
=============================================
[2019-04-01 19:27:32,406] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:27:32,407] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8438
[2019-04-01 19:27:32,451] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 83.0, 129.0, 42.0, 26.0, 24.98788807105153, 7.710475116410247, 0.0, 1.0, 47.82087408034723], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1868400.0000, 
sim time next is 1869000.0000, 
raw observation next is [-4.5, 81.66666666666667, 110.0, 27.99999999999999, 26.0, 25.0701010523715, 7.809386042591512, 0.0, 1.0, 44.39019116266824], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.8166666666666668, 0.36666666666666664, 0.030939226519337004, 1.0, 0.8671572931959284, 0.07809386042591511, 0.0, 1.0, 0.31707279401905886], 
reward next is 0.6829, 
noisyNet noise sample is [array([-1.1334195], dtype=float32), -2.6105351]. 
=============================================
[2019-04-01 19:27:32,460] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[62.910664]
 [62.980183]
 [63.21086 ]
 [63.406902]
 [63.539062]], R is [[63.08805847]
 [63.11560059]
 [63.14694214]
 [63.21126938]
 [63.31848907]].
[2019-04-01 19:27:35,601] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 3.194371e-34], sum to 1.0000
[2019-04-01 19:27:35,601] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0888
[2019-04-01 19:27:35,662] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.1, 71.66666666666667, 162.6666666666667, 54.00000000000001, 26.0, 24.90714776556029, 7.324198689859554, 0.0, 1.0, 54.86217623240849], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1857000.0000, 
sim time next is 1857600.0000, 
raw observation next is [-5.0, 71.0, 152.0, 40.5, 26.0, 25.02525794808502, 7.510264661647139, 0.0, 1.0, 50.7773164860109], 
processed observation next is [0.0, 0.5217391304347826, 0.32409972299168976, 0.71, 0.5066666666666667, 0.044751381215469614, 1.0, 0.8607511354407171, 0.0751026466164714, 0.0, 1.0, 0.36269511775722074], 
reward next is 0.6373, 
noisyNet noise sample is [array([1.1228884], dtype=float32), -1.0559266]. 
=============================================
[2019-04-01 19:27:37,704] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:27:37,704] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4503
[2019-04-01 19:27:37,723] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 54.0, 0.0, 0.0, 26.0, 25.42812354880256, 9.667555260603242, 0.0, 1.0, 39.933081585261], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2322000.0000, 
sim time next is 2322600.0000, 
raw observation next is [-1.7, 54.33333333333333, 0.0, 0.0, 26.0, 25.46117587018824, 9.61632135730087, 0.0, 1.0, 37.21229001486268], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5433333333333333, 0.0, 0.0, 1.0, 0.9230251243126055, 0.09616321357300871, 0.0, 1.0, 0.26580207153473345], 
reward next is 0.7342, 
noisyNet noise sample is [array([0.76322657], dtype=float32), 1.6066861]. 
=============================================
[2019-04-01 19:27:38,412] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4392747e-30 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:27:38,414] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9510
[2019-04-01 19:27:38,455] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.34895617089733, 5.864029012776093, 0.0, 1.0, 41.48856297586708], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1990200.0000, 
sim time next is 1990800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.34764762855979, 5.809380446007481, 0.0, 1.0, 41.39251005291064], 
processed observation next is [1.0, 0.043478260869565216, 0.2908587257617729, 0.87, 0.0, 0.0, 1.0, 0.7639496612228273, 0.05809380446007481, 0.0, 1.0, 0.29566078609221885], 
reward next is 0.7043, 
noisyNet noise sample is [array([-0.18515351], dtype=float32), 0.935464]. 
=============================================
[2019-04-01 19:27:54,422] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:27:54,423] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3404
[2019-04-01 19:27:54,492] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 65.0, 56.0, 0.0, 26.0, 24.99397830201091, 8.422867253984835, 1.0, 1.0, 55.69217121941692], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2131200.0000, 
sim time next is 2131800.0000, 
raw observation next is [-4.5, 65.5, 46.0, 0.0, 26.0, 25.4122725737468, 9.597064798256547, 1.0, 1.0, 34.49092330039029], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.655, 0.15333333333333332, 0.0, 1.0, 0.9160389391066859, 0.09597064798256547, 1.0, 1.0, 0.24636373785993065], 
reward next is 0.7536, 
noisyNet noise sample is [array([1.577401], dtype=float32), -0.8551508]. 
=============================================
[2019-04-01 19:27:57,442] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.1589714e-36 0.0000000e+00 0.0000000e+00 2.0580607e-14 4.7412755e-20
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:27:57,446] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5040
[2019-04-01 19:27:57,478] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 24.94418851494041, 8.589347026295785, 0.0, 1.0, 46.9928859228316], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2146200.0000, 
sim time next is 2146800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.00576958841173, 8.79307248290091, 0.0, 1.0, 45.3914210961645], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 1.0, 0.8579670840588187, 0.08793072482900911, 0.0, 1.0, 0.324224436401175], 
reward next is 0.6758, 
noisyNet noise sample is [array([0.73261845], dtype=float32), -1.3442127]. 
=============================================
[2019-04-01 19:27:59,375] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:27:59,376] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9925
[2019-04-01 19:27:59,406] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.916666666666667, 71.0, 112.0, 150.3333333333333, 26.0, 25.7517123297804, 8.081612777745976, 1.0, 1.0, 19.74099847074931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2196600.0000, 
sim time next is 2197200.0000, 
raw observation next is [-4.833333333333334, 71.0, 114.5, 75.16666666666664, 26.0, 25.73100860025067, 7.943714432818527, 1.0, 1.0, 19.02827802441716], 
processed observation next is [1.0, 0.43478260869565216, 0.32871652816251157, 0.71, 0.38166666666666665, 0.08305709023941066, 1.0, 0.9615726571786674, 0.07943714432818527, 1.0, 1.0, 0.1359162716029797], 
reward next is 0.8641, 
noisyNet noise sample is [array([-0.52816707], dtype=float32), 0.68952394]. 
=============================================
[2019-04-01 19:28:04,679] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0523354e-19 1.2935025e-32 5.9245078e-13 1.8871676e-14 1.2481636e-35
 1.0000000e+00 1.5180511e-37], sum to 1.0000
[2019-04-01 19:28:04,680] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4852
[2019-04-01 19:28:04,701] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.900000000000002, 91.0, 0.0, 0.0, 26.0, 23.67399775840759, 5.293013534056723, 0.0, 1.0, 42.7292796163137], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2265600.0000, 
sim time next is 2266200.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.66466141088107, 5.289652723717412, 0.0, 1.0, 42.673091059801], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 1.0, 0.6663802015544386, 0.05289652723717411, 0.0, 1.0, 0.30480779328429286], 
reward next is 0.6952, 
noisyNet noise sample is [array([1.9147142], dtype=float32), -1.8444295]. 
=============================================
[2019-04-01 19:28:20,595] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:28:20,595] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4096
[2019-04-01 19:28:20,614] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.3, 27.0, 70.0, 732.0, 26.0, 24.963044215217, 7.378232901165002, 0.0, 1.0, 18.42683566453765], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2473200.0000, 
sim time next is 2473800.0000, 
raw observation next is [3.3, 26.83333333333333, 67.33333333333333, 714.6666666666666, 26.0, 24.98037229046079, 7.391739580314095, 0.0, 1.0, 15.52044956320763], 
processed observation next is [0.0, 0.6521739130434783, 0.554016620498615, 0.2683333333333333, 0.22444444444444442, 0.7896869244935543, 1.0, 0.8543388986372555, 0.07391739580314095, 0.0, 1.0, 0.11086035402291164], 
reward next is 0.8891, 
noisyNet noise sample is [array([0.2546232], dtype=float32), -0.43074924]. 
=============================================
[2019-04-01 19:28:26,601] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0384600e-15 1.6959098e-26
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:28:26,604] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0396
[2019-04-01 19:28:26,622] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.39907340004147, 8.979193603895574, 0.0, 1.0, 38.47304896989335], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2584800.0000, 
sim time next is 2585400.0000, 
raw observation next is [-2.983333333333333, 56.5, 0.0, 0.0, 26.0, 25.37339281801696, 8.236821060671872, 0.0, 1.0, 42.3672785876283], 
processed observation next is [1.0, 0.9565217391304348, 0.37996306555863346, 0.565, 0.0, 0.0, 1.0, 0.9104846882881371, 0.08236821060671871, 0.0, 1.0, 0.3026234184830593], 
reward next is 0.6974, 
noisyNet noise sample is [array([0.18995598], dtype=float32), -1.8581796]. 
=============================================
[2019-04-01 19:28:27,992] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:28:27,993] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9952
[2019-04-01 19:28:28,021] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.366666666666667, 59.33333333333334, 212.0, 188.3333333333333, 26.0, 25.65795563348393, 8.230670209048805, 1.0, 1.0, 26.62184192206891], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2632800.0000, 
sim time next is 2633400.0000, 
raw observation next is [-3.1, 58.0, 224.0, 171.0, 26.0, 25.68605207214756, 8.389340217190615, 1.0, 1.0, 30.95762087288961], 
processed observation next is [1.0, 0.4782608695652174, 0.37673130193905824, 0.58, 0.7466666666666667, 0.18895027624309393, 1.0, 0.9551502960210801, 0.08389340217190615, 1.0, 1.0, 0.22112586337778292], 
reward next is 0.7789, 
noisyNet noise sample is [array([0.22915433], dtype=float32), -0.69307697]. 
=============================================
[2019-04-01 19:28:34,119] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 4.1548297e-21], sum to 1.0000
[2019-04-01 19:28:34,121] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6575
[2019-04-01 19:28:34,187] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 83.0, 40.0, 165.0, 26.0, 25.11716504368821, 7.241979276268576, 1.0, 1.0, 65.94677996221506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2707200.0000, 
sim time next is 2707800.0000, 
raw observation next is [-14.83333333333333, 84.33333333333334, 53.33333333333334, 220.0, 26.0, 25.28681711383604, 7.504507756818261, 1.0, 1.0, 62.75165032454131], 
processed observation next is [1.0, 0.34782608695652173, 0.05170821791320413, 0.8433333333333334, 0.1777777777777778, 0.2430939226519337, 1.0, 0.8981167305480058, 0.07504507756818261, 1.0, 1.0, 0.44822607374672363], 
reward next is 0.5518, 
noisyNet noise sample is [array([1.0765257], dtype=float32), 0.52051276]. 
=============================================
[2019-04-01 19:28:35,358] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:28:35,358] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4020
[2019-04-01 19:28:35,403] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 58.0, 109.6666666666667, 789.8333333333334, 26.0, 26.74116102173645, 15.57170790299039, 1.0, 1.0, 29.77056674648613], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2726400.0000, 
sim time next is 2727000.0000, 
raw observation next is [-5.4, 57.5, 109.0, 788.0, 26.0, 26.87434870211889, 16.37396859796722, 1.0, 1.0, 28.00753662773384], 
processed observation next is [1.0, 0.5652173913043478, 0.31301939058171746, 0.575, 0.36333333333333334, 0.8707182320441988, 1.0, 1.1249069574455557, 0.1637396859796722, 1.0, 1.0, 0.20005383305524171], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8791106], dtype=float32), 1.336003]. 
=============================================
[2019-04-01 19:28:35,415] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[62.743446]
 [62.873287]
 [63.058876]
 [62.901207]
 [63.046356]], R is [[61.96496582]
 [61.34531784]
 [60.73186493]
 [60.12454605]
 [59.69554138]].
[2019-04-01 19:28:52,450] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:28:52,454] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2118
[2019-04-01 19:28:52,488] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 54.0, 106.8333333333333, 766.6666666666666, 26.0, 25.20665315486291, 7.760151084233233, 0.0, 1.0, 19.79466333339518], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3062400.0000, 
sim time next is 3063000.0000, 
raw observation next is [-4.0, 54.0, 107.6666666666667, 774.3333333333333, 26.0, 25.13662130429118, 7.726010650334719, 0.0, 1.0, 18.67103683835729], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.358888888888889, 0.8556169429097605, 1.0, 0.8766601863273112, 0.07726010650334719, 0.0, 1.0, 0.13336454884540921], 
reward next is 0.8666, 
noisyNet noise sample is [array([0.31611055], dtype=float32), -0.95744324]. 
=============================================
[2019-04-01 19:28:52,492] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[60.434437]
 [59.847755]
 [59.281853]
 [58.85586 ]
 [58.44357 ]], R is [[61.39195633]
 [61.63664627]
 [61.87008286]
 [62.09119034]
 [62.29883575]].
[2019-04-01 19:28:56,363] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:28:56,364] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9548
[2019-04-01 19:28:56,378] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.1666666666666666, 39.5, 94.0, 741.0, 26.0, 25.10855399634605, 8.390872837992665, 0.0, 1.0, 17.30621380394449], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3077400.0000, 
sim time next is 3078000.0000, 
raw observation next is [0.0, 39.0, 91.5, 724.0, 26.0, 25.10149002501038, 8.406858004934302, 0.0, 1.0, 17.81677490733791], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.39, 0.305, 0.8, 1.0, 0.8716414321443402, 0.08406858004934302, 0.0, 1.0, 0.12726267790955648], 
reward next is 0.8727, 
noisyNet noise sample is [array([1.0432504], dtype=float32), 2.3236778]. 
=============================================
[2019-04-01 19:28:56,382] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.145164]
 [85.02586 ]
 [84.89949 ]
 [84.806175]
 [84.715004]], R is [[85.25315857]
 [85.27700806]
 [85.30490875]
 [85.33482361]
 [85.36860657]].
[2019-04-01 19:28:56,962] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.0292019e-34], sum to 1.0000
[2019-04-01 19:28:56,963] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4154
[2019-04-01 19:28:56,975] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.333333333333334, 97.66666666666667, 113.0, 795.1666666666666, 26.0, 26.85209838014629, 16.04652439309822, 1.0, 1.0, 6.796295104032524], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3151200.0000, 
sim time next is 3151800.0000, 
raw observation next is [7.5, 96.5, 114.0, 805.0, 26.0, 26.8288345845389, 16.6097849981159, 1.0, 1.0, 6.410460138527338], 
processed observation next is [1.0, 0.4782608695652174, 0.6703601108033241, 0.965, 0.38, 0.8895027624309392, 1.0, 1.1184049406484142, 0.16609784998115898, 1.0, 1.0, 0.04578900098948099], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.32783586], dtype=float32), -1.0888796]. 
=============================================
[2019-04-01 19:28:59,443] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.6775260e-17 9.6758935e-16 1.2005556e-16 1.8287118e-36 1.5476485e-12
 1.0000000e+00 1.8775610e-09], sum to 1.0000
[2019-04-01 19:28:59,445] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4800
[2019-04-01 19:28:59,466] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.4, 99.33333333333334, 62.33333333333333, 529.0, 26.0, 26.87122200390336, 21.06640787755347, 1.0, 1.0, 1.216249668251368], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3169200.0000, 
sim time next is 3169800.0000, 
raw observation next is [6.3, 99.5, 58.0, 499.0, 26.0, 26.03145716255735, 18.73886772558897, 1.0, 1.0, 1.12824018906161], 
processed observation next is [1.0, 0.6956521739130435, 0.6371191135734073, 0.995, 0.19333333333333333, 0.5513812154696133, 1.0, 1.0044938803653356, 0.1873886772558897, 1.0, 1.0, 0.008058858493297214], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.740179], dtype=float32), 2.3928864]. 
=============================================
[2019-04-01 19:29:12,425] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:29:12,425] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8187
[2019-04-01 19:29:12,453] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.110223024625157e-16, 56.00000000000001, 97.0, 618.6666666666667, 26.0, 26.3130371180175, 11.75787638404479, 1.0, 1.0, 19.21989655420651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3403200.0000, 
sim time next is 3403800.0000, 
raw observation next is [0.5, 54.0, 99.0, 658.0, 26.0, 26.39370501322109, 12.21588027878468, 1.0, 1.0, 18.02914474941984], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.54, 0.33, 0.7270718232044199, 1.0, 1.0562435733172986, 0.1221588027878468, 1.0, 1.0, 0.12877960535299884], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5588249], dtype=float32), -0.22751108]. 
=============================================
[2019-04-01 19:29:13,929] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-01 19:29:13,930] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 19:29:13,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:29:13,931] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 19:29:13,931] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 19:29:13,931] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:29:13,933] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:29:13,938] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run40
[2019-04-01 19:29:13,959] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run40
[2019-04-01 19:29:13,982] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run40
[2019-04-01 19:29:20,822] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.436875], dtype=float32), -0.78354186]
[2019-04-01 19:29:20,822] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.8, 74.33333333333334, 0.0, 0.0, 26.0, 24.17409462842562, 5.364813549099961, 0.0, 1.0, 54.34227759867746]
[2019-04-01 19:29:20,822] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 19:29:20,822] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.4224267725280181
[2019-04-01 19:29:46,119] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.436875], dtype=float32), -0.78354186]
[2019-04-01 19:29:46,119] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.663974535, 78.61475507, 0.0, 0.0, 26.0, 24.19371027352852, 5.393820895894247, 0.0, 1.0, 39.00628272325012]
[2019-04-01 19:29:46,120] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:29:46,120] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.8048163662902241
[2019-04-01 19:30:22,670] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.436875], dtype=float32), -0.78354186]
[2019-04-01 19:30:22,670] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.4632337219999999, 39.225070985, 0.0, 0.0, 26.0, 24.93976991403058, 7.404092558823021, 0.0, 1.0, 36.15269391251754]
[2019-04-01 19:30:22,670] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:30:22,671] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.9695937303229832
[2019-04-01 19:30:49,808] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.436875], dtype=float32), -0.78354186]
[2019-04-01 19:30:49,808] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.466558991833333, 47.12338276166667, 0.0, 0.0, 26.0, 25.52776059454694, 10.6963440524921, 1.0, 1.0, 7.950870556358498]
[2019-04-01 19:30:49,809] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:30:49,809] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0954393e-16 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sampled 0.4415462322920072
[2019-04-01 19:30:55,989] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 19:31:16,513] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 19:31:19,075] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 19:31:20,099] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 3900000, evaluation results [3900000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 19:31:29,754] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 7.017414e-30], sum to 1.0000
[2019-04-01 19:31:29,755] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1961
[2019-04-01 19:31:29,763] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.1666666666666666, 42.83333333333334, 78.33333333333334, 637.6666666666667, 26.0, 25.37425947902365, 10.17999458831495, 0.0, 1.0, 10.14631062809468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3599400.0000, 
sim time next is 3600000.0000, 
raw observation next is [0.0, 43.0, 74.5, 607.0, 26.0, 25.36554384976981, 10.07403407621097, 0.0, 1.0, 9.531847925359509], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.43, 0.24833333333333332, 0.6707182320441989, 1.0, 0.909363407109973, 0.10074034076210969, 0.0, 1.0, 0.0680846280382822], 
reward next is 0.9319, 
noisyNet noise sample is [array([-1.2211841], dtype=float32), -0.47329387]. 
=============================================
[2019-04-01 19:31:29,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.738   ]
 [84.49977 ]
 [84.285866]
 [84.18977 ]
 [84.15724 ]], R is [[85.05093384]
 [85.12795258]
 [85.20126343]
 [85.26235962]
 [85.30149841]].
[2019-04-01 19:31:31,387] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 8.210291e-38 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 19:31:31,387] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0809
[2019-04-01 19:31:31,407] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.66507483406078, 8.275144195871997, 0.0, 1.0, 21.76627601269155], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3631800.0000, 
sim time next is 3632400.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.6275339269724, 8.095104475598369, 0.0, 1.0, 20.70821467293711], 
processed observation next is [0.0, 0.043478260869565216, 0.7119113573407203, 0.25, 0.0, 0.0, 1.0, 0.9467905609960573, 0.08095104475598369, 0.0, 1.0, 0.14791581909240792], 
reward next is 0.8521, 
noisyNet noise sample is [array([-0.24872331], dtype=float32), -0.01125015]. 
=============================================
[2019-04-01 19:31:32,816] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:31:32,818] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6866
[2019-04-01 19:31:32,832] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.666666666666666, 43.0, 116.3333333333333, 827.1666666666667, 26.0, 25.35776457572169, 9.74556120948917, 0.0, 1.0, 12.08447815241879], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3674400.0000, 
sim time next is 3675000.0000, 
raw observation next is [4.833333333333334, 42.5, 115.6666666666667, 825.3333333333334, 26.0, 25.34486392504248, 9.758094409154348, 0.0, 1.0, 9.336199210873058], 
processed observation next is [0.0, 0.5217391304347826, 0.5964912280701755, 0.425, 0.38555555555555565, 0.9119705340699816, 1.0, 0.9064091321489255, 0.09758094409154347, 0.0, 1.0, 0.06668713722052184], 
reward next is 0.9333, 
noisyNet noise sample is [array([0.32896456], dtype=float32), -2.395312]. 
=============================================
[2019-04-01 19:31:32,845] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.42146]
 [80.2801 ]
 [80.22977]
 [80.3777 ]
 [80.78635]], R is [[80.90150452]
 [81.00617218]
 [81.1463089 ]
 [81.28440094]
 [81.42028046]].
[2019-04-01 19:31:36,614] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1413335e-35 0.0000000e+00 1.5470321e-34 3.3429409e-17 1.0571540e-37
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:31:36,623] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6249
[2019-04-01 19:31:36,634] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 26.0, 25.74137050193847, 9.450568117716783, 0.0, 1.0, 24.10875515973535], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3704400.0000, 
sim time next is 3705000.0000, 
raw observation next is [1.666666666666667, 63.66666666666667, 0.0, 0.0, 26.0, 25.72545312793978, 9.19150299671775, 0.0, 1.0, 22.9085628632152], 
processed observation next is [0.0, 0.9130434782608695, 0.5087719298245615, 0.6366666666666667, 0.0, 0.0, 1.0, 0.9607790182771113, 0.0919150299671775, 0.0, 1.0, 0.16363259188010856], 
reward next is 0.8364, 
noisyNet noise sample is [array([-0.08435639], dtype=float32), 1.1819329]. 
=============================================
[2019-04-01 19:31:36,646] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[76.88251]
 [76.76639]
 [76.60733]
 [76.61847]
 [76.49948]], R is [[76.98547363]
 [77.04341888]
 [77.09159088]
 [77.1292038 ]
 [77.15590668]].
[2019-04-01 19:31:46,386] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:31:46,388] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8374
[2019-04-01 19:31:46,409] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.166666666666667, 72.0, 0.0, 0.0, 26.0, 25.28491716563952, 8.308119021996065, 0.0, 1.0, 40.03410581122813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3903000.0000, 
sim time next is 3903600.0000, 
raw observation next is [-3.333333333333333, 73.0, 0.0, 0.0, 26.0, 25.30647440117109, 8.1594307191834, 0.0, 1.0, 40.1277554250084], 
processed observation next is [1.0, 0.17391304347826086, 0.37026777469990774, 0.73, 0.0, 0.0, 1.0, 0.9009249144530129, 0.081594307191834, 0.0, 1.0, 0.28662682446434573], 
reward next is 0.7134, 
noisyNet noise sample is [array([1.1671933], dtype=float32), -1.1957154]. 
=============================================
[2019-04-01 19:31:49,972] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.3323454e-26 4.4990000e-33
 1.0000000e+00 2.2055063e-10], sum to 1.0000
[2019-04-01 19:31:49,972] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6263
[2019-04-01 19:31:49,986] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 55.5, 0.0, 0.0, 26.0, 24.9822455941561, 7.814388705985145, 0.0, 1.0, 43.50608716978785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3972600.0000, 
sim time next is 3973200.0000, 
raw observation next is [-9.666666666666666, 56.33333333333333, 0.0, 0.0, 26.0, 24.93231023430795, 7.645878583866957, 0.0, 1.0, 43.5216159428273], 
processed observation next is [1.0, 1.0, 0.19482917820867962, 0.5633333333333332, 0.0, 0.0, 1.0, 0.8474728906154212, 0.07645878583866957, 0.0, 1.0, 0.31086868530590933], 
reward next is 0.6891, 
noisyNet noise sample is [array([0.39457762], dtype=float32), 1.1670911]. 
=============================================
[2019-04-01 19:31:50,757] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6549666e-29 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:31:50,760] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9775
[2019-04-01 19:31:50,786] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 34.5, 0.0, 0.0, 26.0, 26.61191804455496, 15.16816124231882, 1.0, 1.0, 5.525535358343058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4126200.0000, 
sim time next is 4126800.0000, 
raw observation next is [3.0, 35.0, 0.0, 0.0, 26.0, 26.59527234708913, 12.6092560082824, 1.0, 1.0, 5.81518671229639], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.35, 0.0, 0.0, 1.0, 1.0850389067270183, 0.12609256008282402, 1.0, 1.0, 0.04153704794497421], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.1342427], dtype=float32), 0.9595031]. 
=============================================
[2019-04-01 19:31:55,163] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:31:55,164] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2597
[2019-04-01 19:31:55,202] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.64825845915627, 8.767238275546491, 0.0, 1.0, 26.85315368357964], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4221000.0000, 
sim time next is 4221600.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.61155223573422, 8.560644301563512, 0.0, 1.0, 25.44750500774572], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 1.0, 0.9445074622477456, 0.08560644301563512, 0.0, 1.0, 0.18176789291246942], 
reward next is 0.8182, 
noisyNet noise sample is [array([0.23130739], dtype=float32), 1.399532]. 
=============================================
[2019-04-01 19:32:12,291] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:12,291] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:12,299] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run30
[2019-04-01 19:32:19,639] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:32:19,642] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3510
[2019-04-01 19:32:19,657] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8333333333333334, 54.33333333333334, 0.0, 0.0, 26.0, 25.35848178809591, 8.601515859383918, 0.0, 1.0, 40.35380279082181], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4830600.0000, 
sim time next is 4831200.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.39100560706583, 8.669871003531497, 0.0, 1.0, 39.91499794552722], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 1.0, 0.9130008010094041, 0.08669871003531497, 0.0, 1.0, 0.2851071281823373], 
reward next is 0.7149, 
noisyNet noise sample is [array([-0.45861578], dtype=float32), 0.1837418]. 
=============================================
[2019-04-01 19:32:19,910] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:19,911] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:19,921] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run30
[2019-04-01 19:32:23,024] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 3.1576813e-29 1.0000000e+00], sum to 1.0000
[2019-04-01 19:32:23,024] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2864
[2019-04-01 19:32:23,032] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 44.16666666666667, 248.0, 378.6666666666667, 26.0, 25.07452753503435, 8.31414411814953, 0.0, 1.0, 12.77064686544693], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4889400.0000, 
sim time next is 4890000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 242.0, 376.3333333333334, 26.0, 25.06716265558774, 8.331211638911308, 0.0, 1.0, 12.988548619837], 
processed observation next is [0.0, 0.6086956521739131, 0.5272391505078486, 0.4433333333333334, 0.8066666666666666, 0.4158379373848988, 1.0, 0.86673752222682, 0.08331211638911308, 0.0, 1.0, 0.09277534728454999], 
reward next is 0.9072, 
noisyNet noise sample is [array([0.283288], dtype=float32), 0.4095061]. 
=============================================
[2019-04-01 19:32:23,049] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[78.143234]
 [78.14039 ]
 [78.1488  ]
 [78.19659 ]
 [78.20658 ]], R is [[78.29174042]
 [78.41761017]
 [78.54454041]
 [78.6724472 ]
 [78.80860901]].
[2019-04-01 19:32:24,541] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:32:24,541] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4560
[2019-04-01 19:32:24,603] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.43565042905745, 8.661669966981435, 0.0, 1.0, 32.69025925192658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4690800.0000, 
sim time next is 4691400.0000, 
raw observation next is [-0.8333333333333334, 98.66666666666667, 0.0, 0.0, 26.0, 25.35690802087805, 9.21723535847861, 0.0, 1.0, 46.3847252512338], 
processed observation next is [1.0, 0.30434782608695654, 0.43951985226223456, 0.9866666666666667, 0.0, 0.0, 1.0, 0.9081297172682926, 0.09217235358478611, 0.0, 1.0, 0.33131946608024143], 
reward next is 0.6687, 
noisyNet noise sample is [array([-0.725256], dtype=float32), 0.28285372]. 
=============================================
[2019-04-01 19:32:28,791] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 9.5790916e-37 2.8887953e-17 2.0312558e-34
 1.0000000e+00 2.4384455e-31], sum to 1.0000
[2019-04-01 19:32:28,792] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2067
[2019-04-01 19:32:28,805] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.97357537287117, 6.722763261748689, 0.0, 1.0, 38.79762548783525], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4849800.0000, 
sim time next is 4850400.0000, 
raw observation next is [-3.0, 60.00000000000001, 0.0, 0.0, 26.0, 24.94537933533557, 6.655649511832084, 0.0, 1.0, 38.80354173810731], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.6000000000000001, 0.0, 0.0, 1.0, 0.8493399050479385, 0.06655649511832083, 0.0, 1.0, 0.27716815527219507], 
reward next is 0.7228, 
noisyNet noise sample is [array([1.3060334], dtype=float32), -1.3903129]. 
=============================================
[2019-04-01 19:32:33,453] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:33,453] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:33,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run30
[2019-04-01 19:32:37,662] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:37,663] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:37,687] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run30
[2019-04-01 19:32:37,834] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:37,834] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:37,846] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run30
[2019-04-01 19:32:41,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:41,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:41,096] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run30
[2019-04-01 19:32:41,285] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.3157476e-31 0.0000000e+00
 1.0000000e+00 3.2093773e-37], sum to 1.0000
[2019-04-01 19:32:41,286] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1246
[2019-04-01 19:32:41,312] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 37.0, 0.0, 0.0, 26.0, 25.48555126004927, 10.21800711310959, 0.0, 1.0, 37.73561278400453], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5009400.0000, 
sim time next is 5010000.0000, 
raw observation next is [2.333333333333333, 38.0, 0.0, 0.0, 26.0, 25.53391787473988, 10.21608973426756, 0.0, 1.0, 34.53818456935868], 
processed observation next is [1.0, 1.0, 0.5272391505078486, 0.38, 0.0, 0.0, 1.0, 0.9334168392485545, 0.10216089734267561, 0.0, 1.0, 0.24670131835256198], 
reward next is 0.7533, 
noisyNet noise sample is [array([0.30513117], dtype=float32), 0.43382066]. 
=============================================
[2019-04-01 19:32:41,326] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.802124]
 [75.012184]
 [75.27849 ]
 [75.55572 ]
 [76.27789 ]], R is [[74.65222168]
 [74.6361618 ]
 [74.60479736]
 [74.57444   ]
 [74.56965637]].
[2019-04-01 19:32:41,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:41,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:41,550] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run30
[2019-04-01 19:32:41,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:41,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:41,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run30
[2019-04-01 19:32:42,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:42,428] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:42,431] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run30
[2019-04-01 19:32:43,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:43,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:43,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run30
[2019-04-01 19:32:44,180] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:44,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:44,203] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run30
[2019-04-01 19:32:44,480] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:44,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:44,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run30
[2019-04-01 19:32:44,890] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:32:44,891] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6841
[2019-04-01 19:32:44,906] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.4, 20.0, 0.0, 0.0, 26.0, 26.12651099232817, 12.42291738999056, 0.0, 1.0, 6.400721494506749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5094000.0000, 
sim time next is 5094600.0000, 
raw observation next is [8.350000000000001, 22.5, 0.0, 0.0, 26.0, 26.04513995142523, 12.00681130007931, 0.0, 1.0, 6.429086921976931], 
processed observation next is [1.0, 1.0, 0.6939058171745154, 0.225, 0.0, 0.0, 1.0, 1.0064485644893186, 0.12006811300079309, 0.0, 1.0, 0.04592204944269237], 
reward next is 0.9541, 
noisyNet noise sample is [array([1.4771636], dtype=float32), -1.0326861]. 
=============================================
[2019-04-01 19:32:45,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:45,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:45,052] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run30
[2019-04-01 19:32:45,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:45,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:45,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run30
[2019-04-01 19:32:45,978] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:45,978] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:45,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run30
[2019-04-01 19:32:46,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:32:46,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:32:46,088] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run30
[2019-04-01 19:32:55,182] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:32:55,182] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2448
[2019-04-01 19:32:55,247] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 21.0, 0.0, 26.0, 22.30840216808912, 7.208325686404632, 0.0, 1.0, 60.52604755290104], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 30600.0000, 
sim time next is 31200.0000, 
raw observation next is [7.699999999999999, 93.0, 23.83333333333333, 0.0, 26.0, 22.58938655461505, 6.753261531977536, 0.0, 1.0, 58.60546206520611], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.07944444444444443, 0.0, 1.0, 0.51276950780215, 0.06753261531977536, 0.0, 1.0, 0.4186104433229008], 
reward next is 0.5814, 
noisyNet noise sample is [array([0.896801], dtype=float32), -2.167884]. 
=============================================
[2019-04-01 19:32:59,892] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.58298923e-34 0.00000000e+00 0.00000000e+00 2.88924139e-36
 1.09002759e-18 1.00000000e+00 1.08089706e-26], sum to 1.0000
[2019-04-01 19:32:59,893] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6602
[2019-04-01 19:32:59,911] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.416666666666667, 74.83333333333333, 0.0, 0.0, 26.0, 23.37819455406061, 5.35778248549548, 0.0, 1.0, 44.37903157577131], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 107400.0000, 
sim time next is 108000.0000, 
raw observation next is [-6.7, 75.0, 0.0, 0.0, 26.0, 23.32203436827253, 5.372341356540247, 0.0, 1.0, 44.58459147450695], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.75, 0.0, 0.0, 1.0, 0.6174334811817899, 0.053723413565402466, 0.0, 1.0, 0.3184613676750496], 
reward next is 0.6815, 
noisyNet noise sample is [array([1.3763722], dtype=float32), 1.7778317]. 
=============================================
[2019-04-01 19:32:59,930] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[59.64015 ]
 [59.859135]
 [60.075138]
 [60.297028]
 [60.52998 ]], R is [[59.54378128]
 [59.63135147]
 [59.71956253]
 [59.80807877]
 [59.89686584]].
[2019-04-01 19:33:13,400] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8368858e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:33:13,401] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4300
[2019-04-01 19:33:13,417] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.3, 69.0, 0.0, 0.0, 26.0, 23.45336840781442, 5.441237450347259, 0.0, 1.0, 45.64517158365322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 272400.0000, 
sim time next is 273000.0000, 
raw observation next is [-9.4, 69.5, 0.0, 0.0, 26.0, 23.44528466705204, 5.476008116521413, 0.0, 1.0, 45.77815378567987], 
processed observation next is [1.0, 0.13043478260869565, 0.20221606648199447, 0.695, 0.0, 0.0, 1.0, 0.6350406667217199, 0.05476008116521413, 0.0, 1.0, 0.3269868127548562], 
reward next is 0.6730, 
noisyNet noise sample is [array([-0.70674634], dtype=float32), -1.118058]. 
=============================================
[2019-04-01 19:33:13,430] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[58.51569 ]
 [58.546955]
 [58.57579 ]
 [58.634655]
 [58.63406 ]], R is [[58.59352493]
 [58.68155289]
 [58.76974106]
 [58.85772324]
 [58.94570923]].
[2019-04-01 19:33:23,782] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:33:23,782] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6067
[2019-04-01 19:33:23,852] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.2, 38.0, 0.0, 0.0, 26.0, 25.7601389396999, 8.819889128797216, 1.0, 1.0, 29.03285714538704], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 408600.0000, 
sim time next is 409200.0000, 
raw observation next is [-9.3, 38.66666666666666, 0.0, 0.0, 26.0, 25.17581638089504, 8.208537887245269, 1.0, 1.0, 54.26661305707314], 
processed observation next is [1.0, 0.7391304347826086, 0.20498614958448752, 0.38666666666666655, 0.0, 0.0, 1.0, 0.8822594829850056, 0.0820853788724527, 1.0, 1.0, 0.3876186646933795], 
reward next is 0.6124, 
noisyNet noise sample is [array([-1.1357138], dtype=float32), 0.03443168]. 
=============================================
[2019-04-01 19:33:28,455] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:33:28,455] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5942
[2019-04-01 19:33:28,467] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 84.0, 87.0, 0.0, 26.0, 25.45914962786269, 7.119263595939574, 1.0, 1.0, 11.76788889912678], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 903600.0000, 
sim time next is 904200.0000, 
raw observation next is [1.366666666666667, 86.16666666666666, 90.33333333333333, 0.0, 26.0, 25.40348937894897, 7.066934291587773, 1.0, 1.0, 11.18646029663686], 
processed observation next is [1.0, 0.4782608695652174, 0.5004616805170823, 0.8616666666666666, 0.3011111111111111, 0.0, 1.0, 0.9147841969927099, 0.07066934291587773, 1.0, 1.0, 0.07990328783312042], 
reward next is 0.9201, 
noisyNet noise sample is [array([0.49866018], dtype=float32), 1.298895]. 
=============================================
[2019-04-01 19:33:32,841] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:33:32,842] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6738
[2019-04-01 19:33:32,898] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.600000000000001, 89.66666666666667, 0.0, 0.0, 26.0, 25.76807518264165, 10.50569463027878, 1.0, 1.0, 23.87049833576561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 978000.0000, 
sim time next is 978600.0000, 
raw observation next is [9.5, 91.33333333333333, 8.999999999999998, 0.0, 26.0, 25.72613988003954, 10.43308964957449, 1.0, 1.0, 22.32357865471857], 
processed observation next is [1.0, 0.30434782608695654, 0.7257617728531857, 0.9133333333333333, 0.029999999999999995, 0.0, 1.0, 0.9608771257199342, 0.1043308964957449, 1.0, 1.0, 0.1594541332479898], 
reward next is 0.6673, 
noisyNet noise sample is [array([0.08195117], dtype=float32), -0.103883035]. 
=============================================
[2019-04-01 19:33:34,826] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.0416095e-26 3.0393982e-31
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:33:34,830] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0016
[2019-04-01 19:33:34,842] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.3836995467071, 5.657735524945569, 0.0, 1.0, 38.10754372907078], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 888000.0000, 
sim time next is 888600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.46625926682733, 5.688321928546415, 0.0, 1.0, 38.02756608286612], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 1.0, 0.7808941809753327, 0.05688321928546415, 0.0, 1.0, 0.2716254720204723], 
reward next is 0.7284, 
noisyNet noise sample is [array([-0.5874995], dtype=float32), -0.07458618]. 
=============================================
[2019-04-01 19:33:35,568] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:33:35,569] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6728
[2019-04-01 19:33:35,592] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.35, 86.5, 0.0, 0.0, 26.0, 24.55241446722278, 5.947516820158048, 0.0, 1.0, 40.21015677696042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 538200.0000, 
sim time next is 538800.0000, 
raw observation next is [1.266666666666667, 87.0, 0.0, 0.0, 26.0, 24.52779064623851, 5.987600070464083, 0.0, 1.0, 40.22414108988433], 
processed observation next is [0.0, 0.21739130434782608, 0.4976915974145891, 0.87, 0.0, 0.0, 1.0, 0.789684378034073, 0.05987600070464083, 0.0, 1.0, 0.2873152934991738], 
reward next is 0.7127, 
noisyNet noise sample is [array([0.55431324], dtype=float32), -0.9338733]. 
=============================================
[2019-04-01 19:33:48,269] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:33:48,269] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9686
[2019-04-01 19:33:48,283] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.9, 86.33333333333334, 0.0, 0.0, 26.0, 24.01153647143794, 6.718987122364173, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1218000.0000, 
sim time next is 1218600.0000, 
raw observation next is [15.8, 88.0, 0.0, 0.0, 26.0, 23.99746875561257, 6.679288398377142, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9002770083102495, 0.88, 0.0, 0.0, 1.0, 0.7139241079446528, 0.06679288398377142, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0331725], dtype=float32), 2.2989461]. 
=============================================
[2019-04-01 19:33:59,115] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:33:59,115] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1529
[2019-04-01 19:33:59,138] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4, 72.0, 0.0, 0.0, 26.0, 24.593343326568, 5.839252132402369, 0.0, 1.0, 38.48824655041777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 883200.0000, 
sim time next is 883800.0000, 
raw observation next is [-0.3, 72.0, 0.0, 0.0, 26.0, 24.62066423118453, 5.771085079450074, 0.0, 1.0, 38.46418860935103], 
processed observation next is [1.0, 0.21739130434782608, 0.4542936288088643, 0.72, 0.0, 0.0, 1.0, 0.8029520330263615, 0.057710850794500744, 0.0, 1.0, 0.27474420435250735], 
reward next is 0.7253, 
noisyNet noise sample is [array([0.34923372], dtype=float32), 0.578132]. 
=============================================
[2019-04-01 19:34:02,929] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7097613e-23
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:34:02,930] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2143
[2019-04-01 19:34:02,960] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.25, 92.5, 0.0, 0.0, 26.0, 25.39653347874441, 9.426147484133205, 0.0, 1.0, 37.56193063702932], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 952200.0000, 
sim time next is 952800.0000, 
raw observation next is [5.333333333333334, 91.33333333333333, 0.0, 0.0, 26.0, 25.39244125182199, 9.316157435141482, 0.0, 1.0, 36.54093857865492], 
processed observation next is [1.0, 0.0, 0.6103416435826409, 0.9133333333333333, 0.0, 0.0, 1.0, 0.913205893117427, 0.09316157435141482, 0.0, 1.0, 0.2610067041332494], 
reward next is 0.7390, 
noisyNet noise sample is [array([1.222239], dtype=float32), -1.805137]. 
=============================================
[2019-04-01 19:34:09,882] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:34:09,888] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0416
[2019-04-01 19:34:09,904] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.23097329205956, 9.830741917757889, 0.0, 1.0, 39.291801532337], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1390800.0000, 
sim time next is 1391400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.22852734595716, 9.960447278950811, 0.0, 1.0, 39.22966496940329], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.8897896208510231, 0.09960447278950811, 0.0, 1.0, 0.2802118926385949], 
reward next is 0.7198, 
noisyNet noise sample is [array([0.31422538], dtype=float32), -0.05611377]. 
=============================================
[2019-04-01 19:34:11,383] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:34:11,384] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8772
[2019-04-01 19:34:11,404] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.6, 71.0, 0.0, 0.0, 26.0, 25.61076398790819, 14.31669148483139, 0.0, 1.0, 28.79477731729893], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1123200.0000, 
sim time next is 1123800.0000, 
raw observation next is [11.41666666666667, 72.0, 0.0, 0.0, 26.0, 25.64503551985477, 14.50606636591713, 0.0, 1.0, 27.05563456362996], 
processed observation next is [0.0, 0.0, 0.7788550323176363, 0.72, 0.0, 0.0, 1.0, 0.9492907885506813, 0.1450606636591713, 0.0, 1.0, 0.19325453259735687], 
reward next is 0.8067, 
noisyNet noise sample is [array([0.862611], dtype=float32), -1.3045325]. 
=============================================
[2019-04-01 19:34:12,624] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:34:12,624] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4773
[2019-04-01 19:34:12,650] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 46.0, 0.0, 26.0, 26.05430049860171, 11.60587266798134, 1.0, 1.0, 18.88553340176242], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1438200.0000, 
sim time next is 1438800.0000, 
raw observation next is [1.1, 92.0, 41.33333333333334, 0.0, 26.0, 26.09702865192357, 11.58943629838192, 1.0, 1.0, 16.68432994960759], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.1377777777777778, 0.0, 1.0, 1.0138612359890813, 0.1158943629838192, 1.0, 1.0, 0.11917378535433994], 
reward next is 0.2451, 
noisyNet noise sample is [array([-0.19032992], dtype=float32), 1.7356148]. 
=============================================
[2019-04-01 19:34:12,840] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:34:12,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3211
[2019-04-01 19:34:12,851] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 71.0, 0.0, 26.0, 25.8581913296975, 11.56527868328945, 1.0, 1.0, 9.668649465423265], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1348200.0000, 
sim time next is 1348800.0000, 
raw observation next is [1.1, 92.0, 66.5, 0.0, 26.0, 25.86630411665227, 11.39066257021881, 1.0, 1.0, 9.37550769199307], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.22166666666666668, 0.0, 1.0, 0.9809005880931814, 0.1139066257021881, 1.0, 1.0, 0.06696791208566479], 
reward next is 0.3768, 
noisyNet noise sample is [array([-1.5937847], dtype=float32), 0.9548565]. 
=============================================
[2019-04-01 19:34:19,837] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.7205143e-36 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:34:19,838] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0162
[2019-04-01 19:34:19,895] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6, 95.5, 12.0, 0.0, 26.0, 25.48544436031269, 9.054782582648976, 1.0, 1.0, 19.98033904954771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1356600.0000, 
sim time next is 1357200.0000, 
raw observation next is [0.5, 96.0, 9.0, 0.0, 26.0, 25.0293625298632, 9.086112940658928, 1.0, 1.0, 40.30134322409124], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.03, 0.0, 1.0, 0.8613375042661714, 0.09086112940658929, 1.0, 1.0, 0.28786673731493745], 
reward next is 0.7121, 
noisyNet noise sample is [array([0.07020493], dtype=float32), 1.5988322]. 
=============================================
[2019-04-01 19:34:32,382] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:34:32,386] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7642
[2019-04-01 19:34:32,394] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 49.0, 160.5, 0.0, 26.0, 27.1203090770213, 19.03091146495044, 1.0, 1.0, 5.401991302503632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1605600.0000, 
sim time next is 1606200.0000, 
raw observation next is [13.8, 49.0, 155.3333333333333, 0.0, 26.0, 27.14545779952612, 19.48603669967423, 1.0, 1.0, 5.551678384438255], 
processed observation next is [1.0, 0.6086956521739131, 0.844875346260388, 0.49, 0.5177777777777777, 0.0, 1.0, 1.1636368285037315, 0.1948603669967423, 1.0, 1.0, 0.039654845603130394], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.27609292], dtype=float32), -0.64339346]. 
=============================================
[2019-04-01 19:34:35,617] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0555432e-35
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:34:35,617] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4877
[2019-04-01 19:34:35,648] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 83.0, 0.0, 0.0, 26.0, 25.14475558043424, 8.623273226562032, 0.0, 1.0, 41.66617991844424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2152800.0000, 
sim time next is 2153400.0000, 
raw observation next is [-6.800000000000001, 82.83333333333334, 0.0, 0.0, 26.0, 25.11877183589482, 7.802542620844378, 0.0, 1.0, 42.35265427368661], 
processed observation next is [1.0, 0.9565217391304348, 0.2742382271468144, 0.8283333333333335, 0.0, 0.0, 1.0, 0.8741102622706884, 0.07802542620844378, 0.0, 1.0, 0.3025189590977615], 
reward next is 0.6975, 
noisyNet noise sample is [array([1.8243011], dtype=float32), 0.015922789]. 
=============================================
[2019-04-01 19:34:50,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:34:50,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7210
[2019-04-01 19:34:50,565] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 80.33333333333333, 24.33333333333334, 0.0, 26.0, 25.12296798854522, 7.729577017674231, 0.0, 1.0, 40.45642732246834], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1874400.0000, 
sim time next is 1875000.0000, 
raw observation next is [-4.5, 81.66666666666667, 19.66666666666667, 0.0, 26.0, 25.14961928630906, 7.665976084534471, 0.0, 1.0, 33.90499742321472], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.8166666666666668, 0.06555555555555558, 0.0, 1.0, 0.8785170409012945, 0.0766597608453447, 0.0, 1.0, 0.24217855302296232], 
reward next is 0.7578, 
noisyNet noise sample is [array([-2.2151117], dtype=float32), 0.37805316]. 
=============================================
[2019-04-01 19:34:50,568] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[61.41452]
 [61.46843]
 [61.49064]
 [61.74253]
 [62.08261]], R is [[61.49909973]
 [61.59513474]
 [61.66220093]
 [61.65126419]
 [61.70856476]].
[2019-04-01 19:34:57,061] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:34:57,062] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6765
[2019-04-01 19:34:57,079] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.71450687706065, 6.599359957196214, 0.0, 1.0, 38.57918940738812], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2340600.0000, 
sim time next is 2341200.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.77682202221107, 6.554425069008249, 0.0, 1.0, 38.64438143972883], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 1.0, 0.8252602888872959, 0.06554425069008249, 0.0, 1.0, 0.2760312959980631], 
reward next is 0.7240, 
noisyNet noise sample is [array([-0.53682417], dtype=float32), 0.21813282]. 
=============================================
[2019-04-01 19:35:01,181] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.5270689e-35 0.0000000e+00 4.4826159e-30 1.1764871e-01 1.0195122e-11
 1.8904946e-06 8.8234943e-01], sum to 1.0000
[2019-04-01 19:35:01,181] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4675
[2019-04-01 19:35:01,223] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 45.0, 42.5, 37.0, 26.0, 24.94236041056367, 7.529483678409413, 0.0, 1.0, 36.56164816709183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2394000.0000, 
sim time next is 2394600.0000, 
raw observation next is [-0.7833333333333333, 44.83333333333334, 30.33333333333333, 30.0, 26.0, 24.97369464627117, 7.528161286849754, 0.0, 1.0, 34.48926500734775], 
processed observation next is [0.0, 0.7391304347826086, 0.44090489381348114, 0.4483333333333334, 0.1011111111111111, 0.03314917127071823, 1.0, 0.85338494946731, 0.07528161286849754, 0.0, 1.0, 0.24635189290962675], 
reward next is 0.7536, 
noisyNet noise sample is [array([0.5943328], dtype=float32), -0.19096561]. 
=============================================
[2019-04-01 19:35:03,179] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6186131e-18 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:35:03,179] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8139
[2019-04-01 19:35:03,201] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.966666666666667, 83.0, 0.0, 0.0, 26.0, 25.19766527785403, 8.872085158899068, 0.0, 1.0, 41.8125232216616], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2150400.0000, 
sim time next is 2151000.0000, 
raw observation next is [-6.15, 83.0, 0.0, 0.0, 26.0, 25.18721182579698, 8.788121865404277, 0.0, 1.0, 41.99208746662389], 
processed observation next is [1.0, 0.9130434782608695, 0.29224376731301943, 0.83, 0.0, 0.0, 1.0, 0.8838874036852827, 0.08788121865404276, 0.0, 1.0, 0.29994348190445635], 
reward next is 0.7001, 
noisyNet noise sample is [array([0.02379562], dtype=float32), 0.5023587]. 
=============================================
[2019-04-01 19:35:03,210] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[59.883614]
 [60.045273]
 [60.264885]
 [60.651115]
 [61.4514  ]], R is [[60.32616806]
 [60.42424774]
 [60.51600647]
 [60.60623169]
 [60.69488525]].
[2019-04-01 19:35:05,084] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-01 19:35:05,084] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 19:35:05,085] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 19:35:05,085] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:35:05,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:35:05,086] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 19:35:05,087] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:35:05,092] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run41
[2019-04-01 19:35:05,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run41
[2019-04-01 19:35:05,740] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run41
[2019-04-01 19:35:39,078] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.3645605], dtype=float32), -0.8910099]
[2019-04-01 19:35:39,078] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [5.686136578666667, 96.15465191666667, 0.0, 0.0, 26.0, 25.45893099019464, 8.526489443849817, 1.0, 1.0, 26.28112774679265]
[2019-04-01 19:35:39,078] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:35:39,078] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.2997625e-22 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sampled 0.0946261941155575
[2019-04-01 19:35:41,000] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.3645605], dtype=float32), -0.8910099]
[2019-04-01 19:35:41,000] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [14.2, 77.33333333333333, 0.0, 0.0, 26.0, 25.72044068648351, 11.81277963908466, 0.0, 1.0, 10.34626267199364]
[2019-04-01 19:35:41,002] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 19:35:41,003] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.8431959274365226
[2019-04-01 19:35:47,362] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.3645605], dtype=float32), -0.8910099]
[2019-04-01 19:35:47,362] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.3, 83.0, 0.0, 0.0, 26.0, 25.79407126779906, 13.35570243218721, 0.0, 1.0, 22.70482210588401]
[2019-04-01 19:35:47,362] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 19:35:47,363] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.4455385943164415
[2019-04-01 19:36:29,121] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.3645605], dtype=float32), -0.8910099]
[2019-04-01 19:36:29,121] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.683333333333333, 58.33333333333334, 0.0, 0.0, 26.0, 24.78681627036194, 6.828024565699354, 0.0, 1.0, 42.93047047041114]
[2019-04-01 19:36:29,121] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 19:36:29,121] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.17092347500034666
[2019-04-01 19:36:46,492] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 19:36:54,530] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.3645605], dtype=float32), -0.8910099]
[2019-04-01 19:36:54,531] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.266666666666667, 68.0, 0.0, 0.0, 26.0, 25.49951159229884, 10.62957454795425, 0.0, 1.0, 32.48915112665772]
[2019-04-01 19:36:54,531] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 19:36:54,532] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.6456666249218703
[2019-04-01 19:37:06,514] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 19:37:09,830] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 19:37:10,854] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 4000000, evaluation results [4000000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 19:37:12,293] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:37:12,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1709
[2019-04-01 19:37:12,393] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 69.5, 0.0, 0.0, 26.0, 25.43133732810056, 7.865303299809793, 1.0, 1.0, 31.01193548374413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2223000.0000, 
sim time next is 2223600.0000, 
raw observation next is [-4.5, 69.0, 0.0, 0.0, 26.0, 24.92089625002045, 8.049389709968857, 1.0, 1.0, 62.0834430685933], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.69, 0.0, 0.0, 1.0, 0.8458423214314931, 0.08049389709968857, 1.0, 1.0, 0.44345316477566643], 
reward next is 0.5565, 
noisyNet noise sample is [array([-0.2577195], dtype=float32), 0.8737886]. 
=============================================
[2019-04-01 19:37:15,143] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.9817461e-32 4.5839666e-36
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:37:15,145] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5943
[2019-04-01 19:37:15,194] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 73.0, 0.0, 0.0, 26.0, 25.21734592830763, 8.382146615749805, 1.0, 1.0, 31.30243941223893], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2140800.0000, 
sim time next is 2141400.0000, 
raw observation next is [-5.0, 73.5, 0.0, 0.0, 26.0, 25.09396769897767, 8.322536362813475, 1.0, 1.0, 32.13990650909085], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.735, 0.0, 0.0, 1.0, 0.8705668141396673, 0.08322536362813475, 1.0, 1.0, 0.22957076077922034], 
reward next is 0.7704, 
noisyNet noise sample is [array([0.2099267], dtype=float32), -0.5674214]. 
=============================================
[2019-04-01 19:37:17,122] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:37:17,122] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1986
[2019-04-01 19:37:17,149] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.43722094897063, 5.856231752199323, 0.0, 1.0, 41.95705291972857], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2168400.0000, 
sim time next is 2169000.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.44748305398537, 5.766991496813868, 0.0, 1.0, 41.9810736062753], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 1.0, 0.7782118648550528, 0.05766991496813868, 0.0, 1.0, 0.299864811473395], 
reward next is 0.7001, 
noisyNet noise sample is [array([-0.52564627], dtype=float32), 1.3225665]. 
=============================================
[2019-04-01 19:37:17,171] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[66.068565]
 [65.92168 ]
 [65.82076 ]
 [65.62199 ]
 [65.45399 ]], R is [[66.15253448]
 [66.19132233]
 [66.22933197]
 [66.26652527]
 [66.30315399]].
[2019-04-01 19:37:21,958] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:37:21,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9836
[2019-04-01 19:37:22,011] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.583333333333333, 69.66666666666667, 0.0, 0.0, 26.0, 25.61691619296756, 9.258852620866186, 1.0, 1.0, 28.53046434307004], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2227800.0000, 
sim time next is 2228400.0000, 
raw observation next is [-4.6, 70.0, 0.0, 0.0, 26.0, 25.49866553880705, 8.648881147251048, 0.0, 1.0, 29.45448001111254], 
processed observation next is [1.0, 0.8260869565217391, 0.33518005540166207, 0.7, 0.0, 0.0, 1.0, 0.92838079125815, 0.08648881147251047, 0.0, 1.0, 0.21038914293651814], 
reward next is 0.7896, 
noisyNet noise sample is [array([0.8199254], dtype=float32), 0.28345633]. 
=============================================
[2019-04-01 19:37:30,647] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.9813029e-32 1.3067758e-30
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:37:30,648] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0547
[2019-04-01 19:37:30,660] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 64.0, 0.0, 0.0, 26.0, 24.98352979292443, 7.221425444117767, 0.0, 1.0, 37.93719918859738], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2334000.0000, 
sim time next is 2334600.0000, 
raw observation next is [-2.3, 63.5, 0.0, 0.0, 26.0, 24.92968016319843, 7.098067338135434, 0.0, 1.0, 37.98527062040557], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.635, 0.0, 0.0, 1.0, 0.8470971661712043, 0.07098067338135433, 0.0, 1.0, 0.2713233615743255], 
reward next is 0.7287, 
noisyNet noise sample is [array([-0.577107], dtype=float32), 1.2089889]. 
=============================================
[2019-04-01 19:37:31,800] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 5.4433636e-05 0.0000000e+00
 9.9994552e-01 6.0474228e-22], sum to 1.0000
[2019-04-01 19:37:31,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5773
[2019-04-01 19:37:31,816] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.583333333333333, 43.50000000000001, 0.0, 0.0, 26.0, 24.95459953585209, 6.539351581907614, 0.0, 1.0, 42.36579077753552], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2412600.0000, 
sim time next is 2413200.0000, 
raw observation next is [-4.666666666666667, 43.0, 0.0, 0.0, 26.0, 24.9352101818158, 6.453833292265166, 0.0, 1.0, 42.38310482483445], 
processed observation next is [0.0, 0.9565217391304348, 0.3333333333333333, 0.43, 0.0, 0.0, 1.0, 0.8478871688308287, 0.06453833292265167, 0.0, 1.0, 0.3027364630345318], 
reward next is 0.6973, 
noisyNet noise sample is [array([1.859733], dtype=float32), 0.5165253]. 
=============================================
[2019-04-01 19:37:39,666] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.8919197e-06 0.0000000e+00
 9.9999607e-01 0.0000000e+00], sum to 1.0000
[2019-04-01 19:37:39,668] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0442
[2019-04-01 19:37:39,724] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 25.25165076666022, 8.24449719621193, 1.0, 1.0, 19.38916788096436], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2571000.0000, 
sim time next is 2571600.0000, 
raw observation next is [0.1333333333333334, 35.33333333333334, 0.0, 0.0, 26.0, 25.30614252340057, 8.101954616508879, 1.0, 1.0, 17.93733677757329], 
processed observation next is [1.0, 0.782608695652174, 0.46629732225300097, 0.35333333333333344, 0.0, 0.0, 1.0, 0.9008775033429386, 0.08101954616508879, 1.0, 1.0, 0.1281238341255235], 
reward next is 0.8719, 
noisyNet noise sample is [array([0.8917306], dtype=float32), 0.09213962]. 
=============================================
[2019-04-01 19:37:42,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:37:42,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7270
[2019-04-01 19:37:42,140] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 79.0, 0.0, 0.0, 26.0, 23.83417122226449, 5.265506261748032, 0.0, 1.0, 44.22257843683267], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2617200.0000, 
sim time next is 2617800.0000, 
raw observation next is [-7.3, 79.00000000000001, 0.0, 0.0, 26.0, 23.814836183171, 5.62944470864268, 0.0, 1.0, 111.1639994397172], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.7900000000000001, 0.0, 0.0, 1.0, 0.6878337404529999, 0.0562944470864268, 0.0, 1.0, 0.7940285674265515], 
reward next is 0.2060, 
noisyNet noise sample is [array([-0.44726035], dtype=float32), 0.3624503]. 
=============================================
[2019-04-01 19:37:48,964] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9987638e-01 0.0000000e+00
 1.2362542e-04 0.0000000e+00], sum to 1.0000
[2019-04-01 19:37:48,964] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1707
[2019-04-01 19:37:48,986] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 50.0, 52.33333333333334, 110.0, 26.0, 25.85470542274752, 10.20220820671398, 1.0, 1.0, 13.81387866413435], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2652600.0000, 
sim time next is 2653200.0000, 
raw observation next is [0.5, 50.0, 41.0, 103.0, 26.0, 26.05975790440805, 10.5356152354753, 1.0, 1.0, 13.44398922878405], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.5, 0.13666666666666666, 0.1138121546961326, 1.0, 1.008536843486864, 0.105356152354753, 1.0, 1.0, 0.09602849449131463], 
reward next is 0.6897, 
noisyNet noise sample is [array([1.113655], dtype=float32), 0.20267877]. 
=============================================
[2019-04-01 19:37:50,546] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2792640e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.5562803e-37
 1.0000000e+00 8.1066126e-26], sum to 1.0000
[2019-04-01 19:37:50,547] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9713
[2019-04-01 19:37:50,604] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 64.0, 90.0, 172.5, 26.0, 25.77177397717238, 8.703967259231552, 1.0, 1.0, 51.87761510551923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2794800.0000, 
sim time next is 2795400.0000, 
raw observation next is [-6.0, 64.0, 108.0, 207.0, 26.0, 25.90442188436997, 8.909618549048782, 1.0, 1.0, 43.7570254004516], 
processed observation next is [1.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.36, 0.2287292817679558, 1.0, 0.9863459834814243, 0.08909618549048781, 1.0, 1.0, 0.3125501814317972], 
reward next is 0.6874, 
noisyNet noise sample is [array([-0.18419816], dtype=float32), -1.5575507]. 
=============================================
[2019-04-01 19:37:52,522] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.1715198e-34 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:37:52,523] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4691
[2019-04-01 19:37:52,545] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 30.0, 0.0, 0.0, 26.0, 25.48444280395189, 8.272414122812862, 1.0, 1.0, 8.971766531382064], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2829600.0000, 
sim time next is 2830200.0000, 
raw observation next is [4.666666666666667, 31.16666666666666, 0.0, 0.0, 26.0, 25.48855394003121, 7.165092162418159, 1.0, 1.0, 13.88665370210873], 
processed observation next is [1.0, 0.782608695652174, 0.5918744228993538, 0.3116666666666666, 0.0, 0.0, 1.0, 0.9269362771473156, 0.07165092162418159, 1.0, 1.0, 0.09919038358649092], 
reward next is 0.9008, 
noisyNet noise sample is [array([0.6920385], dtype=float32), 0.48638374]. 
=============================================
[2019-04-01 19:37:55,085] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:37:55,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4735
[2019-04-01 19:37:55,113] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 98.83333333333334, 0.0, 0.0, 26.0, 25.04626990268722, 6.852022004489936, 0.0, 1.0, 54.1160322467122], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2872200.0000, 
sim time next is 2872800.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 26.0, 25.03117066934572, 6.733986807892521, 0.0, 1.0, 54.35349432630875], 
processed observation next is [1.0, 0.2608695652173913, 0.4903047091412743, 1.0, 0.0, 0.0, 1.0, 0.8615958099065314, 0.06733986807892521, 0.0, 1.0, 0.38823924518791963], 
reward next is 0.6118, 
noisyNet noise sample is [array([0.6041879], dtype=float32), 1.3782667]. 
=============================================
[2019-04-01 19:38:04,473] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:38:04,474] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8367
[2019-04-01 19:38:04,492] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 108.5, 793.5, 26.0, 26.30864057739078, 14.69706690102475, 1.0, 1.0, 5.407558995838778], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3506400.0000, 
sim time next is 3507000.0000, 
raw observation next is [3.0, 49.0, 106.3333333333333, 789.3333333333334, 26.0, 26.47551392390223, 15.30965187383399, 1.0, 1.0, 5.742375142808868], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.35444444444444434, 0.8721915285451197, 1.0, 1.0679305605574616, 0.1530965187383399, 1.0, 1.0, 0.04101696530577763], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4912881], dtype=float32), 2.0066879]. 
=============================================
[2019-04-01 19:38:04,495] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.28468 ]
 [72.13779 ]
 [71.99695 ]
 [72.0102  ]
 [71.984856]], R is [[71.65840912]
 [70.94182587]
 [70.23240662]
 [69.5300827 ]
 [68.83478546]].
[2019-04-01 19:38:07,221] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:38:07,221] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5881
[2019-04-01 19:38:07,235] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.53551062020363, 6.524326510193968, 0.0, 1.0, 42.09911707975898], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2950200.0000, 
sim time next is 2950800.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.50489460504119, 6.436249507635199, 0.0, 1.0, 42.14852717349649], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 1.0, 0.7864135150058844, 0.06436249507635199, 0.0, 1.0, 0.3010609083821178], 
reward next is 0.6989, 
noisyNet noise sample is [array([-0.05291695], dtype=float32), 2.508427]. 
=============================================
[2019-04-01 19:38:09,989] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:38:09,991] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2985
[2019-04-01 19:38:10,014] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.666666666666666, 100.0, 0.0, 0.0, 26.0, 25.36801314036636, 7.36118249599392, 0.0, 1.0, 49.53596027221396], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3135000.0000, 
sim time next is 3135600.0000, 
raw observation next is [6.0, 100.0, 0.0, 0.0, 26.0, 25.38397363021936, 7.548899903016253, 0.0, 1.0, 48.52940833707216], 
processed observation next is [1.0, 0.30434782608695654, 0.6288088642659281, 1.0, 0.0, 0.0, 1.0, 0.9119962328884798, 0.07548899903016254, 0.0, 1.0, 0.34663863097908687], 
reward next is 0.6534, 
noisyNet noise sample is [array([-0.87364995], dtype=float32), -0.7838458]. 
=============================================
[2019-04-01 19:38:12,178] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:38:12,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3442
[2019-04-01 19:38:12,189] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 95.33333333333334, 735.0, 26.0, 26.62931790766851, 19.33589426338844, 1.0, 1.0, 1.642033553737612], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3163800.0000, 
sim time next is 3164400.0000, 
raw observation next is [7.0, 100.0, 92.5, 721.0, 26.0, 26.85792697092146, 20.44505111063243, 1.0, 1.0, 1.513221929922908], 
processed observation next is [1.0, 0.6521739130434783, 0.6565096952908588, 1.0, 0.30833333333333335, 0.7966850828729282, 1.0, 1.122560995845923, 0.2044505111063243, 1.0, 1.0, 0.010808728070877913], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.20083837], dtype=float32), -1.7645473]. 
=============================================
[2019-04-01 19:38:14,530] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:38:14,530] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2668
[2019-04-01 19:38:14,543] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 68.33333333333333, 14.66666666666666, 118.1666666666667, 26.0, 23.60984742684122, 5.26061328304105, 0.0, 1.0, 40.28594341150631], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3051600.0000, 
sim time next is 3052200.0000, 
raw observation next is [-6.0, 66.16666666666667, 28.33333333333333, 166.3333333333333, 26.0, 23.58347378032173, 5.264017183314489, 0.0, 1.0, 40.19997233478577], 
processed observation next is [0.0, 0.30434782608695654, 0.296398891966759, 0.6616666666666667, 0.09444444444444443, 0.18379373848987104, 1.0, 0.6547819686173898, 0.052640171833144886, 0.0, 1.0, 0.28714265953418405], 
reward next is 0.7129, 
noisyNet noise sample is [array([-0.10140332], dtype=float32), -0.61812055]. 
=============================================
[2019-04-01 19:38:23,448] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.0285828e-17], sum to 1.0000
[2019-04-01 19:38:23,448] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3583
[2019-04-01 19:38:23,466] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.33230647512211, 10.04877437928399, 0.0, 1.0, 44.36682516286985], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3277200.0000, 
sim time next is 3277800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.27214184192687, 9.955259383098527, 0.0, 1.0, 43.5896201759425], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 1.0, 0.8960202631324101, 0.09955259383098528, 0.0, 1.0, 0.3113544298281607], 
reward next is 0.6886, 
noisyNet noise sample is [array([-0.8598818], dtype=float32), 0.6715264]. 
=============================================
[2019-04-01 19:38:34,275] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00
 1.2150591e-17 0.0000000e+00], sum to 1.0000
[2019-04-01 19:38:34,276] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5117
[2019-04-01 19:38:34,294] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3333333333333333, 40.0, 22.16666666666666, 204.1666666666667, 26.0, 25.1306303868552, 8.6497176967639, 0.0, 1.0, 21.40006414032673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3604800.0000, 
sim time next is 3605400.0000, 
raw observation next is [-0.5, 40.5, 14.0, 142.0, 26.0, 25.09566611314838, 8.507088394639027, 0.0, 1.0, 21.18489257898517], 
processed observation next is [0.0, 0.7391304347826086, 0.44875346260387816, 0.405, 0.04666666666666667, 0.1569060773480663, 1.0, 0.8708094447354827, 0.08507088394639027, 0.0, 1.0, 0.1513206612784655], 
reward next is 0.8487, 
noisyNet noise sample is [array([-0.6729701], dtype=float32), -0.20580266]. 
=============================================
[2019-04-01 19:38:41,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:38:41,266] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0350
[2019-04-01 19:38:41,278] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.33333333333333, 26.66666666666667, 109.3333333333333, 746.3333333333334, 26.0, 25.72697191840246, 10.33507188824537, 0.0, 1.0, 12.00323549060587], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3666000.0000, 
sim time next is 3666600.0000, 
raw observation next is [11.5, 26.0, 111.0, 763.0, 26.0, 25.7203453616358, 10.47595079536407, 0.0, 1.0, 11.29952901280176], 
processed observation next is [0.0, 0.43478260869565216, 0.7811634349030472, 0.26, 0.37, 0.8430939226519337, 1.0, 0.960049337376543, 0.10475950795364071, 0.0, 1.0, 0.08071092152001257], 
reward next is 0.9193, 
noisyNet noise sample is [array([-0.8939769], dtype=float32), -0.69881725]. 
=============================================
[2019-04-01 19:38:42,431] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.8117891e-37 2.1234958e-29 1.0000000e+00
 5.8483051e-14 0.0000000e+00], sum to 1.0000
[2019-04-01 19:38:42,434] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9336
[2019-04-01 19:38:42,452] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.3436441787995, 8.718582076691064, 0.0, 1.0, 43.44156297655977], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3807600.0000, 
sim time next is 3808200.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.40140943528539, 8.848736091361074, 0.0, 1.0, 43.3152996812614], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 1.0, 0.9144870621836273, 0.08848736091361074, 0.0, 1.0, 0.3093949977232957], 
reward next is 0.6906, 
noisyNet noise sample is [array([0.75292575], dtype=float32), -0.0041170535]. 
=============================================
[2019-04-01 19:38:43,764] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7236484e-26
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:38:43,768] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4656
[2019-04-01 19:38:43,789] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.166666666666667, 62.33333333333333, 0.0, 0.0, 26.0, 25.39529306491865, 9.685407405141042, 0.0, 1.0, 42.12834366024849], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3700200.0000, 
sim time next is 3700800.0000, 
raw observation next is [3.0, 63.0, 0.0, 0.0, 26.0, 25.59789208259067, 10.03576572438074, 0.0, 1.0, 40.53577711876198], 
processed observation next is [0.0, 0.8695652173913043, 0.5457063711911359, 0.63, 0.0, 0.0, 1.0, 0.9425560117986669, 0.1003576572438074, 0.0, 1.0, 0.2895412651340141], 
reward next is 0.7105, 
noisyNet noise sample is [array([1.1248949], dtype=float32), 0.41257775]. 
=============================================
[2019-04-01 19:38:44,346] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:38:44,348] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0662
[2019-04-01 19:38:44,370] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.333333333333333, 60.0, 111.1666666666667, 782.8333333333334, 26.0, 26.43467362843376, 12.93865792017156, 1.0, 1.0, 10.86831478022498], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3840000.0000, 
sim time next is 3840600.0000, 
raw observation next is [-1.166666666666667, 60.0, 112.3333333333333, 790.6666666666667, 26.0, 26.44861118708272, 13.07637222071632, 1.0, 1.0, 9.996117078187035], 
processed observation next is [1.0, 0.43478260869565216, 0.43028624192059095, 0.6, 0.37444444444444436, 0.8736648250460406, 1.0, 1.0640873124403885, 0.1307637222071632, 1.0, 1.0, 0.07140083627276454], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6171982], dtype=float32), -1.0850384]. 
=============================================
[2019-04-01 19:38:46,948] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1529888e-03 5.9054355e-38
 9.9884701e-01 0.0000000e+00], sum to 1.0000
[2019-04-01 19:38:46,949] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3223
[2019-04-01 19:38:46,972] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 55.5, 0.0, 0.0, 26.0, 24.96848421190137, 7.736795538112641, 0.0, 1.0, 43.54097248640897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3972600.0000, 
sim time next is 3973200.0000, 
raw observation next is [-9.666666666666666, 56.33333333333333, 0.0, 0.0, 26.0, 24.92139824581041, 7.575607578722952, 0.0, 1.0, 43.5514944289033], 
processed observation next is [1.0, 1.0, 0.19482917820867962, 0.5633333333333332, 0.0, 0.0, 1.0, 0.8459140351157727, 0.07575607578722952, 0.0, 1.0, 0.31108210306359496], 
reward next is 0.6889, 
noisyNet noise sample is [array([1.5694226], dtype=float32), 1.1849643]. 
=============================================
[2019-04-01 19:38:47,866] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6665064e-04 9.5352134e-22 8.4653828e-13 2.6704852e-10 1.2300601e-16
 1.0172763e-02 9.8966050e-01], sum to 1.0000
[2019-04-01 19:38:47,867] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9008
[2019-04-01 19:38:47,900] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.04494794955159, 7.432610156080756, 0.0, 1.0, 43.03189564048459], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3814200.0000, 
sim time next is 3814800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.0177149423365, 7.305051420600338, 0.0, 1.0, 43.08625441862442], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 1.0, 0.8596735631909286, 0.07305051420600338, 0.0, 1.0, 0.30775896013303156], 
reward next is 0.6922, 
noisyNet noise sample is [array([1.0473593], dtype=float32), -0.26312342]. 
=============================================
[2019-04-01 19:38:49,383] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.6773694e-29 2.7647991e-37 5.0571588e-28 6.1884380e-20 1.4113149e-20
 1.0000000e+00 1.6137036e-13], sum to 1.0000
[2019-04-01 19:38:49,384] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6448
[2019-04-01 19:38:49,410] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.04350930388539, 7.45029712742615, 0.0, 1.0, 43.33259463048985], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3813000.0000, 
sim time next is 3813600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.99909920891965, 7.516654973950661, 0.0, 1.0, 43.11302095051197], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 1.0, 0.857014172702807, 0.07516654973950661, 0.0, 1.0, 0.30795014964651407], 
reward next is 0.6920, 
noisyNet noise sample is [array([0.14360458], dtype=float32), 0.37171078]. 
=============================================
[2019-04-01 19:38:54,990] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:38:54,994] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2882
[2019-04-01 19:38:55,012] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666666, 64.66666666666667, 139.3333333333333, 2.999999999999999, 26.0, 26.15729691446712, 10.77055446021513, 1.0, 1.0, 13.9278599576468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4527600.0000, 
sim time next is 4528200.0000, 
raw observation next is [0.8333333333333334, 62.83333333333333, 155.6666666666667, 5.999999999999998, 26.0, 26.16374610735632, 10.90336402394488, 1.0, 1.0, 13.16855673631013], 
processed observation next is [1.0, 0.391304347826087, 0.4856879039704525, 0.6283333333333333, 0.5188888888888891, 0.006629834254143645, 1.0, 1.023392301050903, 0.1090336402394488, 1.0, 1.0, 0.09406111954507236], 
reward next is 0.5446, 
noisyNet noise sample is [array([0.30907866], dtype=float32), -0.837107]. 
=============================================
[2019-04-01 19:38:58,533] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:38:58,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9223
[2019-04-01 19:38:58,555] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 39.0, 0.0, 0.0, 26.0, 25.09732373826973, 6.854560398647247, 0.0, 1.0, 40.09837419445174], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4072800.0000, 
sim time next is 4073400.0000, 
raw observation next is [-5.0, 39.5, 0.0, 0.0, 26.0, 25.031973239866, 6.729351794745607, 0.0, 1.0, 40.09626480272183], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.395, 0.0, 0.0, 1.0, 0.8617104628380002, 0.06729351794745607, 0.0, 1.0, 0.28640189144801304], 
reward next is 0.7136, 
noisyNet noise sample is [array([0.5140454], dtype=float32), 0.19355674]. 
=============================================
[2019-04-01 19:38:59,797] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:38:59,798] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6721
[2019-04-01 19:38:59,938] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-14.0, 69.0, 0.0, 0.0, 26.0, 23.3831249846294, 5.380611805553204, 0.0, 1.0, 42.34621329162958], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3999600.0000, 
sim time next is 4000200.0000, 
raw observation next is [-13.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.35875908222768, 5.406729730099435, 0.0, 1.0, 109.5353036913931], 
processed observation next is [1.0, 0.30434782608695654, 0.07940904893813489, 0.68, 0.0, 0.0, 1.0, 0.6226798688896688, 0.05406729730099435, 0.0, 1.0, 0.7823950263670936], 
reward next is 0.2176, 
noisyNet noise sample is [array([1.930864], dtype=float32), 1.3416011]. 
=============================================
[2019-04-01 19:39:00,331] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 3.3734873e-36 1.6106814e-31 0.0000000e+00 3.0450290e-37
 1.0000000e+00 8.9427388e-37], sum to 1.0000
[2019-04-01 19:39:00,331] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8150
[2019-04-01 19:39:00,369] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 36.0, 88.0, 724.0, 26.0, 27.15051331079365, 19.21029876668379, 1.0, 1.0, 10.39364492339084], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3943800.0000, 
sim time next is 3944400.0000, 
raw observation next is [-4.0, 35.33333333333333, 84.33333333333334, 692.6666666666667, 26.0, 27.12856580056919, 16.14831857161818, 1.0, 1.0, 11.1738077186006], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.3533333333333333, 0.28111111111111114, 0.765377532228361, 1.0, 1.1612236857955989, 0.1614831857161818, 1.0, 1.0, 0.07981291227571857], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.60803163], dtype=float32), 0.8731478]. 
=============================================
[2019-04-01 19:39:00,570] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5271195e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.6461366e-24
 1.0000000e+00 1.5195130e-12], sum to 1.0000
[2019-04-01 19:39:00,571] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4174
[2019-04-01 19:39:00,591] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4333333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.37716228139444, 10.13382772140123, 0.0, 1.0, 43.05533606278964], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4491600.0000, 
sim time next is 4492200.0000, 
raw observation next is [-0.4666666666666667, 72.83333333333333, 0.0, 0.0, 26.0, 25.39350189137114, 10.26865126414642, 0.0, 1.0, 42.69796404719815], 
processed observation next is [1.0, 1.0, 0.44967682363804257, 0.7283333333333333, 0.0, 0.0, 1.0, 0.9133574130530201, 0.10268651264146421, 0.0, 1.0, 0.3049854574799868], 
reward next is 0.6950, 
noisyNet noise sample is [array([-0.63409287], dtype=float32), -0.15599349]. 
=============================================
[2019-04-01 19:39:00,738] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:39:00,740] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8072
[2019-04-01 19:39:00,750] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 54.5, 0.0, 0.0, 26.0, 26.0313119423761, 14.05105187333526, 0.0, 1.0, 20.83835804821749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4656600.0000, 
sim time next is 4657200.0000, 
raw observation next is [2.0, 55.33333333333334, 0.0, 0.0, 26.0, 26.01558188938106, 13.81073538386948, 0.0, 1.0, 19.69921496362362], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.5533333333333335, 0.0, 0.0, 1.0, 1.0022259841972943, 0.1381073538386948, 0.0, 1.0, 0.1407086783115973], 
reward next is 0.8593, 
noisyNet noise sample is [array([0.15119627], dtype=float32), -0.17539763]. 
=============================================
[2019-04-01 19:39:03,601] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 7.080080e-33 9.239855e-37 5.247221e-32 0.000000e+00
 1.000000e+00 7.851747e-18], sum to 1.0000
[2019-04-01 19:39:03,602] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6760
[2019-04-01 19:39:03,638] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.166666666666667, 37.33333333333334, 96.0, 539.0, 26.0, 26.22157822015604, 10.04380772967382, 1.0, 1.0, 24.4541639095823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4092600.0000, 
sim time next is 4093200.0000, 
raw observation next is [-3.0, 38.0, 98.0, 574.0, 26.0, 26.16467460586715, 10.20462431893864, 1.0, 1.0, 21.29867279540665], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.38, 0.32666666666666666, 0.6342541436464089, 1.0, 1.0235249436953073, 0.1020462431893864, 1.0, 1.0, 0.15213337711004749], 
reward next is 0.7660, 
noisyNet noise sample is [array([-0.4619118], dtype=float32), 0.89829046]. 
=============================================
[2019-04-01 19:39:08,520] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 5.104310e-33 9.817703e-04
 9.990182e-01 0.000000e+00], sum to 1.0000
[2019-04-01 19:39:08,522] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1679
[2019-04-01 19:39:08,540] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 53.33333333333333, 0.0, 0.0, 26.0, 24.98505715562605, 7.009941462390192, 0.0, 1.0, 38.99552061886268], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4165800.0000, 
sim time next is 4166400.0000, 
raw observation next is [-4.0, 52.66666666666667, 0.0, 0.0, 26.0, 24.96984131724589, 6.924961799177527, 0.0, 1.0, 38.99815536121061], 
processed observation next is [0.0, 0.21739130434782608, 0.3518005540166205, 0.5266666666666667, 0.0, 0.0, 1.0, 0.8528344738922701, 0.06924961799177527, 0.0, 1.0, 0.27855825258007577], 
reward next is 0.7214, 
noisyNet noise sample is [array([1.6348455], dtype=float32), -0.60744673]. 
=============================================
[2019-04-01 19:39:10,286] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 9.842514e-18
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 19:39:10,287] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0157
[2019-04-01 19:39:10,302] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 65.5, 0.0, 0.0, 26.0, 24.77271070931195, 6.30714831503519, 0.0, 1.0, 38.79520024080381], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4854600.0000, 
sim time next is 4855200.0000, 
raw observation next is [-3.666666666666667, 67.33333333333333, 0.0, 0.0, 26.0, 24.79756246048352, 6.261490073256899, 0.0, 1.0, 38.81923995964129], 
processed observation next is [0.0, 0.17391304347826086, 0.3610341643582641, 0.6733333333333333, 0.0, 0.0, 1.0, 0.8282232086405027, 0.06261490073256899, 0.0, 1.0, 0.2772802854260092], 
reward next is 0.7227, 
noisyNet noise sample is [array([0.45335913], dtype=float32), -0.17355189]. 
=============================================
[2019-04-01 19:39:12,065] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:39:12,066] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3998
[2019-04-01 19:39:12,105] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4, 52.0, 291.5, 236.0, 26.0, 25.10257752357125, 7.754271421962282, 0.0, 1.0, 15.92481633342155], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4878000.0000, 
sim time next is 4878600.0000, 
raw observation next is [-0.1666666666666667, 51.16666666666667, 288.6666666666666, 260.0, 26.0, 25.05582433385544, 7.679990431416584, 0.0, 1.0, 17.22435697855527], 
processed observation next is [0.0, 0.4782608695652174, 0.4579870729455217, 0.5116666666666667, 0.9622222222222219, 0.287292817679558, 1.0, 0.8651177619793486, 0.07679990431416585, 0.0, 1.0, 0.1230311212753948], 
reward next is 0.8770, 
noisyNet noise sample is [array([-0.66124094], dtype=float32), 1.2108662]. 
=============================================
[2019-04-01 19:39:17,812] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4917079e-36 1.2351445e-25
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:39:17,812] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0371
[2019-04-01 19:39:17,822] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 54.5, 188.0, 717.0, 26.0, 25.30513645549389, 9.247104638744268, 0.0, 1.0, 4.346935780260324], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4282200.0000, 
sim time next is 4282800.0000, 
raw observation next is [7.0, 55.33333333333334, 194.8333333333333, 661.6666666666666, 26.0, 25.31060498021708, 9.262767829262131, 0.0, 1.0, 3.587980320010681], 
processed observation next is [0.0, 0.5652173913043478, 0.6565096952908588, 0.5533333333333335, 0.6494444444444443, 0.731123388581952, 1.0, 0.9015149971738684, 0.09262767829262131, 0.0, 1.0, 0.02562843085721915], 
reward next is 0.9744, 
noisyNet noise sample is [array([-0.6413267], dtype=float32), -1.8442377]. 
=============================================
[2019-04-01 19:39:22,030] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.1798033e-38 2.8537759e-34 6.9497589e-12 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:39:22,033] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0062
[2019-04-01 19:39:22,041] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 78.0, 60.0, 0.0, 26.0, 26.06668567107653, 12.36691897173893, 1.0, 1.0, 12.55959697150706], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4464000.0000, 
sim time next is 4464600.0000, 
raw observation next is [0.0, 78.0, 56.33333333333333, 0.0, 26.0, 26.13079019839817, 12.3124614255459, 1.0, 1.0, 12.0990255155706], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.18777777777777777, 0.0, 1.0, 1.0186843140568815, 0.123124614255459, 1.0, 1.0, 0.08642161082550429], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1988164], dtype=float32), 0.027904822]. 
=============================================
[2019-04-01 19:39:23,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:23,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:23,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run31
[2019-04-01 19:39:25,309] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3173520e-21 5.7051241e-20 2.7027591e-17 8.7561177e-14 1.8249477e-22
 1.0000000e+00 5.5220282e-09], sum to 1.0000
[2019-04-01 19:39:25,310] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4030
[2019-04-01 19:39:25,345] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.1333333333333333, 72.16666666666666, 115.0, 44.00000000000001, 26.0, 25.95217048529828, 10.52663168913106, 1.0, 1.0, 19.65140970095678], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4524600.0000, 
sim time next is 4525200.0000, 
raw observation next is [0.0, 72.0, 117.0, 33.0, 26.0, 26.05989549393809, 10.77866270542664, 1.0, 1.0, 17.16800297556912], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.72, 0.39, 0.036464088397790057, 1.0, 1.0085564991340128, 0.1077866270542664, 1.0, 1.0, 0.12262859268263657], 
reward next is 0.5659, 
noisyNet noise sample is [array([1.1579366], dtype=float32), 0.5909819]. 
=============================================
[2019-04-01 19:39:30,631] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:30,631] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:30,690] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run31
[2019-04-01 19:39:31,451] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:39:31,452] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9134
[2019-04-01 19:39:31,464] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.26840387447853, 8.978291655177719, 0.0, 1.0, 34.32477227296263], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4574400.0000, 
sim time next is 4575000.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.19092771030119, 9.177139757420933, 0.0, 1.0, 40.9640817102917], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.61, 0.0, 0.0, 1.0, 0.8844182443287413, 0.09177139757420934, 0.0, 1.0, 0.29260058364494074], 
reward next is 0.7074, 
noisyNet noise sample is [array([1.1028332], dtype=float32), -1.5711919]. 
=============================================
[2019-04-01 19:39:31,476] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[61.959248]
 [62.54624 ]
 [62.892002]
 [63.290115]
 [63.77426 ]], R is [[61.58868408]
 [61.72761917]
 [61.925457  ]
 [62.12517166]
 [62.3466835 ]].
[2019-04-01 19:39:34,452] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:39:34,453] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8701
[2019-04-01 19:39:34,473] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 49.33333333333334, 0.0, 0.0, 26.0, 25.0306415092768, 6.260778791110764, 0.0, 1.0, 38.25016279043161], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4945800.0000, 
sim time next is 4946400.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.00599627056623, 6.283994687707239, 0.0, 1.0, 38.23285887837736], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 1.0, 0.857999467223747, 0.06283994687707238, 0.0, 1.0, 0.27309184913126683], 
reward next is 0.7269, 
noisyNet noise sample is [array([0.07199556], dtype=float32), 0.2461254]. 
=============================================
[2019-04-01 19:39:35,373] A3C_AGENT_WORKER-Thread-17 INFO:Local step 255000, global step 4069982: loss 0.0313
[2019-04-01 19:39:35,373] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 255000, global step 4069982: learning rate 0.0010
[2019-04-01 19:39:37,453] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:39:37,456] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6843
[2019-04-01 19:39:37,466] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.20303897585003, 12.43531166087021, 0.0, 1.0, 39.85432567037474], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 21000.0000, 
sim time next is 21600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.21641344705857, 12.31042663848802, 0.0, 1.0, 39.8279592322998], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 1.0, 0.3166304924369387, 0.1231042663848802, 0.0, 1.0, 0.2844854230878557], 
reward next is 0.7155, 
noisyNet noise sample is [array([0.69521767], dtype=float32), -0.94301224]. 
=============================================
[2019-04-01 19:39:41,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:39:41,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1294
[2019-04-01 19:39:41,834] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.166666666666667, 79.5, 140.6666666666667, 419.6666666666667, 26.0, 25.8086854622552, 10.38662501292944, 0.0, 1.0, 43.47371353871807], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4783800.0000, 
sim time next is 4784400.0000, 
raw observation next is [-5.0, 77.0, 149.0, 420.0, 26.0, 25.81944697882629, 10.33274872028712, 0.0, 1.0, 40.68928324015932], 
processed observation next is [0.0, 0.391304347826087, 0.32409972299168976, 0.77, 0.49666666666666665, 0.46408839779005523, 1.0, 0.9742067112608984, 0.10332748720287119, 0.0, 1.0, 0.29063773742970944], 
reward next is 0.7094, 
noisyNet noise sample is [array([2.054436], dtype=float32), -1.459171]. 
=============================================
[2019-04-01 19:39:41,854] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00000000e+00 0.00000000e+00 1.11356225e-36 9.99992728e-01
 2.02571584e-27 7.27917131e-06 2.26968649e-37], sum to 1.0000
[2019-04-01 19:39:41,856] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7021
[2019-04-01 19:39:41,872] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 36.5, 87.0, 608.3333333333334, 26.0, 25.21979672213538, 9.59484424687465, 0.0, 1.0, 3.878892902945058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4810200.0000, 
sim time next is 4810800.0000, 
raw observation next is [3.0, 36.0, 84.5, 578.6666666666666, 26.0, 25.21965429920726, 9.595672734693357, 0.0, 1.0, 7.360617223286725], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.36, 0.2816666666666667, 0.6394106813996316, 1.0, 0.8885220427438943, 0.09595672734693357, 0.0, 1.0, 0.05257583730919089], 
reward next is 0.9474, 
noisyNet noise sample is [array([-0.6210764], dtype=float32), 0.06523328]. 
=============================================
[2019-04-01 19:39:42,052] A3C_AGENT_WORKER-Thread-18 INFO:Local step 255000, global step 4073304: loss 0.0670
[2019-04-01 19:39:42,059] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 255000, global step 4073304: learning rate 0.0010
[2019-04-01 19:39:42,238] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:42,239] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:42,251] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run31
[2019-04-01 19:39:43,055] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.7055545e-32 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:39:43,056] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1450
[2019-04-01 19:39:43,069] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 60.0, 0.0, 0.0, 26.0, 25.05015826589273, 6.924042240989642, 0.0, 1.0, 38.7526221972849], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4848000.0000, 
sim time next is 4848600.0000, 
raw observation next is [-2.833333333333333, 60.0, 0.0, 0.0, 26.0, 25.0229274166303, 6.84703167945402, 0.0, 1.0, 38.76422700520604], 
processed observation next is [0.0, 0.08695652173913043, 0.3841181902123731, 0.6, 0.0, 0.0, 1.0, 0.8604182023757571, 0.0684703167945402, 0.0, 1.0, 0.2768873357514717], 
reward next is 0.7231, 
noisyNet noise sample is [array([-1.3929629], dtype=float32), 0.41525954]. 
=============================================
[2019-04-01 19:39:47,110] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:47,110] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:47,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run31
[2019-04-01 19:39:48,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:48,574] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:48,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run31
[2019-04-01 19:39:51,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:51,850] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:51,864] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run31
[2019-04-01 19:39:53,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:53,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:53,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run31
[2019-04-01 19:39:53,444] A3C_AGENT_WORKER-Thread-4 INFO:Local step 255000, global step 4078650: loss 0.1108
[2019-04-01 19:39:53,444] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 255000, global step 4078650: learning rate 0.0010
[2019-04-01 19:39:53,655] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:53,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:53,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run31
[2019-04-01 19:39:54,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:54,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:54,044] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run31
[2019-04-01 19:39:55,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:55,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:55,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run31
[2019-04-01 19:39:55,098] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00
 6.3421193e-09 4.1148532e-32], sum to 1.0000
[2019-04-01 19:39:55,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2252
[2019-04-01 19:39:55,111] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.0, 17.66666666666666, 69.33333333333333, 536.1666666666666, 26.0, 28.93935966594068, 34.5810455924187, 1.0, 1.0, 0.6329245544209606], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5071200.0000, 
sim time next is 5071800.0000, 
raw observation next is [12.0, 17.33333333333334, 62.66666666666667, 487.3333333333334, 26.0, 29.0384127643015, 35.26749513238244, 1.0, 1.0, 0.6237501127923023], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.1733333333333334, 0.2088888888888889, 0.5384898710865563, 1.0, 1.434058966328786, 0.3526749513238244, 1.0, 1.0, 0.004455357948516445], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9131131], dtype=float32), 1.2268648]. 
=============================================
[2019-04-01 19:39:55,498] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:55,499] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:55,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run31
[2019-04-01 19:39:55,550] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:55,550] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:55,554] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run31
[2019-04-01 19:39:55,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:55,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:55,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run31
[2019-04-01 19:39:55,882] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:55,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:55,899] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run31
[2019-04-01 19:39:56,802] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:56,802] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:56,808] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run31
[2019-04-01 19:39:56,842] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:39:56,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:39:56,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run31
[2019-04-01 19:39:56,961] A3C_AGENT_WORKER-Thread-17 INFO:Local step 255500, global step 4079537: loss 78.2726
[2019-04-01 19:39:56,962] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 255500, global step 4079537: learning rate 0.0010
[2019-04-01 19:39:59,012] A3C_AGENT_WORKER-Thread-2 INFO:Local step 255000, global step 4079691: loss -0.2617
[2019-04-01 19:39:59,012] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 255000, global step 4079691: learning rate 0.0010
[2019-04-01 19:40:01,379] A3C_AGENT_WORKER-Thread-20 INFO:Local step 255000, global step 4080010: loss 0.1465
[2019-04-01 19:40:01,386] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 255000, global step 4080010: learning rate 0.0010
[2019-04-01 19:40:04,764] A3C_AGENT_WORKER-Thread-13 INFO:Local step 255000, global step 4080891: loss 0.0219
[2019-04-01 19:40:04,765] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 255000, global step 4080891: learning rate 0.0010
[2019-04-01 19:40:05,113] A3C_AGENT_WORKER-Thread-18 INFO:Local step 255500, global step 4081012: loss 71.9297
[2019-04-01 19:40:05,114] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 255500, global step 4081012: learning rate 0.0010
[2019-04-01 19:40:05,927] A3C_AGENT_WORKER-Thread-15 INFO:Local step 255000, global step 4081248: loss 0.0084
[2019-04-01 19:40:05,928] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 255000, global step 4081248: learning rate 0.0010
[2019-04-01 19:40:06,927] A3C_AGENT_WORKER-Thread-9 INFO:Local step 255000, global step 4081520: loss 0.0254
[2019-04-01 19:40:06,927] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 255000, global step 4081520: learning rate 0.0010
[2019-04-01 19:40:07,324] A3C_AGENT_WORKER-Thread-16 INFO:Local step 255000, global step 4081647: loss 0.0243
[2019-04-01 19:40:07,325] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 255000, global step 4081647: learning rate 0.0010
[2019-04-01 19:40:08,120] A3C_AGENT_WORKER-Thread-5 INFO:Local step 255000, global step 4081904: loss 0.0520
[2019-04-01 19:40:08,120] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 255000, global step 4081904: learning rate 0.0010
[2019-04-01 19:40:08,536] A3C_AGENT_WORKER-Thread-10 INFO:Local step 255000, global step 4082037: loss 0.0029
[2019-04-01 19:40:08,536] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 255000, global step 4082037: learning rate 0.0010
[2019-04-01 19:40:08,692] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7956312e-32 0.0000000e+00 6.2926605e-34 8.5921251e-27 2.2179837e-26
 1.0000000e+00 4.0810643e-28], sum to 1.0000
[2019-04-01 19:40:08,692] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5537
[2019-04-01 19:40:08,727] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 84.33333333333334, 0.0, 0.0, 26.0, 24.09076125637732, 5.438546767044222, 0.0, 1.0, 42.68680840714487], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 98400.0000, 
sim time next is 99000.0000, 
raw observation next is [-3.1, 83.0, 0.0, 0.0, 26.0, 24.07109118882764, 5.402989112409135, 0.0, 1.0, 42.81285681473335], 
processed observation next is [1.0, 0.13043478260869565, 0.37673130193905824, 0.83, 0.0, 0.0, 1.0, 0.7244415984039486, 0.05402989112409135, 0.0, 1.0, 0.3058061201052382], 
reward next is 0.6942, 
noisyNet noise sample is [array([0.36686316], dtype=float32), -1.0333754]. 
=============================================
[2019-04-01 19:40:08,758] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[59.257736]
 [59.329796]
 [59.40929 ]
 [59.473774]
 [59.54019 ]], R is [[59.28380966]
 [59.38606644]
 [59.48789978]
 [59.58985519]
 [59.69168854]].
[2019-04-01 19:40:08,817] A3C_AGENT_WORKER-Thread-12 INFO:Local step 255000, global step 4082125: loss 0.0075
[2019-04-01 19:40:08,818] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 255000, global step 4082125: learning rate 0.0010
[2019-04-01 19:40:09,566] A3C_AGENT_WORKER-Thread-3 INFO:Local step 255000, global step 4082393: loss 0.0092
[2019-04-01 19:40:09,566] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 255000, global step 4082393: learning rate 0.0010
[2019-04-01 19:40:09,585] A3C_AGENT_WORKER-Thread-11 INFO:Local step 255000, global step 4082404: loss 0.0133
[2019-04-01 19:40:09,586] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 255000, global step 4082404: learning rate 0.0010
[2019-04-01 19:40:10,226] A3C_AGENT_WORKER-Thread-14 INFO:Local step 255000, global step 4082663: loss 0.0230
[2019-04-01 19:40:10,227] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 255000, global step 4082663: learning rate 0.0010
[2019-04-01 19:40:10,241] A3C_AGENT_WORKER-Thread-19 INFO:Local step 255000, global step 4082666: loss 0.0038
[2019-04-01 19:40:10,242] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 255000, global step 4082666: learning rate 0.0010
[2019-04-01 19:40:18,027] A3C_AGENT_WORKER-Thread-4 INFO:Local step 255500, global step 4085185: loss 70.7543
[2019-04-01 19:40:18,028] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 255500, global step 4085185: learning rate 0.0010
[2019-04-01 19:40:23,269] A3C_AGENT_WORKER-Thread-2 INFO:Local step 255500, global step 4086743: loss 45.3954
[2019-04-01 19:40:23,277] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 255500, global step 4086743: learning rate 0.0010
[2019-04-01 19:40:23,352] A3C_AGENT_WORKER-Thread-17 INFO:Local step 256000, global step 4086766: loss 0.0521
[2019-04-01 19:40:23,352] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 256000, global step 4086766: learning rate 0.0010
[2019-04-01 19:40:25,285] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:40:25,288] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2904
[2019-04-01 19:40:25,347] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 54.5, 106.0, 658.0, 26.0, 25.77512798278667, 8.426597055245288, 1.0, 1.0, 22.43765510606187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 300600.0000, 
sim time next is 301200.0000, 
raw observation next is [-10.6, 52.66666666666667, 102.1666666666667, 674.6666666666666, 26.0, 25.70982512249739, 8.24430361596633, 1.0, 1.0, 21.36865981274751], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.5266666666666667, 0.34055555555555567, 0.74548802946593, 1.0, 0.9585464460710555, 0.08244303615966331, 1.0, 1.0, 0.15263328437676793], 
reward next is 0.8474, 
noisyNet noise sample is [array([0.95642716], dtype=float32), 1.5834783]. 
=============================================
[2019-04-01 19:40:26,199] A3C_AGENT_WORKER-Thread-20 INFO:Local step 255500, global step 4087819: loss 42.0839
[2019-04-01 19:40:26,200] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 255500, global step 4087819: learning rate 0.0010
[2019-04-01 19:40:26,227] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:40:26,227] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7081
[2019-04-01 19:40:26,242] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.2, 67.5, 0.0, 0.0, 26.0, 22.47565385754535, 6.918326606476519, 0.0, 1.0, 47.23461475830956], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 283800.0000, 
sim time next is 284400.0000, 
raw observation next is [-12.3, 67.0, 0.0, 0.0, 26.0, 22.5105230172064, 6.984489191401288, 0.0, 1.0, 47.25927797207361], 
processed observation next is [1.0, 0.30434782608695654, 0.12188365650969527, 0.67, 0.0, 0.0, 1.0, 0.5015032881723427, 0.06984489191401287, 0.0, 1.0, 0.3375662712290972], 
reward next is 0.6624, 
noisyNet noise sample is [array([1.6665097], dtype=float32), -0.66728544]. 
=============================================
[2019-04-01 19:40:27,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:40:27,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7885
[2019-04-01 19:40:27,742] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.4166666666666667, 45.33333333333334, 83.0, 733.6666666666667, 26.0, 25.71658915287488, 9.242660227072959, 1.0, 1.0, 13.28092013177433], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 742200.0000, 
sim time next is 742800.0000, 
raw observation next is [0.3333333333333334, 45.66666666666667, 81.5, 723.8333333333333, 26.0, 25.78770327044883, 9.444058700150002, 1.0, 1.0, 10.61707824727174], 
processed observation next is [1.0, 0.6086956521739131, 0.4718374884579871, 0.4566666666666667, 0.27166666666666667, 0.7998158379373849, 1.0, 0.9696718957784043, 0.09444058700150001, 1.0, 1.0, 0.07583627319479815], 
reward next is 0.9242, 
noisyNet noise sample is [array([1.6369059], dtype=float32), 0.021327296]. 
=============================================
[2019-04-01 19:40:28,315] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:40:28,316] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2721
[2019-04-01 19:40:28,378] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.51666666666667, 62.5, 89.66666666666667, 469.0, 26.0, 25.87869000365266, 8.686960611576788, 1.0, 1.0, 34.42320632447637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 295800.0000, 
sim time next is 296400.0000, 
raw observation next is [-11.33333333333333, 62.00000000000001, 88.33333333333333, 490.5, 26.0, 25.91954159300919, 8.793688340614791, 1.0, 1.0, 33.01747909062719], 
processed observation next is [1.0, 0.43478260869565216, 0.14866112650046176, 0.6200000000000001, 0.29444444444444445, 0.541988950276243, 1.0, 0.9885059418584555, 0.08793688340614791, 1.0, 1.0, 0.2358391363616228], 
reward next is 0.7642, 
noisyNet noise sample is [array([-0.38298532], dtype=float32), -0.050388787]. 
=============================================
[2019-04-01 19:40:28,519] A3C_AGENT_WORKER-Thread-13 INFO:Local step 255500, global step 4088443: loss 30.3503
[2019-04-01 19:40:28,519] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 255500, global step 4088443: learning rate 0.0010
[2019-04-01 19:40:30,014] A3C_AGENT_WORKER-Thread-18 INFO:Local step 256000, global step 4088876: loss 0.1990
[2019-04-01 19:40:30,015] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 256000, global step 4088876: learning rate 0.0010
[2019-04-01 19:40:30,294] A3C_AGENT_WORKER-Thread-15 INFO:Local step 255500, global step 4088967: loss 26.9822
[2019-04-01 19:40:30,295] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 255500, global step 4088967: learning rate 0.0010
[2019-04-01 19:40:31,061] A3C_AGENT_WORKER-Thread-9 INFO:Local step 255500, global step 4089268: loss 28.2308
[2019-04-01 19:40:31,063] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 255500, global step 4089269: learning rate 0.0010
[2019-04-01 19:40:31,484] A3C_AGENT_WORKER-Thread-16 INFO:Local step 255500, global step 4089420: loss 31.0934
[2019-04-01 19:40:31,485] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 255500, global step 4089420: learning rate 0.0010
[2019-04-01 19:40:32,019] A3C_AGENT_WORKER-Thread-10 INFO:Local step 255500, global step 4089620: loss 31.1106
[2019-04-01 19:40:32,020] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 255500, global step 4089620: learning rate 0.0010
[2019-04-01 19:40:32,267] A3C_AGENT_WORKER-Thread-5 INFO:Local step 255500, global step 4089704: loss 28.5996
[2019-04-01 19:40:32,268] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 255500, global step 4089704: learning rate 0.0010
[2019-04-01 19:40:32,768] A3C_AGENT_WORKER-Thread-12 INFO:Local step 255500, global step 4089879: loss 27.8879
[2019-04-01 19:40:32,769] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 255500, global step 4089879: learning rate 0.0010
[2019-04-01 19:40:34,027] A3C_AGENT_WORKER-Thread-11 INFO:Local step 255500, global step 4090284: loss 24.7050
[2019-04-01 19:40:34,029] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 255500, global step 4090284: learning rate 0.0010
[2019-04-01 19:40:34,081] A3C_AGENT_WORKER-Thread-3 INFO:Local step 255500, global step 4090298: loss 23.3304
[2019-04-01 19:40:34,093] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 255500, global step 4090298: learning rate 0.0010
[2019-04-01 19:40:34,244] A3C_AGENT_WORKER-Thread-19 INFO:Local step 255500, global step 4090345: loss 23.4180
[2019-04-01 19:40:34,248] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 255500, global step 4090346: learning rate 0.0010
[2019-04-01 19:40:34,851] A3C_AGENT_WORKER-Thread-14 INFO:Local step 255500, global step 4090505: loss 23.9664
[2019-04-01 19:40:34,851] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 255500, global step 4090505: learning rate 0.0010
[2019-04-01 19:40:35,169] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:40:35,169] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8167
[2019-04-01 19:40:35,209] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.1, 27.0, 118.0, 0.0, 26.0, 25.17056817798234, 6.08589775500874, 1.0, 1.0, 29.30875557596519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 472800.0000, 
sim time next is 473400.0000, 
raw observation next is [-2.0, 26.5, 122.0, 0.0, 26.0, 25.12515449947006, 6.08743826423949, 1.0, 1.0, 22.41846068249567], 
processed observation next is [1.0, 0.4782608695652174, 0.40720221606648205, 0.265, 0.4066666666666667, 0.0, 1.0, 0.8750220713528657, 0.060874382642394906, 1.0, 1.0, 0.16013186201782623], 
reward next is 0.8399, 
noisyNet noise sample is [array([-0.7383179], dtype=float32), -0.27321422]. 
=============================================
[2019-04-01 19:40:39,773] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:40:39,775] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3770
[2019-04-01 19:40:39,787] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.15, 51.5, 0.0, 0.0, 26.0, 24.21278741829155, 5.883358780695541, 0.0, 1.0, 44.13425218803813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 426600.0000, 
sim time next is 427200.0000, 
raw observation next is [-11.33333333333333, 52.33333333333334, 0.0, 0.0, 26.0, 24.15710167069463, 5.821871637873542, 0.0, 1.0, 44.13087170433824], 
processed observation next is [1.0, 0.9565217391304348, 0.14866112650046176, 0.5233333333333334, 0.0, 0.0, 1.0, 0.7367288100992327, 0.05821871637873542, 0.0, 1.0, 0.31522051217384456], 
reward next is 0.6848, 
noisyNet noise sample is [array([-0.11122697], dtype=float32), 1.5880991]. 
=============================================
[2019-04-01 19:40:42,569] A3C_AGENT_WORKER-Thread-4 INFO:Local step 256000, global step 4093033: loss 0.1005
[2019-04-01 19:40:42,569] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 256000, global step 4093033: learning rate 0.0010
[2019-04-01 19:40:44,064] A3C_AGENT_WORKER-Thread-17 INFO:Local step 256500, global step 4093594: loss 0.0641
[2019-04-01 19:40:44,065] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 256500, global step 4093594: learning rate 0.0010
[2019-04-01 19:40:47,266] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:40:47,269] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5199
[2019-04-01 19:40:47,283] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.533333333333333, 88.33333333333334, 0.0, 0.0, 26.0, 25.07693498268534, 6.85505801322924, 0.0, 1.0, 39.04041497072613], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 524400.0000, 
sim time next is 525000.0000, 
raw observation next is [4.416666666666667, 88.16666666666667, 0.0, 0.0, 26.0, 25.03932923514404, 6.790101752893936, 0.0, 1.0, 39.07638471818997], 
processed observation next is [0.0, 0.043478260869565216, 0.584949215143121, 0.8816666666666667, 0.0, 0.0, 1.0, 0.8627613193062916, 0.06790101752893936, 0.0, 1.0, 0.27911703370135693], 
reward next is 0.7209, 
noisyNet noise sample is [array([1.5957704], dtype=float32), 0.089599155]. 
=============================================
[2019-04-01 19:40:47,286] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.76054 ]
 [79.15743 ]
 [79.400826]
 [79.71418 ]
 [80.11201 ]], R is [[78.45168304]
 [78.38830566]
 [78.32580566]
 [78.26421356]
 [78.20505524]].
[2019-04-01 19:40:48,166] A3C_AGENT_WORKER-Thread-2 INFO:Local step 256000, global step 4095258: loss 0.0228
[2019-04-01 19:40:48,167] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 256000, global step 4095259: learning rate 0.0010
[2019-04-01 19:40:49,872] A3C_AGENT_WORKER-Thread-18 INFO:Local step 256500, global step 4095950: loss 0.0507
[2019-04-01 19:40:49,873] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 256500, global step 4095950: learning rate 0.0010
[2019-04-01 19:40:50,003] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:40:50,003] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4714
[2019-04-01 19:40:50,041] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 57.0, 13.5, 8.5, 26.0, 24.90770008775366, 7.049415348897152, 0.0, 1.0, 42.06717882494514], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 666000.0000, 
sim time next is 666600.0000, 
raw observation next is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.93280934898113, 7.040329218920841, 0.0, 1.0, 39.23661061234085], 
processed observation next is [0.0, 0.7391304347826086, 0.42936288088642666, 0.57, 0.0, 0.0, 1.0, 0.8475441927115901, 0.07040329218920842, 0.0, 1.0, 0.2802615043738632], 
reward next is 0.7197, 
noisyNet noise sample is [array([-0.27804503], dtype=float32), 1.0752438]. 
=============================================
[2019-04-01 19:40:50,543] A3C_AGENT_WORKER-Thread-20 INFO:Local step 256000, global step 4096241: loss 0.0187
[2019-04-01 19:40:50,545] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 256000, global step 4096242: learning rate 0.0010
[2019-04-01 19:40:53,089] A3C_AGENT_WORKER-Thread-13 INFO:Local step 256000, global step 4097410: loss 0.0501
[2019-04-01 19:40:53,090] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 256000, global step 4097410: learning rate 0.0010
[2019-04-01 19:40:53,989] A3C_AGENT_WORKER-Thread-17 INFO:Local step 257000, global step 4097776: loss 1.5795
[2019-04-01 19:40:53,990] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 257000, global step 4097776: learning rate 0.0010
[2019-04-01 19:40:54,519] A3C_AGENT_WORKER-Thread-9 INFO:Local step 256000, global step 4097984: loss 0.0141
[2019-04-01 19:40:54,522] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 256000, global step 4097985: learning rate 0.0010
[2019-04-01 19:40:54,854] A3C_AGENT_WORKER-Thread-15 INFO:Local step 256000, global step 4098120: loss 0.3756
[2019-04-01 19:40:54,855] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 256000, global step 4098120: learning rate 0.0010
[2019-04-01 19:40:55,563] A3C_AGENT_WORKER-Thread-16 INFO:Local step 256000, global step 4098389: loss 0.1130
[2019-04-01 19:40:55,563] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 256000, global step 4098389: learning rate 0.0010
[2019-04-01 19:40:55,981] A3C_AGENT_WORKER-Thread-10 INFO:Local step 256000, global step 4098549: loss 0.0059
[2019-04-01 19:40:55,982] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 256000, global step 4098549: learning rate 0.0010
[2019-04-01 19:40:56,275] A3C_AGENT_WORKER-Thread-12 INFO:Local step 256000, global step 4098669: loss 0.0043
[2019-04-01 19:40:56,276] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 256000, global step 4098669: learning rate 0.0010
[2019-04-01 19:40:56,375] A3C_AGENT_WORKER-Thread-5 INFO:Local step 256000, global step 4098705: loss 0.0127
[2019-04-01 19:40:56,375] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 256000, global step 4098705: learning rate 0.0010
[2019-04-01 19:40:57,362] A3C_AGENT_WORKER-Thread-11 INFO:Local step 256000, global step 4099180: loss 0.1177
[2019-04-01 19:40:57,369] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 256000, global step 4099183: learning rate 0.0010
[2019-04-01 19:40:58,134] A3C_AGENT_WORKER-Thread-19 INFO:Local step 256000, global step 4099548: loss 0.0122
[2019-04-01 19:40:58,134] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 256000, global step 4099548: learning rate 0.0010
[2019-04-01 19:40:58,208] A3C_AGENT_WORKER-Thread-3 INFO:Local step 256000, global step 4099576: loss 0.1667
[2019-04-01 19:40:58,209] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 256000, global step 4099576: learning rate 0.0010
[2019-04-01 19:40:58,749] A3C_AGENT_WORKER-Thread-14 INFO:Local step 256000, global step 4099795: loss 0.2822
[2019-04-01 19:40:58,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 256000, global step 4099796: learning rate 0.0010
[2019-04-01 19:40:58,752] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:40:58,757] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0279
[2019-04-01 19:40:58,789] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.80927793304149, 6.194296656907572, 0.0, 1.0, 41.43127455378613], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 683400.0000, 
sim time next is 684000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.82159803546967, 6.157078837498908, 0.0, 1.0, 41.3510480691841], 
processed observation next is [0.0, 0.9565217391304348, 0.368421052631579, 0.69, 0.0, 0.0, 1.0, 0.8316568622099528, 0.061570788374989076, 0.0, 1.0, 0.29536462906560074], 
reward next is 0.7046, 
noisyNet noise sample is [array([1.2600962], dtype=float32), -0.037204675]. 
=============================================
[2019-04-01 19:40:58,796] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[65.224846]
 [65.295166]
 [65.38465 ]
 [65.46542 ]
 [65.55846 ]], R is [[65.20995331]
 [65.26191711]
 [65.31271362]
 [65.36250305]
 [65.41137695]].
[2019-04-01 19:40:59,239] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-01 19:40:59,242] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 19:40:59,243] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 19:40:59,243] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:40:59,244] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 19:40:59,244] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:40:59,244] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:40:59,249] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run42
[2019-04-01 19:40:59,275] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run42
[2019-04-01 19:40:59,294] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run42
[2019-04-01 19:41:33,554] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.6804498], dtype=float32), -0.8286272]
[2019-04-01 19:41:33,554] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.6, 67.0, 128.0, 489.0, 26.0, 25.04621837449535, 8.56239184285456, 0.0, 1.0, 24.71207422990976]
[2019-04-01 19:41:33,554] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 19:41:33,555] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.6264399313140183
[2019-04-01 19:42:11,402] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.6804498], dtype=float32), -0.8286272]
[2019-04-01 19:42:11,402] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.3, 40.0, 95.0, 334.0, 26.0, 25.45933406774633, 7.096892162002092, 0.0, 1.0, 32.91732287774484]
[2019-04-01 19:42:11,402] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 19:42:11,402] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.5987664432069368
[2019-04-01 19:42:14,643] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.6804498], dtype=float32), -0.8286272]
[2019-04-01 19:42:14,643] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.508455944, 65.230545105, 0.0, 0.0, 26.0, 25.39638596604837, 8.434540448885093, 0.0, 1.0, 39.43258975883003]
[2019-04-01 19:42:14,644] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:42:14,645] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.41199800973280265
[2019-04-01 19:42:32,429] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.6804498], dtype=float32), -0.8286272]
[2019-04-01 19:42:32,430] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.166666666666667, 51.5, 115.3333333333333, 811.6666666666666, 26.0, 26.24612081098373, 14.34829000928758, 1.0, 1.0, 6.049174834891263]
[2019-04-01 19:42:32,430] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 19:42:32,431] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.000000e+00 0.000000e+00 0.000000e+00 9.868731e-38 0.000000e+00
 1.000000e+00 0.000000e+00], sampled 0.027327415335247074
[2019-04-01 19:42:40,219] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 19:42:43,439] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.6804498], dtype=float32), -0.8286272]
[2019-04-01 19:42:43,439] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.86964066, 51.46400108166667, 0.0, 0.0, 26.0, 24.46445369269103, 6.386226404298228, 0.0, 1.0, 43.85798732991897]
[2019-04-01 19:42:43,439] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:42:43,440] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.31105078967297917
[2019-04-01 19:42:44,210] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.6804498], dtype=float32), -0.8286272]
[2019-04-01 19:42:44,210] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.333333333333333, 32.33333333333334, 107.6666666666667, 800.0, 26.0, 26.8954454981475, 16.5546013819823, 1.0, 1.0, 2.038305690420546]
[2019-04-01 19:42:44,210] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 19:42:44,211] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.9111075338266439
[2019-04-01 19:43:01,292] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 19:43:03,923] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 19:43:04,946] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 4100000, evaluation results [4100000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 19:43:06,065] A3C_AGENT_WORKER-Thread-18 INFO:Local step 257000, global step 4100548: loss 3.1970
[2019-04-01 19:43:06,066] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 257000, global step 4100548: learning rate 0.0010
[2019-04-01 19:43:07,373] A3C_AGENT_WORKER-Thread-4 INFO:Local step 256500, global step 4101048: loss 0.2036
[2019-04-01 19:43:07,376] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 256500, global step 4101049: learning rate 0.0010
[2019-04-01 19:43:12,992] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 5.478928e-36 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 19:43:12,994] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0568
[2019-04-01 19:43:13,031] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 102.3333333333333, 0.0, 26.0, 25.63503554487924, 7.519523406323167, 1.0, 1.0, 26.51484285133867], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 817800.0000, 
sim time next is 818400.0000, 
raw observation next is [-4.5, 71.0, 106.1666666666667, 0.0, 26.0, 25.65784579074814, 7.579488524528578, 1.0, 1.0, 26.48413764870636], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.353888888888889, 0.0, 1.0, 0.9511208272497343, 0.07579488524528578, 1.0, 1.0, 0.18917241177647398], 
reward next is 0.8108, 
noisyNet noise sample is [array([0.0359607], dtype=float32), -0.83091825]. 
=============================================
[2019-04-01 19:43:13,172] A3C_AGENT_WORKER-Thread-2 INFO:Local step 256500, global step 4103459: loss 0.0105
[2019-04-01 19:43:13,173] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 256500, global step 4103459: learning rate 0.0010
[2019-04-01 19:43:15,005] A3C_AGENT_WORKER-Thread-17 INFO:Local step 257500, global step 4104278: loss 0.0346
[2019-04-01 19:43:15,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 257500, global step 4104278: learning rate 0.0010
[2019-04-01 19:43:15,228] A3C_AGENT_WORKER-Thread-20 INFO:Local step 256500, global step 4104396: loss 0.0115
[2019-04-01 19:43:15,232] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 256500, global step 4104397: learning rate 0.0010
[2019-04-01 19:43:15,289] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:43:15,294] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1549
[2019-04-01 19:43:15,300] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 26.0, 24.649073391534, 8.604813928828422, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1193400.0000, 
sim time next is 1194000.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 26.0, 24.62716736276676, 8.534421460648362, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 1.0, 0.8038810518238227, 0.08534421460648361, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5441636], dtype=float32), 0.96950793]. 
=============================================
[2019-04-01 19:43:15,306] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[87.066536]
 [87.12269 ]
 [87.19999 ]
 [87.25395 ]
 [87.30471 ]], R is [[87.15879822]
 [87.28720856]
 [87.41433716]
 [87.54019165]
 [87.66478729]].
[2019-04-01 19:43:17,590] A3C_AGENT_WORKER-Thread-4 INFO:Local step 257000, global step 4105640: loss 0.9935
[2019-04-01 19:43:17,592] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 257000, global step 4105641: learning rate 0.0010
[2019-04-01 19:43:17,598] A3C_AGENT_WORKER-Thread-13 INFO:Local step 256500, global step 4105643: loss 0.0068
[2019-04-01 19:43:17,601] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 256500, global step 4105645: learning rate 0.0010
[2019-04-01 19:43:17,755] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9275744e-35 0.0000000e+00 0.0000000e+00 7.5220776e-13 4.4737349e-35
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:43:17,756] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8659
[2019-04-01 19:43:17,805] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.3, 93.0, 78.0, 0.0, 26.0, 26.22561382406854, 11.57402171670176, 1.0, 1.0, 25.85711319425675], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 917400.0000, 
sim time next is 918000.0000, 
raw observation next is [4.4, 93.0, 72.0, 0.0, 26.0, 25.55729124981809, 10.17198047081504, 1.0, 1.0, 23.82166361679785], 
processed observation next is [1.0, 0.6521739130434783, 0.5844875346260389, 0.93, 0.24, 0.0, 1.0, 0.9367558928311556, 0.1017198047081504, 1.0, 1.0, 0.17015474011998463], 
reward next is 0.7611, 
noisyNet noise sample is [array([-0.7250152], dtype=float32), -2.6340678]. 
=============================================
[2019-04-01 19:43:17,808] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[67.01801 ]
 [67.22771 ]
 [67.333694]
 [67.48055 ]
 [67.6139  ]], R is [[66.94963074]
 [66.46583557]
 [65.84482574]
 [65.18637848]
 [64.53451538]].
[2019-04-01 19:43:18,181] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.4383394e-34], sum to 1.0000
[2019-04-01 19:43:18,184] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5525
[2019-04-01 19:43:18,228] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.5, 92.83333333333333, 18.0, 0.0, 26.0, 25.70544335001741, 10.18102349551107, 1.0, 1.0, 19.99829185827922], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 979800.0000, 
sim time next is 980400.0000, 
raw observation next is [9.600000000000001, 92.66666666666667, 22.5, 0.0, 26.0, 25.63571071987014, 10.68356078647806, 1.0, 1.0, 20.34454506769276], 
processed observation next is [1.0, 0.34782608695652173, 0.7285318559556788, 0.9266666666666667, 0.075, 0.0, 1.0, 0.9479586742671628, 0.1068356078647806, 1.0, 1.0, 0.1453181790549483], 
reward next is 0.5813, 
noisyNet noise sample is [array([1.0646535], dtype=float32), 0.121095374]. 
=============================================
[2019-04-01 19:43:19,024] A3C_AGENT_WORKER-Thread-9 INFO:Local step 256500, global step 4106408: loss 0.0070
[2019-04-01 19:43:19,025] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 256500, global step 4106408: learning rate 0.0010
[2019-04-01 19:43:19,481] A3C_AGENT_WORKER-Thread-15 INFO:Local step 256500, global step 4106677: loss 0.0044
[2019-04-01 19:43:19,483] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 256500, global step 4106678: learning rate 0.0010
[2019-04-01 19:43:19,698] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 5.5321997e-32 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:43:19,698] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3021
[2019-04-01 19:43:19,759] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 96.66666666666667, 0.0, 26.0, 25.309744191421, 15.0413432779829, 1.0, 1.0, 49.84470252230407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1690800.0000, 
sim time next is 1691400.0000, 
raw observation next is [1.1, 88.0, 93.33333333333333, 0.0, 26.0, 26.4162427828834, 16.66506078198912, 1.0, 1.0, 38.8577946992998], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3111111111111111, 0.0, 1.0, 1.0594632546976288, 0.1666506078198912, 1.0, 1.0, 0.27755567642357004], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.13803044], dtype=float32), 0.4677571]. 
=============================================
[2019-04-01 19:43:20,097] A3C_AGENT_WORKER-Thread-16 INFO:Local step 256500, global step 4107017: loss 0.0028
[2019-04-01 19:43:20,098] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 256500, global step 4107019: learning rate 0.0010
[2019-04-01 19:43:20,366] A3C_AGENT_WORKER-Thread-10 INFO:Local step 256500, global step 4107182: loss 0.0091
[2019-04-01 19:43:20,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 256500, global step 4107182: learning rate 0.0010
[2019-04-01 19:43:20,770] A3C_AGENT_WORKER-Thread-5 INFO:Local step 256500, global step 4107417: loss 0.0159
[2019-04-01 19:43:20,771] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 256500, global step 4107418: learning rate 0.0010
[2019-04-01 19:43:20,863] A3C_AGENT_WORKER-Thread-12 INFO:Local step 256500, global step 4107480: loss 0.0172
[2019-04-01 19:43:20,866] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 256500, global step 4107482: learning rate 0.0010
[2019-04-01 19:43:21,033] A3C_AGENT_WORKER-Thread-18 INFO:Local step 257500, global step 4107583: loss 0.0954
[2019-04-01 19:43:21,035] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 257500, global step 4107584: learning rate 0.0010
[2019-04-01 19:43:21,469] A3C_AGENT_WORKER-Thread-11 INFO:Local step 256500, global step 4107855: loss 0.0195
[2019-04-01 19:43:21,470] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 256500, global step 4107856: learning rate 0.0010
[2019-04-01 19:43:22,047] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:43:22,055] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4514
[2019-04-01 19:43:22,065] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.333333333333333, 80.66666666666666, 0.0, 0.0, 26.0, 25.5047072134908, 9.183981878979386, 0.0, 1.0, 27.0817555649879], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 960000.0000, 
sim time next is 960600.0000, 
raw observation next is [7.516666666666667, 80.33333333333334, 0.0, 0.0, 26.0, 25.44723750638612, 8.975828395644905, 0.0, 1.0, 30.01237933977498], 
processed observation next is [1.0, 0.08695652173913043, 0.6708217913204063, 0.8033333333333335, 0.0, 0.0, 1.0, 0.9210339294837314, 0.08975828395644905, 0.0, 1.0, 0.21437413814124986], 
reward next is 0.7856, 
noisyNet noise sample is [array([1.2153127], dtype=float32), 0.63383967]. 
=============================================
[2019-04-01 19:43:22,529] A3C_AGENT_WORKER-Thread-19 INFO:Local step 256500, global step 4108551: loss 0.0653
[2019-04-01 19:43:22,531] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 256500, global step 4108551: learning rate 0.0010
[2019-04-01 19:43:22,858] A3C_AGENT_WORKER-Thread-3 INFO:Local step 256500, global step 4108756: loss 0.0051
[2019-04-01 19:43:22,859] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 256500, global step 4108756: learning rate 0.0010
[2019-04-01 19:43:23,146] A3C_AGENT_WORKER-Thread-14 INFO:Local step 256500, global step 4108943: loss 0.0090
[2019-04-01 19:43:23,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 256500, global step 4108944: learning rate 0.0010
[2019-04-01 19:43:23,819] A3C_AGENT_WORKER-Thread-2 INFO:Local step 257000, global step 4109374: loss 0.3899
[2019-04-01 19:43:23,820] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 257000, global step 4109374: learning rate 0.0010
[2019-04-01 19:43:25,476] A3C_AGENT_WORKER-Thread-20 INFO:Local step 257000, global step 4110462: loss 0.1661
[2019-04-01 19:43:25,476] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 257000, global step 4110462: learning rate 0.0010
[2019-04-01 19:43:28,666] A3C_AGENT_WORKER-Thread-13 INFO:Local step 257000, global step 4112569: loss 0.8925
[2019-04-01 19:43:28,668] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 257000, global step 4112571: learning rate 0.0010
[2019-04-01 19:43:30,253] A3C_AGENT_WORKER-Thread-9 INFO:Local step 257000, global step 4113610: loss 0.9451
[2019-04-01 19:43:30,254] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 257000, global step 4113610: learning rate 0.0010
[2019-04-01 19:43:30,568] A3C_AGENT_WORKER-Thread-15 INFO:Local step 257000, global step 4113812: loss 1.1403
[2019-04-01 19:43:30,573] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 257000, global step 4113813: learning rate 0.0010
[2019-04-01 19:43:31,154] A3C_AGENT_WORKER-Thread-16 INFO:Local step 257000, global step 4114180: loss 2.5720
[2019-04-01 19:43:31,155] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 257000, global step 4114180: learning rate 0.0010
[2019-04-01 19:43:31,551] A3C_AGENT_WORKER-Thread-10 INFO:Local step 257000, global step 4114413: loss 1.7114
[2019-04-01 19:43:31,553] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 257000, global step 4114414: learning rate 0.0010
[2019-04-01 19:43:32,060] A3C_AGENT_WORKER-Thread-5 INFO:Local step 257000, global step 4114734: loss 2.3393
[2019-04-01 19:43:32,061] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 257000, global step 4114735: learning rate 0.0010
[2019-04-01 19:43:32,191] A3C_AGENT_WORKER-Thread-12 INFO:Local step 257000, global step 4114817: loss 1.9322
[2019-04-01 19:43:32,193] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 257000, global step 4114818: learning rate 0.0010
[2019-04-01 19:43:32,593] A3C_AGENT_WORKER-Thread-17 INFO:Local step 258000, global step 4115067: loss 0.6648
[2019-04-01 19:43:32,595] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 258000, global step 4115068: learning rate 0.0010
[2019-04-01 19:43:32,741] A3C_AGENT_WORKER-Thread-11 INFO:Local step 257000, global step 4115149: loss 2.5516
[2019-04-01 19:43:32,742] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 257000, global step 4115149: learning rate 0.0010
[2019-04-01 19:43:32,750] A3C_AGENT_WORKER-Thread-4 INFO:Local step 257500, global step 4115152: loss 0.0077
[2019-04-01 19:43:32,751] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 257500, global step 4115152: learning rate 0.0010
[2019-04-01 19:43:33,940] A3C_AGENT_WORKER-Thread-19 INFO:Local step 257000, global step 4115723: loss 3.1597
[2019-04-01 19:43:33,942] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 257000, global step 4115724: learning rate 0.0010
[2019-04-01 19:43:33,969] A3C_AGENT_WORKER-Thread-3 INFO:Local step 257000, global step 4115743: loss 3.3614
[2019-04-01 19:43:33,971] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 257000, global step 4115743: learning rate 0.0010
[2019-04-01 19:43:34,195] A3C_AGENT_WORKER-Thread-14 INFO:Local step 257000, global step 4115870: loss 1.3050
[2019-04-01 19:43:34,196] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 257000, global step 4115870: learning rate 0.0010
[2019-04-01 19:43:34,667] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:43:34,669] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4731
[2019-04-01 19:43:34,677] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 94.33333333333333, 0.0, 26.0, 25.72461807225884, 11.44430816230568, 1.0, 1.0, 14.62316460334408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1345800.0000, 
sim time next is 1346400.0000, 
raw observation next is [1.1, 92.0, 88.5, 0.0, 26.0, 25.77512805710577, 11.5327050307508, 1.0, 1.0, 11.55976023936458], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.295, 0.0, 1.0, 0.9678754367293959, 0.11532705030750799, 1.0, 1.0, 0.08256971599546128], 
reward next is 0.3043, 
noisyNet noise sample is [array([-0.47581005], dtype=float32), -1.1011212]. 
=============================================
[2019-04-01 19:43:35,048] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7664201e-27 5.2488386e-22 8.7083677e-25 9.3886673e-01 5.3664276e-15
 6.1133292e-02 0.0000000e+00], sum to 1.0000
[2019-04-01 19:43:35,048] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9911
[2019-04-01 19:43:35,104] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 90.0, 0.0, 0.0, 26.0, 25.73674152332424, 11.4625105152349, 1.0, 1.0, 22.85608161200717], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1449000.0000, 
sim time next is 1449600.0000, 
raw observation next is [1.1, 90.66666666666666, 0.0, 0.0, 26.0, 25.68217374306628, 11.37800328950426, 1.0, 1.0, 21.69471834355182], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.9066666666666666, 0.0, 0.0, 1.0, 0.9545962490094685, 0.1137800328950426, 1.0, 1.0, 0.154962273882513], 
reward next is 0.2938, 
noisyNet noise sample is [array([0.5270715], dtype=float32), -1.5783662]. 
=============================================
[2019-04-01 19:43:37,195] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:43:37,195] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8119
[2019-04-01 19:43:37,211] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.28556444174004, 9.957454157297688, 0.0, 1.0, 40.97311183587711], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1387800.0000, 
sim time next is 1388400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.23473997604131, 9.868489748660323, 0.0, 1.0, 40.59233056248082], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.890677139434473, 0.09868489748660324, 0.0, 1.0, 0.2899452183034344], 
reward next is 0.7101, 
noisyNet noise sample is [array([-0.5176241], dtype=float32), 0.29593584]. 
=============================================
[2019-04-01 19:43:38,778] A3C_AGENT_WORKER-Thread-18 INFO:Local step 258000, global step 4118218: loss 0.3102
[2019-04-01 19:43:38,778] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 258000, global step 4118218: learning rate 0.0010
[2019-04-01 19:43:39,305] A3C_AGENT_WORKER-Thread-2 INFO:Local step 257500, global step 4118472: loss 0.0089
[2019-04-01 19:43:39,306] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 257500, global step 4118472: learning rate 0.0010
[2019-04-01 19:43:40,301] A3C_AGENT_WORKER-Thread-20 INFO:Local step 257500, global step 4118976: loss 0.0018
[2019-04-01 19:43:40,302] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 257500, global step 4118977: learning rate 0.0010
[2019-04-01 19:43:41,149] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5061056e-36 1.1686791e-34 1.9222807e-35 8.0666032e-26 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:43:41,149] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4528
[2019-04-01 19:43:41,176] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 76.0, 76.0, 85.5, 26.0, 25.86075537439622, 11.25063114899882, 1.0, 1.0, 13.46370490338619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1587600.0000, 
sim time next is 1588200.0000, 
raw observation next is [6.783333333333333, 74.66666666666667, 89.0, 102.3333333333333, 26.0, 25.87274010346188, 11.56400563982398, 1.0, 1.0, 12.75594135191573], 
processed observation next is [1.0, 0.391304347826087, 0.6505078485687905, 0.7466666666666667, 0.2966666666666667, 0.11307550644567216, 1.0, 0.9818200147802685, 0.11564005639823981, 1.0, 1.0, 0.09111386679939808], 
reward next is 0.2833, 
noisyNet noise sample is [array([-1.6557002], dtype=float32), -0.0782885]. 
=============================================
[2019-04-01 19:43:43,598] A3C_AGENT_WORKER-Thread-13 INFO:Local step 257500, global step 4120679: loss 0.0165
[2019-04-01 19:43:43,601] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 257500, global step 4120681: learning rate 0.0010
[2019-04-01 19:43:43,656] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7527647e-05 0.0000000e+00
 9.9998248e-01 0.0000000e+00], sum to 1.0000
[2019-04-01 19:43:43,656] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3756
[2019-04-01 19:43:43,675] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.933333333333333, 97.33333333333334, 75.5, 118.0, 26.0, 25.86073703747186, 10.22848756326089, 1.0, 1.0, 9.193935257570438], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1507200.0000, 
sim time next is 1507800.0000, 
raw observation next is [3.116666666666667, 96.66666666666666, 78.0, 235.9999999999999, 26.0, 25.83594237298757, 10.5731594419922, 1.0, 1.0, 9.183784950967866], 
processed observation next is [1.0, 0.43478260869565216, 0.5489381348107111, 0.9666666666666666, 0.26, 0.2607734806629833, 1.0, 0.9765631961410816, 0.105731594419922, 1.0, 1.0, 0.06559846393548475], 
reward next is 0.7051, 
noisyNet noise sample is [array([-0.44662684], dtype=float32), -0.27545467]. 
=============================================
[2019-04-01 19:43:44,588] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9632814e-16 0.0000000e+00
 1.0000000e+00 8.1115411e-31], sum to 1.0000
[2019-04-01 19:43:44,588] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9439
[2019-04-01 19:43:44,602] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.85, 94.5, 88.0, 708.0, 26.0, 26.10056000964605, 12.73975176553672, 1.0, 1.0, 6.339936549195255], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1510200.0000, 
sim time next is 1510800.0000, 
raw observation next is [4.033333333333333, 94.0, 90.0, 706.6666666666666, 26.0, 26.17010612778785, 10.57191601992123, 1.0, 1.0, 6.086547872427176], 
processed observation next is [1.0, 0.4782608695652174, 0.5743305632502309, 0.94, 0.3, 0.7808471454880295, 1.0, 1.0243008753982643, 0.1057191601992123, 1.0, 1.0, 0.0434753419459084], 
reward next is 0.7278, 
noisyNet noise sample is [array([0.4107883], dtype=float32), -2.1568947]. 
=============================================
[2019-04-01 19:43:44,964] A3C_AGENT_WORKER-Thread-15 INFO:Local step 257500, global step 4121440: loss 0.0350
[2019-04-01 19:43:44,965] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 257500, global step 4121440: learning rate 0.0010
[2019-04-01 19:43:45,014] A3C_AGENT_WORKER-Thread-9 INFO:Local step 257500, global step 4121468: loss 0.0556
[2019-04-01 19:43:45,015] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 257500, global step 4121468: learning rate 0.0010
[2019-04-01 19:43:45,929] A3C_AGENT_WORKER-Thread-10 INFO:Local step 257500, global step 4121974: loss 0.0621
[2019-04-01 19:43:45,933] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 257500, global step 4121977: learning rate 0.0010
[2019-04-01 19:43:46,037] A3C_AGENT_WORKER-Thread-16 INFO:Local step 257500, global step 4122038: loss 0.0857
[2019-04-01 19:43:46,038] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 257500, global step 4122040: learning rate 0.0010
[2019-04-01 19:43:46,893] A3C_AGENT_WORKER-Thread-12 INFO:Local step 257500, global step 4122595: loss 0.4039
[2019-04-01 19:43:46,895] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 257500, global step 4122596: learning rate 0.0010
[2019-04-01 19:43:46,935] A3C_AGENT_WORKER-Thread-5 INFO:Local step 257500, global step 4122614: loss 0.4216
[2019-04-01 19:43:46,949] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 257500, global step 4122615: learning rate 0.0010
[2019-04-01 19:43:47,596] A3C_AGENT_WORKER-Thread-11 INFO:Local step 257500, global step 4123025: loss 0.1185
[2019-04-01 19:43:47,596] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 257500, global step 4123025: learning rate 0.0010
[2019-04-01 19:43:48,542] A3C_AGENT_WORKER-Thread-3 INFO:Local step 257500, global step 4123526: loss 0.2687
[2019-04-01 19:43:48,543] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 257500, global step 4123526: learning rate 0.0010
[2019-04-01 19:43:49,009] A3C_AGENT_WORKER-Thread-14 INFO:Local step 257500, global step 4123770: loss 0.4565
[2019-04-01 19:43:49,010] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 257500, global step 4123771: learning rate 0.0010
[2019-04-01 19:43:49,260] A3C_AGENT_WORKER-Thread-19 INFO:Local step 257500, global step 4123909: loss 0.2965
[2019-04-01 19:43:49,264] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 257500, global step 4123911: learning rate 0.0010
[2019-04-01 19:43:49,725] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:43:49,725] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6367
[2019-04-01 19:43:49,779] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 83.0, 119.0, 0.0, 26.0, 25.06617258767696, 8.4081650381914, 0.0, 1.0, 29.9182454697965], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1776600.0000, 
sim time next is 1777200.0000, 
raw observation next is [-2.8, 83.0, 115.6666666666667, 0.0, 26.0, 25.04272235326673, 8.313326101967876, 0.0, 1.0, 33.18411856448052], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.38555555555555565, 0.0, 1.0, 0.8632460504666756, 0.08313326101967876, 0.0, 1.0, 0.237029418317718], 
reward next is 0.7630, 
noisyNet noise sample is [array([1.9834735], dtype=float32), -0.5179613]. 
=============================================
[2019-04-01 19:43:50,052] A3C_AGENT_WORKER-Thread-4 INFO:Local step 258000, global step 4124268: loss 2.7135
[2019-04-01 19:43:50,054] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 258000, global step 4124268: learning rate 0.0010
[2019-04-01 19:43:53,325] A3C_AGENT_WORKER-Thread-17 INFO:Local step 258500, global step 4125948: loss 0.0307
[2019-04-01 19:43:53,327] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 258500, global step 4125948: learning rate 0.0010
[2019-04-01 19:43:56,780] A3C_AGENT_WORKER-Thread-2 INFO:Local step 258000, global step 4127195: loss 3.4255
[2019-04-01 19:43:56,792] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 258000, global step 4127195: learning rate 0.0010
[2019-04-01 19:43:57,024] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:43:57,025] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6530
[2019-04-01 19:43:57,084] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.733333333333333, 83.66666666666667, 23.5, 0.0, 26.0, 25.03560686060847, 8.446973558276644, 0.0, 1.0, 45.65424218236676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1788000.0000, 
sim time next is 1788600.0000, 
raw observation next is [-3.816666666666666, 82.83333333333334, 19.0, 0.0, 26.0, 25.07080894967694, 8.410869358220637, 0.0, 1.0, 37.19086183074334], 
processed observation next is [0.0, 0.6956521739130435, 0.3568790397045245, 0.8283333333333335, 0.06333333333333334, 0.0, 1.0, 0.8672584213824202, 0.08410869358220636, 0.0, 1.0, 0.2656490130767381], 
reward next is 0.7344, 
noisyNet noise sample is [array([0.0205032], dtype=float32), 0.8140516]. 
=============================================
[2019-04-01 19:43:58,049] A3C_AGENT_WORKER-Thread-20 INFO:Local step 258000, global step 4127605: loss 1.3491
[2019-04-01 19:43:58,049] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 258000, global step 4127605: learning rate 0.0010
[2019-04-01 19:44:00,002] A3C_AGENT_WORKER-Thread-18 INFO:Local step 258500, global step 4128391: loss 0.1441
[2019-04-01 19:44:00,010] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 258500, global step 4128393: learning rate 0.0010
[2019-04-01 19:44:02,038] A3C_AGENT_WORKER-Thread-13 INFO:Local step 258000, global step 4129103: loss 0.0570
[2019-04-01 19:44:02,039] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 258000, global step 4129103: learning rate 0.0010
[2019-04-01 19:44:02,654] A3C_AGENT_WORKER-Thread-15 INFO:Local step 258000, global step 4129314: loss 0.1994
[2019-04-01 19:44:02,655] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 258000, global step 4129314: learning rate 0.0010
[2019-04-01 19:44:03,248] A3C_AGENT_WORKER-Thread-9 INFO:Local step 258000, global step 4129509: loss 0.1325
[2019-04-01 19:44:03,250] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 258000, global step 4129509: learning rate 0.0010
[2019-04-01 19:44:03,946] A3C_AGENT_WORKER-Thread-16 INFO:Local step 258000, global step 4129736: loss 5.6316
[2019-04-01 19:44:03,946] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 258000, global step 4129736: learning rate 0.0010
[2019-04-01 19:44:04,041] A3C_AGENT_WORKER-Thread-10 INFO:Local step 258000, global step 4129767: loss 0.3603
[2019-04-01 19:44:04,042] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 258000, global step 4129767: learning rate 0.0010
[2019-04-01 19:44:05,403] A3C_AGENT_WORKER-Thread-12 INFO:Local step 258000, global step 4130237: loss 1.2819
[2019-04-01 19:44:05,403] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 258000, global step 4130237: learning rate 0.0010
[2019-04-01 19:44:05,554] A3C_AGENT_WORKER-Thread-5 INFO:Local step 258000, global step 4130297: loss 0.9409
[2019-04-01 19:44:05,555] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 258000, global step 4130298: learning rate 0.0010
[2019-04-01 19:44:06,277] A3C_AGENT_WORKER-Thread-11 INFO:Local step 258000, global step 4130593: loss 1.1888
[2019-04-01 19:44:06,279] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 258000, global step 4130593: learning rate 0.0010
[2019-04-01 19:44:06,768] A3C_AGENT_WORKER-Thread-3 INFO:Local step 258000, global step 4130775: loss 0.5654
[2019-04-01 19:44:06,769] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 258000, global step 4130775: learning rate 0.0010
[2019-04-01 19:44:07,139] A3C_AGENT_WORKER-Thread-14 INFO:Local step 258000, global step 4130915: loss 0.1492
[2019-04-01 19:44:07,140] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 258000, global step 4130915: learning rate 0.0010
[2019-04-01 19:44:07,456] A3C_AGENT_WORKER-Thread-19 INFO:Local step 258000, global step 4131020: loss 0.8256
[2019-04-01 19:44:07,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 258000, global step 4131020: learning rate 0.0010
[2019-04-01 19:44:11,852] A3C_AGENT_WORKER-Thread-4 INFO:Local step 258500, global step 4132568: loss 0.0224
[2019-04-01 19:44:11,858] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 258500, global step 4132569: learning rate 0.0010
[2019-04-01 19:44:15,084] A3C_AGENT_WORKER-Thread-17 INFO:Local step 259000, global step 4133753: loss 1.1760
[2019-04-01 19:44:15,084] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 259000, global step 4133753: learning rate 0.0010
[2019-04-01 19:44:15,091] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.2721556e-35 5.3570672e-31
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:44:15,091] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1792
[2019-04-01 19:44:15,226] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 23.97197430298224, 5.315341469525092, 0.0, 1.0, 40.48354546853854], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2012400.0000, 
sim time next is 2013000.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.04652285524886, 5.695399120284459, 0.0, 1.0, 107.0743774171778], 
processed observation next is [1.0, 0.30434782608695654, 0.2908587257617729, 0.87, 0.0, 0.0, 1.0, 0.7209318364641228, 0.05695399120284459, 0.0, 1.0, 0.76481698155127], 
reward next is 0.2352, 
noisyNet noise sample is [array([-0.05748978], dtype=float32), 1.5781788]. 
=============================================
[2019-04-01 19:44:15,230] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[68.40687]
 [68.42965]
 [68.44967]
 [68.46426]
 [68.47105]], R is [[67.42635345]
 [67.46292114]
 [67.49835968]
 [67.53282928]
 [67.56686401]].
[2019-04-01 19:44:17,089] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:44:17,089] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2034
[2019-04-01 19:44:17,144] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.516666666666667, 37.5, 0.0, 0.0, 26.0, 25.04986149950454, 6.257131575855794, 0.0, 1.0, 39.08156539535005], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2505000.0000, 
sim time next is 2505600.0000, 
raw observation next is [-1.7, 38.0, 0.0, 0.0, 26.0, 25.03106309921971, 6.202611730179387, 0.0, 1.0, 39.02964757502578], 
processed observation next is [1.0, 0.0, 0.4155124653739613, 0.38, 0.0, 0.0, 1.0, 0.861580442745673, 0.06202611730179387, 0.0, 1.0, 0.27878319696446985], 
reward next is 0.7212, 
noisyNet noise sample is [array([-1.801929], dtype=float32), -0.40869606]. 
=============================================
[2019-04-01 19:44:17,755] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:44:17,756] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0503
[2019-04-01 19:44:17,814] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.83592543192227, 10.11299848855395, 0.0, 1.0, 28.03995861031146], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2061000.0000, 
sim time next is 2061600.0000, 
raw observation next is [-3.899999999999999, 86.0, 0.0, 0.0, 26.0, 25.5861671824291, 9.527159593901068, 0.0, 1.0, 30.89377432486877], 
processed observation next is [1.0, 0.8695652173913043, 0.35457063711911363, 0.86, 0.0, 0.0, 1.0, 0.9408810260613001, 0.09527159593901068, 0.0, 1.0, 0.2206698166062055], 
reward next is 0.7793, 
noisyNet noise sample is [array([0.82939446], dtype=float32), -0.8478567]. 
=============================================
[2019-04-01 19:44:19,066] A3C_AGENT_WORKER-Thread-2 INFO:Local step 258500, global step 4135143: loss 0.1422
[2019-04-01 19:44:19,068] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 258500, global step 4135143: learning rate 0.0010
[2019-04-01 19:44:20,433] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2246755e-28 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:44:20,433] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9273
[2019-04-01 19:44:20,498] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.00000000000001, 0.0, 0.0, 26.0, 25.37958739371722, 9.011372092925333, 1.0, 1.0, 27.77012285277757], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2052600.0000, 
sim time next is 2053200.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.42245883426907, 8.572714131847773, 1.0, 1.0, 26.53890514294563], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 1.0, 0.9174941191812955, 0.08572714131847774, 1.0, 1.0, 0.18956360816389736], 
reward next is 0.8104, 
noisyNet noise sample is [array([-0.05313615], dtype=float32), -1.3942975]. 
=============================================
[2019-04-01 19:44:21,095] A3C_AGENT_WORKER-Thread-20 INFO:Local step 258500, global step 4135844: loss 0.2407
[2019-04-01 19:44:21,096] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 258500, global step 4135844: learning rate 0.0010
[2019-04-01 19:44:21,374] A3C_AGENT_WORKER-Thread-18 INFO:Local step 259000, global step 4135952: loss 1.2704
[2019-04-01 19:44:21,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 259000, global step 4135952: learning rate 0.0010
[2019-04-01 19:44:21,454] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:44:21,454] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0160
[2019-04-01 19:44:21,473] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.283333333333333, 86.33333333333333, 0.0, 0.0, 26.0, 24.21905999175606, 5.613818168509297, 0.0, 1.0, 43.48783873830823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2092200.0000, 
sim time next is 2092800.0000, 
raw observation next is [-6.366666666666667, 85.66666666666667, 0.0, 0.0, 26.0, 24.28205125212267, 5.623805297231558, 0.0, 1.0, 42.47128719926971], 
processed observation next is [1.0, 0.21739130434782608, 0.28624192059095105, 0.8566666666666667, 0.0, 0.0, 1.0, 0.7545787503032386, 0.056238052972315586, 0.0, 1.0, 0.3033663371376408], 
reward next is 0.6966, 
noisyNet noise sample is [array([0.81376505], dtype=float32), -0.29912955]. 
=============================================
[2019-04-01 19:44:24,243] A3C_AGENT_WORKER-Thread-13 INFO:Local step 258500, global step 4137012: loss 0.1703
[2019-04-01 19:44:24,244] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 258500, global step 4137012: learning rate 0.0010
[2019-04-01 19:44:24,845] A3C_AGENT_WORKER-Thread-15 INFO:Local step 258500, global step 4137236: loss 0.2271
[2019-04-01 19:44:24,847] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 258500, global step 4137237: learning rate 0.0010
[2019-04-01 19:44:25,371] A3C_AGENT_WORKER-Thread-9 INFO:Local step 258500, global step 4137423: loss 0.2129
[2019-04-01 19:44:25,371] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 258500, global step 4137423: learning rate 0.0010
[2019-04-01 19:44:26,126] A3C_AGENT_WORKER-Thread-10 INFO:Local step 258500, global step 4137711: loss 0.1302
[2019-04-01 19:44:26,126] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 258500, global step 4137711: learning rate 0.0010
[2019-04-01 19:44:26,520] A3C_AGENT_WORKER-Thread-16 INFO:Local step 258500, global step 4137865: loss 0.2046
[2019-04-01 19:44:26,522] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 258500, global step 4137865: learning rate 0.0010
[2019-04-01 19:44:27,293] A3C_AGENT_WORKER-Thread-12 INFO:Local step 258500, global step 4138143: loss 0.3631
[2019-04-01 19:44:27,294] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 258500, global step 4138143: learning rate 0.0010
[2019-04-01 19:44:27,317] A3C_AGENT_WORKER-Thread-5 INFO:Local step 258500, global step 4138150: loss 0.1714
[2019-04-01 19:44:27,318] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 258500, global step 4138150: learning rate 0.0010
[2019-04-01 19:44:28,397] A3C_AGENT_WORKER-Thread-11 INFO:Local step 258500, global step 4138549: loss 0.1624
[2019-04-01 19:44:28,398] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 258500, global step 4138549: learning rate 0.0010
[2019-04-01 19:44:28,627] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 5.711538e-31 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 19:44:28,628] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8531
[2019-04-01 19:44:28,658] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.12687345207167, 8.93262817796676, 0.0, 1.0, 42.92829808433977], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2148000.0000, 
sim time next is 2148600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.16965653537429, 8.941786964503523, 0.0, 1.0, 42.73991286762762], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 1.0, 0.8813795050534701, 0.08941786964503523, 0.0, 1.0, 0.3052850919116259], 
reward next is 0.6947, 
noisyNet noise sample is [array([0.30625543], dtype=float32), 1.0188249]. 
=============================================
[2019-04-01 19:44:29,040] A3C_AGENT_WORKER-Thread-3 INFO:Local step 258500, global step 4138778: loss 0.4350
[2019-04-01 19:44:29,041] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 258500, global step 4138778: learning rate 0.0010
[2019-04-01 19:44:29,767] A3C_AGENT_WORKER-Thread-14 INFO:Local step 258500, global step 4139041: loss 0.1345
[2019-04-01 19:44:29,768] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 258500, global step 4139041: learning rate 0.0010
[2019-04-01 19:44:29,828] A3C_AGENT_WORKER-Thread-19 INFO:Local step 258500, global step 4139062: loss 0.0436
[2019-04-01 19:44:29,829] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 258500, global step 4139062: learning rate 0.0010
[2019-04-01 19:44:30,450] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 7.45981e-21 0.00000e+00 1.00000e+00
 0.00000e+00], sum to 1.0000
[2019-04-01 19:44:30,451] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2669
[2019-04-01 19:44:30,505] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666667, 71.0, 121.3333333333333, 0.0, 26.0, 25.68068784171173, 7.685130565068846, 1.0, 1.0, 17.68032832795175], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2198400.0000, 
sim time next is 2199000.0000, 
raw observation next is [-4.583333333333333, 71.0, 125.6666666666667, 0.0, 26.0, 25.64629826881641, 7.588881859094556, 1.0, 1.0, 27.75334718744672], 
processed observation next is [1.0, 0.43478260869565216, 0.3356417359187443, 0.71, 0.418888888888889, 0.0, 1.0, 0.949471181259487, 0.07588881859094557, 1.0, 1.0, 0.19823819419604802], 
reward next is 0.8018, 
noisyNet noise sample is [array([0.06328747], dtype=float32), 1.7628174]. 
=============================================
[2019-04-01 19:44:30,520] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[62.557602]
 [62.829494]
 [63.008507]
 [63.345573]
 [63.754143]], R is [[62.58599472]
 [62.83384705]
 [63.07439804]
 [63.30773544]
 [63.53365326]].
[2019-04-01 19:44:33,693] A3C_AGENT_WORKER-Thread-4 INFO:Local step 259000, global step 4140446: loss 0.2889
[2019-04-01 19:44:33,699] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 259000, global step 4140447: learning rate 0.0010
[2019-04-01 19:44:34,066] A3C_AGENT_WORKER-Thread-17 INFO:Local step 259500, global step 4140586: loss 0.5077
[2019-04-01 19:44:34,067] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 259500, global step 4140587: learning rate 0.0010
[2019-04-01 19:44:35,973] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 5.1097152e-26 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:44:35,975] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8420
[2019-04-01 19:44:35,999] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 51.0, 0.0, 0.0, 26.0, 25.85021747777387, 9.617129670386452, 1.0, 1.0, 17.82541041564095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2310000.0000, 
sim time next is 2310600.0000, 
raw observation next is [-1.1, 51.5, 0.0, 0.0, 26.0, 25.68707459397027, 9.365209238778933, 1.0, 1.0, 15.78896098433824], 
processed observation next is [1.0, 0.7391304347826086, 0.4321329639889197, 0.515, 0.0, 0.0, 1.0, 0.9552963705671813, 0.09365209238778932, 1.0, 1.0, 0.11277829274527315], 
reward next is 0.8872, 
noisyNet noise sample is [array([-0.9785669], dtype=float32), -0.5065437]. 
=============================================
[2019-04-01 19:44:38,591] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 4.05264e-37 0.00000e+00 1.00000e+00
 0.00000e+00], sum to 1.0000
[2019-04-01 19:44:38,593] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2127
[2019-04-01 19:44:38,641] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 43.0, 129.5, 51.0, 26.0, 26.78301127205844, 11.41674802515818, 1.0, 1.0, 23.37781325041483], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2300400.0000, 
sim time next is 2301000.0000, 
raw observation next is [0.9166666666666667, 43.16666666666667, 132.3333333333333, 48.0, 26.0, 26.35040621317487, 11.37191138902982, 1.0, 1.0, 21.66733456668572], 
processed observation next is [1.0, 0.6521739130434783, 0.48799630655586346, 0.4316666666666667, 0.44111111111111095, 0.05303867403314917, 1.0, 1.050058030453553, 0.1137191138902982, 1.0, 1.0, 0.15476667547632658], 
reward next is 0.2965, 
noisyNet noise sample is [array([-1.3600374], dtype=float32), 0.4439099]. 
=============================================
[2019-04-01 19:44:38,644] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.849724]
 [76.755646]
 [76.553856]
 [76.25169 ]
 [76.122246]], R is [[76.27944946]
 [75.78297424]
 [75.02514648]
 [74.27489471]
 [73.53215027]].
[2019-04-01 19:44:40,342] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:44:40,343] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6535
[2019-04-01 19:44:40,352] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.10781316533675, 5.387114111354637, 0.0, 1.0, 40.93540501306782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2358600.0000, 
sim time next is 2359200.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.075540400717, 5.361137094856427, 0.0, 1.0, 41.01736531197189], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.0, 0.0, 1.0, 0.7250772001024285, 0.05361137094856427, 0.0, 1.0, 0.29298118079979923], 
reward next is 0.7070, 
noisyNet noise sample is [array([-0.08863328], dtype=float32), 0.36146817]. 
=============================================
[2019-04-01 19:44:40,611] A3C_AGENT_WORKER-Thread-2 INFO:Local step 259000, global step 4143211: loss 0.1957
[2019-04-01 19:44:40,611] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 259000, global step 4143211: learning rate 0.0010
[2019-04-01 19:44:40,778] A3C_AGENT_WORKER-Thread-18 INFO:Local step 259500, global step 4143269: loss 0.0132
[2019-04-01 19:44:40,780] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 259500, global step 4143269: learning rate 0.0010
[2019-04-01 19:44:42,585] A3C_AGENT_WORKER-Thread-20 INFO:Local step 259000, global step 4143982: loss 0.0836
[2019-04-01 19:44:42,586] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 259000, global step 4143982: learning rate 0.0010
[2019-04-01 19:44:45,622] A3C_AGENT_WORKER-Thread-13 INFO:Local step 259000, global step 4145282: loss 0.2098
[2019-04-01 19:44:45,622] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 259000, global step 4145282: learning rate 0.0010
[2019-04-01 19:44:46,853] A3C_AGENT_WORKER-Thread-15 INFO:Local step 259000, global step 4145768: loss 0.0154
[2019-04-01 19:44:46,854] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 259000, global step 4145768: learning rate 0.0010
[2019-04-01 19:44:46,926] A3C_AGENT_WORKER-Thread-10 INFO:Local step 259000, global step 4145805: loss 0.0087
[2019-04-01 19:44:46,928] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 259000, global step 4145805: learning rate 0.0010
[2019-04-01 19:44:46,984] A3C_AGENT_WORKER-Thread-9 INFO:Local step 259000, global step 4145832: loss 0.0109
[2019-04-01 19:44:46,987] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 259000, global step 4145835: learning rate 0.0010
[2019-04-01 19:44:47,200] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 3.2028324e-38 7.8519908e-32 1.9230014e-18
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:44:47,201] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6129
[2019-04-01 19:44:47,222] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.58978078996581, 5.357259377100244, 0.0, 1.0, 43.74081772912893], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2433600.0000, 
sim time next is 2434200.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.5466155873731, 5.38251016876135, 0.0, 1.0, 43.77766011706727], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 1.0, 0.6495165124818715, 0.053825101687613494, 0.0, 1.0, 0.31269757226476624], 
reward next is 0.6873, 
noisyNet noise sample is [array([-0.122954], dtype=float32), -0.85964537]. 
=============================================
[2019-04-01 19:44:47,556] A3C_AGENT_WORKER-Thread-16 INFO:Local step 259000, global step 4146108: loss 0.0098
[2019-04-01 19:44:47,557] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 259000, global step 4146110: learning rate 0.0010
[2019-04-01 19:44:48,503] A3C_AGENT_WORKER-Thread-12 INFO:Local step 259000, global step 4146560: loss 0.1403
[2019-04-01 19:44:48,504] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 259000, global step 4146560: learning rate 0.0010
[2019-04-01 19:44:48,521] A3C_AGENT_WORKER-Thread-5 INFO:Local step 259000, global step 4146569: loss 0.0159
[2019-04-01 19:44:48,522] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 259000, global step 4146572: learning rate 0.0010
[2019-04-01 19:44:49,843] A3C_AGENT_WORKER-Thread-11 INFO:Local step 259000, global step 4147158: loss 0.2187
[2019-04-01 19:44:49,843] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 259000, global step 4147158: learning rate 0.0010
[2019-04-01 19:44:50,226] A3C_AGENT_WORKER-Thread-3 INFO:Local step 259000, global step 4147326: loss 0.4318
[2019-04-01 19:44:50,226] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 259000, global step 4147326: learning rate 0.0010
[2019-04-01 19:44:50,865] A3C_AGENT_WORKER-Thread-14 INFO:Local step 259000, global step 4147612: loss 0.5155
[2019-04-01 19:44:50,867] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 259000, global step 4147613: learning rate 0.0010
[2019-04-01 19:44:51,083] A3C_AGENT_WORKER-Thread-19 INFO:Local step 259000, global step 4147731: loss 0.3859
[2019-04-01 19:44:51,086] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 259000, global step 4147731: learning rate 0.0010
[2019-04-01 19:44:51,905] A3C_AGENT_WORKER-Thread-4 INFO:Local step 259500, global step 4148139: loss 0.6359
[2019-04-01 19:44:51,906] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 259500, global step 4148139: learning rate 0.0010
[2019-04-01 19:44:52,744] A3C_AGENT_WORKER-Thread-17 INFO:Local step 260000, global step 4148560: loss 0.2805
[2019-04-01 19:44:52,745] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 260000, global step 4148561: learning rate 0.0010
[2019-04-01 19:44:52,926] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1572367e-21 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:44:52,928] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7306
[2019-04-01 19:44:52,955] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 28.0, 171.0, 482.0, 26.0, 25.45683344665037, 7.920229707080274, 1.0, 1.0, 6.263686001087046], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2554200.0000, 
sim time next is 2554800.0000, 
raw observation next is [3.266666666666667, 27.33333333333334, 169.0, 447.5, 26.0, 25.67389006953908, 8.328102131952674, 1.0, 1.0, 5.572168316246596], 
processed observation next is [1.0, 0.5652173913043478, 0.5530932594644506, 0.2733333333333334, 0.5633333333333334, 0.494475138121547, 1.0, 0.9534128670770114, 0.08328102131952674, 1.0, 1.0, 0.03980120225890425], 
reward next is 0.9602, 
noisyNet noise sample is [array([0.20211725], dtype=float32), -1.7367864]. 
=============================================
[2019-04-01 19:44:58,875] A3C_AGENT_WORKER-Thread-2 INFO:Local step 259500, global step 4151329: loss 0.0552
[2019-04-01 19:44:58,876] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 259500, global step 4151329: learning rate 0.0010
[2019-04-01 19:44:59,169] A3C_AGENT_WORKER-Thread-18 INFO:Local step 260000, global step 4151468: loss 0.7956
[2019-04-01 19:44:59,170] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 260000, global step 4151468: learning rate 0.0010
[2019-04-01 19:45:00,393] A3C_AGENT_WORKER-Thread-20 INFO:Local step 259500, global step 4151955: loss 0.0248
[2019-04-01 19:45:00,394] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 259500, global step 4151955: learning rate 0.0010
[2019-04-01 19:45:03,705] A3C_AGENT_WORKER-Thread-13 INFO:Local step 259500, global step 4153364: loss 0.1991
[2019-04-01 19:45:03,721] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 259500, global step 4153364: learning rate 0.0010
[2019-04-01 19:45:04,381] A3C_AGENT_WORKER-Thread-15 INFO:Local step 259500, global step 4153627: loss 3.2869
[2019-04-01 19:45:04,382] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 259500, global step 4153627: learning rate 0.0010
[2019-04-01 19:45:04,761] A3C_AGENT_WORKER-Thread-10 INFO:Local step 259500, global step 4153788: loss 1.3642
[2019-04-01 19:45:04,762] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 259500, global step 4153788: learning rate 0.0010
[2019-04-01 19:45:05,088] A3C_AGENT_WORKER-Thread-9 INFO:Local step 259500, global step 4153939: loss 2.1758
[2019-04-01 19:45:05,088] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 259500, global step 4153939: learning rate 0.0010
[2019-04-01 19:45:05,492] A3C_AGENT_WORKER-Thread-16 INFO:Local step 259500, global step 4154102: loss 2.5137
[2019-04-01 19:45:05,495] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 259500, global step 4154102: learning rate 0.0010
[2019-04-01 19:45:06,321] A3C_AGENT_WORKER-Thread-12 INFO:Local step 259500, global step 4154449: loss 5.4414
[2019-04-01 19:45:06,323] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 259500, global step 4154450: learning rate 0.0010
[2019-04-01 19:45:06,439] A3C_AGENT_WORKER-Thread-5 INFO:Local step 259500, global step 4154499: loss 4.6719
[2019-04-01 19:45:06,439] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 259500, global step 4154499: learning rate 0.0010
[2019-04-01 19:45:07,192] A3C_AGENT_WORKER-Thread-17 INFO:Local step 260500, global step 4154834: loss 0.1605
[2019-04-01 19:45:07,193] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 260500, global step 4154835: learning rate 0.0010
[2019-04-01 19:45:07,975] A3C_AGENT_WORKER-Thread-11 INFO:Local step 259500, global step 4155188: loss 5.3845
[2019-04-01 19:45:07,976] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 259500, global step 4155188: learning rate 0.0010
[2019-04-01 19:45:08,205] A3C_AGENT_WORKER-Thread-3 INFO:Local step 259500, global step 4155291: loss 5.0052
[2019-04-01 19:45:08,206] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 259500, global step 4155291: learning rate 0.0010
[2019-04-01 19:45:08,869] A3C_AGENT_WORKER-Thread-14 INFO:Local step 259500, global step 4155579: loss 7.1389
[2019-04-01 19:45:08,870] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 259500, global step 4155580: learning rate 0.0010
[2019-04-01 19:45:09,287] A3C_AGENT_WORKER-Thread-19 INFO:Local step 259500, global step 4155757: loss 7.9519
[2019-04-01 19:45:09,296] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 259500, global step 4155759: learning rate 0.0010
[2019-04-01 19:45:10,095] A3C_AGENT_WORKER-Thread-4 INFO:Local step 260000, global step 4156161: loss 1.1928
[2019-04-01 19:45:10,096] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 260000, global step 4156161: learning rate 0.0010
[2019-04-01 19:45:14,178] A3C_AGENT_WORKER-Thread-18 INFO:Local step 260500, global step 4158049: loss 2.5208
[2019-04-01 19:45:14,179] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 260500, global step 4158049: learning rate 0.0010
[2019-04-01 19:45:14,788] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5911919e-37 0.0000000e+00 1.3884734e-38 6.2478585e-27 1.8363973e-33
 1.0000000e+00 9.8317337e-35], sum to 1.0000
[2019-04-01 19:45:14,789] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4740
[2019-04-01 19:45:14,813] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 23.8739018727131, 5.39564194468791, 0.0, 1.0, 59.31615346792961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2961600.0000, 
sim time next is 2962200.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 23.8051682162077, 5.356678868732324, 0.0, 1.0, 59.46275259095135], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 1.0, 0.6864526023153859, 0.05356678868732324, 0.0, 1.0, 0.42473394707822393], 
reward next is 0.5753, 
noisyNet noise sample is [array([-0.7153971], dtype=float32), 0.0028648395]. 
=============================================
[2019-04-01 19:45:17,419] A3C_AGENT_WORKER-Thread-2 INFO:Local step 260000, global step 4159571: loss 2.2336
[2019-04-01 19:45:17,422] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 260000, global step 4159571: learning rate 0.0010
[2019-04-01 19:45:18,583] A3C_AGENT_WORKER-Thread-20 INFO:Local step 260000, global step 4160048: loss 2.1746
[2019-04-01 19:45:18,586] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 260000, global step 4160050: learning rate 0.0010
[2019-04-01 19:45:21,714] A3C_AGENT_WORKER-Thread-17 INFO:Local step 261000, global step 4161587: loss 3.4779
[2019-04-01 19:45:21,715] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 261000, global step 4161589: learning rate 0.0010
[2019-04-01 19:45:22,234] A3C_AGENT_WORKER-Thread-13 INFO:Local step 260000, global step 4161844: loss 1.2847
[2019-04-01 19:45:22,236] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 260000, global step 4161846: learning rate 0.0010
[2019-04-01 19:45:22,918] A3C_AGENT_WORKER-Thread-15 INFO:Local step 260000, global step 4162172: loss 0.8464
[2019-04-01 19:45:22,920] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 260000, global step 4162172: learning rate 0.0010
[2019-04-01 19:45:23,124] A3C_AGENT_WORKER-Thread-10 INFO:Local step 260000, global step 4162265: loss 1.3947
[2019-04-01 19:45:23,126] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 260000, global step 4162265: learning rate 0.0010
[2019-04-01 19:45:23,671] A3C_AGENT_WORKER-Thread-9 INFO:Local step 260000, global step 4162542: loss 1.0613
[2019-04-01 19:45:23,674] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 260000, global step 4162543: learning rate 0.0010
[2019-04-01 19:45:23,976] A3C_AGENT_WORKER-Thread-16 INFO:Local step 260000, global step 4162688: loss 1.1179
[2019-04-01 19:45:23,977] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 260000, global step 4162688: learning rate 0.0010
[2019-04-01 19:45:24,340] A3C_AGENT_WORKER-Thread-5 INFO:Local step 260000, global step 4162878: loss 0.9852
[2019-04-01 19:45:24,340] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 260000, global step 4162878: learning rate 0.0010
[2019-04-01 19:45:24,634] A3C_AGENT_WORKER-Thread-4 INFO:Local step 260500, global step 4163044: loss 0.7860
[2019-04-01 19:45:24,634] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 260500, global step 4163044: learning rate 0.0010
[2019-04-01 19:45:24,685] A3C_AGENT_WORKER-Thread-12 INFO:Local step 260000, global step 4163073: loss 0.6872
[2019-04-01 19:45:24,685] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 260000, global step 4163073: learning rate 0.0010
[2019-04-01 19:45:26,084] A3C_AGENT_WORKER-Thread-11 INFO:Local step 260000, global step 4163770: loss 1.2077
[2019-04-01 19:45:26,089] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 260000, global step 4163770: learning rate 0.0010
[2019-04-01 19:45:26,541] A3C_AGENT_WORKER-Thread-3 INFO:Local step 260000, global step 4164038: loss 1.0289
[2019-04-01 19:45:26,542] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 260000, global step 4164038: learning rate 0.0010
[2019-04-01 19:45:26,850] A3C_AGENT_WORKER-Thread-14 INFO:Local step 260000, global step 4164222: loss 1.0662
[2019-04-01 19:45:26,851] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 260000, global step 4164223: learning rate 0.0010
[2019-04-01 19:45:27,437] A3C_AGENT_WORKER-Thread-19 INFO:Local step 260000, global step 4164587: loss 1.1430
[2019-04-01 19:45:27,438] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 260000, global step 4164588: learning rate 0.0010
[2019-04-01 19:45:27,661] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00
 1.4506888e-25 1.0230652e-32], sum to 1.0000
[2019-04-01 19:45:27,663] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0333
[2019-04-01 19:45:27,676] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.666666666666667, 95.33333333333334, 113.1666666666667, 820.0, 26.0, 26.98073338690067, 18.06603506318243, 1.0, 1.0, 4.170810135722771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3154800.0000, 
sim time next is 3155400.0000, 
raw observation next is [7.5, 96.5, 113.0, 823.0, 26.0, 26.99066255639843, 18.13000173129022, 1.0, 1.0, 3.772600497317435], 
processed observation next is [1.0, 0.5217391304347826, 0.6703601108033241, 0.965, 0.37666666666666665, 0.9093922651933701, 1.0, 1.1415232223426328, 0.1813000173129022, 1.0, 1.0, 0.02694714640941025], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0209969], dtype=float32), 0.15002102]. 
=============================================
[2019-04-01 19:45:28,351] A3C_AGENT_WORKER-Thread-18 INFO:Local step 261000, global step 4165164: loss 4.6385
[2019-04-01 19:45:28,357] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 261000, global step 4165165: learning rate 0.0010
[2019-04-01 19:45:29,191] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.8779834e-37 5.5410410e-35 0.0000000e+00 1.1035844e-24 9.9185896e-30
 2.4345133e-09 1.0000000e+00], sum to 1.0000
[2019-04-01 19:45:29,195] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5273
[2019-04-01 19:45:29,232] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 63.66666666666667, 102.5, 695.8333333333334, 26.0, 26.16168620625128, 11.43449929152133, 1.0, 1.0, 16.7238643642373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3836400.0000, 
sim time next is 3837000.0000, 
raw observation next is [-2.333333333333333, 61.83333333333333, 104.0, 711.6666666666666, 26.0, 26.24004959134594, 11.55701471954477, 1.0, 1.0, 16.90669523255382], 
processed observation next is [1.0, 0.391304347826087, 0.3979686057248385, 0.6183333333333333, 0.3466666666666667, 0.7863720073664825, 1.0, 1.0342927987637058, 0.1155701471954477, 1.0, 1.0, 0.12076210880395585], 
reward next is 0.2564, 
noisyNet noise sample is [array([1.3147585], dtype=float32), 0.7359963]. 
=============================================
[2019-04-01 19:45:29,241] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[34.889427]
 [35.841606]
 [37.03521 ]
 [38.4628  ]
 [40.054478]], R is [[34.13322067]
 [34.09863281]
 [34.20902634]
 [34.47597122]
 [34.8754425 ]].
[2019-04-01 19:45:31,799] A3C_AGENT_WORKER-Thread-2 INFO:Local step 260500, global step 4167051: loss 10.6667
[2019-04-01 19:45:31,800] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 260500, global step 4167052: learning rate 0.0010
[2019-04-01 19:45:33,224] A3C_AGENT_WORKER-Thread-20 INFO:Local step 260500, global step 4167792: loss 7.4108
[2019-04-01 19:45:33,230] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 260500, global step 4167792: learning rate 0.0010
[2019-04-01 19:45:36,039] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-01 19:45:36,041] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9509
[2019-04-01 19:45:36,083] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 66.0, 0.0, 0.0, 26.0, 25.47218209181196, 12.46847202157371, 1.0, 1.0, 29.40810737241296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3265800.0000, 
sim time next is 3266400.0000, 
raw observation next is [-4.0, 67.0, 0.0, 0.0, 26.0, 25.59559038658159, 12.33111096279241, 0.0, 1.0, 28.76101933399188], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.67, 0.0, 0.0, 1.0, 0.9422271980830844, 0.1233111096279241, 0.0, 1.0, 0.2054358523856563], 
reward next is 0.7946, 
noisyNet noise sample is [array([-0.5928326], dtype=float32), -1.8789912]. 
=============================================
[2019-04-01 19:45:36,455] A3C_AGENT_WORKER-Thread-17 INFO:Local step 261500, global step 4169295: loss 1.4359
[2019-04-01 19:45:36,460] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 261500, global step 4169297: learning rate 0.0010
[2019-04-01 19:45:36,709] A3C_AGENT_WORKER-Thread-13 INFO:Local step 260500, global step 4169417: loss 1.6681
[2019-04-01 19:45:36,714] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 260500, global step 4169419: learning rate 0.0010
[2019-04-01 19:45:37,551] A3C_AGENT_WORKER-Thread-15 INFO:Local step 260500, global step 4169893: loss 1.6298
[2019-04-01 19:45:37,553] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 260500, global step 4169894: learning rate 0.0010
[2019-04-01 19:45:38,357] A3C_AGENT_WORKER-Thread-10 INFO:Local step 260500, global step 4170295: loss 1.4020
[2019-04-01 19:45:38,358] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 260500, global step 4170295: learning rate 0.0010
[2019-04-01 19:45:38,548] A3C_AGENT_WORKER-Thread-9 INFO:Local step 260500, global step 4170398: loss 0.3479
[2019-04-01 19:45:38,552] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 260500, global step 4170399: learning rate 0.0010
[2019-04-01 19:45:38,821] A3C_AGENT_WORKER-Thread-16 INFO:Local step 260500, global step 4170531: loss 0.2575
[2019-04-01 19:45:38,822] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 260500, global step 4170531: learning rate 0.0010
[2019-04-01 19:45:39,173] A3C_AGENT_WORKER-Thread-4 INFO:Local step 261000, global step 4170707: loss 1.7326
[2019-04-01 19:45:39,174] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 261000, global step 4170707: learning rate 0.0010
[2019-04-01 19:45:39,332] A3C_AGENT_WORKER-Thread-5 INFO:Local step 260500, global step 4170802: loss 0.2210
[2019-04-01 19:45:39,335] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 260500, global step 4170803: learning rate 0.0010
[2019-04-01 19:45:39,855] A3C_AGENT_WORKER-Thread-12 INFO:Local step 260500, global step 4171078: loss 0.2584
[2019-04-01 19:45:39,856] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 260500, global step 4171078: learning rate 0.0010
[2019-04-01 19:45:40,946] A3C_AGENT_WORKER-Thread-11 INFO:Local step 260500, global step 4171670: loss 0.3497
[2019-04-01 19:45:40,948] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 260500, global step 4171670: learning rate 0.0010
[2019-04-01 19:45:41,480] A3C_AGENT_WORKER-Thread-3 INFO:Local step 260500, global step 4171962: loss 0.2291
[2019-04-01 19:45:41,481] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 260500, global step 4171962: learning rate 0.0010
[2019-04-01 19:45:41,664] A3C_AGENT_WORKER-Thread-14 INFO:Local step 260500, global step 4172070: loss 0.2245
[2019-04-01 19:45:41,666] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 260500, global step 4172070: learning rate 0.0010
[2019-04-01 19:45:42,436] A3C_AGENT_WORKER-Thread-19 INFO:Local step 260500, global step 4172479: loss 0.2613
[2019-04-01 19:45:42,437] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 260500, global step 4172480: learning rate 0.0010
[2019-04-01 19:45:43,234] A3C_AGENT_WORKER-Thread-18 INFO:Local step 261500, global step 4172886: loss 1.4873
[2019-04-01 19:45:43,235] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 261500, global step 4172886: learning rate 0.0010
[2019-04-01 19:45:46,170] A3C_AGENT_WORKER-Thread-2 INFO:Local step 261000, global step 4174569: loss 0.8447
[2019-04-01 19:45:46,171] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 261000, global step 4174569: learning rate 0.0010
[2019-04-01 19:45:47,666] A3C_AGENT_WORKER-Thread-20 INFO:Local step 261000, global step 4175449: loss 0.3047
[2019-04-01 19:45:47,669] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 261000, global step 4175449: learning rate 0.0010
[2019-04-01 19:45:48,433] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4227013e-24 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:45:48,437] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3726
[2019-04-01 19:45:48,447] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 25.01950509839942, 7.515166765667058, 0.0, 1.0, 12.16210827527526], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3695400.0000, 
sim time next is 3696000.0000, 
raw observation next is [4.0, 59.0, 0.0, 0.0, 26.0, 24.96334272882751, 7.49101315241568, 0.0, 1.0, 17.18301334635609], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.59, 0.0, 0.0, 1.0, 0.8519061041182157, 0.07491013152415679, 0.0, 1.0, 0.12273580961682923], 
reward next is 0.8773, 
noisyNet noise sample is [array([-2.257325], dtype=float32), -2.0527558]. 
=============================================
[2019-04-01 19:45:48,450] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[78.181076]
 [78.38202 ]
 [78.45475 ]
 [78.47949 ]
 [78.13891 ]], R is [[78.03227234]
 [78.16507721]
 [78.32001495]
 [78.48995972]
 [78.67015076]].
[2019-04-01 19:45:49,608] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.0728134e-10], sum to 1.0000
[2019-04-01 19:45:49,609] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1927
[2019-04-01 19:45:49,617] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.333333333333332, 26.66666666666666, 0.0, 0.0, 26.0, 25.52738623104794, 7.84300954791848, 0.0, 1.0, 27.35300667635509], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3639000.0000, 
sim time next is 3639600.0000, 
raw observation next is [8.2, 27.0, 0.0, 0.0, 26.0, 25.50068782115464, 7.759749696698606, 0.0, 1.0, 28.36214290940895], 
processed observation next is [0.0, 0.13043478260869565, 0.6897506925207757, 0.27, 0.0, 0.0, 1.0, 0.9286696887363769, 0.07759749696698606, 0.0, 1.0, 0.20258673506720679], 
reward next is 0.7974, 
noisyNet noise sample is [array([0.05385951], dtype=float32), 0.53521115]. 
=============================================
[2019-04-01 19:45:50,924] A3C_AGENT_WORKER-Thread-13 INFO:Local step 261000, global step 4177140: loss 0.3164
[2019-04-01 19:45:50,925] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 261000, global step 4177141: learning rate 0.0010
[2019-04-01 19:45:51,970] A3C_AGENT_WORKER-Thread-17 INFO:Local step 262000, global step 4177687: loss 7.3241
[2019-04-01 19:45:51,971] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 262000, global step 4177687: learning rate 0.0010
[2019-04-01 19:45:52,172] A3C_AGENT_WORKER-Thread-15 INFO:Local step 261000, global step 4177814: loss 1.6502
[2019-04-01 19:45:52,176] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 261000, global step 4177816: learning rate 0.0010
[2019-04-01 19:45:52,959] A3C_AGENT_WORKER-Thread-10 INFO:Local step 261000, global step 4178265: loss 2.4769
[2019-04-01 19:45:52,961] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 261000, global step 4178265: learning rate 0.0010
[2019-04-01 19:45:53,158] A3C_AGENT_WORKER-Thread-16 INFO:Local step 261000, global step 4178384: loss 1.7888
[2019-04-01 19:45:53,160] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 261000, global step 4178385: learning rate 0.0010
[2019-04-01 19:45:53,196] A3C_AGENT_WORKER-Thread-9 INFO:Local step 261000, global step 4178411: loss 1.9407
[2019-04-01 19:45:53,201] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 261000, global step 4178411: learning rate 0.0010
[2019-04-01 19:45:53,536] A3C_AGENT_WORKER-Thread-4 INFO:Local step 261500, global step 4178614: loss 1.9446
[2019-04-01 19:45:53,539] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 261500, global step 4178615: learning rate 0.0010
[2019-04-01 19:45:53,856] A3C_AGENT_WORKER-Thread-5 INFO:Local step 261000, global step 4178789: loss 2.2321
[2019-04-01 19:45:53,857] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 261000, global step 4178789: learning rate 0.0010
[2019-04-01 19:45:54,397] A3C_AGENT_WORKER-Thread-12 INFO:Local step 261000, global step 4179110: loss 2.4455
[2019-04-01 19:45:54,398] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 261000, global step 4179110: learning rate 0.0010
[2019-04-01 19:45:55,652] A3C_AGENT_WORKER-Thread-11 INFO:Local step 261000, global step 4179811: loss 4.6191
[2019-04-01 19:45:55,653] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 261000, global step 4179812: learning rate 0.0010
[2019-04-01 19:45:56,030] A3C_AGENT_WORKER-Thread-3 INFO:Local step 261000, global step 4180026: loss 7.1410
[2019-04-01 19:45:56,031] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 261000, global step 4180026: learning rate 0.0010
[2019-04-01 19:45:56,389] A3C_AGENT_WORKER-Thread-14 INFO:Local step 261000, global step 4180227: loss 4.3379
[2019-04-01 19:45:56,389] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 261000, global step 4180227: learning rate 0.0010
[2019-04-01 19:45:57,343] A3C_AGENT_WORKER-Thread-19 INFO:Local step 261000, global step 4180770: loss 6.4501
[2019-04-01 19:45:57,345] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 261000, global step 4180771: learning rate 0.0010
[2019-04-01 19:45:58,756] A3C_AGENT_WORKER-Thread-18 INFO:Local step 262000, global step 4181542: loss 2.6556
[2019-04-01 19:45:58,757] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 262000, global step 4181542: learning rate 0.0010
[2019-04-01 19:46:01,012] A3C_AGENT_WORKER-Thread-2 INFO:Local step 261500, global step 4182666: loss 0.1335
[2019-04-01 19:46:01,015] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 261500, global step 4182666: learning rate 0.0010
[2019-04-01 19:46:03,025] A3C_AGENT_WORKER-Thread-20 INFO:Local step 261500, global step 4183805: loss 0.6819
[2019-04-01 19:46:03,027] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 261500, global step 4183806: learning rate 0.0010
[2019-04-01 19:46:05,800] A3C_AGENT_WORKER-Thread-17 INFO:Local step 262500, global step 4185270: loss 0.1506
[2019-04-01 19:46:05,801] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 262500, global step 4185271: learning rate 0.0010
[2019-04-01 19:46:06,093] A3C_AGENT_WORKER-Thread-13 INFO:Local step 261500, global step 4185438: loss 0.0573
[2019-04-01 19:46:06,098] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 261500, global step 4185441: learning rate 0.0010
[2019-04-01 19:46:07,355] A3C_AGENT_WORKER-Thread-15 INFO:Local step 261500, global step 4186108: loss 0.0254
[2019-04-01 19:46:07,356] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 261500, global step 4186108: learning rate 0.0010
[2019-04-01 19:46:07,588] A3C_AGENT_WORKER-Thread-10 INFO:Local step 261500, global step 4186198: loss 53.8994
[2019-04-01 19:46:07,588] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 261500, global step 4186198: learning rate 0.0010
[2019-04-01 19:46:07,837] A3C_AGENT_WORKER-Thread-16 INFO:Local step 261500, global step 4186306: loss 0.0025
[2019-04-01 19:46:07,838] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 261500, global step 4186306: learning rate 0.0010
[2019-04-01 19:46:08,604] A3C_AGENT_WORKER-Thread-5 INFO:Local step 261500, global step 4186699: loss 0.0051
[2019-04-01 19:46:08,605] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 261500, global step 4186699: learning rate 0.0010
[2019-04-01 19:46:08,702] A3C_AGENT_WORKER-Thread-9 INFO:Local step 261500, global step 4186749: loss 0.0189
[2019-04-01 19:46:08,702] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 261500, global step 4186749: learning rate 0.0010
[2019-04-01 19:46:09,107] A3C_AGENT_WORKER-Thread-4 INFO:Local step 262000, global step 4186937: loss 2.2383
[2019-04-01 19:46:09,108] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 262000, global step 4186937: learning rate 0.0010
[2019-04-01 19:46:09,765] A3C_AGENT_WORKER-Thread-12 INFO:Local step 261500, global step 4187264: loss 0.0092
[2019-04-01 19:46:09,766] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 261500, global step 4187265: learning rate 0.0010
[2019-04-01 19:46:10,676] A3C_AGENT_WORKER-Thread-11 INFO:Local step 261500, global step 4187697: loss 0.2545
[2019-04-01 19:46:10,679] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 261500, global step 4187699: learning rate 0.0010
[2019-04-01 19:46:10,864] A3C_AGENT_WORKER-Thread-3 INFO:Local step 261500, global step 4187801: loss 0.0671
[2019-04-01 19:46:10,864] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 261500, global step 4187801: learning rate 0.0010
[2019-04-01 19:46:11,223] A3C_AGENT_WORKER-Thread-14 INFO:Local step 261500, global step 4187977: loss 0.0215
[2019-04-01 19:46:11,223] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 261500, global step 4187977: learning rate 0.0010
[2019-04-01 19:46:11,488] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:46:11,490] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9753
[2019-04-01 19:46:11,505] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.33333333333333, 65.0, 0.0, 0.0, 26.0, 24.06534051898868, 5.444869821684153, 0.0, 1.0, 43.12959328197773], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3990000.0000, 
sim time next is 3990600.0000, 
raw observation next is [-12.5, 66.0, 0.0, 0.0, 26.0, 24.03945311732016, 5.38612610617151, 0.0, 1.0, 43.14779679180189], 
processed observation next is [1.0, 0.17391304347826086, 0.11634349030470914, 0.66, 0.0, 0.0, 1.0, 0.7199218739028801, 0.0538612610617151, 0.0, 1.0, 0.30819854851287065], 
reward next is 0.6918, 
noisyNet noise sample is [array([0.2908104], dtype=float32), 0.98624665]. 
=============================================
[2019-04-01 19:46:12,233] A3C_AGENT_WORKER-Thread-19 INFO:Local step 261500, global step 4188498: loss 0.1182
[2019-04-01 19:46:12,234] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 261500, global step 4188498: learning rate 0.0010
[2019-04-01 19:46:12,278] A3C_AGENT_WORKER-Thread-18 INFO:Local step 262500, global step 4188523: loss 2.3055
[2019-04-01 19:46:12,279] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 262500, global step 4188523: learning rate 0.0010
[2019-04-01 19:46:12,696] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:46:12,698] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3534
[2019-04-01 19:46:12,713] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.22152025013982, 7.786369946806599, 0.0, 1.0, 39.09960962705225], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4160400.0000, 
sim time next is 4161000.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.1979633733989, 7.725431761374314, 0.0, 1.0, 39.10228423957097], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 1.0, 0.8854233390569856, 0.07725431761374314, 0.0, 1.0, 0.27930203028264977], 
reward next is 0.7207, 
noisyNet noise sample is [array([1.4820334], dtype=float32), 1.071733]. 
=============================================
[2019-04-01 19:46:12,721] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[68.97573 ]
 [68.956   ]
 [68.94958 ]
 [68.997375]
 [69.09853 ]], R is [[69.02529144]
 [69.05575562]
 [69.08591461]
 [69.11556244]
 [69.14492035]].
[2019-04-01 19:46:14,332] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:46:14,336] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9922
[2019-04-01 19:46:14,359] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.749999999999999, 62.83333333333333, 0.0, 0.0, 26.0, 25.69032658975805, 12.15930407661642, 0.0, 1.0, 6.905580458594887], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4405800.0000, 
sim time next is 4406400.0000, 
raw observation next is [7.6, 63.0, 0.0, 0.0, 26.0, 25.64237726780006, 12.33492640466606, 0.0, 1.0, 24.08122365103215], 
processed observation next is [1.0, 0.0, 0.6731301939058172, 0.63, 0.0, 0.0, 1.0, 0.9489110382571516, 0.1233492640466606, 0.0, 1.0, 0.17200874036451536], 
reward next is 0.8280, 
noisyNet noise sample is [array([1.3603944], dtype=float32), -0.6975975]. 
=============================================
[2019-04-01 19:46:15,415] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:46:15,417] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4860
[2019-04-01 19:46:15,438] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 24.76967423285657, 6.296874455162879, 0.0, 1.0, 39.8592263283273], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4078200.0000, 
sim time next is 4078800.0000, 
raw observation next is [-4.0, 34.0, 0.0, 0.0, 26.0, 24.81315704758627, 6.286302101072152, 0.0, 1.0, 39.7781035989347], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.34, 0.0, 0.0, 1.0, 0.8304510067980385, 0.06286302101072151, 0.0, 1.0, 0.28412931142096215], 
reward next is 0.7159, 
noisyNet noise sample is [array([1.7394648], dtype=float32), -0.5281268]. 
=============================================
[2019-04-01 19:46:16,593] A3C_AGENT_WORKER-Thread-2 INFO:Local step 262000, global step 4190817: loss 2.5055
[2019-04-01 19:46:16,597] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 262000, global step 4190819: learning rate 0.0010
[2019-04-01 19:46:18,130] A3C_AGENT_WORKER-Thread-20 INFO:Local step 262000, global step 4191648: loss 0.0420
[2019-04-01 19:46:18,131] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 262000, global step 4191648: learning rate 0.0010
[2019-04-01 19:46:20,437] A3C_AGENT_WORKER-Thread-17 INFO:Local step 263000, global step 4192923: loss 9.7879
[2019-04-01 19:46:20,441] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 263000, global step 4192924: learning rate 0.0010
[2019-04-01 19:46:21,685] A3C_AGENT_WORKER-Thread-13 INFO:Local step 262000, global step 4193586: loss 2.0827
[2019-04-01 19:46:21,686] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 262000, global step 4193586: learning rate 0.0010
[2019-04-01 19:46:22,673] A3C_AGENT_WORKER-Thread-4 INFO:Local step 262500, global step 4194111: loss 0.1647
[2019-04-01 19:46:22,676] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 262500, global step 4194113: learning rate 0.0010
[2019-04-01 19:46:22,743] A3C_AGENT_WORKER-Thread-15 INFO:Local step 262000, global step 4194143: loss 5.2851
[2019-04-01 19:46:22,744] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 262000, global step 4194144: learning rate 0.0010
[2019-04-01 19:46:23,153] A3C_AGENT_WORKER-Thread-16 INFO:Local step 262000, global step 4194378: loss 5.8481
[2019-04-01 19:46:23,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 262000, global step 4194378: learning rate 0.0010
[2019-04-01 19:46:23,438] A3C_AGENT_WORKER-Thread-10 INFO:Local step 262000, global step 4194542: loss 4.8899
[2019-04-01 19:46:23,439] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 262000, global step 4194542: learning rate 0.0010
[2019-04-01 19:46:24,299] A3C_AGENT_WORKER-Thread-5 INFO:Local step 262000, global step 4195070: loss 5.1171
[2019-04-01 19:46:24,301] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 262000, global step 4195070: learning rate 0.0010
[2019-04-01 19:46:24,428] A3C_AGENT_WORKER-Thread-9 INFO:Local step 262000, global step 4195147: loss 6.9158
[2019-04-01 19:46:24,431] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 262000, global step 4195148: learning rate 0.0010
[2019-04-01 19:46:25,504] A3C_AGENT_WORKER-Thread-12 INFO:Local step 262000, global step 4195759: loss 2.1337
[2019-04-01 19:46:25,505] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 262000, global step 4195759: learning rate 0.0010
[2019-04-01 19:46:26,172] A3C_AGENT_WORKER-Thread-3 INFO:Local step 262000, global step 4196117: loss 0.6244
[2019-04-01 19:46:26,174] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 262000, global step 4196118: learning rate 0.0010
[2019-04-01 19:46:26,712] A3C_AGENT_WORKER-Thread-11 INFO:Local step 262000, global step 4196417: loss 0.3882
[2019-04-01 19:46:26,715] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 262000, global step 4196417: learning rate 0.0010
[2019-04-01 19:46:26,872] A3C_AGENT_WORKER-Thread-14 INFO:Local step 262000, global step 4196513: loss 0.4854
[2019-04-01 19:46:26,876] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 262000, global step 4196515: learning rate 0.0010
[2019-04-01 19:46:27,245] A3C_AGENT_WORKER-Thread-18 INFO:Local step 263000, global step 4196734: loss 3.7230
[2019-04-01 19:46:27,245] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 263000, global step 4196734: learning rate 0.0010
[2019-04-01 19:46:27,991] A3C_AGENT_WORKER-Thread-19 INFO:Local step 262000, global step 4197138: loss 0.0046
[2019-04-01 19:46:27,993] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 262000, global step 4197138: learning rate 0.0010
[2019-04-01 19:46:28,485] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6600669e-38 0.0000000e+00 5.2776834e-36 2.3622998e-35 1.5286136e-32
 1.0000000e+00 6.9046057e-25], sum to 1.0000
[2019-04-01 19:46:28,489] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4402
[2019-04-01 19:46:28,528] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 65.0, 59.0, 101.0, 26.0, 25.47101979844313, 8.625658727534102, 1.0, 1.0, 30.60765979854625], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5038200.0000, 
sim time next is 5038800.0000, 
raw observation next is [-2.333333333333333, 65.0, 65.33333333333334, 173.0, 26.0, 25.54111752366948, 8.672607669923236, 0.0, 1.0, 26.70593400843152], 
processed observation next is [1.0, 0.30434782608695654, 0.3979686057248385, 0.65, 0.21777777777777782, 0.19116022099447513, 1.0, 0.9344453605242116, 0.08672607669923237, 0.0, 1.0, 0.19075667148879658], 
reward next is 0.8092, 
noisyNet noise sample is [array([-1.148002], dtype=float32), -0.1716117]. 
=============================================
[2019-04-01 19:46:30,505] A3C_AGENT_WORKER-Thread-2 INFO:Local step 262500, global step 4198592: loss 15.4075
[2019-04-01 19:46:30,506] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 262500, global step 4198593: learning rate 0.0010
[2019-04-01 19:46:32,001] A3C_AGENT_WORKER-Thread-20 INFO:Local step 262500, global step 4199408: loss 14.8287
[2019-04-01 19:46:32,002] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 262500, global step 4199408: learning rate 0.0010
[2019-04-01 19:46:32,307] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:46:32,308] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:46:32,311] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run32
[2019-04-01 19:46:33,105] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-01 19:46:33,109] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 19:46:33,110] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:46:33,110] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 19:46:33,110] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 19:46:33,111] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:46:33,112] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:46:33,126] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run43
[2019-04-01 19:46:33,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run43
[2019-04-01 19:46:33,149] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run43
[2019-04-01 19:48:08,610] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.4435912], dtype=float32), -0.7456332]
[2019-04-01 19:48:08,611] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.0, 67.0, 0.0, 0.0, 26.0, 25.36884657597557, 8.108447807311277, 0.0, 1.0, 36.9411242922905]
[2019-04-01 19:48:08,612] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 19:48:08,613] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.22975868267017963
[2019-04-01 19:48:15,858] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 19:48:26,866] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.4435912], dtype=float32), -0.7456332]
[2019-04-01 19:48:26,866] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.333333333333333, 81.66666666666667, 130.5, 0.0, 26.0, 25.49136638465011, 9.128808958826108, 1.0, 1.0, 25.82024495905278]
[2019-04-01 19:48:26,866] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 19:48:26,867] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 1.4515465e-33 1.3561225e-09 0.0000000e+00
 1.0000000e+00 2.1089996e-34], sampled 0.7679186697156275
[2019-04-01 19:48:29,579] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.4435912], dtype=float32), -0.7456332]
[2019-04-01 19:48:29,579] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.3230500305, 57.92928517833334, 0.0, 0.0, 26.0, 25.57293839989061, 11.00489651236998, 0.0, 1.0, 28.54370427762187]
[2019-04-01 19:48:29,579] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 19:48:29,580] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.7971956202266497
[2019-04-01 19:48:33,836] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 19:48:37,714] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 19:48:38,738] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 4200000, evaluation results [4200000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 19:48:41,435] A3C_AGENT_WORKER-Thread-13 INFO:Local step 262500, global step 4201343: loss 8.6008
[2019-04-01 19:48:41,436] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 262500, global step 4201343: learning rate 0.0010
[2019-04-01 19:48:42,188] A3C_AGENT_WORKER-Thread-15 INFO:Local step 262500, global step 4201744: loss 6.4425
[2019-04-01 19:48:42,189] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 262500, global step 4201744: learning rate 0.0010
[2019-04-01 19:48:42,596] A3C_AGENT_WORKER-Thread-16 INFO:Local step 262500, global step 4201983: loss 1.5440
[2019-04-01 19:48:42,597] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 262500, global step 4201984: learning rate 0.0010
[2019-04-01 19:48:42,885] A3C_AGENT_WORKER-Thread-10 INFO:Local step 262500, global step 4202133: loss 1.8465
[2019-04-01 19:48:42,888] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 262500, global step 4202134: learning rate 0.0010
[2019-04-01 19:48:43,622] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:48:43,625] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5248
[2019-04-01 19:48:43,689] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 76.0, 0.0, 0.0, 26.0, 25.00574684892231, 7.772296176448194, 0.0, 1.0, 52.71464997202018], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4605000.0000, 
sim time next is 4605600.0000, 
raw observation next is [-2.666666666666667, 75.0, 0.0, 0.0, 26.0, 25.17108248675846, 7.895399794598771, 0.0, 1.0, 34.79364207183813], 
processed observation next is [1.0, 0.30434782608695654, 0.38873499538319484, 0.75, 0.0, 0.0, 1.0, 0.8815832123940659, 0.07895399794598772, 0.0, 1.0, 0.2485260147988438], 
reward next is 0.7515, 
noisyNet noise sample is [array([2.1264613], dtype=float32), -0.5367314]. 
=============================================
[2019-04-01 19:48:44,074] A3C_AGENT_WORKER-Thread-9 INFO:Local step 262500, global step 4202705: loss 0.3088
[2019-04-01 19:48:44,084] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 262500, global step 4202705: learning rate 0.0010
[2019-04-01 19:48:44,134] A3C_AGENT_WORKER-Thread-4 INFO:Local step 263000, global step 4202731: loss 14.6558
[2019-04-01 19:48:44,136] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 263000, global step 4202731: learning rate 0.0010
[2019-04-01 19:48:44,194] A3C_AGENT_WORKER-Thread-5 INFO:Local step 262500, global step 4202763: loss 0.2415
[2019-04-01 19:48:44,195] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 262500, global step 4202763: learning rate 0.0010
[2019-04-01 19:48:44,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:48:44,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:48:44,867] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 5.40403e-08
 0.00000e+00], sum to 1.0000
[2019-04-01 19:48:44,870] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7293
[2019-04-01 19:48:44,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run32
[2019-04-01 19:48:44,881] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.833333333333333, 49.0, 126.3333333333333, 843.6666666666667, 26.0, 25.95589867325726, 13.95766425625948, 1.0, 1.0, 2.11964930862468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4625400.0000, 
sim time next is 4626000.0000, 
raw observation next is [4.0, 49.0, 129.5, 836.0, 26.0, 26.24818916665495, 15.53826966054271, 1.0, 1.0, 2.093534097604659], 
processed observation next is [1.0, 0.5652173913043478, 0.5734072022160666, 0.49, 0.43166666666666664, 0.9237569060773481, 1.0, 1.0354555952364213, 0.1553826966054271, 1.0, 1.0, 0.014953814982890421], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3624012], dtype=float32), -1.6230572]. 
=============================================
[2019-04-01 19:48:44,921] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[68.57903]
 [67.99984]
 [67.47021]
 [67.03595]
 [66.34217]], R is [[68.41329193]
 [67.72915649]
 [67.05186462]
 [66.38134766]
 [65.71753693]].
[2019-04-01 19:48:45,170] A3C_AGENT_WORKER-Thread-12 INFO:Local step 262500, global step 4203238: loss 0.6363
[2019-04-01 19:48:45,174] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 262500, global step 4203238: learning rate 0.0010
[2019-04-01 19:48:45,638] A3C_AGENT_WORKER-Thread-3 INFO:Local step 262500, global step 4203486: loss 0.4983
[2019-04-01 19:48:45,639] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 262500, global step 4203486: learning rate 0.0010
[2019-04-01 19:48:46,430] A3C_AGENT_WORKER-Thread-11 INFO:Local step 262500, global step 4203909: loss 0.4433
[2019-04-01 19:48:46,430] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 262500, global step 4203909: learning rate 0.0010
[2019-04-01 19:48:46,633] A3C_AGENT_WORKER-Thread-14 INFO:Local step 262500, global step 4204021: loss 0.1138
[2019-04-01 19:48:46,634] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 262500, global step 4204021: learning rate 0.0010
[2019-04-01 19:48:47,656] A3C_AGENT_WORKER-Thread-19 INFO:Local step 262500, global step 4204540: loss 0.1542
[2019-04-01 19:48:47,658] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 262500, global step 4204540: learning rate 0.0010
[2019-04-01 19:48:48,513] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9998879e-01 0.0000000e+00
 1.1205853e-05 3.0865283e-11], sum to 1.0000
[2019-04-01 19:48:48,515] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2079
[2019-04-01 19:48:48,529] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.733333333333333, 44.0, 130.0, 144.0, 26.0, 26.28458860579992, 16.75297188507201, 1.0, 1.0, 1.118024774194206], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4638000.0000, 
sim time next is 4638600.0000, 
raw observation next is [5.6, 44.5, 117.0, 147.0, 26.0, 26.74475868477557, 18.41439537397092, 1.0, 1.0, 1.095761451652768], 
processed observation next is [1.0, 0.6956521739130435, 0.6177285318559557, 0.445, 0.39, 0.16243093922651933, 1.0, 1.1063940978250812, 0.18414395373970918, 1.0, 1.0, 0.007826867511805486], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5544535], dtype=float32), 0.55082595]. 
=============================================
[2019-04-01 19:48:49,614] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:48:49,615] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4157
[2019-04-01 19:48:49,652] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 71.0, 129.8333333333333, 227.3333333333333, 26.0, 25.84265701476858, 10.26095505787224, 1.0, 1.0, 17.67531818910703], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4610400.0000, 
sim time next is 4611000.0000, 
raw observation next is [-2.0, 71.0, 136.6666666666667, 283.6666666666666, 26.0, 25.99920125045782, 10.52991810487851, 1.0, 1.0, 16.60229685648874], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.4555555555555557, 0.3134438305709023, 1.0, 0.9998858929225456, 0.10529918104878509, 1.0, 1.0, 0.11858783468920528], 
reward next is 0.6694, 
noisyNet noise sample is [array([-0.33660877], dtype=float32), -0.55986345]. 
=============================================
[2019-04-01 19:48:49,666] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[47.510296]
 [47.26642 ]
 [47.20147 ]
 [48.02797 ]
 [49.17525 ]], R is [[47.89725876]
 [48.18765259]
 [48.56244659]
 [48.92121887]
 [49.27786636]].
[2019-04-01 19:48:51,407] A3C_AGENT_WORKER-Thread-2 INFO:Local step 263000, global step 4206470: loss 4.1318
[2019-04-01 19:48:51,410] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 263000, global step 4206470: learning rate 0.0010
[2019-04-01 19:48:53,199] A3C_AGENT_WORKER-Thread-20 INFO:Local step 263000, global step 4207349: loss 7.3521
[2019-04-01 19:48:53,201] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 263000, global step 4207349: learning rate 0.0010
[2019-04-01 19:48:55,395] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:48:55,395] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:48:55,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run32
[2019-04-01 19:48:56,064] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6273212e-16 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:48:56,064] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7341
[2019-04-01 19:48:56,080] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 84.5, 0.0, 0.0, 26.0, 25.45974517526687, 11.02065669955297, 0.0, 1.0, 39.81076634921857], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4743000.0000, 
sim time next is 4743600.0000, 
raw observation next is [-2.666666666666667, 84.33333333333334, 0.0, 0.0, 26.0, 25.47538468996359, 10.88909157355376, 0.0, 1.0, 36.5463061574708], 
processed observation next is [1.0, 0.9130434782608695, 0.38873499538319484, 0.8433333333333334, 0.0, 0.0, 1.0, 0.9250549557090844, 0.1088909157355376, 0.0, 1.0, 0.2610450439819343], 
reward next is 0.7390, 
noisyNet noise sample is [array([0.5065694], dtype=float32), 1.1435692]. 
=============================================
[2019-04-01 19:48:56,321] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5955655e-22
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:48:56,322] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6969
[2019-04-01 19:48:56,342] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 67.33333333333333, 0.0, 0.0, 26.0, 24.80363052297706, 6.272359295295028, 0.0, 1.0, 38.8243396466215], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4855200.0000, 
sim time next is 4855800.0000, 
raw observation next is [-3.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.78058411215563, 6.278412796022496, 0.0, 1.0, 38.89302558214062], 
processed observation next is [0.0, 0.17391304347826086, 0.3564173591874424, 0.6916666666666668, 0.0, 0.0, 1.0, 0.8257977303079471, 0.06278412796022496, 0.0, 1.0, 0.2778073255867187], 
reward next is 0.7222, 
noisyNet noise sample is [array([0.84061044], dtype=float32), 0.85310364]. 
=============================================
[2019-04-01 19:48:56,594] A3C_AGENT_WORKER-Thread-13 INFO:Local step 263000, global step 4209038: loss 14.9225
[2019-04-01 19:48:56,595] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 263000, global step 4209039: learning rate 0.0010
[2019-04-01 19:48:56,796] A3C_AGENT_WORKER-Thread-15 INFO:Local step 263000, global step 4209137: loss 17.1917
[2019-04-01 19:48:56,798] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 263000, global step 4209138: learning rate 0.0010
[2019-04-01 19:48:57,410] A3C_AGENT_WORKER-Thread-16 INFO:Local step 263000, global step 4209403: loss 14.7625
[2019-04-01 19:48:57,411] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 263000, global step 4209403: learning rate 0.0010
[2019-04-01 19:48:57,655] A3C_AGENT_WORKER-Thread-10 INFO:Local step 263000, global step 4209514: loss 16.1031
[2019-04-01 19:48:57,656] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 263000, global step 4209516: learning rate 0.0010
[2019-04-01 19:48:57,846] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:48:57,846] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9568
[2019-04-01 19:48:57,874] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333333, 52.33333333333334, 0.0, 0.0, 26.0, 25.36823029293472, 8.498858011901396, 0.0, 1.0, 37.86536910836862], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4828800.0000, 
sim time next is 4829400.0000, 
raw observation next is [-0.5, 53.0, 0.0, 0.0, 26.0, 25.35438232114134, 8.548269632688418, 0.0, 1.0, 40.78036267048125], 
processed observation next is [0.0, 0.9130434782608695, 0.44875346260387816, 0.53, 0.0, 0.0, 1.0, 0.9077689030201915, 0.08548269632688418, 0.0, 1.0, 0.2912883047891518], 
reward next is 0.7087, 
noisyNet noise sample is [array([-0.45466504], dtype=float32), -0.081888594]. 
=============================================
[2019-04-01 19:48:58,732] A3C_AGENT_WORKER-Thread-9 INFO:Local step 263000, global step 4210087: loss 18.7895
[2019-04-01 19:48:58,733] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 263000, global step 4210087: learning rate 0.0010
[2019-04-01 19:48:59,092] A3C_AGENT_WORKER-Thread-5 INFO:Local step 263000, global step 4210269: loss 11.8723
[2019-04-01 19:48:59,093] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 263000, global step 4210269: learning rate 0.0010
[2019-04-01 19:48:59,940] A3C_AGENT_WORKER-Thread-3 INFO:Local step 263000, global step 4210695: loss 14.1931
[2019-04-01 19:48:59,941] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 263000, global step 4210695: learning rate 0.0010
[2019-04-01 19:49:00,213] A3C_AGENT_WORKER-Thread-12 INFO:Local step 263000, global step 4210842: loss 10.1991
[2019-04-01 19:49:00,215] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 263000, global step 4210842: learning rate 0.0010
[2019-04-01 19:49:00,992] A3C_AGENT_WORKER-Thread-11 INFO:Local step 263000, global step 4211160: loss 8.8483
[2019-04-01 19:49:00,993] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 263000, global step 4211160: learning rate 0.0010
[2019-04-01 19:49:01,139] A3C_AGENT_WORKER-Thread-14 INFO:Local step 263000, global step 4211219: loss 9.1211
[2019-04-01 19:49:01,140] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 263000, global step 4211219: learning rate 0.0010
[2019-04-01 19:49:01,780] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:49:01,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4554
[2019-04-01 19:49:01,821] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 39.0, 100.5, 638.5, 26.0, 25.67349570859169, 8.826942350397578, 1.0, 1.0, 17.27103815930732], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4957200.0000, 
sim time next is 4957800.0000, 
raw observation next is [-0.6666666666666667, 37.5, 103.0, 664.6666666666667, 26.0, 25.89239727237076, 9.214316483629537, 1.0, 1.0, 16.11611104676267], 
processed observation next is [1.0, 0.391304347826087, 0.44413665743305636, 0.375, 0.3433333333333333, 0.734438305709024, 1.0, 0.9846281817672516, 0.09214316483629537, 1.0, 1.0, 0.11511507890544763], 
reward next is 0.8849, 
noisyNet noise sample is [array([0.42222455], dtype=float32), 1.9226178]. 
=============================================
[2019-04-01 19:49:02,384] A3C_AGENT_WORKER-Thread-19 INFO:Local step 263000, global step 4211889: loss 1.0966
[2019-04-01 19:49:02,385] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 263000, global step 4211889: learning rate 0.0010
[2019-04-01 19:49:02,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:49:02,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:49:02,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run32
[2019-04-01 19:49:04,103] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:49:04,104] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:49:04,107] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run32
[2019-04-01 19:49:07,864] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:49:07,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:49:07,883] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run32
[2019-04-01 19:49:08,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:49:08,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:49:08,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run32
[2019-04-01 19:49:08,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:49:08,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:49:08,486] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run32
[2019-04-01 19:49:08,643] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:49:08,643] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:49:08,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run32
[2019-04-01 19:49:09,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:49:09,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:49:09,726] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run32
[2019-04-01 19:49:10,134] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:49:10,134] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:49:10,138] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run32
[2019-04-01 19:49:10,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:49:10,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:49:10,775] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run32
[2019-04-01 19:49:11,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:49:11,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:49:11,072] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run32
[2019-04-01 19:49:11,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:49:11,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:49:11,972] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run32
[2019-04-01 19:49:12,114] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:49:12,114] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:49:12,118] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run32
[2019-04-01 19:49:13,070] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:49:13,070] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:49:13,074] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run32
[2019-04-01 19:49:23,085] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 4.1610587e-08 0.0000000e+00
 1.0000000e+00 2.3995270e-36], sum to 1.0000
[2019-04-01 19:49:23,087] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2567
[2019-04-01 19:49:23,149] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.699999999999999, 82.66666666666667, 39.33333333333334, 0.0, 26.0, 24.43633626418239, 6.730095080712677, 0.0, 1.0, 44.37283575450292], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 57000.0000, 
sim time next is 57600.0000, 
raw observation next is [6.6, 82.0, 34.0, 0.0, 26.0, 24.54236648236881, 6.886367776517396, 0.0, 1.0, 41.32812363406113], 
processed observation next is [0.0, 0.6956521739130435, 0.6454293628808865, 0.82, 0.11333333333333333, 0.0, 1.0, 0.7917666403384013, 0.06886367776517396, 0.0, 1.0, 0.29520088310043663], 
reward next is 0.7048, 
noisyNet noise sample is [array([1.5386713], dtype=float32), -1.0555608]. 
=============================================
[2019-04-01 19:49:24,699] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:49:24,699] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6921
[2019-04-01 19:49:24,724] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.233333333333334, 88.66666666666667, 0.0, 0.0, 26.0, 24.6355117081111, 6.599938685324609, 0.0, 1.0, 40.01946509579765], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 76800.0000, 
sim time next is 77400.0000, 
raw observation next is [1.05, 90.5, 0.0, 0.0, 26.0, 24.6125582864429, 6.545740668963004, 0.0, 1.0, 39.96466262371901], 
processed observation next is [0.0, 0.9130434782608695, 0.49168975069252085, 0.905, 0.0, 0.0, 1.0, 0.8017940409204145, 0.06545740668963004, 0.0, 1.0, 0.2854618758837072], 
reward next is 0.7145, 
noisyNet noise sample is [array([0.30068707], dtype=float32), 0.034435034]. 
=============================================
[2019-04-01 19:49:25,778] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:49:25,778] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5997
[2019-04-01 19:49:25,799] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.33333333333334, 0.0, 0.0, 26.0, 24.22923180500879, 5.491785611072589, 0.0, 1.0, 41.83880550220265], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 613200.0000, 
sim time next is 613800.0000, 
raw observation next is [-3.9, 80.5, 0.0, 0.0, 26.0, 24.20791572832415, 5.476432354276191, 0.0, 1.0, 41.93883062166594], 
processed observation next is [0.0, 0.08695652173913043, 0.3545706371191136, 0.805, 0.0, 0.0, 1.0, 0.7439879611891642, 0.05476432354276191, 0.0, 1.0, 0.29956307586904246], 
reward next is 0.7004, 
noisyNet noise sample is [array([-0.9056732], dtype=float32), 0.81536037]. 
=============================================
[2019-04-01 19:49:34,459] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.1859709e-32 1.4779223e-36 2.8743843e-32 7.1348463e-26
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:49:34,459] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0993
[2019-04-01 19:49:34,565] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.55, 68.5, 30.0, 386.0, 26.0, 25.47883117286318, 7.459522758382175, 1.0, 1.0, 51.53145126238974], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 289800.0000, 
sim time next is 290400.0000, 
raw observation next is [-12.46666666666667, 68.0, 40.83333333333333, 385.5, 26.0, 25.65369359055783, 7.681237024847509, 1.0, 1.0, 48.50040643053012], 
processed observation next is [1.0, 0.34782608695652173, 0.11726685133887339, 0.68, 0.1361111111111111, 0.42596685082872926, 1.0, 0.9505276557939756, 0.07681237024847509, 1.0, 1.0, 0.3464314745037866], 
reward next is 0.6536, 
noisyNet noise sample is [array([0.5835057], dtype=float32), 0.08732632]. 
=============================================
[2019-04-01 19:49:48,409] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:49:48,410] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3325
[2019-04-01 19:49:48,434] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.20070918859648, 6.041483150712343, 0.0, 1.0, 48.01009504270801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 353400.0000, 
sim time next is 354000.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.07652367748752, 6.152430817443729, 0.0, 1.0, 48.20872700981778], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 1.0, 0.5823605253553598, 0.0615243081744373, 0.0, 1.0, 0.344348050070127], 
reward next is 0.6557, 
noisyNet noise sample is [array([-1.2144916], dtype=float32), -1.3255218]. 
=============================================
[2019-04-01 19:49:48,442] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[61.093987]
 [61.02275 ]
 [60.897465]
 [60.668213]
 [60.511578]], R is [[61.15870667]
 [61.2041893 ]
 [61.25083542]
 [61.29849625]
 [61.34679413]].
[2019-04-01 19:49:49,307] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.2663856e-25 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:49:49,308] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5500
[2019-04-01 19:49:49,357] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 36.66666666666667, 17.5, 338.6666666666667, 26.0, 26.06656538947232, 9.480443527527983, 1.0, 1.0, 20.34535050721596], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 405600.0000, 
sim time next is 406200.0000, 
raw observation next is [-8.9, 36.33333333333333, 14.0, 274.3333333333334, 26.0, 25.94934270071529, 9.256259813531926, 1.0, 1.0, 19.23149579580592], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.3633333333333333, 0.04666666666666667, 0.3031307550644568, 1.0, 0.9927632429593274, 0.09256259813531927, 1.0, 1.0, 0.13736782711289944], 
reward next is 0.8626, 
noisyNet noise sample is [array([-1.194231], dtype=float32), 0.50206184]. 
=============================================
[2019-04-01 19:49:51,040] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:49:51,040] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3632
[2019-04-01 19:49:51,092] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 30.33333333333334, 120.6666666666667, 0.0, 26.0, 24.91464716876734, 6.388718351986178, 1.0, 1.0, 40.05571942340075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 480000.0000, 
sim time next is 480600.0000, 
raw observation next is [-0.8999999999999999, 31.5, 119.0, 0.0, 26.0, 24.95247485899312, 6.435243757282197, 1.0, 1.0, 33.70869435979733], 
processed observation next is [1.0, 0.5652173913043478, 0.43767313019390586, 0.315, 0.39666666666666667, 0.0, 1.0, 0.8503535512847316, 0.06435243757282197, 1.0, 1.0, 0.24077638828426662], 
reward next is 0.7592, 
noisyNet noise sample is [array([0.41577208], dtype=float32), -1.3637996]. 
=============================================
[2019-04-01 19:49:55,073] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:49:55,074] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8281
[2019-04-01 19:49:55,115] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.8, 25.5, 124.3333333333333, 0.0, 26.0, 25.14867840132501, 6.103134963282304, 1.0, 1.0, 27.37946673660873], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 474600.0000, 
sim time next is 475200.0000, 
raw observation next is [-1.7, 25.0, 125.5, 0.0, 26.0, 25.16297509674453, 6.134201424156238, 1.0, 1.0, 26.38906632439607], 
processed observation next is [1.0, 0.5217391304347826, 0.4155124653739613, 0.25, 0.41833333333333333, 0.0, 1.0, 0.8804250138206469, 0.06134201424156238, 1.0, 1.0, 0.18849333088854334], 
reward next is 0.8115, 
noisyNet noise sample is [array([0.20274848], dtype=float32), 1.7288897]. 
=============================================
[2019-04-01 19:50:09,128] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.7317575e-30 0.0000000e+00
 1.0000000e+00 5.9484739e-37], sum to 1.0000
[2019-04-01 19:50:09,128] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3731
[2019-04-01 19:50:09,176] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.23333333333333, 66.66666666666666, 241.8333333333333, 232.0, 26.0, 27.32430821655421, 22.29609456610664, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1078800.0000, 
sim time next is 1079400.0000, 
raw observation next is [16.41666666666667, 65.83333333333334, 229.6666666666667, 249.0, 26.0, 26.82133286503655, 22.08734807850503, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.9173591874422903, 0.6583333333333334, 0.7655555555555558, 0.2751381215469613, 1.0, 1.1173332664337927, 0.2208734807850503, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.94975865], dtype=float32), -0.34232947]. 
=============================================
[2019-04-01 19:50:11,382] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:50:11,382] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9138
[2019-04-01 19:50:11,412] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.383333333333333, 62.5, 0.0, 0.0, 26.0, 24.95588621413022, 6.93741170803595, 0.0, 1.0, 45.14142414552416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 673800.0000, 
sim time next is 674400.0000, 
raw observation next is [-2.466666666666667, 63.0, 0.0, 0.0, 26.0, 24.92400213104834, 6.878917397378046, 0.0, 1.0, 47.52499160782565], 
processed observation next is [0.0, 0.8260869565217391, 0.39427516158818104, 0.63, 0.0, 0.0, 1.0, 0.8462860187211915, 0.06878917397378047, 0.0, 1.0, 0.3394642257701832], 
reward next is 0.6605, 
noisyNet noise sample is [array([0.24500184], dtype=float32), -0.2282191]. 
=============================================
[2019-04-01 19:50:13,355] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:50:13,356] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9256
[2019-04-01 19:50:13,487] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.1, 79.0, 85.66666666666667, 0.0, 26.0, 25.13724511213966, 6.979723065167974, 1.0, 1.0, 36.71329928049699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 826800.0000, 
sim time next is 827400.0000, 
raw observation next is [-4.0, 79.0, 80.33333333333333, 0.0, 26.0, 24.60444767998883, 7.744608719594782, 1.0, 1.0, 88.72472614411723], 
processed observation next is [1.0, 0.5652173913043478, 0.3518005540166205, 0.79, 0.2677777777777778, 0.0, 1.0, 0.8006353828555471, 0.07744608719594782, 1.0, 1.0, 0.6337480438865516], 
reward next is 0.3663, 
noisyNet noise sample is [array([2.0234966], dtype=float32), 0.45396245]. 
=============================================
[2019-04-01 19:50:18,996] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:50:18,999] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7545
[2019-04-01 19:50:19,019] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 74.0, 0.0, 0.0, 26.0, 23.85227388680589, 5.307666946419975, 0.0, 1.0, 40.78230505003217], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 788400.0000, 
sim time next is 789000.0000, 
raw observation next is [-7.716666666666667, 74.16666666666667, 0.0, 0.0, 26.0, 23.90814076361379, 5.300257498331701, 0.0, 1.0, 40.69467868999033], 
processed observation next is [1.0, 0.13043478260869565, 0.24884579870729456, 0.7416666666666667, 0.0, 0.0, 1.0, 0.7011629662305415, 0.05300257498331701, 0.0, 1.0, 0.2906762763570738], 
reward next is 0.7093, 
noisyNet noise sample is [array([2.0135329], dtype=float32), 0.3539798]. 
=============================================
[2019-04-01 19:50:19,040] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[66.81439]
 [66.81077]
 [66.83357]
 [66.80769]
 [66.80937]], R is [[66.9368515 ]
 [66.97618103]
 [67.01467133]
 [67.05242157]
 [67.08948517]].
[2019-04-01 19:50:20,184] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:50:20,184] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6827
[2019-04-01 19:50:20,222] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 80.66666666666667, 41.66666666666666, 0.0, 26.0, 25.45215226726436, 7.232080211211607, 1.0, 1.0, 20.08932432967373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 897000.0000, 
sim time next is 897600.0000, 
raw observation next is [1.1, 81.33333333333334, 44.83333333333333, 0.0, 26.0, 25.49688557424611, 7.171404358560406, 1.0, 1.0, 19.07823272876562], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8133333333333335, 0.14944444444444444, 0.0, 1.0, 0.9281265106065872, 0.07171404358560406, 1.0, 1.0, 0.13627309091975442], 
reward next is 0.8637, 
noisyNet noise sample is [array([1.2637806], dtype=float32), 0.72499114]. 
=============================================
[2019-04-01 19:50:23,862] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0646951e-27 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:50:23,863] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6766
[2019-04-01 19:50:23,907] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 84.0, 0.0, 0.0, 26.0, 26.14720750125685, 9.204131452363724, 1.0, 1.0, 27.61046300686975], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 840600.0000, 
sim time next is 841200.0000, 
raw observation next is [-3.899999999999999, 83.33333333333334, 0.0, 0.0, 26.0, 25.4115568767942, 7.882364676487259, 1.0, 1.0, 25.83557279530625], 
processed observation next is [1.0, 0.7391304347826086, 0.35457063711911363, 0.8333333333333335, 0.0, 0.0, 1.0, 0.9159366966848859, 0.07882364676487259, 1.0, 1.0, 0.18453980568075892], 
reward next is 0.8155, 
noisyNet noise sample is [array([1.7972432], dtype=float32), 0.43306378]. 
=============================================
[2019-04-01 19:50:27,855] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:50:27,858] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9316
[2019-04-01 19:50:27,880] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 84.0, 77.0, 0.0, 26.0, 25.54731424073674, 7.249907769614325, 1.0, 1.0, 13.55956766890328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 901800.0000, 
sim time next is 902400.0000, 
raw observation next is [1.1, 84.0, 80.33333333333334, 0.0, 26.0, 25.48154023328412, 7.208442531317822, 1.0, 1.0, 12.98602131650495], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.26777777777777784, 0.0, 1.0, 0.9259343190405888, 0.07208442531317823, 1.0, 1.0, 0.09275729511789249], 
reward next is 0.9072, 
noisyNet noise sample is [array([0.6381858], dtype=float32), -1.0982491]. 
=============================================
[2019-04-01 19:50:34,505] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:50:34,509] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9098
[2019-04-01 19:50:34,522] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 26.00588141331489, 13.40775603593007, 0.0, 1.0, 16.98176372552242], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1032000.0000, 
sim time next is 1032600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.94819630476602, 13.28391079253774, 0.0, 1.0, 16.114662718554], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 1.0, 0.9925994721094317, 0.1328391079253774, 0.0, 1.0, 0.11510473370395716], 
reward next is 0.8849, 
noisyNet noise sample is [array([0.94614613], dtype=float32), 0.41085377]. 
=============================================
[2019-04-01 19:50:35,620] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:50:35,623] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8748
[2019-04-01 19:50:35,629] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.45, 60.5, 181.0, 317.0, 26.0, 26.9408442718622, 20.92961578118222, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1081800.0000, 
sim time next is 1082400.0000, 
raw observation next is [17.73333333333333, 59.0, 179.3333333333333, 264.1666666666667, 26.0, 27.08538997949144, 21.29239625441306, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.9538319482917822, 0.59, 0.5977777777777776, 0.2918968692449356, 1.0, 1.1550557113559197, 0.2129239625441306, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.51779234], dtype=float32), -1.2705936]. 
=============================================
[2019-04-01 19:50:38,360] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:50:38,361] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1581
[2019-04-01 19:50:38,377] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.34683036100244, 10.89033282159252, 0.0, 1.0, 41.65594112235542], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1379400.0000, 
sim time next is 1380000.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.34138069102156, 10.81715387145314, 0.0, 1.0, 41.19344901442435], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.9059115272887942, 0.1081715387145314, 0.0, 1.0, 0.2942389215316025], 
reward next is 0.7058, 
noisyNet noise sample is [array([0.98566544], dtype=float32), 0.64759207]. 
=============================================
[2019-04-01 19:50:38,394] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.083664]
 [81.84968 ]
 [81.80024 ]
 [81.64042 ]
 [80.9363  ]], R is [[80.1209259 ]
 [80.02217865]
 [79.92481232]
 [79.83892822]
 [79.76398468]].
[2019-04-01 19:50:40,140] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:50:40,142] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3374
[2019-04-01 19:50:40,147] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.1, 82.5, 0.0, 0.0, 26.0, 23.99731730409502, 6.732245967127009, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1216200.0000, 
sim time next is 1216800.0000, 
raw observation next is [16.1, 83.0, 0.0, 0.0, 26.0, 23.97656406606567, 6.69111504445272, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9085872576177286, 0.83, 0.0, 0.0, 1.0, 0.7109377237236671, 0.0669111504445272, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20615898], dtype=float32), -1.795215]. 
=============================================
[2019-04-01 19:50:40,168] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:50:40,173] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4760
[2019-04-01 19:50:40,221] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.7, 100.0, 78.0, 0.0, 26.0, 24.00129032563694, 8.22011821467167, 0.0, 1.0, 40.1012406768326], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1247400.0000, 
sim time next is 1248000.0000, 
raw observation next is [14.6, 100.0, 80.83333333333333, 0.0, 26.0, 24.41676166823187, 8.978143419412644, 0.0, 1.0, 29.38442038582294], 
processed observation next is [0.0, 0.43478260869565216, 0.8670360110803325, 1.0, 0.26944444444444443, 0.0, 1.0, 0.7738230954616958, 0.08978143419412644, 0.0, 1.0, 0.20988871704159243], 
reward next is 0.7901, 
noisyNet noise sample is [array([-0.41523913], dtype=float32), 1.0526849]. 
=============================================
[2019-04-01 19:50:40,235] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[88.50475 ]
 [89.08335 ]
 [88.56609 ]
 [86.994385]
 [87.0239  ]], R is [[88.05024719]
 [87.88330841]
 [87.65708923]
 [87.25482941]
 [87.38227844]].
[2019-04-01 19:50:42,398] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:50:42,399] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8776
[2019-04-01 19:50:42,415] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 84.0, 0.0, 0.0, 26.0, 25.6747906423779, 13.43877856194793, 0.0, 1.0, 30.37887785868176], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1639800.0000, 
sim time next is 1640400.0000, 
raw observation next is [7.2, 84.66666666666666, 0.0, 0.0, 26.0, 25.72272542540342, 13.51189973348908, 0.0, 1.0, 25.75351725341695], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.8466666666666666, 0.0, 0.0, 1.0, 0.9603893464862031, 0.1351189973348908, 0.0, 1.0, 0.1839536946672639], 
reward next is 0.8160, 
noisyNet noise sample is [array([0.5166365], dtype=float32), 1.2406476]. 
=============================================
[2019-04-01 19:50:46,095] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 6.640471e-26 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 19:50:46,098] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5347
[2019-04-01 19:50:46,132] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 82.66666666666666, 0.0, 26.0, 25.81386095972889, 11.57837645618012, 1.0, 1.0, 10.13241260737102], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1347000.0000, 
sim time next is 1347600.0000, 
raw observation next is [1.1, 92.0, 76.83333333333334, 0.0, 26.0, 25.84469275178829, 11.58211030967314, 1.0, 1.0, 10.02285216900427], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.2561111111111111, 0.0, 1.0, 0.9778132502554701, 0.11582110309673141, 1.0, 1.0, 0.07159180120717336], 
reward next is 0.2956, 
noisyNet noise sample is [array([0.76592726], dtype=float32), -1.2244239]. 
=============================================
[2019-04-01 19:50:48,820] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.506162e-34
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 19:50:48,820] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4251
[2019-04-01 19:50:48,852] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.37250020375475, 12.90700704979629, 0.0, 1.0, 44.67036732864521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1369800.0000, 
sim time next is 1370400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.51832384914233, 13.10570598032592, 0.0, 1.0, 37.00285507908972], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 1.0, 0.9311891213060471, 0.1310570598032592, 0.0, 1.0, 0.26430610770778373], 
reward next is 0.7357, 
noisyNet noise sample is [array([0.37930292], dtype=float32), 1.1148802]. 
=============================================
[2019-04-01 19:51:08,231] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:08,233] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8268
[2019-04-01 19:51:08,262] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.533333333333333, 78.33333333333334, 0.0, 0.0, 26.0, 23.4135890120918, 5.318270739137127, 0.0, 1.0, 46.42420375510963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1838400.0000, 
sim time next is 1839000.0000, 
raw observation next is [-6.616666666666667, 78.16666666666666, 0.0, 0.0, 26.0, 23.37937606314491, 5.339191038770586, 0.0, 1.0, 46.45347999366135], 
processed observation next is [0.0, 0.2608695652173913, 0.2793167128347184, 0.7816666666666666, 0.0, 0.0, 1.0, 0.6256251518778443, 0.05339191038770586, 0.0, 1.0, 0.33181057138329534], 
reward next is 0.6682, 
noisyNet noise sample is [array([-0.3931965], dtype=float32), 1.9530029]. 
=============================================
[2019-04-01 19:51:08,264] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[67.89801]
 [67.97682]
 [68.03794]
 [68.07291]
 [68.08478]], R is [[67.80558777]
 [67.79592896]
 [67.78652191]
 [67.77728271]
 [67.7681427 ]].
[2019-04-01 19:51:10,368] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:10,368] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7865
[2019-04-01 19:51:10,382] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.866666666666667, 81.33333333333333, 0.0, 0.0, 26.0, 24.20203403016455, 5.586978730468687, 0.0, 1.0, 45.63173642276814], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1820400.0000, 
sim time next is 1821000.0000, 
raw observation next is [-5.933333333333333, 82.16666666666667, 0.0, 0.0, 26.0, 24.16756821507332, 5.546757239364588, 0.0, 1.0, 45.70779340075531], 
processed observation next is [0.0, 0.043478260869565216, 0.29824561403508776, 0.8216666666666668, 0.0, 0.0, 1.0, 0.7382240307247601, 0.05546757239364588, 0.0, 1.0, 0.3264842385768236], 
reward next is 0.6735, 
noisyNet noise sample is [array([-0.22146526], dtype=float32), -0.10621535]. 
=============================================
[2019-04-01 19:51:10,398] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[62.517643]
 [62.413013]
 [62.265377]
 [62.030075]
 [61.74659 ]], R is [[62.67249298]
 [62.71982956]
 [62.76716614]
 [62.81446838]
 [62.86183929]].
[2019-04-01 19:51:11,130] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:11,131] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8759
[2019-04-01 19:51:11,168] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 83.33333333333334, 0.0, 0.0, 26.0, 24.72178658243966, 6.5837368040958, 0.0, 1.0, 45.05670663225055], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1809600.0000, 
sim time next is 1810200.0000, 
raw observation next is [-5.0, 82.66666666666667, 0.0, 0.0, 26.0, 24.69390471523669, 6.511357036658161, 0.0, 1.0, 44.99681273534938], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.8266666666666667, 0.0, 0.0, 1.0, 0.8134149593195273, 0.06511357036658161, 0.0, 1.0, 0.32140580525249557], 
reward next is 0.6786, 
noisyNet noise sample is [array([-0.8626937], dtype=float32), 0.6288584]. 
=============================================
[2019-04-01 19:51:13,562] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:13,564] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5052
[2019-04-01 19:51:13,592] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.13415778336147, 7.312728638163395, 0.0, 1.0, 43.01994973657933], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2404800.0000, 
sim time next is 2405400.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.13120626330604, 7.294915685691443, 0.0, 1.0, 42.3161480890188], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 1.0, 0.8758866090437201, 0.07294915685691443, 0.0, 1.0, 0.3022582006358486], 
reward next is 0.6977, 
noisyNet noise sample is [array([0.9741284], dtype=float32), -0.96043545]. 
=============================================
[2019-04-01 19:51:16,141] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:16,145] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8080
[2019-04-01 19:51:16,203] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.366666666666667, 83.66666666666667, 76.33333333333334, 461.0, 26.0, 25.46422329542589, 7.053998499340255, 1.0, 1.0, 38.66975269540785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1934400.0000, 
sim time next is 1935000.0000, 
raw observation next is [-8.1, 82.5, 85.0, 549.0, 26.0, 25.56223928358323, 7.255717913744256, 1.0, 1.0, 38.00834841028842], 
processed observation next is [1.0, 0.391304347826087, 0.23822714681440446, 0.825, 0.2833333333333333, 0.6066298342541436, 1.0, 0.9374627547976045, 0.07255717913744256, 1.0, 1.0, 0.2714882029306316], 
reward next is 0.7285, 
noisyNet noise sample is [array([2.4856474], dtype=float32), 0.7679015]. 
=============================================
[2019-04-01 19:51:16,238] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[64.680244]
 [63.400143]
 [62.12498 ]
 [61.039482]
 [60.074894]], R is [[66.15148926]
 [66.21376038]
 [66.27816772]
 [66.36682129]
 [66.44873047]].
[2019-04-01 19:51:18,614] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:18,615] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9661
[2019-04-01 19:51:18,660] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.733333333333334, 80.66666666666667, 0.0, 0.0, 26.0, 23.60148244839025, 5.450394653478601, 0.0, 1.0, 44.6908595383882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1917600.0000, 
sim time next is 1918200.0000, 
raw observation next is [-8.816666666666666, 81.33333333333334, 0.0, 0.0, 26.0, 23.58494791329105, 5.443869368374846, 0.0, 1.0, 44.65348272791812], 
processed observation next is [1.0, 0.17391304347826086, 0.21837488457987075, 0.8133333333333335, 0.0, 0.0, 1.0, 0.6549925590415785, 0.05443869368374846, 0.0, 1.0, 0.31895344805655795], 
reward next is 0.6810, 
noisyNet noise sample is [array([1.6892086], dtype=float32), 2.141357]. 
=============================================
[2019-04-01 19:51:21,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:21,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8289
[2019-04-01 19:51:21,798] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.166666666666666, 76.33333333333334, 181.1666666666667, 198.3333333333333, 26.0, 25.8490448988046, 7.865669487190028, 1.0, 1.0, 23.22727971900423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1939200.0000, 
sim time next is 1939800.0000, 
raw observation next is [-5.883333333333333, 75.66666666666666, 191.3333333333333, 160.6666666666667, 26.0, 25.8006270093649, 7.796322959740692, 1.0, 1.0, 22.08151982010872], 
processed observation next is [1.0, 0.43478260869565216, 0.2996306555863343, 0.7566666666666666, 0.6377777777777777, 0.1775322283609577, 1.0, 0.9715181441949855, 0.07796322959740692, 1.0, 1.0, 0.15772514157220516], 
reward next is 0.8423, 
noisyNet noise sample is [array([-0.58268017], dtype=float32), 1.8262686]. 
=============================================
[2019-04-01 19:51:22,403] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:22,403] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0085
[2019-04-01 19:51:22,434] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.283333333333333, 37.5, 222.6666666666667, 38.33333333333333, 26.0, 25.81201369254179, 7.808764279153318, 1.0, 1.0, 7.847254642168688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2549400.0000, 
sim time next is 2550000.0000, 
raw observation next is [1.466666666666667, 36.0, 220.3333333333333, 30.16666666666666, 26.0, 25.81611659610793, 7.752310940082192, 1.0, 1.0, 7.834092031642649], 
processed observation next is [1.0, 0.5217391304347826, 0.5032317636195753, 0.36, 0.7344444444444442, 0.033333333333333326, 1.0, 0.9737309423011327, 0.07752310940082192, 1.0, 1.0, 0.05595780022601892], 
reward next is 0.9440, 
noisyNet noise sample is [array([0.46180144], dtype=float32), 0.47096774]. 
=============================================
[2019-04-01 19:51:22,447] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[79.33682 ]
 [79.465164]
 [79.524475]
 [79.47768 ]
 [79.36847 ]], R is [[79.33140564]
 [79.48204041]
 [79.62697601]
 [79.76828766]
 [79.90396118]].
[2019-04-01 19:51:24,370] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:24,371] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5831
[2019-04-01 19:51:24,424] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.25530897903938, 5.423909017618651, 0.0, 1.0, 40.74399881465406], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2001600.0000, 
sim time next is 2002200.0000, 
raw observation next is [-5.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.17853248320431, 5.378039661704065, 0.0, 1.0, 40.7726061166332], 
processed observation next is [1.0, 0.17391304347826086, 0.30470914127423826, 0.8366666666666667, 0.0, 0.0, 1.0, 0.739790354743473, 0.05378039661704065, 0.0, 1.0, 0.2912329008330943], 
reward next is 0.7088, 
noisyNet noise sample is [array([2.4352546], dtype=float32), -0.40479854]. 
=============================================
[2019-04-01 19:51:26,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:26,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8899
[2019-04-01 19:51:26,946] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 88.5, 0.0, 0.0, 26.0, 24.74947225152426, 6.63622835127168, 0.0, 1.0, 42.09864680170598], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2079000.0000, 
sim time next is 2079600.0000, 
raw observation next is [-4.5, 87.66666666666666, 0.0, 0.0, 26.0, 24.76784698373346, 6.551175229230512, 0.0, 1.0, 42.08438155043091], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.8766666666666666, 0.0, 0.0, 1.0, 0.8239781405333514, 0.06551175229230512, 0.0, 1.0, 0.3006027253602208], 
reward next is 0.6994, 
noisyNet noise sample is [array([-0.27946565], dtype=float32), 1.3888124]. 
=============================================
[2019-04-01 19:51:27,325] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:27,325] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7408
[2019-04-01 19:51:27,337] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.3, 88.5, 0.0, 0.0, 26.0, 24.56939994120797, 5.971639277943797, 0.0, 1.0, 42.7727441519528], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2086200.0000, 
sim time next is 2086800.0000, 
raw observation next is [-5.4, 89.33333333333334, 0.0, 0.0, 26.0, 24.49788023603988, 5.881409627279832, 0.0, 1.0, 42.85142592214629], 
processed observation next is [1.0, 0.13043478260869565, 0.31301939058171746, 0.8933333333333334, 0.0, 0.0, 1.0, 0.7854114622914113, 0.05881409627279832, 0.0, 1.0, 0.30608161372961634], 
reward next is 0.6939, 
noisyNet noise sample is [array([-0.432043], dtype=float32), -0.57477474]. 
=============================================
[2019-04-01 19:51:28,237] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:28,240] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4069
[2019-04-01 19:51:28,272] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 86.0, 0.0, 0.0, 26.0, 25.5861671824291, 9.527159593901068, 0.0, 1.0, 30.89377432486877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2061600.0000, 
sim time next is 2062200.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 25.43181248856331, 9.348437728963924, 0.0, 1.0, 41.77798952974727], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 1.0, 0.9188303555090442, 0.09348437728963925, 0.0, 1.0, 0.29841421092676623], 
reward next is 0.7016, 
noisyNet noise sample is [array([1.4335538], dtype=float32), 1.5389352]. 
=============================================
[2019-04-01 19:51:38,615] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:38,618] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1900
[2019-04-01 19:51:38,684] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.916666666666667, 71.0, 112.0, 150.3333333333333, 26.0, 25.7517123297804, 8.081612777745976, 1.0, 1.0, 19.74099847074931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2196600.0000, 
sim time next is 2197200.0000, 
raw observation next is [-4.833333333333334, 71.0, 114.5, 75.16666666666664, 26.0, 25.73100860025067, 7.943714432818527, 1.0, 1.0, 19.02827802441716], 
processed observation next is [1.0, 0.43478260869565216, 0.32871652816251157, 0.71, 0.38166666666666665, 0.08305709023941066, 1.0, 0.9615726571786674, 0.07943714432818527, 1.0, 1.0, 0.1359162716029797], 
reward next is 0.8641, 
noisyNet noise sample is [array([0.8914669], dtype=float32), -1.3730336]. 
=============================================
[2019-04-01 19:51:46,657] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:46,657] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5509
[2019-04-01 19:51:46,686] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.65, 89.0, 0.0, 0.0, 26.0, 23.9532711670566, 5.321821366266143, 0.0, 1.0, 42.93820298454101], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2262600.0000, 
sim time next is 2263200.0000, 
raw observation next is [-8.733333333333334, 89.66666666666667, 0.0, 0.0, 26.0, 23.91681357514834, 5.298676397116135, 0.0, 1.0, 42.90877306982507], 
processed observation next is [1.0, 0.17391304347826086, 0.22068328716528163, 0.8966666666666667, 0.0, 0.0, 1.0, 0.7024019393069055, 0.05298676397116135, 0.0, 1.0, 0.3064912362130362], 
reward next is 0.6935, 
noisyNet noise sample is [array([-1.2293638], dtype=float32), -0.3375875]. 
=============================================
[2019-04-01 19:51:53,703] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:53,704] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8347
[2019-04-01 19:51:53,739] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.1, 47.16666666666666, 0.0, 0.0, 26.0, 24.4585103065626, 5.577946379818147, 0.0, 1.0, 42.70301901666823], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2422200.0000, 
sim time next is 2422800.0000, 
raw observation next is [-6.2, 48.0, 0.0, 0.0, 26.0, 24.3996848177861, 5.521431479428554, 0.0, 1.0, 42.77177483452567], 
processed observation next is [0.0, 0.043478260869565216, 0.2908587257617729, 0.48, 0.0, 0.0, 1.0, 0.7713835453980141, 0.05521431479428554, 0.0, 1.0, 0.3055126773894691], 
reward next is 0.6945, 
noisyNet noise sample is [array([0.90130717], dtype=float32), 0.31398886]. 
=============================================
[2019-04-01 19:51:55,961] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:55,963] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7138
[2019-04-01 19:51:55,991] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.466666666666667, 50.16666666666667, 135.0, 37.0, 26.0, 25.89102997866988, 7.529755390186428, 1.0, 1.0, 16.92138104771809], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2541000.0000, 
sim time next is 2541600.0000, 
raw observation next is [-1.2, 49.0, 134.5, 39.0, 26.0, 25.91989454753789, 7.577035864616557, 1.0, 1.0, 16.09896168222033], 
processed observation next is [1.0, 0.43478260869565216, 0.42936288088642666, 0.49, 0.4483333333333333, 0.0430939226519337, 1.0, 0.9885563639339843, 0.07577035864616558, 1.0, 1.0, 0.11499258344443093], 
reward next is 0.8850, 
noisyNet noise sample is [array([-1.6807504], dtype=float32), -0.21045382]. 
=============================================
[2019-04-01 19:51:59,055] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:59,057] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4176
[2019-04-01 19:51:59,067] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.016666666666667, 31.5, 202.3333333333333, 175.3333333333333, 26.0, 25.70777741356795, 7.60303246224704, 1.0, 1.0, 7.616929356796647], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2551800.0000, 
sim time next is 2552400.0000, 
raw observation next is [2.2, 30.0, 194.5, 252.0, 26.0, 25.63700699775193, 7.443961651604314, 1.0, 1.0, 7.357565278122767], 
processed observation next is [1.0, 0.5652173913043478, 0.5235457063711911, 0.3, 0.6483333333333333, 0.27845303867403315, 1.0, 0.9481438568217041, 0.07443961651604314, 1.0, 1.0, 0.0525540377008769], 
reward next is 0.9474, 
noisyNet noise sample is [array([-0.9132926], dtype=float32), 0.58997893]. 
=============================================
[2019-04-01 19:51:59,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:59,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7633
[2019-04-01 19:51:59,119] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 47.33333333333333, 0.0, 0.0, 26.0, 25.00188745836813, 5.845291107549717, 0.0, 1.0, 37.99383199668102], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2518800.0000, 
sim time next is 2519400.0000, 
raw observation next is [-1.7, 48.16666666666667, 0.0, 0.0, 26.0, 24.95969379846615, 5.807020926597043, 0.0, 1.0, 37.99503567448993], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4816666666666667, 0.0, 0.0, 1.0, 0.8513848283523073, 0.05807020926597043, 0.0, 1.0, 0.2713931119606423], 
reward next is 0.7286, 
noisyNet noise sample is [array([-0.05340201], dtype=float32), 1.9647664]. 
=============================================
[2019-04-01 19:51:59,324] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:51:59,324] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4095
[2019-04-01 19:51:59,342] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.283333333333333, 82.16666666666667, 0.0, 0.0, 26.0, 24.21291164644663, 5.376083464690434, 0.0, 1.0, 42.63402221399385], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2610600.0000, 
sim time next is 2611200.0000, 
raw observation next is [-6.366666666666667, 81.33333333333334, 0.0, 0.0, 26.0, 24.12946908166685, 5.332556346366239, 0.0, 1.0, 42.78150999962698], 
processed observation next is [1.0, 0.21739130434782608, 0.28624192059095105, 0.8133333333333335, 0.0, 0.0, 1.0, 0.7327812973809787, 0.05332556346366239, 0.0, 1.0, 0.3055822142830498], 
reward next is 0.6944, 
noisyNet noise sample is [array([-0.18053627], dtype=float32), 0.38016918]. 
=============================================
[2019-04-01 19:52:11,369] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:52:11,371] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7766
[2019-04-01 19:52:11,393] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.24963700348388, 7.255228016014179, 0.0, 1.0, 39.16544877548515], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3118200.0000, 
sim time next is 3118800.0000, 
raw observation next is [1.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.275158837879, 7.396446033844001, 0.0, 1.0, 38.28080870105615], 
processed observation next is [1.0, 0.08695652173913043, 0.4995383194829178, 1.0, 0.0, 0.0, 1.0, 0.896451262554143, 0.07396446033844001, 0.0, 1.0, 0.2734343478646868], 
reward next is 0.7266, 
noisyNet noise sample is [array([-0.40332368], dtype=float32), 1.1229774]. 
=============================================
[2019-04-01 19:52:12,120] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.09154510e-03
 0.00000000e+00 1.07044385e-20 9.98908401e-01], sum to 1.0000
[2019-04-01 19:52:12,122] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3248
[2019-04-01 19:52:12,249] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666667, 60.66666666666666, 112.3333333333333, 797.1666666666667, 26.0, 25.23814600017311, 8.637199104470243, 1.0, 1.0, 41.01402013065551], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2724000.0000, 
sim time next is 2724600.0000, 
raw observation next is [-6.333333333333333, 59.83333333333334, 111.6666666666667, 795.3333333333334, 26.0, 24.51468113165655, 10.34314392967458, 1.0, 1.0, 96.18836883991581], 
processed observation next is [1.0, 0.5217391304347826, 0.28716528162511545, 0.5983333333333334, 0.37222222222222234, 0.8788213627992634, 1.0, 0.7878115902366497, 0.10343143929674581, 1.0, 1.0, 0.6870597774279701], 
reward next is 0.1757, 
noisyNet noise sample is [array([0.16575776], dtype=float32), 1.2545475]. 
=============================================
[2019-04-01 19:52:13,679] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00000000e+00 0.00000000e+00 0.00000000e+00 3.14749285e-23
 0.00000000e+00 1.00000000e+00 1.18599165e-35], sum to 1.0000
[2019-04-01 19:52:13,680] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0174
[2019-04-01 19:52:13,704] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 64.0, 112.5, 790.0, 26.0, 25.86812243933579, 9.9026308328894, 1.0, 1.0, 12.32540157900202], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2721600.0000, 
sim time next is 2722200.0000, 
raw observation next is [-7.666666666666667, 63.16666666666667, 112.6666666666667, 793.0, 26.0, 25.86255481823002, 9.970919076075864, 1.0, 1.0, 11.63797073194154], 
processed observation next is [1.0, 0.5217391304347826, 0.2502308402585411, 0.6316666666666667, 0.37555555555555564, 0.876243093922652, 1.0, 0.98036497403286, 0.09970919076075864, 1.0, 1.0, 0.083128362371011], 
reward next is 0.9169, 
noisyNet noise sample is [array([0.19646853], dtype=float32), 1.0645496]. 
=============================================
[2019-04-01 19:52:15,563] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:52:15,564] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1482
[2019-04-01 19:52:15,578] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.68557196403264, 6.243269257969547, 0.0, 1.0, 40.79320753853023], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2773800.0000, 
sim time next is 2774400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.67875707997937, 6.234828947214979, 0.0, 1.0, 40.6606792594288], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 1.0, 0.8112510114256243, 0.062348289472149786, 0.0, 1.0, 0.29043342328163424], 
reward next is 0.7096, 
noisyNet noise sample is [array([-1.6651546], dtype=float32), 0.5765767]. 
=============================================
[2019-04-01 19:52:21,949] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:52:21,949] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5770
[2019-04-01 19:52:21,958] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 100.0, 164.0, 0.0, 26.0, 25.39490846830721, 7.804191725989128, 1.0, 1.0, 11.66632784089982], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2899800.0000, 
sim time next is 2900400.0000, 
raw observation next is [2.0, 100.0, 151.6666666666667, 0.0, 26.0, 25.43491124597627, 7.846711494673992, 1.0, 1.0, 11.62491112258695], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.5055555555555558, 0.0, 1.0, 0.9192730351394671, 0.07846711494673993, 1.0, 1.0, 0.08303507944704965], 
reward next is 0.9170, 
noisyNet noise sample is [array([0.41599074], dtype=float32), 0.0309615]. 
=============================================
[2019-04-01 19:52:25,664] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:52:25,664] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2370
[2019-04-01 19:52:25,691] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 23.59121941090362, 5.27266237643664, 0.0, 1.0, 59.5959342712011], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2964600.0000, 
sim time next is 2965200.0000, 
raw observation next is [-4.0, 77.0, 14.0, 15.66666666666666, 26.0, 23.54709248217641, 5.289669335147079, 0.0, 1.0, 59.58910930836151], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.77, 0.04666666666666667, 0.017311233885819514, 1.0, 0.6495846403109157, 0.05289669335147079, 0.0, 1.0, 0.42563649505972506], 
reward next is 0.5744, 
noisyNet noise sample is [array([-1.1885233], dtype=float32), -1.7765402]. 
=============================================
[2019-04-01 19:52:27,215] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:52:27,215] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4046
[2019-04-01 19:52:27,244] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.2334740039833, 7.551293281428677, 0.0, 1.0, 40.87985994986863], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3723600.0000, 
sim time next is 3724200.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.25934933365996, 7.543535256241856, 0.0, 1.0, 40.67454206440024], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 1.0, 0.8941927619514226, 0.07543535256241855, 0.0, 1.0, 0.29053244331714456], 
reward next is 0.7095, 
noisyNet noise sample is [array([-1.4131923], dtype=float32), -0.22840458]. 
=============================================
[2019-04-01 19:52:30,455] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 3.0905642e-29], sum to 1.0000
[2019-04-01 19:52:30,457] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8514
[2019-04-01 19:52:30,479] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 100.0, 0.0, 0.0, 26.0, 25.30431014077307, 7.406255160927128, 0.0, 1.0, 38.92849874286711], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3112200.0000, 
sim time next is 3112800.0000, 
raw observation next is [0.6666666666666666, 100.0, 0.0, 0.0, 26.0, 25.30037825773036, 7.380581566308699, 0.0, 1.0, 38.75478545662079], 
processed observation next is [1.0, 0.0, 0.4810710987996307, 1.0, 0.0, 0.0, 1.0, 0.9000540368186231, 0.07380581566308698, 0.0, 1.0, 0.27681989611871993], 
reward next is 0.7232, 
noisyNet noise sample is [array([-0.2337554], dtype=float32), -1.1765281]. 
=============================================
[2019-04-01 19:52:31,606] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:52:31,606] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4466
[2019-04-01 19:52:31,623] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.58542233775463, 6.064398869411932, 0.0, 1.0, 37.69740079267832], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3028200.0000, 
sim time next is 3028800.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.60273644919032, 6.027497935536402, 0.0, 1.0, 37.64888877220123], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 1.0, 0.8003909213129029, 0.060274979355364015, 0.0, 1.0, 0.26892063408715167], 
reward next is 0.7311, 
noisyNet noise sample is [array([1.6305649], dtype=float32), 0.016027987]. 
=============================================
[2019-04-01 19:52:32,955] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:52:32,977] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7001
[2019-04-01 19:52:32,992] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 68.0, 0.0, 0.0, 26.0, 25.18895737480962, 8.387978463150482, 0.0, 1.0, 40.24027835131648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3551400.0000, 
sim time next is 3552000.0000, 
raw observation next is [-3.0, 67.0, 0.0, 0.0, 26.0, 25.1532550726981, 8.261644632902602, 0.0, 1.0, 40.28057111387252], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.67, 0.0, 0.0, 1.0, 0.8790364389568717, 0.08261644632902602, 0.0, 1.0, 0.28771836509908943], 
reward next is 0.7123, 
noisyNet noise sample is [array([0.09349109], dtype=float32), -0.12278031]. 
=============================================
[2019-04-01 19:52:32,996] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[71.375755]
 [71.10294 ]
 [70.902985]
 [70.70093 ]
 [70.51862 ]], R is [[71.74560547]
 [71.74072266]
 [71.73588562]
 [71.73078918]
 [71.72471619]].
[2019-04-01 19:52:33,650] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:52:33,653] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5224
[2019-04-01 19:52:33,681] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8333333333333334, 41.5, 102.3333333333333, 785.3333333333334, 26.0, 25.13661273304226, 8.483053318130912, 0.0, 1.0, 16.54981722187848], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3075000.0000, 
sim time next is 3075600.0000, 
raw observation next is [-0.6666666666666667, 41.0, 100.6666666666667, 780.1666666666667, 26.0, 25.13827667738194, 8.464941775049011, 0.0, 1.0, 15.80150289928707], 
processed observation next is [0.0, 0.6086956521739131, 0.44413665743305636, 0.41, 0.33555555555555566, 0.8620626151012892, 1.0, 0.8768966681974197, 0.08464941775049012, 0.0, 1.0, 0.1128678778520505], 
reward next is 0.8871, 
noisyNet noise sample is [array([0.7115231], dtype=float32), 2.30389]. 
=============================================
[2019-04-01 19:52:36,017] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-01 19:52:36,017] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 19:52:36,018] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:52:36,018] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 19:52:36,019] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:52:36,019] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 19:52:36,019] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:52:36,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run44
[2019-04-01 19:52:36,049] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run44
[2019-04-01 19:52:36,049] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run44
[2019-04-01 19:53:55,977] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.2362549], dtype=float32), -0.9066651]
[2019-04-01 19:53:55,978] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.0, 100.0, 114.6666666666667, 0.0, 26.0, 24.97470987813698, 6.977934522946924, 1.0, 1.0, 37.46573634199677]
[2019-04-01 19:53:55,978] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 19:53:55,980] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.000000e+00 0.000000e+00 0.000000e+00 3.281681e-35 0.000000e+00
 1.000000e+00 0.000000e+00], sampled 0.5517847551713595
[2019-04-01 19:54:11,870] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.2362549], dtype=float32), -0.9066651]
[2019-04-01 19:54:11,870] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [18.4, 39.33333333333333, 123.3333333333333, 539.3333333333333, 26.0, 28.81033219309251, 34.71853265932714, 1.0, 0.0, 0.0]
[2019-04-01 19:54:11,871] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 19:54:11,871] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.37791978456018405
[2019-04-01 19:54:18,592] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 19:54:37,542] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 19:54:41,223] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 19:54:42,245] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 4300000, evaluation results [4300000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 19:54:46,881] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:54:46,882] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0067
[2019-04-01 19:54:46,905] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666666, 76.0, 0.0, 0.0, 26.0, 25.89519549829945, 12.8877259705403, 0.0, 1.0, 28.78412707554266], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3534000.0000, 
sim time next is 3534600.0000, 
raw observation next is [-0.8333333333333334, 77.0, 0.0, 0.0, 26.0, 25.88236136728679, 12.51236173329947, 0.0, 1.0, 27.27696702404367], 
processed observation next is [1.0, 0.9130434782608695, 0.43951985226223456, 0.77, 0.0, 0.0, 1.0, 0.9831944810409701, 0.1251236173329947, 0.0, 1.0, 0.1948354787431691], 
reward next is 0.8052, 
noisyNet noise sample is [array([1.2295891], dtype=float32), 1.8703082]. 
=============================================
[2019-04-01 19:54:49,516] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:54:49,521] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1581
[2019-04-01 19:54:49,534] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.95112436057084, 7.605898394196615, 0.0, 1.0, 40.87335595438785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3371400.0000, 
sim time next is 3372000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.91844360969431, 7.829772561933758, 0.0, 1.0, 41.44721236766615], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.8454919442420444, 0.07829772561933758, 0.0, 1.0, 0.29605151691190107], 
reward next is 0.7039, 
noisyNet noise sample is [array([-1.3719795], dtype=float32), -0.10641539]. 
=============================================
[2019-04-01 19:54:49,537] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[67.53993]
 [68.18875]
 [68.97222]
 [70.01741]
 [71.3293 ]], R is [[66.9342041 ]
 [66.97290802]
 [67.0109787 ]
 [67.04852295]
 [67.08540344]].
[2019-04-01 19:54:50,393] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.1408633e-36 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:54:50,394] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1307
[2019-04-01 19:54:50,429] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.36776835110804, 10.02052741432003, 0.0, 1.0, 26.96994130847488], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4046400.0000, 
sim time next is 4047000.0000, 
raw observation next is [-4.0, 30.66666666666667, 0.0, 0.0, 26.0, 25.3927197662793, 9.919777249623584, 1.0, 1.0, 22.73958376545664], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.3066666666666667, 0.0, 0.0, 1.0, 0.9132456808970427, 0.09919777249623583, 1.0, 1.0, 0.16242559832469028], 
reward next is 0.8376, 
noisyNet noise sample is [array([-1.3203037], dtype=float32), 1.5935539]. 
=============================================
[2019-04-01 19:54:50,447] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[73.06042 ]
 [71.17266 ]
 [69.03269 ]
 [69.884735]
 [68.34798 ]], R is [[72.10619354]
 [72.19248962]
 [72.27802277]
 [72.30673218]
 [72.36715698]].
[2019-04-01 19:54:54,180] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.5896387e-11 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:54:54,180] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3332
[2019-04-01 19:54:54,206] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 54.5, 116.0, 823.0, 26.0, 25.8296324755989, 12.25804660437248, 1.0, 1.0, 8.602649974183384], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3501000.0000, 
sim time next is 3501600.0000, 
raw observation next is [2.0, 53.66666666666667, 115.8333333333333, 820.1666666666667, 26.0, 25.90521759867262, 13.13143873001311, 1.0, 1.0, 7.617426019748392], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.5366666666666667, 0.386111111111111, 0.9062615101289135, 1.0, 0.9864596569532316, 0.13131438730013112, 1.0, 1.0, 0.05441018585534566], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5159939], dtype=float32), 0.16722183]. 
=============================================
[2019-04-01 19:55:00,406] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:55:00,410] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4727
[2019-04-01 19:55:00,424] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.24032137112678, 9.689542560219229, 0.0, 1.0, 45.00978473541915], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3451800.0000, 
sim time next is 3452400.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.27382986138702, 9.874220700057291, 0.0, 1.0, 43.29298690340008], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 1.0, 0.8962614087695745, 0.09874220700057292, 0.0, 1.0, 0.30923562073857197], 
reward next is 0.6908, 
noisyNet noise sample is [array([0.8794454], dtype=float32), 0.0071217474]. 
=============================================
[2019-04-01 19:55:03,315] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:55:03,317] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4940
[2019-04-01 19:55:03,354] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.58182308158349, 7.925011331592043, 0.0, 1.0, 23.04509209394281], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3633000.0000, 
sim time next is 3633600.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.52677929401949, 7.811181615998358, 0.0, 1.0, 25.67995835035971], 
processed observation next is [0.0, 0.043478260869565216, 0.7119113573407203, 0.25, 0.0, 0.0, 1.0, 0.9323970420027844, 0.07811181615998358, 0.0, 1.0, 0.18342827393114078], 
reward next is 0.8166, 
noisyNet noise sample is [array([0.40309605], dtype=float32), 1.3271693]. 
=============================================
[2019-04-01 19:55:05,550] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:55:05,550] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8680
[2019-04-01 19:55:05,568] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.42383819146185, 6.122987226621459, 0.0, 1.0, 40.57936293639295], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3566400.0000, 
sim time next is 3567000.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.38041051084529, 6.048351299500091, 0.0, 1.0, 40.6772184279188], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 1.0, 0.7686300729778986, 0.060483512995000904, 0.0, 1.0, 0.29055156019942], 
reward next is 0.7094, 
noisyNet noise sample is [array([-0.99488944], dtype=float32), -2.1536708]. 
=============================================
[2019-04-01 19:55:05,575] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[67.10598 ]
 [67.10724 ]
 [67.11576 ]
 [67.14157 ]
 [67.155365]], R is [[67.158638  ]
 [67.19719696]
 [67.23595428]
 [67.27481842]
 [67.31364441]].
[2019-04-01 19:55:06,117] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:55:06,117] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9547
[2019-04-01 19:55:06,136] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.03753442266338, 7.408468249507784, 0.0, 1.0, 40.27566163934991], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4064400.0000, 
sim time next is 4065000.0000, 
raw observation next is [-6.0, 41.00000000000001, 0.0, 0.0, 26.0, 25.02690307819863, 7.423838939906115, 0.0, 1.0, 40.31946680795429], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.4100000000000001, 0.0, 0.0, 1.0, 0.8609861540283755, 0.07423838939906115, 0.0, 1.0, 0.2879961914853878], 
reward next is 0.7120, 
noisyNet noise sample is [array([-0.02563329], dtype=float32), -1.8634856]. 
=============================================
[2019-04-01 19:55:06,142] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[62.88656 ]
 [62.359318]
 [61.90082 ]
 [61.46526 ]
 [61.143185]], R is [[63.33212662]
 [63.41112518]
 [63.48957825]
 [63.56760025]
 [63.64490128]].
[2019-04-01 19:55:08,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:55:08,806] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6148
[2019-04-01 19:55:08,823] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 27.0, 0.0, 0.0, 26.0, 25.50392283507203, 7.73210923228027, 0.0, 1.0, 29.65924985929439], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3654000.0000, 
sim time next is 3654600.0000, 
raw observation next is [8.833333333333334, 27.83333333333333, 0.0, 0.0, 26.0, 25.48466303210447, 7.684318345519107, 0.0, 1.0, 29.76551448349055], 
processed observation next is [0.0, 0.30434782608695654, 0.7072945521698984, 0.27833333333333327, 0.0, 0.0, 1.0, 0.9263804331577816, 0.07684318345519107, 0.0, 1.0, 0.2126108177392182], 
reward next is 0.7874, 
noisyNet noise sample is [array([-0.7865295], dtype=float32), -1.6734828]. 
=============================================
[2019-04-01 19:55:10,621] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 4.3861777e-38 1.0000000e+00 0.0000000e+00
 4.9271631e-10 1.3969856e-29], sum to 1.0000
[2019-04-01 19:55:10,622] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7924
[2019-04-01 19:55:10,631] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.333333333333333, 49.0, 83.16666666666666, 674.5, 26.0, 25.5539333609978, 10.46876408464083, 0.0, 1.0, 3.230854749352541], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3685200.0000, 
sim time next is 3685800.0000, 
raw observation next is [5.166666666666667, 49.5, 79.33333333333334, 644.0, 26.0, 25.54548451499326, 10.39038646335963, 0.0, 1.0, 3.225647658508251], 
processed observation next is [0.0, 0.6521739130434783, 0.6057248384118191, 0.495, 0.2644444444444445, 0.7116022099447514, 1.0, 0.9350692164276084, 0.1039038646335963, 0.0, 1.0, 0.02304034041791608], 
reward next is 0.9770, 
noisyNet noise sample is [array([0.9272948], dtype=float32), 1.8403941]. 
=============================================
[2019-04-01 19:55:13,487] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8236258e-30 1.3073497e-24 3.4015082e-20 8.8081052e-14 2.0487932e-32
 9.9999738e-01 2.6537282e-06], sum to 1.0000
[2019-04-01 19:55:13,490] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7317
[2019-04-01 19:55:13,521] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 55.5, 117.0, 835.0, 26.0, 25.80991499948543, 11.42369141476281, 1.0, 1.0, 6.7901185988018], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3846600.0000, 
sim time next is 3847200.0000, 
raw observation next is [0.3333333333333333, 54.0, 116.3333333333333, 833.1666666666667, 26.0, 25.03508840607804, 10.98749858068736, 1.0, 1.0, 31.98468947086214], 
processed observation next is [1.0, 0.5217391304347826, 0.4718374884579871, 0.54, 0.38777777777777767, 0.9206261510128915, 1.0, 0.862155486582577, 0.1098749858068736, 1.0, 1.0, 0.22846206764901528], 
reward next is 0.3765, 
noisyNet noise sample is [array([1.0104444], dtype=float32), 0.06373224]. 
=============================================
[2019-04-01 19:55:19,743] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:55:19,749] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7384
[2019-04-01 19:55:19,762] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.333333333333333, 51.66666666666666, 0.0, 0.0, 26.0, 25.7985626892979, 13.36136920850951, 1.0, 1.0, 4.68748090207451], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4646400.0000, 
sim time next is 4647000.0000, 
raw observation next is [3.166666666666667, 52.33333333333334, 0.0, 0.0, 26.0, 25.91776988120137, 13.32127028123106, 1.0, 1.0, 4.752035167679477], 
processed observation next is [1.0, 0.782608695652174, 0.5503231763619576, 0.5233333333333334, 0.0, 0.0, 1.0, 0.9882528401716242, 0.1332127028123106, 1.0, 1.0, 0.03394310834056769], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.898633], dtype=float32), -0.9716724]. 
=============================================
[2019-04-01 19:55:19,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[49.598804]
 [50.17089 ]
 [50.898705]
 [51.933933]
 [52.62991 ]], R is [[48.66713333]
 [48.18046188]
 [47.69865799]
 [47.2807045 ]
 [46.80789948]].
[2019-04-01 19:55:24,190] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:55:24,191] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6519
[2019-04-01 19:55:24,219] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.333333333333334, 74.0, 0.0, 0.0, 26.0, 25.18150318506286, 7.814529483058668, 0.0, 1.0, 40.60933998387758], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3906600.0000, 
sim time next is 3907200.0000, 
raw observation next is [-4.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.18662482168977, 7.634846783297796, 0.0, 1.0, 40.71619756525885], 
processed observation next is [1.0, 0.21739130434782608, 0.3333333333333333, 0.71, 0.0, 0.0, 1.0, 0.8838035459556812, 0.07634846783297795, 0.0, 1.0, 0.2908299826089918], 
reward next is 0.7092, 
noisyNet noise sample is [array([-1.4509904], dtype=float32), 1.0632437]. 
=============================================
[2019-04-01 19:55:26,008] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.2554826e-36
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:55:26,012] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1115
[2019-04-01 19:55:26,036] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.166666666666666, 53.83333333333334, 0.0, 0.0, 26.0, 25.06359521768761, 8.073344925430709, 0.0, 1.0, 43.564570092988], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3971400.0000, 
sim time next is 3972000.0000, 
raw observation next is [-9.333333333333334, 54.66666666666667, 0.0, 0.0, 26.0, 25.01564586822736, 7.903062024543378, 0.0, 1.0, 43.53196340271661], 
processed observation next is [1.0, 1.0, 0.20406278855032317, 0.5466666666666667, 0.0, 0.0, 1.0, 0.859377981175337, 0.07903062024543378, 0.0, 1.0, 0.3109425957336901], 
reward next is 0.6891, 
noisyNet noise sample is [array([0.00168842], dtype=float32), 1.3795018]. 
=============================================
[2019-04-01 19:55:26,038] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[63.77469 ]
 [63.88042 ]
 [63.834785]
 [63.936676]
 [64.137245]], R is [[63.70154953]
 [63.75335693]
 [63.80331421]
 [63.85059357]
 [63.90641022]].
[2019-04-01 19:55:34,972] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:55:34,972] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7213
[2019-04-01 19:55:34,992] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 36.5, 0.0, 0.0, 26.0, 25.10121936956201, 10.77995044466407, 1.0, 1.0, 19.18646649996005], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4128600.0000, 
sim time next is 4129200.0000, 
raw observation next is [3.0, 37.0, 0.0, 0.0, 26.0, 25.38477729226292, 11.12661037414848, 1.0, 1.0, 11.36068125912617], 
processed observation next is [1.0, 0.8260869565217391, 0.5457063711911359, 0.37, 0.0, 0.0, 1.0, 0.9121110417518459, 0.11126610374148481, 1.0, 1.0, 0.08114772327947264], 
reward next is 0.4682, 
noisyNet noise sample is [array([-0.6556291], dtype=float32), 0.8561912]. 
=============================================
[2019-04-01 19:55:35,257] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:55:35,257] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4232
[2019-04-01 19:55:35,272] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 37.16666666666666, 0.0, 0.0, 26.0, 25.71878875014327, 12.75230676938332, 0.0, 1.0, 31.91701138174998], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4135800.0000, 
sim time next is 4136400.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 26.0, 25.85999042246665, 12.88241307054062, 0.0, 1.0, 26.52359404233436], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 1.0, 0.9799986317809503, 0.12882413070540621, 0.0, 1.0, 0.18945424315953113], 
reward next is 0.8105, 
noisyNet noise sample is [array([0.5117932], dtype=float32), 1.4254897]. 
=============================================
[2019-04-01 19:55:35,295] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:55:35,296] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9496
[2019-04-01 19:55:35,314] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.5, 36.0, 93.0, 437.0, 26.0, 27.4331868552168, 15.7218388332553, 1.0, 1.0, 1.382231865888288], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4120200.0000, 
sim time next is 4120800.0000, 
raw observation next is [3.333333333333333, 36.33333333333333, 81.33333333333333, 373.6666666666667, 26.0, 27.38587444465803, 18.32380383381727, 1.0, 1.0, 1.383028964116201], 
processed observation next is [1.0, 0.6956521739130435, 0.5549399815327793, 0.3633333333333333, 0.2711111111111111, 0.41289134438305714, 1.0, 1.1979820635225755, 0.1832380383381727, 1.0, 1.0, 0.009878778315115721], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7655], dtype=float32), -0.61909586]. 
=============================================
[2019-04-01 19:55:36,165] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:55:36,166] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2385
[2019-04-01 19:55:36,195] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.1333333333333333, 72.16666666666666, 115.0, 44.00000000000001, 26.0, 25.95217048529828, 10.52663168913106, 1.0, 1.0, 19.65140970095678], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4524600.0000, 
sim time next is 4525200.0000, 
raw observation next is [0.0, 72.0, 117.0, 33.0, 26.0, 26.05989549393809, 10.77866270542664, 1.0, 1.0, 17.16800297556912], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.72, 0.39, 0.036464088397790057, 1.0, 1.0085564991340128, 0.1077866270542664, 1.0, 1.0, 0.12262859268263657], 
reward next is 0.5659, 
noisyNet noise sample is [array([-1.7004427], dtype=float32), -0.52771443]. 
=============================================
[2019-04-01 19:55:42,426] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:55:42,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:55:42,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run33
[2019-04-01 19:55:47,029] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 8.013582e-09 1.000000e+00], sum to 1.0000
[2019-04-01 19:55:47,031] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7113
[2019-04-01 19:55:47,039] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.53333333333333, 30.33333333333333, 128.3333333333333, 806.5, 26.0, 28.25475941849612, 30.24787868800185, 1.0, 1.0, 1.355656831363616], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4369200.0000, 
sim time next is 4369800.0000, 
raw observation next is [14.51666666666667, 30.66666666666667, 141.6666666666667, 771.0, 26.0, 28.43293658369069, 31.55446056059153, 1.0, 1.0, 1.309087824955852], 
processed observation next is [1.0, 0.5652173913043478, 0.8647276084949217, 0.3066666666666667, 0.4722222222222224, 0.8519337016574585, 1.0, 1.34756236909867, 0.3155446056059153, 1.0, 1.0, 0.009350627321113228], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9772101], dtype=float32), 1.1293203]. 
=============================================
[2019-04-01 19:55:53,448] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:55:53,449] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6406
[2019-04-01 19:55:53,497] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 65.0, 49.16666666666667, 84.16666666666667, 26.0, 25.36507275762299, 8.440482572305724, 1.0, 1.0, 35.04621820445171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5037600.0000, 
sim time next is 5038200.0000, 
raw observation next is [-2.5, 65.0, 59.0, 101.0, 26.0, 25.47043518519113, 8.623225717639345, 1.0, 1.0, 30.60819393902648], 
processed observation next is [1.0, 0.30434782608695654, 0.39335180055401664, 0.65, 0.19666666666666666, 0.11160220994475138, 1.0, 0.9243478835987327, 0.08623225717639345, 1.0, 1.0, 0.218629956707332], 
reward next is 0.7814, 
noisyNet noise sample is [array([1.3664492], dtype=float32), 1.4381114]. 
=============================================
[2019-04-01 19:55:57,246] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:55:57,247] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:55:57,252] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run33
[2019-04-01 19:55:57,979] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 6.4774116e-29 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:55:57,983] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2662
[2019-04-01 19:55:58,024] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 61.0, 146.5, 169.0, 26.0, 25.98598423403662, 9.967829895003854, 1.0, 1.0, 24.19220045429043], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 138000.0000, 
sim time next is 138600.0000, 
raw observation next is [-6.7, 61.0, 148.0, 106.0, 26.0, 25.95547443623953, 9.771718725442184, 1.0, 1.0, 22.86689064331536], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.49333333333333335, 0.11712707182320442, 1.0, 0.9936392051770758, 0.09771718725442184, 1.0, 1.0, 0.1633349331665383], 
reward next is 0.8367, 
noisyNet noise sample is [array([0.6453982], dtype=float32), -0.018843994]. 
=============================================
[2019-04-01 19:56:00,119] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00
 3.5424292e-22 0.0000000e+00], sum to 1.0000
[2019-04-01 19:56:00,120] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5943
[2019-04-01 19:56:00,148] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.5, 79.5, 135.0, 0.0, 26.0, 25.57508489304582, 9.281639356328524, 1.0, 1.0, 23.91563009507411], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4714200.0000, 
sim time next is 4714800.0000, 
raw observation next is [1.666666666666667, 77.33333333333334, 145.1666666666667, 0.9999999999999998, 26.0, 25.63607386601702, 9.708917379170037, 1.0, 1.0, 22.72726660696049], 
processed observation next is [1.0, 0.5652173913043478, 0.5087719298245615, 0.7733333333333334, 0.48388888888888903, 0.0011049723756906074, 1.0, 0.9480105522881459, 0.09708917379170037, 1.0, 1.0, 0.16233761862114637], 
reward next is 0.8377, 
noisyNet noise sample is [array([0.8364096], dtype=float32), -0.38722807]. 
=============================================
[2019-04-01 19:56:02,625] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 7.331532e-36 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 19:56:02,628] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4329
[2019-04-01 19:56:02,650] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 130.5, 0.9999999999999998, 26.0, 26.18280485568399, 10.88553599240523, 1.0, 1.0, 12.11044033534386], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4700400.0000, 
sim time next is 4701000.0000, 
raw observation next is [0.0, 92.0, 146.0, 2.0, 26.0, 26.20773547047327, 11.04658341702184, 1.0, 1.0, 11.5761977713954], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.4866666666666667, 0.0022099447513812156, 1.0, 1.0296764957818958, 0.1104658341702184, 1.0, 1.0, 0.08268712693853857], 
reward next is 0.4987, 
noisyNet noise sample is [array([-0.87470317], dtype=float32), -1.5991993]. 
=============================================
[2019-04-01 19:56:02,672] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[51.603867]
 [51.269577]
 [50.780003]
 [50.587555]
 [50.840935]], R is [[52.04191208]
 [52.08077621]
 [52.17107773]
 [52.36740494]
 [52.68199158]].
[2019-04-01 19:56:03,059] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:56:03,060] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5816
[2019-04-01 19:56:03,068] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 184.0, 655.0, 26.0, 25.16178561242932, 9.599541420671038, 0.0, 1.0, 7.509519387252141], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4802400.0000, 
sim time next is 4803000.0000, 
raw observation next is [3.0, 37.0, 172.0, 684.0, 26.0, 25.16193898238489, 9.623916341700218, 0.0, 1.0, 7.247901120796032], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.5733333333333334, 0.7558011049723757, 1.0, 0.8802769974835556, 0.09623916341700217, 0.0, 1.0, 0.05177072229140023], 
reward next is 0.9482, 
noisyNet noise sample is [array([-0.14624862], dtype=float32), 0.99616396]. 
=============================================
[2019-04-01 19:56:03,081] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[74.71602 ]
 [74.77991 ]
 [74.927185]
 [74.99578 ]
 [74.46237 ]], R is [[74.8141861 ]
 [75.0124054 ]
 [75.20536804]
 [75.39277649]
 [75.57424927]].
[2019-04-01 19:56:04,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:04,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:04,443] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run33
[2019-04-01 19:56:07,682] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 4.1043236e-22 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:56:07,682] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8329
[2019-04-01 19:56:07,709] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.29397026178328, 9.373721323845663, 0.0, 1.0, 20.76849135800333], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4737000.0000, 
sim time next is 4737600.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.18143873996165, 8.998302720481528, 0.0, 1.0, 19.61405482306855], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.78, 0.0, 0.0, 1.0, 0.8830626771373785, 0.08998302720481527, 0.0, 1.0, 0.14010039159334678], 
reward next is 0.8599, 
noisyNet noise sample is [array([-1.1146231], dtype=float32), 0.21346655]. 
=============================================
[2019-04-01 19:56:09,358] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:56:09,358] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7289
[2019-04-01 19:56:09,385] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.01771930452897, 7.922924410781306, 0.0, 1.0, 23.31660677416762], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4820400.0000, 
sim time next is 4821000.0000, 
raw observation next is [1.0, 43.66666666666667, 0.0, 0.0, 26.0, 25.00448439612168, 7.81871057548769, 0.0, 1.0, 20.1951952480674], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4366666666666667, 0.0, 0.0, 1.0, 0.8577834851602398, 0.0781871057548769, 0.0, 1.0, 0.14425139462905284], 
reward next is 0.8557, 
noisyNet noise sample is [array([0.23878518], dtype=float32), 0.08431402]. 
=============================================
[2019-04-01 19:56:09,391] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[75.00503]
 [75.10509]
 [75.34642]
 [75.69013]
 [76.24652]], R is [[75.144104  ]
 [75.22611237]
 [75.29823303]
 [75.36037445]
 [75.40460968]].
[2019-04-01 19:56:12,856] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:12,856] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:12,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run33
[2019-04-01 19:56:15,814] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:15,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:15,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run33
[2019-04-01 19:56:16,061] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:56:16,061] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3711
[2019-04-01 19:56:16,079] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.5, 19.66666666666666, 0.0, 0.0, 26.0, 26.31308997060717, 13.3030888903435, 0.0, 1.0, 6.558360410028921], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5092800.0000, 
sim time next is 5093400.0000, 
raw observation next is [8.450000000000001, 19.83333333333334, 0.0, 0.0, 26.0, 26.20691383102076, 12.8555293026041, 0.0, 1.0, 6.428401522225993], 
processed observation next is [1.0, 0.9565217391304348, 0.6966759002770084, 0.1983333333333334, 0.0, 0.0, 1.0, 1.0295591187172515, 0.12855529302604102, 0.0, 1.0, 0.045917153730185664], 
reward next is 0.9541, 
noisyNet noise sample is [array([-0.40299052], dtype=float32), -0.13016282]. 
=============================================
[2019-04-01 19:56:17,249] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:17,250] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:17,272] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run33
[2019-04-01 19:56:19,597] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.7752570e-38 0.0000000e+00 0.0000000e+00
 6.2637955e-01 3.7362045e-01], sum to 1.0000
[2019-04-01 19:56:19,598] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8246
[2019-04-01 19:56:19,611] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.833333333333332, 25.16666666666667, 85.33333333333334, 729.6666666666667, 26.0, 26.53420746571373, 18.21973021599144, 1.0, 1.0, 1.037025277375912], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4981800.0000, 
sim time next is 4982400.0000, 
raw observation next is [9.0, 25.0, 82.0, 707.5, 26.0, 27.02546854213345, 20.58089720424735, 1.0, 1.0, 0.9688114889326075], 
processed observation next is [1.0, 0.6956521739130435, 0.7119113573407203, 0.25, 0.2733333333333333, 0.7817679558011049, 1.0, 1.1464955060190642, 0.2058089720424735, 1.0, 1.0, 0.0069200820638043396], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9209318], dtype=float32), -0.080872886]. 
=============================================
[2019-04-01 19:56:19,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:19,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:19,918] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run33
[2019-04-01 19:56:20,052] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:56:20,052] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2286
[2019-04-01 19:56:20,089] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.700000000000001, 61.0, 145.0, 231.9999999999999, 26.0, 25.99029759763395, 10.12944964272775, 1.0, 1.0, 25.64723096074252], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 137400.0000, 
sim time next is 138000.0000, 
raw observation next is [-6.700000000000001, 61.0, 146.5, 169.0, 26.0, 25.98598423403662, 9.967829895003854, 1.0, 1.0, 24.19220045429043], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.48833333333333334, 0.1867403314917127, 1.0, 0.9979977477195173, 0.09967829895003855, 1.0, 1.0, 0.1728014318163602], 
reward next is 0.8272, 
noisyNet noise sample is [array([-0.14776146], dtype=float32), 0.5668576]. 
=============================================
[2019-04-01 19:56:20,095] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[52.0986  ]
 [50.54892 ]
 [49.08168 ]
 [47.82087 ]
 [46.737083]], R is [[54.01821518]
 [54.24306107]
 [54.39377975]
 [54.51405334]
 [54.63263702]].
[2019-04-01 19:56:20,236] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:56:20,236] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8884
[2019-04-01 19:56:20,250] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 25.83855012480947, 12.49757376757167, 0.0, 1.0, 21.1743108446148], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5004000.0000, 
sim time next is 5004600.0000, 
raw observation next is [3.0, 36.5, 0.0, 0.0, 26.0, 25.85933858173377, 11.08577455031652, 0.0, 1.0, 21.96923669534098], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.365, 0.0, 0.0, 1.0, 0.9799055116762528, 0.1108577455031652, 0.0, 1.0, 0.15692311925243557], 
reward next is 0.8431, 
noisyNet noise sample is [array([1.1863165], dtype=float32), -1.4532423]. 
=============================================
[2019-04-01 19:56:20,488] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:20,488] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:20,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run33
[2019-04-01 19:56:20,786] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:20,787] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:20,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run33
[2019-04-01 19:56:20,868] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:20,868] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:20,881] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run33
[2019-04-01 19:56:21,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:21,118] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:21,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run33
[2019-04-01 19:56:21,151] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:21,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:21,164] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run33
[2019-04-01 19:56:22,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:22,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:22,110] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run33
[2019-04-01 19:56:22,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:22,215] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:22,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run33
[2019-04-01 19:56:22,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:22,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:22,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run33
[2019-04-01 19:56:25,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 19:56:25,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:56:25,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run33
[2019-04-01 19:56:32,261] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:56:32,262] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4719
[2019-04-01 19:56:32,353] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.9, 86.0, 85.66666666666666, 0.0, 26.0, 24.41280361494912, 6.304572619661748, 0.0, 1.0, 32.54122137794904], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 49200.0000, 
sim time next is 49800.0000, 
raw observation next is [7.8, 86.0, 84.33333333333334, 0.0, 26.0, 24.37229006592142, 6.309986271370327, 0.0, 1.0, 36.94034258568596], 
processed observation next is [0.0, 0.5652173913043478, 0.6786703601108034, 0.86, 0.28111111111111114, 0.0, 1.0, 0.7674700094173456, 0.06309986271370327, 0.0, 1.0, 0.2638595898977569], 
reward next is 0.7361, 
noisyNet noise sample is [array([-0.9374755], dtype=float32), 0.7387606]. 
=============================================
[2019-04-01 19:56:33,223] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1970495e-11 2.5638543e-11 2.9810344e-15 4.0778492e-11 3.1074879e-03
 9.9689251e-01 8.9664844e-22], sum to 1.0000
[2019-04-01 19:56:33,228] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9859
[2019-04-01 19:56:33,304] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.63333333333333, 69.0, 25.0, 325.8333333333334, 26.0, 25.14204127033756, 7.139662995029812, 1.0, 1.0, 62.86764520667337], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 289200.0000, 
sim time next is 289800.0000, 
raw observation next is [-12.55, 68.5, 30.0, 386.0, 26.0, 25.47883117286318, 7.459522758382175, 1.0, 1.0, 51.53145126238974], 
processed observation next is [1.0, 0.34782608695652173, 0.11495844875346259, 0.685, 0.1, 0.4265193370165746, 1.0, 0.9255473104090258, 0.07459522758382174, 1.0, 1.0, 0.36808179473135527], 
reward next is 0.6319, 
noisyNet noise sample is [array([-1.0838727], dtype=float32), -1.7030686]. 
=============================================
[2019-04-01 19:56:34,570] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:56:34,571] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4816
[2019-04-01 19:56:34,586] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.5639158104693, 6.414967078021182, 0.0, 1.0, 39.7585381752914], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 79200.0000, 
sim time next is 79800.0000, 
raw observation next is [0.4666666666666667, 95.83333333333333, 0.0, 0.0, 26.0, 24.54166533085451, 6.366870990003875, 0.0, 1.0, 39.70635737708008], 
processed observation next is [0.0, 0.9565217391304348, 0.4755309325946445, 0.9583333333333333, 0.0, 0.0, 1.0, 0.7916664758363586, 0.06366870990003876, 0.0, 1.0, 0.28361683840771484], 
reward next is 0.7164, 
noisyNet noise sample is [array([-0.37087488], dtype=float32), -0.57828784]. 
=============================================
[2019-04-01 19:56:44,619] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.446666e-32
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 19:56:44,620] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5326
[2019-04-01 19:56:44,638] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.87197712664877, 6.17431619333887, 0.0, 1.0, 44.16701016617053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 191400.0000, 
sim time next is 192000.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.88612297273327, 6.197982305867922, 0.0, 1.0, 44.21172494018148], 
processed observation next is [1.0, 0.21739130434782608, 0.2160664819944598, 0.78, 0.0, 0.0, 1.0, 0.5551604246761812, 0.06197982305867922, 0.0, 1.0, 0.3157980352870106], 
reward next is 0.6842, 
noisyNet noise sample is [array([-0.39192864], dtype=float32), -0.6949166]. 
=============================================
[2019-04-01 19:56:44,654] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[66.06038 ]
 [66.08198 ]
 [66.11664 ]
 [66.191055]
 [66.26479 ]], R is [[66.08194733]
 [66.10565186]
 [66.12973022]
 [66.15413666]
 [66.17881775]].
[2019-04-01 19:56:55,679] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:56:55,679] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4407
[2019-04-01 19:56:55,717] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 42.0, 54.66666666666667, 398.0000000000001, 26.0, 26.50679695681545, 12.44929706277608, 1.0, 1.0, 26.30247386500651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 316200.0000, 
sim time next is 316800.0000, 
raw observation next is [-9.5, 42.0, 47.0, 358.5, 26.0, 26.54285258571203, 12.26977164438796, 1.0, 1.0, 24.78817355676031], 
processed observation next is [1.0, 0.6956521739130435, 0.1994459833795014, 0.42, 0.15666666666666668, 0.3961325966850829, 1.0, 1.0775503693874329, 0.1226977164438796, 1.0, 1.0, 0.17705838254828793], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.82590675], dtype=float32), -1.3135502]. 
=============================================
[2019-04-01 19:56:57,724] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 1.328499e-23 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 19:56:57,726] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8555
[2019-04-01 19:56:57,775] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 60.0, 0.0, 0.0, 26.0, 25.64542120428845, 9.439370878942093, 1.0, 1.0, 56.05619772219485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 325800.0000, 
sim time next is 326400.0000, 
raw observation next is [-12.1, 61.0, 0.0, 0.0, 26.0, 25.53228099464746, 9.254129291764515, 1.0, 1.0, 54.75126948461988], 
processed observation next is [1.0, 0.782608695652174, 0.12742382271468145, 0.61, 0.0, 0.0, 1.0, 0.9331829992353516, 0.09254129291764515, 1.0, 1.0, 0.39108049631871344], 
reward next is 0.6089, 
noisyNet noise sample is [array([-0.01584403], dtype=float32), -0.6287708]. 
=============================================
[2019-04-01 19:57:09,681] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:57:09,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6344
[2019-04-01 19:57:09,701] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 94.0, 0.0, 0.0, 26.0, 24.94506029296722, 6.8673706486062, 0.0, 1.0, 40.07918741951416], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 513000.0000, 
sim time next is 513600.0000, 
raw observation next is [3.1, 94.66666666666667, 0.0, 0.0, 26.0, 24.93828461359638, 6.847755524480699, 0.0, 1.0, 39.95832689979778], 
processed observation next is [1.0, 0.9565217391304348, 0.5484764542936289, 0.9466666666666668, 0.0, 0.0, 1.0, 0.8483263733709114, 0.06847755524480699, 0.0, 1.0, 0.2854166207128413], 
reward next is 0.7146, 
noisyNet noise sample is [array([-0.02379746], dtype=float32), 1.6219784]. 
=============================================
[2019-04-01 19:57:14,326] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:57:14,326] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6168
[2019-04-01 19:57:14,382] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 83.0, 113.5, 270.0, 26.0, 24.9769782285637, 8.058850131339996, 0.0, 1.0, 31.99331179263962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 572400.0000, 
sim time next is 573000.0000, 
raw observation next is [-1.2, 83.0, 109.0, 204.3333333333333, 26.0, 24.96682930664412, 8.035105077790291, 0.0, 1.0, 33.24653377455424], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.36333333333333334, 0.22578268876611413, 1.0, 0.8524041866634455, 0.08035105077790292, 0.0, 1.0, 0.237475241246816], 
reward next is 0.7625, 
noisyNet noise sample is [array([2.234424], dtype=float32), -0.7240868]. 
=============================================
[2019-04-01 19:57:14,392] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[70.8864  ]
 [70.225174]
 [69.5368  ]
 [69.06242 ]
 [68.98666 ]], R is [[71.54742432]
 [71.60342407]
 [71.66201782]
 [71.72564697]
 [71.81534576]].
[2019-04-01 19:57:16,394] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3389472e-11
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:57:16,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0285
[2019-04-01 19:57:16,418] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 26.0, 24.89497672656292, 6.75054987055309, 0.0, 1.0, 42.38187587253328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 597600.0000, 
sim time next is 598200.0000, 
raw observation next is [-2.9, 83.0, 0.0, 0.0, 26.0, 24.86473780096197, 6.675382743112135, 0.0, 1.0, 42.34950858387246], 
processed observation next is [0.0, 0.9565217391304348, 0.38227146814404434, 0.83, 0.0, 0.0, 1.0, 0.8378196858517098, 0.06675382743112135, 0.0, 1.0, 0.3024964898848033], 
reward next is 0.6975, 
noisyNet noise sample is [array([-1.4941359], dtype=float32), -1.2577044]. 
=============================================
[2019-04-01 19:57:16,742] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:57:16,749] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5574
[2019-04-01 19:57:16,776] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 65.5, 0.0, 0.0, 26.0, 23.70447622287311, 5.242593987009275, 0.0, 1.0, 43.33583534129019], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 625800.0000, 
sim time next is 626400.0000, 
raw observation next is [-4.5, 65.0, 0.0, 0.0, 26.0, 23.71454934565597, 5.240764020761001, 0.0, 1.0, 43.24124287703692], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.65, 0.0, 0.0, 1.0, 0.6735070493794245, 0.05240764020761001, 0.0, 1.0, 0.3088660205502637], 
reward next is 0.6911, 
noisyNet noise sample is [array([0.18348846], dtype=float32), -0.8245161]. 
=============================================
[2019-04-01 19:57:18,043] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:57:18,043] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9405
[2019-04-01 19:57:18,100] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.533333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 24.91616879555493, 8.274655717900059, 1.0, 1.0, 57.27993175192171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 754800.0000, 
sim time next is 755400.0000, 
raw observation next is [-3.716666666666666, 55.66666666666667, 0.0, 0.0, 26.0, 25.19628800722327, 8.917466945988734, 1.0, 1.0, 36.83246471261894], 
processed observation next is [1.0, 0.7391304347826086, 0.3596491228070176, 0.5566666666666668, 0.0, 0.0, 1.0, 0.8851840010318958, 0.08917466945988733, 1.0, 1.0, 0.2630890336615639], 
reward next is 0.7369, 
noisyNet noise sample is [array([-0.8867584], dtype=float32), -0.17203726]. 
=============================================
[2019-04-01 19:57:23,962] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:57:23,963] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4841
[2019-04-01 19:57:24,069] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.105915300038, 5.537182742052058, 0.0, 1.0, 107.9666006873869], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 717000.0000, 
sim time next is 717600.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.47540442308973, 6.27112997089576, 1.0, 1.0, 82.49428957387282], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.0, 0.0, 1.0, 0.7822006318699616, 0.0627112997089576, 1.0, 1.0, 0.5892449255276629], 
reward next is 0.4108, 
noisyNet noise sample is [array([-0.7475071], dtype=float32), 0.040245056]. 
=============================================
[2019-04-01 19:57:24,456] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:57:24,457] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3055
[2019-04-01 19:57:24,464] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.71666666666667, 63.33333333333333, 163.6666666666667, 0.0, 26.0, 25.092129894429, 10.5261000720503, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1165800.0000, 
sim time next is 1166400.0000, 
raw observation next is [18.8, 63.0, 165.5, 0.0, 26.0, 25.08205210781592, 10.50964715538164, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.9833795013850417, 0.63, 0.5516666666666666, 0.0, 1.0, 0.8688645868308456, 0.10509647155381639, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04447094], dtype=float32), 0.8346194]. 
=============================================
[2019-04-01 19:57:24,667] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:57:24,668] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7562
[2019-04-01 19:57:24,688] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.80927793304149, 6.194296656907572, 0.0, 1.0, 41.43127455378613], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 683400.0000, 
sim time next is 684000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.82159803546967, 6.157078837498908, 0.0, 1.0, 41.3510480691841], 
processed observation next is [0.0, 0.9565217391304348, 0.368421052631579, 0.69, 0.0, 0.0, 1.0, 0.8316568622099528, 0.061570788374989076, 0.0, 1.0, 0.29536462906560074], 
reward next is 0.7046, 
noisyNet noise sample is [array([-0.6174217], dtype=float32), -1.014592]. 
=============================================
[2019-04-01 19:57:24,704] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[63.352875]
 [63.426888]
 [63.516827]
 [63.591972]
 [63.678356]], R is [[63.35968018]
 [63.43014908]
 [63.49925995]
 [63.56718445]
 [63.63401794]].
[2019-04-01 19:57:26,253] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:57:26,254] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1630
[2019-04-01 19:57:26,299] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.783333333333333, 71.66666666666667, 94.66666666666666, 0.0, 26.0, 25.59444473828751, 7.417001084669761, 1.0, 1.0, 28.60342670728271], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 816600.0000, 
sim time next is 817200.0000, 
raw observation next is [-4.5, 71.0, 98.5, 0.0, 26.0, 25.62958522822914, 7.469904436462485, 1.0, 1.0, 27.4390737521743], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.3283333333333333, 0.0, 1.0, 0.9470836040327342, 0.07469904436462485, 1.0, 1.0, 0.19599338394410212], 
reward next is 0.8040, 
noisyNet noise sample is [array([0.36781725], dtype=float32), 0.6608494]. 
=============================================
[2019-04-01 19:57:41,806] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:57:41,808] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8417
[2019-04-01 19:57:41,817] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.9, 68.5, 0.0, 0.0, 26.0, 25.67901657065765, 13.64923901481661, 0.0, 1.0, 11.67411143322617], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1121400.0000, 
sim time next is 1122000.0000, 
raw observation next is [11.8, 69.33333333333333, 0.0, 0.0, 26.0, 25.61836963276745, 13.6504114354036, 0.0, 1.0, 20.79434658069888], 
processed observation next is [1.0, 1.0, 0.7894736842105264, 0.6933333333333332, 0.0, 0.0, 1.0, 0.9454813761096358, 0.136504114354036, 0.0, 1.0, 0.14853104700499198], 
reward next is 0.8515, 
noisyNet noise sample is [array([-0.2886144], dtype=float32), -0.05096998]. 
=============================================
[2019-04-01 19:57:41,825] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[73.80962 ]
 [74.087944]
 [73.95014 ]
 [74.20592 ]
 [74.26245 ]], R is [[73.94629669]
 [74.1234436 ]
 [74.29529572]
 [74.46130371]
 [74.62155151]].
[2019-04-01 19:57:43,768] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:57:43,769] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4227
[2019-04-01 19:57:43,777] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 26.12973572073928, 14.1441625084142, 0.0, 1.0, 17.67412759961321], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1039200.0000, 
sim time next is 1039800.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 26.11266544689241, 14.00317821105943, 0.0, 1.0, 16.9212990355091], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 1.0, 1.016095063841773, 0.1400317821105943, 0.0, 1.0, 0.12086642168220786], 
reward next is 0.8791, 
noisyNet noise sample is [array([0.2731033], dtype=float32), -1.3520269]. 
=============================================
[2019-04-01 19:57:53,945] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:57:53,946] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2061
[2019-04-01 19:57:53,962] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.1, 92.0, 0.0, 0.0, 26.0, 25.33170907948095, 11.15202951691578, 0.0, 1.0, 40.66891595064907], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1311000.0000, 
sim time next is 1311600.0000, 
raw observation next is [2.0, 92.0, 0.0, 0.0, 26.0, 25.33547690539413, 11.34940952610903, 0.0, 1.0, 41.87861756903641], 
processed observation next is [1.0, 0.17391304347826086, 0.518005540166205, 0.92, 0.0, 0.0, 1.0, 0.9050681293420187, 0.11349409526109029, 0.0, 1.0, 0.2991329826359743], 
reward next is 0.7009, 
noisyNet noise sample is [array([0.41905344], dtype=float32), -1.8356438]. 
=============================================
[2019-04-01 19:57:57,729] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 4.2839597e-34 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:57:57,732] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4610
[2019-04-01 19:57:57,748] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.33800872449854, 10.60591004749954, 0.0, 1.0, 40.0409545653518], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1383000.0000, 
sim time next is 1383600.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.32502325663993, 11.12474479844281, 0.0, 1.0, 40.44344024105034], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.9035747509485615, 0.11124744798442811, 0.0, 1.0, 0.2888817160075024], 
reward next is 0.7111, 
noisyNet noise sample is [array([-1.261323], dtype=float32), 0.23383822]. 
=============================================
[2019-04-01 19:58:02,625] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.52912015e-33
 0.00000000e+00 1.00000000e+00 0.00000000e+00], sum to 1.0000
[2019-04-01 19:58:02,625] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2262
[2019-04-01 19:58:02,656] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 90.0, 0.0, 0.0, 26.0, 25.61609252207353, 8.825670927616011, 1.0, 1.0, 14.70722166248112], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1445400.0000, 
sim time next is 1446000.0000, 
raw observation next is [1.1, 89.33333333333334, 0.0, 0.0, 26.0, 25.25571336979166, 8.710667001218285, 1.0, 1.0, 13.68971037998981], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.8933333333333334, 0.0, 0.0, 1.0, 0.8936733385416658, 0.08710667001218285, 1.0, 1.0, 0.09778364557135578], 
reward next is 0.9022, 
noisyNet noise sample is [array([1.7258492], dtype=float32), 0.705318]. 
=============================================
[2019-04-01 19:58:02,664] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[62.148743]
 [62.14799 ]
 [62.16923 ]
 [62.18069 ]
 [62.303844]], R is [[62.46403885]
 [62.73434448]
 [62.99563217]
 [62.95600891]
 [62.94513321]].
[2019-04-01 19:58:07,131] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 19:58:07,131] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8510
[2019-04-01 19:58:07,145] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.683333333333334, 81.0, 0.0, 0.0, 26.0, 25.52019956351774, 11.11721206547464, 0.0, 1.0, 24.74394124126033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1551000.0000, 
sim time next is 1551600.0000, 
raw observation next is [5.5, 82.0, 0.0, 0.0, 26.0, 25.41658830675365, 11.40639239443982, 0.0, 1.0, 33.34517974076387], 
processed observation next is [1.0, 1.0, 0.6149584487534627, 0.82, 0.0, 0.0, 1.0, 0.9166554723933784, 0.1140639239443982, 0.0, 1.0, 0.23817985529117047], 
reward next is 0.7618, 
noisyNet noise sample is [array([0.2474176], dtype=float32), 0.23929605]. 
=============================================
[2019-04-01 19:58:08,300] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 5.2508770e-35 1.9483875e-18 7.0000639e-22
 1.0000000e+00 1.2455023e-24], sum to 1.0000
[2019-04-01 19:58:08,306] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0789
[2019-04-01 19:58:08,313] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.78333333333333, 59.83333333333334, 0.0, 0.0, 26.0, 26.34780706130657, 16.14944026697435, 1.0, 1.0, 5.870881861560666], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1619400.0000, 
sim time next is 1620000.0000, 
raw observation next is [10.5, 61.0, 0.0, 0.0, 26.0, 26.33524424568041, 16.37106519792964, 1.0, 1.0, 6.063923699076229], 
processed observation next is [1.0, 0.782608695652174, 0.7534626038781165, 0.61, 0.0, 0.0, 1.0, 1.0478920350972014, 0.16371065197929638, 1.0, 1.0, 0.04331374070768735], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.32312948], dtype=float32), 0.93030584]. 
=============================================
[2019-04-01 19:58:08,331] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[43.42129 ]
 [43.08198 ]
 [42.721752]
 [42.34251 ]
 [42.220398]], R is [[43.28011322]
 [42.84731293]
 [42.4188385 ]
 [41.99465179]
 [41.57470703]].
[2019-04-01 19:58:14,028] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1646875e-33 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 19:58:14,029] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5855
[2019-04-01 19:58:14,093] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 81.33333333333334, 0.0, 0.0, 26.0, 25.18785534279521, 8.565732903751503, 0.0, 1.0, 46.15497889954257], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1974000.0000, 
sim time next is 1974600.0000, 
raw observation next is [-5.6, 80.5, 0.0, 0.0, 26.0, 25.1902214430594, 8.558346393824328, 0.0, 1.0, 44.18420710378399], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.805, 0.0, 0.0, 1.0, 0.8843173490084857, 0.08558346393824329, 0.0, 1.0, 0.3156014793127428], 
reward next is 0.6844, 
noisyNet noise sample is [array([0.6938816], dtype=float32), -0.45207745]. 
=============================================
[2019-04-01 19:58:19,114] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-01 19:58:19,115] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 19:58:19,115] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:58:19,116] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 19:58:19,117] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 19:58:19,117] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:58:19,119] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 19:58:19,122] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run45
[2019-04-01 19:58:19,146] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run45
[2019-04-01 19:58:19,166] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run45
[2019-04-01 19:59:47,067] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.3039968], dtype=float32), -1.0590304]
[2019-04-01 19:59:47,067] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-0.95, 68.0, 88.0, 346.0, 26.0, 26.16402169983776, 12.77443128329449, 1.0, 1.0, 7.895983870225764]
[2019-04-01 19:59:47,068] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 19:59:47,068] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 7.1312166e-29 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sampled 0.4889712095161213
[2019-04-01 20:00:01,061] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 20:00:20,984] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 20:00:24,714] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 20:00:25,736] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 4400000, evaluation results [4400000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 20:00:25,930] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:00:25,930] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1810
[2019-04-01 20:00:25,964] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.733333333333333, 79.66666666666667, 0.0, 0.0, 26.0, 24.24211792121222, 5.659182013976031, 0.0, 1.0, 45.50400661445282], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1819200.0000, 
sim time next is 1819800.0000, 
raw observation next is [-5.8, 80.5, 0.0, 0.0, 26.0, 24.22065223809417, 5.626828781535568, 0.0, 1.0, 45.56546840445898], 
processed observation next is [0.0, 0.043478260869565216, 0.30193905817174516, 0.805, 0.0, 0.0, 1.0, 0.7458074625848816, 0.056268287815355686, 0.0, 1.0, 0.3254676314604213], 
reward next is 0.6745, 
noisyNet noise sample is [array([-1.401637], dtype=float32), 0.0714425]. 
=============================================
[2019-04-01 20:00:28,185] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:00:28,190] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4638
[2019-04-01 20:00:28,210] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.0937476441671, 7.076314213615132, 0.0, 1.0, 42.46929185809314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2408400.0000, 
sim time next is 2409000.0000, 
raw observation next is [-3.583333333333333, 42.33333333333334, 0.0, 0.0, 26.0, 25.14298934796171, 7.034452702162644, 0.0, 1.0, 42.41134184067531], 
processed observation next is [0.0, 0.9130434782608695, 0.36334256694367506, 0.42333333333333345, 0.0, 0.0, 1.0, 0.8775699068516728, 0.07034452702162644, 0.0, 1.0, 0.30293815600482366], 
reward next is 0.6971, 
noisyNet noise sample is [array([0.18318358], dtype=float32), -0.93433285]. 
=============================================
[2019-04-01 20:00:28,238] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[65.20162 ]
 [65.11004 ]
 [65.001274]
 [64.97092 ]
 [65.01208 ]], R is [[65.30056763]
 [65.34420776]
 [65.38700104]
 [65.42914581]
 [65.47077942]].
[2019-04-01 20:00:28,930] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:00:28,931] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4670
[2019-04-01 20:00:28,955] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.45, 85.0, 0.0, 0.0, 26.0, 24.32225685979789, 5.576419088095882, 0.0, 1.0, 42.73934441902104], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2093400.0000, 
sim time next is 2094000.0000, 
raw observation next is [-6.533333333333333, 84.33333333333333, 0.0, 0.0, 26.0, 24.28511749368752, 5.521505203016903, 0.0, 1.0, 42.83153921100624], 
processed observation next is [1.0, 0.21739130434782608, 0.2816251154201293, 0.8433333333333333, 0.0, 0.0, 1.0, 0.7550167848125027, 0.055215052030169035, 0.0, 1.0, 0.3059395657929017], 
reward next is 0.6941, 
noisyNet noise sample is [array([-0.61637574], dtype=float32), 0.055789694]. 
=============================================
[2019-04-01 20:00:28,965] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[61.85554 ]
 [61.961525]
 [62.070137]
 [62.237286]
 [62.37155 ]], R is [[61.85025406]
 [61.9264679 ]
 [62.00383759]
 [62.07317352]
 [62.1464119 ]].
[2019-04-01 20:00:33,860] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:00:33,862] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8954
[2019-04-01 20:00:33,917] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.916666666666667, 85.5, 0.0, 0.0, 26.0, 25.08255186192085, 7.608287301237806, 0.0, 1.0, 43.64341885681097], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1879800.0000, 
sim time next is 1880400.0000, 
raw observation next is [-4.833333333333334, 85.0, 0.0, 0.0, 26.0, 25.10263943485933, 7.561842026284737, 0.0, 1.0, 33.52921035544669], 
processed observation next is [0.0, 0.782608695652174, 0.32871652816251157, 0.85, 0.0, 0.0, 1.0, 0.871805633551333, 0.07561842026284736, 0.0, 1.0, 0.2394943596817621], 
reward next is 0.7605, 
noisyNet noise sample is [array([0.4590123], dtype=float32), -0.48385257]. 
=============================================
[2019-04-01 20:00:43,372] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:00:43,373] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5126
[2019-04-01 20:00:43,402] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.48724537176495, 6.130414589029546, 0.0, 1.0, 42.11666243722584], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1986600.0000, 
sim time next is 1987200.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.45606724479908, 6.080047223442911, 0.0, 1.0, 42.00779452503919], 
processed observation next is [1.0, 0.0, 0.30747922437673136, 0.83, 0.0, 0.0, 1.0, 0.7794381778284399, 0.06080047223442911, 0.0, 1.0, 0.30005567517885134], 
reward next is 0.6999, 
noisyNet noise sample is [array([-0.09850445], dtype=float32), -1.3323607]. 
=============================================
[2019-04-01 20:01:12,853] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.465654e-31
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 20:01:12,853] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0322
[2019-04-01 20:01:12,876] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8629313e-36 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:01:12,877] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6392
[2019-04-01 20:01:12,879] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.42845037925037, 5.470743684310665, 0.0, 1.0, 43.798138626213], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2436000.0000, 
sim time next is 2436600.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.39252227008688, 5.50406654849242, 0.0, 1.0, 43.77835101121482], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 1.0, 0.6275031814409827, 0.0550406654849242, 0.0, 1.0, 0.312702507222963], 
reward next is 0.6873, 
noisyNet noise sample is [array([1.1359644], dtype=float32), -0.2709369]. 
=============================================
[2019-04-01 20:01:12,904] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 25.0, 83.0, 38.0, 26.0, 25.9491922644996, 9.46479671672438, 1.0, 1.0, 16.98881909805598], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2822400.0000, 
sim time next is 2823000.0000, 
raw observation next is [6.5, 25.5, 79.0, 50.66666666666667, 26.0, 26.04324709506112, 8.714047025078083, 1.0, 1.0, 17.15399144290224], 
processed observation next is [1.0, 0.6956521739130435, 0.6426592797783934, 0.255, 0.2633333333333333, 0.0559852670349908, 1.0, 1.0061781564373027, 0.08714047025078082, 1.0, 1.0, 0.12252851030644456], 
reward next is 0.8775, 
noisyNet noise sample is [array([1.1396819], dtype=float32), 0.093598925]. 
=============================================
[2019-04-01 20:01:12,916] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[69.87634]
 [70.15951]
 [70.55518]
 [70.63626]
 [70.72943]], R is [[69.94052887]
 [70.11978149]
 [70.29415131]
 [70.46367645]
 [70.63529968]].
[2019-04-01 20:01:21,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:01:21,942] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5081
[2019-04-01 20:01:21,959] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 47.33333333333333, 0.0, 0.0, 26.0, 25.00188745836813, 5.845291107549717, 0.0, 1.0, 37.99383199668102], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2518800.0000, 
sim time next is 2519400.0000, 
raw observation next is [-1.7, 48.16666666666667, 0.0, 0.0, 26.0, 24.95969379846615, 5.807020926597043, 0.0, 1.0, 37.99503567448993], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4816666666666667, 0.0, 0.0, 1.0, 0.8513848283523073, 0.05807020926597043, 0.0, 1.0, 0.2713931119606423], 
reward next is 0.7286, 
noisyNet noise sample is [array([-1.3486426], dtype=float32), 2.0704372]. 
=============================================
[2019-04-01 20:01:23,443] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:01:23,444] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5666
[2019-04-01 20:01:23,464] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.399999999999999, 61.5, 0.0, 0.0, 26.0, 24.8318293284, 6.70505757913953, 0.0, 1.0, 41.3604242932384], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2591400.0000, 
sim time next is 2592000.0000, 
raw observation next is [-4.5, 62.0, 0.0, 0.0, 26.0, 24.78787185562146, 6.616194306575778, 0.0, 1.0, 41.3733533852574], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.62, 0.0, 0.0, 1.0, 0.8268388365173516, 0.06616194306575779, 0.0, 1.0, 0.29552395275183857], 
reward next is 0.7045, 
noisyNet noise sample is [array([0.14816691], dtype=float32), -1.3994386]. 
=============================================
[2019-04-01 20:01:23,477] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[69.58448 ]
 [69.72646 ]
 [69.865776]
 [69.99145 ]
 [70.17408 ]], R is [[69.34382629]
 [69.35495758]
 [69.36606598]
 [69.37695312]
 [69.38762665]].
[2019-04-01 20:01:32,944] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:01:32,946] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8738
[2019-04-01 20:01:32,962] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.97979210700561, 7.44318521339823, 0.0, 1.0, 44.53264199509194], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2761800.0000, 
sim time next is 2762400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.93285775481328, 7.318111374014499, 0.0, 1.0, 44.23800952494081], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 1.0, 0.8475511078304686, 0.07318111374014499, 0.0, 1.0, 0.31598578232100577], 
reward next is 0.6840, 
noisyNet noise sample is [array([-1.1395682], dtype=float32), 0.2764804]. 
=============================================
[2019-04-01 20:01:40,316] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:01:40,317] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7365
[2019-04-01 20:01:40,342] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 89.5, 0.0, 0.0, 26.0, 25.30852323195405, 7.414355309944291, 0.0, 1.0, 52.52488894675363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2863800.0000, 
sim time next is 2864400.0000, 
raw observation next is [1.0, 90.66666666666667, 0.0, 0.0, 26.0, 25.23443391804643, 7.230714817937849, 0.0, 1.0, 54.09060428868865], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.9066666666666667, 0.0, 0.0, 1.0, 0.8906334168637758, 0.07230714817937849, 0.0, 1.0, 0.38636145920491893], 
reward next is 0.6136, 
noisyNet noise sample is [array([-1.2742172], dtype=float32), 0.36403123]. 
=============================================
[2019-04-01 20:01:41,616] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 2.753462e-38 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 20:01:41,617] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9609
[2019-04-01 20:01:41,659] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 48.66666666666666, 51.83333333333334, 439.6666666666667, 26.0, 26.16520543259755, 13.41742039960452, 1.0, 1.0, 9.713048918457794], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3343200.0000, 
sim time next is 3343800.0000, 
raw observation next is [-2.0, 49.33333333333334, 43.66666666666667, 378.3333333333334, 26.0, 26.32589190233929, 11.62410628314509, 1.0, 1.0, 9.594244540816197], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.4933333333333334, 0.14555555555555558, 0.41804788213628, 1.0, 1.0465559860484699, 0.11624106283145091, 1.0, 1.0, 0.06853031814868712], 
reward next is 0.2818, 
noisyNet noise sample is [array([-0.18005767], dtype=float32), 1.4043771]. 
=============================================
[2019-04-01 20:01:46,366] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:01:46,367] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5951
[2019-04-01 20:01:46,395] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 93.0, 6.0, 41.0, 26.0, 25.76826589643392, 8.69177244980083, 1.0, 1.0, 12.53431796323003], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2914200.0000, 
sim time next is 2914800.0000, 
raw observation next is [1.333333333333333, 93.0, 0.0, 0.0, 26.0, 25.09083676306273, 8.484919657424426, 1.0, 1.0, 25.29470608384106], 
processed observation next is [1.0, 0.7391304347826086, 0.4995383194829178, 0.93, 0.0, 0.0, 1.0, 0.8701195375803898, 0.08484919657424425, 1.0, 1.0, 0.18067647202743614], 
reward next is 0.8193, 
noisyNet noise sample is [array([-0.1607905], dtype=float32), -0.068600275]. 
=============================================
[2019-04-01 20:01:55,145] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00
 5.379484e-26 2.702493e-08], sum to 1.0000
[2019-04-01 20:01:55,146] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1664
[2019-04-01 20:01:55,164] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 49.33333333333334, 43.66666666666667, 378.3333333333334, 26.0, 26.32589190233929, 11.62410628314509, 1.0, 1.0, 9.594244540816197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3343800.0000, 
sim time next is 3344400.0000, 
raw observation next is [-2.0, 50.0, 35.5, 317.0, 26.0, 26.37606352956444, 13.62016056261992, 1.0, 1.0, 9.265769012775253], 
processed observation next is [1.0, 0.7391304347826086, 0.40720221606648205, 0.5, 0.11833333333333333, 0.35027624309392263, 1.0, 1.0537233613663486, 0.1362016056261992, 1.0, 1.0, 0.0661840643769661], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3798492], dtype=float32), -0.022169283]. 
=============================================
[2019-04-01 20:01:57,180] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:01:57,181] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4832
[2019-04-01 20:01:57,196] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 70.0, 0.0, 0.0, 26.0, 24.98094336929946, 8.297102921698245, 0.0, 1.0, 43.16250708061778], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3286800.0000, 
sim time next is 3287400.0000, 
raw observation next is [-7.166666666666667, 71.16666666666667, 0.0, 0.0, 26.0, 24.97280807393727, 8.26309034943354, 0.0, 1.0, 43.17650224205838], 
processed observation next is [1.0, 0.043478260869565216, 0.26408125577100644, 0.7116666666666667, 0.0, 0.0, 1.0, 0.8532582962767528, 0.0826309034943354, 0.0, 1.0, 0.3084035874432742], 
reward next is 0.6916, 
noisyNet noise sample is [array([-1.1965728], dtype=float32), 0.52597725]. 
=============================================
[2019-04-01 20:01:57,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:01:57,200] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8800
[2019-04-01 20:01:57,235] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 63.66666666666667, 0.0, 0.0, 26.0, 24.77673151799807, 6.403576416454214, 0.0, 1.0, 41.81843227152428], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3390000.0000, 
sim time next is 3390600.0000, 
raw observation next is [-3.333333333333333, 61.83333333333333, 0.0, 0.0, 26.0, 24.80901956393799, 6.276015164834763, 0.0, 1.0, 42.1838057463272], 
processed observation next is [1.0, 0.21739130434782608, 0.37026777469990774, 0.6183333333333333, 0.0, 0.0, 1.0, 0.8298599377054272, 0.06276015164834763, 0.0, 1.0, 0.30131289818805146], 
reward next is 0.6987, 
noisyNet noise sample is [array([0.90124726], dtype=float32), -0.35999116]. 
=============================================
[2019-04-01 20:01:59,506] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:01:59,508] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2027
[2019-04-01 20:01:59,521] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 37.5, 0.0, 0.0, 26.0, 24.86760672275964, 6.347749101745481, 0.0, 1.0, 39.90451742615787], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4077000.0000, 
sim time next is 4077600.0000, 
raw observation next is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 26.0, 24.82512238791937, 6.242687986847732, 0.0, 1.0, 39.89286653246236], 
processed observation next is [1.0, 0.17391304347826086, 0.3425669436749769, 0.3633333333333333, 0.0, 0.0, 1.0, 0.8321603411313389, 0.062426879868477315, 0.0, 1.0, 0.2849490466604454], 
reward next is 0.7151, 
noisyNet noise sample is [array([1.0684633], dtype=float32), 0.21826395]. 
=============================================
[2019-04-01 20:02:08,003] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:02:08,004] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9460
[2019-04-01 20:02:08,021] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.166666666666666, 59.83333333333334, 0.0, 0.0, 26.0, 24.88878001024941, 7.067097695323025, 0.0, 1.0, 41.48993789201692], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3910200.0000, 
sim time next is 3910800.0000, 
raw observation next is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 26.0, 24.90954782841563, 7.012884679345478, 0.0, 1.0, 41.59912176919394], 
processed observation next is [1.0, 0.2608695652173913, 0.28716528162511545, 0.6066666666666667, 0.0, 0.0, 1.0, 0.84422111834509, 0.07012884679345478, 0.0, 1.0, 0.297136584065671], 
reward next is 0.7029, 
noisyNet noise sample is [array([0.6823061], dtype=float32), -0.007814222]. 
=============================================
[2019-04-01 20:02:09,812] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7901912e-35
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:02:09,813] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9367
[2019-04-01 20:02:09,832] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.01329397705909, 7.918241011894463, 0.0, 1.0, 40.96816920679748], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3369600.0000, 
sim time next is 3370200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.00125731085495, 7.793251000584853, 0.0, 1.0, 40.92793072640654], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.8573224729792785, 0.07793251000584853, 0.0, 1.0, 0.29234236233147526], 
reward next is 0.7077, 
noisyNet noise sample is [array([-0.67262346], dtype=float32), 0.55655]. 
=============================================
[2019-04-01 20:02:12,639] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.0940013e-38 0.0000000e+00 1.4882960e-33 1.3040657e-18 0.0000000e+00
 1.5096715e-01 8.4903282e-01], sum to 1.0000
[2019-04-01 20:02:12,640] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7229
[2019-04-01 20:02:12,662] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 58.33333333333334, 115.1666666666667, 812.1666666666666, 26.0, 26.32232185871425, 13.54962406508014, 1.0, 1.0, 7.44433414454693], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3498000.0000, 
sim time next is 3498600.0000, 
raw observation next is [1.833333333333333, 57.66666666666666, 115.3333333333333, 814.3333333333334, 26.0, 26.31842282783144, 11.59881153371345, 1.0, 1.0, 7.674202734774461], 
processed observation next is [1.0, 0.4782608695652174, 0.5133887349953832, 0.5766666666666665, 0.3844444444444443, 0.899815837937385, 1.0, 1.0454889754044916, 0.1159881153371345, 1.0, 1.0, 0.05481573381981758], 
reward next is 0.3057, 
noisyNet noise sample is [array([1.4666755], dtype=float32), 0.35463098]. 
=============================================
[2019-04-01 20:02:17,411] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:02:17,412] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2332
[2019-04-01 20:02:17,437] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.833333333333333, 54.16666666666667, 116.6666666666667, 820.6666666666667, 26.0, 25.14318439047831, 9.535439325918055, 0.0, 1.0, 24.64107882396157], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3586200.0000, 
sim time next is 3586800.0000, 
raw observation next is [-2.666666666666667, 53.33333333333334, 117.3333333333333, 821.8333333333334, 26.0, 25.15150755005605, 9.65790837387157, 0.0, 1.0, 24.40750554257576], 
processed observation next is [0.0, 0.5217391304347826, 0.38873499538319484, 0.5333333333333334, 0.391111111111111, 0.9081031307550645, 1.0, 0.8787867928651499, 0.0965790837387157, 0.0, 1.0, 0.17433932530411256], 
reward next is 0.8257, 
noisyNet noise sample is [array([0.46096212], dtype=float32), 2.145633]. 
=============================================
[2019-04-01 20:02:24,713] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 2.6290677e-37 1.7685058e-34 9.4317616e-16 0.0000000e+00
 1.6279205e-25 1.0000000e+00], sum to 1.0000
[2019-04-01 20:02:24,715] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6376
[2019-04-01 20:02:24,738] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 51.5, 124.3333333333333, 811.0, 26.0, 26.76101858934904, 12.39984338316691, 1.0, 1.0, 6.743323528005661], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4619400.0000, 
sim time next is 4620000.0000, 
raw observation next is [2.333333333333333, 51.0, 123.1666666666667, 822.0, 26.0, 26.69250820130056, 15.65044006864571, 1.0, 1.0, 6.32806352820699], 
processed observation next is [1.0, 0.4782608695652174, 0.5272391505078486, 0.51, 0.4105555555555557, 0.9082872928176795, 1.0, 1.098929743042937, 0.1565044006864571, 1.0, 1.0, 0.04520045377290707], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1693177], dtype=float32), -0.8078183]. 
=============================================
[2019-04-01 20:02:24,751] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[12.354977 ]
 [12.160857 ]
 [12.063229 ]
 [11.727615 ]
 [12.7313385]], R is [[12.16543961]
 [12.0437851 ]
 [11.92334747]
 [11.80411434]
 [11.6860733 ]].
[2019-04-01 20:02:35,897] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:02:35,897] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6666
[2019-04-01 20:02:35,913] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 74.0, 0.0, 0.0, 26.0, 25.27189820534734, 8.15669380102622, 0.0, 1.0, 40.24181031497096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3904200.0000, 
sim time next is 3904800.0000, 
raw observation next is [-3.666666666666667, 75.0, 0.0, 0.0, 26.0, 25.28805099910114, 7.999679632037766, 0.0, 1.0, 40.31556381459887], 
processed observation next is [1.0, 0.17391304347826086, 0.3610341643582641, 0.75, 0.0, 0.0, 1.0, 0.8982929998715916, 0.07999679632037765, 0.0, 1.0, 0.2879683129614205], 
reward next is 0.7120, 
noisyNet noise sample is [array([0.12098984], dtype=float32), 1.1562613]. 
=============================================
[2019-04-01 20:02:36,759] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.2127187e-35], sum to 1.0000
[2019-04-01 20:02:36,760] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4708
[2019-04-01 20:02:36,781] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 26.0, 24.82512238791937, 6.242687986847732, 0.0, 1.0, 39.89286653246236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4077600.0000, 
sim time next is 4078200.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 24.76967423285657, 6.296874455162879, 0.0, 1.0, 39.8592263283273], 
processed observation next is [1.0, 0.17391304347826086, 0.3471837488457987, 0.35166666666666674, 0.0, 0.0, 1.0, 0.8242391761223671, 0.06296874455162879, 0.0, 1.0, 0.2847087594880522], 
reward next is 0.7153, 
noisyNet noise sample is [array([-0.3853144], dtype=float32), -2.3189075]. 
=============================================
[2019-04-01 20:02:37,950] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 6.6600064e-33], sum to 1.0000
[2019-04-01 20:02:37,952] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8262
[2019-04-01 20:02:37,976] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.33333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 27.59178933345284, 25.50947087089384, 1.0, 1.0, 6.408355623014676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4390800.0000, 
sim time next is 4391400.0000, 
raw observation next is [11.16666666666667, 56.66666666666667, 0.0, 0.0, 26.0, 27.48639136997601, 24.82104688059031, 1.0, 1.0, 6.36155907572966], 
processed observation next is [1.0, 0.8260869565217391, 0.7719298245614037, 0.5666666666666668, 0.0, 0.0, 1.0, 1.2123416242822869, 0.24821046880590308, 1.0, 1.0, 0.04543970768378328], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.4950328], dtype=float32), 2.41202]. 
=============================================
[2019-04-01 20:02:39,561] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:02:39,562] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2449
[2019-04-01 20:02:39,572] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.0, 67.0, 0.0, 0.0, 26.0, 23.72377574146117, 5.27613392529702, 0.0, 1.0, 43.21971210093525], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3993600.0000, 
sim time next is 3994200.0000, 
raw observation next is [-13.0, 66.0, 0.0, 0.0, 26.0, 23.68739718440316, 5.260021881026044, 0.0, 1.0, 43.17919215277563], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.66, 0.0, 0.0, 1.0, 0.6696281692004514, 0.05260021881026044, 0.0, 1.0, 0.3084228010912545], 
reward next is 0.6916, 
noisyNet noise sample is [array([0.32302696], dtype=float32), -0.39550138]. 
=============================================
[2019-04-01 20:02:45,653] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:02:45,653] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7599
[2019-04-01 20:02:45,665] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 38.66666666666666, 0.0, 0.0, 26.0, 25.93424637740915, 12.34767245577046, 0.0, 1.0, 20.5295472757425], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4138800.0000, 
sim time next is 4139400.0000, 
raw observation next is [1.0, 39.33333333333334, 0.0, 0.0, 26.0, 25.92547616372207, 12.09730201762757, 0.0, 1.0, 19.49744478049703], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.3933333333333334, 0.0, 0.0, 1.0, 0.9893537376745815, 0.1209730201762757, 0.0, 1.0, 0.13926746271783594], 
reward next is 0.8607, 
noisyNet noise sample is [array([-0.44006377], dtype=float32), 0.13286015]. 
=============================================
[2019-04-01 20:02:48,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:02:48,431] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:02:48,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run34
[2019-04-01 20:02:50,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:02:50,064] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1320
[2019-04-01 20:02:50,073] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.333333333333333, 31.33333333333334, 118.1666666666667, 842.8333333333334, 26.0, 25.07256390110699, 8.605510679952841, 0.0, 1.0, 12.85353626379562], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4191600.0000, 
sim time next is 4192200.0000, 
raw observation next is [1.5, 32.0, 118.0, 847.0, 26.0, 25.08923492708344, 8.656936343521082, 0.0, 1.0, 13.55054005475879], 
processed observation next is [0.0, 0.5217391304347826, 0.5041551246537397, 0.32, 0.3933333333333333, 0.9359116022099447, 1.0, 0.8698907038690626, 0.08656936343521082, 0.0, 1.0, 0.09678957181970564], 
reward next is 0.9032, 
noisyNet noise sample is [array([-1.1549027], dtype=float32), -0.16216436]. 
=============================================
[2019-04-01 20:02:50,445] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:02:50,448] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0728
[2019-04-01 20:02:50,462] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.86666666666667, 39.33333333333334, 113.6666666666667, 762.8333333333333, 26.0, 27.49872530939651, 19.65523602330367, 1.0, 1.0, 7.368676029534664], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4357200.0000, 
sim time next is 4357800.0000, 
raw observation next is [11.3, 38.0, 115.0, 780.0, 26.0, 27.59134761629115, 20.12807983205133, 1.0, 1.0, 6.594166838516332], 
processed observation next is [1.0, 0.43478260869565216, 0.7756232686980611, 0.38, 0.38333333333333336, 0.861878453038674, 1.0, 1.2273353737558783, 0.2012807983205133, 1.0, 1.0, 0.04710119170368809], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6261468], dtype=float32), 0.2032792]. 
=============================================
[2019-04-01 20:02:52,461] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:02:52,461] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9240
[2019-04-01 20:02:52,482] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.36568846723343, 7.374903724958235, 0.0, 1.0, 39.73383074642118], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4251000.0000, 
sim time next is 4251600.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.36731486831319, 7.452139970764788, 0.0, 1.0, 39.72699122663432], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.45, 0.0, 0.0, 1.0, 0.9096164097590272, 0.07452139970764789, 0.0, 1.0, 0.28376422304738796], 
reward next is 0.7162, 
noisyNet noise sample is [array([-0.96915513], dtype=float32), 1.137305]. 
=============================================
[2019-04-01 20:02:53,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 4.7907951e-37 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 3.3875478e-26], sum to 1.0000
[2019-04-01 20:02:53,538] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2417
[2019-04-01 20:02:53,566] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.65, 82.0, 120.0, 232.0, 26.0, 25.59450328489786, 11.93024216879803, 1.0, 1.0, 18.76505197016642], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4437000.0000, 
sim time next is 4437600.0000, 
raw observation next is [1.533333333333333, 82.66666666666667, 127.5, 198.5, 26.0, 25.97148298535588, 12.56256211657112, 1.0, 1.0, 15.64440632319765], 
processed observation next is [1.0, 0.34782608695652173, 0.505078485687904, 0.8266666666666667, 0.425, 0.21933701657458562, 1.0, 0.9959261407651258, 0.12562562116571122, 1.0, 1.0, 0.11174575945141178], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6998795], dtype=float32), -0.048228174]. 
=============================================
[2019-04-01 20:03:02,269] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.2242967e-33 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:03:02,281] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8800
[2019-04-01 20:03:02,300] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4, 72.5, 0.0, 0.0, 26.0, 25.35353145651382, 10.14369409977797, 0.0, 1.0, 43.16341528741187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4491000.0000, 
sim time next is 4491600.0000, 
raw observation next is [-0.4333333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.37716228139444, 10.13382772140123, 0.0, 1.0, 43.05533606278964], 
processed observation next is [1.0, 1.0, 0.45060018467220686, 0.7266666666666667, 0.0, 0.0, 1.0, 0.9110231830563487, 0.1013382772140123, 0.0, 1.0, 0.3075381147342117], 
reward next is 0.6925, 
noisyNet noise sample is [array([-2.6381087], dtype=float32), -0.8351404]. 
=============================================
[2019-04-01 20:03:02,806] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:03:02,810] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6356
[2019-04-01 20:03:02,826] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 42.33333333333333, 0.0, 0.0, 26.0, 25.12073367885472, 8.072073320534685, 0.0, 1.0, 42.67498646189276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4909200.0000, 
sim time next is 4909800.0000, 
raw observation next is [1.0, 41.16666666666666, 0.0, 0.0, 26.0, 25.23511126216569, 8.341799691777615, 0.0, 1.0, 40.64342451389971], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.41166666666666657, 0.0, 0.0, 1.0, 0.8907301803093846, 0.08341799691777614, 0.0, 1.0, 0.29031017509928364], 
reward next is 0.7097, 
noisyNet noise sample is [array([0.77928], dtype=float32), -1.331951]. 
=============================================
[2019-04-01 20:03:08,111] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:08,111] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:08,130] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run34
[2019-04-01 20:03:10,529] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:03:10,531] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0144
[2019-04-01 20:03:10,587] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 67.0, 167.0, 524.0, 26.0, 25.70107497695741, 10.19856692095423, 0.0, 1.0, 28.80765559832881], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4787400.0000, 
sim time next is 4788000.0000, 
raw observation next is [-3.0, 65.0, 163.5, 575.5, 26.0, 25.65788699123564, 10.08996204449201, 0.0, 1.0, 26.96941874410512], 
processed observation next is [0.0, 0.43478260869565216, 0.3795013850415513, 0.65, 0.545, 0.6359116022099448, 1.0, 0.9511267130336627, 0.1008996204449201, 0.0, 1.0, 0.19263870531503657], 
reward next is 0.8074, 
noisyNet noise sample is [array([0.03300585], dtype=float32), -0.31709424]. 
=============================================
[2019-04-01 20:03:10,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[68.56582 ]
 [68.73444 ]
 [68.6549  ]
 [68.21709 ]
 [67.833466]], R is [[68.77102661]
 [68.87754822]
 [68.96843719]
 [69.04284668]
 [69.09980011]].
[2019-04-01 20:03:11,723] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.1716612e-29 0.0000000e+00
 1.0000000e+00 1.2635386e-32], sum to 1.0000
[2019-04-01 20:03:11,724] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7223
[2019-04-01 20:03:11,747] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.166666666666667, 84.83333333333334, 0.0, 0.0, 26.0, 25.37582746951998, 10.93810881402526, 0.0, 1.0, 42.8015244263968], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4741800.0000, 
sim time next is 4742400.0000, 
raw observation next is [-2.333333333333333, 84.66666666666667, 0.0, 0.0, 26.0, 25.42400283554589, 11.02781813012231, 0.0, 1.0, 42.34703258632739], 
processed observation next is [1.0, 0.9130434782608695, 0.3979686057248385, 0.8466666666666667, 0.0, 0.0, 1.0, 0.9177146907922697, 0.11027818130122309, 0.0, 1.0, 0.3024788041880528], 
reward next is 0.6975, 
noisyNet noise sample is [array([0.7988255], dtype=float32), -0.9173661]. 
=============================================
[2019-04-01 20:03:12,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:12,142] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:12,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run34
[2019-04-01 20:03:15,323] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.12286276e-32 1.08663014e-29 5.37724668e-35 0.00000000e+00
 5.87688048e-32 9.99748766e-01 2.51181016e-04], sum to 1.0000
[2019-04-01 20:03:15,324] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6357
[2019-04-01 20:03:15,374] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.63333333333333, 69.0, 25.0, 325.8333333333334, 26.0, 25.14204127033756, 7.139662995029812, 1.0, 1.0, 62.86764520667337], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 289200.0000, 
sim time next is 289800.0000, 
raw observation next is [-12.55, 68.5, 30.0, 386.0, 26.0, 25.47883117286318, 7.459522758382175, 1.0, 1.0, 51.53145126238974], 
processed observation next is [1.0, 0.34782608695652173, 0.11495844875346259, 0.685, 0.1, 0.4265193370165746, 1.0, 0.9255473104090258, 0.07459522758382174, 1.0, 1.0, 0.36808179473135527], 
reward next is 0.6319, 
noisyNet noise sample is [array([0.23158579], dtype=float32), -0.33754742]. 
=============================================
[2019-04-01 20:03:16,176] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:03:16,177] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5043
[2019-04-01 20:03:16,231] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.333333333333334, 73.0, 165.6666666666667, 420.6666666666667, 26.0, 25.76113205339961, 10.19915228223947, 0.0, 1.0, 35.36635166868077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4785600.0000, 
sim time next is 4786200.0000, 
raw observation next is [-4.0, 71.0, 174.0, 421.0, 26.0, 25.72730981709435, 10.10531054317656, 0.0, 1.0, 33.02666339235881], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.58, 0.46519337016574586, 1.0, 0.9610442595849074, 0.10105310543176561, 0.0, 1.0, 0.23590473851684868], 
reward next is 0.7641, 
noisyNet noise sample is [array([-0.3901084], dtype=float32), 1.1282691]. 
=============================================
[2019-04-01 20:03:21,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:21,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:21,468] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run34
[2019-04-01 20:03:23,647] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:03:23,649] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4997
[2019-04-01 20:03:23,664] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 67.33333333333334, 0.0, 0.0, 26.0, 24.76572389002897, 6.123574871819389, 0.0, 1.0, 39.07297380522721], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4857600.0000, 
sim time next is 4858200.0000, 
raw observation next is [-3.5, 65.5, 0.0, 0.0, 26.0, 24.73457861322764, 6.054484006288864, 0.0, 1.0, 39.10640447515945], 
processed observation next is [0.0, 0.21739130434782608, 0.36565096952908593, 0.655, 0.0, 0.0, 1.0, 0.8192255161753772, 0.06054484006288864, 0.0, 1.0, 0.2793314605368532], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.7041772], dtype=float32), -0.015419203]. 
=============================================
[2019-04-01 20:03:24,136] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:03:24,150] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0521
[2019-04-01 20:03:24,164] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 39.0, 0.0, 0.0, 26.0, 25.39963594712715, 7.642131549864589, 0.0, 1.0, 35.0300343519413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4921200.0000, 
sim time next is 4921800.0000, 
raw observation next is [0.1666666666666667, 39.16666666666666, 0.0, 0.0, 26.0, 25.34938474451118, 7.598961587695023, 0.0, 1.0, 37.52969997704141], 
processed observation next is [0.0, 1.0, 0.4672206832871654, 0.39166666666666655, 0.0, 0.0, 1.0, 0.9070549635015972, 0.07598961587695023, 0.0, 1.0, 0.2680692855502958], 
reward next is 0.7319, 
noisyNet noise sample is [array([0.3484527], dtype=float32), 1.1730155]. 
=============================================
[2019-04-01 20:03:24,289] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 2.1206448e-38 1.3886718e-12 0.0000000e+00
 1.0000000e+00 1.7324065e-14], sum to 1.0000
[2019-04-01 20:03:24,289] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3705
[2019-04-01 20:03:24,300] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 26.0, 26.95345888446801, 21.55432466236792, 1.0, 1.0, 2.636887740719815], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5077800.0000, 
sim time next is 5078400.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 26.0, 27.15377102637295, 21.9416201198772, 1.0, 1.0, 2.535859176252028], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.0, 0.0, 1.0, 1.164824432338993, 0.219416201198772, 1.0, 1.0, 0.01811327983037163], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21026488], dtype=float32), 0.15833792]. 
=============================================
[2019-04-01 20:03:26,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:26,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:26,052] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run34
[2019-04-01 20:03:26,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:26,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:26,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run34
[2019-04-01 20:03:29,559] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:29,560] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:29,567] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run34
[2019-04-01 20:03:29,599] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:29,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:29,627] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run34
[2019-04-01 20:03:29,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:29,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:29,704] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run34
[2019-04-01 20:03:30,437] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 2.018362e-14], sum to 1.0000
[2019-04-01 20:03:30,439] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5798
[2019-04-01 20:03:30,478] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.7, 33.83333333333334, 110.6666666666667, 0.0, 26.0, 25.54647595848096, 7.043907232845837, 1.0, 1.0, 24.51488819315014], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 481800.0000, 
sim time next is 482400.0000, 
raw observation next is [-0.6, 35.0, 106.5, 0.0, 26.0, 25.63730827917538, 7.072641492546853, 1.0, 1.0, 23.14125495348161], 
processed observation next is [1.0, 0.6086956521739131, 0.44598337950138506, 0.35, 0.355, 0.0, 1.0, 0.9481868970250541, 0.07072641492546854, 1.0, 1.0, 0.16529467823915436], 
reward next is 0.8347, 
noisyNet noise sample is [array([0.8460132], dtype=float32), -0.19927907]. 
=============================================
[2019-04-01 20:03:31,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:31,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:31,444] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run34
[2019-04-01 20:03:31,668] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:31,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:31,672] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run34
[2019-04-01 20:03:31,784] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:31,784] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:31,788] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run34
[2019-04-01 20:03:32,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:32,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:32,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run34
[2019-04-01 20:03:33,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:33,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:33,047] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run34
[2019-04-01 20:03:33,503] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:03:33,503] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6403
[2019-04-01 20:03:33,531] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.83333333333333, 17.33333333333334, 0.0, 0.0, 26.0, 27.35435275194626, 22.79406773168728, 1.0, 1.0, 4.761168843646902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5080200.0000, 
sim time next is 5080800.0000, 
raw observation next is [10.66666666666667, 17.66666666666667, 0.0, 0.0, 26.0, 27.35529933859481, 22.46550974391646, 0.0, 1.0, 4.031387957895413], 
processed observation next is [1.0, 0.8260869565217391, 0.7580794090489382, 0.17666666666666672, 0.0, 0.0, 1.0, 1.1936141912278302, 0.2246550974391646, 0.0, 1.0, 0.028795628270681523], 
reward next is 0.9712, 
noisyNet noise sample is [array([-2.6796339], dtype=float32), 0.69811565]. 
=============================================
[2019-04-01 20:03:35,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:35,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:35,368] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run34
[2019-04-01 20:03:35,694] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:03:35,695] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8021
[2019-04-01 20:03:35,779] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.699999999999999, 93.0, 62.5, 0.0, 26.0, 23.9841518135754, 5.629582614547732, 0.0, 1.0, 55.24979657106073], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 38400.0000, 
sim time next is 39000.0000, 
raw observation next is [7.7, 93.0, 65.0, 0.0, 26.0, 24.06922289734134, 5.692092001118229, 0.0, 1.0, 55.09777267384202], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.21666666666666667, 0.0, 1.0, 0.7241746996201915, 0.05692092001118229, 0.0, 1.0, 0.3935555190988716], 
reward next is 0.6064, 
noisyNet noise sample is [array([-0.04058603], dtype=float32), 0.12091177]. 
=============================================
[2019-04-01 20:03:35,783] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[72.67745]
 [72.67334]
 [72.62701]
 [72.66568]
 [72.66394]], R is [[72.56303406]
 [72.44276428]
 [72.32229614]
 [72.20145416]
 [72.08078766]].
[2019-04-01 20:03:36,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:03:36,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:03:36,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run34
[2019-04-01 20:03:42,939] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:03:42,940] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7556
[2019-04-01 20:03:42,975] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.6, 95.5, 0.0, 0.0, 22.0, 18.88656820865644, 26.6713327023333, 0.0, 1.0, 56.18013684655242], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1800.0000, 
sim time next is 2400.0000, 
raw observation next is [4.8, 95.66666666666667, 0.0, 0.0, 23.0, 19.10796852867617, 24.62406098430766, 0.0, 1.0, 48.95296385093867], 
processed observation next is [0.0, 0.0, 0.5955678670360112, 0.9566666666666667, 0.0, 0.0, 0.5714285714285714, 0.015424075525167271, 0.2462406098430766, 0.0, 1.0, 0.34966402750670483], 
reward next is 0.6503, 
noisyNet noise sample is [array([0.9678478], dtype=float32), -2.1257129]. 
=============================================
[2019-04-01 20:03:45,805] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:03:45,805] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8834
[2019-04-01 20:03:45,830] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 91.0, 0.0, 0.0, 26.0, 24.25199256736319, 5.645863470277857, 0.0, 1.0, 41.50880037644065], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 93600.0000, 
sim time next is 94200.0000, 
raw observation next is [-1.883333333333333, 90.33333333333334, 0.0, 0.0, 26.0, 24.24738362777365, 5.609782382017362, 0.0, 1.0, 41.66281896757927], 
processed observation next is [1.0, 0.08695652173913043, 0.4104339796860573, 0.9033333333333334, 0.0, 0.0, 1.0, 0.749626232539093, 0.056097823820173615, 0.0, 1.0, 0.2975915640541376], 
reward next is 0.7024, 
noisyNet noise sample is [array([-0.89352363], dtype=float32), -0.91990304]. 
=============================================
[2019-04-01 20:03:50,692] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:03:50,693] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6136
[2019-04-01 20:03:50,725] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.566666666666666, 72.0, 0.0, 0.0, 26.0, 23.77223660711671, 5.494143042746015, 0.0, 1.0, 43.83818963029462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 174000.0000, 
sim time next is 174600.0000, 
raw observation next is [-8.65, 72.5, 0.0, 0.0, 26.0, 23.77399434757785, 5.479102286129486, 0.0, 1.0, 43.79454719736235], 
processed observation next is [1.0, 0.0, 0.22299168975069253, 0.725, 0.0, 0.0, 1.0, 0.6819991925111216, 0.05479102286129486, 0.0, 1.0, 0.3128181942668739], 
reward next is 0.6872, 
noisyNet noise sample is [array([-0.34106466], dtype=float32), -0.0058585433]. 
=============================================
[2019-04-01 20:03:51,831] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4288653e-23 1.3795456e-23
 1.0000000e+00 9.1839331e-10], sum to 1.0000
[2019-04-01 20:03:51,831] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2583
[2019-04-01 20:03:51,878] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 26.0, 25.06696567612856, 7.869015015186537, 0.0, 1.0, 46.1083604507152], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 159600.0000, 
sim time next is 160200.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 26.0, 24.92625871930041, 7.524466314996846, 0.0, 1.0, 45.65723960629556], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 1.0, 0.8466083884714871, 0.07524466314996846, 0.0, 1.0, 0.3261231400449683], 
reward next is 0.6739, 
noisyNet noise sample is [array([0.48144248], dtype=float32), 0.19852564]. 
=============================================
[2019-04-01 20:03:51,955] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.5632904e-37 5.0388831e-36 4.9087218e-10 0.0000000e+00
 1.0000000e+00 2.7792234e-14], sum to 1.0000
[2019-04-01 20:03:51,955] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6435
[2019-04-01 20:03:52,005] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.15, 61.5, 87.0, 512.0, 26.0, 25.94921607847397, 8.847453284438785, 1.0, 1.0, 31.2024871826648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 297000.0000, 
sim time next is 297600.0000, 
raw observation next is [-10.96666666666667, 61.0, 90.16666666666666, 536.3333333333334, 26.0, 25.96009564573635, 8.847264887245778, 1.0, 1.0, 29.39279124847075], 
processed observation next is [1.0, 0.43478260869565216, 0.15881809787626952, 0.61, 0.3005555555555555, 0.592633517495396, 1.0, 0.9942993779623356, 0.08847264887245779, 1.0, 1.0, 0.2099485089176482], 
reward next is 0.7901, 
noisyNet noise sample is [array([-1.7250754], dtype=float32), -0.17226362]. 
=============================================
[2019-04-01 20:04:04,372] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 5.808597e-21 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 20:04:04,373] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6036
[2019-04-01 20:04:04,413] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.8, 77.0, 0.0, 0.0, 26.0, 24.63905041576356, 6.802947343316386, 0.0, 1.0, 46.96746551769535], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 334800.0000, 
sim time next is 335400.0000, 
raw observation next is [-12.9, 77.83333333333334, 0.0, 0.0, 26.0, 24.56949876307968, 6.638872002352151, 0.0, 1.0, 47.00500188596634], 
processed observation next is [1.0, 0.9130434782608695, 0.10526315789473682, 0.7783333333333334, 0.0, 0.0, 1.0, 0.7956426804399541, 0.0663887200235215, 0.0, 1.0, 0.3357500134711881], 
reward next is 0.6642, 
noisyNet noise sample is [array([1.3805221], dtype=float32), 0.22322136]. 
=============================================
[2019-04-01 20:04:08,072] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:04:08,072] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6912
[2019-04-01 20:04:08,090] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.0, 76.33333333333334, 0.0, 0.0, 26.0, 22.08007589327058, 8.941718767666883, 0.0, 1.0, 47.85651139322508], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 366000.0000, 
sim time next is 366600.0000, 
raw observation next is [-16.1, 77.16666666666666, 0.0, 0.0, 26.0, 21.98774895550582, 9.335434879200799, 0.0, 1.0, 47.95836198641961], 
processed observation next is [1.0, 0.21739130434782608, 0.01662049861495839, 0.7716666666666666, 0.0, 0.0, 1.0, 0.42682127935797404, 0.09335434879200799, 0.0, 1.0, 0.3425597284744258], 
reward next is 0.6574, 
noisyNet noise sample is [array([-0.34261063], dtype=float32), -1.5026368]. 
=============================================
[2019-04-01 20:04:14,232] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-01 20:04:14,240] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 20:04:14,240] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 20:04:14,240] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:04:14,241] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 20:04:14,242] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:04:14,245] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:04:14,847] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run46
[2019-04-01 20:04:15,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run46
[2019-04-01 20:04:15,809] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run46
[2019-04-01 20:05:55,624] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 20:06:17,773] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.0259887], dtype=float32), -1.1680417]
[2019-04-01 20:06:17,773] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [9.666666666666668, 19.0, 0.0, 0.0, 26.0, 27.10381855527907, 20.18791942222861, 0.0, 1.0, 4.915482229708673]
[2019-04-01 20:06:17,773] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 20:06:17,774] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.6307765741096149
[2019-04-01 20:06:17,867] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 20:06:19,998] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 20:06:21,022] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 4500000, evaluation results [4500000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 20:06:21,779] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:06:21,781] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1068
[2019-04-01 20:06:21,829] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 44.0, 0.0, 0.0, 26.0, 22.51397860272772, 7.416401339727419, 0.0, 1.0, 45.90086884964033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 453600.0000, 
sim time next is 454200.0000, 
raw observation next is [-9.316666666666666, 43.83333333333334, 0.0, 0.0, 26.0, 22.5156689733121, 7.433214276575022, 0.0, 1.0, 45.81442664754865], 
processed observation next is [1.0, 0.2608695652173913, 0.20452446906740537, 0.4383333333333334, 0.0, 0.0, 1.0, 0.5022384247588713, 0.07433214276575022, 0.0, 1.0, 0.32724590462534753], 
reward next is 0.6728, 
noisyNet noise sample is [array([1.1228453], dtype=float32), -1.3952861]. 
=============================================
[2019-04-01 20:06:28,795] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.9146224e-30], sum to 1.0000
[2019-04-01 20:06:28,796] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6930
[2019-04-01 20:06:28,850] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.133333333333333, 80.16666666666667, 132.6666666666667, 462.3333333333334, 26.0, 25.01389106387748, 7.817756867832277, 0.0, 1.0, 28.73997323771699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 564600.0000, 
sim time next is 565200.0000, 
raw observation next is [-1.2, 80.0, 134.0, 495.5, 26.0, 24.97085535805458, 7.751453655349266, 0.0, 1.0, 29.70680093502609], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.44666666666666666, 0.5475138121546961, 1.0, 0.8529793368649398, 0.07751453655349266, 0.0, 1.0, 0.21219143525018636], 
reward next is 0.7878, 
noisyNet noise sample is [array([-1.3389488], dtype=float32), -0.26457855]. 
=============================================
[2019-04-01 20:06:37,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:06:37,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3602
[2019-04-01 20:06:37,072] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.8, 100.0, 42.16666666666667, 0.0, 26.0, 25.65646243619524, 9.719882545167723, 1.0, 1.0, 13.89910172658373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1502400.0000, 
sim time next is 1503000.0000, 
raw observation next is [1.9, 100.0, 47.0, 0.0, 26.0, 25.74768948354812, 9.988144785381849, 1.0, 1.0, 13.20248339794204], 
processed observation next is [1.0, 0.391304347826087, 0.515235457063712, 1.0, 0.15666666666666668, 0.0, 1.0, 0.963955640506874, 0.09988144785381849, 1.0, 1.0, 0.09430345284244314], 
reward next is 0.9057, 
noisyNet noise sample is [array([0.23944564], dtype=float32), -0.11459862]. 
=============================================
[2019-04-01 20:06:37,088] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[67.278595]
 [68.32736 ]
 [68.39255 ]
 [68.356186]
 [67.78914 ]], R is [[66.5223999 ]
 [66.75789642]
 [66.98627472]
 [67.20764923]
 [67.42121887]].
[2019-04-01 20:06:42,397] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:06:42,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9451
[2019-04-01 20:06:42,412] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.383333333333333, 75.83333333333333, 0.0, 0.0, 26.0, 24.26089793349499, 5.270906815280462, 0.0, 1.0, 41.03887011032518], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 708600.0000, 
sim time next is 709200.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.26719408582279, 5.263431256245831, 0.0, 1.0, 41.02885275313388], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 1.0, 0.7524562979746844, 0.05263431256245831, 0.0, 1.0, 0.2930632339509563], 
reward next is 0.7069, 
noisyNet noise sample is [array([1.078884], dtype=float32), 0.5137209]. 
=============================================
[2019-04-01 20:06:52,083] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:06:52,087] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2062
[2019-04-01 20:06:52,122] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.0411335137843, 7.7566623974072, 0.0, 1.0, 45.99774937097657], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1800000.0000, 
sim time next is 1800600.0000, 
raw observation next is [-4.583333333333333, 83.5, 0.0, 0.0, 26.0, 25.03182018233533, 7.673235556337471, 0.0, 1.0, 45.94852667348997], 
processed observation next is [0.0, 0.8695652173913043, 0.3356417359187443, 0.835, 0.0, 0.0, 1.0, 0.8616885974764757, 0.07673235556337471, 0.0, 1.0, 0.3282037619534998], 
reward next is 0.6718, 
noisyNet noise sample is [array([-0.9017568], dtype=float32), 1.007152]. 
=============================================
[2019-04-01 20:06:54,451] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:06:54,452] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6124
[2019-04-01 20:06:54,479] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.616666666666667, 78.50000000000001, 0.0, 0.0, 26.0, 24.52650417560931, 5.849674268699488, 0.0, 1.0, 38.86032788496138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 875400.0000, 
sim time next is 876000.0000, 
raw observation next is [-1.533333333333333, 78.0, 0.0, 0.0, 26.0, 24.51741530995438, 5.870439593856051, 0.0, 1.0, 38.81870218109704], 
processed observation next is [1.0, 0.13043478260869565, 0.42012927054478305, 0.78, 0.0, 0.0, 1.0, 0.78820218713634, 0.05870439593856051, 0.0, 1.0, 0.27727644415069314], 
reward next is 0.7227, 
noisyNet noise sample is [array([0.5690271], dtype=float32), 1.1979856]. 
=============================================
[2019-04-01 20:06:54,513] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[70.63859 ]
 [70.68965 ]
 [70.757256]
 [70.83286 ]
 [70.789276]], R is [[70.62818909]
 [70.64433289]
 [70.66011047]
 [70.67539978]
 [70.69036102]].
[2019-04-01 20:06:59,014] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:06:59,020] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9028
[2019-04-01 20:06:59,030] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 26.0, 25.65632333307473, 13.36105915457137, 0.0, 1.0, 22.27512193866749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1135200.0000, 
sim time next is 1135800.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 26.0, 25.64328321395885, 13.30874847271424, 0.0, 1.0, 23.44098500404233], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 1.0, 0.9490404591369784, 0.1330874847271424, 0.0, 1.0, 0.16743560717173092], 
reward next is 0.8326, 
noisyNet noise sample is [array([-0.82816154], dtype=float32), -0.3600012]. 
=============================================
[2019-04-01 20:07:00,164] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 6.915339e-31 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 20:07:00,167] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2685
[2019-04-01 20:07:00,178] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.0, 57.0, 0.0, 0.0, 26.0, 26.39244570501097, 17.28165931929359, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1105200.0000, 
sim time next is 1105800.0000, 
raw observation next is [14.8, 57.5, 0.0, 0.0, 26.0, 26.38478038067739, 17.329372873887, 1.0, 1.0, 15.1844461188284], 
processed observation next is [1.0, 0.8260869565217391, 0.8725761772853187, 0.575, 0.0, 0.0, 1.0, 1.0549686258110558, 0.17329372873887, 1.0, 1.0, 0.10846032942020287], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.0178451], dtype=float32), 0.35221928]. 
=============================================
[2019-04-01 20:07:03,952] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:07:03,954] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0735
[2019-04-01 20:07:03,966] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 58.5, 0.0, 0.0, 26.0, 26.17668794294715, 16.1536471837745, 0.0, 1.0, 5.554015331550954], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1107000.0000, 
sim time next is 1107600.0000, 
raw observation next is [14.2, 59.0, 0.0, 0.0, 26.0, 26.06138752713763, 15.61446433806597, 1.0, 1.0, 5.498738632807236], 
processed observation next is [1.0, 0.8260869565217391, 0.8559556786703602, 0.59, 0.0, 0.0, 1.0, 1.008769646733947, 0.1561446433806597, 1.0, 1.0, 0.039276704520051685], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.01120441], dtype=float32), 1.2277313]. 
=============================================
[2019-04-01 20:07:07,383] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:07:07,387] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5057
[2019-04-01 20:07:07,395] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.3, 64.0, 0.0, 0.0, 26.0, 24.85992897267146, 9.411881906448665, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1186200.0000, 
sim time next is 1186800.0000, 
raw observation next is [18.3, 63.66666666666667, 0.0, 0.0, 26.0, 24.83992720453042, 9.33605341460756, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.6366666666666667, 0.0, 0.0, 1.0, 0.834275314932917, 0.0933605341460756, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.611171], dtype=float32), -0.39231992]. 
=============================================
[2019-04-01 20:07:16,909] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8806530e-34 1.9939659e-33 3.7203782e-32 2.4227407e-27 1.7148738e-23
 9.9999881e-01 1.1672997e-06], sum to 1.0000
[2019-04-01 20:07:16,910] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9720
[2019-04-01 20:07:16,923] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 109.5, 0.0, 26.0, 25.72107887673209, 10.89891642990677, 1.0, 1.0, 9.470417242871184], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1342800.0000, 
sim time next is 1343400.0000, 
raw observation next is [1.1, 92.0, 108.3333333333333, 0.0, 26.0, 25.65135415361538, 10.8987298692622, 1.0, 1.0, 9.502536757140849], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.361111111111111, 0.0, 1.0, 0.9501934505164827, 0.108987298692622, 1.0, 1.0, 0.06787526255100607], 
reward next is 0.5726, 
noisyNet noise sample is [array([-0.31973967], dtype=float32), 0.22909075]. 
=============================================
[2019-04-01 20:07:19,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.         0.         0.         0.00453664 0.         0.9925292
 0.00293404], sum to 1.0000
[2019-04-01 20:07:19,922] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3427
[2019-04-01 20:07:19,945] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.83508980559717, 10.66735322778401, 1.0, 1.0, 18.36050458004952], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1704600.0000, 
sim time next is 1705200.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 25.70356044244372, 11.15548860235705, 1.0, 1.0, 17.45389829959805], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.0, 0.0, 1.0, 0.9576514917776743, 0.1115548860235705, 1.0, 1.0, 0.12467070213998606], 
reward next is 0.4131, 
noisyNet noise sample is [array([-0.00993344], dtype=float32), -0.6496852]. 
=============================================
[2019-04-01 20:07:39,160] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:07:39,161] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7570
[2019-04-01 20:07:39,174] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 26.0, 24.0673908980003, 5.319506790998723, 0.0, 1.0, 44.74088701290464], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1911600.0000, 
sim time next is 1912200.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.94893431964002, 5.307553753112745, 0.0, 1.0, 43.86120591069826], 
processed observation next is [1.0, 0.13043478260869565, 0.2299168975069252, 0.78, 0.0, 0.0, 1.0, 0.7069906170914315, 0.053075537531127454, 0.0, 1.0, 0.313294327933559], 
reward next is 0.6867, 
noisyNet noise sample is [array([1.5218879], dtype=float32), 2.2837462]. 
=============================================
[2019-04-01 20:07:58,019] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 8.7778795e-35], sum to 1.0000
[2019-04-01 20:07:58,019] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5613
[2019-04-01 20:07:58,071] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.2, 84.0, 71.0, 0.0, 26.0, 26.25601817879022, 10.7472452748642, 1.0, 1.0, 25.38363345952219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2043000.0000, 
sim time next is 2043600.0000, 
raw observation next is [-4.1, 83.33333333333334, 64.5, 0.0, 26.0, 25.44007558330317, 9.141076636476823, 1.0, 1.0, 24.23921986471926], 
processed observation next is [1.0, 0.6521739130434783, 0.3490304709141275, 0.8333333333333335, 0.215, 0.0, 1.0, 0.9200107976147385, 0.09141076636476823, 1.0, 1.0, 0.17313728474799472], 
reward next is 0.8269, 
noisyNet noise sample is [array([-0.12253526], dtype=float32), 0.52179044]. 
=============================================
[2019-04-01 20:08:00,447] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:08:00,449] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8483
[2019-04-01 20:08:00,490] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 61.0, 144.6666666666667, 228.6666666666667, 26.0, 25.92415823698686, 8.811404533956418, 1.0, 1.0, 28.90985851470057], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2798400.0000, 
sim time next is 2799000.0000, 
raw observation next is [-4.5, 59.5, 152.0, 233.0, 26.0, 25.94179394063506, 8.91851653855567, 1.0, 1.0, 26.63215238907249], 
processed observation next is [1.0, 0.391304347826087, 0.3379501385041552, 0.595, 0.5066666666666667, 0.2574585635359116, 1.0, 0.9916848486621512, 0.0891851653855567, 1.0, 1.0, 0.19022965992194638], 
reward next is 0.8098, 
noisyNet noise sample is [array([1.649768], dtype=float32), -0.9959724]. 
=============================================
[2019-04-01 20:08:00,497] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[66.104355]
 [66.40801 ]
 [66.75282 ]
 [67.141365]
 [67.27592 ]], R is [[66.08876801]
 [66.22138214]
 [66.33495331]
 [66.42870331]
 [66.50135803]].
[2019-04-01 20:08:00,630] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:08:00,632] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7924
[2019-04-01 20:08:00,663] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 91.00000000000001, 0.0, 0.0, 26.0, 24.80087064704836, 6.92950081724283, 0.0, 1.0, 42.11268837953904], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2074200.0000, 
sim time next is 2074800.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 26.0, 24.76844208739948, 7.054726576612739, 0.0, 1.0, 42.73534470290401], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 1.0, 0.8240631553427831, 0.0705472657661274, 0.0, 1.0, 0.3052524621636001], 
reward next is 0.6947, 
noisyNet noise sample is [array([0.26366338], dtype=float32), 0.2990451]. 
=============================================
[2019-04-01 20:08:07,627] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.8744427e-37], sum to 1.0000
[2019-04-01 20:08:07,627] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3065
[2019-04-01 20:08:07,645] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.1, 67.0, 0.0, 0.0, 26.0, 24.23384037182818, 5.497375778435445, 0.0, 1.0, 40.69221271427716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2356200.0000, 
sim time next is 2356800.0000, 
raw observation next is [-3.2, 67.66666666666667, 0.0, 0.0, 26.0, 24.19765232883455, 5.454832948505516, 0.0, 1.0, 40.75835538017586], 
processed observation next is [0.0, 0.2608695652173913, 0.37396121883656513, 0.6766666666666667, 0.0, 0.0, 1.0, 0.7425217612620786, 0.054548329485055155, 0.0, 1.0, 0.29113110985839896], 
reward next is 0.7089, 
noisyNet noise sample is [array([-0.08746409], dtype=float32), 2.3941693]. 
=============================================
[2019-04-01 20:08:14,347] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0285258e-36 0.0000000e+00
 1.0000000e+00 2.9079895e-17], sum to 1.0000
[2019-04-01 20:08:14,347] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1267
[2019-04-01 20:08:14,383] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666666, 70.16666666666667, 0.0, 0.0, 26.0, 25.29158917522591, 8.328779400243898, 1.0, 1.0, 30.7463220759128], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2229000.0000, 
sim time next is 2229600.0000, 
raw observation next is [-4.733333333333333, 70.33333333333334, 0.0, 0.0, 26.0, 25.13829473526745, 7.960553994238228, 1.0, 1.0, 31.83878006926859], 
processed observation next is [1.0, 0.8260869565217391, 0.3314866112650046, 0.7033333333333335, 0.0, 0.0, 1.0, 0.8768992478953501, 0.07960553994238227, 1.0, 1.0, 0.22741985763763278], 
reward next is 0.7726, 
noisyNet noise sample is [array([-0.10363629], dtype=float32), 0.20673208]. 
=============================================
[2019-04-01 20:08:16,375] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:08:16,375] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8860
[2019-04-01 20:08:16,409] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.0, 64.0, 114.5, 727.5, 26.0, 26.02863532693626, 10.08093572242968, 1.0, 1.0, 17.12945969167367], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2718000.0000, 
sim time next is 2718600.0000, 
raw observation next is [-8.833333333333334, 64.0, 113.6666666666667, 745.3333333333334, 26.0, 26.00272388034918, 10.04833477898705, 1.0, 1.0, 16.19399900745986], 
processed observation next is [1.0, 0.4782608695652174, 0.21791320406278855, 0.64, 0.378888888888889, 0.823572744014733, 1.0, 1.0003891257641686, 0.1004833477898705, 1.0, 1.0, 0.11567142148185613], 
reward next is 0.8650, 
noisyNet noise sample is [array([0.63553774], dtype=float32), -1.1913575]. 
=============================================
[2019-04-01 20:08:19,705] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:08:19,709] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3380
[2019-04-01 20:08:19,744] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.8773559879666, 6.909756456584986, 0.0, 1.0, 38.08176584370454], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2337000.0000, 
sim time next is 2337600.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.85920292981394, 6.812970445917337, 0.0, 1.0, 38.1575002283203], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 1.0, 0.8370289899734199, 0.06812970445917337, 0.0, 1.0, 0.27255357305943073], 
reward next is 0.7274, 
noisyNet noise sample is [array([-1.0377613], dtype=float32), 0.17638242]. 
=============================================
[2019-04-01 20:08:22,728] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:08:22,728] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6735
[2019-04-01 20:08:22,742] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 60.5, 0.0, 0.0, 26.0, 22.94668645922317, 6.222151644756352, 0.0, 1.0, 43.5433650369091], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2445000.0000, 
sim time next is 2445600.0000, 
raw observation next is [-9.5, 60.0, 0.0, 0.0, 26.0, 22.9134280781166, 6.287808153165962, 0.0, 1.0, 43.58589021311335], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.6, 0.0, 0.0, 1.0, 0.559061154016657, 0.06287808153165962, 0.0, 1.0, 0.31132778723652393], 
reward next is 0.6887, 
noisyNet noise sample is [array([0.9030684], dtype=float32), -0.40113798]. 
=============================================
[2019-04-01 20:08:31,227] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.6056948e-19 0.0000000e+00
 1.0000000e+00 1.5401058e-36], sum to 1.0000
[2019-04-01 20:08:31,228] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3972
[2019-04-01 20:08:31,275] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 89.66666666666667, 0.0, 0.0, 26.0, 25.17716279395186, 9.866590132214993, 1.0, 1.0, 31.49157808305926], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2920200.0000, 
sim time next is 2920800.0000, 
raw observation next is [-1.0, 87.33333333333334, 0.0, 0.0, 26.0, 25.35527001253613, 9.766621706500842, 0.0, 1.0, 26.00993031491523], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.8733333333333334, 0.0, 0.0, 1.0, 0.9078957160765901, 0.09766621706500843, 0.0, 1.0, 0.1857852165351088], 
reward next is 0.8142, 
noisyNet noise sample is [array([2.9300094], dtype=float32), -1.2237138]. 
=============================================
[2019-04-01 20:08:35,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 8.69185e-19
 1.00000e+00], sum to 1.0000
[2019-04-01 20:08:35,217] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6883
[2019-04-01 20:08:35,278] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 61.5, 113.0, 799.0, 26.0, 25.79959513758431, 8.695726271859675, 1.0, 1.0, 11.2423397246515], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2723400.0000, 
sim time next is 2724000.0000, 
raw observation next is [-6.666666666666667, 60.66666666666666, 112.3333333333333, 797.1666666666667, 26.0, 25.23814600017311, 8.637199104470243, 1.0, 1.0, 41.01402013065551], 
processed observation next is [1.0, 0.5217391304347826, 0.27793167128347185, 0.6066666666666666, 0.37444444444444436, 0.8808471454880296, 1.0, 0.8911637143104443, 0.08637199104470243, 1.0, 1.0, 0.29295728664753934], 
reward next is 0.7070, 
noisyNet noise sample is [array([1.529279], dtype=float32), 0.42220742]. 
=============================================
[2019-04-01 20:08:35,283] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[64.79767 ]
 [64.979034]
 [65.21994 ]
 [65.519005]
 [65.87065 ]], R is [[64.64183044]
 [64.91510773]
 [65.18758392]
 [65.45257568]
 [65.71001434]].
[2019-04-01 20:08:36,849] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:08:36,850] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6519
[2019-04-01 20:08:36,878] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 51.33333333333333, 80.66666666666667, 587.3333333333334, 26.0, 27.02328768556081, 16.95103561932136, 1.0, 1.0, 12.11582208806173], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2734800.0000, 
sim time next is 2735400.0000, 
raw observation next is [-3.166666666666667, 50.66666666666667, 75.33333333333334, 560.6666666666666, 26.0, 27.03988503758397, 16.09205822723907, 1.0, 1.0, 23.58291022333303], 
processed observation next is [1.0, 0.6521739130434783, 0.3748845798707295, 0.5066666666666667, 0.2511111111111111, 0.6195211786372007, 1.0, 1.1485550053691387, 0.16092058227239073, 1.0, 1.0, 0.16844935873809308], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.49947453], dtype=float32), -1.0943288]. 
=============================================
[2019-04-01 20:08:41,439] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 6.04127e-23
 1.00000e+00], sum to 1.0000
[2019-04-01 20:08:41,439] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7129
[2019-04-01 20:08:41,462] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.333333333333334, 62.33333333333334, 112.8333333333333, 796.0, 26.0, 25.87669400256051, 9.828514255679444, 1.0, 1.0, 10.97289142533429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2722800.0000, 
sim time next is 2723400.0000, 
raw observation next is [-7.0, 61.5, 113.0, 799.0, 26.0, 25.79959513758431, 8.695726271859675, 1.0, 1.0, 11.2423397246515], 
processed observation next is [1.0, 0.5217391304347826, 0.2686980609418283, 0.615, 0.37666666666666665, 0.8828729281767956, 1.0, 0.9713707339406155, 0.08695726271859675, 1.0, 1.0, 0.08030242660465357], 
reward next is 0.9197, 
noisyNet noise sample is [array([-1.2582291], dtype=float32), 0.07128551]. 
=============================================
[2019-04-01 20:08:45,829] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.7158496e-32], sum to 1.0000
[2019-04-01 20:08:45,830] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3953
[2019-04-01 20:08:45,869] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 93.0, 105.0, 156.0, 26.0, 25.0847612250916, 7.525383087859342, 1.0, 1.0, 38.031303063916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2881800.0000, 
sim time next is 2882400.0000, 
raw observation next is [1.333333333333333, 93.0, 95.66666666666666, 130.0, 26.0, 25.28729210829715, 7.689720011669057, 1.0, 1.0, 29.39070023483268], 
processed observation next is [1.0, 0.34782608695652173, 0.4995383194829178, 0.93, 0.31888888888888883, 0.143646408839779, 1.0, 0.8981845868995931, 0.07689720011669057, 1.0, 1.0, 0.2099335731059477], 
reward next is 0.7901, 
noisyNet noise sample is [array([0.8796663], dtype=float32), -0.9666605]. 
=============================================
[2019-04-01 20:08:48,355] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:08:48,356] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6405
[2019-04-01 20:08:48,379] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 26.0, 25.27914064494862, 7.976847030137288, 0.0, 1.0, 50.2799694759772], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2854800.0000, 
sim time next is 2855400.0000, 
raw observation next is [1.0, 73.16666666666667, 0.0, 0.0, 26.0, 25.30583387772277, 7.994580991202651, 0.0, 1.0, 49.47195175197548], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7316666666666667, 0.0, 0.0, 1.0, 0.9008334111032527, 0.0799458099120265, 0.0, 1.0, 0.353371083942682], 
reward next is 0.6466, 
noisyNet noise sample is [array([-1.0095863], dtype=float32), -0.02131895]. 
=============================================
[2019-04-01 20:08:54,753] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:08:54,758] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2782
[2019-04-01 20:08:54,789] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 26.0, 25.47670740986487, 7.809559731809237, 0.0, 1.0, 36.86903659820997], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3114600.0000, 
sim time next is 3115200.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 26.0, 25.54729233619803, 7.73780267050537, 0.0, 1.0, 29.85104806217393], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 1.0, 0.0, 0.0, 1.0, 0.9353274765997186, 0.0773780267050537, 0.0, 1.0, 0.21322177187267094], 
reward next is 0.7868, 
noisyNet noise sample is [array([-1.2884464], dtype=float32), -0.36504182]. 
=============================================
[2019-04-01 20:09:02,587] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:09:02,589] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2192
[2019-04-01 20:09:02,619] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.27537712282128, 5.688708775347543, 0.0, 1.0, 43.03588009928642], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3987000.0000, 
sim time next is 3987600.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.24649665112335, 5.663569831080227, 0.0, 1.0, 43.02938419028165], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 1.0, 0.7494995215890502, 0.056635698310802264, 0.0, 1.0, 0.3073527442162975], 
reward next is 0.6926, 
noisyNet noise sample is [array([2.3115413], dtype=float32), 0.52816826]. 
=============================================
[2019-04-01 20:09:06,926] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:09:06,927] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6090
[2019-04-01 20:09:06,949] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 26.0, 25.29471700859033, 7.467136359776589, 0.0, 1.0, 39.87821404135199], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3108600.0000, 
sim time next is 3109200.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 26.0, 25.29860592452792, 7.46526583823529, 0.0, 1.0, 39.56450217469304], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 1.0, 0.8998008463611313, 0.0746526583823529, 0.0, 1.0, 0.28260358696209315], 
reward next is 0.7174, 
noisyNet noise sample is [array([1.0341823], dtype=float32), -0.4674359]. 
=============================================
[2019-04-01 20:09:10,190] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:09:10,191] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4926
[2019-04-01 20:09:10,206] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.37002751700108, 7.538579877058751, 0.0, 1.0, 40.80903692926005], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3730800.0000, 
sim time next is 3731400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.4043912821102, 7.430984507637469, 0.0, 1.0, 40.79332431889064], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 1.0, 0.9149130403014573, 0.0743098450763747, 0.0, 1.0, 0.291380887992076], 
reward next is 0.7086, 
noisyNet noise sample is [array([0.36494163], dtype=float32), -1.3995093]. 
=============================================
[2019-04-01 20:09:14,415] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 7.9332857e-16 1.0000000e+00], sum to 1.0000
[2019-04-01 20:09:14,423] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3941
[2019-04-01 20:09:14,463] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.7, 86.0, 109.0, 752.0, 26.0, 26.27357250557099, 14.69834000567332, 1.0, 1.0, 10.55438435408932], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3234600.0000, 
sim time next is 3235200.0000, 
raw observation next is [-2.6, 84.0, 109.6666666666667, 761.8333333333334, 26.0, 26.2700368112308, 15.03645089751505, 1.0, 1.0, 9.218234200115367], 
processed observation next is [1.0, 0.43478260869565216, 0.3905817174515236, 0.84, 0.3655555555555557, 0.841804788213628, 1.0, 1.0385766873186857, 0.1503645089751505, 1.0, 1.0, 0.06584453000082405], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.623605], dtype=float32), 0.3879895]. 
=============================================
[2019-04-01 20:09:14,697] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:09:14,698] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7452
[2019-04-01 20:09:14,714] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.333333333333333, 61.83333333333333, 0.0, 0.0, 26.0, 24.80901956393799, 6.276015164834763, 0.0, 1.0, 42.1838057463272], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3390600.0000, 
sim time next is 3391200.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.74697789822178, 6.193375554460303, 0.0, 1.0, 42.29269490004405], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6, 0.0, 0.0, 1.0, 0.8209968426031112, 0.06193375554460303, 0.0, 1.0, 0.30209067785745747], 
reward next is 0.6979, 
noisyNet noise sample is [array([0.08413874], dtype=float32), 0.26228854]. 
=============================================
[2019-04-01 20:09:18,575] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1411286e-35 8.3608175e-26 3.6483856e-15 2.8496153e-15 0.0000000e+00
 9.2455316e-22 1.0000000e+00], sum to 1.0000
[2019-04-01 20:09:18,579] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8000
[2019-04-01 20:09:18,597] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 114.3333333333333, 809.6666666666666, 26.0, 25.90687177645809, 11.68517522949844, 1.0, 1.0, 18.82197931673708], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3417000.0000, 
sim time next is 3417600.0000, 
raw observation next is [3.0, 49.0, 113.6666666666667, 807.8333333333334, 26.0, 25.93379534253864, 12.92165667311668, 1.0, 1.0, 18.80782645898975], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.378888888888889, 0.892633517495396, 1.0, 0.9905421917912344, 0.1292165667311668, 1.0, 1.0, 0.1343416175642125], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4034632], dtype=float32), -1.0342916]. 
=============================================
[2019-04-01 20:09:34,283] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.2395714e-26], sum to 1.0000
[2019-04-01 20:09:34,284] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3229
[2019-04-01 20:09:34,294] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.833333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 24.92098382893385, 7.446397280424397, 0.0, 1.0, 19.52444799778179], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3697800.0000, 
sim time next is 3698400.0000, 
raw observation next is [3.666666666666667, 60.33333333333334, 0.0, 0.0, 26.0, 24.89990062274699, 7.693460667190323, 0.0, 1.0, 39.58159000932762], 
processed observation next is [0.0, 0.8260869565217391, 0.564173591874423, 0.6033333333333334, 0.0, 0.0, 1.0, 0.842842946106713, 0.07693460667190323, 0.0, 1.0, 0.2827256429237687], 
reward next is 0.7173, 
noisyNet noise sample is [array([-0.8540978], dtype=float32), -0.26928735]. 
=============================================
[2019-04-01 20:09:39,806] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.3948269e-32 3.0771798e-27 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:09:39,807] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1688
[2019-04-01 20:09:39,826] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666667, 74.0, 40.83333333333333, 25.16666666666667, 26.0, 26.16540636970041, 9.798146426709296, 1.0, 1.0, 13.37905188163517], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4728000.0000, 
sim time next is 4728600.0000, 
raw observation next is [0.5, 75.0, 29.0, 28.0, 26.0, 25.88547357458313, 9.488811159977063, 1.0, 1.0, 12.72047526569585], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.75, 0.09666666666666666, 0.030939226519337018, 1.0, 0.9836390820833044, 0.09488811159977063, 1.0, 1.0, 0.09086053761211321], 
reward next is 0.9091, 
noisyNet noise sample is [array([-0.26795053], dtype=float32), -0.081366286]. 
=============================================
[2019-04-01 20:09:40,061] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:09:40,064] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9047
[2019-04-01 20:09:40,094] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.04350930388539, 7.45029712742615, 0.0, 1.0, 43.33259463048985], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3813000.0000, 
sim time next is 3813600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.99909920891965, 7.516654973950661, 0.0, 1.0, 43.11302095051197], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 1.0, 0.857014172702807, 0.07516654973950661, 0.0, 1.0, 0.30795014964651407], 
reward next is 0.6920, 
noisyNet noise sample is [array([0.15463758], dtype=float32), 0.57163244]. 
=============================================
[2019-04-01 20:09:51,651] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.2614046e-16 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:09:51,652] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2230
[2019-04-01 20:09:51,686] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.166666666666666, 41.66666666666667, 0.0, 0.0, 26.0, 25.78196576212125, 11.79521481991308, 1.0, 1.0, 21.46585977490865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3957000.0000, 
sim time next is 3957600.0000, 
raw observation next is [-6.333333333333333, 42.33333333333334, 0.0, 0.0, 26.0, 25.74591269300517, 11.186874459941, 1.0, 1.0, 22.13507306069252], 
processed observation next is [1.0, 0.8260869565217391, 0.28716528162511545, 0.42333333333333345, 0.0, 0.0, 1.0, 0.9637018132864529, 0.11186874459941, 1.0, 1.0, 0.1581076647192323], 
reward next is 0.3671, 
noisyNet noise sample is [array([-0.1613112], dtype=float32), 0.68319607]. 
=============================================
[2019-04-01 20:09:51,695] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7749063e-36 5.9631827e-34 6.3867706e-23 0.0000000e+00 0.0000000e+00
 1.5170835e-08 1.0000000e+00], sum to 1.0000
[2019-04-01 20:09:51,695] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6682
[2019-04-01 20:09:51,708] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 29.5, 117.0, 803.0, 26.0, 26.57855219084493, 12.40822468720624, 1.0, 1.0, 9.780026021033907], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4962600.0000, 
sim time next is 4963200.0000, 
raw observation next is [2.333333333333333, 29.33333333333334, 117.8333333333333, 810.0, 26.0, 26.61516255719091, 12.81717898428801, 1.0, 1.0, 8.979276661580444], 
processed observation next is [1.0, 0.43478260869565216, 0.5272391505078486, 0.2933333333333334, 0.39277777777777767, 0.8950276243093923, 1.0, 1.0878803653129872, 0.1281717898428801, 1.0, 1.0, 0.06413769043986031], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.16148195], dtype=float32), -1.1890622]. 
=============================================
[2019-04-01 20:09:52,431] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:09:52,432] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9990
[2019-04-01 20:09:52,441] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 50.66666666666667, 0.0, 0.0, 26.0, 24.86395487208551, 6.650629486394055, 0.0, 1.0, 39.13103734638917], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4168200.0000, 
sim time next is 4168800.0000, 
raw observation next is [-4.0, 50.0, 0.0, 0.0, 26.0, 24.82958772519382, 6.571764205092445, 0.0, 1.0, 39.19642331695653], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.5, 0.0, 0.0, 1.0, 0.8327982464562602, 0.06571764205092445, 0.0, 1.0, 0.2799744522639752], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.62673205], dtype=float32), 0.6681839]. 
=============================================
[2019-04-01 20:09:53,914] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:09:53,914] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6682
[2019-04-01 20:09:53,932] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.14274959381288, 7.220697751788291, 0.0, 1.0, 40.39618066342071], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4068000.0000, 
sim time next is 4068600.0000, 
raw observation next is [-5.833333333333333, 40.50000000000001, 0.0, 0.0, 26.0, 25.11230224878902, 7.259875130138859, 0.0, 1.0, 40.38416715314616], 
processed observation next is [1.0, 0.08695652173913043, 0.30101569713758086, 0.4050000000000001, 0.0, 0.0, 1.0, 0.8731860355412887, 0.07259875130138858, 0.0, 1.0, 0.28845833680818683], 
reward next is 0.7115, 
noisyNet noise sample is [array([-0.62630004], dtype=float32), 0.81900865]. 
=============================================
[2019-04-01 20:09:54,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:09:54,533] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2722
[2019-04-01 20:09:54,548] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666666, 40.0, 0.0, 0.0, 26.0, 24.62250338012341, 5.847660117550149, 0.0, 1.0, 39.51925712331246], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4084800.0000, 
sim time next is 4085400.0000, 
raw observation next is [-4.833333333333334, 40.5, 0.0, 0.0, 26.0, 24.61176983556211, 5.802851089890162, 0.0, 1.0, 39.47093443854536], 
processed observation next is [1.0, 0.2608695652173913, 0.32871652816251157, 0.405, 0.0, 0.0, 1.0, 0.8016814050803015, 0.058028510898901625, 0.0, 1.0, 0.28193524598960973], 
reward next is 0.7181, 
noisyNet noise sample is [array([-2.271728], dtype=float32), -0.24458045]. 
=============================================
[2019-04-01 20:09:56,696] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-01 20:09:56,701] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 20:09:56,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:09:56,701] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 20:09:56,702] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 20:09:56,702] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:09:56,702] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:09:56,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run47
[2019-04-01 20:09:56,708] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run47
[2019-04-01 20:09:56,755] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run47
[2019-04-01 20:10:37,016] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.81807935], dtype=float32), -1.195599]
[2019-04-01 20:10:37,016] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.0, 95.0, 0.0, 0.0, 26.0, 25.49942321550322, 10.35766516373209, 0.0, 1.0, 28.04364290606494]
[2019-04-01 20:10:37,016] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 20:10:37,017] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.34120306950612656
[2019-04-01 20:11:33,096] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.81807935], dtype=float32), -1.195599]
[2019-04-01 20:11:33,096] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [16.76666666666667, 64.66666666666667, 0.0, 0.0, 26.0, 28.31306581334003, 42.6510825659733, 0.0, 0.0, 0.0]
[2019-04-01 20:11:33,096] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 20:11:33,097] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.24093369437024892
[2019-04-01 20:11:38,463] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 20:11:45,731] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.81807935], dtype=float32), -1.195599]
[2019-04-01 20:11:45,732] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [4.35, 73.5, 0.0, 0.0, 26.0, 25.71174089872496, 9.036366864937953, 0.0, 1.0, 24.11838673985856]
[2019-04-01 20:11:45,732] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 20:11:45,733] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.5173145305751032
[2019-04-01 20:11:59,355] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 20:12:02,481] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 20:12:03,505] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 4600000, evaluation results [4600000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 20:12:05,071] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:05,071] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:05,120] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run35
[2019-04-01 20:12:05,497] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:05,499] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9927
[2019-04-01 20:12:05,524] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.32242539623163, 7.220705038120765, 0.0, 1.0, 38.70354787328168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4260600.0000, 
sim time next is 4261200.0000, 
raw observation next is [3.0, 49.0, 18.33333333333333, 8.833333333333332, 26.0, 25.32660241812735, 7.248666932893246, 0.0, 1.0, 38.66234370615681], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.061111111111111095, 0.009760589318600367, 1.0, 0.9038003454467644, 0.07248666932893247, 0.0, 1.0, 0.2761595979011201], 
reward next is 0.7238, 
noisyNet noise sample is [array([-1.1313934], dtype=float32), -1.3178384]. 
=============================================
[2019-04-01 20:12:09,156] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:09,156] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9347
[2019-04-01 20:12:09,169] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 72.0, 0.0, 0.0, 26.0, 25.69887781813656, 8.422669218273306, 0.0, 1.0, 20.60177882787962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4327200.0000, 
sim time next is 4327800.0000, 
raw observation next is [4.416666666666667, 71.83333333333333, 0.0, 0.0, 26.0, 25.63096706894992, 8.189933097006383, 0.0, 1.0, 22.90977301529075], 
processed observation next is [1.0, 0.08695652173913043, 0.584949215143121, 0.7183333333333333, 0.0, 0.0, 1.0, 0.9472810098499888, 0.08189933097006383, 0.0, 1.0, 0.16364123582350534], 
reward next is 0.8364, 
noisyNet noise sample is [array([0.71978354], dtype=float32), 0.9967914]. 
=============================================
[2019-04-01 20:12:19,141] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:19,143] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1946
[2019-04-01 20:12:19,167] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333334, 42.0, 0.0, 0.0, 26.0, 25.4687523677381, 7.998872967305135, 0.0, 1.0, 38.10966051075597], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4927200.0000, 
sim time next is 4927800.0000, 
raw observation next is [0.1666666666666666, 42.5, 0.0, 0.0, 26.0, 25.61417582754211, 8.148778460762786, 0.0, 1.0, 31.23564794337673], 
processed observation next is [1.0, 0.0, 0.4672206832871654, 0.425, 0.0, 0.0, 1.0, 0.9448822610774441, 0.08148778460762786, 0.0, 1.0, 0.2231117710241195], 
reward next is 0.7769, 
noisyNet noise sample is [array([-1.5622164], dtype=float32), 1.908043]. 
=============================================
[2019-04-01 20:12:26,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:26,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:26,704] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run35
[2019-04-01 20:12:27,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:27,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:27,840] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run35
[2019-04-01 20:12:31,193] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:31,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7147
[2019-04-01 20:12:31,218] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 39.5, 0.0, 0.0, 26.0, 25.31605254949502, 7.62313845016066, 0.0, 1.0, 39.03636573501343], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4923000.0000, 
sim time next is 4923600.0000, 
raw observation next is [0.6666666666666666, 39.66666666666666, 0.0, 0.0, 26.0, 25.31646088383943, 7.62772259319146, 0.0, 1.0, 38.6286976514528], 
processed observation next is [0.0, 1.0, 0.4810710987996307, 0.39666666666666656, 0.0, 0.0, 1.0, 0.9023515548342042, 0.0762772259319146, 0.0, 1.0, 0.2759192689389486], 
reward next is 0.7241, 
noisyNet noise sample is [array([0.70871824], dtype=float32), 0.19475563]. 
=============================================
[2019-04-01 20:12:31,783] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 2.033416e-37 1.716750e-28 0.000000e+00
 1.000000e+00 7.395194e-25], sum to 1.0000
[2019-04-01 20:12:31,783] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3770
[2019-04-01 20:12:31,805] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 172.0, 684.0, 26.0, 25.16374725210577, 9.640191244357043, 0.0, 1.0, 7.505159270716161], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4803000.0000, 
sim time next is 4803600.0000, 
raw observation next is [3.0, 37.0, 160.0, 713.0, 26.0, 25.17201367383438, 9.687796504419005, 0.0, 1.0, 7.090796057147486], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.5333333333333333, 0.7878453038674034, 1.0, 0.8817162391191969, 0.09687796504419005, 0.0, 1.0, 0.050648543265339185], 
reward next is 0.9494, 
noisyNet noise sample is [array([0.07600105], dtype=float32), 0.8758543]. 
=============================================
[2019-04-01 20:12:39,895] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:39,895] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:39,902] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run35
[2019-04-01 20:12:39,946] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:39,947] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7995
[2019-04-01 20:12:39,968] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.43199039753186, 8.417976007674227, 0.0, 1.0, 34.24505095953156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5023800.0000, 
sim time next is 5024400.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.39564478424237, 8.467617070775928, 0.0, 1.0, 36.14502763731977], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.55, 0.0, 0.0, 1.0, 0.913663540606053, 0.08467617070775928, 0.0, 1.0, 0.25817876883799834], 
reward next is 0.7418, 
noisyNet noise sample is [array([-0.12349387], dtype=float32), 0.52577305]. 
=============================================
[2019-04-01 20:12:41,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:41,355] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3091
[2019-04-01 20:12:41,378] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.833333333333333, 40.0, 0.0, 0.0, 26.0, 25.56912095059675, 9.659689329510103, 0.0, 1.0, 26.36502579452745], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5011800.0000, 
sim time next is 5012400.0000, 
raw observation next is [1.666666666666667, 40.0, 0.0, 0.0, 26.0, 25.53499567672138, 9.645299778960066, 0.0, 1.0, 28.65805996526418], 
processed observation next is [1.0, 0.0, 0.5087719298245615, 0.4, 0.0, 0.0, 1.0, 0.9335708109601971, 0.09645299778960066, 0.0, 1.0, 0.20470042832331556], 
reward next is 0.7953, 
noisyNet noise sample is [array([0.7714671], dtype=float32), 0.71993184]. 
=============================================
[2019-04-01 20:12:42,766] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:42,767] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5760
[2019-04-01 20:12:42,795] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.27315813130746, 6.871887125918438, 0.0, 1.0, 38.84026757320873], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4935600.0000, 
sim time next is 4936200.0000, 
raw observation next is [-1.166666666666667, 50.0, 0.0, 0.0, 26.0, 25.20879409426171, 6.86844820205287, 0.0, 1.0, 38.92552251981987], 
processed observation next is [1.0, 0.13043478260869565, 0.43028624192059095, 0.5, 0.0, 0.0, 1.0, 0.8869705848945298, 0.06868448202052871, 0.0, 1.0, 0.27803944657014196], 
reward next is 0.7220, 
noisyNet noise sample is [array([-0.00907975], dtype=float32), 0.3810905]. 
=============================================
[2019-04-01 20:12:44,211] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:44,214] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1623
[2019-04-01 20:12:44,222] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.0, 19.0, 0.0, 0.0, 26.0, 27.19765899032482, 20.9381872907906, 0.0, 1.0, 4.425542034374822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5083200.0000, 
sim time next is 5083800.0000, 
raw observation next is [9.833333333333334, 19.0, 0.0, 0.0, 26.0, 27.15090997055037, 20.56469006626707, 0.0, 1.0, 4.719999976766699], 
processed observation next is [1.0, 0.8695652173913043, 0.7349953831948293, 0.19, 0.0, 0.0, 1.0, 1.164415710078624, 0.2056469006626707, 0.0, 1.0, 0.03371428554833356], 
reward next is 0.9663, 
noisyNet noise sample is [array([-1.3798565], dtype=float32), 1.6153538]. 
=============================================
[2019-04-01 20:12:44,305] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:44,305] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:44,316] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run35
[2019-04-01 20:12:44,431] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:44,432] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:44,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run35
[2019-04-01 20:12:45,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:45,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:45,766] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run35
[2019-04-01 20:12:45,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:45,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:45,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run35
[2019-04-01 20:12:46,165] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:46,168] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9443
[2019-04-01 20:12:46,183] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666667, 52.5, 0.0, 0.0, 26.0, 25.57057735199479, 8.851957405273282, 0.0, 1.0, 26.12465581767602], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5021400.0000, 
sim time next is 5022000.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.50074884810175, 8.769041499349035, 0.0, 1.0, 25.15850034351527], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.55, 0.0, 0.0, 1.0, 0.9286784068716786, 0.08769041499349034, 0.0, 1.0, 0.17970357388225194], 
reward next is 0.8203, 
noisyNet noise sample is [array([0.7586355], dtype=float32), -0.34989464]. 
=============================================
[2019-04-01 20:12:46,187] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[67.55115 ]
 [67.52186 ]
 [67.523224]
 [67.64096 ]
 [67.79415 ]], R is [[67.74537659]
 [67.88131714]
 [68.00041962]
 [68.06785583]
 [68.12047577]].
[2019-04-01 20:12:46,743] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.0140526e-36 0.0000000e+00 1.4422345e-34 2.3477094e-21 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:12:46,768] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6337
[2019-04-01 20:12:46,787] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 26.0, 27.15564383280141, 21.95958940578884, 1.0, 1.0, 2.535941388068185], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5078400.0000, 
sim time next is 5079000.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 26.0, 27.2913046265264, 22.43752182677865, 1.0, 1.0, 5.284184712271252], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.0, 0.0, 1.0, 1.1844720895037715, 0.2243752182677865, 1.0, 1.0, 0.03774417651622323], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.39845914], dtype=float32), 1.7654555]. 
=============================================
[2019-04-01 20:12:46,808] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[52.28533 ]
 [51.23394 ]
 [50.40416 ]
 [49.31716 ]
 [47.104294]], R is [[52.87292099]
 [52.3441925 ]
 [51.82075119]
 [51.30254364]
 [50.78952026]].
[2019-04-01 20:12:46,931] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:46,931] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9813
[2019-04-01 20:12:46,943] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.37783892115264, 11.18213688224707, 0.0, 1.0, 39.76103951651614], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 26400.0000, 
sim time next is 27000.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.40938828521553, 11.04490612039999, 0.0, 1.0, 39.75177985298163], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 1.0, 0.3441983264593616, 0.1104490612039999, 0.0, 1.0, 0.28394128466415447], 
reward next is 0.7161, 
noisyNet noise sample is [array([-0.77007407], dtype=float32), -0.7115648]. 
=============================================
[2019-04-01 20:12:46,958] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[71.708755]
 [71.67999 ]
 [71.64029 ]
 [71.58785 ]
 [71.59065 ]], R is [[71.73628235]
 [71.73491669]
 [71.73353577]
 [71.7321701 ]
 [71.73081207]].
[2019-04-01 20:12:48,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:48,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:48,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run35
[2019-04-01 20:12:50,395] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:50,397] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:50,404] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run35
[2019-04-01 20:12:50,665] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:50,666] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:50,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run35
[2019-04-01 20:12:50,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:50,838] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:50,847] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run35
[2019-04-01 20:12:50,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:50,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:50,954] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run35
[2019-04-01 20:12:51,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:51,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:51,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run35
[2019-04-01 20:12:53,024] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:53,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:53,050] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run35
[2019-04-01 20:12:54,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:12:54,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:12:54,560] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run35
[2019-04-01 20:12:55,951] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:55,951] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4657
[2019-04-01 20:12:55,969] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.15403568542662, 12.77022887216812, 0.0, 1.0, 39.94123235229479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 19200.0000, 
sim time next is 19800.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.17962506229494, 12.68933853864458, 0.0, 1.0, 39.91549209850079], 
processed observation next is [0.0, 0.21739130434782608, 0.6759002770083103, 0.93, 0.0, 0.0, 1.0, 0.31137500889927694, 0.12689338538644582, 0.0, 1.0, 0.28511065784643425], 
reward next is 0.7149, 
noisyNet noise sample is [array([0.24789205], dtype=float32), 0.09159595]. 
=============================================
[2019-04-01 20:12:56,245] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:56,245] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3575
[2019-04-01 20:12:56,262] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.28052433477785, 11.85353904100809, 0.0, 1.0, 39.77145427985151], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 23400.0000, 
sim time next is 24000.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 26.0, 21.30095065384896, 11.73163789190723, 0.0, 1.0, 39.76458577349467], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 1.0, 0.3287072362641373, 0.1173163789190723, 0.0, 1.0, 0.28403275552496193], 
reward next is 0.7160, 
noisyNet noise sample is [array([1.2490752], dtype=float32), 0.43488184]. 
=============================================
[2019-04-01 20:12:56,278] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[70.14507 ]
 [70.17081 ]
 [70.195816]
 [70.22654 ]
 [70.255486]], R is [[70.12594604]
 [70.14060974]
 [70.15499878]
 [70.16911316]
 [70.18293762]].
[2019-04-01 20:12:56,912] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 4.067660e-30 0.000000e+00 6.992558e-30
 9.673404e-03 9.903266e-01], sum to 1.0000
[2019-04-01 20:12:56,912] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1262
[2019-04-01 20:12:56,952] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 61.0, 133.5, 95.83333333333334, 26.0, 25.90886191079862, 9.54041991028551, 1.0, 1.0, 21.6823319938256], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 139200.0000, 
sim time next is 139800.0000, 
raw observation next is [-6.7, 61.0, 119.0, 85.66666666666667, 26.0, 25.84350381204023, 9.20269701329716, 1.0, 1.0, 20.54563777162578], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.39666666666666667, 0.09465930018416206, 1.0, 0.9776434017200327, 0.09202697013297159, 1.0, 1.0, 0.14675455551161273], 
reward next is 0.8532, 
noisyNet noise sample is [array([1.2314306], dtype=float32), 2.5431576]. 
=============================================
[2019-04-01 20:12:58,630] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:58,630] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7255
[2019-04-01 20:12:58,643] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.00457115533417, 13.85470292483106, 0.0, 1.0, 40.3427436289133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 16200.0000, 
sim time next is 16800.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 26.0, 21.0181306162229, 13.70420854490574, 0.0, 1.0, 40.26907797027927], 
processed observation next is [0.0, 0.17391304347826086, 0.6759002770083103, 0.93, 0.0, 0.0, 1.0, 0.2883043737461288, 0.13704208544905738, 0.0, 1.0, 0.2876362712162805], 
reward next is 0.7124, 
noisyNet noise sample is [array([-0.07999787], dtype=float32), -0.756021]. 
=============================================
[2019-04-01 20:12:59,903] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:12:59,905] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6044
[2019-04-01 20:12:59,919] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.9, 68.0, 0.0, 0.0, 26.0, 23.71976356800237, 5.633205619627657, 0.0, 1.0, 46.55683914453935], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 343800.0000, 
sim time next is 344400.0000, 
raw observation next is [-13.9, 67.33333333333334, 0.0, 0.0, 26.0, 23.65209868651556, 5.592148738932795, 0.0, 1.0, 46.58837789306447], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.6733333333333335, 0.0, 0.0, 1.0, 0.6645855266450802, 0.05592148738932795, 0.0, 1.0, 0.33277412780760335], 
reward next is 0.6672, 
noisyNet noise sample is [array([-0.17023408], dtype=float32), 1.1069592]. 
=============================================
[2019-04-01 20:13:06,226] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:13:06,228] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6542
[2019-04-01 20:13:06,255] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.783333333333333, 85.66666666666667, 0.0, 0.0, 26.0, 24.70215319422512, 6.764370843211606, 0.0, 1.0, 40.169170165946], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 75000.0000, 
sim time next is 75600.0000, 
raw observation next is [1.6, 85.0, 0.0, 0.0, 26.0, 24.68050871109467, 6.709531026368623, 0.0, 1.0, 40.12310218087705], 
processed observation next is [0.0, 0.9130434782608695, 0.5069252077562327, 0.85, 0.0, 0.0, 1.0, 0.8115012444420958, 0.06709531026368623, 0.0, 1.0, 0.28659358700626464], 
reward next is 0.7134, 
noisyNet noise sample is [array([-1.9513515], dtype=float32), 1.550298]. 
=============================================
[2019-04-01 20:13:16,381] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:13:16,389] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2799
[2019-04-01 20:13:16,423] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.8, 79.16666666666667, 0.0, 0.0, 26.0, 24.69373905536185, 6.089072961245383, 0.0, 1.0, 38.99616054690875], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 870600.0000, 
sim time next is 871200.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 26.0, 24.63194550112634, 6.088129458836893, 0.0, 1.0, 39.00550266991885], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.79, 0.0, 0.0, 1.0, 0.8045636430180484, 0.06088129458836893, 0.0, 1.0, 0.2786107333565632], 
reward next is 0.7214, 
noisyNet noise sample is [array([-0.94504815], dtype=float32), -0.1971649]. 
=============================================
[2019-04-01 20:13:19,360] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:13:19,362] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3547
[2019-04-01 20:13:19,376] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.233333333333334, 75.0, 0.0, 0.0, 26.0, 24.05297573532603, 5.553595701555852, 0.0, 1.0, 43.85559265782764], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 260400.0000, 
sim time next is 261000.0000, 
raw observation next is [-5.6, 73.0, 0.0, 0.0, 26.0, 24.06933375600809, 5.543498796920313, 0.0, 1.0, 43.9106206164147], 
processed observation next is [1.0, 0.0, 0.30747922437673136, 0.73, 0.0, 0.0, 1.0, 0.724190536572584, 0.05543498796920313, 0.0, 1.0, 0.31364729011724785], 
reward next is 0.6864, 
noisyNet noise sample is [array([0.34149662], dtype=float32), 0.2602424]. 
=============================================
[2019-04-01 20:13:19,379] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[66.98119]
 [67.32819]
 [67.74606]
 [68.15813]
 [68.36237]], R is [[66.81195068]
 [66.83057404]
 [66.84944153]
 [66.86838531]
 [66.8871994 ]].
[2019-04-01 20:13:34,151] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:13:34,151] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2734
[2019-04-01 20:13:34,209] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.766666666666667, 30.66666666666667, 92.0, 0.0, 26.0, 25.2904073409247, 6.053704516123489, 1.0, 1.0, 23.26585759273662], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 469200.0000, 
sim time next is 469800.0000, 
raw observation next is [-3.4, 30.0, 98.0, 0.0, 26.0, 25.24978096838727, 5.995608862576023, 1.0, 1.0, 23.70058238463783], 
processed observation next is [1.0, 0.43478260869565216, 0.368421052631579, 0.3, 0.32666666666666666, 0.0, 1.0, 0.8928258526267528, 0.059956088625760226, 1.0, 1.0, 0.1692898741759845], 
reward next is 0.8307, 
noisyNet noise sample is [array([-1.3561661], dtype=float32), -1.9009802]. 
=============================================
[2019-04-01 20:13:36,728] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:13:36,730] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4924
[2019-04-01 20:13:36,747] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.36666666666667, 54.66666666666667, 0.0, 0.0, 26.0, 23.6350938500053, 5.581841660899372, 0.0, 1.0, 44.74534408863887], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 434400.0000, 
sim time next is 435000.0000, 
raw observation next is [-11.28333333333333, 54.83333333333334, 0.0, 0.0, 26.0, 23.58651692403993, 5.54307966611983, 0.0, 1.0, 44.77647632492253], 
processed observation next is [1.0, 0.0, 0.1500461680517083, 0.5483333333333335, 0.0, 0.0, 1.0, 0.6552167034342757, 0.0554307966611983, 0.0, 1.0, 0.3198319737494466], 
reward next is 0.6802, 
noisyNet noise sample is [array([0.4668347], dtype=float32), 1.5230188]. 
=============================================
[2019-04-01 20:13:36,754] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[68.09182 ]
 [68.044395]
 [68.05101 ]
 [68.16015 ]
 [68.35126 ]], R is [[68.19745636]
 [68.19586945]
 [68.19480133]
 [68.19412994]
 [68.19400787]].
[2019-04-01 20:13:41,711] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:13:41,714] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5455
[2019-04-01 20:13:41,738] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.716666666666667, 79.16666666666667, 0.0, 0.0, 26.0, 24.68518521171142, 6.455063196017633, 0.0, 1.0, 40.43570733482958], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 861000.0000, 
sim time next is 861600.0000, 
raw observation next is [-2.633333333333333, 79.33333333333334, 0.0, 0.0, 26.0, 24.65388454159562, 6.390717491046537, 0.0, 1.0, 40.30379011293121], 
processed observation next is [1.0, 1.0, 0.38965835641735924, 0.7933333333333334, 0.0, 0.0, 1.0, 0.8076977916565171, 0.06390717491046537, 0.0, 1.0, 0.2878842150923658], 
reward next is 0.7121, 
noisyNet noise sample is [array([-0.77497923], dtype=float32), 0.29908046]. 
=============================================
[2019-04-01 20:13:43,051] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.4039852e-19 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:13:43,053] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0280
[2019-04-01 20:13:43,099] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 93.0, 60.0, 0.0, 26.0, 26.3123692441897, 11.65130985818346, 1.0, 1.0, 19.87984875122346], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 919200.0000, 
sim time next is 919800.0000, 
raw observation next is [4.4, 93.0, 54.0, 0.0, 26.0, 26.43509129852141, 11.90637109315308, 1.0, 1.0, 18.61289091213392], 
processed observation next is [1.0, 0.6521739130434783, 0.5844875346260389, 0.93, 0.18, 0.0, 1.0, 1.0621558997887728, 0.1190637109315308, 1.0, 1.0, 0.13294922080095659], 
reward next is 0.1045, 
noisyNet noise sample is [array([-0.4280477], dtype=float32), 1.3578682]. 
=============================================
[2019-04-01 20:13:46,081] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:13:46,081] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4825
[2019-04-01 20:13:46,145] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.7333333333333334, 81.66666666666667, 95.83333333333333, 178.5, 26.0, 24.7893067282738, 7.214968010865971, 0.0, 1.0, 46.98862050883037], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 560400.0000, 
sim time next is 561000.0000, 
raw observation next is [-0.7666666666666667, 81.33333333333333, 102.6666666666667, 222.0, 26.0, 24.80968185252073, 7.450411435293508, 0.0, 1.0, 44.69288992138733], 
processed observation next is [0.0, 0.4782608695652174, 0.44136657433056325, 0.8133333333333332, 0.3422222222222223, 0.24530386740331492, 1.0, 0.8299545503601041, 0.07450411435293508, 0.0, 1.0, 0.3192349280099095], 
reward next is 0.6808, 
noisyNet noise sample is [array([-1.1434556], dtype=float32), -1.0340894]. 
=============================================
[2019-04-01 20:13:46,163] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[70.30359 ]
 [70.34498 ]
 [70.12634 ]
 [69.891335]
 [69.95523 ]], R is [[70.35004425]
 [70.31091309]
 [70.3263092 ]
 [70.35935211]
 [70.42858887]].
[2019-04-01 20:13:52,248] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:13:52,249] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5130
[2019-04-01 20:13:52,264] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.1, 67.0, 0.0, 0.0, 26.0, 25.02283874282804, 6.656038963878136, 0.0, 1.0, 41.97384314589656], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 678600.0000, 
sim time next is 679200.0000, 
raw observation next is [-3.2, 67.66666666666667, 0.0, 0.0, 26.0, 25.00230945438664, 6.582564730124129, 0.0, 1.0, 41.9130377336039], 
processed observation next is [0.0, 0.8695652173913043, 0.37396121883656513, 0.6766666666666667, 0.0, 0.0, 1.0, 0.8574727791980915, 0.06582564730124128, 0.0, 1.0, 0.29937884095431355], 
reward next is 0.7006, 
noisyNet noise sample is [array([0.77317727], dtype=float32), -0.20847638]. 
=============================================
[2019-04-01 20:13:53,390] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:13:53,393] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2782
[2019-04-01 20:13:53,423] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.716666666666667, 75.16666666666667, 0.0, 0.0, 26.0, 24.21606792715581, 5.285430690334063, 0.0, 1.0, 41.09282215002984], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 706200.0000, 
sim time next is 706800.0000, 
raw observation next is [-2.633333333333333, 75.33333333333334, 0.0, 0.0, 26.0, 24.26948863665003, 5.28928152840097, 0.0, 1.0, 41.02007152179955], 
processed observation next is [1.0, 0.17391304347826086, 0.38965835641735924, 0.7533333333333334, 0.0, 0.0, 1.0, 0.7527840909500044, 0.0528928152840097, 0.0, 1.0, 0.2930005108699968], 
reward next is 0.7070, 
noisyNet noise sample is [array([-0.97849035], dtype=float32), 0.886709]. 
=============================================
[2019-04-01 20:14:08,433] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:08,434] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6838
[2019-04-01 20:14:08,487] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.2286490711921, 6.880973454352774, 1.0, 1.0, 36.45825665339045], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 891000.0000, 
sim time next is 891600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.26470120269703, 6.780724192759426, 1.0, 1.0, 34.57632257500253], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 1.0, 0.8949573146710043, 0.06780724192759427, 1.0, 1.0, 0.2469737326785895], 
reward next is 0.7530, 
noisyNet noise sample is [array([0.01543804], dtype=float32), 1.0580822]. 
=============================================
[2019-04-01 20:14:08,788] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:08,789] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5819
[2019-04-01 20:14:08,804] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.29148405908735, 9.177237455695073, 0.0, 1.0, 36.54805886783322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1485600.0000, 
sim time next is 1486200.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.28754122732929, 9.243308456120767, 0.0, 1.0, 36.42585685230133], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 1.0, 0.8982201753327556, 0.09243308456120766, 0.0, 1.0, 0.26018469180215237], 
reward next is 0.7398, 
noisyNet noise sample is [array([0.9330062], dtype=float32), -1.070943]. 
=============================================
[2019-04-01 20:14:09,431] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:09,435] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9083
[2019-04-01 20:14:09,449] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 98.66666666666667, 0.0, 0.0, 26.0, 25.04083868620403, 8.676807652695494, 0.0, 1.0, 40.19042019899707], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 940800.0000, 
sim time next is 941400.0000, 
raw observation next is [5.0, 98.0, 0.0, 0.0, 26.0, 25.09310080132659, 8.772388454182414, 0.0, 1.0, 39.95622438317078], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.98, 0.0, 0.0, 1.0, 0.8704429716180841, 0.08772388454182414, 0.0, 1.0, 0.2854016027369341], 
reward next is 0.7146, 
noisyNet noise sample is [array([-1.8003336], dtype=float32), -0.29026604]. 
=============================================
[2019-04-01 20:14:10,120] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:10,121] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3177
[2019-04-01 20:14:10,145] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.966666666666667, 81.33333333333334, 0.0, 0.0, 26.0, 25.57015575969058, 9.618777266593503, 0.0, 1.0, 25.73228414160277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 958800.0000, 
sim time next is 959400.0000, 
raw observation next is [7.15, 81.0, 0.0, 0.0, 26.0, 25.5581510208485, 9.407453189304725, 0.0, 1.0, 24.49767362097455], 
processed observation next is [1.0, 0.08695652173913043, 0.6606648199445985, 0.81, 0.0, 0.0, 1.0, 0.9368787172640712, 0.09407453189304725, 0.0, 1.0, 0.17498338300696106], 
reward next is 0.8250, 
noisyNet noise sample is [array([0.32263145], dtype=float32), 0.863548]. 
=============================================
[2019-04-01 20:14:10,376] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:10,379] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2281
[2019-04-01 20:14:10,418] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.5, 75.0, 41.0, 0.0, 26.0, 27.026337640398, 14.0105451882652, 1.0, 1.0, 8.327680689667956], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1008000.0000, 
sim time next is 1008600.0000, 
raw observation next is [15.5, 75.5, 35.66666666666666, 0.0, 26.0, 25.94462121180796, 13.4411767725282, 1.0, 1.0, 7.897513441719583], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.755, 0.11888888888888886, 0.0, 1.0, 0.9920887445439942, 0.134411767725282, 1.0, 1.0, 0.05641081029799702], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1797334], dtype=float32), -1.0246994]. 
=============================================
[2019-04-01 20:14:10,707] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:10,708] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7154
[2019-04-01 20:14:10,715] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.28333333333333, 65.66666666666666, 0.0, 0.0, 26.0, 25.89131166274288, 14.9838507368831, 0.0, 1.0, 13.95041512532104], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1119000.0000, 
sim time next is 1119600.0000, 
raw observation next is [12.2, 66.0, 0.0, 0.0, 26.0, 25.8451239022504, 14.65074085972161, 0.0, 1.0, 13.31955825568403], 
processed observation next is [1.0, 1.0, 0.8005540166204987, 0.66, 0.0, 0.0, 1.0, 0.9778748431786285, 0.14650740859721612, 0.0, 1.0, 0.0951397018263145], 
reward next is 0.9049, 
noisyNet noise sample is [array([0.5898846], dtype=float32), -1.2441789]. 
=============================================
[2019-04-01 20:14:13,602] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 2.0821286e-37 9.9999964e-01 2.9034355e-30
 3.9752240e-07 5.3420175e-34], sum to 1.0000
[2019-04-01 20:14:13,606] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4597
[2019-04-01 20:14:13,614] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.38333333333334, 55.66666666666667, 174.3333333333333, 105.6666666666666, 26.0, 25.92562871743232, 18.61772080101177, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1084200.0000, 
sim time next is 1084800.0000, 
raw observation next is [18.46666666666667, 55.33333333333334, 172.6666666666667, 52.83333333333332, 26.0, 26.52796873213384, 20.05087024732086, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.9741458910433982, 0.5533333333333335, 0.5755555555555557, 0.058379373848987094, 1.0, 1.0754241045905484, 0.2005087024732086, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3705099], dtype=float32), -0.31920323]. 
=============================================
[2019-04-01 20:14:14,142] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1216463e-34 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:14:14,145] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3234
[2019-04-01 20:14:14,163] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 24.92280794876286, 10.76926691324615, 0.0, 1.0, 60.42858042789277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1024200.0000, 
sim time next is 1024800.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 25.10966033707084, 11.95829109423195, 0.0, 1.0, 45.75959537903682], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 1.0, 0.8728086195815484, 0.11958291094231949, 0.0, 1.0, 0.3268542527074058], 
reward next is 0.6731, 
noisyNet noise sample is [array([-1.1236128], dtype=float32), 0.98908937]. 
=============================================
[2019-04-01 20:14:15,502] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:15,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7960
[2019-04-01 20:14:15,518] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.38333333333334, 55.66666666666667, 174.3333333333333, 105.6666666666666, 26.0, 25.92562871743232, 18.61772080101177, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1084200.0000, 
sim time next is 1084800.0000, 
raw observation next is [18.46666666666667, 55.33333333333334, 172.6666666666667, 52.83333333333332, 26.0, 26.52796873213384, 20.05087024732086, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.9741458910433982, 0.5533333333333335, 0.5755555555555557, 0.058379373848987094, 1.0, 1.0754241045905484, 0.2005087024732086, 1.0, 0.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.19110847], dtype=float32), 0.20016474]. 
=============================================
[2019-04-01 20:14:16,412] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:16,412] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3429
[2019-04-01 20:14:16,425] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 100.0, 64.0, 0.0, 26.0, 24.70802382854963, 9.478425618619523, 0.0, 1.0, 5.487863383033902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1263600.0000, 
sim time next is 1264200.0000, 
raw observation next is [13.8, 100.0, 59.66666666666667, 0.0, 26.0, 24.69066405704199, 9.422031897494547, 0.0, 1.0, 5.313659595098392], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.1988888888888889, 0.0, 1.0, 0.8129520081488556, 0.09422031897494547, 0.0, 1.0, 0.037954711393559944], 
reward next is 0.9620, 
noisyNet noise sample is [array([-0.1178989], dtype=float32), -1.275059]. 
=============================================
[2019-04-01 20:14:22,728] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3513722e-23 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:14:22,730] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0501
[2019-04-01 20:14:22,747] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.32502325663993, 11.12474479844281, 0.0, 1.0, 40.44344024105034], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1383600.0000, 
sim time next is 1384200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.46464901183901, 11.19620144004163, 0.0, 1.0, 39.37873414745165], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 1.0, 0.923521287405573, 0.1119620144004163, 0.0, 1.0, 0.2812766724817975], 
reward next is 0.7187, 
noisyNet noise sample is [array([-0.15343289], dtype=float32), -0.43144915]. 
=============================================
[2019-04-01 20:14:26,448] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:26,450] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1025
[2019-04-01 20:14:26,466] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.3814566722934, 10.35607051933338, 0.0, 1.0, 39.05530289573513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1393200.0000, 
sim time next is 1393800.0000, 
raw observation next is [-0.09999999999999999, 95.83333333333334, 0.0, 0.0, 26.0, 25.44155295862797, 10.4911163844069, 0.0, 1.0, 38.55634444261091], 
processed observation next is [1.0, 0.13043478260869565, 0.4598337950138504, 0.9583333333333335, 0.0, 0.0, 1.0, 0.9202218512325671, 0.10491116384406901, 0.0, 1.0, 0.2754024603043636], 
reward next is 0.7246, 
noisyNet noise sample is [array([-0.27277967], dtype=float32), -0.9112609]. 
=============================================
[2019-04-01 20:14:27,059] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 5.3766831e-15
 2.4879576e-35 0.0000000e+00], sum to 1.0000
[2019-04-01 20:14:27,065] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9399
[2019-04-01 20:14:27,115] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.99964221174591, 11.64593572297295, 1.0, 1.0, 47.47059290643566], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1368600.0000, 
sim time next is 1369200.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.18949267365124, 12.38456514804959, 0.0, 1.0, 47.59731538072684], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 1.0, 0.8842132390930344, 0.12384565148049591, 0.0, 1.0, 0.33998082414804887], 
reward next is 0.6600, 
noisyNet noise sample is [array([1.5346984], dtype=float32), 1.7271347]. 
=============================================
[2019-04-01 20:14:27,594] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:27,594] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8496
[2019-04-01 20:14:27,608] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.7, 92.0, 0.0, 0.0, 26.0, 25.52948183858256, 10.07246429244861, 0.0, 1.0, 25.89853840978923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1473000.0000, 
sim time next is 1473600.0000, 
raw observation next is [1.8, 92.0, 0.0, 0.0, 26.0, 25.52085083456415, 9.776534736894318, 0.0, 1.0, 25.37499260829081], 
processed observation next is [1.0, 0.043478260869565216, 0.5124653739612189, 0.92, 0.0, 0.0, 1.0, 0.9315501192234501, 0.09776534736894318, 0.0, 1.0, 0.1812499472020772], 
reward next is 0.8188, 
noisyNet noise sample is [array([-0.40994602], dtype=float32), 0.85893494]. 
=============================================
[2019-04-01 20:14:29,574] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:29,577] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8024
[2019-04-01 20:14:29,617] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 83.0, 15.0, 0.0, 26.0, 25.13106740912202, 7.529826652535831, 0.0, 1.0, 29.8933219076189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1875600.0000, 
sim time next is 1876200.0000, 
raw observation next is [-4.583333333333333, 83.5, 10.33333333333333, 0.0, 26.0, 25.08836918333874, 7.36114621737471, 0.0, 1.0, 33.33997523775795], 
processed observation next is [0.0, 0.7391304347826086, 0.3356417359187443, 0.835, 0.03444444444444444, 0.0, 1.0, 0.8697670261912488, 0.0736114621737471, 0.0, 1.0, 0.23814268026969962], 
reward next is 0.7619, 
noisyNet noise sample is [array([0.64723974], dtype=float32), 0.27592912]. 
=============================================
[2019-04-01 20:14:29,705] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 5.1554441e-34 1.4245998e-02 5.1800621e-28
 9.8575401e-01 0.0000000e+00], sum to 1.0000
[2019-04-01 20:14:29,707] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7668
[2019-04-01 20:14:29,719] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 78.0, 0.0, 26.0, 25.82984254388524, 10.18871543782587, 1.0, 1.0, 9.780271869611129], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1421400.0000, 
sim time next is 1422000.0000, 
raw observation next is [0.0, 95.0, 81.0, 0.0, 26.0, 25.82438530310144, 10.04050257857569, 1.0, 1.0, 9.208081434270632], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.27, 0.0, 1.0, 0.9749121861573486, 0.1004050257857569, 1.0, 1.0, 0.06577201024479022], 
reward next is 0.9180, 
noisyNet noise sample is [array([-0.7343941], dtype=float32), -0.67255414]. 
=============================================
[2019-04-01 20:14:29,742] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[57.748856]
 [57.43155 ]
 [57.25051 ]
 [56.75676 ]
 [56.175144]], R is [[58.21087646]
 [58.48342514]
 [58.74011993]
 [58.95961761]
 [59.12450409]].
[2019-04-01 20:14:30,243] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9375502e-03 1.8492053e-01 6.9114810e-04 2.0539947e-02 5.0322900e-29
 7.9091078e-01 7.3563918e-32], sum to 1.0000
[2019-04-01 20:14:30,244] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7673
[2019-04-01 20:14:30,258] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.3, 92.0, 66.0, 0.0, 26.0, 25.79538391726135, 10.36088088793866, 1.0, 1.0, 12.79605253998899], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1679400.0000, 
sim time next is 1680000.0000, 
raw observation next is [1.233333333333333, 92.0, 68.83333333333333, 0.0, 26.0, 25.70699560305533, 10.30280703809653, 1.0, 1.0, 12.43084860186398], 
processed observation next is [1.0, 0.43478260869565216, 0.49676823638042483, 0.92, 0.22944444444444442, 0.0, 1.0, 0.9581422290079041, 0.1030280703809653, 1.0, 1.0, 0.08879177572759986], 
reward next is 0.7901, 
noisyNet noise sample is [array([0.9908732], dtype=float32), 1.0645183]. 
=============================================
[2019-04-01 20:14:30,278] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[52.170876]
 [52.03186 ]
 [51.73878 ]
 [51.23716 ]
 [50.61778 ]], R is [[52.47665024]
 [52.71613312]
 [52.82690811]
 [52.91741562]
 [52.92591095]].
[2019-04-01 20:14:32,061] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.995988e-27
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 20:14:32,061] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5609
[2019-04-01 20:14:32,080] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.34324769283486, 9.257061462498859, 0.0, 1.0, 34.8525117983822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1483200.0000, 
sim time next is 1483800.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.29862016256528, 9.354152895602992, 0.0, 1.0, 36.0507949175365], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 1.0, 0.8998028803664683, 0.09354152895602992, 0.0, 1.0, 0.2575056779824036], 
reward next is 0.7425, 
noisyNet noise sample is [array([-0.64280236], dtype=float32), -0.9034927]. 
=============================================
[2019-04-01 20:14:33,031] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:33,031] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4479
[2019-04-01 20:14:33,043] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 91.00000000000001, 0.0, 0.0, 26.0, 23.79166293598002, 5.275949833192935, 0.0, 1.0, 42.72925571830105], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2265000.0000, 
sim time next is 2265600.0000, 
raw observation next is [-8.900000000000002, 91.0, 0.0, 0.0, 26.0, 23.72249015416446, 5.289184571759709, 0.0, 1.0, 42.69364814117583], 
processed observation next is [1.0, 0.21739130434782608, 0.2160664819944598, 0.91, 0.0, 0.0, 1.0, 0.6746414505949231, 0.052891845717597086, 0.0, 1.0, 0.30495462957982733], 
reward next is 0.6950, 
noisyNet noise sample is [array([1.8718106], dtype=float32), 1.1674596]. 
=============================================
[2019-04-01 20:14:38,214] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-01 20:14:38,214] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0037
[2019-04-01 20:14:38,222] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.8, 49.0, 162.5, 62.0, 26.0, 25.71909161564803, 14.66139933846083, 1.0, 1.0, 4.178259125408271], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1602000.0000, 
sim time next is 1602600.0000, 
raw observation next is [13.8, 49.0, 167.0, 41.33333333333332, 26.0, 26.27645451844604, 15.51900472232301, 1.0, 1.0, 4.116232861149034], 
processed observation next is [1.0, 0.5652173913043478, 0.844875346260388, 0.49, 0.5566666666666666, 0.045672191528545104, 1.0, 1.0394935026351486, 0.1551900472232301, 1.0, 1.0, 0.02940166329392167], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.0992224], dtype=float32), 0.707414]. 
=============================================
[2019-04-01 20:14:50,451] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:50,453] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2144
[2019-04-01 20:14:50,482] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 23.2287898341087, 5.469690398606457, 0.0, 1.0, 46.53455701131882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1841400.0000, 
sim time next is 1842000.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 23.20131326041403, 5.497132866000373, 0.0, 1.0, 46.51222772980248], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 1.0, 0.6001876086305755, 0.05497132866000373, 0.0, 1.0, 0.33223019807001775], 
reward next is 0.6678, 
noisyNet noise sample is [array([-1.6661707], dtype=float32), 0.29115698]. 
=============================================
[2019-04-01 20:14:50,485] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[60.432663]
 [60.470535]
 [60.52877 ]
 [60.60859 ]
 [60.713593]], R is [[60.48026276]
 [60.54307175]
 [60.60519791]
 [60.66676331]
 [60.72789764]].
[2019-04-01 20:14:53,721] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:14:53,721] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2778
[2019-04-01 20:14:53,776] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.666666666666667, 71.0, 128.3333333333333, 6.666666666666665, 26.0, 25.06704371316036, 7.202793307124293, 0.0, 1.0, 31.87845443283187], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1860000.0000, 
sim time next is 1860600.0000, 
raw observation next is [-4.583333333333333, 71.0, 136.6666666666667, 13.33333333333333, 26.0, 24.99735259349834, 7.075860247101079, 0.0, 1.0, 37.91641424736378], 
processed observation next is [0.0, 0.5217391304347826, 0.3356417359187443, 0.71, 0.4555555555555557, 0.0147329650092081, 1.0, 0.8567646562140487, 0.07075860247101079, 0.0, 1.0, 0.27083153033831275], 
reward next is 0.7292, 
noisyNet noise sample is [array([0.80567306], dtype=float32), -1.1044154]. 
=============================================
[2019-04-01 20:15:04,239] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:15:04,239] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5058
[2019-04-01 20:15:04,255] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.13377407747507, 5.333388845189148, 0.0, 1.0, 40.62114029999648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2008800.0000, 
sim time next is 2009400.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.13594807568742, 5.326271824960952, 0.0, 1.0, 40.61849328981923], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 1.0, 0.7337068679553457, 0.05326271824960952, 0.0, 1.0, 0.2901320949272802], 
reward next is 0.7099, 
noisyNet noise sample is [array([-0.6095596], dtype=float32), 1.3207731]. 
=============================================
[2019-04-01 20:15:08,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:15:08,736] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6793
[2019-04-01 20:15:08,788] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.666666666666667, 36.5, 201.6666666666667, 514.3333333333334, 26.0, 25.57221609544611, 10.501095627018, 1.0, 1.0, 29.97962411334687], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2811000.0000, 
sim time next is 2811600.0000, 
raw observation next is [4.0, 35.0, 213.5, 429.0, 26.0, 25.95221736337724, 10.7988577038137, 1.0, 1.0, 27.87297567646918], 
processed observation next is [1.0, 0.5652173913043478, 0.5734072022160666, 0.35, 0.7116666666666667, 0.4740331491712707, 1.0, 0.9931739090538915, 0.10798857703813701, 1.0, 1.0, 0.1990926834033513], 
reward next is 0.4814, 
noisyNet noise sample is [array([0.38756365], dtype=float32), -1.4578131]. 
=============================================
[2019-04-01 20:15:10,383] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00
 1.27957e-24], sum to 1.0000
[2019-04-01 20:15:10,386] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9428
[2019-04-01 20:15:10,456] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 75.0, 8.499999999999998, 43.66666666666666, 26.0, 25.16087666673801, 7.222438637191381, 1.0, 1.0, 50.44686976109291], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2187600.0000, 
sim time next is 2188200.0000, 
raw observation next is [-5.6, 75.0, 15.0, 87.33333333333331, 26.0, 25.35998861290891, 7.25648568316296, 1.0, 1.0, 44.68247006226101], 
processed observation next is [1.0, 0.30434782608695654, 0.30747922437673136, 0.75, 0.05, 0.09650092081031306, 1.0, 0.90856980184413, 0.0725648568316296, 1.0, 1.0, 0.3191605004447215], 
reward next is 0.6808, 
noisyNet noise sample is [array([0.19581272], dtype=float32), 2.1117702]. 
=============================================
[2019-04-01 20:15:10,806] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 3.1962977e-11 1.0000000e+00], sum to 1.0000
[2019-04-01 20:15:10,807] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6362
[2019-04-01 20:15:10,836] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.199999999999999, 73.16666666666667, 264.6666666666666, 87.33333333333334, 26.0, 25.81417374868391, 8.911428929885801, 1.0, 1.0, 16.54389857210316], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2113800.0000, 
sim time next is 2114400.0000, 
raw observation next is [-7.100000000000001, 71.33333333333334, 278.8333333333334, 94.16666666666667, 26.0, 25.78747567952243, 8.93659430732838, 1.0, 1.0, 15.78936975473178], 
processed observation next is [1.0, 0.4782608695652174, 0.26592797783933514, 0.7133333333333334, 0.9294444444444447, 0.10405156537753224, 1.0, 0.9696393827889186, 0.08936594307328379, 1.0, 1.0, 0.11278121253379843], 
reward next is 0.8872, 
noisyNet noise sample is [array([-1.2647014], dtype=float32), -1.1284707]. 
=============================================
[2019-04-01 20:15:12,027] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:15:12,027] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5968
[2019-04-01 20:15:12,042] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 59.0, 9.166666666666664, 102.6666666666667, 26.0, 22.87277814444217, 6.32215554331687, 0.0, 1.0, 43.58699557018183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2446800.0000, 
sim time next is 2447400.0000, 
raw observation next is [-9.5, 58.5, 15.33333333333333, 165.3333333333333, 26.0, 22.85049641346469, 6.282628610911682, 0.0, 1.0, 43.50466854149469], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.585, 0.0511111111111111, 0.18268876611418042, 1.0, 0.5500709162092414, 0.06282628610911682, 0.0, 1.0, 0.31074763243924775], 
reward next is 0.6893, 
noisyNet noise sample is [array([0.5175689], dtype=float32), 0.44766268]. 
=============================================
[2019-04-01 20:15:23,577] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 9.0493914e-23 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:15:23,581] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4768
[2019-04-01 20:15:23,637] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.550000000000001, 82.5, 101.0, 39.0, 26.0, 25.54308334263389, 7.379002387704648, 1.0, 1.0, 36.49888489665511], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2280600.0000, 
sim time next is 2281200.0000, 
raw observation next is [-7.266666666666667, 81.0, 113.8333333333333, 40.83333333333333, 26.0, 25.56638743996054, 7.536580000249185, 1.0, 1.0, 35.64925755418131], 
processed observation next is [1.0, 0.391304347826087, 0.26131117266851345, 0.81, 0.3794444444444443, 0.04511970534069981, 1.0, 0.9380553485657914, 0.07536580000249185, 1.0, 1.0, 0.25463755395843796], 
reward next is 0.7454, 
noisyNet noise sample is [array([1.2434672], dtype=float32), 1.8336121]. 
=============================================
[2019-04-01 20:15:25,069] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3257179e-33 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:15:25,069] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9146
[2019-04-01 20:15:25,149] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 91.0, 24.0, 18.0, 26.0, 25.41612444826598, 7.214149330091352, 1.0, 1.0, 47.27130252593282], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2275200.0000, 
sim time next is 2275800.0000, 
raw observation next is [-9.316666666666666, 90.33333333333334, 31.0, 17.33333333333333, 26.0, 25.38887712337624, 7.101577098242113, 1.0, 1.0, 44.53587739344936], 
processed observation next is [1.0, 0.34782608695652173, 0.20452446906740537, 0.9033333333333334, 0.10333333333333333, 0.01915285451197053, 1.0, 0.9126967319108914, 0.07101577098242112, 1.0, 1.0, 0.31811340995320975], 
reward next is 0.6819, 
noisyNet noise sample is [array([0.33108518], dtype=float32), -1.5696813]. 
=============================================
[2019-04-01 20:15:39,678] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:15:39,678] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4167
[2019-04-01 20:15:39,696] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 34.33333333333334, 0.0, 0.0, 26.0, 25.26789893643598, 6.837626528364038, 0.0, 1.0, 39.73218672344971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2497200.0000, 
sim time next is 2497800.0000, 
raw observation next is [-1.2, 33.66666666666666, 0.0, 0.0, 26.0, 25.25401418798701, 6.76711145604791, 0.0, 1.0, 39.80546057106492], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.33666666666666656, 0.0, 0.0, 1.0, 0.8934305982838586, 0.0676711145604791, 0.0, 1.0, 0.28432471836474943], 
reward next is 0.7157, 
noisyNet noise sample is [array([0.59932953], dtype=float32), 0.17367321]. 
=============================================
[2019-04-01 20:15:42,377] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:15:42,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4785
[2019-04-01 20:15:42,415] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4166666666666667, 35.83333333333333, 0.0, 0.0, 26.0, 25.03089221587036, 7.37008188039641, 1.0, 1.0, 22.98614382473117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2573400.0000, 
sim time next is 2574000.0000, 
raw observation next is [-0.6, 36.0, 0.0, 0.0, 26.0, 24.9516764753545, 7.451083563622397, 1.0, 1.0, 31.59339637750491], 
processed observation next is [1.0, 0.8260869565217391, 0.44598337950138506, 0.36, 0.0, 0.0, 1.0, 0.8502394964792143, 0.07451083563622397, 1.0, 1.0, 0.22566711698217792], 
reward next is 0.7743, 
noisyNet noise sample is [array([0.84731215], dtype=float32), -1.3519706]. 
=============================================
[2019-04-01 20:15:42,420] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.81067 ]
 [73.6984  ]
 [73.52549 ]
 [73.401   ]
 [73.253075]], R is [[73.95007324]
 [74.04638672]
 [74.18632507]
 [74.31980133]
 [74.44848633]].
[2019-04-01 20:15:43,235] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3516484e-26 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:15:43,237] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0747
[2019-04-01 20:15:43,274] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.9666666666666667, 38.66666666666667, 0.0, 0.0, 26.0, 25.02043487218051, 7.824534305853757, 0.0, 1.0, 35.27713076339673], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2575200.0000, 
sim time next is 2575800.0000, 
raw observation next is [-1.15, 40.0, 0.0, 0.0, 26.0, 25.02140125347888, 7.792779319741835, 0.0, 1.0, 31.36765763650358], 
processed observation next is [1.0, 0.8260869565217391, 0.4307479224376732, 0.4, 0.0, 0.0, 1.0, 0.8602001790684116, 0.07792779319741835, 0.0, 1.0, 0.224054697403597], 
reward next is 0.7759, 
noisyNet noise sample is [array([1.1319928], dtype=float32), 0.502052]. 
=============================================
[2019-04-01 20:15:46,178] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:15:46,178] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0491
[2019-04-01 20:15:46,190] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 59.0, 0.0, 0.0, 26.0, 25.04969687751868, 7.214267861652719, 0.0, 1.0, 41.5753729216743], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2588400.0000, 
sim time next is 2589000.0000, 
raw observation next is [-4.0, 59.5, 0.0, 0.0, 26.0, 24.99018689294534, 7.084733557764298, 0.0, 1.0, 41.42020876099344], 
processed observation next is [1.0, 1.0, 0.3518005540166205, 0.595, 0.0, 0.0, 1.0, 0.855740984706477, 0.07084733557764297, 0.0, 1.0, 0.295858634007096], 
reward next is 0.7041, 
noisyNet noise sample is [array([-1.8805027], dtype=float32), 1.7840896]. 
=============================================
[2019-04-01 20:15:46,198] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.994576]
 [74.07928 ]
 [74.13239 ]
 [74.12923 ]
 [74.326675]], R is [[73.91783905]
 [73.88169861]
 [73.8442688 ]
 [73.81331635]
 [73.79625702]].
[2019-04-01 20:15:47,488] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.1965905e-29 4.2871650e-26 6.4909486e-27 1.0000000e+00 6.8757251e-31
 1.0916644e-08 9.3153965e-26], sum to 1.0000
[2019-04-01 20:15:47,489] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4510
[2019-04-01 20:15:47,504] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.0, 64.0, 112.5, 790.0, 26.0, 25.88744342656556, 9.984672385189555, 1.0, 1.0, 12.43393748568097], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2721600.0000, 
sim time next is 2722200.0000, 
raw observation next is [-7.666666666666667, 63.16666666666667, 112.6666666666667, 793.0, 26.0, 25.88148319223021, 10.06206498436881, 1.0, 1.0, 11.74539381217627], 
processed observation next is [1.0, 0.5217391304347826, 0.2502308402585411, 0.6316666666666667, 0.37555555555555564, 0.876243093922652, 1.0, 0.9830690274614585, 0.1006206498436881, 1.0, 1.0, 0.08389567008697335], 
reward next is 0.8913, 
noisyNet noise sample is [array([-1.4288055], dtype=float32), 2.2286537]. 
=============================================
[2019-04-01 20:15:48,366] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:15:48,367] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4009
[2019-04-01 20:15:48,414] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 64.0, 0.0, 0.0, 26.0, 25.02287801670641, 9.086255602598484, 0.0, 1.0, 50.25855603409192], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2665800.0000, 
sim time next is 2666400.0000, 
raw observation next is [-1.2, 64.33333333333333, 0.0, 0.0, 26.0, 25.13781255496383, 9.439138518542693, 0.0, 1.0, 46.67739878100616], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.6433333333333333, 0.0, 0.0, 1.0, 0.8768303649948328, 0.09439138518542693, 0.0, 1.0, 0.33340999129290116], 
reward next is 0.6666, 
noisyNet noise sample is [array([-0.43111622], dtype=float32), -0.6536692]. 
=============================================
[2019-04-01 20:15:50,424] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:15:50,424] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1425
[2019-04-01 20:15:50,453] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 64.66666666666667, 0.0, 0.0, 26.0, 25.27168416859771, 9.71520807226041, 0.0, 1.0, 44.59841140857819], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2667000.0000, 
sim time next is 2667600.0000, 
raw observation next is [-1.2, 65.0, 0.0, 0.0, 26.0, 25.36519923150348, 9.96867406625043, 0.0, 1.0, 44.43941347912842], 
processed observation next is [1.0, 0.9130434782608695, 0.42936288088642666, 0.65, 0.0, 0.0, 1.0, 0.9093141759290688, 0.0996867406625043, 0.0, 1.0, 0.31742438199377443], 
reward next is 0.6826, 
noisyNet noise sample is [array([0.66410583], dtype=float32), 0.10021132]. 
=============================================
[2019-04-01 20:15:57,050] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-01 20:15:57,052] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 20:15:57,052] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:15:57,053] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 20:15:57,058] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:15:57,058] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 20:15:57,059] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:15:57,064] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run48
[2019-04-01 20:15:57,089] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run48
[2019-04-01 20:15:57,124] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run48
[2019-04-01 20:17:04,626] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.0965439], dtype=float32), -1.3860934]
[2019-04-01 20:17:04,627] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-6.766666666666667, 65.0, 0.0, 0.0, 26.0, 23.93615278576857, 5.33978776186886, 0.0, 1.0, 40.87916204832484]
[2019-04-01 20:17:04,627] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 20:17:04,627] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.4629353946313882
[2019-04-01 20:17:10,392] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([1.0965439], dtype=float32), -1.3860934]
[2019-04-01 20:17:10,392] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [6.766666666666667, 41.33333333333334, 117.3333333333333, 397.5, 26.0, 24.92104556888551, 7.109335204273907, 0.0, 1.0, 8.859201765906178]
[2019-04-01 20:17:10,392] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 20:17:10,393] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.4466506176615431
[2019-04-01 20:17:20,021] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([1.0965439], dtype=float32), -1.3860934]
[2019-04-01 20:17:20,022] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.0, 65.0, 0.0, 0.0, 26.0, 24.91115320250459, 6.747348056829307, 0.0, 1.0, 37.55532671274752]
[2019-04-01 20:17:20,022] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 20:17:20,022] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.4763748969417273
[2019-04-01 20:17:38,759] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 20:17:42,120] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.0965439], dtype=float32), -1.3860934]
[2019-04-01 20:17:42,120] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-16.01358213833333, 69.77987020500001, 42.78655236666665, 176.2533622666666, 26.0, 24.78759473859063, 6.833109466408892, 1.0, 1.0, 76.25360516864276]
[2019-04-01 20:17:42,120] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 20:17:42,121] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.44260751505780016
[2019-04-01 20:17:58,072] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 20:18:02,473] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 20:18:03,497] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 4700000, evaluation results [4700000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 20:18:05,501] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:18:05,501] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0705
[2019-04-01 20:18:05,600] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 33.5, 0.0, 0.0, 26.0, 23.61501477810715, 6.738620936312739, 1.0, 1.0, 111.6261069969771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2831400.0000, 
sim time next is 2832000.0000, 
raw observation next is [3.666666666666667, 34.66666666666666, 0.0, 0.0, 26.0, 24.20527705701316, 9.364867465847214, 1.0, 1.0, 102.0851031390089], 
processed observation next is [1.0, 0.782608695652174, 0.564173591874423, 0.34666666666666657, 0.0, 0.0, 1.0, 0.743611008144737, 0.09364867465847214, 1.0, 1.0, 0.7291793081357778], 
reward next is 0.2708, 
noisyNet noise sample is [array([0.86765844], dtype=float32), 0.13145837]. 
=============================================
[2019-04-01 20:18:05,608] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[74.88113]
 [75.20007]
 [75.5564 ]
 [75.45187]
 [75.35617]], R is [[74.38845062]
 [73.84723663]
 [73.60401917]
 [73.7687912 ]
 [73.96701813]].
[2019-04-01 20:18:19,807] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:18:19,812] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2288
[2019-04-01 20:18:19,826] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.3333333333333334, 100.0, 0.0, 0.0, 26.0, 25.53553874502287, 11.09118340992476, 0.0, 1.0, 26.65366090425426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3202800.0000, 
sim time next is 3203400.0000, 
raw observation next is [0.1666666666666666, 100.0, 0.0, 0.0, 26.0, 25.40858927875949, 10.8709534505975, 0.0, 1.0, 30.76397024831616], 
processed observation next is [1.0, 0.043478260869565216, 0.4672206832871654, 1.0, 0.0, 0.0, 1.0, 0.9155127541084985, 0.10870953450597501, 0.0, 1.0, 0.2197426446308297], 
reward next is 0.7803, 
noisyNet noise sample is [array([-0.6116374], dtype=float32), 0.64695376]. 
=============================================
[2019-04-01 20:18:19,984] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 4.2630972e-32 7.6417454e-31 1.0000000e+00 8.8370654e-23
 8.2963214e-34 3.7673729e-30], sum to 1.0000
[2019-04-01 20:18:19,984] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5935
[2019-04-01 20:18:19,996] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.666666666666667, 95.33333333333334, 113.1666666666667, 820.0, 26.0, 26.98073338690067, 18.06603506318243, 1.0, 1.0, 4.170810135722771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3154800.0000, 
sim time next is 3155400.0000, 
raw observation next is [7.5, 96.5, 113.0, 823.0, 26.0, 26.99066255639843, 18.13000173129022, 1.0, 1.0, 3.772600497317435], 
processed observation next is [1.0, 0.5217391304347826, 0.6703601108033241, 0.965, 0.37666666666666665, 0.9093922651933701, 1.0, 1.1415232223426328, 0.1813000173129022, 1.0, 1.0, 0.02694714640941025], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.86356425], dtype=float32), 0.67284167]. 
=============================================
[2019-04-01 20:18:25,452] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3202993e-08 6.1664046e-35
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:18:25,453] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0650
[2019-04-01 20:18:25,464] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.48829389253654, 13.34126662090057, 0.0, 1.0, 38.17977586628597], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3195600.0000, 
sim time next is 3196200.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.5583895924699, 13.33553528885692, 0.0, 1.0, 30.88894115178371], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 1.0, 0.9369127989242715, 0.1333553528885692, 0.0, 1.0, 0.2206352939413122], 
reward next is 0.7794, 
noisyNet noise sample is [array([0.23609352], dtype=float32), 0.43568859]. 
=============================================
[2019-04-01 20:18:27,886] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.2812726e-08
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:18:27,886] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2019
[2019-04-01 20:18:27,903] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.31690466424579, 12.81320028297723, 0.0, 1.0, 41.47626711514301], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3194400.0000, 
sim time next is 3195000.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.39734388899686, 13.15860173574619, 0.0, 1.0, 39.97611954682077], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 1.0, 0.9139062698566944, 0.1315860173574619, 0.0, 1.0, 0.2855437110487198], 
reward next is 0.7145, 
noisyNet noise sample is [array([1.561542], dtype=float32), 1.9741489]. 
=============================================
[2019-04-01 20:18:27,917] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.17021 ]
 [73.5327  ]
 [73.28635 ]
 [72.91211 ]
 [72.656044]], R is [[73.0921936 ]
 [73.0650177 ]
 [73.0298996 ]
 [73.01651001]
 [73.07949066]].
[2019-04-01 20:18:29,319] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00
 4.0493423e-25 0.0000000e+00], sum to 1.0000
[2019-04-01 20:18:29,319] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7387
[2019-04-01 20:18:29,333] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 9.0, 104.0, 26.0, 25.91893990181566, 12.05596009645535, 1.0, 1.0, 8.690795343365231], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3259800.0000, 
sim time next is 3260400.0000, 
raw observation next is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.77390969865707, 12.03129562019084, 1.0, 1.0, 8.74782542621138], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.69, 0.0, 0.0, 1.0, 0.9677013855224388, 0.12031295620190839, 1.0, 1.0, 0.062484467330081284], 
reward next is 0.1250, 
noisyNet noise sample is [array([0.54705346], dtype=float32), 0.09617264]. 
=============================================
[2019-04-01 20:18:32,187] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 2.1201778e-36 1.0000000e+00 0.0000000e+00
 6.2887523e-18 1.5304788e-12], sum to 1.0000
[2019-04-01 20:18:32,188] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3831
[2019-04-01 20:18:32,202] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 83.66666666666667, 660.0, 26.0, 26.22836822552494, 15.59513960132423, 1.0, 1.0, 5.099593117921469], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3512400.0000, 
sim time next is 3513000.0000, 
raw observation next is [3.0, 49.0, 79.33333333333333, 633.0, 26.0, 26.56309811564508, 16.70507546321685, 1.0, 1.0, 8.72611012772492], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.2644444444444444, 0.6994475138121546, 1.0, 1.0804425879492972, 0.1670507546321685, 1.0, 1.0, 0.062329358055178], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.092604], dtype=float32), -1.4497188]. 
=============================================
[2019-04-01 20:18:32,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[48.177853]
 [47.904263]
 [47.794655]
 [47.697323]
 [47.33916 ]], R is [[47.90370178]
 [47.42466354]
 [46.95041656]
 [46.48091125]
 [46.01610184]].
[2019-04-01 20:18:41,550] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2355219e-32 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:18:41,553] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6696
[2019-04-01 20:18:41,584] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.09410466896608, 7.896986752868865, 0.0, 1.0, 20.96722104175669], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3611400.0000, 
sim time next is 3612000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.05635885434169, 7.997970014916777, 0.0, 1.0, 37.04152773027153], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 1.0, 0.8651941220488126, 0.07997970014916776, 0.0, 1.0, 0.264582340930511], 
reward next is 0.7354, 
noisyNet noise sample is [array([0.6720684], dtype=float32), -1.7357029]. 
=============================================
[2019-04-01 20:18:41,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[86.759   ]
 [86.61806 ]
 [86.401245]
 [86.0152  ]
 [85.57068 ]], R is [[86.46114349]
 [86.44676971]
 [86.39016724]
 [86.35289764]
 [86.33309174]].
[2019-04-01 20:18:43,931] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2335238e-34 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:18:43,931] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0080
[2019-04-01 20:18:43,956] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.666666666666667, 50.0, 249.8333333333333, 58.83333333333333, 26.0, 25.92199482141633, 11.04762865983979, 1.0, 1.0, 7.599258214709551], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4542000.0000, 
sim time next is 4542600.0000, 
raw observation next is [2.833333333333333, 49.5, 252.6666666666667, 69.66666666666666, 26.0, 26.00917061705982, 9.814651572054084, 1.0, 1.0, 8.081868085609708], 
processed observation next is [1.0, 0.5652173913043478, 0.541089566020314, 0.495, 0.8422222222222224, 0.07697974217311233, 1.0, 1.0013100881514028, 0.09814651572054084, 1.0, 1.0, 0.05772762918292648], 
reward next is 0.9423, 
noisyNet noise sample is [array([-0.83617336], dtype=float32), -0.7825472]. 
=============================================
[2019-04-01 20:18:49,546] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 4.1585948e-35 0.0000000e+00 0.0000000e+00
 2.5855264e-01 7.4144739e-01], sum to 1.0000
[2019-04-01 20:18:49,550] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9974
[2019-04-01 20:18:49,559] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 108.0, 800.0, 26.0, 25.24141447211391, 9.854038194948187, 0.0, 1.0, 10.07867118929968], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3592800.0000, 
sim time next is 3593400.0000, 
raw observation next is [-1.0, 42.0, 106.0, 796.0, 26.0, 25.2317194073733, 9.804208615883033, 0.0, 1.0, 9.715766953468945], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.35333333333333333, 0.8795580110497238, 1.0, 0.8902456296247573, 0.09804208615883032, 0.0, 1.0, 0.06939833538192104], 
reward next is 0.9306, 
noisyNet noise sample is [array([0.77117074], dtype=float32), 1.3390175]. 
=============================================
[2019-04-01 20:18:51,309] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:18:51,309] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4082
[2019-04-01 20:18:51,322] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.166666666666667, 43.33333333333333, 0.0, 0.0, 26.0, 25.5064278497695, 8.526335541131239, 0.0, 1.0, 28.32829624174087], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3618600.0000, 
sim time next is 3619200.0000, 
raw observation next is [-1.333333333333333, 44.66666666666667, 0.0, 0.0, 26.0, 25.45031506441491, 8.369004060009681, 0.0, 1.0, 30.96041752164514], 
processed observation next is [0.0, 0.9130434782608695, 0.42566943674976926, 0.4466666666666667, 0.0, 0.0, 1.0, 0.9214735806307013, 0.08369004060009681, 0.0, 1.0, 0.22114583944032243], 
reward next is 0.7789, 
noisyNet noise sample is [array([0.4668156], dtype=float32), 0.9865273]. 
=============================================
[2019-04-01 20:18:55,810] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:18:55,814] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8798
[2019-04-01 20:18:55,836] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 23.76792600176119, 5.240991415050554, 0.0, 1.0, 46.13691724325466], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3817800.0000, 
sim time next is 3818400.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 23.74941744311095, 5.236229543155673, 0.0, 1.0, 45.97840117303593], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 1.0, 0.6784882061587071, 0.05236229543155673, 0.0, 1.0, 0.3284171512359709], 
reward next is 0.6716, 
noisyNet noise sample is [array([0.6367111], dtype=float32), 1.0044508]. 
=============================================
[2019-04-01 20:18:56,725] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:18:56,728] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4697
[2019-04-01 20:18:56,743] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 59.16666666666666, 0.0, 0.0, 26.0, 25.1743184950017, 9.107886009747986, 0.0, 1.0, 43.61599911578672], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3883800.0000, 
sim time next is 3884400.0000, 
raw observation next is [-1.0, 60.0, 0.0, 0.0, 26.0, 25.16203314833951, 9.385594430088966, 0.0, 1.0, 42.83049620640205], 
processed observation next is [1.0, 1.0, 0.4349030470914128, 0.6, 0.0, 0.0, 1.0, 0.8802904497627873, 0.09385594430088967, 0.0, 1.0, 0.30593211576001467], 
reward next is 0.6941, 
noisyNet noise sample is [array([1.5424353], dtype=float32), 3.0760274]. 
=============================================
[2019-04-01 20:19:00,653] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 8.8528897e-38 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:19:00,654] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0924
[2019-04-01 20:19:00,678] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.6509284725811, 9.438996890875195, 0.0, 1.0, 33.35524835917776], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3897000.0000, 
sim time next is 3897600.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.70846242470078, 9.2937116977201, 0.0, 1.0, 28.51285792569078], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 1.0, 0.9583517749572543, 0.092937116977201, 0.0, 1.0, 0.2036632708977913], 
reward next is 0.7963, 
noisyNet noise sample is [array([-0.38977236], dtype=float32), 1.3011885]. 
=============================================
[2019-04-01 20:19:06,908] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 1. 0. 0. 0.], sum to 1.0000
[2019-04-01 20:19:06,910] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8684
[2019-04-01 20:19:06,938] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.0, 25.0, 17.0, 152.0, 26.0, 27.02044607980417, 18.65903101440218, 1.0, 1.0, 2.800279345064902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4989600.0000, 
sim time next is 4990200.0000, 
raw observation next is [6.0, 24.66666666666667, 0.0, 0.0, 26.0, 27.07719324233955, 18.8929229024034, 1.0, 1.0, 2.585287213917853], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2466666666666667, 0.0, 0.0, 1.0, 1.15388474890565, 0.18892922902403397, 1.0, 1.0, 0.018466337242270377], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5341854], dtype=float32), 1.564418]. 
=============================================
[2019-04-01 20:19:09,951] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
 1.000000e+00 8.870033e-37], sum to 1.0000
[2019-04-01 20:19:09,952] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7585
[2019-04-01 20:19:09,960] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 58.33333333333334, 203.8333333333333, 15.0, 26.0, 26.24269664260105, 11.41779584837533, 1.0, 1.0, 9.528035199599548], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4531200.0000, 
sim time next is 4531800.0000, 
raw observation next is [1.833333333333333, 57.66666666666666, 186.6666666666667, 12.0, 26.0, 26.24365773712394, 11.43266935774807, 1.0, 1.0, 9.019797758054825], 
processed observation next is [1.0, 0.43478260869565216, 0.5133887349953832, 0.5766666666666665, 0.6222222222222223, 0.013259668508287293, 1.0, 1.0348082481605627, 0.1143266935774807, 1.0, 1.0, 0.06442712684324875], 
reward next is 0.3625, 
noisyNet noise sample is [array([1.1575807], dtype=float32), -1.0623218]. 
=============================================
[2019-04-01 20:19:11,932] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:19:11,932] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6436
[2019-04-01 20:19:11,946] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.9, 61.33333333333334, 0.0, 0.0, 26.0, 25.56307833497701, 9.895099306295522, 0.0, 1.0, 26.7587434507585], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4579800.0000, 
sim time next is 4580400.0000, 
raw observation next is [0.8, 61.66666666666667, 0.0, 0.0, 26.0, 25.53497821692762, 9.756870746546278, 0.0, 1.0, 25.8480586788424], 
processed observation next is [1.0, 0.0, 0.4847645429362882, 0.6166666666666667, 0.0, 0.0, 1.0, 0.9335683167039457, 0.09756870746546278, 0.0, 1.0, 0.18462899056315998], 
reward next is 0.8154, 
noisyNet noise sample is [array([0.6359025], dtype=float32), 1.6292113]. 
=============================================
[2019-04-01 20:19:12,479] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:19:12,479] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:19:12,506] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run36
[2019-04-01 20:19:23,767] A3C_AGENT_WORKER-Thread-17 INFO:Local step 297500, global step 4741413: loss 0.0567
[2019-04-01 20:19:23,767] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 297500, global step 4741413: learning rate 0.0010
[2019-04-01 20:19:23,917] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3888270e-28 1.4714540e-27 6.1267555e-20 2.4352056e-35 6.9585859e-29
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:19:23,919] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8565
[2019-04-01 20:19:23,953] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 24.91673694315713, 9.333731232668079, 1.0, 1.0, 33.3049477375988], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4564800.0000, 
sim time next is 4565400.0000, 
raw observation next is [2.0, 52.83333333333334, 0.0, 0.0, 26.0, 25.07601152764276, 9.627043007224117, 1.0, 1.0, 24.03160158412066], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.5283333333333334, 0.0, 0.0, 1.0, 0.8680016468061085, 0.09627043007224118, 1.0, 1.0, 0.1716542970294333], 
reward next is 0.8283, 
noisyNet noise sample is [array([1.6816404], dtype=float32), 1.8120615]. 
=============================================
[2019-04-01 20:19:25,068] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:19:25,069] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1261
[2019-04-01 20:19:25,111] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666666, 53.33333333333334, 294.3333333333334, 212.0, 26.0, 25.15437697237307, 7.831653887716899, 0.0, 1.0, 16.8180646998048], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4877400.0000, 
sim time next is 4878000.0000, 
raw observation next is [-0.4, 52.0, 291.5, 236.0, 26.0, 25.10257752357125, 7.754271421962282, 0.0, 1.0, 15.92481633342155], 
processed observation next is [0.0, 0.4782608695652174, 0.45152354570637127, 0.52, 0.9716666666666667, 0.26077348066298345, 1.0, 0.871796789081607, 0.07754271421962282, 0.0, 1.0, 0.11374868809586822], 
reward next is 0.8863, 
noisyNet noise sample is [array([-0.41830114], dtype=float32), -1.1165056]. 
=============================================
[2019-04-01 20:19:25,115] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.5806 ]
 [76.23963]
 [75.96832]
 [75.27091]
 [74.72885]], R is [[77.2405777 ]
 [77.34803772]
 [77.44756317]
 [77.53870392]
 [77.62097931]].
[2019-04-01 20:19:25,405] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 8.8963580e-38 2.0180555e-35 1.2977421e-26
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:19:25,407] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9614
[2019-04-01 20:19:25,455] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 68.0, 18.5, 4.5, 26.0, 25.0408849993947, 6.723921530242947, 1.0, 1.0, 50.66916836131928], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 115200.0000, 
sim time next is 115800.0000, 
raw observation next is [-7.383333333333333, 66.83333333333334, 24.66666666666667, 6.000000000000001, 26.0, 25.05577302873838, 6.605392564651005, 1.0, 1.0, 47.84985214165896], 
processed observation next is [1.0, 0.34782608695652173, 0.25807940904893817, 0.6683333333333334, 0.08222222222222224, 0.006629834254143647, 1.0, 0.8651104326769113, 0.06605392564651005, 1.0, 1.0, 0.3417846581547068], 
reward next is 0.6582, 
noisyNet noise sample is [array([0.36896548], dtype=float32), -1.7798866]. 
=============================================
[2019-04-01 20:19:29,645] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:19:29,656] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6934
[2019-04-01 20:19:29,670] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.3, 84.0, 142.5, 131.5, 26.0, 26.29636604909967, 12.88124394641018, 1.0, 1.0, 14.7119518883803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4438800.0000, 
sim time next is 4439400.0000, 
raw observation next is [1.25, 84.33333333333333, 150.0, 97.99999999999999, 26.0, 26.22775841416062, 12.72117841920628, 1.0, 1.0, 12.85394816897209], 
processed observation next is [1.0, 0.391304347826087, 0.497229916897507, 0.8433333333333333, 0.5, 0.10828729281767954, 1.0, 1.0325369163086597, 0.1272117841920628, 1.0, 1.0, 0.09181391549265779], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.724251], dtype=float32), -0.18330954]. 
=============================================
[2019-04-01 20:19:33,878] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4884451e-22 0.0000000e+00
 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-04-01 20:19:33,889] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2036
[2019-04-01 20:19:33,899] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.9, 49.66666666666666, 201.6666666666667, 520.6666666666666, 26.0, 26.95891835807524, 19.48766864281803, 1.0, 1.0, 1.44099424405037], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4632000.0000, 
sim time next is 4632600.0000, 
raw observation next is [4.95, 49.83333333333334, 200.3333333333333, 442.3333333333334, 26.0, 27.1858187503557, 20.50747251729536, 1.0, 1.0, 1.363849330125306], 
processed observation next is [1.0, 0.6086956521739131, 0.5997229916897507, 0.4983333333333334, 0.6677777777777776, 0.48876611418047894, 1.0, 1.169402678622243, 0.20507472517295358, 1.0, 1.0, 0.009741780929466473], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.56081206], dtype=float32), 0.1611085]. 
=============================================
[2019-04-01 20:19:34,582] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9558724e-32 0.0000000e+00 0.0000000e+00 1.0000000e+00 1.7810537e-16
 2.2849632e-10 0.0000000e+00], sum to 1.0000
[2019-04-01 20:19:34,582] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7181
[2019-04-01 20:19:34,639] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 24.89700053932163, 8.36109886260713, 1.0, 1.0, 26.86850875972883], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4563600.0000, 
sim time next is 4564200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 24.84149940355686, 8.918264997657593, 0.0, 1.0, 44.76949516520358], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 0.8344999147938371, 0.08918264997657593, 0.0, 1.0, 0.31978210832288273], 
reward next is 0.6802, 
noisyNet noise sample is [array([-0.21970403], dtype=float32), -1.4733552]. 
=============================================
[2019-04-01 20:19:36,257] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:19:36,257] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0562
[2019-04-01 20:19:36,271] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.28523196669772, 8.19486872148593, 0.0, 1.0, 31.38100532527536], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4684800.0000, 
sim time next is 4685400.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.20085005198557, 8.932750249548228, 0.0, 1.0, 42.00913096372383], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 1.0, 0.0, 0.0, 1.0, 0.8858357217122241, 0.08932750249548228, 0.0, 1.0, 0.3000652211694559], 
reward next is 0.6999, 
noisyNet noise sample is [array([0.06565079], dtype=float32), -0.4612252]. 
=============================================
[2019-04-01 20:19:36,460] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:19:36,460] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:19:36,471] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run36
[2019-04-01 20:19:37,970] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:19:37,971] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:19:38,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run36
[2019-04-01 20:19:47,214] A3C_AGENT_WORKER-Thread-17 INFO:Local step 298000, global step 4753363: loss 15.0117
[2019-04-01 20:19:47,217] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 298000, global step 4753366: learning rate 0.0010
[2019-04-01 20:19:48,149] A3C_AGENT_WORKER-Thread-18 INFO:Local step 297500, global step 4753840: loss 0.1001
[2019-04-01 20:19:48,150] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 297500, global step 4753840: learning rate 0.0010
[2019-04-01 20:19:49,709] A3C_AGENT_WORKER-Thread-4 INFO:Local step 297500, global step 4754703: loss 0.0397
[2019-04-01 20:19:49,710] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 297500, global step 4754703: learning rate 0.0010
[2019-04-01 20:19:50,547] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:19:50,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:19:50,559] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run36
[2019-04-01 20:19:51,339] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00
 2.306723e-13 0.000000e+00], sum to 1.0000
[2019-04-01 20:19:51,340] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7236
[2019-04-01 20:19:51,398] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.1, 61.0, 130.0, 603.0, 26.0, 25.67280109206044, 8.838356817909933, 1.0, 1.0, 27.17711909654282], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 131400.0000, 
sim time next is 132000.0000, 
raw observation next is [-8.0, 61.0, 131.5, 583.1666666666666, 26.0, 25.54967821647951, 8.837246168559846, 1.0, 1.0, 38.77079125428948], 
processed observation next is [1.0, 0.5217391304347826, 0.24099722991689754, 0.61, 0.43833333333333335, 0.6443830570902394, 1.0, 0.93566831663993, 0.08837246168559845, 1.0, 1.0, 0.27693422324492484], 
reward next is 0.7231, 
noisyNet noise sample is [array([-2.2755141], dtype=float32), -0.25802281]. 
=============================================
[2019-04-01 20:19:51,404] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[33.153305]
 [33.839546]
 [34.51178 ]
 [35.325428]
 [36.588707]], R is [[33.3621521 ]
 [33.83440781]
 [34.27719116]
 [34.6841011 ]
 [35.02721024]].
[2019-04-01 20:19:53,746] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:19:53,747] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7661
[2019-04-01 20:19:53,760] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 26.0, 25.41330187403376, 7.928839728400878, 0.0, 1.0, 34.04223664257624], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4915200.0000, 
sim time next is 4915800.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 26.0, 25.37084174430822, 7.928256519463645, 0.0, 1.0, 37.42340116187054], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 1.0, 0.9101202491868884, 0.07928256519463645, 0.0, 1.0, 0.26731000829907525], 
reward next is 0.7327, 
noisyNet noise sample is [array([-1.4247758], dtype=float32), 0.8963441]. 
=============================================
[2019-04-01 20:19:54,339] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:19:54,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:19:54,374] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run36
[2019-04-01 20:19:54,410] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 9.7802516e-37 1.0000000e+00 0.0000000e+00
 4.1068503e-27 4.5635633e-09], sum to 1.0000
[2019-04-01 20:19:54,414] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7777
[2019-04-01 20:19:54,435] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.833333333333334, 24.16666666666666, 121.0, 863.3333333333334, 26.0, 27.07779260463059, 16.635204738839, 1.0, 1.0, 2.057505607161835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4971000.0000, 
sim time next is 4971600.0000, 
raw observation next is [7.0, 24.0, 120.0, 862.5, 26.0, 27.07513563814318, 16.87026641552623, 1.0, 1.0, 2.037051157749253], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 0.24, 0.4, 0.9530386740331491, 1.0, 1.1535908054490258, 0.1687026641552623, 1.0, 1.0, 0.014550365412494666], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3199845], dtype=float32), -1.2657948]. 
=============================================
[2019-04-01 20:19:56,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:19:56,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:19:56,046] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run36
[2019-04-01 20:19:56,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:19:56,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:19:56,402] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run36
[2019-04-01 20:19:56,963] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:19:56,963] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:19:57,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run36
[2019-04-01 20:19:59,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:19:59,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:19:59,274] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run36
[2019-04-01 20:19:59,933] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 1.7639615e-28 1.2553849e-16 2.1319702e-34
 9.9999905e-01 9.6668157e-07], sum to 1.0000
[2019-04-01 20:19:59,935] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9346
[2019-04-01 20:19:59,973] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.0, 17.0, 56.0, 438.5, 26.0, 29.17556096373134, 29.00635964285925, 1.0, 1.0, 0.6278682316008353], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5072400.0000, 
sim time next is 5073000.0000, 
raw observation next is [11.83333333333333, 17.0, 49.33333333333333, 389.6666666666666, 26.0, 29.16014469743265, 35.03884069569188, 1.0, 1.0, 0.6417399411046657], 
processed observation next is [1.0, 0.7391304347826086, 0.7903970452446908, 0.17, 0.16444444444444442, 0.4305709023941067, 1.0, 1.4514492424903784, 0.3503884069569188, 1.0, 1.0, 0.004583856722176183], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.9353658], dtype=float32), 1.086812]. 
=============================================
[2019-04-01 20:19:59,987] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[41.78502 ]
 [40.16035 ]
 [39.190586]
 [38.326393]
 [37.636795]], R is [[42.54560852]
 [42.12015152]
 [41.69895172]
 [41.28196335]
 [40.86914444]].
[2019-04-01 20:20:00,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:20:00,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:20:00,947] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run36
[2019-04-01 20:20:01,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:20:01,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:20:01,170] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run36
[2019-04-01 20:20:01,421] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 6.3693043e-22 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:20:01,422] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3584
[2019-04-01 20:20:01,434] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.0, 19.0, 0.0, 0.0, 26.0, 27.19765899032482, 20.9381872907906, 0.0, 1.0, 4.425542034374822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5083200.0000, 
sim time next is 5083800.0000, 
raw observation next is [9.833333333333334, 19.0, 0.0, 0.0, 26.0, 27.15090997055037, 20.56469006626707, 0.0, 1.0, 4.719999976766699], 
processed observation next is [1.0, 0.8695652173913043, 0.7349953831948293, 0.19, 0.0, 0.0, 1.0, 1.164415710078624, 0.2056469006626707, 0.0, 1.0, 0.03371428554833356], 
reward next is 0.9663, 
noisyNet noise sample is [array([-1.0985659], dtype=float32), -2.3065305]. 
=============================================
[2019-04-01 20:20:01,678] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:20:01,678] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:20:01,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run36
[2019-04-01 20:20:02,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:20:02,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:20:02,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run36
[2019-04-01 20:20:02,519] A3C_AGENT_WORKER-Thread-2 INFO:Local step 297500, global step 4759851: loss 0.0271
[2019-04-01 20:20:02,520] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 297500, global step 4759851: learning rate 0.0010
[2019-04-01 20:20:02,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:20:02,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:20:02,898] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run36
[2019-04-01 20:20:02,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:20:02,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:20:02,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run36
[2019-04-01 20:20:03,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:20:03,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:20:03,397] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run36
[2019-04-01 20:20:06,955] A3C_AGENT_WORKER-Thread-20 INFO:Local step 297500, global step 4760497: loss 0.0099
[2019-04-01 20:20:06,957] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 297500, global step 4760497: learning rate 0.0010
[2019-04-01 20:20:09,608] A3C_AGENT_WORKER-Thread-10 INFO:Local step 297500, global step 4760990: loss 0.0543
[2019-04-01 20:20:09,610] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 297500, global step 4760990: learning rate 0.0010
[2019-04-01 20:20:10,188] A3C_AGENT_WORKER-Thread-16 INFO:Local step 297500, global step 4761197: loss 0.2606
[2019-04-01 20:20:10,189] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 297500, global step 4761198: learning rate 0.0010
[2019-04-01 20:20:10,756] A3C_AGENT_WORKER-Thread-17 INFO:Local step 298500, global step 4761441: loss 0.0354
[2019-04-01 20:20:10,759] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 298500, global step 4761442: learning rate 0.0010
[2019-04-01 20:20:11,007] A3C_AGENT_WORKER-Thread-15 INFO:Local step 297500, global step 4761556: loss 0.1767
[2019-04-01 20:20:11,009] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 297500, global step 4761556: learning rate 0.0010
[2019-04-01 20:20:11,414] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:20:11,415] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1402
[2019-04-01 20:20:11,444] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 74.5, 0.0, 0.0, 26.0, 24.36140605383758, 5.421556228787405, 0.0, 1.0, 40.5997864040094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 697800.0000, 
sim time next is 698400.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.36112607782151, 5.403282042074068, 0.0, 1.0, 40.59354526647478], 
processed observation next is [1.0, 0.08695652173913043, 0.368421052631579, 0.75, 0.0, 0.0, 1.0, 0.7658751539745013, 0.054032820420740674, 0.0, 1.0, 0.2899538947605341], 
reward next is 0.7100, 
noisyNet noise sample is [array([0.26158017], dtype=float32), 0.42169097]. 
=============================================
[2019-04-01 20:20:12,487] A3C_AGENT_WORKER-Thread-9 INFO:Local step 297500, global step 4762048: loss 0.1022
[2019-04-01 20:20:12,488] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 297500, global step 4762048: learning rate 0.0010
[2019-04-01 20:20:13,018] A3C_AGENT_WORKER-Thread-18 INFO:Local step 298000, global step 4762178: loss 18.6670
[2019-04-01 20:20:13,020] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 298000, global step 4762178: learning rate 0.0010
[2019-04-01 20:20:14,106] A3C_AGENT_WORKER-Thread-4 INFO:Local step 298000, global step 4762485: loss 9.6264
[2019-04-01 20:20:14,108] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 298000, global step 4762485: learning rate 0.0010
[2019-04-01 20:20:14,213] A3C_AGENT_WORKER-Thread-11 INFO:Local step 297500, global step 4762526: loss 0.0059
[2019-04-01 20:20:14,215] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 297500, global step 4762526: learning rate 0.0010
[2019-04-01 20:20:14,647] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00
 1.1952337e-21 7.8586811e-15], sum to 1.0000
[2019-04-01 20:20:14,648] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0899
[2019-04-01 20:20:14,706] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.8, 51.0, 58.0, 834.5, 26.0, 26.25862579682494, 9.12644995980542, 1.0, 1.0, 34.74950002232195], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 388800.0000, 
sim time next is 389400.0000, 
raw observation next is [-12.61666666666667, 51.0, 58.0, 858.0, 26.0, 25.76779519992379, 8.499374452670743, 1.0, 1.0, 32.44575270067882], 
processed observation next is [1.0, 0.5217391304347826, 0.11311172668513378, 0.51, 0.19333333333333333, 0.9480662983425414, 1.0, 0.9668278857033984, 0.08499374452670744, 1.0, 1.0, 0.23175537643342012], 
reward next is 0.7682, 
noisyNet noise sample is [array([0.47825414], dtype=float32), 2.0922863]. 
=============================================
[2019-04-01 20:20:14,950] A3C_AGENT_WORKER-Thread-14 INFO:Local step 297500, global step 4762760: loss 0.0732
[2019-04-01 20:20:14,951] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 297500, global step 4762760: learning rate 0.0010
[2019-04-01 20:20:15,391] A3C_AGENT_WORKER-Thread-3 INFO:Local step 297500, global step 4762908: loss 0.0469
[2019-04-01 20:20:15,391] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 297500, global step 4762908: learning rate 0.0010
[2019-04-01 20:20:15,824] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9134302e-33
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:20:15,826] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7800
[2019-04-01 20:20:15,826] A3C_AGENT_WORKER-Thread-13 INFO:Local step 297500, global step 4763054: loss 0.1653
[2019-04-01 20:20:15,826] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 297500, global step 4763054: learning rate 0.0010
[2019-04-01 20:20:15,837] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0668790e-27 2.8963664e-31 1.9185845e-24 2.1682313e-20 2.7705934e-16
 1.0000000e+00 9.4348537e-24], sum to 1.0000
[2019-04-01 20:20:15,837] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5274
[2019-04-01 20:20:15,860] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.066666666666667, 88.0, 0.0, 0.0, 26.0, 24.77061558055967, 7.131894797621793, 0.0, 1.0, 41.09891167421696], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 70800.0000, 
sim time next is 71400.0000, 
raw observation next is [2.883333333333334, 88.5, 0.0, 0.0, 26.0, 24.80198998031904, 7.108368646721824, 0.0, 1.0, 40.85863301135833], 
processed observation next is [0.0, 0.8260869565217391, 0.5424746075715605, 0.885, 0.0, 0.0, 1.0, 0.8288557114741485, 0.07108368646721824, 0.0, 1.0, 0.2918473786525595], 
reward next is 0.7082, 
noisyNet noise sample is [array([-0.07314627], dtype=float32), 0.52867746]. 
=============================================
[2019-04-01 20:20:15,935] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 26.0, 24.10015083160152, 6.198109915423781, 1.0, 1.0, 84.46282825003047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 113400.0000, 
sim time next is 114000.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 26.0, 24.70825786237226, 6.580717039163413, 0.0, 1.0, 72.41699787855738], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 1.0, 0.8154654089103229, 0.06580717039163413, 0.0, 1.0, 0.5172642705611241], 
reward next is 0.4827, 
noisyNet noise sample is [array([1.8781228], dtype=float32), 0.13054103]. 
=============================================
[2019-04-01 20:20:15,950] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[46.02794 ]
 [51.34247 ]
 [59.605713]
 [61.0857  ]
 [60.984573]], R is [[50.86153793]
 [50.74961853]
 [50.55194855]
 [50.2461853 ]
 [50.42105865]].
[2019-04-01 20:20:16,407] A3C_AGENT_WORKER-Thread-12 INFO:Local step 297500, global step 4763255: loss 0.1390
[2019-04-01 20:20:16,412] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 297500, global step 4763255: learning rate 0.0010
[2019-04-01 20:20:16,455] A3C_AGENT_WORKER-Thread-5 INFO:Local step 297500, global step 4763278: loss 0.0755
[2019-04-01 20:20:16,456] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 297500, global step 4763278: learning rate 0.0010
[2019-04-01 20:20:16,848] A3C_AGENT_WORKER-Thread-19 INFO:Local step 297500, global step 4763429: loss 0.1588
[2019-04-01 20:20:16,849] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 297500, global step 4763429: learning rate 0.0010
[2019-04-01 20:20:23,563] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3581581e-30 0.0000000e+00
 1.0000000e+00 1.1675624e-31], sum to 1.0000
[2019-04-01 20:20:23,563] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0059
[2019-04-01 20:20:23,589] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.22052008443885, 5.971864200881406, 0.0, 1.0, 44.40745843913739], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 167400.0000, 
sim time next is 168000.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.18168351170392, 5.909367016529257, 0.0, 1.0, 44.36441362494914], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 1.0, 0.7402405016719885, 0.059093670165292565, 0.0, 1.0, 0.31688866874963667], 
reward next is 0.6831, 
noisyNet noise sample is [array([-0.17508273], dtype=float32), 1.8224964]. 
=============================================
[2019-04-01 20:20:23,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[55.59972 ]
 [55.930424]
 [56.615566]
 [57.285934]
 [58.7644  ]], R is [[55.5192337 ]
 [55.64684677]
 [55.7726326 ]
 [55.89669418]
 [56.01911545]].
[2019-04-01 20:20:24,222] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:20:24,224] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8739
[2019-04-01 20:20:24,259] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.366666666666667, 68.33333333333333, 0.0, 0.0, 26.0, 23.83168571581204, 5.356330017438542, 0.0, 1.0, 45.04608835422837], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 268800.0000, 
sim time next is 269400.0000, 
raw observation next is [-8.633333333333333, 67.66666666666667, 0.0, 0.0, 26.0, 23.74252567067604, 5.352487563372549, 0.0, 1.0, 45.1278456049296], 
processed observation next is [1.0, 0.08695652173913043, 0.22345337026777473, 0.6766666666666667, 0.0, 0.0, 1.0, 0.6775036672394344, 0.05352487563372549, 0.0, 1.0, 0.3223417543209257], 
reward next is 0.6777, 
noisyNet noise sample is [array([-0.09393305], dtype=float32), -0.40394324]. 
=============================================
[2019-04-01 20:20:26,207] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:20:26,210] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8475
[2019-04-01 20:20:26,239] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.883333333333334, 82.66666666666667, 0.0, 0.0, 26.0, 24.76062617296093, 6.304959545570725, 0.0, 1.0, 39.57506717989494], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 532200.0000, 
sim time next is 532800.0000, 
raw observation next is [2.7, 82.0, 0.0, 0.0, 26.0, 24.7392236340188, 6.265609159249529, 0.0, 1.0, 39.62406847254118], 
processed observation next is [0.0, 0.17391304347826086, 0.5373961218836566, 0.82, 0.0, 0.0, 1.0, 0.8198890905741142, 0.06265609159249529, 0.0, 1.0, 0.2830290605181513], 
reward next is 0.7170, 
noisyNet noise sample is [array([1.4951204], dtype=float32), -1.2604208]. 
=============================================
[2019-04-01 20:20:28,223] A3C_AGENT_WORKER-Thread-2 INFO:Local step 298000, global step 4767071: loss 0.8794
[2019-04-01 20:20:28,224] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 298000, global step 4767071: learning rate 0.0010
[2019-04-01 20:20:31,337] A3C_AGENT_WORKER-Thread-17 INFO:Local step 299000, global step 4768129: loss 16.5916
[2019-04-01 20:20:31,338] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 299000, global step 4768129: learning rate 0.0010
[2019-04-01 20:20:32,226] A3C_AGENT_WORKER-Thread-20 INFO:Local step 298000, global step 4768486: loss 0.2095
[2019-04-01 20:20:32,226] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 298000, global step 4768486: learning rate 0.0010
[2019-04-01 20:20:33,945] A3C_AGENT_WORKER-Thread-10 INFO:Local step 298000, global step 4769060: loss -5.9269
[2019-04-01 20:20:33,946] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 298000, global step 4769060: learning rate 0.0010
[2019-04-01 20:20:35,265] A3C_AGENT_WORKER-Thread-16 INFO:Local step 298000, global step 4769418: loss 3.3842
[2019-04-01 20:20:35,267] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 298000, global step 4769418: learning rate 0.0010
[2019-04-01 20:20:35,792] A3C_AGENT_WORKER-Thread-15 INFO:Local step 298000, global step 4769571: loss 2.5632
[2019-04-01 20:20:35,792] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 298000, global step 4769571: learning rate 0.0010
[2019-04-01 20:20:37,219] A3C_AGENT_WORKER-Thread-18 INFO:Local step 298500, global step 4770048: loss 0.0239
[2019-04-01 20:20:37,219] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 298500, global step 4770048: learning rate 0.0010
[2019-04-01 20:20:37,729] A3C_AGENT_WORKER-Thread-9 INFO:Local step 298000, global step 4770271: loss 0.0459
[2019-04-01 20:20:37,731] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 298000, global step 4770271: learning rate 0.0010
[2019-04-01 20:20:37,937] A3C_AGENT_WORKER-Thread-4 INFO:Local step 298500, global step 4770357: loss 0.0059
[2019-04-01 20:20:37,937] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 298500, global step 4770357: learning rate 0.0010
[2019-04-01 20:20:39,163] A3C_AGENT_WORKER-Thread-14 INFO:Local step 298000, global step 4770905: loss 0.5093
[2019-04-01 20:20:39,164] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 298000, global step 4770905: learning rate 0.0010
[2019-04-01 20:20:39,520] A3C_AGENT_WORKER-Thread-11 INFO:Local step 298000, global step 4771051: loss 0.2326
[2019-04-01 20:20:39,520] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 298000, global step 4771051: learning rate 0.0010
[2019-04-01 20:20:39,589] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:20:39,590] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2452
[2019-04-01 20:20:39,642] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.1, 73.33333333333334, 89.0, 40.83333333333334, 26.0, 25.85373522507668, 7.649027198241927, 1.0, 1.0, 24.44600956491043], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 724800.0000, 
sim time next is 725400.0000, 
raw observation next is [-2.0, 72.0, 101.0, 49.0, 26.0, 25.91102156575158, 7.76471690675002, 1.0, 1.0, 23.08282780794525], 
processed observation next is [1.0, 0.391304347826087, 0.40720221606648205, 0.72, 0.33666666666666667, 0.05414364640883978, 1.0, 0.9872887951073684, 0.0776471690675002, 1.0, 1.0, 0.1648773414853232], 
reward next is 0.8351, 
noisyNet noise sample is [array([0.14951152], dtype=float32), -0.1250405]. 
=============================================
[2019-04-01 20:20:40,149] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:20:40,149] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9039
[2019-04-01 20:20:40,251] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.28333333333333, 78.5, 0.0, 0.0, 26.0, 21.58396698883535, 9.426194244558552, 0.0, 1.0, 117.6588908841844], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 371400.0000, 
sim time next is 372000.0000, 
raw observation next is [-16.36666666666667, 79.0, 0.0, 0.0, 26.0, 21.8582861111403, 7.616933968579628, 1.0, 1.0, 101.0157909872595], 
processed observation next is [1.0, 0.30434782608695654, 0.009233610341643453, 0.79, 0.0, 0.0, 1.0, 0.40832658730575694, 0.07616933968579627, 1.0, 1.0, 0.7215413641947107], 
reward next is 0.2785, 
noisyNet noise sample is [array([1.1355982], dtype=float32), 1.7372581]. 
=============================================
[2019-04-01 20:20:40,262] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[63.103878]
 [64.526276]
 [64.49899 ]
 [64.52098 ]
 [64.55747 ]], R is [[56.83416748]
 [56.42540741]
 [56.51370239]
 [56.60155106]
 [56.68932343]].
[2019-04-01 20:20:40,475] A3C_AGENT_WORKER-Thread-3 INFO:Local step 298000, global step 4771406: loss 1.6227
[2019-04-01 20:20:40,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 298000, global step 4771406: learning rate 0.0010
[2019-04-01 20:20:40,657] A3C_AGENT_WORKER-Thread-13 INFO:Local step 298000, global step 4771463: loss 4.3158
[2019-04-01 20:20:40,658] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 298000, global step 4771463: learning rate 0.0010
[2019-04-01 20:20:40,719] A3C_AGENT_WORKER-Thread-5 INFO:Local step 298000, global step 4771483: loss 1.9330
[2019-04-01 20:20:40,721] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 298000, global step 4771484: learning rate 0.0010
[2019-04-01 20:20:41,427] A3C_AGENT_WORKER-Thread-12 INFO:Local step 298000, global step 4771707: loss 0.7342
[2019-04-01 20:20:41,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 298000, global step 4771707: learning rate 0.0010
[2019-04-01 20:20:41,751] A3C_AGENT_WORKER-Thread-19 INFO:Local step 298000, global step 4771784: loss 3.9243
[2019-04-01 20:20:41,752] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 298000, global step 4771784: learning rate 0.0010
[2019-04-01 20:20:42,067] A3C_AGENT_WORKER-Thread-17 INFO:Local step 299500, global step 4771875: loss 0.0058
[2019-04-01 20:20:42,068] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 299500, global step 4771875: learning rate 0.0010
[2019-04-01 20:20:45,998] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:20:46,001] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5241
[2019-04-01 20:20:46,032] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.8, 52.0, 0.0, 0.0, 26.0, 22.76348826884535, 6.747914367632823, 0.0, 1.0, 45.9958838961857], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 448800.0000, 
sim time next is 449400.0000, 
raw observation next is [-10.7, 52.0, 0.0, 0.0, 26.0, 22.73935828292463, 6.85262361121266, 0.0, 1.0, 46.04567946213142], 
processed observation next is [1.0, 0.17391304347826086, 0.1662049861495845, 0.52, 0.0, 0.0, 1.0, 0.5341940404178044, 0.0685262361121266, 0.0, 1.0, 0.32889771044379584], 
reward next is 0.6711, 
noisyNet noise sample is [array([0.8514133], dtype=float32), -0.17050885]. 
=============================================
[2019-04-01 20:20:46,949] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:20:46,949] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2494
[2019-04-01 20:20:46,965] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.683333333333334, 45.33333333333334, 0.0, 0.0, 26.0, 22.55076207013728, 7.387689940399542, 0.0, 1.0, 45.9633250703524], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 453000.0000, 
sim time next is 453600.0000, 
raw observation next is [-9.5, 44.0, 0.0, 0.0, 26.0, 22.51397860272772, 7.416401339727419, 0.0, 1.0, 45.90086884964033], 
processed observation next is [1.0, 0.2608695652173913, 0.1994459833795014, 0.44, 0.0, 0.0, 1.0, 0.5019969432468169, 0.07416401339727419, 0.0, 1.0, 0.3278633489260024], 
reward next is 0.6721, 
noisyNet noise sample is [array([-0.09303395], dtype=float32), 0.22091304]. 
=============================================
[2019-04-01 20:20:49,404] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 4.3755366e-21
 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:20:49,407] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3983
[2019-04-01 20:20:49,464] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 84.0, 0.0, 0.0, 26.0, 25.05001838989221, 7.394643194221291, 1.0, 1.0, 38.99226722462599], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 496800.0000, 
sim time next is 497400.0000, 
raw observation next is [0.6000000000000001, 86.0, 0.0, 0.0, 26.0, 25.3187454293593, 7.630979599088303, 1.0, 1.0, 31.39702048593931], 
processed observation next is [1.0, 0.782608695652174, 0.479224376731302, 0.86, 0.0, 0.0, 1.0, 0.9026779184799, 0.07630979599088303, 1.0, 1.0, 0.22426443204242366], 
reward next is 0.7757, 
noisyNet noise sample is [array([-0.19078335], dtype=float32), -0.4523097]. 
=============================================
[2019-04-01 20:20:49,842] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4025749e-16 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:20:49,842] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3759
[2019-04-01 20:20:49,894] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 25.04983839353326, 7.440937259584217, 1.0, 1.0, 32.45054608423897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 504000.0000, 
sim time next is 504600.0000, 
raw observation next is [1.183333333333333, 96.0, 0.0, 0.0, 26.0, 25.13053529794428, 7.421126432638872, 1.0, 1.0, 42.72248221487322], 
processed observation next is [1.0, 0.8695652173913043, 0.49538319482917825, 0.96, 0.0, 0.0, 1.0, 0.875790756849183, 0.07421126432638872, 1.0, 1.0, 0.3051605872490944], 
reward next is 0.6948, 
noisyNet noise sample is [array([-0.92860085], dtype=float32), -1.2987741]. 
=============================================
[2019-04-01 20:20:51,606] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:20:51,607] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3160
[2019-04-01 20:20:51,657] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 70.0, 96.33333333333334, 34.00000000000001, 26.0, 25.24132919398767, 6.7957108753624, 0.0, 1.0, 44.16954035548119], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 637800.0000, 
sim time next is 638400.0000, 
raw observation next is [-3.9, 69.0, 115.6666666666667, 42.5, 26.0, 25.19664532492639, 6.712693018332383, 0.0, 1.0, 41.55582212359641], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.69, 0.38555555555555565, 0.04696132596685083, 1.0, 0.8852350464180557, 0.06712693018332383, 0.0, 1.0, 0.2968273008828315], 
reward next is 0.7032, 
noisyNet noise sample is [array([-1.2416196], dtype=float32), 0.47699666]. 
=============================================
[2019-04-01 20:20:51,810] A3C_AGENT_WORKER-Thread-2 INFO:Local step 298500, global step 4775338: loss 0.0156
[2019-04-01 20:20:51,810] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 298500, global step 4775338: learning rate 0.0010
[2019-04-01 20:20:55,650] A3C_AGENT_WORKER-Thread-20 INFO:Local step 298500, global step 4776830: loss 0.0519
[2019-04-01 20:20:55,652] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 298500, global step 4776831: learning rate 0.0010
[2019-04-01 20:20:56,942] A3C_AGENT_WORKER-Thread-18 INFO:Local step 299000, global step 4777341: loss 8.1381
[2019-04-01 20:20:56,944] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 299000, global step 4777341: learning rate 0.0010
[2019-04-01 20:20:57,223] A3C_AGENT_WORKER-Thread-10 INFO:Local step 298500, global step 4777469: loss 0.1102
[2019-04-01 20:20:57,225] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 298500, global step 4777471: learning rate 0.0010
[2019-04-01 20:20:57,995] A3C_AGENT_WORKER-Thread-17 INFO:Local step 300000, global step 4777837: loss 22.9737
[2019-04-01 20:20:57,996] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 300000, global step 4777837: learning rate 0.0010
[2019-04-01 20:20:58,083] A3C_AGENT_WORKER-Thread-4 INFO:Local step 299000, global step 4777877: loss 3.9599
[2019-04-01 20:20:58,085] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 299000, global step 4777877: learning rate 0.0010
[2019-04-01 20:20:58,582] A3C_AGENT_WORKER-Thread-16 INFO:Local step 298500, global step 4778150: loss 0.0046
[2019-04-01 20:20:58,583] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 298500, global step 4778150: learning rate 0.0010
[2019-04-01 20:20:59,004] A3C_AGENT_WORKER-Thread-15 INFO:Local step 298500, global step 4778363: loss 0.0086
[2019-04-01 20:20:59,004] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 298500, global step 4778363: learning rate 0.0010
[2019-04-01 20:20:59,831] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:20:59,831] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7441
[2019-04-01 20:20:59,848] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.55, 79.0, 0.0, 0.0, 26.0, 25.65341353804819, 13.13747156720172, 0.0, 1.0, 30.40836790416785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1056600.0000, 
sim time next is 1057200.0000, 
raw observation next is [13.46666666666667, 79.33333333333333, 0.0, 0.0, 26.0, 25.78298644477719, 13.45180291810751, 0.0, 1.0, 22.74281004303936], 
processed observation next is [1.0, 0.21739130434782608, 0.8356417359187445, 0.7933333333333333, 0.0, 0.0, 1.0, 0.9689980635395986, 0.1345180291810751, 0.0, 1.0, 0.16244864316456686], 
reward next is 0.8376, 
noisyNet noise sample is [array([0.17799991], dtype=float32), -0.2679932]. 
=============================================
[2019-04-01 20:21:00,978] A3C_AGENT_WORKER-Thread-9 INFO:Local step 298500, global step 4779174: loss 0.0615
[2019-04-01 20:21:00,981] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 298500, global step 4779174: learning rate 0.0010
[2019-04-01 20:21:00,999] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:21:01,000] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1142
[2019-04-01 20:21:01,021] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:21:01,021] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3836
[2019-04-01 20:21:01,034] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.683333333333334, 97.0, 0.0, 0.0, 26.0, 25.45915788584557, 10.56969361943119, 0.0, 1.0, 26.35674292276059], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1662600.0000, 
sim time next is 1663200.0000, 
raw observation next is [5.5, 97.0, 0.0, 0.0, 26.0, 25.4060247286252, 10.93963993713233, 0.0, 1.0, 35.33695015233739], 
processed observation next is [1.0, 0.2608695652173913, 0.6149584487534627, 0.97, 0.0, 0.0, 1.0, 0.9151463898036002, 0.10939639937132331, 0.0, 1.0, 0.2524067868024099], 
reward next is 0.7476, 
noisyNet noise sample is [array([0.30319032], dtype=float32), -0.47632268]. 
=============================================
[2019-04-01 20:21:01,046] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.75, 59.5, 182.0, 93.0, 26.0, 24.88898493518173, 7.201401710898502, 0.0, 1.0, 45.54430091384223], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 653400.0000, 
sim time next is 654000.0000, 
raw observation next is [-1.566666666666667, 59.66666666666667, 165.1666666666667, 86.83333333333333, 26.0, 24.97078059501123, 7.30753839661899, 0.0, 1.0, 41.75514970434319], 
processed observation next is [0.0, 0.5652173913043478, 0.4192059095106187, 0.5966666666666667, 0.5505555555555557, 0.09594843462246777, 1.0, 0.8529686564301758, 0.0730753839661899, 0.0, 1.0, 0.29825106931673706], 
reward next is 0.7017, 
noisyNet noise sample is [array([-0.66094553], dtype=float32), 0.6387715]. 
=============================================
[2019-04-01 20:21:01,051] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[76.653564]
 [76.40551 ]
 [76.23895 ]
 [76.05008 ]
 [75.86937 ]], R is [[76.45139313]
 [76.36156464]
 [76.25338745]
 [76.17620087]
 [76.13288879]].
[2019-04-01 20:21:02,398] A3C_AGENT_WORKER-Thread-14 INFO:Local step 298500, global step 4779785: loss 0.1123
[2019-04-01 20:21:02,399] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 298500, global step 4779785: learning rate 0.0010
[2019-04-01 20:21:02,785] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:21:02,786] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2457
[2019-04-01 20:21:02,834] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 56.0, 22.5, 12.83333333333333, 26.0, 24.83703171328983, 6.952023266492549, 0.0, 1.0, 45.91869095458966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 664800.0000, 
sim time next is 665400.0000, 
raw observation next is [-1.1, 56.5, 18.0, 10.66666666666667, 26.0, 24.87095996116911, 7.018981665795337, 0.0, 1.0, 44.69766198136703], 
processed observation next is [0.0, 0.6956521739130435, 0.4321329639889197, 0.565, 0.06, 0.011786372007366486, 1.0, 0.8387085658813014, 0.07018981665795337, 0.0, 1.0, 0.3192690141526216], 
reward next is 0.6807, 
noisyNet noise sample is [array([-0.66265297], dtype=float32), -1.2014328]. 
=============================================
[2019-04-01 20:21:02,952] A3C_AGENT_WORKER-Thread-11 INFO:Local step 298500, global step 4779999: loss 0.0771
[2019-04-01 20:21:02,953] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 298500, global step 4779999: learning rate 0.0010
[2019-04-01 20:21:03,316] A3C_AGENT_WORKER-Thread-3 INFO:Local step 298500, global step 4780158: loss 0.0589
[2019-04-01 20:21:03,317] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 298500, global step 4780158: learning rate 0.0010
[2019-04-01 20:21:03,618] A3C_AGENT_WORKER-Thread-5 INFO:Local step 298500, global step 4780311: loss 0.0893
[2019-04-01 20:21:03,619] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 298500, global step 4780311: learning rate 0.0010
[2019-04-01 20:21:04,104] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0722318e-26
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:21:04,104] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7727
[2019-04-01 20:21:04,118] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.2, 67.66666666666667, 0.0, 0.0, 26.0, 25.00230945438664, 6.582564730124129, 0.0, 1.0, 41.9130377336039], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 679200.0000, 
sim time next is 679800.0000, 
raw observation next is [-3.3, 68.33333333333333, 0.0, 0.0, 26.0, 24.9645280212142, 6.523136038700787, 0.0, 1.0, 41.83404734026679], 
processed observation next is [0.0, 0.8695652173913043, 0.37119113573407203, 0.6833333333333332, 0.0, 0.0, 1.0, 0.8520754316020286, 0.06523136038700787, 0.0, 1.0, 0.2988146238590485], 
reward next is 0.7012, 
noisyNet noise sample is [array([1.7576846], dtype=float32), 1.6574063]. 
=============================================
[2019-04-01 20:21:04,181] A3C_AGENT_WORKER-Thread-13 INFO:Local step 298500, global step 4780611: loss 0.1955
[2019-04-01 20:21:04,189] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 298500, global step 4780611: learning rate 0.0010
[2019-04-01 20:21:04,359] A3C_AGENT_WORKER-Thread-12 INFO:Local step 298500, global step 4780727: loss 0.1149
[2019-04-01 20:21:04,360] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 298500, global step 4780727: learning rate 0.0010
[2019-04-01 20:21:04,450] A3C_AGENT_WORKER-Thread-19 INFO:Local step 298500, global step 4780787: loss 0.0632
[2019-04-01 20:21:04,452] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 298500, global step 4780788: learning rate 0.0010
[2019-04-01 20:21:07,452] A3C_AGENT_WORKER-Thread-18 INFO:Local step 299500, global step 4782163: loss 0.0157
[2019-04-01 20:21:07,454] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 299500, global step 4782163: learning rate 0.0010
[2019-04-01 20:21:08,303] A3C_AGENT_WORKER-Thread-4 INFO:Local step 299500, global step 4782492: loss 0.0325
[2019-04-01 20:21:08,304] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 299500, global step 4782492: learning rate 0.0010
[2019-04-01 20:21:10,498] A3C_AGENT_WORKER-Thread-2 INFO:Local step 299000, global step 4783456: loss 3.8059
[2019-04-01 20:21:10,502] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 299000, global step 4783457: learning rate 0.0010
[2019-04-01 20:21:14,364] A3C_AGENT_WORKER-Thread-20 INFO:Local step 299000, global step 4784971: loss 19.3641
[2019-04-01 20:21:14,367] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 299000, global step 4784971: learning rate 0.0010
[2019-04-01 20:21:14,756] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:21:14,758] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5947
[2019-04-01 20:21:14,854] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.899999999999999, 84.66666666666666, 24.16666666666667, 0.0, 26.0, 24.70874311166364, 7.761007052002131, 1.0, 1.0, 80.68532655752514], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 837600.0000, 
sim time next is 838200.0000, 
raw observation next is [-3.9, 85.33333333333334, 19.33333333333334, 0.0, 26.0, 25.26080397564569, 9.409897496901534, 1.0, 1.0, 44.35597325551915], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.8533333333333334, 0.06444444444444447, 0.0, 1.0, 0.8944005679493843, 0.09409897496901534, 1.0, 1.0, 0.31682838039656536], 
reward next is 0.6832, 
noisyNet noise sample is [array([0.9136769], dtype=float32), -0.6466459]. 
=============================================
[2019-04-01 20:21:16,061] A3C_AGENT_WORKER-Thread-17 INFO:Local step 300500, global step 4785746: loss 0.6094
[2019-04-01 20:21:16,061] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 300500, global step 4785746: learning rate 0.0010
[2019-04-01 20:21:16,073] A3C_AGENT_WORKER-Thread-10 INFO:Local step 299000, global step 4785752: loss 20.8486
[2019-04-01 20:21:16,075] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 299000, global step 4785752: learning rate 0.0010
[2019-04-01 20:21:16,194] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:21:16,195] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8781
[2019-04-01 20:21:16,215] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.683333333333334, 87.83333333333334, 0.0, 0.0, 26.0, 25.35514518045317, 9.3049589516723, 0.0, 1.0, 37.41547564986951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 954600.0000, 
sim time next is 955200.0000, 
raw observation next is [5.866666666666667, 86.66666666666667, 0.0, 0.0, 26.0, 25.3679143644876, 9.384559547737597, 0.0, 1.0, 37.80521653143527], 
processed observation next is [1.0, 0.043478260869565216, 0.6251154201292707, 0.8666666666666667, 0.0, 0.0, 1.0, 0.9097020520696572, 0.09384559547737598, 0.0, 1.0, 0.27003726093882335], 
reward next is 0.7300, 
noisyNet noise sample is [array([-0.33618736], dtype=float32), -0.8305127]. 
=============================================
[2019-04-01 20:21:17,593] A3C_AGENT_WORKER-Thread-16 INFO:Local step 299000, global step 4786621: loss 20.8065
[2019-04-01 20:21:17,594] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 299000, global step 4786621: learning rate 0.0010
[2019-04-01 20:21:18,226] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:21:18,227] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6709
[2019-04-01 20:21:18,296] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 91.0, 12.33333333333333, 7.666666666666665, 26.0, 25.19683820667079, 6.20276740495206, 1.0, 1.0, 50.96585837208062], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1929000.0000, 
sim time next is 1929600.0000, 
raw observation next is [-9.5, 91.0, 17.5, 11.0, 26.0, 25.18910169519932, 6.165026917326801, 1.0, 1.0, 48.06756697782464], 
processed observation next is [1.0, 0.34782608695652173, 0.1994459833795014, 0.91, 0.058333333333333334, 0.012154696132596685, 1.0, 0.8841573850284742, 0.06165026917326801, 1.0, 1.0, 0.34333976412731887], 
reward next is 0.6567, 
noisyNet noise sample is [array([1.7979486], dtype=float32), -1.6913586]. 
=============================================
[2019-04-01 20:21:18,579] A3C_AGENT_WORKER-Thread-15 INFO:Local step 299000, global step 4787178: loss 20.2938
[2019-04-01 20:21:18,580] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 299000, global step 4787178: learning rate 0.0010
[2019-04-01 20:21:19,776] A3C_AGENT_WORKER-Thread-9 INFO:Local step 299000, global step 4787836: loss 34.1683
[2019-04-01 20:21:19,776] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 299000, global step 4787836: learning rate 0.0010
[2019-04-01 20:21:20,490] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:21:20,492] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4083
[2019-04-01 20:21:20,505] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 97.33333333333334, 0.0, 0.0, 26.0, 25.13553654076933, 8.851011640119077, 0.0, 1.0, 39.73688974812577], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 942000.0000, 
sim time next is 942600.0000, 
raw observation next is [5.0, 96.66666666666666, 0.0, 0.0, 26.0, 25.17049548099824, 8.916439927571899, 0.0, 1.0, 39.52495260100487], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.9666666666666666, 0.0, 0.0, 1.0, 0.8814993544283202, 0.089164399275719, 0.0, 1.0, 0.28232109000717764], 
reward next is 0.7177, 
noisyNet noise sample is [array([1.1725163], dtype=float32), -0.012195435]. 
=============================================
[2019-04-01 20:21:21,078] A3C_AGENT_WORKER-Thread-14 INFO:Local step 299000, global step 4788674: loss 52.4288
[2019-04-01 20:21:21,080] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 299000, global step 4788674: learning rate 0.0010
[2019-04-01 20:21:21,246] A3C_AGENT_WORKER-Thread-2 INFO:Local step 299500, global step 4788783: loss 0.0145
[2019-04-01 20:21:21,247] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 299500, global step 4788784: learning rate 0.0010
[2019-04-01 20:21:21,504] A3C_AGENT_WORKER-Thread-11 INFO:Local step 299000, global step 4788965: loss 52.9740
[2019-04-01 20:21:21,506] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 299000, global step 4788966: learning rate 0.0010
[2019-04-01 20:21:22,210] A3C_AGENT_WORKER-Thread-18 INFO:Local step 300000, global step 4789411: loss 33.8999
[2019-04-01 20:21:22,214] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 300000, global step 4789414: learning rate 0.0010
[2019-04-01 20:21:22,245] A3C_AGENT_WORKER-Thread-3 INFO:Local step 299000, global step 4789435: loss 53.6653
[2019-04-01 20:21:22,246] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 299000, global step 4789436: learning rate 0.0010
[2019-04-01 20:21:22,513] A3C_AGENT_WORKER-Thread-13 INFO:Local step 299000, global step 4789592: loss 50.3150
[2019-04-01 20:21:22,514] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 299000, global step 4789592: learning rate 0.0010
[2019-04-01 20:21:22,637] A3C_AGENT_WORKER-Thread-5 INFO:Local step 299000, global step 4789669: loss 47.2785
[2019-04-01 20:21:22,643] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 299000, global step 4789672: learning rate 0.0010
[2019-04-01 20:21:22,976] A3C_AGENT_WORKER-Thread-12 INFO:Local step 299000, global step 4789882: loss 86.7690
[2019-04-01 20:21:22,980] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 299000, global step 4789884: learning rate 0.0010
[2019-04-01 20:21:23,029] A3C_AGENT_WORKER-Thread-4 INFO:Local step 300000, global step 4789914: loss 64.3112
[2019-04-01 20:21:23,032] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 300000, global step 4789914: learning rate 0.0010
[2019-04-01 20:21:23,484] A3C_AGENT_WORKER-Thread-19 INFO:Local step 299000, global step 4790204: loss 91.7793
[2019-04-01 20:21:23,485] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 299000, global step 4790204: learning rate 0.0010
[2019-04-01 20:21:24,515] A3C_AGENT_WORKER-Thread-20 INFO:Local step 299500, global step 4790880: loss 0.2186
[2019-04-01 20:21:24,519] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 299500, global step 4790881: learning rate 0.0010
[2019-04-01 20:21:25,148] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:21:25,148] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8579
[2019-04-01 20:21:25,157] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.1, 79.0, 0.0, 0.0, 26.0, 24.10671608416803, 7.029683863151294, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1211400.0000, 
sim time next is 1212000.0000, 
raw observation next is [16.1, 79.33333333333333, 0.0, 0.0, 26.0, 24.11596068721375, 7.045100956891953, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7933333333333333, 0.0, 0.0, 1.0, 0.7308515267448215, 0.07045100956891953, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6703304], dtype=float32), 1.0974147]. 
=============================================
[2019-04-01 20:21:25,164] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[ 99.700775]
 [100.794815]
 [101.97783 ]
 [103.267555]
 [104.735115]], R is [[98.73429871]
 [98.74695587]
 [98.75949097]
 [98.77189636]
 [98.78417969]].
[2019-04-01 20:21:26,256] A3C_AGENT_WORKER-Thread-10 INFO:Local step 299500, global step 4792030: loss 0.1564
[2019-04-01 20:21:26,257] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 299500, global step 4792030: learning rate 0.0010
[2019-04-01 20:21:28,166] A3C_AGENT_WORKER-Thread-16 INFO:Local step 299500, global step 4793242: loss 0.2129
[2019-04-01 20:21:28,169] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 299500, global step 4793243: learning rate 0.0010
[2019-04-01 20:21:29,465] A3C_AGENT_WORKER-Thread-15 INFO:Local step 299500, global step 4794068: loss 0.1289
[2019-04-01 20:21:29,467] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 299500, global step 4794069: learning rate 0.0010
[2019-04-01 20:21:30,946] A3C_AGENT_WORKER-Thread-9 INFO:Local step 299500, global step 4795054: loss 0.0492
[2019-04-01 20:21:30,947] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 299500, global step 4795054: learning rate 0.0010
[2019-04-01 20:21:32,046] A3C_AGENT_WORKER-Thread-14 INFO:Local step 299500, global step 4795674: loss 0.1295
[2019-04-01 20:21:32,054] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 299500, global step 4795675: learning rate 0.0010
[2019-04-01 20:21:32,682] A3C_AGENT_WORKER-Thread-11 INFO:Local step 299500, global step 4795991: loss 0.1170
[2019-04-01 20:21:32,684] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 299500, global step 4795992: learning rate 0.0010
[2019-04-01 20:21:33,375] A3C_AGENT_WORKER-Thread-3 INFO:Local step 299500, global step 4796281: loss 0.0946
[2019-04-01 20:21:33,379] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 299500, global step 4796281: learning rate 0.0010
[2019-04-01 20:21:33,465] A3C_AGENT_WORKER-Thread-13 INFO:Local step 299500, global step 4796328: loss 0.0253
[2019-04-01 20:21:33,467] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 299500, global step 4796328: learning rate 0.0010
[2019-04-01 20:21:33,775] A3C_AGENT_WORKER-Thread-5 INFO:Local step 299500, global step 4796488: loss 0.0371
[2019-04-01 20:21:33,777] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 299500, global step 4796491: learning rate 0.0010
[2019-04-01 20:21:34,024] A3C_AGENT_WORKER-Thread-12 INFO:Local step 299500, global step 4796627: loss 0.0291
[2019-04-01 20:21:34,027] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 299500, global step 4796627: learning rate 0.0010
[2019-04-01 20:21:34,273] A3C_AGENT_WORKER-Thread-19 INFO:Local step 299500, global step 4796757: loss 0.0577
[2019-04-01 20:21:34,273] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 299500, global step 4796757: learning rate 0.0010
[2019-04-01 20:21:36,285] A3C_AGENT_WORKER-Thread-17 INFO:Local step 301000, global step 4797828: loss 0.0460
[2019-04-01 20:21:36,288] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 301000, global step 4797832: learning rate 0.0010
[2019-04-01 20:21:36,475] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:21:36,475] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5941
[2019-04-01 20:21:36,480] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 106.0, 0.0, 26.0, 25.69940992386467, 11.11875269755706, 1.0, 1.0, 15.73053567715892], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1344600.0000, 
sim time next is 1345200.0000, 
raw observation next is [1.1, 92.0, 100.1666666666667, 0.0, 26.0, 25.67955275668443, 11.25060457447849, 1.0, 1.0, 15.03243164761957], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.333888888888889, 0.0, 1.0, 0.95422182238349, 0.1125060457447849, 1.0, 1.0, 0.10737451176871121], 
reward next is 0.3924, 
noisyNet noise sample is [array([0.91206443], dtype=float32), 1.0259717]. 
=============================================
[2019-04-01 20:21:36,680] A3C_AGENT_WORKER-Thread-2 INFO:Local step 300000, global step 4798038: loss 84.7619
[2019-04-01 20:21:36,683] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 300000, global step 4798038: learning rate 0.0010
[2019-04-01 20:21:39,971] A3C_AGENT_WORKER-Thread-20 INFO:Local step 300000, global step 4799623: loss 79.5445
[2019-04-01 20:21:39,975] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 300000, global step 4799626: learning rate 0.0010
[2019-04-01 20:21:40,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00457174e-32
 0.00000000e+00 1.00000000e+00 0.00000000e+00], sum to 1.0000
[2019-04-01 20:21:40,425] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8551
[2019-04-01 20:21:40,433] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.983333333333333, 66.83333333333334, 171.6666666666667, 104.0, 26.0, 26.48871612277446, 14.46198530275793, 1.0, 1.0, 8.531788803663304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1591800.0000, 
sim time next is 1592400.0000, 
raw observation next is [8.266666666666667, 65.66666666666667, 185.8333333333333, 96.0, 26.0, 26.54873242670222, 14.89911237798332, 1.0, 1.0, 7.731278613059623], 
processed observation next is [1.0, 0.43478260869565216, 0.6915974145891045, 0.6566666666666667, 0.6194444444444442, 0.10607734806629834, 1.0, 1.0783903466717457, 0.1489911237798332, 1.0, 1.0, 0.055223418664711595], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.9680473], dtype=float32), 2.1081872]. 
=============================================
[2019-04-01 20:21:40,583] A3C_AGENT_WORKER-Thread-18 INFO:Local step 300500, global step 4799931: loss 0.3585
[2019-04-01 20:21:40,584] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 300500, global step 4799931: learning rate 0.0010
[2019-04-01 20:21:40,730] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-01 20:21:40,738] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 20:21:40,739] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 20:21:40,739] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:21:40,739] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 20:21:40,739] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:21:40,741] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:21:40,746] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run49
[2019-04-01 20:21:40,771] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run49
[2019-04-01 20:21:40,811] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run49
[2019-04-01 20:22:15,489] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.97244143], dtype=float32), -1.5399886]
[2019-04-01 20:22:15,489] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-0.8333333333333334, 62.33333333333334, 85.83333333333334, 393.0, 26.0, 25.48135492620457, 8.943406976567651, 0.0, 1.0, 26.58725761476649]
[2019-04-01 20:22:15,489] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 20:22:15,490] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.6071846410024528
[2019-04-01 20:23:23,408] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.97244143], dtype=float32), -1.5399886]
[2019-04-01 20:23:23,408] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [5.166666666666667, 84.66666666666667, 0.0, 0.0, 26.0, 25.94630488061548, 12.8069806207379, 0.0, 1.0, 20.87420138982569]
[2019-04-01 20:23:23,408] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-01 20:23:23,409] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.896988220833598
[2019-04-01 20:23:24,638] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 20:23:40,809] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.97244143], dtype=float32), -1.5399886]
[2019-04-01 20:23:40,810] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [8.0, 26.0, 100.5, 796.5, 26.0, 27.69790726225424, 21.74270428309675, 1.0, 1.0, 1.290183963843844]
[2019-04-01 20:23:40,810] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-01 20:23:40,811] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.6978200447524509
[2019-04-01 20:23:42,586] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 20:23:46,003] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 20:23:47,028] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 4800000, evaluation results [4800000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 20:23:47,706] A3C_AGENT_WORKER-Thread-10 INFO:Local step 300000, global step 4800318: loss 42.7348
[2019-04-01 20:23:47,707] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 300000, global step 4800319: learning rate 0.0010
[2019-04-01 20:23:47,854] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:23:47,854] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0899
[2019-04-01 20:23:47,874] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 41.33333333333334, 0.0, 26.0, 26.09702865192357, 11.58943629838192, 1.0, 1.0, 16.68432994960759], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1438800.0000, 
sim time next is 1439400.0000, 
raw observation next is [1.1, 92.0, 36.66666666666667, 0.0, 26.0, 26.11576866543101, 11.60139805145044, 1.0, 1.0, 15.73769550800279], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.12222222222222223, 0.0, 1.0, 1.0165383807758583, 0.1160139805145044, 1.0, 1.0, 0.1124121107714485], 
reward next is 0.2470, 
noisyNet noise sample is [array([0.04282806], dtype=float32), -0.80001926]. 
=============================================
[2019-04-01 20:23:47,998] A3C_AGENT_WORKER-Thread-4 INFO:Local step 300500, global step 4800470: loss 0.2933
[2019-04-01 20:23:47,999] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 300500, global step 4800470: learning rate 0.0010
[2019-04-01 20:23:49,534] A3C_AGENT_WORKER-Thread-16 INFO:Local step 300000, global step 4801310: loss 52.1579
[2019-04-01 20:23:49,534] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 300000, global step 4801310: learning rate 0.0010
[2019-04-01 20:23:50,751] A3C_AGENT_WORKER-Thread-15 INFO:Local step 300000, global step 4801988: loss 43.6055
[2019-04-01 20:23:50,752] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 300000, global step 4801988: learning rate 0.0010
[2019-04-01 20:23:51,507] A3C_AGENT_WORKER-Thread-9 INFO:Local step 300000, global step 4802382: loss 46.7457
[2019-04-01 20:23:51,508] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 300000, global step 4802383: learning rate 0.0010
[2019-04-01 20:23:52,434] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:23:52,437] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4383
[2019-04-01 20:23:52,460] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.35, 84.5, 53.0, 0.0, 26.0, 25.52132336527103, 11.39867250228164, 1.0, 1.0, 19.78136107701812], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1697400.0000, 
sim time next is 1698000.0000, 
raw observation next is [1.433333333333333, 83.33333333333334, 49.16666666666667, 0.0, 26.0, 25.76749131835683, 11.86387981114954, 1.0, 1.0, 17.52415086049668], 
processed observation next is [1.0, 0.6521739130434783, 0.502308402585411, 0.8333333333333335, 0.16388888888888892, 0.0, 1.0, 0.9667844740509756, 0.1186387981114954, 1.0, 1.0, 0.12517250614640485], 
reward next is 0.1293, 
noisyNet noise sample is [array([-0.64273936], dtype=float32), 1.2655398]. 
=============================================
[2019-04-01 20:23:52,487] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[46.945538]
 [46.640823]
 [46.5036  ]
 [46.274048]
 [46.078747]], R is [[46.99421692]
 [46.82351303]
 [47.0716362 ]
 [46.60092163]
 [46.1349144 ]].
[2019-04-01 20:23:52,619] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:23:52,625] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8732
[2019-04-01 20:23:52,672] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.683333333333334, 73.0, 0.0, 0.0, 26.0, 25.09269296319546, 7.594341940634616, 1.0, 1.0, 30.9674118754692], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1969800.0000, 
sim time next is 1970400.0000, 
raw observation next is [-4.866666666666667, 75.0, 0.0, 0.0, 26.0, 25.02701731289549, 7.520955563090169, 1.0, 1.0, 39.32905673502485], 
processed observation next is [1.0, 0.8260869565217391, 0.3277931671283472, 0.75, 0.0, 0.0, 1.0, 0.8610024732707845, 0.07520955563090169, 1.0, 1.0, 0.28092183382160607], 
reward next is 0.7191, 
noisyNet noise sample is [array([0.72885513], dtype=float32), 0.887609]. 
=============================================
[2019-04-01 20:23:53,105] A3C_AGENT_WORKER-Thread-14 INFO:Local step 300000, global step 4803293: loss 56.2625
[2019-04-01 20:23:53,107] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 300000, global step 4803294: learning rate 0.0010
[2019-04-01 20:23:53,457] A3C_AGENT_WORKER-Thread-11 INFO:Local step 300000, global step 4803482: loss 56.3805
[2019-04-01 20:23:53,458] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 300000, global step 4803482: learning rate 0.0010
[2019-04-01 20:23:54,514] A3C_AGENT_WORKER-Thread-3 INFO:Local step 300000, global step 4804102: loss 40.4459
[2019-04-01 20:23:54,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 300000, global step 4804102: learning rate 0.0010
[2019-04-01 20:23:54,531] A3C_AGENT_WORKER-Thread-13 INFO:Local step 300000, global step 4804116: loss 30.4863
[2019-04-01 20:23:54,532] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 300000, global step 4804116: learning rate 0.0010
[2019-04-01 20:23:54,711] A3C_AGENT_WORKER-Thread-5 INFO:Local step 300000, global step 4804217: loss 46.9571
[2019-04-01 20:23:54,712] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 300000, global step 4804217: learning rate 0.0010
[2019-04-01 20:23:55,005] A3C_AGENT_WORKER-Thread-12 INFO:Local step 300000, global step 4804363: loss 44.3131
[2019-04-01 20:23:55,032] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 300000, global step 4804379: learning rate 0.0010
[2019-04-01 20:23:55,582] A3C_AGENT_WORKER-Thread-19 INFO:Local step 300000, global step 4804692: loss 37.7465
[2019-04-01 20:23:55,582] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 300000, global step 4804692: learning rate 0.0010
[2019-04-01 20:23:57,975] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:23:57,975] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5415
[2019-04-01 20:23:57,999] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.116666666666667, 92.0, 20.33333333333334, 0.0, 26.0, 25.49682071437257, 10.12841629650962, 1.0, 1.0, 18.00737032220479], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1671000.0000, 
sim time next is 1671600.0000, 
raw observation next is [2.933333333333334, 92.0, 25.16666666666667, 0.0, 26.0, 25.41514766080889, 10.44053812220487, 1.0, 1.0, 18.68523997970831], 
processed observation next is [1.0, 0.34782608695652173, 0.543859649122807, 0.92, 0.0838888888888889, 0.0, 1.0, 0.9164496658298417, 0.1044053812220487, 1.0, 1.0, 0.13346599985505936], 
reward next is 0.6903, 
noisyNet noise sample is [array([1.9407494], dtype=float32), 0.604545]. 
=============================================
[2019-04-01 20:24:00,992] A3C_AGENT_WORKER-Thread-2 INFO:Local step 300500, global step 4807127: loss 0.0509
[2019-04-01 20:24:00,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 300500, global step 4807127: learning rate 0.0010
[2019-04-01 20:24:03,232] A3C_AGENT_WORKER-Thread-17 INFO:Local step 301500, global step 4808033: loss 2.4700
[2019-04-01 20:24:03,232] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 301500, global step 4808033: learning rate 0.0010
[2019-04-01 20:24:04,215] A3C_AGENT_WORKER-Thread-20 INFO:Local step 300500, global step 4808329: loss 0.2444
[2019-04-01 20:24:04,216] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 300500, global step 4808329: learning rate 0.0010
[2019-04-01 20:24:04,549] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:24:04,550] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8833
[2019-04-01 20:24:04,594] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.866666666666667, 81.33333333333333, 0.0, 0.0, 26.0, 24.20203403016455, 5.586978730468687, 0.0, 1.0, 45.63173642276814], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1820400.0000, 
sim time next is 1821000.0000, 
raw observation next is [-5.933333333333333, 82.16666666666667, 0.0, 0.0, 26.0, 24.16756821507332, 5.546757239364588, 0.0, 1.0, 45.70779340075531], 
processed observation next is [0.0, 0.043478260869565216, 0.29824561403508776, 0.8216666666666668, 0.0, 0.0, 1.0, 0.7382240307247601, 0.05546757239364588, 0.0, 1.0, 0.3264842385768236], 
reward next is 0.6735, 
noisyNet noise sample is [array([-2.66357], dtype=float32), -0.042769186]. 
=============================================
[2019-04-01 20:24:04,599] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[65.83043]
 [65.76888]
 [65.66126]
 [65.50356]
 [65.29029]], R is [[65.84221649]
 [65.85784912]
 [65.87380219]
 [65.89003754]
 [65.90665436]].
[2019-04-01 20:24:05,849] A3C_AGENT_WORKER-Thread-10 INFO:Local step 300500, global step 4808874: loss 0.2051
[2019-04-01 20:24:05,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 300500, global step 4808874: learning rate 0.0010
[2019-04-01 20:24:07,368] A3C_AGENT_WORKER-Thread-18 INFO:Local step 301000, global step 4809470: loss 0.0214
[2019-04-01 20:24:07,370] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 301000, global step 4809470: learning rate 0.0010
[2019-04-01 20:24:07,674] A3C_AGENT_WORKER-Thread-16 INFO:Local step 300500, global step 4809614: loss 0.3318
[2019-04-01 20:24:07,675] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 300500, global step 4809614: learning rate 0.0010
[2019-04-01 20:24:08,347] A3C_AGENT_WORKER-Thread-4 INFO:Local step 301000, global step 4809905: loss 0.0012
[2019-04-01 20:24:08,348] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 301000, global step 4809905: learning rate 0.0010
[2019-04-01 20:24:09,401] A3C_AGENT_WORKER-Thread-15 INFO:Local step 300500, global step 4810320: loss 0.1526
[2019-04-01 20:24:09,403] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 300500, global step 4810320: learning rate 0.0010
[2019-04-01 20:24:09,721] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:24:09,721] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8720
[2019-04-01 20:24:09,749] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.616666666666667, 78.16666666666666, 0.0, 0.0, 26.0, 23.37937606314491, 5.339191038770586, 0.0, 1.0, 46.45347999366135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1839000.0000, 
sim time next is 1839600.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 23.34591030508214, 5.36210152991881, 0.0, 1.0, 46.49046998405801], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 1.0, 0.6208443292974485, 0.0536210152991881, 0.0, 1.0, 0.33207478560041437], 
reward next is 0.6679, 
noisyNet noise sample is [array([0.17609467], dtype=float32), -0.97912735]. 
=============================================
[2019-04-01 20:24:10,197] A3C_AGENT_WORKER-Thread-9 INFO:Local step 300500, global step 4810544: loss 0.1582
[2019-04-01 20:24:10,229] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 300500, global step 4810547: learning rate 0.0010
[2019-04-01 20:24:11,825] A3C_AGENT_WORKER-Thread-14 INFO:Local step 300500, global step 4811030: loss 0.2575
[2019-04-01 20:24:11,828] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 300500, global step 4811030: learning rate 0.0010
[2019-04-01 20:24:12,324] A3C_AGENT_WORKER-Thread-11 INFO:Local step 300500, global step 4811193: loss 0.0407
[2019-04-01 20:24:12,343] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 300500, global step 4811193: learning rate 0.0010
[2019-04-01 20:24:13,212] A3C_AGENT_WORKER-Thread-13 INFO:Local step 300500, global step 4811475: loss 0.0349
[2019-04-01 20:24:13,215] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 300500, global step 4811477: learning rate 0.0010
[2019-04-01 20:24:13,222] A3C_AGENT_WORKER-Thread-3 INFO:Local step 300500, global step 4811483: loss 0.0291
[2019-04-01 20:24:13,224] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 300500, global step 4811483: learning rate 0.0010
[2019-04-01 20:24:13,511] A3C_AGENT_WORKER-Thread-5 INFO:Local step 300500, global step 4811590: loss 0.0470
[2019-04-01 20:24:13,512] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 300500, global step 4811590: learning rate 0.0010
[2019-04-01 20:24:13,800] A3C_AGENT_WORKER-Thread-12 INFO:Local step 300500, global step 4811723: loss 0.0292
[2019-04-01 20:24:13,801] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 300500, global step 4811723: learning rate 0.0010
[2019-04-01 20:24:14,481] A3C_AGENT_WORKER-Thread-19 INFO:Local step 300500, global step 4812017: loss 0.0282
[2019-04-01 20:24:14,482] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 300500, global step 4812017: learning rate 0.0010
[2019-04-01 20:24:17,861] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:24:17,862] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9466
[2019-04-01 20:24:17,878] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 62.0, 86.66666666666666, 0.0, 26.0, 25.67553466976876, 7.659462370370963, 1.0, 1.0, 14.52285129185249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1955400.0000, 
sim time next is 1956000.0000, 
raw observation next is [-2.8, 62.0, 80.33333333333334, 0.0, 26.0, 25.63655130933576, 7.660739207829852, 1.0, 1.0, 23.9810762966528], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.26777777777777784, 0.0, 1.0, 0.9480787584765373, 0.07660739207829853, 1.0, 1.0, 0.17129340211894858], 
reward next is 0.8287, 
noisyNet noise sample is [array([0.48297033], dtype=float32), -1.2269282]. 
=============================================
[2019-04-01 20:24:17,884] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[70.72527 ]
 [70.66645 ]
 [70.480286]
 [70.251686]
 [70.00213 ]], R is [[70.74575806]
 [70.93456268]
 [71.11587524]
 [71.28966522]
 [71.45588684]].
[2019-04-01 20:24:21,879] A3C_AGENT_WORKER-Thread-17 INFO:Local step 302000, global step 4814737: loss 0.0641
[2019-04-01 20:24:21,884] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 302000, global step 4814739: learning rate 0.0010
[2019-04-01 20:24:22,444] A3C_AGENT_WORKER-Thread-2 INFO:Local step 301000, global step 4814914: loss 0.0287
[2019-04-01 20:24:22,445] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 301000, global step 4814914: learning rate 0.0010
[2019-04-01 20:24:26,064] A3C_AGENT_WORKER-Thread-20 INFO:Local step 301000, global step 4816075: loss 0.0320
[2019-04-01 20:24:26,066] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 301000, global step 4816076: learning rate 0.0010
[2019-04-01 20:24:27,554] A3C_AGENT_WORKER-Thread-10 INFO:Local step 301000, global step 4816630: loss 0.0439
[2019-04-01 20:24:27,555] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 301000, global step 4816630: learning rate 0.0010
[2019-04-01 20:24:29,135] A3C_AGENT_WORKER-Thread-16 INFO:Local step 301000, global step 4817280: loss 0.0002
[2019-04-01 20:24:29,138] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 301000, global step 4817280: learning rate 0.0010
[2019-04-01 20:24:29,243] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 5.6352671e-02 5.3675636e-22
 9.4364738e-01 3.9896056e-17], sum to 1.0000
[2019-04-01 20:24:29,247] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2530
[2019-04-01 20:24:29,296] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 68.0, 116.3333333333333, 190.0, 26.0, 26.40951567646947, 11.47097308181019, 1.0, 1.0, 24.0860685254371], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2217000.0000, 
sim time next is 2217600.0000, 
raw observation next is [-3.9, 68.0, 96.0, 142.5, 26.0, 26.41428657146018, 11.20914221422395, 1.0, 1.0, 24.59019325157889], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.68, 0.32, 0.1574585635359116, 1.0, 1.059183795922883, 0.11209142214223951, 1.0, 1.0, 0.17564423751127778], 
reward next is 0.3407, 
noisyNet noise sample is [array([0.21901359], dtype=float32), -0.5934893]. 
=============================================
[2019-04-01 20:24:29,401] A3C_AGENT_WORKER-Thread-18 INFO:Local step 301500, global step 4817362: loss 0.8098
[2019-04-01 20:24:29,406] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 301500, global step 4817364: learning rate 0.0010
[2019-04-01 20:24:30,362] A3C_AGENT_WORKER-Thread-4 INFO:Local step 301500, global step 4817678: loss 2.2485
[2019-04-01 20:24:30,377] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 301500, global step 4817679: learning rate 0.0010
[2019-04-01 20:24:31,061] A3C_AGENT_WORKER-Thread-15 INFO:Local step 301000, global step 4817948: loss 0.0077
[2019-04-01 20:24:31,062] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 301000, global step 4817948: learning rate 0.0010
[2019-04-01 20:24:31,740] A3C_AGENT_WORKER-Thread-9 INFO:Local step 301000, global step 4818196: loss 0.0017
[2019-04-01 20:24:31,750] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 301000, global step 4818198: learning rate 0.0010
[2019-04-01 20:24:32,967] A3C_AGENT_WORKER-Thread-14 INFO:Local step 301000, global step 4818657: loss 0.0057
[2019-04-01 20:24:32,970] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 301000, global step 4818657: learning rate 0.0010
[2019-04-01 20:24:33,490] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:24:33,491] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0874
[2019-04-01 20:24:33,516] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.616666666666667, 77.5, 0.0, 0.0, 26.0, 24.12487009311793, 5.421348037096974, 0.0, 1.0, 41.56048668779941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2175000.0000, 
sim time next is 2175600.0000, 
raw observation next is [-6.533333333333334, 77.0, 0.0, 0.0, 26.0, 24.1008763704433, 5.400059058420688, 0.0, 1.0, 41.53964378860406], 
processed observation next is [1.0, 0.17391304347826086, 0.28162511542012925, 0.77, 0.0, 0.0, 1.0, 0.7286966243490427, 0.05400059058420688, 0.0, 1.0, 0.2967117413471719], 
reward next is 0.7033, 
noisyNet noise sample is [array([0.4044871], dtype=float32), -0.87365127]. 
=============================================
[2019-04-01 20:24:33,873] A3C_AGENT_WORKER-Thread-11 INFO:Local step 301000, global step 4819014: loss 0.0010
[2019-04-01 20:24:33,873] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 301000, global step 4819014: learning rate 0.0010
[2019-04-01 20:24:34,776] A3C_AGENT_WORKER-Thread-13 INFO:Local step 301000, global step 4819425: loss 0.0016
[2019-04-01 20:24:34,777] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 301000, global step 4819425: learning rate 0.0010
[2019-04-01 20:24:34,855] A3C_AGENT_WORKER-Thread-5 INFO:Local step 301000, global step 4819460: loss 0.0013
[2019-04-01 20:24:34,856] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 301000, global step 4819460: learning rate 0.0010
[2019-04-01 20:24:34,943] A3C_AGENT_WORKER-Thread-3 INFO:Local step 301000, global step 4819496: loss 0.0137
[2019-04-01 20:24:34,948] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 301000, global step 4819498: learning rate 0.0010
[2019-04-01 20:24:35,272] A3C_AGENT_WORKER-Thread-12 INFO:Local step 301000, global step 4819609: loss 0.0039
[2019-04-01 20:24:35,273] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 301000, global step 4819609: learning rate 0.0010
[2019-04-01 20:24:36,319] A3C_AGENT_WORKER-Thread-19 INFO:Local step 301000, global step 4819962: loss 0.0246
[2019-04-01 20:24:36,320] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 301000, global step 4819962: learning rate 0.0010
[2019-04-01 20:24:37,157] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:24:37,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3860
[2019-04-01 20:24:37,218] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.416666666666667, 76.33333333333333, 152.3333333333333, 46.33333333333333, 26.0, 25.66684557079882, 7.900931059081929, 1.0, 1.0, 32.87588462426831], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2283000.0000, 
sim time next is 2283600.0000, 
raw observation next is [-6.133333333333335, 74.66666666666667, 165.1666666666667, 48.16666666666667, 26.0, 25.71323603400182, 8.076338305287797, 1.0, 1.0, 31.91582661540896], 
processed observation next is [1.0, 0.43478260869565216, 0.29270544783010155, 0.7466666666666667, 0.5505555555555557, 0.05322283609576428, 1.0, 0.9590337191431172, 0.08076338305287796, 1.0, 1.0, 0.227970190110064], 
reward next is 0.7720, 
noisyNet noise sample is [array([-0.80300325], dtype=float32), -0.7361386]. 
=============================================
[2019-04-01 20:24:38,589] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:24:38,589] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6113
[2019-04-01 20:24:38,645] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 68.0, 116.3333333333333, 190.0, 26.0, 26.40951567646947, 11.47097308181019, 1.0, 1.0, 24.0860685254371], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2217000.0000, 
sim time next is 2217600.0000, 
raw observation next is [-3.9, 68.0, 96.0, 142.5, 26.0, 26.41428657146018, 11.20914221422395, 1.0, 1.0, 24.59019325157889], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.68, 0.32, 0.1574585635359116, 1.0, 1.059183795922883, 0.11209142214223951, 1.0, 1.0, 0.17564423751127778], 
reward next is 0.3407, 
noisyNet noise sample is [array([0.2196151], dtype=float32), -0.48981088]. 
=============================================
[2019-04-01 20:24:41,390] A3C_AGENT_WORKER-Thread-17 INFO:Local step 302500, global step 4821870: loss 0.2147
[2019-04-01 20:24:41,391] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 302500, global step 4821870: learning rate 0.0010
[2019-04-01 20:24:44,317] A3C_AGENT_WORKER-Thread-2 INFO:Local step 301500, global step 4822977: loss 5.0078
[2019-04-01 20:24:44,320] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 301500, global step 4822979: learning rate 0.0010
[2019-04-01 20:24:47,751] A3C_AGENT_WORKER-Thread-20 INFO:Local step 301500, global step 4824479: loss 1.0804
[2019-04-01 20:24:47,753] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 301500, global step 4824480: learning rate 0.0010
[2019-04-01 20:24:48,066] A3C_AGENT_WORKER-Thread-18 INFO:Local step 302000, global step 4824598: loss 0.0226
[2019-04-01 20:24:48,075] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 302000, global step 4824603: learning rate 0.0010
[2019-04-01 20:24:48,570] A3C_AGENT_WORKER-Thread-4 INFO:Local step 302000, global step 4824807: loss 0.0427
[2019-04-01 20:24:48,571] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 302000, global step 4824807: learning rate 0.0010
[2019-04-01 20:24:49,277] A3C_AGENT_WORKER-Thread-10 INFO:Local step 301500, global step 4825099: loss 2.6058
[2019-04-01 20:24:49,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 301500, global step 4825100: learning rate 0.0010
[2019-04-01 20:24:50,164] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.959639e-37
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 20:24:50,164] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4274
[2019-04-01 20:24:50,181] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 53.0, 0.0, 0.0, 26.0, 24.13044030393151, 5.330006117179093, 0.0, 1.0, 42.96841291195923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2426400.0000, 
sim time next is 2427000.0000, 
raw observation next is [-7.383333333333333, 53.33333333333334, 0.0, 0.0, 26.0, 24.07410034244984, 5.30919220545687, 0.0, 1.0, 42.9945729905459], 
processed observation next is [0.0, 0.08695652173913043, 0.25807940904893817, 0.5333333333333334, 0.0, 0.0, 1.0, 0.7248714774928341, 0.053091922054568697, 0.0, 1.0, 0.30710409278961354], 
reward next is 0.6929, 
noisyNet noise sample is [array([1.2262942], dtype=float32), -2.4440725]. 
=============================================
[2019-04-01 20:24:50,185] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[67.11516]
 [67.18164]
 [67.31748]
 [67.46828]
 [67.64192]], R is [[67.03977966]
 [67.06246948]
 [67.08505249]
 [67.10760498]
 [67.13025665]].
[2019-04-01 20:24:50,239] A3C_AGENT_WORKER-Thread-16 INFO:Local step 301500, global step 4825508: loss 3.4429
[2019-04-01 20:24:50,240] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 301500, global step 4825509: learning rate 0.0010
[2019-04-01 20:24:52,136] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:24:52,137] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2373
[2019-04-01 20:24:52,194] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.0, 84.0, 44.0, 245.0, 26.0, 25.71778334870908, 9.354421709129008, 1.0, 1.0, 44.96288590549558], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3312000.0000, 
sim time next is 3312600.0000, 
raw observation next is [-10.66666666666667, 82.83333333333333, 58.00000000000001, 295.3333333333334, 26.0, 25.69896152529363, 9.14483409899018, 1.0, 1.0, 42.05103664201236], 
processed observation next is [1.0, 0.34782608695652173, 0.16712834718374878, 0.8283333333333333, 0.19333333333333336, 0.3263351749539596, 1.0, 0.9569945036133757, 0.0914483409899018, 1.0, 1.0, 0.30036454744294544], 
reward next is 0.6996, 
noisyNet noise sample is [array([0.11656469], dtype=float32), -1.533988]. 
=============================================
[2019-04-01 20:24:52,534] A3C_AGENT_WORKER-Thread-15 INFO:Local step 301500, global step 4826484: loss 2.8826
[2019-04-01 20:24:52,542] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 301500, global step 4826486: learning rate 0.0010
[2019-04-01 20:24:53,119] A3C_AGENT_WORKER-Thread-9 INFO:Local step 301500, global step 4826764: loss 6.5457
[2019-04-01 20:24:53,121] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 301500, global step 4826764: learning rate 0.0010
[2019-04-01 20:24:53,836] A3C_AGENT_WORKER-Thread-14 INFO:Local step 301500, global step 4827118: loss 2.8871
[2019-04-01 20:24:53,837] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 301500, global step 4827118: learning rate 0.0010
[2019-04-01 20:24:55,246] A3C_AGENT_WORKER-Thread-11 INFO:Local step 301500, global step 4827743: loss 0.8926
[2019-04-01 20:24:55,248] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 301500, global step 4827743: learning rate 0.0010
[2019-04-01 20:24:55,959] A3C_AGENT_WORKER-Thread-17 INFO:Local step 303000, global step 4828079: loss 0.0078
[2019-04-01 20:24:55,960] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 303000, global step 4828079: learning rate 0.0010
[2019-04-01 20:24:56,066] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:24:56,066] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5772
[2019-04-01 20:24:56,117] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.466666666666667, 27.5, 0.0, 0.0, 26.0, 24.75464105530395, 6.645253704965948, 0.0, 1.0, 48.42925324259608], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2483400.0000, 
sim time next is 2484000.0000, 
raw observation next is [1.1, 28.0, 0.0, 0.0, 26.0, 24.80565644172955, 6.855297302159585, 0.0, 1.0, 41.53771857568398], 
processed observation next is [0.0, 0.782608695652174, 0.49307479224376743, 0.28, 0.0, 0.0, 1.0, 0.82937949167565, 0.06855297302159585, 0.0, 1.0, 0.2966979898263142], 
reward next is 0.7033, 
noisyNet noise sample is [array([0.42275608], dtype=float32), 1.4457202]. 
=============================================
[2019-04-01 20:24:56,125] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[68.30376 ]
 [68.927765]
 [69.55729 ]
 [69.86441 ]
 [70.24616 ]], R is [[68.13225555]
 [68.10501099]
 [68.20822144]
 [68.37706757]
 [68.58542633]].
[2019-04-01 20:24:56,126] A3C_AGENT_WORKER-Thread-13 INFO:Local step 301500, global step 4828169: loss 1.8741
[2019-04-01 20:24:56,128] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 301500, global step 4828169: learning rate 0.0010
[2019-04-01 20:24:56,337] A3C_AGENT_WORKER-Thread-3 INFO:Local step 301500, global step 4828211: loss 0.5682
[2019-04-01 20:24:56,338] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 301500, global step 4828212: learning rate 0.0010
[2019-04-01 20:24:56,367] A3C_AGENT_WORKER-Thread-5 INFO:Local step 301500, global step 4828225: loss 1.5365
[2019-04-01 20:24:56,367] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 301500, global step 4828225: learning rate 0.0010
[2019-04-01 20:24:56,710] A3C_AGENT_WORKER-Thread-12 INFO:Local step 301500, global step 4828352: loss 2.8489
[2019-04-01 20:24:56,713] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 301500, global step 4828353: learning rate 0.0010
[2019-04-01 20:24:58,242] A3C_AGENT_WORKER-Thread-19 INFO:Local step 301500, global step 4829115: loss 0.4194
[2019-04-01 20:24:58,245] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 301500, global step 4829115: learning rate 0.0010
[2019-04-01 20:25:02,290] A3C_AGENT_WORKER-Thread-2 INFO:Local step 302000, global step 4830950: loss 0.0348
[2019-04-01 20:25:02,304] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 302000, global step 4830957: learning rate 0.0010
[2019-04-01 20:25:05,744] A3C_AGENT_WORKER-Thread-20 INFO:Local step 302000, global step 4832384: loss 0.0161
[2019-04-01 20:25:05,744] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 302000, global step 4832384: learning rate 0.0010
[2019-04-01 20:25:06,520] A3C_AGENT_WORKER-Thread-18 INFO:Local step 302500, global step 4832684: loss 7.7576
[2019-04-01 20:25:06,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 302500, global step 4832684: learning rate 0.0010
[2019-04-01 20:25:07,289] A3C_AGENT_WORKER-Thread-4 INFO:Local step 302500, global step 4833025: loss 2.7481
[2019-04-01 20:25:07,294] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 302500, global step 4833026: learning rate 0.0010
[2019-04-01 20:25:07,522] A3C_AGENT_WORKER-Thread-10 INFO:Local step 302000, global step 4833142: loss 0.0070
[2019-04-01 20:25:07,527] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 302000, global step 4833142: learning rate 0.0010
[2019-04-01 20:25:08,312] A3C_AGENT_WORKER-Thread-16 INFO:Local step 302000, global step 4833527: loss 0.0062
[2019-04-01 20:25:08,312] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 302000, global step 4833527: learning rate 0.0010
[2019-04-01 20:25:10,345] A3C_AGENT_WORKER-Thread-15 INFO:Local step 302000, global step 4834408: loss 0.0057
[2019-04-01 20:25:10,347] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 302000, global step 4834408: learning rate 0.0010
[2019-04-01 20:25:10,632] A3C_AGENT_WORKER-Thread-17 INFO:Local step 303500, global step 4834555: loss 0.8269
[2019-04-01 20:25:10,635] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 303500, global step 4834556: learning rate 0.0010
[2019-04-01 20:25:11,491] A3C_AGENT_WORKER-Thread-9 INFO:Local step 302000, global step 4834938: loss 0.0013
[2019-04-01 20:25:11,493] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 302000, global step 4834938: learning rate 0.0010
[2019-04-01 20:25:12,114] A3C_AGENT_WORKER-Thread-14 INFO:Local step 302000, global step 4835216: loss 0.0168
[2019-04-01 20:25:12,115] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 302000, global step 4835217: learning rate 0.0010
[2019-04-01 20:25:13,411] A3C_AGENT_WORKER-Thread-11 INFO:Local step 302000, global step 4835797: loss 0.0022
[2019-04-01 20:25:13,411] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 302000, global step 4835797: learning rate 0.0010
[2019-04-01 20:25:13,930] A3C_AGENT_WORKER-Thread-3 INFO:Local step 302000, global step 4836042: loss 0.0584
[2019-04-01 20:25:13,932] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 302000, global step 4836044: learning rate 0.0010
[2019-04-01 20:25:14,208] A3C_AGENT_WORKER-Thread-5 INFO:Local step 302000, global step 4836191: loss 0.0084
[2019-04-01 20:25:14,209] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 302000, global step 4836191: learning rate 0.0010
[2019-04-01 20:25:14,464] A3C_AGENT_WORKER-Thread-13 INFO:Local step 302000, global step 4836313: loss 0.0345
[2019-04-01 20:25:14,465] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 302000, global step 4836313: learning rate 0.0010
[2019-04-01 20:25:14,871] A3C_AGENT_WORKER-Thread-12 INFO:Local step 302000, global step 4836462: loss 0.0041
[2019-04-01 20:25:14,875] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 302000, global step 4836462: learning rate 0.0010
[2019-04-01 20:25:16,105] A3C_AGENT_WORKER-Thread-19 INFO:Local step 302000, global step 4837041: loss 0.0014
[2019-04-01 20:25:16,106] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 302000, global step 4837041: learning rate 0.0010
[2019-04-01 20:25:18,126] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8799471e-17 1.5839951e-36
 1.0000000e+00 1.4577017e-16], sum to 1.0000
[2019-04-01 20:25:18,127] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7962
[2019-04-01 20:25:18,136] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 82.5, 0.0, 26.0, 25.52281188303429, 7.676993935463595, 1.0, 1.0, 19.09534693381172], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2890800.0000, 
sim time next is 2891400.0000, 
raw observation next is [1.0, 94.16666666666666, 84.0, 0.0, 26.0, 25.53648974655691, 7.607954572847302, 1.0, 1.0, 18.20232630013134], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.9416666666666665, 0.28, 0.0, 1.0, 0.93378424950813, 0.07607954572847303, 1.0, 1.0, 0.13001661642950957], 
reward next is 0.8700, 
noisyNet noise sample is [array([0.56577724], dtype=float32), 0.2738687]. 
=============================================
[2019-04-01 20:25:19,412] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 5.898484e-32 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 20:25:19,426] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8070
[2019-04-01 20:25:19,443] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 96.5, 71.0, 54.0, 26.0, 25.97780773206418, 10.51089809955155, 1.0, 1.0, 15.43436480472], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2910600.0000, 
sim time next is 2911200.0000, 
raw observation next is [2.0, 95.33333333333334, 60.16666666666667, 51.83333333333333, 26.0, 26.01198972393983, 10.50102736504227, 1.0, 1.0, 14.6263550850171], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.9533333333333335, 0.20055555555555557, 0.057274401473296495, 1.0, 1.00171281770569, 0.1050102736504227, 1.0, 1.0, 0.10447396489297928], 
reward next is 0.6951, 
noisyNet noise sample is [array([0.17329735], dtype=float32), 0.5038162]. 
=============================================
[2019-04-01 20:25:20,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:25:20,562] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1489
[2019-04-01 20:25:20,579] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 40.0, 70.5, 579.5, 26.0, 25.22589907628693, 8.560378834984826, 0.0, 1.0, 11.1253688924948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3081600.0000, 
sim time next is 3082200.0000, 
raw observation next is [0.8333333333333334, 45.33333333333334, 66.0, 548.3333333333334, 26.0, 25.22133017994332, 8.413294017858426, 0.0, 1.0, 11.19785798794659], 
processed observation next is [0.0, 0.6956521739130435, 0.4856879039704525, 0.4533333333333334, 0.22, 0.6058931860036832, 1.0, 0.888761454277617, 0.08413294017858426, 0.0, 1.0, 0.07998469991390421], 
reward next is 0.9200, 
noisyNet noise sample is [array([1.0000275], dtype=float32), 0.6670628]. 
=============================================
[2019-04-01 20:25:20,626] A3C_AGENT_WORKER-Thread-2 INFO:Local step 302500, global step 4839112: loss 2.3191
[2019-04-01 20:25:20,627] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 302500, global step 4839112: learning rate 0.0010
[2019-04-01 20:25:21,053] A3C_AGENT_WORKER-Thread-18 INFO:Local step 303000, global step 4839328: loss 0.0146
[2019-04-01 20:25:21,054] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 303000, global step 4839328: learning rate 0.0010
[2019-04-01 20:25:21,667] A3C_AGENT_WORKER-Thread-4 INFO:Local step 303000, global step 4839612: loss 0.0016
[2019-04-01 20:25:21,668] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 303000, global step 4839612: learning rate 0.0010
[2019-04-01 20:25:24,599] A3C_AGENT_WORKER-Thread-20 INFO:Local step 302500, global step 4840977: loss 4.5462
[2019-04-01 20:25:24,600] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 302500, global step 4840977: learning rate 0.0010
[2019-04-01 20:25:25,173] A3C_AGENT_WORKER-Thread-17 INFO:Local step 304000, global step 4841227: loss 0.3431
[2019-04-01 20:25:25,174] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 304000, global step 4841227: learning rate 0.0010
[2019-04-01 20:25:25,936] A3C_AGENT_WORKER-Thread-10 INFO:Local step 302500, global step 4841583: loss 3.8380
[2019-04-01 20:25:25,938] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 302500, global step 4841583: learning rate 0.0010
[2019-04-01 20:25:26,935] A3C_AGENT_WORKER-Thread-16 INFO:Local step 302500, global step 4842008: loss 3.4408
[2019-04-01 20:25:26,936] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 302500, global step 4842008: learning rate 0.0010
[2019-04-01 20:25:28,944] A3C_AGENT_WORKER-Thread-15 INFO:Local step 302500, global step 4843010: loss 3.1879
[2019-04-01 20:25:28,944] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 302500, global step 4843010: learning rate 0.0010
[2019-04-01 20:25:29,605] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:25:29,606] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8628
[2019-04-01 20:25:29,617] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.25891430566864, 5.501487851690196, 0.0, 1.0, 38.96065428152622], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3034800.0000, 
sim time next is 3035400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.22395888626292, 5.484179146352538, 0.0, 1.0, 39.10060519476693], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 1.0, 0.7462798408947029, 0.054841791463525386, 0.0, 1.0, 0.27929003710547806], 
reward next is 0.7207, 
noisyNet noise sample is [array([-2.224183], dtype=float32), 0.14898345]. 
=============================================
[2019-04-01 20:25:30,038] A3C_AGENT_WORKER-Thread-9 INFO:Local step 302500, global step 4843628: loss 0.8738
[2019-04-01 20:25:30,041] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 302500, global step 4843629: learning rate 0.0010
[2019-04-01 20:25:30,303] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7467822e-34
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:25:30,304] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4135
[2019-04-01 20:25:30,324] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 26.0, 25.29860592452792, 7.46526583823529, 0.0, 1.0, 39.56450217469304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3109200.0000, 
sim time next is 3109800.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 26.0, 25.29779245679741, 7.449191290408362, 0.0, 1.0, 39.38429810706869], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 1.0, 0.8996846366853443, 0.07449191290408362, 0.0, 1.0, 0.2813164150504906], 
reward next is 0.7187, 
noisyNet noise sample is [array([-1.4169278], dtype=float32), 0.48756114]. 
=============================================
[2019-04-01 20:25:30,391] A3C_AGENT_WORKER-Thread-14 INFO:Local step 302500, global step 4843828: loss 1.2112
[2019-04-01 20:25:30,393] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 302500, global step 4843828: learning rate 0.0010
[2019-04-01 20:25:31,567] A3C_AGENT_WORKER-Thread-11 INFO:Local step 302500, global step 4844335: loss 0.2890
[2019-04-01 20:25:31,571] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 302500, global step 4844335: learning rate 0.0010
[2019-04-01 20:25:32,112] A3C_AGENT_WORKER-Thread-3 INFO:Local step 302500, global step 4844623: loss 0.2249
[2019-04-01 20:25:32,113] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 302500, global step 4844623: learning rate 0.0010
[2019-04-01 20:25:32,653] A3C_AGENT_WORKER-Thread-5 INFO:Local step 302500, global step 4844912: loss 0.0379
[2019-04-01 20:25:32,656] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 302500, global step 4844912: learning rate 0.0010
[2019-04-01 20:25:32,878] A3C_AGENT_WORKER-Thread-13 INFO:Local step 302500, global step 4845031: loss 0.0123
[2019-04-01 20:25:32,882] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 302500, global step 4845031: learning rate 0.0010
[2019-04-01 20:25:33,382] A3C_AGENT_WORKER-Thread-12 INFO:Local step 302500, global step 4845315: loss 0.0193
[2019-04-01 20:25:33,384] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 302500, global step 4845316: learning rate 0.0010
[2019-04-01 20:25:34,783] A3C_AGENT_WORKER-Thread-19 INFO:Local step 302500, global step 4846157: loss 0.5641
[2019-04-01 20:25:34,785] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 302500, global step 4846158: learning rate 0.0010
[2019-04-01 20:25:35,225] A3C_AGENT_WORKER-Thread-18 INFO:Local step 303500, global step 4846400: loss 0.2503
[2019-04-01 20:25:35,227] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 303500, global step 4846401: learning rate 0.0010
[2019-04-01 20:25:35,239] A3C_AGENT_WORKER-Thread-2 INFO:Local step 303000, global step 4846408: loss 0.4350
[2019-04-01 20:25:35,241] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 303000, global step 4846408: learning rate 0.0010
[2019-04-01 20:25:35,898] A3C_AGENT_WORKER-Thread-4 INFO:Local step 303500, global step 4846766: loss 0.1495
[2019-04-01 20:25:35,902] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 303500, global step 4846768: learning rate 0.0010
[2019-04-01 20:25:39,443] A3C_AGENT_WORKER-Thread-20 INFO:Local step 303000, global step 4848658: loss 0.1781
[2019-04-01 20:25:39,445] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 303000, global step 4848658: learning rate 0.0010
[2019-04-01 20:25:39,908] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.3723032e-37 5.9221314e-33
 1.0000000e+00 1.6291095e-22], sum to 1.0000
[2019-04-01 20:25:39,914] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4717
[2019-04-01 20:25:39,935] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.47317824772529, 10.33975089434571, 0.0, 1.0, 32.71259856981587], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3361800.0000, 
sim time next is 3362400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.41609689114349, 10.37469960312931, 0.0, 1.0, 36.14540646033941], 
processed observation next is [1.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 1.0, 0.9165852701633556, 0.1037469960312931, 0.0, 1.0, 0.25818147471671005], 
reward next is 0.7418, 
noisyNet noise sample is [array([0.16128588], dtype=float32), -2.1857631]. 
=============================================
[2019-04-01 20:25:40,709] A3C_AGENT_WORKER-Thread-17 INFO:Local step 304500, global step 4849276: loss 0.6935
[2019-04-01 20:25:40,711] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 304500, global step 4849276: learning rate 0.0010
[2019-04-01 20:25:40,757] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.2965253e-18], sum to 1.0000
[2019-04-01 20:25:40,758] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2692
[2019-04-01 20:25:40,773] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.933333333333334, 58.00000000000001, 222.1666666666667, 440.3333333333334, 26.0, 25.37395898723748, 9.4065733269441, 0.0, 1.0, 5.179596630184607], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4285200.0000, 
sim time next is 4285800.0000, 
raw observation next is [6.9, 58.5, 229.0, 385.0, 26.0, 25.37725673531837, 9.381924406523408, 0.0, 1.0, 4.067988086975482], 
processed observation next is [0.0, 0.6086956521739131, 0.6537396121883658, 0.585, 0.7633333333333333, 0.425414364640884, 1.0, 0.9110366764740527, 0.09381924406523408, 0.0, 1.0, 0.029057057764110585], 
reward next is 0.9709, 
noisyNet noise sample is [array([-0.56895036], dtype=float32), 1.7785882]. 
=============================================
[2019-04-01 20:25:41,060] A3C_AGENT_WORKER-Thread-10 INFO:Local step 303000, global step 4849455: loss 0.0734
[2019-04-01 20:25:41,062] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 303000, global step 4849455: learning rate 0.0010
[2019-04-01 20:25:41,792] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9993992e-01 4.1985846e-21
 6.0034934e-05 0.0000000e+00], sum to 1.0000
[2019-04-01 20:25:41,794] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0969
[2019-04-01 20:25:41,853] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 68.0, 0.0, 0.0, 26.0, 25.60212391248433, 12.21228178074608, 1.0, 1.0, 28.8941215812537], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3267000.0000, 
sim time next is 3267600.0000, 
raw observation next is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.58652133610077, 11.97726975365232, 1.0, 1.0, 28.89352255696009], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.69, 0.0, 0.0, 1.0, 0.9409316194429671, 0.11977269753652321, 1.0, 1.0, 0.20638230397828636], 
reward next is 0.0027, 
noisyNet noise sample is [array([-0.07196131], dtype=float32), 1.2241541]. 
=============================================
[2019-04-01 20:25:41,925] A3C_AGENT_WORKER-Thread-16 INFO:Local step 303000, global step 4849864: loss 0.1135
[2019-04-01 20:25:41,926] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 303000, global step 4849864: learning rate 0.0010
[2019-04-01 20:25:42,617] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:25:42,622] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1481
[2019-04-01 20:25:42,634] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.666666666666667, 88.33333333333333, 0.0, 0.0, 26.0, 25.63625087130162, 12.00981931953643, 0.0, 1.0, 27.07017530883198], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3274800.0000, 
sim time next is 3275400.0000, 
raw observation next is [-5.833333333333333, 90.16666666666667, 0.0, 0.0, 26.0, 25.56281289294513, 11.57093499820525, 0.0, 1.0, 25.69997756776029], 
processed observation next is [1.0, 0.9130434782608695, 0.30101569713758086, 0.9016666666666667, 0.0, 0.0, 1.0, 0.9375446989921612, 0.1157093499820525, 0.0, 1.0, 0.18357126834114493], 
reward next is 0.8164, 
noisyNet noise sample is [array([1.2890118], dtype=float32), 0.020427521]. 
=============================================
[2019-04-01 20:25:43,725] A3C_AGENT_WORKER-Thread-15 INFO:Local step 303000, global step 4850718: loss 0.2710
[2019-04-01 20:25:43,726] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 303000, global step 4850718: learning rate 0.0010
[2019-04-01 20:25:45,160] A3C_AGENT_WORKER-Thread-9 INFO:Local step 303000, global step 4851435: loss 0.1481
[2019-04-01 20:25:45,165] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 303000, global step 4851436: learning rate 0.0010
[2019-04-01 20:25:45,806] A3C_AGENT_WORKER-Thread-14 INFO:Local step 303000, global step 4851790: loss 0.0162
[2019-04-01 20:25:45,807] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 303000, global step 4851790: learning rate 0.0010
[2019-04-01 20:25:46,932] A3C_AGENT_WORKER-Thread-11 INFO:Local step 303000, global step 4852369: loss 0.0697
[2019-04-01 20:25:46,933] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 303000, global step 4852369: learning rate 0.0010
[2019-04-01 20:25:47,336] A3C_AGENT_WORKER-Thread-3 INFO:Local step 303000, global step 4852587: loss 0.0450
[2019-04-01 20:25:47,340] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 303000, global step 4852588: learning rate 0.0010
[2019-04-01 20:25:47,539] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.4405721e-27], sum to 1.0000
[2019-04-01 20:25:47,541] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5933
[2019-04-01 20:25:47,591] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 55.5, 91.0, 466.0, 26.0, 25.88339262461619, 10.45913067902165, 1.0, 1.0, 29.09223143226895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3918600.0000, 
sim time next is 3919200.0000, 
raw observation next is [-8.0, 54.66666666666666, 92.5, 503.6666666666667, 26.0, 25.93189606161982, 10.56014128361972, 1.0, 1.0, 27.13936252054765], 
processed observation next is [1.0, 0.34782608695652173, 0.24099722991689754, 0.5466666666666665, 0.30833333333333335, 0.5565377532228362, 1.0, 0.9902708659456886, 0.1056014128361972, 1.0, 1.0, 0.1938525894324832], 
reward next is 0.5821, 
noisyNet noise sample is [array([0.31347844], dtype=float32), -1.0430826]. 
=============================================
[2019-04-01 20:25:48,061] A3C_AGENT_WORKER-Thread-5 INFO:Local step 303000, global step 4852978: loss 0.1224
[2019-04-01 20:25:48,064] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 303000, global step 4852979: learning rate 0.0010
[2019-04-01 20:25:48,280] A3C_AGENT_WORKER-Thread-13 INFO:Local step 303000, global step 4853107: loss 0.0249
[2019-04-01 20:25:48,282] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 303000, global step 4853108: learning rate 0.0010
[2019-04-01 20:25:48,526] A3C_AGENT_WORKER-Thread-12 INFO:Local step 303000, global step 4853241: loss 0.1082
[2019-04-01 20:25:48,526] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 303000, global step 4853241: learning rate 0.0010
[2019-04-01 20:25:48,598] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:25:48,600] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8883
[2019-04-01 20:25:48,636] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 74.0, 0.0, 0.0, 26.0, 24.81778841810119, 6.516253052959821, 0.0, 1.0, 41.38818413555777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3385800.0000, 
sim time next is 3386400.0000, 
raw observation next is [-5.333333333333333, 73.0, 0.0, 0.0, 26.0, 24.79549504481288, 6.387484580146861, 0.0, 1.0, 41.55464706077803], 
processed observation next is [1.0, 0.17391304347826086, 0.3148661126500462, 0.73, 0.0, 0.0, 1.0, 0.8279278635446973, 0.06387484580146861, 0.0, 1.0, 0.2968189075769859], 
reward next is 0.7032, 
noisyNet noise sample is [array([1.9655864], dtype=float32), -1.2397685]. 
=============================================
[2019-04-01 20:25:49,860] A3C_AGENT_WORKER-Thread-19 INFO:Local step 303000, global step 4853901: loss 0.2067
[2019-04-01 20:25:49,863] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 303000, global step 4853901: learning rate 0.0010
[2019-04-01 20:25:50,298] A3C_AGENT_WORKER-Thread-2 INFO:Local step 303500, global step 4854149: loss 1.0072
[2019-04-01 20:25:50,299] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 303500, global step 4854149: learning rate 0.0010
[2019-04-01 20:25:50,586] A3C_AGENT_WORKER-Thread-18 INFO:Local step 304000, global step 4854317: loss 0.0018
[2019-04-01 20:25:50,589] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 304000, global step 4854319: learning rate 0.0010
[2019-04-01 20:25:51,144] A3C_AGENT_WORKER-Thread-4 INFO:Local step 304000, global step 4854636: loss 0.2145
[2019-04-01 20:25:51,147] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 304000, global step 4854637: learning rate 0.0010
[2019-04-01 20:25:52,331] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5811337e-28 4.4444695e-26 1.4860421e-21 2.6442196e-06 1.8846410e-14
 9.9999738e-01 1.9996741e-20], sum to 1.0000
[2019-04-01 20:25:52,332] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7171
[2019-04-01 20:25:52,348] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 67.0, 61.0, 513.0, 26.0, 26.40194424183235, 14.78119189727975, 1.0, 1.0, 9.34993199817529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3429000.0000, 
sim time next is 3429600.0000, 
raw observation next is [2.0, 67.0, 52.83333333333334, 447.6666666666667, 26.0, 26.58989618398172, 15.07445066397783, 1.0, 1.0, 10.05900881626145], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.17611111111111113, 0.4946593001841621, 1.0, 1.0842708834259598, 0.1507445066397783, 1.0, 1.0, 0.07185006297329607], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1647842], dtype=float32), 0.22714883]. 
=============================================
[2019-04-01 20:25:54,063] A3C_AGENT_WORKER-Thread-20 INFO:Local step 303500, global step 4856248: loss 0.2360
[2019-04-01 20:25:54,066] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 303500, global step 4856248: learning rate 0.0010
[2019-04-01 20:25:54,542] A3C_AGENT_WORKER-Thread-17 INFO:Local step 305000, global step 4856558: loss 1.1898
[2019-04-01 20:25:54,543] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 305000, global step 4856559: learning rate 0.0010
[2019-04-01 20:25:55,232] A3C_AGENT_WORKER-Thread-10 INFO:Local step 303500, global step 4856956: loss 0.3848
[2019-04-01 20:25:55,234] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 303500, global step 4856956: learning rate 0.0010
[2019-04-01 20:25:56,492] A3C_AGENT_WORKER-Thread-16 INFO:Local step 303500, global step 4857673: loss 0.2194
[2019-04-01 20:25:56,504] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 303500, global step 4857676: learning rate 0.0010
[2019-04-01 20:25:57,945] A3C_AGENT_WORKER-Thread-15 INFO:Local step 303500, global step 4858365: loss 0.0742
[2019-04-01 20:25:57,947] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 303500, global step 4858365: learning rate 0.0010
[2019-04-01 20:25:59,197] A3C_AGENT_WORKER-Thread-9 INFO:Local step 303500, global step 4859048: loss 0.0076
[2019-04-01 20:25:59,200] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 303500, global step 4859049: learning rate 0.0010
[2019-04-01 20:26:00,557] A3C_AGENT_WORKER-Thread-14 INFO:Local step 303500, global step 4859813: loss 0.0030
[2019-04-01 20:26:00,559] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 303500, global step 4859813: learning rate 0.0010
[2019-04-01 20:26:01,893] A3C_AGENT_WORKER-Thread-11 INFO:Local step 303500, global step 4860519: loss 0.1500
[2019-04-01 20:26:01,895] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 303500, global step 4860520: learning rate 0.0010
[2019-04-01 20:26:01,964] A3C_AGENT_WORKER-Thread-3 INFO:Local step 303500, global step 4860554: loss 0.1279
[2019-04-01 20:26:01,966] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 303500, global step 4860555: learning rate 0.0010
[2019-04-01 20:26:02,828] A3C_AGENT_WORKER-Thread-5 INFO:Local step 303500, global step 4861048: loss 0.1861
[2019-04-01 20:26:02,830] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 303500, global step 4861049: learning rate 0.0010
[2019-04-01 20:26:02,911] A3C_AGENT_WORKER-Thread-13 INFO:Local step 303500, global step 4861104: loss 0.0836
[2019-04-01 20:26:02,912] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 303500, global step 4861104: learning rate 0.0010
[2019-04-01 20:26:03,076] A3C_AGENT_WORKER-Thread-12 INFO:Local step 303500, global step 4861201: loss 0.0547
[2019-04-01 20:26:03,079] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 303500, global step 4861202: learning rate 0.0010
[2019-04-01 20:26:04,664] A3C_AGENT_WORKER-Thread-19 INFO:Local step 303500, global step 4862119: loss 0.2556
[2019-04-01 20:26:04,666] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 303500, global step 4862119: learning rate 0.0010
[2019-04-01 20:26:05,314] A3C_AGENT_WORKER-Thread-2 INFO:Local step 304000, global step 4862440: loss 0.0263
[2019-04-01 20:26:05,318] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 304000, global step 4862440: learning rate 0.0010
[2019-04-01 20:26:06,663] A3C_AGENT_WORKER-Thread-18 INFO:Local step 304500, global step 4863102: loss 0.1251
[2019-04-01 20:26:06,667] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 304500, global step 4863102: learning rate 0.0010
[2019-04-01 20:26:07,156] A3C_AGENT_WORKER-Thread-4 INFO:Local step 304500, global step 4863362: loss 0.1698
[2019-04-01 20:26:07,158] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 304500, global step 4863364: learning rate 0.0010
[2019-04-01 20:26:09,663] A3C_AGENT_WORKER-Thread-20 INFO:Local step 304000, global step 4864631: loss 0.1263
[2019-04-01 20:26:09,667] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 304000, global step 4864635: learning rate 0.0010
[2019-04-01 20:26:10,536] A3C_AGENT_WORKER-Thread-10 INFO:Local step 304000, global step 4865038: loss 0.0494
[2019-04-01 20:26:10,537] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 304000, global step 4865038: learning rate 0.0010
[2019-04-01 20:26:10,718] A3C_AGENT_WORKER-Thread-17 INFO:Local step 305500, global step 4865130: loss 1.9857
[2019-04-01 20:26:10,720] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 305500, global step 4865131: learning rate 0.0010
[2019-04-01 20:26:11,910] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.8413455e-30 8.2475875e-22 3.0508391e-09 1.0000000e+00
 6.9793656e-24 5.8369549e-16], sum to 1.0000
[2019-04-01 20:26:11,910] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1042
[2019-04-01 20:26:11,928] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 41.0, 41.0, 365.0, 26.0, 27.13685166749231, 18.49653046770771, 1.0, 1.0, 7.436192088480894], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3862800.0000, 
sim time next is 3863400.0000, 
raw observation next is [2.833333333333333, 42.16666666666667, 33.33333333333333, 303.0, 26.0, 27.2504448086601, 14.84951177785668, 1.0, 1.0, 7.855454011249178], 
processed observation next is [1.0, 0.7391304347826086, 0.541089566020314, 0.4216666666666667, 0.11111111111111109, 0.33480662983425413, 1.0, 1.1786349726657284, 0.1484951177785668, 1.0, 1.0, 0.05611038579463699], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2569999], dtype=float32), -0.3717201]. 
=============================================
[2019-04-01 20:26:12,233] A3C_AGENT_WORKER-Thread-16 INFO:Local step 304000, global step 4865931: loss 0.0030
[2019-04-01 20:26:12,234] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 304000, global step 4865931: learning rate 0.0010
[2019-04-01 20:26:12,542] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 5.726361e-35 0.000000e+00 0.000000e+00
 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-04-01 20:26:12,543] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8330
[2019-04-01 20:26:12,587] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 55.5, 91.0, 466.0, 26.0, 25.88339262461619, 10.45913067902165, 1.0, 1.0, 29.09223143226895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3918600.0000, 
sim time next is 3919200.0000, 
raw observation next is [-8.0, 54.66666666666666, 92.5, 503.6666666666667, 26.0, 25.93189606161982, 10.56014128361972, 1.0, 1.0, 27.13936252054765], 
processed observation next is [1.0, 0.34782608695652173, 0.24099722991689754, 0.5466666666666665, 0.30833333333333335, 0.5565377532228362, 1.0, 0.9902708659456886, 0.1056014128361972, 1.0, 1.0, 0.1938525894324832], 
reward next is 0.5821, 
noisyNet noise sample is [array([1.7792621], dtype=float32), -2.7217102]. 
=============================================
[2019-04-01 20:26:13,777] A3C_AGENT_WORKER-Thread-15 INFO:Local step 304000, global step 4866708: loss 0.0039
[2019-04-01 20:26:13,780] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 304000, global step 4866710: learning rate 0.0010
[2019-04-01 20:26:14,205] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 9.4448262e-32 0.0000000e+00 2.9827946e-31
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:26:14,206] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0420
[2019-04-01 20:26:14,213] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 49.5, 0.0, 0.0, 26.0, 26.12060192153584, 12.78110109479277, 1.0, 1.0, 8.085840371866661], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3868200.0000, 
sim time next is 3868800.0000, 
raw observation next is [1.333333333333333, 50.0, 0.0, 0.0, 26.0, 25.96776560846798, 12.4288090488493, 1.0, 1.0, 8.131812304697933], 
processed observation next is [1.0, 0.782608695652174, 0.4995383194829178, 0.5, 0.0, 0.0, 1.0, 0.9953950869239974, 0.12428809048849301, 1.0, 1.0, 0.05808437360498523], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2551365], dtype=float32), 0.5630203]. 
=============================================
[2019-04-01 20:26:14,864] A3C_AGENT_WORKER-Thread-9 INFO:Local step 304000, global step 4867192: loss 0.0017
[2019-04-01 20:26:14,865] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 304000, global step 4867192: learning rate 0.0010
[2019-04-01 20:26:16,340] A3C_AGENT_WORKER-Thread-14 INFO:Local step 304000, global step 4867889: loss 0.0551
[2019-04-01 20:26:16,341] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 304000, global step 4867889: learning rate 0.0010
[2019-04-01 20:26:17,221] A3C_AGENT_WORKER-Thread-3 INFO:Local step 304000, global step 4868334: loss 0.0027
[2019-04-01 20:26:17,222] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 304000, global step 4868334: learning rate 0.0010
[2019-04-01 20:26:17,499] A3C_AGENT_WORKER-Thread-11 INFO:Local step 304000, global step 4868477: loss 0.0436
[2019-04-01 20:26:17,499] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 304000, global step 4868477: learning rate 0.0010
[2019-04-01 20:26:18,179] A3C_AGENT_WORKER-Thread-5 INFO:Local step 304000, global step 4868822: loss 0.0447
[2019-04-01 20:26:18,180] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 304000, global step 4868822: learning rate 0.0010
[2019-04-01 20:26:18,324] A3C_AGENT_WORKER-Thread-13 INFO:Local step 304000, global step 4868904: loss 0.0686
[2019-04-01 20:26:18,326] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 304000, global step 4868905: learning rate 0.0010
[2019-04-01 20:26:18,461] A3C_AGENT_WORKER-Thread-12 INFO:Local step 304000, global step 4868972: loss 0.0030
[2019-04-01 20:26:18,463] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 304000, global step 4868974: learning rate 0.0010
[2019-04-01 20:26:20,135] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9792330e-25 3.3821057e-22 6.1210882e-17 1.0000000e+00 9.3233411e-30
 3.1173545e-17 9.3948497e-38], sum to 1.0000
[2019-04-01 20:26:20,141] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2033
[2019-04-01 20:26:20,167] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 22.0, 78.0, 630.0, 26.0, 27.14842704605465, 18.39317707498559, 1.0, 1.0, 7.074116171937842], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4032000.0000, 
sim time next is 4032600.0000, 
raw observation next is [-1.166666666666667, 22.33333333333334, 74.33333333333333, 602.6666666666666, 26.0, 27.2290366542, 18.69497981546595, 1.0, 1.0, 6.160737948117083], 
processed observation next is [1.0, 0.6956521739130435, 0.43028624192059095, 0.22333333333333338, 0.24777777777777776, 0.6659300184162062, 1.0, 1.1755766648857144, 0.18694979815465948, 1.0, 1.0, 0.04400527105797916], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.43621334], dtype=float32), 1.1172241]. 
=============================================
[2019-04-01 20:26:20,423] A3C_AGENT_WORKER-Thread-18 INFO:Local step 305000, global step 4869945: loss 0.8550
[2019-04-01 20:26:20,424] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 305000, global step 4869945: learning rate 0.0010
[2019-04-01 20:26:20,518] A3C_AGENT_WORKER-Thread-19 INFO:Local step 304000, global step 4869997: loss 0.0607
[2019-04-01 20:26:20,520] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 304000, global step 4869998: learning rate 0.0010
[2019-04-01 20:26:20,792] A3C_AGENT_WORKER-Thread-4 INFO:Local step 305000, global step 4870158: loss 0.1076
[2019-04-01 20:26:20,794] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 305000, global step 4870158: learning rate 0.0010
[2019-04-01 20:26:21,190] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:26:21,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:26:21,200] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run37
[2019-04-01 20:26:21,300] A3C_AGENT_WORKER-Thread-2 INFO:Local step 304500, global step 4870432: loss 2.1466
[2019-04-01 20:26:21,302] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 304500, global step 4870434: learning rate 0.0010
[2019-04-01 20:26:23,196] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00
 8.2821635e-32 1.1933472e-29], sum to 1.0000
[2019-04-01 20:26:23,197] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0049
[2019-04-01 20:26:23,215] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.333333333333333, 28.16666666666667, 120.3333333333333, 832.6666666666667, 26.0, 26.01396920548858, 12.2406277767426, 1.0, 1.0, 6.136203331348944], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4104600.0000, 
sim time next is 4105200.0000, 
raw observation next is [1.666666666666667, 28.33333333333334, 120.1666666666667, 836.8333333333334, 26.0, 25.82573360059784, 12.06878299809662, 1.0, 1.0, 5.802793671443363], 
processed observation next is [1.0, 0.5217391304347826, 0.5087719298245615, 0.2833333333333334, 0.40055555555555566, 0.9246777163904236, 1.0, 0.9751048000854057, 0.1206878299809662, 1.0, 1.0, 0.04144852622459545], 
reward next is 0.1310, 
noisyNet noise sample is [array([0.59717196], dtype=float32), -0.4644086]. 
=============================================
[2019-04-01 20:26:24,902] A3C_AGENT_WORKER-Thread-20 INFO:Local step 304500, global step 4872265: loss 2.6444
[2019-04-01 20:26:24,903] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 304500, global step 4872266: learning rate 0.0010
[2019-04-01 20:26:26,332] A3C_AGENT_WORKER-Thread-10 INFO:Local step 304500, global step 4873068: loss 2.1105
[2019-04-01 20:26:26,333] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 304500, global step 4873068: learning rate 0.0010
[2019-04-01 20:26:27,247] A3C_AGENT_WORKER-Thread-16 INFO:Local step 304500, global step 4873595: loss 2.3207
[2019-04-01 20:26:27,247] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 304500, global step 4873595: learning rate 0.0010
[2019-04-01 20:26:29,110] A3C_AGENT_WORKER-Thread-15 INFO:Local step 304500, global step 4874639: loss 2.0673
[2019-04-01 20:26:29,111] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 304500, global step 4874639: learning rate 0.0010
[2019-04-01 20:26:29,276] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:26:29,278] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9601
[2019-04-01 20:26:29,293] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.066666666666666, 92.33333333333334, 0.0, 0.0, 26.0, 24.03949550120162, 5.545838487123253, 0.0, 1.0, 41.18060357859492], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4774800.0000, 
sim time next is 4775400.0000, 
raw observation next is [-6.1, 92.5, 0.0, 0.0, 26.0, 24.00000993675024, 5.506583286547439, 0.0, 1.0, 41.23072737452511], 
processed observation next is [0.0, 0.2608695652173913, 0.29362880886426596, 0.925, 0.0, 0.0, 1.0, 0.7142871338214627, 0.05506583286547439, 0.0, 1.0, 0.2945051955323222], 
reward next is 0.7055, 
noisyNet noise sample is [array([-1.1809431], dtype=float32), -1.6896268]. 
=============================================
[2019-04-01 20:26:30,132] A3C_AGENT_WORKER-Thread-9 INFO:Local step 304500, global step 4875236: loss 2.1828
[2019-04-01 20:26:30,134] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 304500, global step 4875236: learning rate 0.0010
[2019-04-01 20:26:31,235] A3C_AGENT_WORKER-Thread-14 INFO:Local step 304500, global step 4875834: loss 1.8035
[2019-04-01 20:26:31,238] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 304500, global step 4875834: learning rate 0.0010
[2019-04-01 20:26:32,246] A3C_AGENT_WORKER-Thread-3 INFO:Local step 304500, global step 4876408: loss 2.2623
[2019-04-01 20:26:32,250] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 304500, global step 4876409: learning rate 0.0010
[2019-04-01 20:26:32,665] A3C_AGENT_WORKER-Thread-11 INFO:Local step 304500, global step 4876656: loss 2.0482
[2019-04-01 20:26:32,674] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 304500, global step 4876659: learning rate 0.0010
[2019-04-01 20:26:32,914] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:26:32,916] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6380
[2019-04-01 20:26:32,936] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 52.0, 120.3333333333333, 838.6666666666667, 26.0, 25.1696064891096, 8.65972690542396, 0.0, 1.0, 6.181527097821257], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4277400.0000, 
sim time next is 4278000.0000, 
raw observation next is [7.0, 52.0, 120.1666666666667, 842.8333333333334, 26.0, 25.18071528198153, 8.746553754833448, 0.0, 1.0, 5.944409809555373], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.40055555555555566, 0.9313075506445673, 1.0, 0.8829593259973615, 0.08746553754833447, 0.0, 1.0, 0.04246007006825266], 
reward next is 0.9575, 
noisyNet noise sample is [array([0.30520958], dtype=float32), -2.3829556]. 
=============================================
[2019-04-01 20:26:32,947] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[77.569565]
 [78.37313 ]
 [79.00117 ]
 [79.46399 ]
 [79.73307 ]], R is [[76.90460968]
 [77.09141541]
 [77.27441406]
 [77.45392609]
 [77.62825775]].
[2019-04-01 20:26:33,156] A3C_AGENT_WORKER-Thread-5 INFO:Local step 304500, global step 4876929: loss 3.0883
[2019-04-01 20:26:33,158] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 304500, global step 4876929: learning rate 0.0010
[2019-04-01 20:26:33,287] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:26:33,288] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0537
[2019-04-01 20:26:33,298] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.9, 69.66666666666667, 0.0, 0.0, 26.0, 24.87857243819548, 7.155738919652975, 0.0, 1.0, 16.74368714115296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4302600.0000, 
sim time next is 4303200.0000, 
raw observation next is [5.8, 70.33333333333334, 0.0, 0.0, 26.0, 24.84165987005324, 7.367589721496032, 0.0, 1.0, 42.83268928313139], 
processed observation next is [0.0, 0.8260869565217391, 0.6232686980609419, 0.7033333333333335, 0.0, 0.0, 1.0, 0.8345228385790341, 0.07367589721496032, 0.0, 1.0, 0.3059477805937956], 
reward next is 0.6941, 
noisyNet noise sample is [array([1.5572778], dtype=float32), 2.0029936]. 
=============================================
[2019-04-01 20:26:33,676] A3C_AGENT_WORKER-Thread-13 INFO:Local step 304500, global step 4877227: loss 3.0300
[2019-04-01 20:26:33,676] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 304500, global step 4877227: learning rate 0.0010
[2019-04-01 20:26:33,950] A3C_AGENT_WORKER-Thread-12 INFO:Local step 304500, global step 4877402: loss 2.1411
[2019-04-01 20:26:33,952] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 304500, global step 4877403: learning rate 0.0010
[2019-04-01 20:26:34,826] A3C_AGENT_WORKER-Thread-2 INFO:Local step 305000, global step 4877849: loss 0.4730
[2019-04-01 20:26:34,827] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 305000, global step 4877849: learning rate 0.0010
[2019-04-01 20:26:35,412] A3C_AGENT_WORKER-Thread-18 INFO:Local step 305500, global step 4878146: loss 0.0321
[2019-04-01 20:26:35,415] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 305500, global step 4878147: learning rate 0.0010
[2019-04-01 20:26:35,694] A3C_AGENT_WORKER-Thread-19 INFO:Local step 304500, global step 4878296: loss 2.7111
[2019-04-01 20:26:35,697] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 304500, global step 4878296: learning rate 0.0010
[2019-04-01 20:26:36,033] A3C_AGENT_WORKER-Thread-4 INFO:Local step 305500, global step 4878479: loss 0.0152
[2019-04-01 20:26:36,036] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 305500, global step 4878479: learning rate 0.0010
[2019-04-01 20:26:38,852] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:26:38,852] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7370
[2019-04-01 20:26:38,869] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.4, 61.0, 0.0, 0.0, 26.0, 26.67340450057056, 19.16217046155807, 0.0, 1.0, 6.590674435103897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4399200.0000, 
sim time next is 4399800.0000, 
raw observation next is [9.25, 61.16666666666667, 0.0, 0.0, 26.0, 26.62672820561962, 17.16360983592497, 0.0, 1.0, 6.689882851306905], 
processed observation next is [1.0, 0.9565217391304348, 0.718836565096953, 0.6116666666666667, 0.0, 0.0, 1.0, 1.089532600802803, 0.1716360983592497, 0.0, 1.0, 0.047784877509335036], 
reward next is 0.9522, 
noisyNet noise sample is [array([-0.21201658], dtype=float32), -0.8812668]. 
=============================================
[2019-04-01 20:26:39,174] A3C_AGENT_WORKER-Thread-20 INFO:Local step 305000, global step 4880198: loss 2.2266
[2019-04-01 20:26:39,175] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 305000, global step 4880198: learning rate 0.0010
[2019-04-01 20:26:39,832] A3C_AGENT_WORKER-Thread-10 INFO:Local step 305000, global step 4880573: loss 2.4931
[2019-04-01 20:26:39,833] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 305000, global step 4880573: learning rate 0.0010
[2019-04-01 20:26:40,957] A3C_AGENT_WORKER-Thread-16 INFO:Local step 305000, global step 4881175: loss 3.7019
[2019-04-01 20:26:40,957] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 305000, global step 4881175: learning rate 0.0010
[2019-04-01 20:26:42,123] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:26:42,129] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5838
[2019-04-01 20:26:42,146] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3, 72.0, 0.0, 0.0, 26.0, 25.23175094402388, 9.94175619945318, 0.0, 1.0, 44.49463131254841], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4489200.0000, 
sim time next is 4489800.0000, 
raw observation next is [-0.3333333333333333, 72.16666666666667, 0.0, 0.0, 26.0, 25.26730023054164, 10.08650650727474, 0.0, 1.0, 43.64920853601389], 
processed observation next is [1.0, 1.0, 0.4533702677747, 0.7216666666666667, 0.0, 0.0, 1.0, 0.8953286043630914, 0.1008650650727474, 0.0, 1.0, 0.3117800609715278], 
reward next is 0.6882, 
noisyNet noise sample is [array([-0.5545288], dtype=float32), 0.9100066]. 
=============================================
[2019-04-01 20:26:43,084] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9941782e-38 3.4251176e-31 6.4708208e-28 2.2481082e-15 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:26:43,096] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3771
[2019-04-01 20:26:43,145] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.666666666666667, 45.0, 109.5, 670.5, 26.0, 26.45070727472026, 13.05981365293111, 1.0, 1.0, 12.8060966609711], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5044800.0000, 
sim time next is 5045400.0000, 
raw observation next is [2.0, 44.0, 112.0, 698.0, 26.0, 26.5329895258024, 13.76836844616127, 1.0, 1.0, 12.77755777373558], 
processed observation next is [1.0, 0.391304347826087, 0.518005540166205, 0.44, 0.37333333333333335, 0.7712707182320442, 1.0, 1.0761413608289143, 0.1376836844616127, 1.0, 1.0, 0.091268269812397], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.28746298], dtype=float32), 1.8383741]. 
=============================================
[2019-04-01 20:26:43,160] A3C_AGENT_WORKER-Thread-15 INFO:Local step 305000, global step 4882248: loss 0.3771
[2019-04-01 20:26:43,163] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 305000, global step 4882248: learning rate 0.0010
[2019-04-01 20:26:44,471] A3C_AGENT_WORKER-Thread-9 INFO:Local step 305000, global step 4882935: loss 1.1954
[2019-04-01 20:26:44,473] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 305000, global step 4882935: learning rate 0.0010
[2019-04-01 20:26:45,318] A3C_AGENT_WORKER-Thread-14 INFO:Local step 305000, global step 4883358: loss 0.0257
[2019-04-01 20:26:45,319] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 305000, global step 4883359: learning rate 0.0010
[2019-04-01 20:26:45,515] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.6218782e-16 3.3992627e-26 1.0000000e+00 4.0966870e-16
 3.8838111e-27 1.5637365e-16], sum to 1.0000
[2019-04-01 20:26:45,515] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1666
[2019-04-01 20:26:45,523] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.35, 49.0, 139.0, 813.0, 26.0, 26.82771906648984, 17.82179924475086, 1.0, 1.0, 1.912163607755925], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4627800.0000, 
sim time next is 4628400.0000, 
raw observation next is [4.466666666666667, 49.0, 149.6666666666667, 777.3333333333333, 26.0, 26.93728461992619, 18.32908360963813, 1.0, 1.0, 1.864137616135152], 
processed observation next is [1.0, 0.5652173913043478, 0.5863342566943676, 0.49, 0.49888888888888905, 0.8589318600368323, 1.0, 1.1338978028465985, 0.1832908360963813, 1.0, 1.0, 0.013315268686679657], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1651816], dtype=float32), 0.97052455]. 
=============================================
[2019-04-01 20:26:46,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:26:46,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:26:46,367] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run37
[2019-04-01 20:26:46,457] A3C_AGENT_WORKER-Thread-3 INFO:Local step 305000, global step 4883922: loss 1.0739
[2019-04-01 20:26:46,460] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 305000, global step 4883924: learning rate 0.0010
[2019-04-01 20:26:46,656] A3C_AGENT_WORKER-Thread-11 INFO:Local step 305000, global step 4884023: loss 0.4472
[2019-04-01 20:26:46,659] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 305000, global step 4884024: learning rate 0.0010
[2019-04-01 20:26:46,975] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:26:46,975] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:26:46,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run37
[2019-04-01 20:26:47,118] A3C_AGENT_WORKER-Thread-5 INFO:Local step 305000, global step 4884237: loss 0.1040
[2019-04-01 20:26:47,120] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 305000, global step 4884238: learning rate 0.0010
[2019-04-01 20:26:48,007] A3C_AGENT_WORKER-Thread-12 INFO:Local step 305000, global step 4884666: loss 0.1370
[2019-04-01 20:26:48,009] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 305000, global step 4884666: learning rate 0.0010
[2019-04-01 20:26:48,135] A3C_AGENT_WORKER-Thread-13 INFO:Local step 305000, global step 4884729: loss 0.1763
[2019-04-01 20:26:48,138] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 305000, global step 4884729: learning rate 0.0010
[2019-04-01 20:26:50,003] A3C_AGENT_WORKER-Thread-19 INFO:Local step 305000, global step 4885556: loss 0.0504
[2019-04-01 20:26:50,005] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 305000, global step 4885557: learning rate 0.0010
[2019-04-01 20:26:50,588] A3C_AGENT_WORKER-Thread-2 INFO:Local step 305500, global step 4885817: loss 0.0031
[2019-04-01 20:26:50,590] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 305500, global step 4885817: learning rate 0.0010
[2019-04-01 20:26:51,770] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:26:51,770] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9667
[2019-04-01 20:26:51,783] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 39.33333333333334, 0.0, 0.0, 26.0, 25.45020434075141, 8.70069105474073, 0.0, 1.0, 39.5952694997845], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4911000.0000, 
sim time next is 4911600.0000, 
raw observation next is [1.0, 38.66666666666667, 0.0, 0.0, 26.0, 25.52327586591574, 8.739177511822186, 0.0, 1.0, 32.50140544174755], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.3866666666666667, 0.0, 0.0, 1.0, 0.931896552273677, 0.08739177511822187, 0.0, 1.0, 0.2321528960124825], 
reward next is 0.7678, 
noisyNet noise sample is [array([1.151273], dtype=float32), -1.3657125]. 
=============================================
[2019-04-01 20:26:52,264] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:26:52,264] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6603
[2019-04-01 20:26:52,292] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.666666666666667, 38.0, 208.0, 597.0, 26.0, 25.17017318713927, 9.54243279613567, 0.0, 1.0, 8.475705880985982], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4801200.0000, 
sim time next is 4801800.0000, 
raw observation next is [2.833333333333333, 37.5, 196.0, 626.0, 26.0, 25.16492068639561, 9.561300631235042, 0.0, 1.0, 7.968355626002595], 
processed observation next is [0.0, 0.5652173913043478, 0.541089566020314, 0.375, 0.6533333333333333, 0.6917127071823205, 1.0, 0.880702955199373, 0.09561300631235042, 0.0, 1.0, 0.056916825900018536], 
reward next is 0.9431, 
noisyNet noise sample is [array([-0.17455386], dtype=float32), 0.53820163]. 
=============================================
[2019-04-01 20:26:54,782] A3C_AGENT_WORKER-Thread-20 INFO:Local step 305500, global step 4887896: loss 0.0082
[2019-04-01 20:26:54,783] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 305500, global step 4887896: learning rate 0.0010
[2019-04-01 20:26:55,534] A3C_AGENT_WORKER-Thread-10 INFO:Local step 305500, global step 4888219: loss 0.0115
[2019-04-01 20:26:55,535] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 305500, global step 4888219: learning rate 0.0010
[2019-04-01 20:26:55,694] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.3806046e-36
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:26:55,697] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8638
[2019-04-01 20:26:55,724] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.1, 92.5, 0.0, 0.0, 26.0, 24.00000993675024, 5.506583286547439, 0.0, 1.0, 41.23072737452511], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4775400.0000, 
sim time next is 4776000.0000, 
raw observation next is [-6.133333333333333, 92.66666666666667, 0.0, 0.0, 26.0, 23.96271217812433, 5.471134882126876, 0.0, 1.0, 41.27593037510738], 
processed observation next is [0.0, 0.2608695652173913, 0.2927054478301016, 0.9266666666666667, 0.0, 0.0, 1.0, 0.7089588825891902, 0.054711348821268754, 0.0, 1.0, 0.29482807410790984], 
reward next is 0.7052, 
noisyNet noise sample is [array([0.4486324], dtype=float32), 1.4972247]. 
=============================================
[2019-04-01 20:26:55,740] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[68.442764]
 [68.533646]
 [68.62518 ]
 [68.7387  ]
 [68.83732 ]], R is [[68.37059021]
 [68.39237976]
 [68.41431427]
 [68.43641663]
 [68.4587326 ]].
[2019-04-01 20:26:56,042] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:26:56,044] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6160
[2019-04-01 20:26:56,059] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 25.83855012480947, 12.49757376757167, 0.0, 1.0, 21.1743108446148], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5004000.0000, 
sim time next is 5004600.0000, 
raw observation next is [3.0, 36.5, 0.0, 0.0, 26.0, 25.85933858173377, 11.08577455031652, 0.0, 1.0, 21.96923669534098], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.365, 0.0, 0.0, 1.0, 0.9799055116762528, 0.1108577455031652, 0.0, 1.0, 0.15692311925243557], 
reward next is 0.8431, 
noisyNet noise sample is [array([0.4746335], dtype=float32), -2.93875]. 
=============================================
[2019-04-01 20:26:56,469] A3C_AGENT_WORKER-Thread-16 INFO:Local step 305500, global step 4888707: loss 0.1803
[2019-04-01 20:26:56,470] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 305500, global step 4888707: learning rate 0.0010
[2019-04-01 20:26:57,163] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.1179384e-37
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:26:57,165] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1662
[2019-04-01 20:26:57,220] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 82.0, 34.0, 0.0, 26.0, 24.54236648236881, 6.886367776517396, 0.0, 1.0, 41.32812363406113], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 57600.0000, 
sim time next is 58200.0000, 
raw observation next is [6.416666666666667, 82.66666666666667, 28.66666666666666, 0.0, 26.0, 24.62330758056298, 6.974910784442943, 0.0, 1.0, 32.27639897667802], 
processed observation next is [0.0, 0.6956521739130435, 0.6403508771929826, 0.8266666666666667, 0.09555555555555553, 0.0, 1.0, 0.8033296543661399, 0.06974910784442943, 0.0, 1.0, 0.23054570697627155], 
reward next is 0.7695, 
noisyNet noise sample is [array([-0.42919758], dtype=float32), -1.097857]. 
=============================================
[2019-04-01 20:26:58,239] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:26:58,240] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5439
[2019-04-01 20:26:58,280] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 41.5, 0.0, 0.0, 26.0, 24.96996077295175, 8.034665670503726, 0.0, 1.0, 28.30251059263643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4818600.0000, 
sim time next is 4819200.0000, 
raw observation next is [1.333333333333333, 42.0, 0.0, 0.0, 26.0, 24.99675743707355, 8.054114155474386, 0.0, 1.0, 25.88271031840758], 
processed observation next is [0.0, 0.782608695652174, 0.4995383194829178, 0.42, 0.0, 0.0, 1.0, 0.8566796338676502, 0.08054114155474386, 0.0, 1.0, 0.18487650227433985], 
reward next is 0.8151, 
noisyNet noise sample is [array([0.25294396], dtype=float32), 0.6332693]. 
=============================================
[2019-04-01 20:26:58,676] A3C_AGENT_WORKER-Thread-15 INFO:Local step 305500, global step 4889786: loss 0.3399
[2019-04-01 20:26:58,679] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 305500, global step 4889786: learning rate 0.0010
[2019-04-01 20:26:59,999] A3C_AGENT_WORKER-Thread-9 INFO:Local step 305500, global step 4890512: loss 0.2934
[2019-04-01 20:27:00,001] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 305500, global step 4890512: learning rate 0.0010
[2019-04-01 20:27:00,473] A3C_AGENT_WORKER-Thread-14 INFO:Local step 305500, global step 4890771: loss 0.3138
[2019-04-01 20:27:00,479] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 305500, global step 4890771: learning rate 0.0010
[2019-04-01 20:27:01,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:27:01,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:01,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run37
[2019-04-01 20:27:01,743] A3C_AGENT_WORKER-Thread-3 INFO:Local step 305500, global step 4891383: loss 0.5128
[2019-04-01 20:27:01,744] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 305500, global step 4891383: learning rate 0.0010
[2019-04-01 20:27:01,850] A3C_AGENT_WORKER-Thread-11 INFO:Local step 305500, global step 4891433: loss 0.5491
[2019-04-01 20:27:01,851] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 305500, global step 4891434: learning rate 0.0010
[2019-04-01 20:27:02,282] A3C_AGENT_WORKER-Thread-5 INFO:Local step 305500, global step 4891625: loss 0.2712
[2019-04-01 20:27:02,285] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 305500, global step 4891625: learning rate 0.0010
[2019-04-01 20:27:03,072] A3C_AGENT_WORKER-Thread-12 INFO:Local step 305500, global step 4891984: loss 0.5824
[2019-04-01 20:27:03,073] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 305500, global step 4891984: learning rate 0.0010
[2019-04-01 20:27:03,439] A3C_AGENT_WORKER-Thread-13 INFO:Local step 305500, global step 4892156: loss 0.5166
[2019-04-01 20:27:03,440] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 305500, global step 4892156: learning rate 0.0010
[2019-04-01 20:27:04,679] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.03430276e-19
 4.42729103e-10 1.00000000e+00 0.00000000e+00], sum to 1.0000
[2019-04-01 20:27:04,680] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4871
[2019-04-01 20:27:04,719] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 26.0, 24.92625871930041, 7.524466314996846, 0.0, 1.0, 45.65723960629556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 160200.0000, 
sim time next is 160800.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 26.0, 24.80369730171635, 7.25146325903701, 0.0, 1.0, 45.456721165121], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 1.0, 0.8290996145309073, 0.0725146325903701, 0.0, 1.0, 0.32469086546515], 
reward next is 0.6753, 
noisyNet noise sample is [array([-0.74323106], dtype=float32), 0.66166043]. 
=============================================
[2019-04-01 20:27:04,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:27:04,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:04,812] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run37
[2019-04-01 20:27:05,383] A3C_AGENT_WORKER-Thread-19 INFO:Local step 305500, global step 4893105: loss 0.0895
[2019-04-01 20:27:05,384] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 305500, global step 4893105: learning rate 0.0010
[2019-04-01 20:27:05,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:27:05,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:05,663] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run37
[2019-04-01 20:27:06,963] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:27:06,964] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:06,990] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run37
[2019-04-01 20:27:09,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:27:09,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:09,224] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run37
[2019-04-01 20:27:10,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:27:10,195] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:10,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run37
[2019-04-01 20:27:10,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:27:10,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:10,831] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run37
[2019-04-01 20:27:12,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:27:12,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:12,295] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run37
[2019-04-01 20:27:12,414] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:27:12,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:12,422] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run37
[2019-04-01 20:27:12,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:27:12,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:12,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run37
[2019-04-01 20:27:13,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:27:13,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:13,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run37
[2019-04-01 20:27:13,512] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:27:13,512] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:13,528] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run37
[2019-04-01 20:27:16,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-01 20:27:16,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:16,350] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run37
[2019-04-01 20:27:16,743] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:27:16,743] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5409
[2019-04-01 20:27:16,779] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.03278446228394, 13.54411371464521, 0.0, 1.0, 40.19115259201804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 17400.0000, 
sim time next is 18000.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.04982897861431, 13.3853479876934, 0.0, 1.0, 40.11806911466478], 
processed observation next is [0.0, 0.21739130434782608, 0.6759002770083103, 0.93, 0.0, 0.0, 1.0, 0.2928327112306159, 0.133853479876934, 0.0, 1.0, 0.28655763653331984], 
reward next is 0.7134, 
noisyNet noise sample is [array([0.45675194], dtype=float32), -0.76449114]. 
=============================================
[2019-04-01 20:27:16,808] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.50324]
 [78.62116]
 [78.71554]
 [78.79535]
 [78.76978]], R is [[78.37850952]
 [78.30764771]
 [78.23693848]
 [78.16640472]
 [78.09604645]].
[2019-04-01 20:27:22,842] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:27:22,842] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5751
[2019-04-01 20:27:22,882] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.2, 95.0, 0.0, 0.0, 26.0, 24.37956400701728, 6.045451936137748, 0.0, 1.0, 39.56130207948951], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 84000.0000, 
sim time next is 84600.0000, 
raw observation next is [0.15, 95.0, 0.0, 0.0, 26.0, 24.36545766694717, 6.007781739899968, 0.0, 1.0, 39.56108909557437], 
processed observation next is [0.0, 1.0, 0.46675900277008314, 0.95, 0.0, 0.0, 1.0, 0.7664939524210241, 0.060077817398999686, 0.0, 1.0, 0.2825792078255312], 
reward next is 0.7174, 
noisyNet noise sample is [array([0.24725519], dtype=float32), 0.8864864]. 
=============================================
[2019-04-01 20:27:25,912] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.1250033e-36 2.8284522e-36 1.2322572e-04 1.2745520e-22
 9.9987674e-01 0.0000000e+00], sum to 1.0000
[2019-04-01 20:27:25,912] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2949
[2019-04-01 20:27:25,936] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 60.5, 0.0, 0.0, 26.0, 24.84476795750942, 7.245690397253408, 0.0, 1.0, 43.9580409156595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 766200.0000, 
sim time next is 766800.0000, 
raw observation next is [-5.6, 61.0, 0.0, 0.0, 26.0, 24.84003114148233, 7.163310464587113, 0.0, 1.0, 43.7742292400151], 
processed observation next is [1.0, 0.9130434782608695, 0.30747922437673136, 0.61, 0.0, 0.0, 1.0, 0.8342901630689045, 0.07163310464587112, 0.0, 1.0, 0.31267306600010786], 
reward next is 0.6873, 
noisyNet noise sample is [array([0.05843133], dtype=float32), 1.1962998]. 
=============================================
[2019-04-01 20:27:29,149] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-01 20:27:29,150] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 20:27:29,150] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:29,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run50
[2019-04-01 20:27:29,189] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 20:27:29,190] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:29,190] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 20:27:29,191] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:27:29,195] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run50
[2019-04-01 20:27:29,216] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run50
[2019-04-01 20:29:10,556] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 20:29:30,631] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 20:29:32,719] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 20:29:33,743] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 4900000, evaluation results [4900000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
[2019-04-01 20:29:41,610] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 7.7738308e-28 2.2261491e-31 3.3394969e-26 9.9999988e-01
 1.2554924e-07 1.7867270e-33], sum to 1.0000
[2019-04-01 20:29:41,610] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7724
[2019-04-01 20:29:41,657] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 62.0, 133.0, 0.0, 26.0, 25.67492112226966, 8.397986984154116, 1.0, 1.0, 30.16812135404705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 223200.0000, 
sim time next is 223800.0000, 
raw observation next is [-3.3, 61.5, 127.6666666666667, 0.0, 26.0, 25.86430322723675, 8.553433514550315, 1.0, 1.0, 29.29969794315895], 
processed observation next is [1.0, 0.6086956521739131, 0.37119113573407203, 0.615, 0.4255555555555557, 0.0, 1.0, 0.9806147467481071, 0.08553433514550315, 1.0, 1.0, 0.20928355673684965], 
reward next is 0.7907, 
noisyNet noise sample is [array([-0.14397791], dtype=float32), 1.1746728]. 
=============================================
[2019-04-01 20:29:56,183] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 2.9402586e-25], sum to 1.0000
[2019-04-01 20:29:56,183] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4017
[2019-04-01 20:29:56,279] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3, 36.0, 94.0, 0.0, 26.0, 24.4877849694556, 6.446879372153641, 1.0, 1.0, 82.62836705100347], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 484200.0000, 
sim time next is 484800.0000, 
raw observation next is [-0.2, 36.33333333333333, 87.66666666666667, 0.0, 26.0, 25.07171671778681, 7.482174684791801, 1.0, 1.0, 45.77327878335446], 
processed observation next is [1.0, 0.6086956521739131, 0.4570637119113574, 0.3633333333333333, 0.2922222222222222, 0.0, 1.0, 0.8673881025409729, 0.07482174684791801, 1.0, 1.0, 0.3269519913096747], 
reward next is 0.6730, 
noisyNet noise sample is [array([0.39805403], dtype=float32), 0.33352217]. 
=============================================
[2019-04-01 20:30:03,907] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:30:03,907] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0074
[2019-04-01 20:30:03,922] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 79.00000000000001, 0.0, 0.0, 26.0, 24.66633960380449, 6.148627580973295, 0.0, 1.0, 38.93853409515807], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 871800.0000, 
sim time next is 872400.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 26.0, 24.72056056662414, 6.100170442932274, 0.0, 1.0, 38.91034085333249], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.79, 0.0, 0.0, 1.0, 0.8172229380891629, 0.06100170442932274, 0.0, 1.0, 0.2779310060952321], 
reward next is 0.7221, 
noisyNet noise sample is [array([0.7325902], dtype=float32), -0.22793007]. 
=============================================
[2019-04-01 20:30:19,512] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.4967654e-20 1.0000000e+00], sum to 1.0000
[2019-04-01 20:30:19,513] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5223
[2019-04-01 20:30:19,531] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 45.0, 84.5, 743.5, 26.0, 25.74526709949405, 9.18182442169167, 1.0, 1.0, 13.30387734824577], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 741600.0000, 
sim time next is 742200.0000, 
raw observation next is [0.4166666666666667, 45.33333333333334, 83.0, 733.6666666666667, 26.0, 25.77035973915287, 9.363208191846281, 1.0, 1.0, 10.66405923305469], 
processed observation next is [1.0, 0.6086956521739131, 0.47414589104339805, 0.4533333333333334, 0.27666666666666667, 0.810681399631676, 1.0, 0.9671942484504099, 0.09363208191846281, 1.0, 1.0, 0.07617185166467635], 
reward next is 0.9238, 
noisyNet noise sample is [array([0.18376723], dtype=float32), 2.2738392]. 
=============================================
[2019-04-01 20:30:26,137] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:30:26,143] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0402
[2019-04-01 20:30:26,162] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.56268616290413, 6.188434915197597, 0.0, 1.0, 39.42944627666317], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 865200.0000, 
sim time next is 865800.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.58070049190673, 6.246367070055366, 0.0, 1.0, 39.29256114681203], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 1.0, 0.7972429274152469, 0.06246367070055366, 0.0, 1.0, 0.28066115104865735], 
reward next is 0.7193, 
noisyNet noise sample is [array([1.5487827], dtype=float32), 0.42242408]. 
=============================================
[2019-04-01 20:30:30,499] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:30:30,500] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1135
[2019-04-01 20:30:30,512] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.266666666666667, 92.0, 0.0, 0.0, 26.0, 25.5115147585411, 9.861910196912147, 0.0, 1.0, 31.73396469173804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1462800.0000, 
sim time next is 1463400.0000, 
raw observation next is [1.35, 92.0, 0.0, 0.0, 26.0, 25.33199578958276, 9.49769000762028, 0.0, 1.0, 32.32004415635335], 
processed observation next is [1.0, 0.9565217391304348, 0.5000000000000001, 0.92, 0.0, 0.0, 1.0, 0.9045708270832513, 0.0949769000762028, 0.0, 1.0, 0.23085745825966678], 
reward next is 0.7691, 
noisyNet noise sample is [array([0.11643225], dtype=float32), -1.014992]. 
=============================================
[2019-04-01 20:30:33,981] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:30:33,985] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2582
[2019-04-01 20:30:34,001] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.59975049990361, 6.255603528707762, 0.0, 1.0, 39.67202216467308], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 864000.0000, 
sim time next is 864600.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.58609093833697, 6.204894776248914, 0.0, 1.0, 39.54788306605298], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 1.0, 0.7980129911909957, 0.06204894776248914, 0.0, 1.0, 0.2824848790432356], 
reward next is 0.7175, 
noisyNet noise sample is [array([0.5074079], dtype=float32), -2.62562]. 
=============================================
[2019-04-01 20:30:34,212] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:30:34,215] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9983
[2019-04-01 20:30:34,249] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 80.5, 0.0, 0.0, 26.0, 25.35252764401098, 9.110113921786057, 0.0, 1.0, 38.138374906934], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 961800.0000, 
sim time next is 962400.0000, 
raw observation next is [7.7, 81.0, 0.0, 0.0, 26.0, 25.32734403762004, 9.598487309881214, 0.0, 1.0, 40.28824518752797], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.81, 0.0, 0.0, 1.0, 0.9039062910885772, 0.09598487309881215, 0.0, 1.0, 0.28777317991091406], 
reward next is 0.7122, 
noisyNet noise sample is [array([0.4579909], dtype=float32), -0.8161259]. 
=============================================
[2019-04-01 20:30:35,580] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:30:35,581] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2941
[2019-04-01 20:30:35,599] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.800000000000001, 81.33333333333334, 0.0, 0.0, 26.0, 25.44012242574742, 10.83905603340847, 0.0, 1.0, 36.35869958628901], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1563600.0000, 
sim time next is 1564200.0000, 
raw observation next is [4.7, 82.5, 0.0, 0.0, 26.0, 25.44795662141733, 11.67193220572951, 0.0, 1.0, 37.53016046114102], 
processed observation next is [1.0, 0.08695652173913043, 0.592797783933518, 0.825, 0.0, 0.0, 1.0, 0.9211366602024756, 0.1167193220572951, 0.0, 1.0, 0.2680725747224359], 
reward next is 0.7319, 
noisyNet noise sample is [array([-0.02731192], dtype=float32), 0.842746]. 
=============================================
[2019-04-01 20:30:36,327] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:30:36,331] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7714
[2019-04-01 20:30:36,342] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.516666666666667, 69.33333333333334, 143.3333333333333, 120.0, 26.0, 26.3367952098745, 13.72251121203739, 1.0, 1.0, 9.641559822675697], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1590600.0000, 
sim time next is 1591200.0000, 
raw observation next is [7.7, 68.0, 157.5, 112.0, 26.0, 26.41060472107306, 14.17147857440405, 1.0, 1.0, 8.971444672572918], 
processed observation next is [1.0, 0.43478260869565216, 0.6759002770083103, 0.68, 0.525, 0.12375690607734807, 1.0, 1.0586578172961514, 0.1417147857440405, 1.0, 1.0, 0.06408174766123513], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.18438219], dtype=float32), 0.938271]. 
=============================================
[2019-04-01 20:30:37,202] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7928997e-21 2.3792732e-29
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:30:37,205] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1420
[2019-04-01 20:30:37,216] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 78.33333333333333, 0.0, 0.0, 26.0, 25.26727898075556, 9.775459569298881, 1.0, 1.0, 3.730146490255271], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1021200.0000, 
sim time next is 1021800.0000, 
raw observation next is [14.4, 77.66666666666667, 0.0, 0.0, 26.0, 25.17168599027751, 9.511549044560903, 1.0, 1.0, 3.627895312672541], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.7766666666666667, 0.0, 0.0, 1.0, 0.8816694271825014, 0.09511549044560903, 1.0, 1.0, 0.025913537947661006], 
reward next is 0.9741, 
noisyNet noise sample is [array([0.13767664], dtype=float32), -0.5200855]. 
=============================================
[2019-04-01 20:30:39,381] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:30:39,382] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1557
[2019-04-01 20:30:39,390] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.0, 77.66666666666667, 0.0, 0.0, 26.0, 25.85191466351545, 12.64289068949347, 0.0, 1.0, 11.07122570134835], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1045200.0000, 
sim time next is 1045800.0000, 
raw observation next is [14.1, 77.5, 0.0, 0.0, 26.0, 25.8398021582544, 12.12933960021952, 0.0, 1.0, 11.48347973642323], 
processed observation next is [1.0, 0.08695652173913043, 0.8531855955678671, 0.775, 0.0, 0.0, 1.0, 0.9771145940363429, 0.1212933960021952, 0.0, 1.0, 0.08202485526016594], 
reward next is 0.9180, 
noisyNet noise sample is [array([1.406604], dtype=float32), 0.52783704]. 
=============================================
[2019-04-01 20:30:40,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:30:40,980] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6743
[2019-04-01 20:30:40,984] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.8, 63.0, 54.5, 0.0, 26.0, 25.0361213766416, 10.23161493569107, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1180800.0000, 
sim time next is 1181400.0000, 
raw observation next is [18.71666666666667, 63.33333333333333, 46.0, 0.0, 26.0, 25.02124560653599, 10.14165097571785, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.981071098799631, 0.6333333333333333, 0.15333333333333332, 0.0, 1.0, 0.860177943790856, 0.1014165097571785, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3797259], dtype=float32), 0.1926144]. 
=============================================
[2019-04-01 20:30:42,563] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:30:42,567] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8114
[2019-04-01 20:30:42,610] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.7000000000000001, 92.0, 22.5, 0.0, 26.0, 25.91255352815045, 12.39673298153035, 1.0, 1.0, 20.31293535168966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1327200.0000, 
sim time next is 1327800.0000, 
raw observation next is [0.6, 92.0, 27.0, 0.0, 26.0, 25.98277141412553, 12.38998587624965, 1.0, 1.0, 19.30785549614247], 
processed observation next is [1.0, 0.34782608695652173, 0.479224376731302, 0.92, 0.09, 0.0, 1.0, 0.9975387734465043, 0.1238998587624965, 1.0, 1.0, 0.1379132535438748], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8109784], dtype=float32), -0.53495806]. 
=============================================
[2019-04-01 20:30:48,524] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:30:48,524] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2362
[2019-04-01 20:30:48,616] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.9, 93.5, 0.0, 0.0, 26.0, 25.6048846939154, 12.82687947466856, 0.0, 1.0, 28.27859047919288], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1299000.0000, 
sim time next is 1299600.0000, 
raw observation next is [3.8, 93.0, 0.0, 0.0, 26.0, 25.62583443289872, 12.79333454829679, 0.0, 1.0, 27.28546854877408], 
processed observation next is [1.0, 0.043478260869565216, 0.5678670360110805, 0.93, 0.0, 0.0, 1.0, 0.9465477761283887, 0.12793334548296792, 0.0, 1.0, 0.19489620391981485], 
reward next is 0.8051, 
noisyNet noise sample is [array([-0.37733433], dtype=float32), 0.7730151]. 
=============================================
[2019-04-01 20:30:49,564] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:30:49,565] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0062
[2019-04-01 20:30:49,573] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 118.6666666666667, 0.0, 26.0, 26.09894155835899, 12.50954955983956, 1.0, 1.0, 9.430820659804219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1336200.0000, 
sim time next is 1336800.0000, 
raw observation next is [1.1, 92.0, 122.8333333333333, 0.0, 26.0, 26.10195437917863, 12.50928202241411, 1.0, 1.0, 9.09557443524254], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.40944444444444433, 0.0, 1.0, 1.0145649113112327, 0.12509282022414112, 1.0, 1.0, 0.064968388823161], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.260055], dtype=float32), 0.80215067]. 
=============================================
[2019-04-01 20:30:57,584] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.00000000e+00 0.00000000e+00 1.47209789e-37 0.00000000e+00
 1.23516184e-20 1.00000000e+00 2.62737409e-29], sum to 1.0000
[2019-04-01 20:30:57,588] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1037
[2019-04-01 20:30:57,620] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:30:57,620] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1376
[2019-04-01 20:30:57,627] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.93333333333334, 52.00000000000001, 54.66666666666667, 30.83333333333334, 26.0, 27.40803862214772, 20.77923748918133, 1.0, 1.0, 6.227139930172107], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1614000.0000, 
sim time next is 1614600.0000, 
raw observation next is [12.75, 52.5, 50.0, 37.0, 26.0, 27.45489519396633, 21.0424421459053, 1.0, 1.0, 6.325788109962442], 
processed observation next is [1.0, 0.6956521739130435, 0.8157894736842106, 0.525, 0.16666666666666666, 0.04088397790055249, 1.0, 1.2078421705666185, 0.21042442145905302, 1.0, 1.0, 0.045184200785446015], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7869538], dtype=float32), -0.6116415]. 
=============================================
[2019-04-01 20:30:57,638] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 95.66666666666666, 0.0, 0.0, 26.0, 25.39075449316588, 9.42037332915037, 0.0, 1.0, 35.07063676081918], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1482600.0000, 
sim time next is 1483200.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.34324769283486, 9.257061462498859, 0.0, 1.0, 34.8525117983822], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 1.0, 0.9061782418335517, 0.09257061462498858, 0.0, 1.0, 0.24894651284558716], 
reward next is 0.7511, 
noisyNet noise sample is [array([0.23128277], dtype=float32), -1.0982103]. 
=============================================
[2019-04-01 20:31:05,746] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:31:05,749] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3588
[2019-04-01 20:31:05,762] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.9, 81.0, 0.0, 0.0, 26.0, 25.97034218810947, 15.87066988071206, 0.0, 1.0, 22.48070701825701], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1632600.0000, 
sim time next is 1633200.0000, 
raw observation next is [6.8, 82.66666666666666, 0.0, 0.0, 26.0, 25.99983190326754, 15.7325284733632, 0.0, 1.0, 21.29642710592107], 
processed observation next is [1.0, 0.9130434782608695, 0.6509695290858727, 0.8266666666666665, 0.0, 0.0, 1.0, 0.9999759861810773, 0.157325284733632, 0.0, 1.0, 0.15211733647086478], 
reward next is 0.8479, 
noisyNet noise sample is [array([-0.21851504], dtype=float32), 0.16806646]. 
=============================================
[2019-04-01 20:31:28,064] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00
 1.0000000e+00 1.5619381e-36], sum to 1.0000
[2019-04-01 20:31:28,064] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3938
[2019-04-01 20:31:28,116] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.683333333333333, 76.33333333333333, 150.0, 0.0, 26.0, 25.75822474326576, 8.220687101253782, 1.0, 1.0, 21.79094947343461], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2029800.0000, 
sim time next is 2030400.0000, 
raw observation next is [-4.5, 75.0, 151.5, 0.0, 26.0, 25.77785247376814, 8.211924059592455, 1.0, 1.0, 20.97121947730898], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.75, 0.505, 0.0, 1.0, 0.9682646391097341, 0.08211924059592456, 1.0, 1.0, 0.1497944248379213], 
reward next is 0.8502, 
noisyNet noise sample is [array([0.6913514], dtype=float32), -0.11705442]. 
=============================================
[2019-04-01 20:31:35,131] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:31:35,131] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9807
[2019-04-01 20:31:35,183] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.633333333333333, 81.0, 89.0, 50.5, 26.0, 25.62759149698555, 7.783929858055781, 1.0, 1.0, 34.37624459512733], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2104800.0000, 
sim time next is 2105400.0000, 
raw observation next is [-7.716666666666667, 81.5, 106.0, 63.99999999999999, 26.0, 25.53567017658392, 7.851696294114653, 1.0, 1.0, 29.83550500227565], 
processed observation next is [1.0, 0.34782608695652173, 0.24884579870729456, 0.815, 0.35333333333333333, 0.07071823204419889, 1.0, 0.9336671680834172, 0.07851696294114653, 1.0, 1.0, 0.21311075001625465], 
reward next is 0.7869, 
noisyNet noise sample is [array([1.1753386], dtype=float32), 1.5624735]. 
=============================================
[2019-04-01 20:31:37,400] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:31:37,402] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9277
[2019-04-01 20:31:37,436] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.199999999999999, 80.83333333333334, 0.0, 0.0, 26.0, 24.20086335245191, 5.677547441626906, 0.0, 1.0, 43.403148596077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2253000.0000, 
sim time next is 2253600.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 26.0, 24.18966831299251, 5.615181233193105, 0.0, 1.0, 43.39655333523998], 
processed observation next is [1.0, 0.08695652173913043, 0.26038781163434904, 0.82, 0.0, 0.0, 1.0, 0.7413811875703585, 0.05615181233193105, 0.0, 1.0, 0.30997538096599986], 
reward next is 0.6900, 
noisyNet noise sample is [array([1.799827], dtype=float32), 1.5185587]. 
=============================================
[2019-04-01 20:31:38,639] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:31:38,640] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5408
[2019-04-01 20:31:38,689] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 68.0, 157.0, 285.0, 26.0, 26.20044926640728, 11.35802240038731, 1.0, 1.0, 26.95770389874279], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2215800.0000, 
sim time next is 2216400.0000, 
raw observation next is [-3.899999999999999, 68.0, 136.6666666666667, 237.5, 26.0, 26.35498621817547, 11.58698718063721, 1.0, 1.0, 25.50980018237231], 
processed observation next is [1.0, 0.6521739130434783, 0.35457063711911363, 0.68, 0.4555555555555557, 0.26243093922651933, 1.0, 1.05071231688221, 0.11586987180637211, 1.0, 1.0, 0.1822128584455165], 
reward next is 0.1830, 
noisyNet noise sample is [array([-2.1159465], dtype=float32), 0.3003837]. 
=============================================
[2019-04-01 20:32:02,671] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:32:02,680] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2503
[2019-04-01 20:32:02,706] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.9544350258131, 6.460837861210424, 0.0, 1.0, 41.0833574848764], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2599800.0000, 
sim time next is 2600400.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.91660358560698, 6.364601096777387, 0.0, 1.0, 41.12650663995286], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 1.0, 0.8452290836581398, 0.06364601096777388, 0.0, 1.0, 0.293760761713949], 
reward next is 0.7062, 
noisyNet noise sample is [array([0.28158888], dtype=float32), 0.46386433]. 
=============================================
[2019-04-01 20:32:19,006] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:32:19,007] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3660
[2019-04-01 20:32:19,031] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 25.08451677213648, 7.060998046043814, 0.0, 1.0, 54.64935855206352], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2865600.0000, 
sim time next is 2866200.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 25.08388386208017, 7.038692326379952, 0.0, 1.0, 54.63371767828805], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 1.0, 0.869126266011453, 0.07038692326379951, 0.0, 1.0, 0.3902408405592004], 
reward next is 0.6098, 
noisyNet noise sample is [array([-0.03927437], dtype=float32), 0.8855652]. 
=============================================
[2019-04-01 20:32:20,967] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:32:20,967] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9838
[2019-04-01 20:32:21,019] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 0.0, 0.0, 26.0, 25.32164375522205, 10.53260177455357, 1.0, 1.0, 37.98369572092799], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3223200.0000, 
sim time next is 3223800.0000, 
raw observation next is [-3.0, 92.0, 1.0, 82.0, 26.0, 25.63455177533934, 10.41453877809171, 1.0, 1.0, 33.59490974576318], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.0033333333333333335, 0.09060773480662983, 1.0, 0.9477931107627627, 0.1041453877809171, 1.0, 1.0, 0.23996364104116558], 
reward next is 0.5942, 
noisyNet noise sample is [array([0.90902185], dtype=float32), -1.5421722]. 
=============================================
[2019-04-01 20:32:21,058] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:32:21,058] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5743
[2019-04-01 20:32:21,088] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 52.0, 3.0, 53.0, 26.0, 25.71472646972244, 9.003585646240309, 1.0, 1.0, 24.03356571855798], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2741400.0000, 
sim time next is 2742000.0000, 
raw observation next is [-3.666666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 25.26260139603923, 8.988300136611814, 1.0, 1.0, 51.1123631494136], 
processed observation next is [1.0, 0.7391304347826086, 0.3610341643582641, 0.5266666666666667, 0.0, 0.0, 1.0, 0.8946573422913185, 0.08988300136611814, 1.0, 1.0, 0.3650883082100972], 
reward next is 0.6349, 
noisyNet noise sample is [array([0.25715128], dtype=float32), -1.9655186]. 
=============================================
[2019-04-01 20:32:21,101] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[74.112755]
 [73.80695 ]
 [73.465   ]
 [73.24122 ]
 [72.88206 ]], R is [[73.91447449]
 [74.00366211]
 [74.09047699]
 [73.96543884]
 [73.84644318]].
[2019-04-01 20:32:22,825] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:32:22,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2718
[2019-04-01 20:32:22,842] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.75037583995759, 6.286136408813843, 0.0, 1.0, 40.87390823308423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2773200.0000, 
sim time next is 2773800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.68557196403264, 6.243269257969547, 0.0, 1.0, 40.79320753853023], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 1.0, 0.8122245662903771, 0.06243269257969547, 0.0, 1.0, 0.2913800538466445], 
reward next is 0.7086, 
noisyNet noise sample is [array([0.65058804], dtype=float32), -0.49820718]. 
=============================================
[2019-04-01 20:32:25,837] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:32:25,845] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0328
[2019-04-01 20:32:25,866] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.833333333333334, 25.0, 110.3333333333333, 0.0, 26.0, 25.51325010994763, 8.167705468586194, 1.0, 1.0, 17.36735922210761], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2818200.0000, 
sim time next is 2818800.0000, 
raw observation next is [7.0, 24.0, 106.5, 0.0, 26.0, 24.85707433767242, 7.510673910614287, 1.0, 1.0, 33.31214271670618], 
processed observation next is [1.0, 0.6521739130434783, 0.6565096952908588, 0.24, 0.355, 0.0, 1.0, 0.8367249053817741, 0.07510673910614286, 1.0, 1.0, 0.23794387654790125], 
reward next is 0.7621, 
noisyNet noise sample is [array([-0.3780603], dtype=float32), -1.0708688]. 
=============================================
[2019-04-01 20:32:29,043] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:32:29,044] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9459
[2019-04-01 20:32:29,060] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.6666666666666666, 95.33333333333334, 79.5, 0.0, 26.0, 25.49628556883407, 7.603489066466838, 1.0, 1.0, 21.04020584217096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2889600.0000, 
sim time next is 2890200.0000, 
raw observation next is [0.8333333333333334, 94.16666666666666, 81.0, 0.0, 26.0, 25.51253679385616, 7.637861168553954, 1.0, 1.0, 20.04091856321218], 
processed observation next is [1.0, 0.43478260869565216, 0.4856879039704525, 0.9416666666666665, 0.27, 0.0, 1.0, 0.9303623991223086, 0.07637861168553954, 1.0, 1.0, 0.14314941830865843], 
reward next is 0.8569, 
noisyNet noise sample is [array([-0.84615797], dtype=float32), 0.5137943]. 
=============================================
[2019-04-01 20:32:29,408] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:32:29,408] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2422
[2019-04-01 20:32:29,438] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.44156439751496, 5.77383772615308, 0.0, 1.0, 38.16510561838438], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3031200.0000, 
sim time next is 3031800.0000, 
raw observation next is [-5.166666666666667, 72.0, 0.0, 0.0, 26.0, 24.4361778589665, 5.733154256028705, 0.0, 1.0, 38.23787245460367], 
processed observation next is [0.0, 0.08695652173913043, 0.31948291782086796, 0.72, 0.0, 0.0, 1.0, 0.7765968369952141, 0.057331542560287045, 0.0, 1.0, 0.2731276603900262], 
reward next is 0.7269, 
noisyNet noise sample is [array([1.4849644], dtype=float32), -1.049945]. 
=============================================
[2019-04-01 20:32:29,723] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:32:29,724] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2888
[2019-04-01 20:32:29,746] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.35861592791335, 6.144109115617489, 0.0, 1.0, 42.41455747560742], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2953800.0000, 
sim time next is 2954400.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.32848704666375, 6.088109207435264, 0.0, 1.0, 42.40199974745733], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.84, 0.0, 0.0, 1.0, 0.7612124352376785, 0.06088109207435264, 0.0, 1.0, 0.30287142676755235], 
reward next is 0.6971, 
noisyNet noise sample is [array([-0.4785186], dtype=float32), -0.33614656]. 
=============================================
[2019-04-01 20:32:43,948] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:32:43,948] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8031
[2019-04-01 20:32:43,965] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 100.0, 28.33333333333333, 185.3333333333333, 26.0, 25.53088493771507, 7.685483358171195, 1.0, 1.0, 29.63791320149877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3138600.0000, 
sim time next is 3139200.0000, 
raw observation next is [6.0, 100.0, 42.0, 237.0, 26.0, 25.46700889199392, 7.961636193382937, 1.0, 1.0, 29.87442547092686], 
processed observation next is [1.0, 0.34782608695652173, 0.6288088642659281, 1.0, 0.14, 0.261878453038674, 1.0, 0.9238584131419886, 0.07961636193382937, 1.0, 1.0, 0.21338875336376328], 
reward next is 0.7866, 
noisyNet noise sample is [array([-0.31178525], dtype=float32), 0.04253566]. 
=============================================
[2019-04-01 20:32:44,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4400435e-38 1.2976923e-34 2.3003802e-34 0.0000000e+00 0.0000000e+00
 1.0000000e+00 7.1333170e-35], sum to 1.0000
[2019-04-01 20:32:44,887] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8441
[2019-04-01 20:32:44,936] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.0, 77.0, 95.0, 505.5, 26.0, 26.02829915880316, 10.9842712430714, 1.0, 1.0, 29.53635501569706], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3315600.0000, 
sim time next is 3316200.0000, 
raw observation next is [-8.833333333333334, 75.83333333333334, 98.0, 542.0, 26.0, 26.1228236470014, 11.29349959836567, 1.0, 1.0, 27.65642584688783], 
processed observation next is [1.0, 0.391304347826087, 0.21791320406278855, 0.7583333333333334, 0.32666666666666666, 0.5988950276243094, 1.0, 1.0175462352859144, 0.11293499598365671, 1.0, 1.0, 0.19754589890634167], 
reward next is 0.2851, 
noisyNet noise sample is [array([-0.7847068], dtype=float32), -0.045519076]. 
=============================================
[2019-04-01 20:32:46,848] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 0. 0. 1.], sum to 1.0000
[2019-04-01 20:32:46,852] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6034
[2019-04-01 20:32:46,870] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.2, 75.5, 113.0, 811.0, 26.0, 26.44941285339799, 16.2106422474353, 1.0, 1.0, 7.540294804108392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3238200.0000, 
sim time next is 3238800.0000, 
raw observation next is [-2.133333333333333, 74.0, 113.3333333333333, 813.0, 26.0, 26.47001457417421, 16.26038730401768, 1.0, 1.0, 7.278406741869941], 
processed observation next is [1.0, 0.4782608695652174, 0.4035087719298246, 0.74, 0.37777777777777766, 0.8983425414364641, 1.0, 1.0671449391677446, 0.1626038730401768, 1.0, 1.0, 0.05198861958478529], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.97577244], dtype=float32), -1.0040879]. 
=============================================
[2019-04-01 20:32:53,271] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.7431271e-34 0.0000000e+00
 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-04-01 20:32:53,271] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1960
[2019-04-01 20:32:53,316] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.58652133610077, 11.97726975365232, 1.0, 1.0, 28.89352255696009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3267600.0000, 
sim time next is 3268200.0000, 
raw observation next is [-4.0, 70.0, 0.0, 0.0, 26.0, 25.53155241384079, 11.68965363604071, 1.0, 1.0, 28.91708130048885], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.7, 0.0, 0.0, 1.0, 0.9330789162629698, 0.1168965363604071, 1.0, 1.0, 0.2065505807177775], 
reward next is 0.1176, 
noisyNet noise sample is [array([0.6980997], dtype=float32), 0.61474127]. 
=============================================
[2019-04-01 20:32:54,594] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.4646300e-25 0.0000000e+00
 1.0000000e+00 2.4202824e-38], sum to 1.0000
[2019-04-01 20:32:54,595] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4765
[2019-04-01 20:32:54,601] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 26.1643686830575, 12.92694727053931, 1.0, 1.0, 9.21876275476289], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3522000.0000, 
sim time next is 3522600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 26.0738950977003, 12.37100807176965, 1.0, 1.0, 9.374457823432392], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 1.0, 1.0105564425286144, 0.1237100807176965, 1.0, 1.0, 0.0669604130245171], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.29175353], dtype=float32), 1.8559192]. 
=============================================
[2019-04-01 20:33:02,454] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00
 2.7601983e-34 0.0000000e+00], sum to 1.0000
[2019-04-01 20:33:02,455] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3768
[2019-04-01 20:33:02,465] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 60.0, 102.8333333333333, 765.1666666666667, 26.0, 26.75343576884396, 15.78782581142887, 1.0, 1.0, 8.361338819034364], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3768000.0000, 
sim time next is 3768600.0000, 
raw observation next is [0.0, 60.0, 99.66666666666666, 754.3333333333333, 26.0, 26.8127551177883, 16.14126796620554, 1.0, 1.0, 8.389366456808741], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.6, 0.3322222222222222, 0.8335174953959483, 1.0, 1.116107873969757, 0.16141267966205539, 1.0, 1.0, 0.059924046120062434], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7131722], dtype=float32), 0.11690876]. 
=============================================
[2019-04-01 20:33:02,863] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:33:02,864] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2737
[2019-04-01 20:33:02,883] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 59.0, 39.5, 343.5, 26.0, 25.48299213701049, 9.563965701712826, 0.0, 1.0, 5.718754639269803], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3690000.0000, 
sim time next is 3690600.0000, 
raw observation next is [4.0, 59.0, 31.33333333333333, 284.0, 26.0, 25.45214055686549, 9.301507162630635, 0.0, 1.0, 5.10836758766352], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.10444444444444442, 0.3138121546961326, 1.0, 0.9217343652664987, 0.09301507162630634, 0.0, 1.0, 0.03648833991188229], 
reward next is 0.9635, 
noisyNet noise sample is [array([-0.5543653], dtype=float32), -0.25940695]. 
=============================================
[2019-04-01 20:33:07,488] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0. 0. 0. 0. 0. 1. 0.], sum to 1.0000
[2019-04-01 20:33:07,491] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6402
[2019-04-01 20:33:07,507] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 39.5, 0.0, 0.0, 26.0, 25.41718616263391, 11.62661326377215, 0.0, 1.0, 51.11416114015652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4134600.0000, 
sim time next is 4135200.0000, 
raw observation next is [1.0, 38.33333333333334, 0.0, 0.0, 26.0, 25.51698059385349, 12.34374648932506, 0.0, 1.0, 36.88517012921086], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.3833333333333334, 0.0, 0.0, 1.0, 0.9309972276933556, 0.1234374648932506, 0.0, 1.0, 0.26346550092293475], 
reward next is 0.7365, 
noisyNet noise sample is [array([-0.04307264], dtype=float32), -0.5268862]. 
=============================================
[2019-04-01 20:33:15,296] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.7486740e-16 1.5475331e-11 6.4481881e-08 9.8235738e-01 1.7618535e-02
 2.3943101e-05 1.6436231e-35], sum to 1.0000
[2019-04-01 20:33:15,298] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8947
[2019-04-01 20:33:15,311] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 26.0, 25.87595738043543, 13.12575820233713, 1.0, 1.0, 14.3711577605303], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3780600.0000, 
sim time next is 3781200.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 26.0, 26.08764877721906, 10.63670839478923, 1.0, 1.0, 14.06935079222941], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 1.0, 1.012521253888437, 0.1063670839478923, 1.0, 1.0, 0.10049536280163864], 
reward next is 0.6448, 
noisyNet noise sample is [array([0.5448383], dtype=float32), -0.15784736]. 
=============================================
[2019-04-01 20:33:17,315] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-01 20:33:17,315] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-01 20:33:17,316] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:33:17,316] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-01 20:33:17,317] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:33:17,317] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-01 20:33:17,317] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-01 20:33:17,753] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run51
[2019-04-01 20:33:17,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run51
[2019-04-01 20:33:18,194] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/1/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run51
[2019-04-01 20:33:35,885] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.2833581], dtype=float32), -1.32581]
[2019-04-01 20:33:35,885] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-9.218574615666666, 40.94133568, 0.0, 0.0, 26.0, 25.27505398980423, 7.95131317712341, 1.0, 1.0, 48.27730665941363]
[2019-04-01 20:33:35,885] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 20:33:35,887] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.00000000e+00 2.18159598e-36 0.00000000e+00 4.49658960e-14
 1.02869243e-25 1.00000000e+00 6.05046752e-17], sampled 0.5643145054960219
[2019-04-01 20:33:38,843] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.2833581], dtype=float32), -1.32581]
[2019-04-01 20:33:38,843] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.094897829, 95.02164383, 0.0, 0.0, 26.0, 24.69216432645149, 6.28234936258567, 1.0, 1.0, 42.1701559006822]
[2019-04-01 20:33:38,844] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 20:33:38,844] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 6.7316199e-35 8.4235535e-36
 1.0000000e+00 0.0000000e+00], sampled 0.8487158964458733
[2019-04-01 20:35:00,498] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5517.8156 233538.7105 39811.3890
[2019-04-01 20:35:09,270] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.2833581], dtype=float32), -1.32581]
[2019-04-01 20:35:09,270] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.297098432, 74.42066915, 0.0, 0.0, 26.0, 25.34880204325035, 8.25785253071463, 0.0, 1.0, 40.15766475143123]
[2019-04-01 20:35:09,271] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 20:35:09,271] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0. 0. 0. 0. 0. 1. 0.], sampled 0.06340153596003772
[2019-04-01 20:35:18,151] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5386.1372 256763.9510 36460.3398
[2019-04-01 20:35:21,152] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([1.2833581], dtype=float32), -1.32581]
[2019-04-01 20:35:21,153] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [9.238555471, 17.03478249, 32.57599344, 159.4985828, 26.0, 27.25052923810402, 18.00457543055062, 1.0, 1.0, 2.626918246360423]
[2019-04-01 20:35:21,153] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-01 20:35:21,154] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 5.6156376e-23 7.3219079e-31
 1.0000000e+00 3.1092353e-22], sampled 0.3860028548992426
[2019-04-01 20:35:22,619] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5740.8359 267616.4533 29513.1047
[2019-04-01 20:35:23,644] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 5000000, evaluation results [5000000.0, 5386.137212821335, 256763.95095060606, 36460.339777514426, 5517.815641687992, 233538.7104574513, 39811.38900943811, 5740.835928419105, 267616.4533325891, 29513.10471680222]
