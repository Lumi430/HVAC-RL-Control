Using TensorFlow backend.
[2019-04-07 10:47:55,965] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v2', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-Repeat-v1', eval_act_func='part4_v3', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=20000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=5e-06, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2000000, metric_func='part4_v1', model_dir='None', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-Repeat-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_heuri_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=0.5, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=10, test_env=['Part4-Light-Pit-Test-Repeat-v1', 'Part4-Light-Pit-Test-Repeat-v2'], test_mode='Multiple', train_act_func='part4_v3', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=10.0, weight_initer='glorot_uniform', window_len=7)
[2019-04-07 10:47:55,965] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-07 10:47:56.003470: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-07 10:48:12,049] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-07 10:48:12,049] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-Repeat-v1', 'Part4-Light-Pit-Test-Repeat-v1', 'Part4-Light-Pit-Test-Repeat-v2'] ...
[2019-04-07 10:48:12,073] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation worker starts!
[2019-04-07 10:48:12,097] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation worker starts!
[2019-04-07 10:48:12,121] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation worker starts!
[2019-04-07 10:48:12,121] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:12,122] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-07 10:48:12,197] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:12,199] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run1
[2019-04-07 10:48:13,123] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:13,125] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-07 10:48:13,199] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:13,200] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run1
[2019-04-07 10:48:14,126] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:14,127] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-07 10:48:14,205] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:14,206] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run1
[2019-04-07 10:48:15,128] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:15,129] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-07 10:48:15,209] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:15,210] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run1
[2019-04-07 10:48:16,130] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:16,131] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-07 10:48:16,239] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:16,241] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run1
[2019-04-07 10:48:16,374] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-07 10:48:16,375] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 10:48:16,375] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:16,375] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 10:48:16,376] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 10:48:16,376] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:16,376] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:16,379] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run1
[2019-04-07 10:48:16,393] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run1
[2019-04-07 10:48:16,393] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run1
[2019-04-07 10:48:17,132] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:17,133] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-07 10:48:17,299] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:17,301] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run1
[2019-04-07 10:48:18,133] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:18,134] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-07 10:48:18,287] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:18,289] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run1
[2019-04-07 10:48:19,135] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:19,136] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-07 10:48:19,252] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:19,254] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run1
[2019-04-07 10:48:20,137] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:20,138] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-07 10:48:20,266] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:20,268] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run1
[2019-04-07 10:48:21,138] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:21,139] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-07 10:48:21,325] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:21,327] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run1
[2019-04-07 10:48:22,140] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:22,141] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-07 10:48:22,219] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:22,220] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run1
[2019-04-07 10:48:23,142] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:23,143] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-07 10:48:23,248] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:23,254] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run1
[2019-04-07 10:48:24,143] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:24,144] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-07 10:48:24,322] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:24,324] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run1
[2019-04-07 10:48:25,145] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:25,146] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-07 10:48:25,274] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:25,276] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run1
[2019-04-07 10:48:26,147] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:26,150] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-07 10:48:26,330] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:26,332] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run1
[2019-04-07 10:48:27,150] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-07 10:48:27,151] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-07 10:48:27,344] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:48:27,345] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run1
[2019-04-07 10:50:04,033] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2585.7257 70526270.1862 300.9053
[2019-04-07 10:50:20,093] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2556.8237 79426137.4059 240.9558
[2019-04-07 10:50:26,539] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2545.6357 82792564.2513 181.0022
[2019-04-07 10:50:27,563] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 2556.823749712911, 79426137.4058814, 240.9557992473907, 2585.725652290566, 70526270.18618171, 300.9052990885012, 2545.6357263389746, 82792564.25132345, 181.00220636639153]
[2019-04-07 10:51:22,233] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.09647579 0.08618375 0.12434031 0.13436456 0.0919436  0.22522771
 0.15983984 0.08162447], sum to 1.0000
[2019-04-07 10:51:22,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8773
[2019-04-07 10:51:22,734] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.65, 86.5, 0.0, 0.0, 22.0, 23.26604176118445, -0.140959137679921, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 606600.0000, 
sim time next is 608400.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 25.0, 23.16266453600552, -0.146520655765554, 0.0, 1.0, 76818.64878955779], 
processed observation next is [0.0, 0.043478260869565216, 0.3545706371191136, 0.86, 0.0, 0.0, 0.5833333333333334, 0.43022204466712655, 0.451159781411482, 0.0, 1.0, 0.3658030894740847], 
reward next is 0.7771, 
noisyNet noise sample is [array([0.44585583], dtype=float32), 0.48097378]. 
=============================================
[2019-04-07 10:51:44,917] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0876965  0.06756476 0.10273019 0.13600205 0.11312745 0.25473338
 0.14664084 0.09150486], sum to 1.0000
[2019-04-07 10:51:44,917] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4628
[2019-04-07 10:51:45,413] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.8, 71.0, 0.0, 0.0, 22.0, 22.40486688496165, -0.3220250751417267, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 784800.0000, 
sim time next is 786600.0000, 
raw observation next is [-7.8, 72.5, 0.0, 0.0, 23.0, 22.17907674983105, -0.2973297365959576, 0.0, 1.0, 157245.21709287842], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.725, 0.0, 0.0, 0.4166666666666667, 0.3482563958192542, 0.4008900878013475, 0.0, 1.0, 0.7487867480613258], 
reward next is 0.6798, 
noisyNet noise sample is [array([0.49473327], dtype=float32), -0.9088628]. 
=============================================
[2019-04-07 10:51:53,590] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7612: loss 37.6283
[2019-04-07 10:51:53,777] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7612: learning rate 0.0000
[2019-04-07 10:51:54,312] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7657: loss 34.1821
[2019-04-07 10:51:54,313] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7657: learning rate 0.0000
[2019-04-07 10:51:54,639] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7680: loss 37.2608
[2019-04-07 10:51:54,639] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7680: learning rate 0.0000
[2019-04-07 10:51:56,196] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7783: loss 42.1976
[2019-04-07 10:51:56,196] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7783: learning rate 0.0000
[2019-04-07 10:51:56,981] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 7840: loss 43.5161
[2019-04-07 10:51:56,981] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 7840: learning rate 0.0000
[2019-04-07 10:51:57,535] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 7879: loss 41.8181
[2019-04-07 10:51:57,536] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 7879: learning rate 0.0000
[2019-04-07 10:51:57,932] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 7914: loss 45.1625
[2019-04-07 10:51:57,932] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 7914: learning rate 0.0000
[2019-04-07 10:51:58,401] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7949: loss 43.5590
[2019-04-07 10:51:58,401] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7949: learning rate 0.0000
[2019-04-07 10:51:59,282] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8032: loss 45.3909
[2019-04-07 10:51:59,283] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8032: learning rate 0.0000
[2019-04-07 10:52:00,017] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8104: loss 44.2735
[2019-04-07 10:52:00,018] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8104: learning rate 0.0000
[2019-04-07 10:52:00,130] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8114: loss 44.8420
[2019-04-07 10:52:00,134] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8114: learning rate 0.0000
[2019-04-07 10:52:00,843] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8196: loss 35.4327
[2019-04-07 10:52:00,844] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8196: learning rate 0.0000
[2019-04-07 10:52:01,330] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8244: loss 45.3846
[2019-04-07 10:52:01,330] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8244: learning rate 0.0000
[2019-04-07 10:52:02,247] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 8337: loss 39.5527
[2019-04-07 10:52:02,247] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 8337: learning rate 0.0000
[2019-04-07 10:52:02,611] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8378: loss 30.7871
[2019-04-07 10:52:02,611] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8378: learning rate 0.0000
[2019-04-07 10:52:04,567] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 8575: loss 42.1373
[2019-04-07 10:52:04,567] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 8575: learning rate 0.0000
[2019-04-07 10:52:04,823] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.07693145 0.06606204 0.08088741 0.1450957  0.11679191 0.25867245
 0.1448061  0.11075289], sum to 1.0000
[2019-04-07 10:52:04,823] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0212
[2019-04-07 10:52:05,371] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [4.4, 93.0, 54.0, 0.0, 21.0, 24.71466112847135, 0.1710639444678193, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 919800.0000, 
sim time next is 921600.0000, 
raw observation next is [4.4, 93.0, 36.0, 0.0, 19.0, 24.68490786074498, 0.1746720656739606, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5844875346260389, 0.93, 0.12, 0.0, 0.08333333333333333, 0.5570756550620816, 0.5582240218913203, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48679012], dtype=float32), -1.3822577]. 
=============================================
[2019-04-07 10:52:16,765] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.09992027 0.09226446 0.12756869 0.15980305 0.098121   0.19970156
 0.14102946 0.08159152], sum to 1.0000
[2019-04-07 10:52:16,766] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2973
[2019-04-07 10:52:17,641] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [18.8, 63.0, 165.5, 0.0, 22.0, 23.87304804849263, 0.2054376364022716, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 1166400.0000, 
sim time next is 1168200.0000, 
raw observation next is [18.55, 64.0, 171.0, 0.0, 21.0, 23.89153529613753, 0.2147951716094825, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.976454293628809, 0.64, 0.57, 0.0, 0.25, 0.4909612746781275, 0.5715983905364942, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17774233], dtype=float32), -1.2110636]. 
=============================================
[2019-04-07 10:52:21,925] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.06801034 0.05020697 0.08178863 0.17527053 0.10288729 0.26016724
 0.12126245 0.14040652], sum to 1.0000
[2019-04-07 10:52:21,933] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9563
[2019-04-07 10:52:21,951] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [14.4, 58.5, 0.0, 0.0, 24.0, 25.64051081720585, 0.5996240210608742, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1107000.0000, 
sim time next is 1108800.0000, 
raw observation next is [13.8, 60.0, 0.0, 0.0, 26.0, 25.38396572973788, 0.5579628540616081, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.844875346260388, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6153304774781567, 0.685987618020536, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23116353], dtype=float32), -1.661802]. 
=============================================
[2019-04-07 10:52:45,984] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04954525 0.05588041 0.06636505 0.16795057 0.07470158 0.3682515
 0.14198385 0.0753218 ], sum to 1.0000
[2019-04-07 10:52:45,984] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0520
[2019-04-07 10:52:47,111] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 100.0, 32.0, 0.0, 25.0, 24.54068812636643, 0.1885860884030767, 1.0, 1.0, 5199.4197329344], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1414800.0000, 
sim time next is 1416600.0000, 
raw observation next is [-0.3, 97.5, 46.0, 0.0, 20.0, 24.65465513928746, 0.1988196663459671, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4542936288088643, 0.975, 0.15333333333333332, 0.0, 0.16666666666666666, 0.5545545949406216, 0.5662732221153224, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.868055], dtype=float32), -0.97522664]. 
=============================================
[2019-04-07 10:53:07,243] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15480: loss 36.3119
[2019-04-07 10:53:07,293] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15480: learning rate 0.0000
[2019-04-07 10:53:08,351] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15611: loss 34.1929
[2019-04-07 10:53:08,352] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15611: learning rate 0.0000
[2019-04-07 10:53:08,396] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 15618: loss 33.8343
[2019-04-07 10:53:08,397] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 15618: learning rate 0.0000
[2019-04-07 10:53:10,388] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 15888: loss 29.2625
[2019-04-07 10:53:10,388] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 15888: learning rate 0.0000
[2019-04-07 10:53:10,583] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 15906: loss 35.8232
[2019-04-07 10:53:10,584] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 15906: learning rate 0.0000
[2019-04-07 10:53:11,249] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 15970: loss 30.4003
[2019-04-07 10:53:11,249] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 15970: learning rate 0.0000
[2019-04-07 10:53:11,567] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 16003: loss 26.7813
[2019-04-07 10:53:11,568] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 16003: learning rate 0.0000
[2019-04-07 10:53:12,045] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16054: loss 31.0304
[2019-04-07 10:53:12,045] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16054: learning rate 0.0000
[2019-04-07 10:53:12,325] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16089: loss 26.5054
[2019-04-07 10:53:12,325] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16089: learning rate 0.0000
[2019-04-07 10:53:13,426] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16210: loss 26.5632
[2019-04-07 10:53:13,427] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16210: learning rate 0.0000
[2019-04-07 10:53:13,569] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16226: loss 27.7244
[2019-04-07 10:53:13,569] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16226: learning rate 0.0000
[2019-04-07 10:53:14,179] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 16293: loss 39.4888
[2019-04-07 10:53:14,180] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 16293: learning rate 0.0000
[2019-04-07 10:53:14,213] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16297: loss 33.7914
[2019-04-07 10:53:14,214] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16297: learning rate 0.0000
[2019-04-07 10:53:14,734] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 16356: loss 30.0535
[2019-04-07 10:53:14,734] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 16356: learning rate 0.0000
[2019-04-07 10:53:15,039] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 16392: loss 37.6288
[2019-04-07 10:53:15,039] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 16392: learning rate 0.0000
[2019-04-07 10:53:16,066] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0957137  0.06722224 0.08569667 0.18992028 0.10296848 0.2383937
 0.1512358  0.06884907], sum to 1.0000
[2019-04-07 10:53:16,067] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2722
[2019-04-07 10:53:16,307] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 24.0, 22.35357762313771, -0.3324414917010032, 0.0, 1.0, 48060.250756625544], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1839600.0000, 
sim time next is 1841400.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 22.0, 22.23827925641485, -0.3759979263003854, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.3333333333333333, 0.3531899380345707, 0.37466735789987154, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6829147], dtype=float32), 0.027064146]. 
=============================================
[2019-04-07 10:53:17,191] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 16601: loss 29.6845
[2019-04-07 10:53:17,191] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 16601: learning rate 0.0000
[2019-04-07 10:53:19,339] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.07655237 0.07188551 0.07645568 0.17533618 0.0787738  0.2592081
 0.19273417 0.06905414], sum to 1.0000
[2019-04-07 10:53:19,340] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0755
[2019-04-07 10:53:19,605] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-7.3, 80.5, 0.0, 0.0, 24.0, 22.81637596408511, -0.2413542912936218, 0.0, 1.0, 86762.81836774928], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1899000.0000, 
sim time next is 1900800.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 19.0, 22.82249801249957, -0.2700901778063131, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.08333333333333333, 0.4018748343749641, 0.40996994073122894, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.088607], dtype=float32), 1.0854149]. 
=============================================
[2019-04-07 10:53:29,553] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.04147897 0.04043779 0.04528907 0.15426551 0.08084617 0.43359208
 0.12309307 0.08099733], sum to 1.0000
[2019-04-07 10:53:29,553] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8976
[2019-04-07 10:53:29,790] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 85.5, 0.0, 24.0, 24.97292367283844, 0.1036539252514784, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2023200.0000, 
sim time next is 2025000.0000, 
raw observation next is [-5.6, 83.0, 102.0, 0.0, 25.0, 24.81215312551899, 0.08388504404119884, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.30747922437673136, 0.83, 0.34, 0.0, 0.5833333333333334, 0.5676794271265825, 0.5279616813470663, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.91153115], dtype=float32), 0.11854743]. 
=============================================
[2019-04-07 10:53:29,805] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[2.3362048]
 [2.6441705]
 [2.2170088]
 [2.524237 ]
 [2.2526426]], R is [[3.35988808]
 [4.32628918]
 [5.28302622]
 [6.230196  ]
 [6.67357254]].
[2019-04-07 10:53:56,837] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-07 10:53:56,837] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 10:53:56,837] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:53:56,839] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run2
[2019-04-07 10:53:56,871] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 10:53:56,878] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:53:56,880] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 10:53:56,880] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:53:56,884] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run2
[2019-04-07 10:53:56,903] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run2
[2019-04-07 10:55:21,974] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.01415936], dtype=float32), 0.01582615]
[2019-04-07 10:55:21,974] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [0.85, 84.0, 190.0, 187.0, 25.0, 24.69386670973225, 0.1743286247108039, 1.0, 1.0, 0.0]
[2019-04-07 10:55:21,974] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 10:55:21,974] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [0.05548556 0.05193445 0.0542656  0.18142425 0.06543509 0.3537176
 0.15847214 0.0792653 ], sampled 0.6516634757220353
[2019-04-07 10:55:36,050] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.01415936], dtype=float32), 0.01582615]
[2019-04-07 10:55:36,050] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-6.0, 59.0, 0.0, 0.0, 24.0, 23.36040527689469, -0.1182674535266819, 0.0, 1.0, 42537.355571869215]
[2019-04-07 10:55:36,050] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 10:55:36,051] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [0.07267509 0.05980332 0.06884883 0.19167799 0.09751766 0.29351524
 0.13973828 0.0762236 ], sampled 0.5986898263531651
[2019-04-07 10:55:57,255] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2605.9363 70602114.6948 343.3720
[2019-04-07 10:56:17,038] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2580.3506 79903538.1128 260.8468
[2019-04-07 10:56:18,477] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2570.1400 83429321.0053 178.8187
[2019-04-07 10:56:19,499] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 20000, evaluation results [20000.0, 2580.3506396012335, 79903538.11283994, 260.8467625601103, 2605.936277990664, 70602114.69483033, 343.37200467790103, 2570.140036565932, 83429321.00527398, 178.81873765729415]
[2019-04-07 10:56:20,514] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.03017011 0.03323726 0.04872807 0.19863334 0.0651616  0.41050375
 0.1566174  0.05694849], sum to 1.0000
[2019-04-07 10:56:20,514] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1588
[2019-04-07 10:56:20,701] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 24.0, 22.37989458234685, -0.2541557366346321, 0.0, 1.0, 117996.4116522357], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 2251800.0000, 
sim time next is 2253600.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 22.0, 22.48432701088525, -0.2774749510053158, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.26038781163434904, 0.82, 0.0, 0.0, 0.3333333333333333, 0.37369391757377074, 0.40750834966489474, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.79163563], dtype=float32), 0.73262316]. 
=============================================
[2019-04-07 10:56:40,706] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 22980: loss 37.1146
[2019-04-07 10:56:40,707] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 22980: learning rate 0.0000
[2019-04-07 10:56:41,099] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23041: loss 38.1676
[2019-04-07 10:56:41,108] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23042: learning rate 0.0000
[2019-04-07 10:56:43,694] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 23488: loss 32.6942
[2019-04-07 10:56:43,695] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 23488: learning rate 0.0000
[2019-04-07 10:56:44,015] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 23532: loss 31.5075
[2019-04-07 10:56:44,015] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 23532: learning rate 0.0000
[2019-04-07 10:56:44,108] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 23544: loss 32.9350
[2019-04-07 10:56:44,109] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 23544: learning rate 0.0000
[2019-04-07 10:56:45,532] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23710: loss 36.5792
[2019-04-07 10:56:45,533] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23710: learning rate 0.0000
[2019-04-07 10:56:46,988] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23941: loss 30.2457
[2019-04-07 10:56:46,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23941: learning rate 0.0000
[2019-04-07 10:56:47,429] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24013: loss 31.1412
[2019-04-07 10:56:47,430] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24013: learning rate 0.0000
[2019-04-07 10:56:48,029] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 24096: loss 27.4863
[2019-04-07 10:56:48,030] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 24096: learning rate 0.0000
[2019-04-07 10:56:48,451] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24167: loss 34.2887
[2019-04-07 10:56:48,452] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24167: learning rate 0.0000
[2019-04-07 10:56:48,589] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 24188: loss 33.8818
[2019-04-07 10:56:48,596] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 24190: learning rate 0.0000
[2019-04-07 10:56:48,792] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 24220: loss 25.6131
[2019-04-07 10:56:48,793] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 24220: learning rate 0.0000
[2019-04-07 10:56:48,904] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 24238: loss 22.2877
[2019-04-07 10:56:48,905] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 24238: learning rate 0.0000
[2019-04-07 10:56:48,953] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 24243: loss 30.7072
[2019-04-07 10:56:48,954] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 24243: learning rate 0.0000
[2019-04-07 10:56:49,336] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24297: loss 26.4163
[2019-04-07 10:56:49,349] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24297: learning rate 0.0000
[2019-04-07 10:56:51,945] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24621: loss 32.8483
[2019-04-07 10:56:51,946] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24621: learning rate 0.0000
[2019-04-07 10:57:01,713] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.01284407 0.0153679  0.03498076 0.21295078 0.04439738 0.5289955
 0.10068581 0.0497777 ], sum to 1.0000
[2019-04-07 10:57:01,713] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5376
[2019-04-07 10:57:01,797] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 22.0, 23.92488232882954, 0.003513825192445877, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2858400.0000, 
sim time next is 2860200.0000, 
raw observation next is [1.0, 82.5, 0.0, 0.0, 24.0, 23.72169329414874, -0.05054472366099039, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.825, 0.0, 0.0, 0.5, 0.4768077745123949, 0.48315175877966987, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0789282], dtype=float32), -0.12832002]. 
=============================================
[2019-04-07 10:57:36,007] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00966466 0.01350383 0.01965005 0.21678439 0.04281183 0.5630199
 0.09555176 0.03901362], sum to 1.0000
[2019-04-07 10:57:36,008] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8865
[2019-04-07 10:57:36,703] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 77.0, 0.0, 0.0, 24.0, 23.41644848276525, -0.02175513282994645, 0.0, 1.0, 41286.673610995254], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 3297600.0000, 
sim time next is 3299400.0000, 
raw observation next is [-9.45, 76.5, 0.0, 0.0, 25.0, 23.42044129015328, -0.02584798626273609, 0.0, 1.0, 52820.98324579838], 
processed observation next is [1.0, 0.17391304347826086, 0.20083102493074795, 0.765, 0.0, 0.0, 0.5833333333333334, 0.45170344084610675, 0.491384004579088, 0.0, 1.0, 0.25152849164665897], 
reward next is 0.8913, 
noisyNet noise sample is [array([-0.22525331], dtype=float32), -0.4494468]. 
=============================================
[2019-04-07 10:57:43,458] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 30558: loss 30.8108
[2019-04-07 10:57:43,459] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 30558: learning rate 0.0000
[2019-04-07 10:57:44,250] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 30650: loss 23.5004
[2019-04-07 10:57:44,251] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 30650: learning rate 0.0000
[2019-04-07 10:57:50,936] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 31368: loss 25.6196
[2019-04-07 10:57:50,936] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 31368: learning rate 0.0000
[2019-04-07 10:57:52,106] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 31499: loss 45.9834
[2019-04-07 10:57:52,106] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 31499: learning rate 0.0000
[2019-04-07 10:57:52,961] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 31581: loss 31.7913
[2019-04-07 10:57:52,964] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 31581: learning rate 0.0000
[2019-04-07 10:57:55,254] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31821: loss 29.9658
[2019-04-07 10:57:55,255] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31821: learning rate 0.0000
[2019-04-07 10:57:56,621] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31978: loss 37.0944
[2019-04-07 10:57:56,622] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31978: learning rate 0.0000
[2019-04-07 10:57:59,302] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32275: loss 37.8525
[2019-04-07 10:57:59,320] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32275: learning rate 0.0000
[2019-04-07 10:58:01,277] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32471: loss 39.7789
[2019-04-07 10:58:01,289] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32471: learning rate 0.0000
[2019-04-07 10:58:01,890] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32530: loss 25.2378
[2019-04-07 10:58:01,902] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32530: learning rate 0.0000
[2019-04-07 10:58:03,504] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 32703: loss 30.5299
[2019-04-07 10:58:03,504] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 32703: learning rate 0.0000
[2019-04-07 10:58:03,612] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32717: loss 29.3195
[2019-04-07 10:58:03,612] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32717: learning rate 0.0000
[2019-04-07 10:58:04,516] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 32818: loss 37.8193
[2019-04-07 10:58:04,523] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 32818: learning rate 0.0000
[2019-04-07 10:58:05,209] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32891: loss 46.4983
[2019-04-07 10:58:05,245] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32891: learning rate 0.0000
[2019-04-07 10:58:06,568] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 33052: loss 28.0284
[2019-04-07 10:58:06,576] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 33052: learning rate 0.0000
[2019-04-07 10:58:11,172] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 33557: loss 37.7733
[2019-04-07 10:58:11,172] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 33557: learning rate 0.0000
[2019-04-07 10:58:35,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.00293479 0.00714108 0.0052928  0.13500501 0.01635431 0.73359746
 0.05068558 0.04898899], sum to 1.0000
[2019-04-07 10:58:35,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2485
[2019-04-07 10:58:36,047] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 49.0, 119.5, 822.5, 24.0, 25.03602876117846, 0.2791667933396298, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3931200.0000, 
sim time next is 3933000.0000, 
raw observation next is [-6.0, 47.0, 119.0, 835.0, 24.0, 23.71896642238993, 0.1255955147927243, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.47, 0.39666666666666667, 0.9226519337016574, 0.5, 0.47658053519916077, 0.5418651715975747, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6889043], dtype=float32), -0.42789283]. 
=============================================
[2019-04-07 10:58:36,139] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[6.724741 ]
 [6.7012677]
 [6.65237  ]
 [6.5672   ]
 [6.555536 ]], R is [[ 7.67201138]
 [ 8.59529114]
 [ 9.50933838]
 [10.41424465]
 [11.31010246]].
[2019-04-07 10:58:43,183] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00254311 0.00538548 0.00431907 0.2150271  0.01051477 0.66504776
 0.06211867 0.03504407], sum to 1.0000
[2019-04-07 10:58:43,183] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4843
[2019-04-07 10:58:43,746] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 30.0, 121.0, 816.0, 24.0, 26.11205728043582, 0.3313967618012034, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4102200.0000, 
sim time next is 4104000.0000, 
raw observation next is [1.0, 28.0, 120.5, 828.5, 24.0, 25.93142865301884, 0.3233124880743979, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.28, 0.40166666666666667, 0.9154696132596685, 0.5, 0.6609523877515701, 0.6077708293581326, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9861804], dtype=float32), -0.09335408]. 
=============================================
[2019-04-07 10:58:43,800] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[7.3769975]
 [7.3375163]
 [7.280435 ]
 [6.9423285]
 [7.285437 ]], R is [[ 8.30881977]
 [ 9.22573185]
 [10.13347435]
 [11.03213978]
 [11.92181873]].
[2019-04-07 10:58:48,415] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00470518 0.00792596 0.00762974 0.16373181 0.01876616 0.6774709
 0.08362021 0.03615   ], sum to 1.0000
[2019-04-07 10:58:48,416] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1532
[2019-04-07 10:58:48,453] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.2, 64.0, 0.0, 0.0, 26.0, 25.06784340024414, 0.2811677145692797, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 4298400.0000, 
sim time next is 4300200.0000, 
raw observation next is [6.1, 66.5, 0.0, 0.0, 22.0, 24.88175706555677, 0.2390263483844619, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6315789473684211, 0.665, 0.0, 0.0, 0.3333333333333333, 0.5734797554630641, 0.5796754494614873, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0273945], dtype=float32), 1.0064975]. 
=============================================
[2019-04-07 10:58:53,236] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 38236: loss 17.2709
[2019-04-07 10:58:53,236] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 38236: learning rate 0.0000
[2019-04-07 10:58:53,967] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 38358: loss 11.3100
[2019-04-07 10:58:53,968] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 38358: learning rate 0.0000
[2019-04-07 10:58:58,852] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 38981: loss 11.2672
[2019-04-07 10:58:58,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 38981: learning rate 0.0000
[2019-04-07 10:58:59,181] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 39030: loss 10.6929
[2019-04-07 10:58:59,215] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 39030: learning rate 0.0000
[2019-04-07 10:59:00,558] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 39259: loss 15.0746
[2019-04-07 10:59:00,568] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 39259: learning rate 0.0000
[2019-04-07 10:59:02,935] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39577: loss 17.6928
[2019-04-07 10:59:02,936] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39577: learning rate 0.0000
[2019-04-07 10:59:04,543] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 39799: loss 15.4777
[2019-04-07 10:59:04,543] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 39799: learning rate 0.0000
[2019-04-07 10:59:06,035] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-07 10:59:06,045] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 10:59:06,045] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:59:06,053] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run3
[2019-04-07 10:59:06,095] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 10:59:06,096] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:59:06,098] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run3
[2019-04-07 10:59:06,153] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 10:59:06,154] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 10:59:06,156] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run3
[2019-04-07 11:00:53,782] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2684.0782 70216735.9458 240.8940
[2019-04-07 11:01:09,486] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2669.2332 79348474.1074 171.5224
[2019-04-07 11:01:15,027] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2670.5753 83213183.3473 100.8199
[2019-04-07 11:01:16,049] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 40000, evaluation results [40000.0, 2669.233222096624, 79348474.10740772, 171.52235056488922, 2684.0782197877224, 70216735.94583417, 240.89398342325336, 2670.575261421026, 83213183.34731027, 100.8199169280915]
[2019-04-07 11:01:16,322] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 40078: loss 17.2477
[2019-04-07 11:01:16,323] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 40078: learning rate 0.0000
[2019-04-07 11:01:16,408] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40098: loss 9.2675
[2019-04-07 11:01:16,409] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40098: learning rate 0.0000
[2019-04-07 11:01:18,480] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40590: loss 14.9468
[2019-04-07 11:01:18,481] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40590: learning rate 0.0000
[2019-04-07 11:01:18,592] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 40625: loss 13.1791
[2019-04-07 11:01:18,592] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 40625: learning rate 0.0000
[2019-04-07 11:01:18,901] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 40707: loss 18.1874
[2019-04-07 11:01:18,902] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 40707: learning rate 0.0000
[2019-04-07 11:01:18,947] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00082213 0.001188   0.00243612 0.1619158  0.00705572 0.77726215
 0.03685115 0.0124689 ], sum to 1.0000
[2019-04-07 11:01:18,947] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1433
[2019-04-07 11:01:18,959] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40723: loss 21.8734
[2019-04-07 11:01:18,965] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40723: learning rate 0.0000
[2019-04-07 11:01:19,024] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 71.0, 143.5, 340.0, 22.0, 24.24137860724387, 0.103499367794496, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 4611600.0000, 
sim time next is 4613400.0000, 
raw observation next is [-1.0, 65.5, 164.0, 509.0, 22.0, 24.3086936427739, 0.1581027184289527, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.655, 0.5466666666666666, 0.5624309392265193, 0.3333333333333333, 0.5257244702311583, 0.5527009061429843, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35130206], dtype=float32), 0.77834505]. 
=============================================
[2019-04-07 11:01:19,359] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40823: loss 8.4528
[2019-04-07 11:01:19,359] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40823: learning rate 0.0000
[2019-04-07 11:01:20,169] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 40986: loss 17.1479
[2019-04-07 11:01:20,169] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 40986: learning rate 0.0000
[2019-04-07 11:01:22,487] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 41546: loss 13.5975
[2019-04-07 11:01:22,487] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 41546: learning rate 0.0000
[2019-04-07 11:01:24,779] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0539011e-04 2.6544539e-04 5.6797848e-04 1.0069939e-01 1.4589998e-03
 8.8154709e-01 1.0309740e-02 5.0460170e-03], sum to 1.0000
[2019-04-07 11:01:24,780] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8053
[2019-04-07 11:01:24,847] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5296727e-04 1.1957173e-03 1.0835453e-03 1.4071970e-01 4.2159995e-03
 8.1544340e-01 2.4383578e-02 1.2605076e-02], sum to 1.0000
[2019-04-07 11:01:24,848] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6528
[2019-04-07 11:01:24,852] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 72.0, 171.5, 3.0, 24.0, 23.86314503959219, 0.03082717744266249, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4719600.0000, 
sim time next is 4721400.0000, 
raw observation next is [1.0, 72.0, 147.0, 0.0, 24.0, 23.91011303572822, 0.07004036222037291, 1.0, 1.0, 56834.16913394807], 
processed observation next is [1.0, 0.6521739130434783, 0.4903047091412743, 0.72, 0.49, 0.0, 0.5, 0.4925094196440183, 0.523346787406791, 1.0, 1.0, 0.27063890063784796], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6370008], dtype=float32), 1.0941913]. 
=============================================
[2019-04-07 11:01:24,888] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.7, 49.0, 171.0, 706.0, 24.0, 25.62771721726495, 0.4951653798125339, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4629600.0000, 
sim time next is 4631400.0000, 
raw observation next is [4.85, 49.5, 203.0, 599.0, 24.0, 26.14125356729776, 0.5710409192956868, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5969529085872576, 0.495, 0.6766666666666666, 0.661878453038674, 0.5, 0.6784377972748133, 0.6903469730985622, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7125922], dtype=float32), -0.4363196]. 
=============================================
[2019-04-07 11:01:25,359] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0031189  0.00535001 0.00490421 0.29914266 0.01099602 0.6085608
 0.04629267 0.02163468], sum to 1.0000
[2019-04-07 11:01:25,359] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0755
[2019-04-07 11:01:25,417] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 24.0, 23.57658735064254, -0.06577188868744649, 0.0, 1.0, 33517.90560407033], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 4917600.0000, 
sim time next is 4919400.0000, 
raw observation next is [0.5, 37.5, 0.0, 0.0, 22.0, 23.57513998796652, -0.08562603356247522, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.4764542936288089, 0.375, 0.0, 0.0, 0.3333333333333333, 0.46459499899720996, 0.4714579888125083, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6446254], dtype=float32), -0.69791627]. 
=============================================
[2019-04-07 11:01:32,291] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00080281 0.00221656 0.00312405 0.1726274  0.00478308 0.7453576
 0.05437954 0.01670907], sum to 1.0000
[2019-04-07 11:01:32,291] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4475
[2019-04-07 11:01:32,325] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 46.5, 0.0, 0.0, 22.0, 24.05390738024747, -0.02661435810478249, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4930200.0000, 
sim time next is 4932000.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 24.0, 23.85877646831695, -0.05558651698964789, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.5, 0.48823137235974584, 0.4814711610034507, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4611143], dtype=float32), 1.9353905]. 
=============================================
[2019-04-07 11:01:32,350] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[8.912727]
 [8.63739 ]
 [8.75859 ]
 [7.813029]
 [8.079077]], R is [[ 9.7107935 ]
 [10.61368561]
 [11.50754929]
 [11.97439384]
 [12.85464954]].
[2019-04-07 11:01:32,973] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:32,975] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:32,982] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run2
[2019-04-07 11:01:33,874] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:33,874] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:33,876] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run2
[2019-04-07 11:01:36,002] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.9842215e-05 2.6294595e-04 3.7973031e-04 9.5265612e-02 9.5777423e-04
 8.7333894e-01 2.4476571e-02 5.2285618e-03], sum to 1.0000
[2019-04-07 11:01:36,002] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4851
[2019-04-07 11:01:36,165] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.5, 18.0, 0.0, 0.0, 24.0, 27.02887624852502, 0.8161047450009437, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5081400.0000, 
sim time next is 5083200.0000, 
raw observation next is [10.0, 19.0, 0.0, 0.0, 24.0, 26.93755713493636, 0.7883319603434794, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.739612188365651, 0.19, 0.0, 0.0, 0.5, 0.7447964279113632, 0.7627773201144931, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45175448], dtype=float32), 0.5821559]. 
=============================================
[2019-04-07 11:01:36,769] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:36,769] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:36,771] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run2
[2019-04-07 11:01:37,473] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:37,474] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:37,475] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run2
[2019-04-07 11:01:37,652] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:37,652] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:37,654] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run2
[2019-04-07 11:01:38,809] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:38,809] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:38,811] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run2
[2019-04-07 11:01:39,489] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:39,489] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:39,491] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run2
[2019-04-07 11:01:41,120] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6161835e-04 6.1073847e-04 6.0953345e-04 7.4392684e-02 3.6255075e-03
 8.9555430e-01 1.8413631e-02 6.5319999e-03], sum to 1.0000
[2019-04-07 11:01:41,121] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2759
[2019-04-07 11:01:41,173] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 78.0, 317.0, 24.0, 23.46308318755158, -0.03221144666787439, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5040000.0000, 
sim time next is 5041800.0000, 
raw observation next is [-0.5, 56.0, 97.0, 533.0, 24.0, 23.7518230541937, 0.06751681825450877, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44875346260387816, 0.56, 0.3233333333333333, 0.5889502762430939, 0.5, 0.479318587849475, 0.5225056060848362, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15864553], dtype=float32), 0.669119]. 
=============================================
[2019-04-07 11:01:41,440] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:41,440] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:41,442] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run2
[2019-04-07 11:01:41,597] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:41,597] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:41,599] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run2
[2019-04-07 11:01:43,348] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:43,348] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:43,356] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run2
[2019-04-07 11:01:43,783] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:43,783] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:43,785] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run2
[2019-04-07 11:01:43,936] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:43,937] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:43,938] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run2
[2019-04-07 11:01:44,349] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:44,357] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:44,370] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run2
[2019-04-07 11:01:44,422] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:44,422] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:44,424] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run2
[2019-04-07 11:01:44,509] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:44,509] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:44,511] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run2
[2019-04-07 11:01:46,845] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:01:46,845] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:01:46,847] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run2
[2019-04-07 11:02:00,503] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6956157e-04 5.5905670e-04 1.2233108e-03 1.0580471e-01 1.8371805e-03
 8.4601504e-01 3.7526421e-02 6.7647058e-03], sum to 1.0000
[2019-04-07 11:02:00,503] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4097
[2019-04-07 11:02:00,551] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3, 93.0, 0.0, 0.0, 24.0, 23.29549084032065, -0.0556550964927611, 0.0, 1.0, 43024.523289429904], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 88200.0000, 
sim time next is 90000.0000, 
raw observation next is [-0.6, 91.0, 0.0, 0.0, 24.0, 23.50012831502124, -0.07388622922479207, 0.0, 1.0, 12506.12999807686], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.91, 0.0, 0.0, 0.5, 0.45834402625176995, 0.47537125692506926, 0.0, 1.0, 0.05955299999084219], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1691424], dtype=float32), -2.852054]. 
=============================================
[2019-04-07 11:02:00,555] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[11.176936]
 [10.766433]
 [10.552941]
 [10.527572]
 [10.602   ]], R is [[12.16711617]
 [13.04544544]
 [13.84354591]
 [14.70511055]
 [15.55805969]].
[2019-04-07 11:02:11,925] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1879888e-04 2.9350418e-04 8.9840038e-04 1.3066702e-01 1.3152616e-03
 8.3310586e-01 2.4240924e-02 9.3603423e-03], sum to 1.0000
[2019-04-07 11:02:11,925] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4680
[2019-04-07 11:02:12,214] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.0, 23.08859034652084, -0.2021314741041371, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 237600.0000, 
sim time next is 239400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 24.0, 23.22109045212324, -0.2142232480348826, 1.0, 1.0, 38192.29131538726], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.5, 0.4350908710102699, 0.4285922506550391, 1.0, 1.0, 0.18186805388279648], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.62714756], dtype=float32), -0.7262976]. 
=============================================
[2019-04-07 11:02:14,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.5824584e-05 2.8906221e-04 2.4759557e-04 9.2048585e-02 9.6060283e-04
 8.8414121e-01 1.5981132e-02 6.2660417e-03], sum to 1.0000
[2019-04-07 11:02:14,380] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1130
[2019-04-07 11:02:15,031] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 65.0, 141.0, 0.0, 24.0, 24.37844821039984, -0.1098823639035943, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 216000.0000, 
sim time next is 217800.0000, 
raw observation next is [-4.75, 65.0, 129.0, 0.0, 24.0, 23.66900211002072, -0.1069041982376345, 1.0, 1.0, 40709.05535826501], 
processed observation next is [1.0, 0.5217391304347826, 0.3310249307479225, 0.65, 0.43, 0.0, 0.5, 0.4724168425017267, 0.46436526725412186, 1.0, 1.0, 0.19385264456316673], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6017927], dtype=float32), 0.6095329]. 
=============================================
[2019-04-07 11:02:35,849] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.2527218e-05 3.3252852e-04 7.6036149e-04 7.2126754e-02 1.1730443e-03
 9.0543860e-01 1.3253708e-02 6.8424153e-03], sum to 1.0000
[2019-04-07 11:02:35,849] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3997
[2019-04-07 11:02:36,214] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 24.0, 23.32703417257834, -0.06928452190394699, 1.0, 1.0, 151944.2782339488], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 410400.0000, 
sim time next is 412200.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 24.0, 23.72371253131595, -0.1216956852348725, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.5, 0.47697604427632917, 0.4594347715883758, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12743133], dtype=float32), 1.1731055]. 
=============================================
[2019-04-07 11:02:42,663] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00186843 0.0032337  0.00373194 0.1652514  0.0069257  0.74504334
 0.04564758 0.02829794], sum to 1.0000
[2019-04-07 11:02:42,664] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6293
[2019-04-07 11:02:42,746] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 75.0, 0.0, 0.0, 24.0, 22.83327887395438, -0.2389866272606831, 0.0, 1.0, 77721.90406364734], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 619200.0000, 
sim time next is 621000.0000, 
raw observation next is [-4.5, 71.5, 0.0, 0.0, 24.0, 22.76384226483949, -0.2500300157266981, 0.0, 1.0, 46891.56909054392], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.715, 0.0, 0.0, 0.5, 0.39698685540329076, 0.41665666142443397, 0.0, 1.0, 0.22329318614544724], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34363958], dtype=float32), -0.59471554]. 
=============================================
[2019-04-07 11:02:42,751] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[9.226582 ]
 [9.2348995]
 [9.360519 ]
 [9.732597 ]
 [9.977657 ]], R is [[ 9.9128437 ]
 [10.72932529]
 [11.62203217]
 [12.50581169]
 [13.38075352]].
[2019-04-07 11:03:25,305] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8298319e-04 4.2145632e-04 9.9586416e-04 2.3175183e-01 1.0776803e-03
 7.1642560e-01 3.1029312e-02 1.8115235e-02], sum to 1.0000
[2019-04-07 11:03:25,306] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4898
[2019-04-07 11:03:25,362] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.0, 79.0, 0.0, 0.0, 24.0, 23.82713146265877, 0.2080499361629667, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1130400.0000, 
sim time next is 1132200.0000, 
raw observation next is [10.55, 78.0, 0.0, 0.0, 24.0, 23.84048665887261, 0.193843507064708, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.754847645429363, 0.78, 0.0, 0.0, 0.5, 0.4867072215727175, 0.5646145023549026, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.68431985], dtype=float32), 0.49067745]. 
=============================================
[2019-04-07 11:03:33,733] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9923673e-06 2.5442607e-05 4.9857121e-05 7.4889429e-02 1.9167166e-04
 9.1148597e-01 1.1041841e-02 2.3058439e-03], sum to 1.0000
[2019-04-07 11:03:33,733] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6421
[2019-04-07 11:03:33,995] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 24.0, 23.57567909511295, -0.1122236633073683, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 849600.0000, 
sim time next is 851400.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 24.0, 23.32523585029518, -0.1143224238874452, 0.0, 1.0, 102254.84508934298], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.5, 0.44376965419126496, 0.4618925253708516, 0.0, 1.0, 0.48692783375877613], 
reward next is 0.7988, 
noisyNet noise sample is [array([0.3801006], dtype=float32), 0.6305077]. 
=============================================
[2019-04-07 11:03:42,397] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8872664e-04 6.7377294e-04 1.1024292e-03 1.3241532e-01 1.8761244e-03
 8.2440948e-01 3.1573154e-02 7.7610202e-03], sum to 1.0000
[2019-04-07 11:03:42,398] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9651
[2019-04-07 11:03:42,492] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.0, 65.0, 0.0, 0.0, 24.0, 23.7074614818731, 0.1635643413152282, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1189800.0000, 
sim time next is 1191600.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 25.0, 23.65523722341848, 0.149878032241468, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.5833333333333334, 0.4712697686182068, 0.5499593440804893, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86027634], dtype=float32), -1.6771569]. 
=============================================
[2019-04-07 11:03:53,008] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.7199869e-06 2.6799500e-05 8.8040528e-05 7.9582624e-02 2.0791063e-04
 9.1267675e-01 4.9480544e-03 2.4611200e-03], sum to 1.0000
[2019-04-07 11:03:53,008] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6127
[2019-04-07 11:03:53,110] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.1, 77.5, 0.0, 0.0, 24.0, 24.00529515349362, 0.1558232757631487, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1053000.0000, 
sim time next is 1054800.0000, 
raw observation next is [13.8, 78.0, 0.0, 0.0, 24.0, 23.91130652338152, 0.1291800498414314, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.844875346260388, 0.78, 0.0, 0.0, 0.5, 0.49260887694845995, 0.5430600166138104, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31154573], dtype=float32), -0.09735296]. 
=============================================
[2019-04-07 11:03:57,295] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5495249e-06 3.1741260e-05 5.6553141e-05 5.1325031e-02 9.5625801e-05
 9.3680507e-01 1.0357138e-02 1.3263501e-03], sum to 1.0000
[2019-04-07 11:03:57,296] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6286
[2019-04-07 11:03:57,351] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.0, 63.0, 0.0, 0.0, 24.0, 23.9301900739563, 0.2193476712714934, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1114200.0000, 
sim time next is 1116000.0000, 
raw observation next is [12.7, 64.0, 0.0, 0.0, 24.0, 23.80360843089066, 0.1889719812509961, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8144044321329641, 0.64, 0.0, 0.0, 0.5, 0.4836340359075549, 0.5629906604169986, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5478883], dtype=float32), 0.28388548]. 
=============================================
[2019-04-07 11:03:57,475] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[17.664278]
 [17.868212]
 [17.596083]
 [17.714846]
 [17.292128]], R is [[19.14104843]
 [19.94963837]
 [20.75014305]
 [21.54264259]
 [22.3272171 ]].
[2019-04-07 11:04:26,347] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.7469468e-05 1.3968429e-04 3.2600455e-04 6.1814375e-02 7.1719947e-04
 8.9816856e-01 3.2169405e-02 6.6172401e-03], sum to 1.0000
[2019-04-07 11:04:26,347] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0589
[2019-04-07 11:04:26,519] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 83.0, 0.0, 0.0, 24.0, 23.06669136168445, -0.1704836795547816, 0.0, 1.0, 47236.19223733587], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1821600.0000, 
sim time next is 1823400.0000, 
raw observation next is [-6.1, 85.0, 0.0, 0.0, 22.0, 22.96347726826153, -0.2135953266009887, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.29362880886426596, 0.85, 0.0, 0.0, 0.3333333333333333, 0.41362310568846095, 0.42880155779967044, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2137893], dtype=float32), -1.9397519]. 
=============================================
[2019-04-07 11:04:32,518] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-07 11:04:32,519] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:04:32,519] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:04:32,533] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run4
[2019-04-07 11:04:32,622] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:04:32,630] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 11:04:32,633] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:04:32,635] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run4
[2019-04-07 11:04:32,734] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:04:32,742] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run4
[2019-04-07 11:04:47,260] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.05317671], dtype=float32), 0.054436788]
[2019-04-07 11:04:47,260] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [7.5, 93.0, 140.0, 20.0, 24.0, 23.41433792776161, -0.09042829335540252, 1.0, 1.0, 0.0]
[2019-04-07 11:04:47,260] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 11:04:47,262] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [1.6504077e-06 8.6459513e-06 1.7363331e-05 3.8079899e-02 5.6969431e-05
 9.5627576e-01 4.4861818e-03 1.0735000e-03], sampled 0.3991396090947852
[2019-04-07 11:06:17,089] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2765.2991 70459907.0188 174.5758
[2019-04-07 11:06:35,457] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2749.6508 79039790.2557 106.8367
[2019-04-07 11:06:40,190] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2751.0580 83335169.0000 59.0720
[2019-04-07 11:06:41,212] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 60000, evaluation results [60000.0, 2749.6508084662696, 79039790.25571215, 106.83666843515742, 2765.299109160662, 70459907.0188299, 174.57577357292365, 2751.0579857724574, 83335168.99996842, 59.071952749260575]
[2019-04-07 11:06:43,045] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5840914e-06 1.5919069e-05 2.8405644e-05 4.2060833e-02 7.3584473e-05
 9.4554740e-01 1.0281909e-02 1.9893441e-03], sum to 1.0000
[2019-04-07 11:06:43,046] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5500
[2019-04-07 11:06:43,307] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.2, 88.5, 33.0, 21.0, 24.0, 23.40665906950982, -0.2300099372411002, 1.0, 1.0, 9375.505162288622], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1931400.0000, 
sim time next is 1933200.0000, 
raw observation next is [-8.9, 86.0, 59.0, 285.0, 24.0, 23.5911808450806, -0.1623044544367287, 1.0, 1.0, 34865.67781210454], 
processed observation next is [1.0, 0.391304347826087, 0.21606648199445982, 0.86, 0.19666666666666666, 0.3149171270718232, 0.5, 0.46593173709005004, 0.4458985151877571, 1.0, 1.0, 0.16602703720049783], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09869396], dtype=float32), 0.61665726]. 
=============================================
[2019-04-07 11:06:43,806] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.7563374e-05 1.9624835e-04 1.9876042e-04 8.7591849e-02 1.0689606e-03
 8.8727617e-01 1.6430330e-02 7.1601053e-03], sum to 1.0000
[2019-04-07 11:06:43,808] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2923
[2019-04-07 11:06:43,867] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 83.0, 0.0, 0.0, 22.0, 22.00667449144786, -0.4183664766968085, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1828800.0000, 
sim time next is 1830600.0000, 
raw observation next is [-6.2, 81.0, 0.0, 0.0, 24.0, 21.78951310829566, -0.4010190700426615, 0.0, 1.0, 147880.50964998425], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.81, 0.0, 0.0, 0.5, 0.3157927590246385, 0.3663269766524462, 0.0, 1.0, 0.7041929030951631], 
reward next is 0.5815, 
noisyNet noise sample is [array([0.522416], dtype=float32), 1.7113235]. 
=============================================
[2019-04-07 11:07:10,496] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3212581e-07 3.6590113e-06 2.0623968e-06 7.1271166e-02 7.9673064e-06
 9.2601258e-01 2.4343748e-03 2.6802626e-04], sum to 1.0000
[2019-04-07 11:07:10,496] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9482
[2019-04-07 11:07:10,774] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.65, 68.0, 144.0, 0.0, 24.0, 23.67882182815945, -0.1695135912605689, 1.0, 1.0, 32644.005575601663], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2208600.0000, 
sim time next is 2210400.0000, 
raw observation next is [-3.9, 71.0, 132.0, 0.0, 24.0, 23.93334764790074, -0.03154415770256252, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.71, 0.44, 0.0, 0.5, 0.4944456373250616, 0.4894852807658125, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8745297], dtype=float32), 1.0498638]. 
=============================================
[2019-04-07 11:07:17,699] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2708681e-07 1.2962976e-06 7.9990132e-06 2.7305963e-02 1.8457931e-05
 9.6859789e-01 3.2356782e-03 8.3242613e-04], sum to 1.0000
[2019-04-07 11:07:17,699] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6063
[2019-04-07 11:07:17,771] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 79.0, 0.0, 0.0, 24.0, 23.04403335293656, -0.1766637180359158, 0.0, 1.0, 43868.11489116386], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2163600.0000, 
sim time next is 2165400.0000, 
raw observation next is [-7.0, 78.5, 0.0, 0.0, 24.0, 22.92003861939593, -0.1945004329561957, 0.0, 1.0, 43901.23471148937], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.785, 0.0, 0.0, 0.5, 0.410003218282994, 0.4351665223479348, 0.0, 1.0, 0.20905349862613987], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27476987], dtype=float32), -0.083111815]. 
=============================================
[2019-04-07 11:07:57,709] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5141412e-09 7.0198617e-08 6.8461168e-07 7.1038236e-03 7.7210865e-07
 9.9226147e-01 5.4555747e-04 8.7506232e-05], sum to 1.0000
[2019-04-07 11:07:57,709] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0697
[2019-04-07 11:07:57,780] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 30.0, 194.5, 252.0, 24.0, 24.0774004927304, -0.07081744162209459, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2552400.0000, 
sim time next is 2554200.0000, 
raw observation next is [3.0, 28.0, 171.0, 482.0, 24.0, 23.93833772138891, -0.02260065206314579, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.28, 0.57, 0.532596685082873, 0.5, 0.49486147678240905, 0.4924664493122847, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.62936914], dtype=float32), -1.0252042]. 
=============================================
[2019-04-07 11:08:04,247] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.99931360e-08 1.19176356e-07 6.09990593e-07 1.24967908e-02
 4.82166001e-07 9.85871732e-01 1.53489981e-03 9.53483177e-05], sum to 1.0000
[2019-04-07 11:08:04,248] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4460
[2019-04-07 11:08:04,342] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 75.5, 0.0, 0.0, 24.0, 23.48769202767808, -0.1054355390642104, 0.0, 1.0, 43487.30725430244], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2856600.0000, 
sim time next is 2858400.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 24.0, 23.46309270494186, -0.1001542336377032, 0.0, 1.0, 66780.68331422335], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.5, 0.45525772541182175, 0.46661525545409893, 0.0, 1.0, 0.31800325387725403], 
reward next is 0.9677, 
noisyNet noise sample is [array([-0.6767095], dtype=float32), -0.5159498]. 
=============================================
[2019-04-07 11:08:05,848] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8753925e-09 2.9552879e-08 2.3493388e-07 8.9296978e-03 6.7627769e-07
 9.9042648e-01 5.7287357e-04 6.9986832e-05], sum to 1.0000
[2019-04-07 11:08:05,849] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4810
[2019-04-07 11:08:05,951] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 50.0, 54.0, 454.0, 24.0, 24.50241616564842, 0.123637372657553, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2737800.0000, 
sim time next is 2739600.0000, 
raw observation next is [-3.0, 50.0, 28.5, 253.5, 24.0, 24.11110419063827, 0.05452670134028959, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3795013850415513, 0.5, 0.095, 0.28011049723756903, 0.5, 0.5092586825531891, 0.5181755671134299, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3480043], dtype=float32), 1.6324424]. 
=============================================
[2019-04-07 11:08:19,604] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.8930715e-11 6.5000449e-10 5.5926712e-09 3.4072297e-03 5.6592686e-08
 9.9629080e-01 2.7797234e-04 2.3924935e-05], sum to 1.0000
[2019-04-07 11:08:19,605] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4390
[2019-04-07 11:08:19,647] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 24.0, 24.15315807277805, 0.2568844425571246, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3182400.0000, 
sim time next is 3184200.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 24.0, 23.90772868286247, 0.2064047262744509, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.5, 0.49231072357187244, 0.5688015754248169, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6831926], dtype=float32), -1.8650525]. 
=============================================
[2019-04-07 11:09:07,442] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.8640347e-07 5.0496794e-07 1.8786703e-06 3.2427814e-02 3.1267948e-06
 9.6536869e-01 1.8295452e-03 3.6794844e-04], sum to 1.0000
[2019-04-07 11:09:07,442] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4237
[2019-04-07 11:09:07,690] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 54.0, 112.5, 787.0, 24.0, 23.22984277388676, -0.003824785239365807, 0.0, 1.0, 37086.69556719638], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3582000.0000, 
sim time next is 3583800.0000, 
raw observation next is [-3.5, 54.5, 114.0, 816.0, 24.0, 23.3205480048773, 0.01209229404796357, 0.0, 1.0, 18707.44239215585], 
processed observation next is [0.0, 0.4782608695652174, 0.36565096952908593, 0.545, 0.38, 0.901657458563536, 0.5, 0.44337900040644157, 0.5040307646826545, 0.0, 1.0, 0.08908305901026595], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1923805], dtype=float32), -0.8293903]. 
=============================================
[2019-04-07 11:09:40,881] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-07 11:09:40,885] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:09:40,885] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:09:40,887] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run5
[2019-04-07 11:09:40,984] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:09:40,984] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:09:40,992] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run5
[2019-04-07 11:09:41,082] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 11:09:41,084] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:09:41,127] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run5
[2019-04-07 11:11:11,850] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.07164509], dtype=float32), 0.07268202]
[2019-04-07 11:11:11,850] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [2.85, 48.0, 0.0, 0.0, 24.0, 23.47750123257551, -0.07119147658728676, 0.0, 1.0, 37131.75192403125]
[2019-04-07 11:11:11,850] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 11:11:11,850] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [1.6834797e-08 1.2636988e-07 5.8942322e-07 1.0204426e-02 6.8590600e-07
 9.8821825e-01 1.4187745e-03 1.5721776e-04], sampled 0.6574887162407308
[2019-04-07 11:11:27,402] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2784.5378 70848900.9400 166.6449
[2019-04-07 11:11:34,007] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.07164509], dtype=float32), 0.07268202]
[2019-04-07 11:11:34,007] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-4.0, 52.0, 0.0, 0.0, 24.0, 23.39857989495088, -0.07446545861719532, 0.0, 1.0, 35132.4708912923]
[2019-04-07 11:11:34,007] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 11:11:34,008] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [1.5030047e-07 7.0714759e-07 2.5417057e-06 1.5674245e-02 3.5973999e-06
 9.8117745e-01 2.7631600e-03 3.7819389e-04], sampled 0.01830360815409582
[2019-04-07 11:11:45,895] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2780.6967 79414526.1671 96.2455
[2019-04-07 11:11:49,445] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2782.0095 83774223.2530 32.9443
[2019-04-07 11:11:50,469] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 80000, evaluation results [80000.0, 2780.6967353052983, 79414526.16713932, 96.24545796194109, 2784.537836658437, 70848900.94002618, 166.6448956460125, 2782.009525045283, 83774223.25297394, 32.94426082361752]
[2019-04-07 11:11:55,000] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0277465e-11 1.2391251e-10 3.5129879e-09 1.1791820e-03 1.5638204e-08
 9.9868470e-01 1.2015754e-04 1.5971777e-05], sum to 1.0000
[2019-04-07 11:11:55,000] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2843
[2019-04-07 11:11:55,201] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 29.0, 118.0, 835.5, 24.0, 24.91884854391619, 0.2488636791122527, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4107600.0000, 
sim time next is 4109400.0000, 
raw observation next is [3.0, 30.0, 116.0, 830.0, 24.0, 24.64445998591704, 0.2547297311317806, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.3, 0.38666666666666666, 0.9171270718232044, 0.5, 0.5537049988264201, 0.5849099103772603, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46345437], dtype=float32), 0.90582913]. 
=============================================
[2019-04-07 11:12:04,644] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6033511e-10 5.4991234e-09 4.7913464e-09 2.0477949e-03 1.4114496e-08
 9.9770397e-01 2.4097014e-04 7.3220126e-06], sum to 1.0000
[2019-04-07 11:12:04,644] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7843
[2019-04-07 11:12:04,693] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 31.0, 0.0, 0.0, 24.0, 23.61554953274039, 0.04412904666682119, 0.0, 1.0, 46860.65790821243], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4053600.0000, 
sim time next is 4055400.0000, 
raw observation next is [-5.5, 34.0, 0.0, 0.0, 24.0, 23.49253549075117, -0.03224112119132161, 0.0, 1.0, 57646.28068815038], 
processed observation next is [1.0, 0.9565217391304348, 0.3102493074792244, 0.34, 0.0, 0.0, 0.5, 0.4577112908959308, 0.4892529596028928, 0.0, 1.0, 0.2745060985150018], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28812394], dtype=float32), -1.2465084]. 
=============================================
[2019-04-07 11:12:10,874] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3889734e-13 4.3719208e-12 4.3606216e-11 4.4658800e-04 1.9920003e-10
 9.9954069e-01 1.2168410e-05 5.4520939e-07], sum to 1.0000
[2019-04-07 11:12:10,874] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5144
[2019-04-07 11:12:10,897] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.95, 61.5, 0.0, 0.0, 24.0, 24.57247036957737, 0.2697578661555355, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4401000.0000, 
sim time next is 4402800.0000, 
raw observation next is [8.5, 62.0, 0.0, 0.0, 24.0, 24.23534518347449, 0.2134958472203421, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.698060941828255, 0.62, 0.0, 0.0, 0.5, 0.5196120986228742, 0.5711652824067807, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9396777], dtype=float32), 1.5714259]. 
=============================================
[2019-04-07 11:12:21,478] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2661790e-14 3.3602294e-12 1.4533915e-11 5.8679430e-05 6.3518593e-11
 9.9993730e-01 3.7601510e-06 1.6099276e-07], sum to 1.0000
[2019-04-07 11:12:21,478] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0806
[2019-04-07 11:12:21,548] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.0, 26.0, 100.5, 796.5, 24.0, 26.5425110521279, 0.5884211229222855, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4978800.0000, 
sim time next is 4980600.0000, 
raw observation next is [8.5, 25.5, 92.0, 774.0, 24.0, 26.14753070349354, 0.5630703532056264, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.698060941828255, 0.255, 0.30666666666666664, 0.8552486187845304, 0.5, 0.6789608919577951, 0.6876901177352087, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0264854], dtype=float32), 1.0848875]. 
=============================================
[2019-04-07 11:12:22,640] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0261861e-13 2.9272595e-11 3.0513103e-10 3.1120327e-04 3.3265760e-10
 9.9962091e-01 6.7734363e-05 1.2540224e-07], sum to 1.0000
[2019-04-07 11:12:22,640] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4937
[2019-04-07 11:12:22,688] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 24.0, 23.45107196775264, 0.0158170330178633, 0.0, 1.0, 101339.41014328349], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4575600.0000, 
sim time next is 4577400.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 24.0, 23.52611511492252, 0.0264919395563336, 0.0, 1.0, 31767.47145426751], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.5, 0.4605095929102099, 0.5088306465187779, 0.0, 1.0, 0.15127367359175004], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3869451], dtype=float32), -0.8318513]. 
=============================================
[2019-04-07 11:12:22,738] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:22,738] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:22,740] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run3
[2019-04-07 11:12:26,009] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:26,009] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:26,011] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run3
[2019-04-07 11:12:28,753] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:28,753] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:28,758] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run3
[2019-04-07 11:12:29,437] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:29,437] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:29,439] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run3
[2019-04-07 11:12:31,277] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:31,277] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:31,279] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run3
[2019-04-07 11:12:32,253] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:32,253] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:32,255] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run3
[2019-04-07 11:12:34,836] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:34,837] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:34,838] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run3
[2019-04-07 11:12:41,097] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:41,097] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:41,099] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run3
[2019-04-07 11:12:42,955] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:42,955] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:42,957] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run3
[2019-04-07 11:12:43,058] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:43,058] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:43,060] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run3
[2019-04-07 11:12:47,833] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:47,833] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:47,835] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run3
[2019-04-07 11:12:49,024] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:49,024] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:49,026] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run3
[2019-04-07 11:12:49,848] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:49,848] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:49,851] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run3
[2019-04-07 11:12:50,617] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:50,617] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:50,619] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run3
[2019-04-07 11:12:52,190] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:52,191] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:52,192] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run3
[2019-04-07 11:12:52,425] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:12:52,425] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:12:52,427] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run3
[2019-04-07 11:13:03,947] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2877209e-09 4.5687556e-09 3.4201843e-08 5.4817442e-03 1.4461676e-07
 9.9430215e-01 1.7993807e-04 3.5925681e-05], sum to 1.0000
[2019-04-07 11:13:03,947] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8425
[2019-04-07 11:13:04,035] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.45, 94.5, 0.0, 0.0, 24.0, 20.75195243092939, -0.6874634758983964, 0.0, 1.0, 41825.57035195166], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 12600.0000, 
sim time next is 14400.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 24.0, 20.88710572992931, -0.6666360283300962, 0.0, 1.0, 41360.04505425665], 
processed observation next is [0.0, 0.17391304347826086, 0.6759002770083103, 0.93, 0.0, 0.0, 0.5, 0.24059214416077582, 0.2777879905566346, 0.0, 1.0, 0.19695259549646021], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14016818], dtype=float32), -0.38103613]. 
=============================================
[2019-04-07 11:13:14,276] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1442375e-13 2.0206408e-11 4.1828641e-10 1.0515485e-04 2.8125413e-10
 9.9986422e-01 2.8639757e-05 2.0138414e-06], sum to 1.0000
[2019-04-07 11:13:14,276] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3819
[2019-04-07 11:13:14,372] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.4, 82.0, 0.0, 0.0, 24.0, 22.82357769935841, -0.236755901592478, 0.0, 1.0, 48556.052128359384], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 338400.0000, 
sim time next is 340200.0000, 
raw observation next is [-13.65, 76.0, 0.0, 0.0, 24.0, 22.70817698976851, -0.261793579152117, 0.0, 1.0, 48403.48502198897], 
processed observation next is [1.0, 0.9565217391304348, 0.08448753462603877, 0.76, 0.0, 0.0, 0.5, 0.39234808248070924, 0.412735473615961, 0.0, 1.0, 0.2304927858189951], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2411512], dtype=float32), -0.059377987]. 
=============================================
[2019-04-07 11:13:40,625] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.3260278e-12 6.1578395e-11 1.6957526e-09 9.0447447e-04 4.1499515e-09
 9.9903321e-01 5.3920940e-05 8.3273362e-06], sum to 1.0000
[2019-04-07 11:13:40,626] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2763
[2019-04-07 11:13:40,734] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-14.5, 69.0, 0.0, 0.0, 24.0, 22.03617747865524, -0.4144261935818901, 0.0, 1.0, 48602.49438641304], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 349200.0000, 
sim time next is 351000.0000, 
raw observation next is [-14.75, 69.0, 0.0, 0.0, 24.0, 22.09648445217096, -0.4216346286668751, 0.0, 1.0, 48741.789753793855], 
processed observation next is [1.0, 0.043478260869565216, 0.05401662049861495, 0.69, 0.0, 0.0, 0.5, 0.3413737043475799, 0.35945512377770833, 0.0, 1.0, 0.2321037607323517], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1399176], dtype=float32), -0.97631127]. 
=============================================
[2019-04-07 11:13:40,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[45.99425 ]
 [47.662716]
 [48.45504 ]
 [49.47366 ]
 [49.81531 ]], R is [[45.12997818]
 [45.67868042]
 [46.22189331]
 [46.75967407]
 [47.29207611]].
[2019-04-07 11:14:39,887] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-07 11:14:39,917] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:14:39,917] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:14:39,939] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run6
[2019-04-07 11:14:39,983] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:14:39,985] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:14:39,985] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 11:14:39,986] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:14:39,987] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run6
[2019-04-07 11:14:40,090] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run6
[2019-04-07 11:16:08,906] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.08457288], dtype=float32), 0.08589144]
[2019-04-07 11:16:08,906] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-5.0, 71.0, 109.5, 225.5, 24.0, 24.26580105822482, 0.004496619828989097, 1.0, 1.0, 0.0]
[2019-04-07 11:16:08,906] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 11:16:08,907] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [4.4143790e-13 4.0915921e-12 4.4666829e-11 9.1074297e-05 1.6936301e-10
 9.9990177e-01 6.7907181e-06 3.8157867e-07], sampled 0.35654968329415493
[2019-04-07 11:16:41,410] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 11:16:58,127] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 11:17:03,508] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0008 83781220.5220 33.4722
[2019-04-07 11:17:04,530] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 100000, evaluation results [100000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.000823862555, 83781220.52197354, 33.4721933316778]
[2019-04-07 11:17:10,991] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8800458e-10 4.8284319e-09 1.7207022e-08 1.1868330e-03 6.7055353e-08
 9.9831200e-01 4.7195045e-04 2.9140188e-05], sum to 1.0000
[2019-04-07 11:17:11,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3878
[2019-04-07 11:17:11,027] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 24.0, 22.46102616398596, -0.1074549919781477, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1231200.0000, 
sim time next is 1233000.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 24.0, 22.42495761029751, -0.1128088115880493, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.5, 0.3687464675247926, 0.46239706280398357, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08211931], dtype=float32), -0.8212404]. 
=============================================
[2019-04-07 11:17:11,031] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[38.85581 ]
 [38.67504 ]
 [38.494556]
 [38.147907]
 [37.879185]], R is [[39.55144501]
 [40.15592957]
 [40.75437164]
 [41.34682846]
 [41.93336105]].
[2019-04-07 11:17:13,504] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.4041064e-16 2.9391220e-14 2.2612325e-13 4.9565660e-06 1.5920824e-11
 9.9999487e-01 1.1718140e-07 3.0934590e-08], sum to 1.0000
[2019-04-07 11:17:13,506] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9567
[2019-04-07 11:17:13,552] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 94.0, 0.0, 0.0, 24.0, 23.53039086267809, 0.007163942698893829, 0.0, 1.0, 24773.71597038143], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1479600.0000, 
sim time next is 1481400.0000, 
raw observation next is [2.2, 95.0, 0.0, 0.0, 24.0, 23.55481266233096, 0.03715990927240905, 0.0, 1.0, 51436.45873840485], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.95, 0.0, 0.0, 0.5, 0.4629010551942467, 0.5123866364241364, 0.0, 1.0, 0.24493551780192785], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36590043], dtype=float32), -0.6937342]. 
=============================================
[2019-04-07 11:17:13,565] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.82404630e-14 5.52653592e-14 2.25753118e-12 1.89133898e-05
 4.62879075e-11 9.99977469e-01 3.40262295e-06 1.14562106e-07], sum to 1.0000
[2019-04-07 11:17:13,566] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1379
[2019-04-07 11:17:13,620] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 24.0, 23.51172492384105, 0.01465034008441456, 0.0, 1.0, 31434.79527126928], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1488600.0000, 
sim time next is 1490400.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 24.0, 23.53323907670188, 0.007875075682776215, 0.0, 1.0, 19226.50914962136], 
processed observation next is [1.0, 0.2608695652173913, 0.5235457063711911, 0.96, 0.0, 0.0, 0.5, 0.4611032563918232, 0.502625025227592, 0.0, 1.0, 0.09155480547438744], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7107893], dtype=float32), -0.32686785]. 
=============================================
[2019-04-07 11:17:13,718] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2658455e-13 1.1546012e-12 2.7528240e-11 2.4294910e-05 3.3793600e-11
 9.9997187e-01 3.6564888e-06 1.5685568e-07], sum to 1.0000
[2019-04-07 11:17:13,718] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0029
[2019-04-07 11:17:13,914] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.35, 71.0, 83.0, 0.0, 24.0, 23.8454934042848, 0.1587554386653743, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1157400.0000, 
sim time next is 1159200.0000, 
raw observation next is [17.2, 67.0, 106.5, 0.0, 24.0, 23.72900890149115, 0.1300440971231525, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.9390581717451525, 0.67, 0.355, 0.0, 0.5, 0.4774174084575957, 0.5433480323743841, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5466193], dtype=float32), 0.80107313]. 
=============================================
[2019-04-07 11:17:14,757] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5561513e-16 6.9288705e-15 2.6763578e-14 1.2763420e-05 1.1922168e-12
 9.9998593e-01 1.2812598e-06 7.4370758e-09], sum to 1.0000
[2019-04-07 11:17:14,763] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7625
[2019-04-07 11:17:14,814] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 24.0, 23.49652608631182, 0.06485692652543042, 0.0, 1.0, 11396.294387386455], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1380600.0000, 
sim time next is 1382400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 24.0, 23.43672946815675, 0.06657635145173278, 0.0, 1.0, 56641.03253506609], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.5, 0.4530607890130624, 0.5221921171505776, 0.0, 1.0, 0.2697192025479338], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15894417], dtype=float32), 0.9031327]. 
=============================================
[2019-04-07 11:17:19,541] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0507429e-18 3.9251454e-17 2.2842503e-15 4.2243698e-07 3.4632717e-15
 9.9999952e-01 1.5733427e-08 2.9179381e-10], sum to 1.0000
[2019-04-07 11:17:19,542] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0281
[2019-04-07 11:17:19,607] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.3, 51.0, 64.0, 18.5, 24.0, 25.64017510440906, 0.4318469442563406, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1612800.0000, 
sim time next is 1614600.0000, 
raw observation next is [12.75, 52.5, 50.0, 37.0, 24.0, 25.29427078506336, 0.3769586204081925, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8157894736842106, 0.525, 0.16666666666666666, 0.04088397790055249, 0.5, 0.6078558987552801, 0.6256528734693975, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9579816], dtype=float32), 0.20038968]. 
=============================================
[2019-04-07 11:17:22,753] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7990306e-13 2.6286989e-12 6.2363691e-11 5.2528379e-05 2.6160668e-10
 9.9992704e-01 1.9847092e-05 5.7467588e-07], sum to 1.0000
[2019-04-07 11:17:22,756] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0796
[2019-04-07 11:17:22,804] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.9, 79.0, 0.0, 0.0, 24.0, 23.24600573192475, -0.1765061581482829, 0.0, 1.0, 46197.105656792184], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1891800.0000, 
sim time next is 1893600.0000, 
raw observation next is [-6.2, 75.0, 0.0, 0.0, 24.0, 23.17097032223834, -0.1922917693660889, 0.0, 1.0, 46130.72564406949], 
processed observation next is [0.0, 0.9565217391304348, 0.2908587257617729, 0.75, 0.0, 0.0, 0.5, 0.43091419351986165, 0.43590274354463704, 0.0, 1.0, 0.21967012211461662], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22669432], dtype=float32), -0.23979223]. 
=============================================
[2019-04-07 11:17:42,261] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3275747e-18 2.0622771e-17 1.3480562e-15 4.1142292e-07 3.9129509e-15
 9.9999964e-01 1.9845075e-08 1.3613545e-09], sum to 1.0000
[2019-04-07 11:17:42,261] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3086
[2019-04-07 11:17:42,447] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 0.0, 0.0, 24.0, 23.6266689530615, -0.06706285007669836, 1.0, 1.0, 24972.18937642981], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1969200.0000, 
sim time next is 1971000.0000, 
raw observation next is [-5.05, 77.0, 0.0, 0.0, 24.0, 23.44100895665624, -0.0924917948168611, 1.0, 1.0, 40475.509127273224], 
processed observation next is [1.0, 0.8260869565217391, 0.32271468144044324, 0.77, 0.0, 0.0, 0.5, 0.4534174130546867, 0.469169401727713, 1.0, 1.0, 0.19274051965368202], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7140349], dtype=float32), 1.9233524]. 
=============================================
[2019-04-07 11:17:42,451] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[69.58428 ]
 [69.83077 ]
 [69.03217 ]
 [68.88621 ]
 [68.537575]], R is [[69.75688171]
 [70.05931091]
 [70.35871887]
 [70.63755035]
 [70.68488312]].
[2019-04-07 11:17:42,983] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7216265e-17 9.5275420e-16 3.0934853e-14 4.3033033e-05 2.3116134e-13
 9.9995673e-01 1.8766673e-07 7.2274071e-09], sum to 1.0000
[2019-04-07 11:17:42,983] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7141
[2019-04-07 11:17:43,065] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.9, 80.5, 0.0, 0.0, 24.0, 23.46732544372968, -0.05936681486050353, 0.0, 1.0, 21024.193425320715], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1978200.0000, 
sim time next is 1980000.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 24.0, 23.36533669968679, -0.05179912033791129, 0.0, 1.0, 73577.79250213182], 
processed observation next is [1.0, 0.9565217391304348, 0.2908587257617729, 0.83, 0.0, 0.0, 0.5, 0.4471113916405658, 0.48273362655402957, 0.0, 1.0, 0.350370440486342], 
reward next is 0.9353, 
noisyNet noise sample is [array([0.34152424], dtype=float32), -0.8368196]. 
=============================================
[2019-04-07 11:17:43,124] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[64.81264 ]
 [65.348045]
 [66.46756 ]
 [66.813194]
 [67.94583 ]], R is [[64.16879272]
 [64.52710724]
 [64.88183594]
 [64.96660614]
 [65.31694031]].
[2019-04-07 11:18:05,180] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7535145e-17 1.0917992e-14 1.2431179e-13 3.0965650e-06 9.5948325e-13
 9.9999690e-01 4.0670674e-08 3.7957983e-09], sum to 1.0000
[2019-04-07 11:18:05,180] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4850
[2019-04-07 11:18:05,394] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 71.0, 109.5, 225.5, 24.0, 24.2647308054521, 0.004567144755582161, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2196000.0000, 
sim time next is 2197800.0000, 
raw observation next is [-4.75, 71.0, 117.0, 0.0, 24.0, 24.24816047500818, -0.02280860788732057, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3310249307479225, 0.71, 0.39, 0.0, 0.5, 0.5206800395840151, 0.49239713070422647, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5353913], dtype=float32), -0.96673185]. 
=============================================
[2019-04-07 11:19:16,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2461810e-14 1.7845110e-13 5.0593605e-12 1.0768280e-05 3.4324769e-11
 9.9998653e-01 2.5671893e-06 2.1493202e-08], sum to 1.0000
[2019-04-07 11:19:16,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1537
[2019-04-07 11:19:17,229] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.11679855486375, -0.1243672385176269, 0.0, 1.0, 31660.058318723284], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3006000.0000, 
sim time next is 3007800.0000, 
raw observation next is [-2.5, 62.5, 0.0, 0.0, 24.0, 23.11739718236412, -0.06867312502068515, 0.0, 1.0, 130383.42448703363], 
processed observation next is [0.0, 0.8260869565217391, 0.39335180055401664, 0.625, 0.0, 0.0, 0.5, 0.4264497651970099, 0.4771089583264383, 0.0, 1.0, 0.6208734499382553], 
reward next is 0.6648, 
noisyNet noise sample is [array([-0.87155277], dtype=float32), -0.9020693]. 
=============================================
[2019-04-07 11:19:24,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2237068e-22 2.0247719e-20 3.0900172e-17 1.8295262e-08 5.9710416e-17
 1.0000000e+00 2.4020086e-10 1.2743929e-11], sum to 1.0000
[2019-04-07 11:19:24,587] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5293
[2019-04-07 11:19:24,741] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 100.0, 0.0, 0.0, 24.0, 24.02887565536741, 0.2743572204273164, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3189600.0000, 
sim time next is 3191400.0000, 
raw observation next is [2.0, 96.5, 0.0, 0.0, 24.0, 23.8744947364828, 0.1669186866628781, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.965, 0.0, 0.0, 0.5, 0.4895412280402332, 0.5556395622209593, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.353455], dtype=float32), -0.643981]. 
=============================================
[2019-04-07 11:19:27,806] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-07 11:19:27,807] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:19:27,807] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:19:27,809] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run7
[2019-04-07 11:19:27,877] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:19:27,877] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:19:27,917] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 11:19:27,918] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:19:27,927] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run7
[2019-04-07 11:19:28,012] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run7
[2019-04-07 11:20:51,131] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.0934357], dtype=float32), 0.095555015]
[2019-04-07 11:20:51,131] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [2.2, 67.0, 88.5, 140.5, 24.0, 23.62861134234415, 0.07294875025555576, 1.0, 1.0, 0.0]
[2019-04-07 11:20:51,131] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 11:20:51,131] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [2.9678438e-16 3.6779229e-15 1.2365086e-13 8.4969548e-07 4.7474053e-13
 9.9999905e-01 9.7628110e-08 4.6555559e-09], sampled 0.6043755787976255
[2019-04-07 11:21:33,274] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2788.7623 70927233.8163 166.2180
[2019-04-07 11:21:50,990] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 11:21:55,002] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.3593 83805026.4189 32.8860
[2019-04-07 11:21:56,024] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 120000, evaluation results [120000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2788.7623188653442, 70927233.8162715, 166.21801628696863, 2785.3592590356807, 83805026.41892925, 32.88598103848735]
[2019-04-07 11:22:00,463] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8876390e-20 4.3551917e-19 1.8223575e-17 3.7014005e-09 1.2614058e-16
 1.0000000e+00 1.7855839e-09 7.7948151e-12], sum to 1.0000
[2019-04-07 11:22:00,463] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3837
[2019-04-07 11:22:00,552] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 46.0, 73.5, 587.5, 24.0, 24.86358397938576, 0.1585864305260779, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3340800.0000, 
sim time next is 3342600.0000, 
raw observation next is [-2.0, 48.0, 60.0, 501.0, 24.0, 24.2302653839376, 0.1453823126128198, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.48, 0.2, 0.5535911602209945, 0.5, 0.5191887819947999, 0.5484607708709399, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1890854], dtype=float32), 0.4759659]. 
=============================================
[2019-04-07 11:22:08,830] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3479215e-17 2.3339801e-17 5.5940652e-15 1.4198737e-07 1.4329482e-14
 9.9999988e-01 5.3774682e-09 9.8264903e-11], sum to 1.0000
[2019-04-07 11:22:08,830] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7415
[2019-04-07 11:22:08,953] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 77.0, 94.5, 552.0, 24.0, 24.1045477826951, 0.04663871164803627, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3747600.0000, 
sim time next is 3749400.0000, 
raw observation next is [-3.5, 77.0, 100.0, 675.0, 24.0, 24.54428334643686, 0.1108185894016948, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.36565096952908593, 0.77, 0.3333333333333333, 0.7458563535911602, 0.5, 0.545356945536405, 0.5369395298005649, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9151338], dtype=float32), 0.16290085]. 
=============================================
[2019-04-07 11:22:12,061] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5245011e-18 1.5311117e-16 4.3048379e-15 2.4983257e-07 6.3089678e-15
 9.9999976e-01 1.2805225e-08 3.7287013e-09], sum to 1.0000
[2019-04-07 11:22:12,061] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0529
[2019-04-07 11:22:12,081] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 24.0, 24.43329471059008, 0.2234390819981325, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3520800.0000, 
sim time next is 3522600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 24.0, 24.42956051362993, 0.1848859843986877, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.5, 0.5357967094691608, 0.5616286614662293, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9005661], dtype=float32), -0.9181333]. 
=============================================
[2019-04-07 11:22:13,101] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.1399724e-18 7.9694233e-17 2.8476324e-15 1.6410017e-06 4.3114231e-14
 9.9999821e-01 6.3691687e-08 1.1620236e-09], sum to 1.0000
[2019-04-07 11:22:13,101] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1231
[2019-04-07 11:22:13,263] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.0, 48.5, 101.0, 698.0, 24.0, 24.94734111923047, 0.1780123686244542, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4008600.0000, 
sim time next is 4010400.0000, 
raw observation next is [-9.0, 44.0, 106.5, 739.5, 24.0, 25.03649799907537, 0.2007561096936195, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.21329639889196678, 0.44, 0.355, 0.8171270718232044, 0.5, 0.5863748332562807, 0.5669187032312065, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7687811], dtype=float32), -0.63843393]. 
=============================================
[2019-04-07 11:22:19,378] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.6980881e-18 2.1807015e-17 4.5591273e-16 9.1597116e-08 1.0601103e-14
 9.9999988e-01 2.4467244e-08 7.4488454e-10], sum to 1.0000
[2019-04-07 11:22:19,379] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7766
[2019-04-07 11:22:19,426] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 74.0, 0.0, 0.0, 24.0, 23.78961765143954, 0.1039485952254345, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3792600.0000, 
sim time next is 3794400.0000, 
raw observation next is [-3.0, 77.0, 0.0, 0.0, 24.0, 23.64716312404344, 0.07948514393684863, 0.0, 1.0, 39414.37855548818], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.77, 0.0, 0.0, 0.5, 0.4705969270036201, 0.5264950479789495, 0.0, 1.0, 0.1876875169308961], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0675385], dtype=float32), -0.59807247]. 
=============================================
[2019-04-07 11:22:39,799] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.4504327e-20 4.3093504e-18 1.1891743e-16 1.2274521e-08 1.2723914e-15
 1.0000000e+00 9.3368724e-10 6.9547618e-11], sum to 1.0000
[2019-04-07 11:22:39,803] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7372
[2019-04-07 11:22:39,844] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 38.0, 0.0, 0.0, 24.0, 23.71539660705468, 0.1411912065484373, 0.0, 1.0, 79312.70606229366], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4138200.0000, 
sim time next is 4140000.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 24.0, 23.92505663137866, 0.1419293511580048, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.4, 0.0, 0.0, 0.5, 0.49375471928155495, 0.5473097837193349, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.84802824], dtype=float32), 0.968036]. 
=============================================
[2019-04-07 11:22:39,860] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[77.784805]
 [78.73831 ]
 [79.69184 ]
 [81.526245]
 [84.32057 ]], R is [[77.37244415]
 [77.50675201]
 [77.39448547]
 [77.62054443]
 [77.84433746]].
[2019-04-07 11:22:46,093] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3865653e-20 7.6958866e-18 1.0967146e-16 1.9888356e-08 8.5777266e-17
 1.0000000e+00 6.4103757e-11 7.8390367e-12], sum to 1.0000
[2019-04-07 11:22:46,094] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5159
[2019-04-07 11:22:46,128] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 24.0, 23.75524481396849, 0.07077199314276715, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4667400.0000, 
sim time next is 4669200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 24.0, 23.72789931426498, 0.08419855702884844, 0.0, 1.0, 42719.88616805462], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.52, 0.0, 0.0, 0.5, 0.4773249428554151, 0.5280661856762828, 0.0, 1.0, 0.20342802937168866], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.82310146], dtype=float32), -0.27791384]. 
=============================================
[2019-04-07 11:22:49,538] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:22:49,538] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:22:49,556] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run4
[2019-04-07 11:22:51,201] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.56107587e-20 8.13173301e-18 2.36787134e-16 1.01120285e-08
 1.83735994e-15 1.00000000e+00 1.51459623e-09 1.95408967e-10], sum to 1.0000
[2019-04-07 11:22:51,203] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3487
[2019-04-07 11:22:51,242] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 24.0, 23.72789931426498, 0.08419855702884844, 0.0, 1.0, 42719.88616805462], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4669200.0000, 
sim time next is 4671000.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 24.0, 23.79211157205026, 0.05889144810400554, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.57, 0.0, 0.0, 0.5, 0.48267596433752163, 0.5196304827013352, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2778208], dtype=float32), -0.25663218]. 
=============================================
[2019-04-07 11:22:51,244] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[78.43256]
 [79.69396]
 [80.6571 ]
 [81.93217]
 [81.11512]], R is [[76.66812134]
 [76.90144348]
 [77.13243103]
 [77.36110687]
 [77.5874939 ]].
[2019-04-07 11:22:54,851] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:22:54,851] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:22:54,853] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run4
[2019-04-07 11:22:59,257] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:22:59,258] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:22:59,261] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run4
[2019-04-07 11:22:59,850] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1466941e-17 8.6016069e-16 8.9585728e-14 1.9099598e-07 1.5349428e-13
 9.9999869e-01 1.1047639e-06 2.7087339e-08], sum to 1.0000
[2019-04-07 11:22:59,851] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2588
[2019-04-07 11:22:59,882] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 37.0, 33.0, 185.0, 24.0, 23.45669397637472, -0.02577545843756971, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4815000.0000, 
sim time next is 4816800.0000, 
raw observation next is [2.0, 40.0, 16.5, 92.5, 24.0, 23.34474321623599, -0.07395053532933285, 0.0, 1.0, 12453.607780153694], 
processed observation next is [0.0, 0.782608695652174, 0.518005540166205, 0.4, 0.055, 0.10220994475138122, 0.5, 0.44539526801966584, 0.47534982155688904, 0.0, 1.0, 0.059302894191208065], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0704362], dtype=float32), -0.92055184]. 
=============================================
[2019-04-07 11:23:01,448] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:23:01,449] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:23:01,450] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run4
[2019-04-07 11:23:01,925] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:23:01,926] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:23:01,928] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run4
[2019-04-07 11:23:02,473] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:23:02,473] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:23:02,475] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run4
[2019-04-07 11:23:05,616] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0418376e-18 8.1000325e-17 1.0808493e-15 2.3886025e-07 2.6498140e-14
 9.9999976e-01 1.7882931e-09 4.6634402e-10], sum to 1.0000
[2019-04-07 11:23:05,616] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3594
[2019-04-07 11:23:05,664] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 48.0, 0.0, 0.0, 24.0, 23.43790398360306, -0.125418855971441, 0.0, 1.0, 49202.99945822582], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4941000.0000, 
sim time next is 4942800.0000, 
raw observation next is [-2.0, 46.0, 0.0, 0.0, 24.0, 23.49257427284778, -0.1412412563033897, 0.0, 1.0, 15544.951928232835], 
processed observation next is [1.0, 0.21739130434782608, 0.40720221606648205, 0.46, 0.0, 0.0, 0.5, 0.45771452273731494, 0.4529195812322034, 0.0, 1.0, 0.07402358061063255], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.88674134], dtype=float32), -0.5533771]. 
=============================================
[2019-04-07 11:23:06,733] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:23:06,733] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:23:06,735] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run4
[2019-04-07 11:23:09,271] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:23:09,271] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:23:09,273] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run4
[2019-04-07 11:23:10,980] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:23:10,980] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:23:10,982] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run4
[2019-04-07 11:23:13,301] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:23:13,301] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:23:13,303] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run4
[2019-04-07 11:23:13,485] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:23:13,485] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:23:13,487] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run4
[2019-04-07 11:23:13,725] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:23:13,725] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:23:13,727] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run4
[2019-04-07 11:23:15,744] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:23:15,744] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:23:15,746] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run4
[2019-04-07 11:23:19,112] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:23:19,113] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:23:19,114] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run4
[2019-04-07 11:23:19,561] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:23:19,561] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:23:19,563] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run4
[2019-04-07 11:23:20,491] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:23:20,492] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:23:20,495] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run4
[2019-04-07 11:23:38,816] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.0172451e-22 1.7475877e-20 2.6522920e-18 3.2156283e-10 6.2206364e-17
 1.0000000e+00 1.6448055e-10 1.3808644e-12], sum to 1.0000
[2019-04-07 11:23:38,816] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0113
[2019-04-07 11:23:39,020] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.1, 73.5, 184.0, 13.0, 24.0, 24.01438407538299, -0.03072627086813747, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 127800.0000, 
sim time next is 129600.0000, 
raw observation next is [-8.4, 61.0, 157.0, 308.0, 24.0, 23.89580114178438, -0.02197865691218433, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2299168975069252, 0.61, 0.5233333333333333, 0.34033149171270716, 0.5, 0.4913167618153649, 0.4926737810292719, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4362096], dtype=float32), -0.29086587]. 
=============================================
[2019-04-07 11:24:14,134] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-07 11:24:14,135] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:24:14,135] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:24:14,137] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run8
[2019-04-07 11:24:14,183] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:24:14,185] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 11:24:14,186] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:24:14,187] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run8
[2019-04-07 11:24:14,238] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:24:14,240] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run8
[2019-04-07 11:26:23,262] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 11:26:41,497] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.7460 79463814.5229 95.0531
[2019-04-07 11:26:44,143] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 11:26:45,165] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 140000, evaluation results [140000.0, 2782.745986527074, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 11:27:04,808] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0074282e-21 6.0622379e-21 3.3834417e-18 3.4106362e-10 4.7900216e-17
 1.0000000e+00 2.0274642e-10 2.4406701e-12], sum to 1.0000
[2019-04-07 11:27:04,808] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4522
[2019-04-07 11:27:04,860] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 24.0, 23.78039691424004, 0.1439197485961865, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1062000.0000, 
sim time next is 1063800.0000, 
raw observation next is [12.75, 81.5, 0.0, 0.0, 24.0, 23.96931216167453, 0.1668043155457952, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.8157894736842106, 0.815, 0.0, 0.0, 0.5, 0.4974426801395442, 0.5556014385152651, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5348955], dtype=float32), -0.01416361]. 
=============================================
[2019-04-07 11:27:12,743] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.6412125e-24 3.6247878e-23 3.5561193e-21 2.5581868e-11 2.4506017e-19
 1.0000000e+00 5.7319904e-12 1.9338680e-13], sum to 1.0000
[2019-04-07 11:27:12,744] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1991
[2019-04-07 11:27:12,760] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 127.0, 0.0, 24.0, 24.43966252938922, 0.1878565277811722, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1337400.0000, 
sim time next is 1339200.0000, 
raw observation next is [1.1, 92.0, 120.0, 0.0, 24.0, 24.39733510228491, 0.1775684207343826, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.4, 0.0, 0.5, 0.5331112585237424, 0.5591894735781275, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2583326], dtype=float32), 0.8364497]. 
=============================================
[2019-04-07 11:27:56,958] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.6225110e-18 1.2899110e-16 4.3284349e-15 7.2774668e-08 8.3571328e-15
 9.9999988e-01 6.6984458e-09 8.2155766e-10], sum to 1.0000
[2019-04-07 11:27:56,958] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9014
[2019-04-07 11:27:57,309] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 87.0, 81.0, 0.0, 24.0, 23.0311951110985, -0.1197653812321054, 0.0, 1.0, 44807.215389795], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1764000.0000, 
sim time next is 1765800.0000, 
raw observation next is [-2.3, 87.0, 97.0, 0.0, 24.0, 23.08706263831215, -0.08185803907664259, 0.0, 1.0, 50784.95313094659], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.3233333333333333, 0.0, 0.5, 0.4239218865260126, 0.4727139869744525, 0.0, 1.0, 0.24183311014736472], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0881502], dtype=float32), 0.56822777]. 
=============================================
[2019-04-07 11:28:01,033] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7372676e-24 4.2814839e-23 4.4395297e-20 3.2749585e-09 5.2227696e-20
 1.0000000e+00 1.9967462e-11 3.5706668e-12], sum to 1.0000
[2019-04-07 11:28:01,033] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0655
[2019-04-07 11:28:01,281] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.55, 69.0, 0.0, 0.0, 24.0, 23.60461251670856, -0.1245740577996566, 1.0, 1.0, 37262.70052916998], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2226600.0000, 
sim time next is 2228400.0000, 
raw observation next is [-4.6, 70.0, 0.0, 0.0, 24.0, 23.26353406613754, -0.0358054058989911, 0.0, 1.0, 93620.49416623893], 
processed observation next is [1.0, 0.8260869565217391, 0.33518005540166207, 0.7, 0.0, 0.0, 0.5, 0.4386278388447951, 0.4880648647003363, 0.0, 1.0, 0.4458118769820901], 
reward next is 0.8399, 
noisyNet noise sample is [array([-0.01393827], dtype=float32), 0.88139844]. 
=============================================
[2019-04-07 11:28:29,369] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.2344031e-20 6.6188713e-19 4.9491669e-17 1.2972096e-08 1.1894284e-16
 1.0000000e+00 1.9174787e-10 1.7818975e-11], sum to 1.0000
[2019-04-07 11:28:29,369] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3273
[2019-04-07 11:28:29,814] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 24.0, 22.92840606593572, -0.1501986610171353, 1.0, 1.0, 94536.83902135114], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2100600.0000, 
sim time next is 2102400.0000, 
raw observation next is [-7.3, 79.0, 36.5, 18.5, 24.0, 23.49414900012665, -0.1046845016572593, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.26038781163434904, 0.79, 0.12166666666666667, 0.020441988950276244, 0.5, 0.4578457500105542, 0.4651051661142469, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37130663], dtype=float32), 1.5512775]. 
=============================================
[2019-04-07 11:29:14,088] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-07 11:29:14,096] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:29:14,096] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:29:14,106] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run9
[2019-04-07 11:29:14,151] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:29:14,152] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:29:14,165] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 11:29:14,165] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:29:14,167] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run9
[2019-04-07 11:29:14,264] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run9
[2019-04-07 11:29:47,365] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.10238016], dtype=float32), 0.106538855]
[2019-04-07 11:29:47,365] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-8.864674148, 59.45333615999999, 0.0, 0.0, 24.0, 21.59743458393294, -0.5754613014517698, 0.0, 1.0, 46007.17029910055]
[2019-04-07 11:29:47,366] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 11:29:47,367] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [3.26085958e-15 2.14974692e-14 6.87853745e-13 5.97019095e-07
 2.55175134e-12 9.99999285e-01 1.55096387e-07 1.13996075e-08], sampled 0.4688763249268334
[2019-04-07 11:31:24,777] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 11:31:26,876] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.10238016], dtype=float32), 0.106538855]
[2019-04-07 11:31:26,876] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [9.4145799185, 37.04678052, 31.66127148, 524.1194198, 24.0, 23.69575616691463, -0.02738887208329273, 0.0, 1.0, 0.0]
[2019-04-07 11:31:26,876] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 11:31:26,877] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [5.3553476e-17 5.4301064e-16 2.5757793e-14 1.0728722e-07 9.8722571e-14
 9.9999988e-01 2.9612316e-08 1.4172437e-09], sampled 0.3927623320847071
[2019-04-07 11:31:42,941] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 11:31:47,377] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 11:31:48,401] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 160000, evaluation results [160000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 11:32:12,467] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3655120e-17 1.2531666e-15 5.0900778e-14 5.7463641e-08 9.9717448e-14
 1.0000000e+00 2.4341988e-08 1.7065480e-09], sum to 1.0000
[2019-04-07 11:32:12,468] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2313
[2019-04-07 11:32:12,535] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 24.0, 22.74655898721855, -0.2608616401502608, 0.0, 1.0, 41157.08811715681], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3042000.0000, 
sim time next is 3043800.0000, 
raw observation next is [-6.0, 73.5, 0.0, 0.0, 24.0, 22.71540219626197, -0.269698762372358, 0.0, 1.0, 41085.770503366024], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.735, 0.0, 0.0, 0.5, 0.39295018302183077, 0.4101004125425473, 0.0, 1.0, 0.19564652620650488], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09958817], dtype=float32), -1.2025521]. 
=============================================
[2019-04-07 11:32:19,367] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.49121623e-19 5.65831773e-17 2.60661811e-15 5.20328047e-09
 3.54207706e-15 1.00000000e+00 4.51424120e-10 1.15842405e-11], sum to 1.0000
[2019-04-07 11:32:19,367] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8924
[2019-04-07 11:32:19,444] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 55.0, 0.0, 0.0, 24.0, 23.50157547346836, -0.05083542437335178, 0.0, 1.0, 41114.63710087224], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3623400.0000, 
sim time next is 3625200.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 24.0, 23.49431626251871, -0.06210897046380198, 0.0, 1.0, 35993.003268215114], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.6, 0.0, 0.0, 0.5, 0.45785968854322573, 0.4792970098453993, 0.0, 1.0, 0.1713952536581672], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08544441], dtype=float32), 0.80357575]. 
=============================================
[2019-04-07 11:32:33,499] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.1134847e-25 2.2604737e-23 1.4862849e-20 8.9609344e-12 1.5460284e-20
 1.0000000e+00 9.8998171e-12 4.1185344e-14], sum to 1.0000
[2019-04-07 11:32:33,501] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0071
[2019-04-07 11:32:33,535] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 75.0, 0.0, 0.0, 24.0, 24.12993212360826, 0.190527028984341, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3533400.0000, 
sim time next is 3535200.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 24.0, 24.0415160044497, 0.1580811775646742, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.78, 0.0, 0.0, 0.5, 0.503459667037475, 0.5526937258548914, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6764623], dtype=float32), 0.032598283]. 
=============================================
[2019-04-07 11:32:34,703] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3984512e-25 5.8892253e-23 8.0901366e-21 6.5303452e-10 1.4810899e-19
 1.0000000e+00 2.0803298e-12 4.1464841e-14], sum to 1.0000
[2019-04-07 11:32:34,703] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8983
[2019-04-07 11:32:34,741] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 24.0, 23.69387094667335, 0.1555247082687069, 0.0, 1.0, 92552.72687123304], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3445200.0000, 
sim time next is 3447000.0000, 
raw observation next is [1.0, 82.5, 0.0, 0.0, 24.0, 24.06286686593406, 0.167943275060719, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.825, 0.0, 0.0, 0.5, 0.5052389054945049, 0.5559810916869063, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36213952], dtype=float32), -0.60002804]. 
=============================================
[2019-04-07 11:32:34,751] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[100.63602 ]
 [101.77045 ]
 [101.57308 ]
 [104.35679 ]
 [104.314606]], R is [[101.19736481]
 [101.03038025]
 [100.598526  ]
 [100.59254456]
 [100.41017914]].
[2019-04-07 11:32:36,881] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0025610e-26 1.0741822e-23 3.6957079e-20 5.0186608e-11 1.1527301e-20
 1.0000000e+00 9.1424101e-13 2.9396190e-14], sum to 1.0000
[2019-04-07 11:32:36,881] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3553
[2019-04-07 11:32:36,951] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 63.0, 0.0, 0.0, 24.0, 23.6830575057227, 0.07125068543190688, 0.0, 1.0, 6251.7213142617065], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3540600.0000, 
sim time next is 3542400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.55662533278679, 0.02750398712864148, 0.0, 1.0, 16764.518396380958], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.5, 0.4630521110655659, 0.5091679957095472, 0.0, 1.0, 0.07983103998276647], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.749943], dtype=float32), 1.0244654]. 
=============================================
[2019-04-07 11:32:40,281] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2579888e-18 2.1423690e-17 1.6633774e-15 1.7316058e-08 1.0600087e-15
 1.0000000e+00 5.8806466e-09 2.8866409e-10], sum to 1.0000
[2019-04-07 11:32:40,282] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5964
[2019-04-07 11:32:40,318] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 24.0, 23.46466124095561, -0.09486545974680788, 0.0, 1.0, 52063.52050655752], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4251600.0000, 
sim time next is 4253400.0000, 
raw observation next is [3.0, 47.0, 0.0, 0.0, 24.0, 23.51710835574436, -0.1029415875197667, 0.0, 1.0, 20854.764484328367], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.47, 0.0, 0.0, 0.5, 0.45975902964536325, 0.4656861374934111, 0.0, 1.0, 0.09930840230632555], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0937667], dtype=float32), 0.8311486]. 
=============================================
[2019-04-07 11:32:45,939] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4494607e-27 1.1817435e-25 1.3237741e-23 9.5166849e-14 3.4261655e-22
 1.0000000e+00 4.7396288e-13 3.7910878e-15], sum to 1.0000
[2019-04-07 11:32:45,939] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6149
[2019-04-07 11:32:45,973] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 24.0, 24.1430696596327, 0.1567944244770281, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3780000.0000, 
sim time next is 3781800.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 24.0, 24.29359319215518, 0.133717956311152, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.5, 0.5244660993462649, 0.5445726521037173, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3476335], dtype=float32), -0.38176218]. 
=============================================
[2019-04-07 11:32:48,937] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.0307318e-28 3.2554496e-26 1.9349401e-22 5.8920702e-11 2.5261131e-22
 1.0000000e+00 6.1850529e-12 2.4991014e-16], sum to 1.0000
[2019-04-07 11:32:48,938] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9272
[2019-04-07 11:32:49,000] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 48.0, 106.0, 782.0, 24.0, 25.33911428611963, 0.3506447744745152, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3853800.0000, 
sim time next is 3855600.0000, 
raw observation next is [2.0, 48.0, 96.5, 749.5, 24.0, 25.50284671896991, 0.2828453993882348, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 0.48, 0.32166666666666666, 0.8281767955801105, 0.5, 0.6252372265808258, 0.5942817997960782, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.711563], dtype=float32), -0.5966589]. 
=============================================
[2019-04-07 11:33:00,127] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0082812e-23 4.1459588e-23 3.5142487e-20 4.5770880e-11 9.7539256e-19
 1.0000000e+00 1.6710348e-11 5.3387557e-13], sum to 1.0000
[2019-04-07 11:33:00,127] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4954
[2019-04-07 11:33:00,258] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 31.5, 0.0, 24.0, 23.41191643833383, -0.06062636531277044, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4694400.0000, 
sim time next is 4696200.0000, 
raw observation next is [0.0, 92.0, 63.0, 0.0, 24.0, 23.8080752891234, 0.0389132796347401, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.21, 0.0, 0.5, 0.4840062740936168, 0.51297109321158, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8546194], dtype=float32), 0.37126213]. 
=============================================
[2019-04-07 11:33:07,655] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8112915e-22 2.1233910e-21 1.6400613e-19 5.7352006e-10 2.8001821e-18
 1.0000000e+00 1.0579486e-11 1.8555131e-13], sum to 1.0000
[2019-04-07 11:33:07,655] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4383
[2019-04-07 11:33:07,695] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 68.0, 0.0, 0.0, 24.0, 23.51374412911485, 0.09179210675736947, 0.0, 1.0, 140085.04960495917], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4428000.0000, 
sim time next is 4429800.0000, 
raw observation next is [2.5, 74.0, 0.0, 0.0, 24.0, 23.75604281505299, 0.09950483280539431, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.5318559556786704, 0.74, 0.0, 0.0, 0.5, 0.4796702345877491, 0.5331682776017981, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4213308], dtype=float32), -1.3960139]. 
=============================================
[2019-04-07 11:33:11,335] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:33:11,335] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:33:11,337] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run5
[2019-04-07 11:33:22,864] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:33:22,865] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:33:22,867] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run5
[2019-04-07 11:33:25,821] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:33:25,822] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:33:25,823] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run5
[2019-04-07 11:33:36,501] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:33:36,501] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:33:36,503] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run5
[2019-04-07 11:33:38,394] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-07 11:33:38,396] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:33:38,397] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:33:38,397] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:33:38,398] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:33:38,399] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run10
[2019-04-07 11:33:38,397] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 11:33:38,444] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:33:38,445] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run10
[2019-04-07 11:33:38,443] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run10
[2019-04-07 11:33:46,998] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.10515194], dtype=float32), 0.110400215]
[2019-04-07 11:33:46,999] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [5.6677790455, 90.05760602000001, 0.0, 0.0, 24.0, 20.88088047728078, -0.6565069021240018, 0.0, 1.0, 40845.458436703375]
[2019-04-07 11:33:46,999] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 11:33:47,000] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [2.1849808e-18 1.7772793e-17 1.5363376e-15 1.5181950e-08 7.3914526e-15
 1.0000000e+00 4.2410626e-09 2.0363974e-10], sampled 0.39047939198544834
[2019-04-07 11:35:08,015] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.10515194], dtype=float32), 0.110400215]
[2019-04-07 11:35:08,015] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-3.2750482295, 85.566569365, 0.0, 0.0, 24.0, 23.08358385775036, -0.09391892684288523, 0.0, 1.0, 138847.58960874716]
[2019-04-07 11:35:08,015] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 11:35:08,016] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [2.4114744e-21 3.9786428e-20 5.4688922e-18 1.0944491e-09 4.1405664e-17
 1.0000000e+00 2.1268523e-10 6.0602309e-12], sampled 0.8192801917310921
[2019-04-07 11:35:42,151] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2788.9052 70927233.8163 166.2180
[2019-04-07 11:35:59,644] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 11:36:03,769] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.2164 83805026.4189 32.8860
[2019-04-07 11:36:04,791] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 180000, evaluation results [180000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2788.9051760082016, 70927233.8162715, 166.21801628696863, 2785.216401892823, 83805026.41892925, 32.88598103848735]
[2019-04-07 11:36:06,413] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:36:06,413] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:36:06,415] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run5
[2019-04-07 11:36:08,189] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:36:08,189] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:36:08,191] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run5
[2019-04-07 11:36:10,249] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:36:10,249] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:36:10,251] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run5
[2019-04-07 11:36:11,379] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:36:11,379] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:36:11,382] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run5
[2019-04-07 11:36:11,733] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:36:11,733] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:36:11,735] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run5
[2019-04-07 11:36:12,896] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1469571e-21 1.6845027e-20 6.4992288e-18 5.3540165e-11 2.7986458e-17
 1.0000000e+00 2.1904560e-10 8.4806606e-12], sum to 1.0000
[2019-04-07 11:36:12,896] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1203
[2019-04-07 11:36:12,959] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 24.0, 23.59064672093353, 0.000891876713436171, 0.0, 1.0, 37035.57988488432], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5022000.0000, 
sim time next is 5023800.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 24.0, 23.6425667624127, -0.01015778678812672, 0.0, 1.0, 26815.43030084703], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.55, 0.0, 0.0, 0.5, 0.4702138968677249, 0.49661407107062444, 0.0, 1.0, 0.1276925252421287], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06384221], dtype=float32), -0.69335103]. 
=============================================
[2019-04-07 11:36:13,446] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:36:13,446] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:36:13,449] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run5
[2019-04-07 11:36:14,037] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.9146202e-19 1.3161095e-17 4.4288168e-15 1.5952665e-08 1.8352314e-14
 1.0000000e+00 2.4372278e-08 2.3291413e-10], sum to 1.0000
[2019-04-07 11:36:14,037] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3926
[2019-04-07 11:36:14,077] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.45, 94.5, 0.0, 0.0, 24.0, 20.75195243092939, -0.6874634758983964, 0.0, 1.0, 41825.57035195166], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 12600.0000, 
sim time next is 14400.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 24.0, 20.88710572992931, -0.6666360283300962, 0.0, 1.0, 41360.04505425665], 
processed observation next is [0.0, 0.17391304347826086, 0.6759002770083103, 0.93, 0.0, 0.0, 0.5, 0.24059214416077582, 0.2777879905566346, 0.0, 1.0, 0.19695259549646021], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0930016], dtype=float32), -1.251699]. 
=============================================
[2019-04-07 11:36:14,355] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:36:14,355] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:36:14,357] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run5
[2019-04-07 11:36:15,669] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:36:15,669] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:36:15,671] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run5
[2019-04-07 11:36:16,390] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:36:16,390] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:36:16,392] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run5
[2019-04-07 11:36:17,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1816696e-22 1.0118513e-19 8.0236659e-19 9.7070885e-10 1.9504558e-17
 1.0000000e+00 6.8498018e-10 1.8703428e-11], sum to 1.0000
[2019-04-07 11:36:17,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0300
[2019-04-07 11:36:17,870] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 24.0, 22.82690701781876, -0.2047717574014153, 0.0, 1.0, 46654.26369172938], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 167400.0000, 
sim time next is 169200.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 24.0, 22.73374998847608, -0.229197639983223, 0.0, 1.0, 46337.35280507108], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.5, 0.39447916570634006, 0.42360078667225903, 0.0, 1.0, 0.22065406097652895], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8264376], dtype=float32), 0.9667758]. 
=============================================
[2019-04-07 11:36:18,041] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:36:18,041] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:36:18,043] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run5
[2019-04-07 11:36:19,069] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:36:19,069] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:36:19,071] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run5
[2019-04-07 11:36:19,593] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:36:19,593] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:36:19,614] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run5
[2019-04-07 11:36:33,197] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8247908e-20 2.5807301e-19 9.9534825e-18 2.4082256e-09 5.8739670e-16
 1.0000000e+00 3.5318048e-10 4.8395440e-11], sum to 1.0000
[2019-04-07 11:36:33,197] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8333
[2019-04-07 11:36:33,288] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 24.0, 22.05595623601171, -0.4032687297377799, 0.0, 1.0, 45012.99443205605], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 187200.0000, 
sim time next is 189000.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 24.0, 21.97778284810968, -0.4322752498983053, 0.0, 1.0, 45091.037072850915], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.5, 0.33148190400913996, 0.3559082500338982, 0.0, 1.0, 0.21471922415643294], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1233974], dtype=float32), 1.0008857]. 
=============================================
[2019-04-07 11:36:33,293] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[84.627815]
 [84.42778 ]
 [84.27124 ]
 [85.19813 ]
 [86.42944 ]], R is [[84.84174347]
 [84.99332428]
 [85.14339447]
 [85.29196167]
 [85.43904114]].
[2019-04-07 11:36:34,502] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4913122e-24 1.1430903e-23 7.2861138e-21 3.3959753e-12 1.4959784e-19
 1.0000000e+00 2.5358340e-12 5.1543797e-15], sum to 1.0000
[2019-04-07 11:36:34,502] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8088
[2019-04-07 11:36:34,687] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.1, 61.0, 130.0, 603.0, 24.0, 23.64351888213438, -0.03606688296584665, 1.0, 1.0, 65675.31588276669], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 131400.0000, 
sim time next is 133200.0000, 
raw observation next is [-7.8, 61.0, 134.5, 543.5, 24.0, 24.00963268211738, 0.05972549207653747, 1.0, 1.0, 49176.2538779974], 
processed observation next is [1.0, 0.5652173913043478, 0.24653739612188366, 0.61, 0.4483333333333333, 0.6005524861878453, 0.5, 0.5008027235097817, 0.5199084973588458, 1.0, 1.0, 0.23417263751427334], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39691356], dtype=float32), -1.3141739]. 
=============================================
[2019-04-07 11:36:43,919] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.4612111e-25 1.1889031e-22 3.4707901e-20 1.1486793e-11 1.2935555e-19
 1.0000000e+00 3.5580013e-12 1.5394103e-13], sum to 1.0000
[2019-04-07 11:36:43,920] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8072
[2019-04-07 11:36:44,341] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 24.0, 23.66570872305907, -0.2165194267953542, 1.0, 1.0, 26659.566164876054], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 235800.0000, 
sim time next is 237600.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 24.0, 22.84568918857346, -0.1435536229328979, 1.0, 1.0, 147938.74978529822], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.5, 0.4038074323811216, 0.45214879235570066, 1.0, 1.0, 0.7044702370728486], 
reward next is 0.5812, 
noisyNet noise sample is [array([-0.6359088], dtype=float32), -1.3934203]. 
=============================================
[2019-04-07 11:37:01,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3727948e-27 2.5286833e-25 2.2965538e-22 3.1126672e-13 1.4276507e-21
 1.0000000e+00 4.1444463e-13 2.9080826e-14], sum to 1.0000
[2019-04-07 11:37:01,744] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8651
[2019-04-07 11:37:01,880] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 24.0, 22.94991468450925, -0.2251486106711371, 1.0, 1.0, 40130.09982181786], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 504000.0000, 
sim time next is 505800.0000, 
raw observation next is [1.35, 96.0, 0.0, 0.0, 24.0, 22.94211624666031, -0.1958110431879486, 0.0, 1.0, 105541.32121692832], 
processed observation next is [1.0, 0.8695652173913043, 0.5000000000000001, 0.96, 0.0, 0.0, 0.5, 0.4118430205550257, 0.43472965227068383, 0.0, 1.0, 0.5025777200806111], 
reward next is 0.7831, 
noisyNet noise sample is [array([-0.10846148], dtype=float32), 0.89899856]. 
=============================================
[2019-04-07 11:37:06,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1511210e-23 1.2228480e-24 4.5972582e-21 7.5457952e-11 9.1381764e-20
 1.0000000e+00 1.1070877e-10 4.0329030e-13], sum to 1.0000
[2019-04-07 11:37:06,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3997
[2019-04-07 11:37:06,253] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8, 90.0, 0.0, 0.0, 24.0, 23.10844893952315, -0.212792559397309, 1.0, 1.0, 61079.6895153583], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 498600.0000, 
sim time next is 500400.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 24.0, 23.12329228578083, -0.2412788551868757, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.5, 0.42694102381506926, 0.4195737149377081, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.56384087], dtype=float32), -0.17390895]. 
=============================================
[2019-04-07 11:37:15,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9928900e-18 2.3888631e-18 7.8260984e-16 4.1506350e-08 2.0589391e-15
 1.0000000e+00 1.3255470e-09 2.0042525e-10], sum to 1.0000
[2019-04-07 11:37:15,526] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6388
[2019-04-07 11:37:15,691] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 65.0, 117.5, 25.5, 24.0, 23.1182338409543, -0.2224823914875851, 0.0, 1.0, 8419.94860832634], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 640800.0000, 
sim time next is 642600.0000, 
raw observation next is [-3.65, 65.0, 100.0, 0.0, 24.0, 22.96774431564786, -0.2131352758391564, 0.0, 1.0, 79256.2155101598], 
processed observation next is [0.0, 0.43478260869565216, 0.3614958448753463, 0.65, 0.3333333333333333, 0.0, 0.5, 0.413978692970655, 0.4289549080536145, 0.0, 1.0, 0.37741055004838003], 
reward next is 0.9083, 
noisyNet noise sample is [array([-0.12228975], dtype=float32), -0.44175747]. 
=============================================
[2019-04-07 11:37:36,648] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.4320629e-27 1.0013119e-25 2.2240970e-22 6.8121116e-13 7.3687631e-22
 1.0000000e+00 5.3782581e-13 1.6363839e-15], sum to 1.0000
[2019-04-07 11:37:36,650] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1349
[2019-04-07 11:37:36,757] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 45.0, 0.0, 24.0, 24.21474834672013, 0.1602563040673092, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1330200.0000, 
sim time next is 1332000.0000, 
raw observation next is [0.5, 92.0, 73.5, 0.0, 24.0, 24.28204161477503, 0.1636206047246589, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4764542936288089, 0.92, 0.245, 0.0, 0.5, 0.523503467897919, 0.5545402015748863, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2216172], dtype=float32), -0.05438925]. 
=============================================
[2019-04-07 11:37:36,819] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[109.231   ]
 [107.48968 ]
 [106.152725]
 [104.29636 ]
 [103.623856]], R is [[109.56027222]
 [109.46466827]
 [109.37002563]
 [109.27632904]
 [109.18356323]].
[2019-04-07 11:37:46,245] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5095239e-27 5.1370160e-27 4.2533701e-23 4.9307379e-13 1.8349090e-22
 1.0000000e+00 6.4215980e-11 9.5342486e-15], sum to 1.0000
[2019-04-07 11:37:46,246] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8453
[2019-04-07 11:37:46,444] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.3, 56.0, 176.0, 158.5, 24.0, 25.37772330343361, 0.4861287938667246, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1083600.0000, 
sim time next is 1085400.0000, 
raw observation next is [18.55, 55.0, 171.0, 0.0, 24.0, 25.81695306432577, 0.5554897801688282, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.976454293628809, 0.55, 0.57, 0.0, 0.5, 0.6514127553604808, 0.6851632600562761, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44399813], dtype=float32), -0.42636567]. 
=============================================
[2019-04-07 11:38:40,029] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0690986e-22 8.4774568e-20 1.1870384e-18 4.1842720e-09 2.2116982e-18
 1.0000000e+00 4.6573345e-10 1.7337040e-12], sum to 1.0000
[2019-04-07 11:38:40,029] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7533
[2019-04-07 11:38:40,511] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 24.0, 22.58097063486317, -0.2453294219875967, 1.0, 1.0, 150367.40320334263], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1927800.0000, 
sim time next is 1929600.0000, 
raw observation next is [-9.5, 91.0, 17.5, 11.0, 24.0, 23.62909488802488, -0.1723413572741507, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.1994459833795014, 0.91, 0.058333333333333334, 0.012154696132596685, 0.5, 0.46909124066874003, 0.4425528809086164, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.596791], dtype=float32), 0.9848813]. 
=============================================
[2019-04-07 11:38:55,532] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.4853796e-27 6.4861283e-25 4.2815023e-22 3.5171649e-13 1.9229544e-21
 1.0000000e+00 9.5493952e-14 2.5643748e-16], sum to 1.0000
[2019-04-07 11:38:55,532] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4134
[2019-04-07 11:38:55,760] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.2, 69.5, 35.0, 0.0, 24.0, 23.87583811925877, -0.05457312781469868, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2219400.0000, 
sim time next is 2221200.0000, 
raw observation next is [-4.5, 71.0, 19.0, 0.0, 24.0, 23.42648419769237, -0.1193391772750142, 1.0, 1.0, 57443.02557304163], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.71, 0.06333333333333334, 0.0, 0.5, 0.4522070164743643, 0.4602202742416619, 1.0, 1.0, 0.27353821701448394], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5242177], dtype=float32), -0.25388974]. 
=============================================
[2019-04-07 11:39:04,732] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-07 11:39:04,733] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:39:04,733] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:39:04,735] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run11
[2019-04-07 11:39:04,774] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:39:04,774] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:39:04,776] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run11
[2019-04-07 11:39:04,805] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 11:39:04,805] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:39:04,807] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run11
[2019-04-07 11:41:07,869] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.10690115], dtype=float32), 0.11313458]
[2019-04-07 11:41:07,869] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-7.368438232, 58.2832944, 73.33375681, 621.6898718, 24.0, 23.46130539027762, -0.03191514942246786, 0.0, 1.0, 11211.834166337396]
[2019-04-07 11:41:07,869] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 11:41:07,870] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [8.4118737e-19 8.2332388e-18 6.3082440e-16 1.1127985e-08 2.9845490e-15
 1.0000000e+00 3.1244092e-09 1.3107282e-10], sampled 0.6665074416345048
[2019-04-07 11:41:10,127] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 11:41:26,733] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 11:41:29,640] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 11:41:30,662] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 200000, evaluation results [200000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 11:41:37,474] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5712415e-19 8.9640653e-19 1.0402906e-17 4.9369495e-09 6.4647169e-16
 1.0000000e+00 5.4858496e-10 1.5228163e-10], sum to 1.0000
[2019-04-07 11:41:37,474] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0352
[2019-04-07 11:41:37,548] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.75, 42.5, 0.0, 0.0, 24.0, 23.35192588483691, -0.1520170195201705, 0.0, 1.0, 44840.60625019503], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2413800.0000, 
sim time next is 2415600.0000, 
raw observation next is [-5.0, 41.0, 0.0, 0.0, 24.0, 23.26194561829645, -0.1699740195126907, 0.0, 1.0, 44640.93519792185], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.41, 0.0, 0.0, 0.5, 0.4384954681913709, 0.4433419934957698, 0.0, 1.0, 0.21257588189486595], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8545813], dtype=float32), -0.11208651]. 
=============================================
[2019-04-07 11:42:14,128] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0989165e-25 9.2918809e-23 1.6710551e-21 8.9673505e-11 3.0850805e-18
 1.0000000e+00 1.4949347e-11 1.9505305e-13], sum to 1.0000
[2019-04-07 11:42:14,129] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2060
[2019-04-07 11:42:14,235] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 54.0, 234.5, 159.0, 24.0, 23.97398308538066, -0.03738800411536521, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2635200.0000, 
sim time next is 2637000.0000, 
raw observation next is [-1.45, 50.5, 245.0, 147.0, 24.0, 23.47771350563491, -0.1116922395194194, 1.0, 1.0, 44769.64231455873], 
processed observation next is [1.0, 0.5217391304347826, 0.422437673130194, 0.505, 0.8166666666666667, 0.16243093922651933, 0.5, 0.4564761254695758, 0.4627692534935269, 1.0, 1.0, 0.21318877292647015], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.73747355], dtype=float32), 1.5225511]. 
=============================================
[2019-04-07 11:42:14,259] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[111.46567]
 [110.67101]
 [109.5409 ]
 [108.95753]
 [107.19171]], R is [[112.16436768]
 [112.04272461]
 [111.92230225]
 [111.8030777 ]
 [111.68505096]].
[2019-04-07 11:42:17,278] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0534235e-24 5.3334128e-24 2.9765257e-20 1.1977775e-11 9.9423966e-19
 1.0000000e+00 8.0303238e-12 1.9278272e-14], sum to 1.0000
[2019-04-07 11:42:17,279] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9988
[2019-04-07 11:42:17,319] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 24.0, 23.53254448762437, -0.07454187443582909, 0.0, 1.0, 18754.856441948], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2854800.0000, 
sim time next is 2856600.0000, 
raw observation next is [1.0, 75.5, 0.0, 0.0, 24.0, 23.49036682875298, -0.1033610798407556, 0.0, 1.0, 39330.01405666183], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.755, 0.0, 0.0, 0.5, 0.45753056906274825, 0.4655463067197481, 0.0, 1.0, 0.1872857812221992], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.87086785], dtype=float32), 0.07028175]. 
=============================================
[2019-04-07 11:42:25,540] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5910176e-24 3.7718130e-24 3.9160630e-20 3.3405799e-11 8.9885735e-21
 1.0000000e+00 9.9402708e-12 8.4986583e-14], sum to 1.0000
[2019-04-07 11:42:25,540] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9462
[2019-04-07 11:42:25,792] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 24.0, 23.36393709290072, -0.1091803487354661, 1.0, 1.0, 20005.06642049585], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2878200.0000, 
sim time next is 2880000.0000, 
raw observation next is [2.0, 93.0, 52.5, 91.5, 24.0, 23.5160716166138, -0.0986416915560006, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.518005540166205, 0.93, 0.175, 0.1011049723756906, 0.5, 0.45967263471781666, 0.4671194361479998, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04665221], dtype=float32), -0.43769076]. 
=============================================
[2019-04-07 11:42:25,830] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[105.13242 ]
 [103.23099 ]
 [102.585365]
 [102.22677 ]
 [101.15803 ]], R is [[106.55117798]
 [106.48566437]
 [106.42080688]
 [106.33913422]
 [106.21891785]].
[2019-04-07 11:42:48,840] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5873287e-20 2.7593887e-18 2.6921395e-17 7.2681452e-08 1.5115663e-16
 9.9999988e-01 4.9565241e-10 7.2697702e-11], sum to 1.0000
[2019-04-07 11:42:48,846] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8215
[2019-04-07 11:42:48,900] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 40.5, 14.0, 142.0, 24.0, 23.56971555780021, 0.002561733938309441, 0.0, 1.0, 35763.082320935064], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3605400.0000, 
sim time next is 3607200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 24.0, 23.45861450755944, -0.03178151531956986, 0.0, 1.0, 23825.51052141793], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 0.5, 0.4548845422966199, 0.4894061615601433, 0.0, 1.0, 0.11345481200675206], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.50196517], dtype=float32), -0.18076703]. 
=============================================
[2019-04-07 11:42:49,281] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.2537066e-25 1.6812775e-23 5.7952734e-21 6.3961107e-11 1.7043925e-19
 1.0000000e+00 3.3502662e-11 6.2691958e-14], sum to 1.0000
[2019-04-07 11:42:49,282] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4660
[2019-04-07 11:42:49,331] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 48.0, 104.0, 711.0, 24.0, 24.86723922850283, 0.1955272550496857, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3405600.0000, 
sim time next is 3407400.0000, 
raw observation next is [2.5, 48.5, 109.0, 764.0, 24.0, 24.88230762084509, 0.2208015222862647, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5318559556786704, 0.485, 0.36333333333333334, 0.8441988950276244, 0.5, 0.5735256350704242, 0.5736005074287549, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29225522], dtype=float32), 0.632781]. 
=============================================
[2019-04-07 11:42:59,726] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7070371e-20 5.2317028e-20 3.7551464e-16 2.3049949e-09 8.6911340e-16
 1.0000000e+00 3.8990752e-10 2.7750642e-11], sum to 1.0000
[2019-04-07 11:42:59,727] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9783
[2019-04-07 11:42:59,825] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 24.0, 23.17394176972111, -0.1247406691210969, 0.0, 1.0, 16533.301955401326], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3697200.0000, 
sim time next is 3699000.0000, 
raw observation next is [3.5, 61.0, 0.0, 0.0, 24.0, 23.03588558510716, -0.1023201232851954, 0.0, 1.0, 148925.8238624904], 
processed observation next is [0.0, 0.8260869565217391, 0.5595567867036012, 0.61, 0.0, 0.0, 0.5, 0.4196571320922633, 0.4658932922382682, 0.0, 1.0, 0.7091705898213828], 
reward next is 0.5765, 
noisyNet noise sample is [array([0.4973379], dtype=float32), 0.6030747]. 
=============================================
[2019-04-07 11:42:59,832] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[86.14548]
 [86.83493]
 [87.83673]
 [88.7907 ]
 [89.48683]], R is [[86.35974121]
 [86.49614716]
 [86.63118744]
 [86.76487732]
 [86.89723206]].
[2019-04-07 11:43:39,757] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.4137662e-24 2.1688898e-23 4.9465924e-20 2.1301559e-11 2.1901296e-18
 1.0000000e+00 7.3814926e-11 6.1363555e-13], sum to 1.0000
[2019-04-07 11:43:39,757] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4747
[2019-04-07 11:43:39,910] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 26.0, 0.0, 0.0, 24.0, 24.43442831641295, 0.164908531455411, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4039200.0000, 
sim time next is 4041000.0000, 
raw observation next is [-3.5, 28.5, 0.0, 0.0, 24.0, 23.38696718685491, 0.02532778536215228, 1.0, 1.0, 64018.27805267187], 
processed observation next is [1.0, 0.782608695652174, 0.36565096952908593, 0.285, 0.0, 0.0, 0.5, 0.4489139322379092, 0.5084425951207174, 1.0, 1.0, 0.3048489431079613], 
reward next is 0.9809, 
noisyNet noise sample is [array([-1.1998653], dtype=float32), 0.3962369]. 
=============================================
[2019-04-07 11:43:39,921] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[104.94135]
 [106.23997]
 [107.20771]
 [107.69578]
 [107.77888]], R is [[104.01708221]
 [103.97691345]
 [103.93714905]
 [103.89778137]
 [103.8588028 ]].
[2019-04-07 11:43:51,266] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-07 11:43:51,273] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:43:51,274] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:43:51,286] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run12
[2019-04-07 11:43:51,314] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:43:51,314] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:43:51,317] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run12
[2019-04-07 11:43:51,344] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 11:43:51,346] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:43:51,348] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run12
[2019-04-07 11:44:55,747] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.10824652], dtype=float32), 0.115456216]
[2019-04-07 11:44:55,748] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [0.0, 95.0, 0.0, 0.0, 24.0, 23.61319204302352, 0.08875473919859052, 0.0, 1.0, 19111.225192184957]
[2019-04-07 11:44:55,748] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 11:44:55,749] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [1.6635292e-23 3.2533209e-22 1.1036629e-19 7.6391560e-11 6.1904005e-19
 1.0000000e+00 1.8347162e-11 3.5815033e-13], sampled 0.5804619677309212
[2019-04-07 11:45:50,902] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 11:46:05,129] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.10824652], dtype=float32), 0.115456216]
[2019-04-07 11:46:05,130] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [1.3962663825, 67.22286871, 0.0, 0.0, 24.0, 23.5045388088589, -0.01687384764273258, 0.0, 1.0, 29817.39657134331]
[2019-04-07 11:46:05,130] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 11:46:05,131] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [5.5478059e-21 8.4472738e-20 1.5433496e-17 1.0887962e-09 6.2084041e-17
 1.0000000e+00 2.5640745e-10 8.5534540e-12], sampled 0.1016243628178517
[2019-04-07 11:46:07,328] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 11:46:12,458] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 11:46:13,480] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 220000, evaluation results [220000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 11:46:16,801] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:16,801] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:16,807] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run6
[2019-04-07 11:46:22,633] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0618661e-23 6.0874679e-23 4.9501741e-22 1.6968836e-11 4.5804031e-20
 1.0000000e+00 1.1105805e-12 1.0625421e-13], sum to 1.0000
[2019-04-07 11:46:22,635] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2942
[2019-04-07 11:46:22,706] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 24.0, 23.42420934503762, -0.03660092325616358, 0.0, 1.0, 37976.83529639135], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4516200.0000, 
sim time next is 4518000.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 24.0, 23.51794952147753, -0.03249194960935935, 0.0, 1.0, 26483.925061323203], 
processed observation next is [1.0, 0.30434782608695654, 0.4349030470914128, 0.71, 0.0, 0.0, 0.5, 0.4598291267897941, 0.4891693501302135, 0.0, 1.0, 0.12611392886344383], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46109504], dtype=float32), 0.60207623]. 
=============================================
[2019-04-07 11:46:22,730] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[99.40527 ]
 [98.97302 ]
 [98.983475]
 [99.26731 ]
 [99.085175]], R is [[99.26985168]
 [99.27715302]
 [99.28438568]
 [99.29154205]
 [99.29862976]].
[2019-04-07 11:46:29,011] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:29,011] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:29,015] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run6
[2019-04-07 11:46:29,059] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:29,059] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:29,063] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run6
[2019-04-07 11:46:29,698] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2416049e-24 3.9867019e-24 1.3722842e-19 2.6739350e-11 4.8127992e-20
 1.0000000e+00 2.1149046e-12 6.9265833e-13], sum to 1.0000
[2019-04-07 11:46:29,698] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0270
[2019-04-07 11:46:29,747] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 24.0, 23.96636150396602, 0.061107199277527, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4687200.0000, 
sim time next is 4689000.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 24.0, 23.79873286771944, 0.01878627527532266, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 0.5, 0.48322773897662, 0.506262091758441, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3811386], dtype=float32), 0.2851024]. 
=============================================
[2019-04-07 11:46:29,751] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[106.25391]
 [104.84598]
 [103.59056]
 [101.76422]
 [100.97284]], R is [[107.85675049]
 [107.77818298]
 [107.40891266]
 [107.33482361]
 [107.26147461]].
[2019-04-07 11:46:34,565] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:34,566] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:34,569] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run6
[2019-04-07 11:46:36,709] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4439024e-19 5.0870586e-18 1.5236601e-16 9.0931440e-09 2.9449095e-15
 1.0000000e+00 2.4071509e-09 9.1441510e-10], sum to 1.0000
[2019-04-07 11:46:36,709] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3737
[2019-04-07 11:46:36,779] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 24.0, 20.58943728148884, -0.7294043441150452, 0.0, 1.0, 42405.57484272158], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 10800.0000, 
sim time next is 12600.0000, 
raw observation next is [7.45, 94.5, 0.0, 0.0, 24.0, 20.75195243092939, -0.6874634758983964, 0.0, 1.0, 41825.57035195166], 
processed observation next is [0.0, 0.13043478260869565, 0.6689750692520776, 0.945, 0.0, 0.0, 0.5, 0.22932936924411576, 0.27084550803386787, 0.0, 1.0, 0.19916938262834125], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1895734], dtype=float32), 0.07262171]. 
=============================================
[2019-04-07 11:46:41,296] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:41,296] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:41,301] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run6
[2019-04-07 11:46:41,452] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:41,452] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:41,461] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run6
[2019-04-07 11:46:41,961] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.0858446e-23 1.1112697e-22 5.8270428e-20 7.3701151e-10 2.8801453e-18
 1.0000000e+00 4.5832557e-11 1.1281600e-12], sum to 1.0000
[2019-04-07 11:46:41,961] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9640
[2019-04-07 11:46:42,039] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.4, 95.5, 0.0, 0.0, 24.0, 23.28933781553005, -0.08920553165886728, 0.0, 1.0, 41424.42002047473], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 81000.0000, 
sim time next is 82800.0000, 
raw observation next is [0.3, 95.0, 0.0, 0.0, 24.0, 23.25844786066373, -0.09468647211922217, 0.0, 1.0, 41211.496349879504], 
processed observation next is [0.0, 1.0, 0.47091412742382277, 0.95, 0.0, 0.0, 0.5, 0.4382039883886441, 0.46843784262692595, 0.0, 1.0, 0.19624522071371192], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7305569], dtype=float32), 0.7607569]. 
=============================================
[2019-04-07 11:46:43,549] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.8347949e-23 2.0234727e-22 1.3245200e-18 2.8798658e-10 9.6079448e-19
 1.0000000e+00 1.0938282e-11 7.3996695e-13], sum to 1.0000
[2019-04-07 11:46:43,549] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6623
[2019-04-07 11:46:43,604] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 48.0, 0.0, 0.0, 24.0, 23.41282814050346, -0.1551983991372449, 0.0, 1.0, 54342.83162408599], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4944600.0000, 
sim time next is 4946400.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 24.0, 23.33045460414173, -0.1482932421867341, 0.0, 1.0, 55236.71890661842], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.5, 0.444204550345144, 0.45056891927108866, 0.0, 1.0, 0.26303199479342104], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.487554], dtype=float32), 1.0715084]. 
=============================================
[2019-04-07 11:46:44,357] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:44,357] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:44,380] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run6
[2019-04-07 11:46:46,729] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:46,729] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:46,733] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run6
[2019-04-07 11:46:46,754] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:46,754] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:46,758] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run6
[2019-04-07 11:46:48,309] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:48,309] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:48,313] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run6
[2019-04-07 11:46:48,505] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:48,505] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:48,510] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run6
[2019-04-07 11:46:48,771] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00189461 0.00135599 0.00264263 0.0376588  0.00515738 0.9090029
 0.02882804 0.01345963], sum to 1.0000
[2019-04-07 11:46:48,771] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9162
[2019-04-07 11:46:48,848] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 18.7323782676697, -0.75, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 0.0000, 
sim time next is 1800.0000, 
raw observation next is [3.6, 95.5, 0.0, 0.0, 24.0, 18.90128101974305, -0.9919784346224004, 0.0, 1.0, 164399.77376409786], 
processed observation next is [0.0, 0.0, 0.5623268698060943, 0.955, 0.0, 0.0, 0.5, 0.07510675164525409, 0.1693405217925332, 0.0, 1.0, 0.7828560655433231], 
reward next is 0.5029, 
noisyNet noise sample is [array([-0.14264223], dtype=float32), -0.5335972]. 
=============================================
[2019-04-07 11:46:50,184] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:50,184] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:50,193] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run6
[2019-04-07 11:46:51,687] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:51,687] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:51,691] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run6
[2019-04-07 11:46:52,415] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:52,415] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:52,419] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run6
[2019-04-07 11:46:53,933] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:53,933] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:53,936] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run6
[2019-04-07 11:46:54,641] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:46:54,641] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:46:54,645] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run6
[2019-04-07 11:47:08,622] A3C_AGENT_WORKER-Thread-15 INFO:Local step 14500, global step 228531: loss 21.6623
[2019-04-07 11:47:08,622] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 14500, global step 228531: learning rate 0.0000
[2019-04-07 11:47:24,041] A3C_AGENT_WORKER-Thread-12 INFO:Local step 14500, global step 230245: loss 21.4521
[2019-04-07 11:47:24,041] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 14500, global step 230245: learning rate 0.0000
[2019-04-07 11:47:24,759] A3C_AGENT_WORKER-Thread-6 INFO:Local step 14500, global step 230357: loss 18.6954
[2019-04-07 11:47:24,760] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 14500, global step 230357: learning rate 0.0000
[2019-04-07 11:47:33,022] A3C_AGENT_WORKER-Thread-16 INFO:Local step 14500, global step 231081: loss 21.4426
[2019-04-07 11:47:33,023] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 14500, global step 231081: learning rate 0.0000
[2019-04-07 11:47:41,794] A3C_AGENT_WORKER-Thread-20 INFO:Local step 14500, global step 232020: loss 21.5286
[2019-04-07 11:47:41,795] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 14500, global step 232020: learning rate 0.0000
[2019-04-07 11:47:41,869] A3C_AGENT_WORKER-Thread-19 INFO:Local step 14500, global step 232029: loss 23.1679
[2019-04-07 11:47:41,885] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 14500, global step 232029: learning rate 0.0000
[2019-04-07 11:47:44,129] A3C_AGENT_WORKER-Thread-4 INFO:Local step 14500, global step 232284: loss 22.3128
[2019-04-07 11:47:44,129] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 14500, global step 232284: learning rate 0.0000
[2019-04-07 11:47:46,605] A3C_AGENT_WORKER-Thread-18 INFO:Local step 14500, global step 232517: loss 22.8762
[2019-04-07 11:47:46,606] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 14500, global step 232517: learning rate 0.0000
[2019-04-07 11:47:49,641] A3C_AGENT_WORKER-Thread-14 INFO:Local step 14500, global step 232791: loss 20.7959
[2019-04-07 11:47:49,642] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 14500, global step 232791: learning rate 0.0000
[2019-04-07 11:47:49,892] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0339191e-23 4.2337390e-21 4.7764593e-19 5.1257754e-10 1.4704788e-18
 1.0000000e+00 6.7454254e-12 1.8233947e-12], sum to 1.0000
[2019-04-07 11:47:49,892] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3134
[2019-04-07 11:47:49,954] A3C_AGENT_WORKER-Thread-3 INFO:Local step 14500, global step 232838: loss 19.6517
[2019-04-07 11:47:49,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 14500, global step 232838: learning rate 0.0000
[2019-04-07 11:47:50,056] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 83.0, 113.5, 270.0, 24.0, 23.13273101365264, -0.1036773107634101, 0.0, 1.0, 21293.64384140423], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 572400.0000, 
sim time next is 574200.0000, 
raw observation next is [-1.2, 83.0, 100.0, 73.0, 24.0, 23.07966411690581, -0.1145209109019648, 0.0, 1.0, 41268.961855148664], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.3333333333333333, 0.08066298342541436, 0.5, 0.42330534307548423, 0.46182636303267843, 0.0, 1.0, 0.1965188659768984], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.447271], dtype=float32), -0.45558295]. 
=============================================
[2019-04-07 11:47:51,060] A3C_AGENT_WORKER-Thread-2 INFO:Local step 14500, global step 232963: loss 21.5787
[2019-04-07 11:47:51,073] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 14500, global step 232963: learning rate 0.0000
[2019-04-07 11:47:52,305] A3C_AGENT_WORKER-Thread-11 INFO:Local step 14500, global step 233088: loss 21.6102
[2019-04-07 11:47:52,305] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 14500, global step 233088: learning rate 0.0000
[2019-04-07 11:47:53,652] A3C_AGENT_WORKER-Thread-5 INFO:Local step 14500, global step 233209: loss 21.8178
[2019-04-07 11:47:53,653] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 14500, global step 233209: learning rate 0.0000
[2019-04-07 11:47:55,342] A3C_AGENT_WORKER-Thread-15 INFO:Local step 15000, global step 233365: loss 28.0110
[2019-04-07 11:47:55,343] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 15000, global step 233365: learning rate 0.0000
[2019-04-07 11:47:58,329] A3C_AGENT_WORKER-Thread-13 INFO:Local step 14500, global step 233670: loss 22.5321
[2019-04-07 11:47:58,329] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 14500, global step 233670: learning rate 0.0000
[2019-04-07 11:47:58,781] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.3166051e-26 7.7884813e-24 2.0956997e-22 8.2794518e-12 8.5362212e-21
 1.0000000e+00 9.1985751e-13 7.2228193e-15], sum to 1.0000
[2019-04-07 11:47:58,781] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3158
[2019-04-07 11:47:58,884] A3C_AGENT_WORKER-Thread-10 INFO:Local step 14500, global step 233745: loss 21.3830
[2019-04-07 11:47:58,884] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 14500, global step 233745: learning rate 0.0000
[2019-04-07 11:47:58,961] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.85, 83.0, 0.0, 0.0, 24.0, 23.78750794544139, 0.08087089306713147, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1575000.0000, 
sim time next is 1576800.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 24.0, 23.83014488846288, 0.08600652553960049, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.6011080332409973, 0.82, 0.0, 0.0, 0.5, 0.48584540737190657, 0.5286688418465335, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9148338], dtype=float32), 0.59027594]. 
=============================================
[2019-04-07 11:47:59,133] A3C_AGENT_WORKER-Thread-17 INFO:Local step 14500, global step 233773: loss 19.7573
[2019-04-07 11:47:59,145] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 14500, global step 233773: learning rate 0.0000
[2019-04-07 11:48:06,323] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0013337e-29 9.3300752e-28 8.9777648e-24 4.7674113e-12 1.3666297e-22
 1.0000000e+00 6.3876157e-14 2.9221370e-15], sum to 1.0000
[2019-04-07 11:48:06,323] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5472
[2019-04-07 11:48:06,387] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 24.0, 24.16159518011719, 0.187993714229894, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1033200.0000, 
sim time next is 1035000.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 24.0, 24.01441955804582, 0.159885547296804, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.5, 0.5012016298371517, 0.5532951824322679, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48842114], dtype=float32), 1.5697063]. 
=============================================
[2019-04-07 11:48:06,391] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[118.84803]
 [117.94379]
 [118.08769]
 [117.81554]
 [117.29355]], R is [[118.38448334]
 [118.20063782]
 [118.01863098]
 [117.83844757]
 [117.6600647 ]].
[2019-04-07 11:48:19,390] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.1934008e-22 1.1721739e-21 4.9567926e-20 4.1375336e-10 3.0193206e-19
 1.0000000e+00 4.3505685e-11 3.0497626e-13], sum to 1.0000
[2019-04-07 11:48:19,390] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2467
[2019-04-07 11:48:19,500] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 82.0, 0.0, 0.0, 24.0, 23.2322171369994, -0.1210802817726213, 0.0, 1.0, 47004.570876707345], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1810800.0000, 
sim time next is 1812600.0000, 
raw observation next is [-5.0, 80.5, 0.0, 0.0, 24.0, 23.17601349009997, -0.13374640764551, 0.0, 1.0, 46712.428196090164], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.805, 0.0, 0.0, 0.5, 0.4313344575083307, 0.4554178641181634, 0.0, 1.0, 0.22244013426709602], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2301486], dtype=float32), 1.3918841]. 
=============================================
[2019-04-07 11:48:22,965] A3C_AGENT_WORKER-Thread-12 INFO:Local step 15000, global step 236290: loss 27.0354
[2019-04-07 11:48:22,965] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 15000, global step 236290: learning rate 0.0000
[2019-04-07 11:48:25,620] A3C_AGENT_WORKER-Thread-6 INFO:Local step 15000, global step 236598: loss 27.5200
[2019-04-07 11:48:25,638] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 15000, global step 236598: learning rate 0.0000
[2019-04-07 11:48:32,571] A3C_AGENT_WORKER-Thread-16 INFO:Local step 15000, global step 237406: loss 26.8240
[2019-04-07 11:48:32,572] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 15000, global step 237406: learning rate 0.0000
[2019-04-07 11:48:57,533] A3C_AGENT_WORKER-Thread-20 INFO:Local step 15000, global step 239767: loss 25.8621
[2019-04-07 11:48:57,545] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 15000, global step 239767: learning rate 0.0000
[2019-04-07 11:49:00,226] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-07 11:49:00,258] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:49:00,259] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:49:00,265] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run13
[2019-04-07 11:49:00,383] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:49:00,416] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:49:00,417] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 11:49:00,417] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:49:00,425] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run13
[2019-04-07 11:49:00,555] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run13
[2019-04-07 11:51:01,492] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 11:51:11,693] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.10938982], dtype=float32), 0.11750719]
[2019-04-07 11:51:11,693] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [2.0, 52.0, 0.0, 0.0, 24.0, 23.75524481396849, 0.07077199314276715, 0.0, 1.0, 0.0]
[2019-04-07 11:51:11,693] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 11:51:11,694] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [2.3945513e-22 3.9050966e-21 1.1152002e-18 2.5270200e-10 5.5936521e-18
 1.0000000e+00 6.1874630e-11 1.5527341e-12], sampled 0.4578292724018057
[2019-04-07 11:51:17,498] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 11:51:22,794] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 11:51:23,816] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 240000, evaluation results [240000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 11:51:23,875] A3C_AGENT_WORKER-Thread-19 INFO:Local step 15000, global step 240024: loss 26.2322
[2019-04-07 11:51:23,876] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 15000, global step 240024: learning rate 0.0000
[2019-04-07 11:51:25,051] A3C_AGENT_WORKER-Thread-4 INFO:Local step 15000, global step 240233: loss 26.6560
[2019-04-07 11:51:25,052] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 15000, global step 240233: learning rate 0.0000
[2019-04-07 11:51:28,321] A3C_AGENT_WORKER-Thread-18 INFO:Local step 15000, global step 240856: loss 26.0304
[2019-04-07 11:51:28,322] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 15000, global step 240857: learning rate 0.0000
[2019-04-07 11:51:29,822] A3C_AGENT_WORKER-Thread-14 INFO:Local step 15000, global step 241102: loss 25.9828
[2019-04-07 11:51:29,823] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 15000, global step 241102: learning rate 0.0000
[2019-04-07 11:51:30,299] A3C_AGENT_WORKER-Thread-3 INFO:Local step 15000, global step 241169: loss 25.9891
[2019-04-07 11:51:30,305] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 15000, global step 241169: learning rate 0.0000
[2019-04-07 11:51:30,878] A3C_AGENT_WORKER-Thread-2 INFO:Local step 15000, global step 241274: loss 26.1517
[2019-04-07 11:51:30,878] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 15000, global step 241274: learning rate 0.0000
[2019-04-07 11:51:32,674] A3C_AGENT_WORKER-Thread-11 INFO:Local step 15000, global step 241601: loss 26.0290
[2019-04-07 11:51:32,678] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 15000, global step 241602: learning rate 0.0000
[2019-04-07 11:51:33,448] A3C_AGENT_WORKER-Thread-5 INFO:Local step 15000, global step 241757: loss 25.8726
[2019-04-07 11:51:33,449] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 15000, global step 241757: learning rate 0.0000
[2019-04-07 11:51:35,251] A3C_AGENT_WORKER-Thread-13 INFO:Local step 15000, global step 242088: loss 25.7078
[2019-04-07 11:51:35,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 15000, global step 242088: learning rate 0.0000
[2019-04-07 11:51:35,732] A3C_AGENT_WORKER-Thread-10 INFO:Local step 15000, global step 242162: loss 25.2799
[2019-04-07 11:51:35,770] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 15000, global step 242167: learning rate 0.0000
[2019-04-07 11:51:35,912] A3C_AGENT_WORKER-Thread-17 INFO:Local step 15000, global step 242189: loss 26.2228
[2019-04-07 11:51:35,912] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 15000, global step 242189: learning rate 0.0000
[2019-04-07 11:51:40,955] A3C_AGENT_WORKER-Thread-15 INFO:Local step 15500, global step 243066: loss 1.1544
[2019-04-07 11:51:40,955] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 15500, global step 243066: learning rate 0.0000
[2019-04-07 11:51:59,758] A3C_AGENT_WORKER-Thread-12 INFO:Local step 15500, global step 245408: loss 1.2306
[2019-04-07 11:51:59,758] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 15500, global step 245408: learning rate 0.0000
[2019-04-07 11:52:03,633] A3C_AGENT_WORKER-Thread-6 INFO:Local step 15500, global step 245884: loss 1.1177
[2019-04-07 11:52:03,633] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 15500, global step 245884: learning rate 0.0000
[2019-04-07 11:52:08,217] A3C_AGENT_WORKER-Thread-16 INFO:Local step 15500, global step 246479: loss 1.1302
[2019-04-07 11:52:08,217] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 15500, global step 246479: learning rate 0.0000
[2019-04-07 11:52:13,687] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.29916801e-27 2.07388204e-25 1.08730876e-23 1.62545216e-12
 3.12254926e-21 1.00000000e+00 3.19770306e-13 4.86594085e-15], sum to 1.0000
[2019-04-07 11:52:13,687] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5416
[2019-04-07 11:52:13,823] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 65.0, 56.0, 0.0, 24.0, 24.24204638766066, -0.06280381027676359, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2131200.0000, 
sim time next is 2133000.0000, 
raw observation next is [-4.5, 66.5, 26.0, 0.0, 24.0, 23.82705005061689, -0.02513515490520116, 1.0, 1.0, 34073.995806230065], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.665, 0.08666666666666667, 0.0, 0.5, 0.4855875042180742, 0.49162161503159957, 1.0, 1.0, 0.16225712288680982], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4221805], dtype=float32), -0.60629094]. 
=============================================
[2019-04-07 11:52:13,827] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[114.04824 ]
 [114.41223 ]
 [114.478745]
 [114.41204 ]
 [115.76069 ]], R is [[112.94460297]
 [112.81515503]
 [112.56999207]
 [112.44429016]
 [112.31984711]].
[2019-04-07 11:52:20,424] A3C_AGENT_WORKER-Thread-20 INFO:Local step 15500, global step 248146: loss 1.3062
[2019-04-07 11:52:20,444] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 15500, global step 248146: learning rate 0.0000
[2019-04-07 11:52:21,824] A3C_AGENT_WORKER-Thread-19 INFO:Local step 15500, global step 248372: loss 1.2513
[2019-04-07 11:52:21,849] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 15500, global step 248372: learning rate 0.0000
[2019-04-07 11:52:23,275] A3C_AGENT_WORKER-Thread-4 INFO:Local step 15500, global step 248615: loss 1.2134
[2019-04-07 11:52:23,276] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 15500, global step 248615: learning rate 0.0000
[2019-04-07 11:52:24,617] A3C_AGENT_WORKER-Thread-18 INFO:Local step 15500, global step 248817: loss 1.1188
[2019-04-07 11:52:24,618] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 15500, global step 248817: learning rate 0.0000
[2019-04-07 11:52:26,995] A3C_AGENT_WORKER-Thread-14 INFO:Local step 15500, global step 249215: loss 1.1970
[2019-04-07 11:52:26,995] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 15500, global step 249215: learning rate 0.0000
[2019-04-07 11:52:27,487] A3C_AGENT_WORKER-Thread-2 INFO:Local step 15500, global step 249311: loss 1.2336
[2019-04-07 11:52:27,487] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 15500, global step 249311: learning rate 0.0000
[2019-04-07 11:52:27,822] A3C_AGENT_WORKER-Thread-3 INFO:Local step 15500, global step 249366: loss 1.2289
[2019-04-07 11:52:27,824] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 15500, global step 249367: learning rate 0.0000
[2019-04-07 11:52:27,960] A3C_AGENT_WORKER-Thread-15 INFO:Local step 16000, global step 249392: loss 19.6230
[2019-04-07 11:52:27,961] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 16000, global step 249392: learning rate 0.0000
[2019-04-07 11:52:30,591] A3C_AGENT_WORKER-Thread-13 INFO:Local step 15500, global step 249796: loss 1.1655
[2019-04-07 11:52:30,592] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 15500, global step 249796: learning rate 0.0000
[2019-04-07 11:52:31,209] A3C_AGENT_WORKER-Thread-5 INFO:Local step 15500, global step 249907: loss 1.2364
[2019-04-07 11:52:31,209] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 15500, global step 249907: learning rate 0.0000
[2019-04-07 11:52:31,233] A3C_AGENT_WORKER-Thread-11 INFO:Local step 15500, global step 249910: loss 1.2487
[2019-04-07 11:52:31,234] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 15500, global step 249910: learning rate 0.0000
[2019-04-07 11:52:31,238] A3C_AGENT_WORKER-Thread-17 INFO:Local step 15500, global step 249912: loss 1.1426
[2019-04-07 11:52:31,240] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 15500, global step 249913: learning rate 0.0000
[2019-04-07 11:52:31,859] A3C_AGENT_WORKER-Thread-10 INFO:Local step 15500, global step 250062: loss 1.1703
[2019-04-07 11:52:31,861] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 15500, global step 250062: learning rate 0.0000
[2019-04-07 11:52:49,761] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5126299e-29 1.2260363e-27 4.5339433e-24 5.2338203e-12 2.5256120e-22
 1.0000000e+00 7.7726562e-14 1.1996996e-16], sum to 1.0000
[2019-04-07 11:52:49,761] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9330
[2019-04-07 11:52:49,829] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 24.0, 24.15146293840699, 0.2565920635660581, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3182400.0000, 
sim time next is 3184200.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 24.0, 23.90598134479505, 0.2060884568266055, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.5, 0.4921651120662543, 0.5686961522755352, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00012243], dtype=float32), 1.8221707]. 
=============================================
[2019-04-07 11:52:49,838] A3C_AGENT_WORKER-Thread-12 INFO:Local step 16000, global step 252190: loss 19.7437
[2019-04-07 11:52:49,838] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 16000, global step 252190: learning rate 0.0000
[2019-04-07 11:52:56,968] A3C_AGENT_WORKER-Thread-6 INFO:Local step 16000, global step 253089: loss 20.2326
[2019-04-07 11:52:56,968] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 16000, global step 253089: learning rate 0.0000
[2019-04-07 11:53:02,139] A3C_AGENT_WORKER-Thread-16 INFO:Local step 16000, global step 253684: loss 19.9094
[2019-04-07 11:53:02,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 16000, global step 253684: learning rate 0.0000
[2019-04-07 11:53:14,915] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8265314e-19 8.9375004e-19 1.2287822e-16 4.1337000e-10 9.0123204e-16
 1.0000000e+00 3.3692122e-09 9.8897210e-12], sum to 1.0000
[2019-04-07 11:53:14,915] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8916
[2019-04-07 11:53:15,081] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 24.0, 23.25799023238572, -0.1457529220772104, 0.0, 1.0, 39515.59047310208], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3029400.0000, 
sim time next is 3031200.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 24.0, 23.17563605861469, -0.1619398054971332, 0.0, 1.0, 39695.240770442055], 
processed observation next is [0.0, 0.08695652173913043, 0.32409972299168976, 0.71, 0.0, 0.0, 0.5, 0.4313030048845574, 0.4460200648342889, 0.0, 1.0, 0.18902495604972408], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32365304], dtype=float32), -0.24211884]. 
=============================================
[2019-04-07 11:53:23,677] A3C_AGENT_WORKER-Thread-20 INFO:Local step 16000, global step 255850: loss 19.7770
[2019-04-07 11:53:23,677] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 16000, global step 255850: learning rate 0.0000
[2019-04-07 11:53:25,437] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9993977e-28 2.9758708e-25 8.0046032e-23 3.9045020e-13 5.3623261e-22
 1.0000000e+00 2.3879992e-13 8.5177019e-16], sum to 1.0000
[2019-04-07 11:53:25,437] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8298
[2019-04-07 11:53:25,552] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 64.0, 113.5, 769.0, 24.0, 24.8132107484019, 0.2243841986288489, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3322800.0000, 
sim time next is 3324600.0000, 
raw observation next is [-6.5, 59.0, 116.0, 798.0, 24.0, 24.82392371535015, 0.2379186043382477, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.28254847645429365, 0.59, 0.38666666666666666, 0.881767955801105, 0.5, 0.5686603096125126, 0.5793062014460826, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48301318], dtype=float32), 0.73836195]. 
=============================================
[2019-04-07 11:53:26,298] A3C_AGENT_WORKER-Thread-19 INFO:Local step 16000, global step 256131: loss 19.5680
[2019-04-07 11:53:26,407] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 16000, global step 256131: learning rate 0.0000
[2019-04-07 11:53:28,286] A3C_AGENT_WORKER-Thread-15 INFO:Local step 16500, global step 256349: loss 5.6759
[2019-04-07 11:53:28,287] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 16500, global step 256349: learning rate 0.0000
[2019-04-07 11:53:29,788] A3C_AGENT_WORKER-Thread-4 INFO:Local step 16000, global step 256505: loss 19.9829
[2019-04-07 11:53:29,789] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 16000, global step 256505: learning rate 0.0000
[2019-04-07 11:53:30,636] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.9541100e-22 2.9911849e-21 3.2112820e-18 3.1506248e-10 2.3152342e-17
 1.0000000e+00 1.6263570e-09 3.7057605e-12], sum to 1.0000
[2019-04-07 11:53:30,636] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2798
[2019-04-07 11:53:30,853] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 54.5, 111.0, 805.0, 24.0, 23.2715564805002, -0.08151509329253044, 0.0, 1.0, 18709.332203299055], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3065400.0000, 
sim time next is 3067200.0000, 
raw observation next is [-3.0, 55.0, 112.5, 811.0, 24.0, 23.19439491601145, -0.08078970534202955, 0.0, 1.0, 18931.822780372906], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.55, 0.375, 0.8961325966850828, 0.5, 0.4328662430009542, 0.4730700982193235, 0.0, 1.0, 0.09015153704939478], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9353838], dtype=float32), 0.01532437]. 
=============================================
[2019-04-07 11:53:32,489] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8561408e-23 1.1816783e-21 1.1066272e-18 4.3073281e-11 8.1177553e-18
 1.0000000e+00 3.9383954e-11 7.9282114e-14], sum to 1.0000
[2019-04-07 11:53:32,489] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6072
[2019-04-07 11:53:32,614] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 24.0, 23.39082376119416, -0.08319694579483099, 0.0, 1.0, 43844.606346347515], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3382200.0000, 
sim time next is 3384000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 24.0, 23.43547220580709, -0.09306292157739109, 0.0, 1.0, 33218.29934873765], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.5, 0.4529560171505909, 0.46897902614086967, 0.0, 1.0, 0.15818237785113168], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3657241], dtype=float32), -1.448423]. 
=============================================
[2019-04-07 11:53:32,626] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[96.43613 ]
 [96.278244]
 [96.89895 ]
 [97.80296 ]
 [98.99144 ]], R is [[96.46492004]
 [96.50027466]
 [96.53527069]
 [96.56991577]
 [96.60421753]].
[2019-04-07 11:53:33,334] A3C_AGENT_WORKER-Thread-18 INFO:Local step 16000, global step 256926: loss 19.7601
[2019-04-07 11:53:33,367] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 16000, global step 256926: learning rate 0.0000
[2019-04-07 11:53:36,457] A3C_AGENT_WORKER-Thread-14 INFO:Local step 16000, global step 257283: loss 19.4611
[2019-04-07 11:53:36,473] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 16000, global step 257283: learning rate 0.0000
[2019-04-07 11:53:38,312] A3C_AGENT_WORKER-Thread-2 INFO:Local step 16000, global step 257477: loss 19.8492
[2019-04-07 11:53:38,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 16000, global step 257477: learning rate 0.0000
[2019-04-07 11:53:39,754] A3C_AGENT_WORKER-Thread-3 INFO:Local step 16000, global step 257626: loss 19.6152
[2019-04-07 11:53:39,755] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 16000, global step 257626: learning rate 0.0000
[2019-04-07 11:53:43,694] A3C_AGENT_WORKER-Thread-13 INFO:Local step 16000, global step 258049: loss 19.6796
[2019-04-07 11:53:43,720] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 16000, global step 258050: learning rate 0.0000
[2019-04-07 11:53:46,744] A3C_AGENT_WORKER-Thread-17 INFO:Local step 16000, global step 258359: loss 19.7082
[2019-04-07 11:53:46,755] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 16000, global step 258359: learning rate 0.0000
[2019-04-07 11:53:46,878] A3C_AGENT_WORKER-Thread-11 INFO:Local step 16000, global step 258376: loss 19.5930
[2019-04-07 11:53:46,879] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 16000, global step 258376: learning rate 0.0000
[2019-04-07 11:53:49,010] A3C_AGENT_WORKER-Thread-10 INFO:Local step 16000, global step 258580: loss 19.8086
[2019-04-07 11:53:49,010] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 16000, global step 258580: learning rate 0.0000
[2019-04-07 11:53:49,411] A3C_AGENT_WORKER-Thread-5 INFO:Local step 16000, global step 258624: loss 19.3803
[2019-04-07 11:53:49,417] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 16000, global step 258624: learning rate 0.0000
[2019-04-07 11:53:56,466] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1091692e-26 3.6812764e-25 7.7502227e-23 3.9682077e-12 5.6003690e-21
 1.0000000e+00 2.6190923e-11 2.3607396e-14], sum to 1.0000
[2019-04-07 11:53:56,467] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6440
[2019-04-07 11:53:56,557] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 115.5, 814.5, 24.0, 24.13619370923308, 0.1854157506233747, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3502800.0000, 
sim time next is 3504600.0000, 
raw observation next is [2.5, 50.5, 115.0, 806.0, 24.0, 24.79696535770502, 0.2804797610380711, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5318559556786704, 0.505, 0.38333333333333336, 0.8906077348066298, 0.5, 0.5664137798087516, 0.593493253679357, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.778079], dtype=float32), 0.9379991]. 
=============================================
[2019-04-07 11:53:57,863] A3C_AGENT_WORKER-Thread-12 INFO:Local step 16500, global step 259560: loss 5.5299
[2019-04-07 11:53:57,880] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 16500, global step 259560: learning rate 0.0000
[2019-04-07 11:54:01,909] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-07 11:54:01,917] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:54:01,917] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:54:01,925] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run14
[2019-04-07 11:54:01,960] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:54:01,960] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:54:01,972] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run14
[2019-04-07 11:54:02,022] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 11:54:02,022] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:54:02,024] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run14
[2019-04-07 11:56:01,608] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.3337 70927233.8163 166.2180
[2019-04-07 11:56:18,564] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 11:56:22,745] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 11:56:23,767] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 260000, evaluation results [260000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.333747436773, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 11:56:28,752] A3C_AGENT_WORKER-Thread-6 INFO:Local step 16500, global step 261133: loss 6.5447
[2019-04-07 11:56:28,757] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 16500, global step 261135: learning rate 0.0000
[2019-04-07 11:56:28,832] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7896016e-21 7.2705971e-20 6.5175049e-17 8.3450180e-10 1.1401852e-16
 1.0000000e+00 3.4986083e-11 4.0646694e-11], sum to 1.0000
[2019-04-07 11:56:28,833] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6728
[2019-04-07 11:56:28,879] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 40.0, 16.5, 92.5, 24.0, 23.34474321623599, -0.07395053532933285, 0.0, 1.0, 12453.607780153694], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4816800.0000, 
sim time next is 4818600.0000, 
raw observation next is [1.5, 41.5, 0.0, 0.0, 24.0, 23.18736577762183, -0.106665684981477, 0.0, 1.0, 24143.221289916633], 
processed observation next is [0.0, 0.782608695652174, 0.5041551246537397, 0.415, 0.0, 0.0, 0.5, 0.43228048146848597, 0.46444477167284104, 0.0, 1.0, 0.11496772042817445], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7444222], dtype=float32), 1.3297423]. 
=============================================
[2019-04-07 11:56:29,708] A3C_AGENT_WORKER-Thread-16 INFO:Local step 16500, global step 261339: loss 6.2451
[2019-04-07 11:56:29,709] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 16500, global step 261339: learning rate 0.0000
[2019-04-07 11:56:33,587] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1032601e-22 1.7524201e-21 1.5758492e-18 2.9875279e-11 6.4000267e-19
 1.0000000e+00 4.1334061e-11 4.3324128e-13], sum to 1.0000
[2019-04-07 11:56:33,587] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0415
[2019-04-07 11:56:33,658] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.0, 58.0, 0.0, 0.0, 24.0, 23.23882859337726, -0.07035646978730921, 0.0, 1.0, 46011.8248786617], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3974400.0000, 
sim time next is 3976200.0000, 
raw observation next is [-10.5, 58.0, 0.0, 0.0, 24.0, 23.23092140746718, -0.08187190509190688, 0.0, 1.0, 45784.833257084574], 
processed observation next is [1.0, 0.0, 0.17174515235457063, 0.58, 0.0, 0.0, 0.5, 0.43591011728893153, 0.4727093649693644, 0.0, 1.0, 0.21802301550992653], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.662333], dtype=float32), 0.041688092]. 
=============================================
[2019-04-07 11:56:41,033] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:56:41,033] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:56:41,037] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run7
[2019-04-07 11:56:41,825] A3C_AGENT_WORKER-Thread-20 INFO:Local step 16500, global step 263895: loss 6.6034
[2019-04-07 11:56:41,826] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 16500, global step 263895: learning rate 0.0000
[2019-04-07 11:56:44,177] A3C_AGENT_WORKER-Thread-19 INFO:Local step 16500, global step 264305: loss 6.0074
[2019-04-07 11:56:44,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 16500, global step 264305: learning rate 0.0000
[2019-04-07 11:56:45,571] A3C_AGENT_WORKER-Thread-4 INFO:Local step 16500, global step 264559: loss 6.6240
[2019-04-07 11:56:45,577] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 16500, global step 264560: learning rate 0.0000
[2019-04-07 11:56:45,782] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5386422e-22 3.6985693e-21 3.1778794e-18 1.1823748e-10 1.2520705e-18
 1.0000000e+00 1.6701117e-10 1.7089078e-11], sum to 1.0000
[2019-04-07 11:56:45,782] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7655
[2019-04-07 11:56:45,853] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.5, 60.5, 0.0, 0.0, 24.0, 23.24879014315617, -0.107438511710802, 0.0, 1.0, 45227.22030656069], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3979800.0000, 
sim time next is 3981600.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 24.0, 23.04894994929905, -0.1384355664641582, 0.0, 1.0, 45114.56356248209], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.63, 0.0, 0.0, 0.5, 0.4207458291082542, 0.453854811178614, 0.0, 1.0, 0.21483125505943854], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0063943], dtype=float32), -0.2637487]. 
=============================================
[2019-04-07 11:56:48,003] A3C_AGENT_WORKER-Thread-18 INFO:Local step 16500, global step 265016: loss 6.5219
[2019-04-07 11:56:48,003] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 16500, global step 265016: learning rate 0.0000
[2019-04-07 11:56:49,316] A3C_AGENT_WORKER-Thread-14 INFO:Local step 16500, global step 265334: loss 6.7937
[2019-04-07 11:56:49,333] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 16500, global step 265334: learning rate 0.0000
[2019-04-07 11:56:49,519] A3C_AGENT_WORKER-Thread-2 INFO:Local step 16500, global step 265384: loss 6.8022
[2019-04-07 11:56:49,520] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 16500, global step 265384: learning rate 0.0000
[2019-04-07 11:56:51,276] A3C_AGENT_WORKER-Thread-3 INFO:Local step 16500, global step 265771: loss 6.6381
[2019-04-07 11:56:51,288] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 16500, global step 265773: learning rate 0.0000
[2019-04-07 11:56:52,500] A3C_AGENT_WORKER-Thread-13 INFO:Local step 16500, global step 266072: loss 7.8302
[2019-04-07 11:56:52,500] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 16500, global step 266072: learning rate 0.0000
[2019-04-07 11:56:54,704] A3C_AGENT_WORKER-Thread-11 INFO:Local step 16500, global step 266647: loss 7.1626
[2019-04-07 11:56:54,704] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 16500, global step 266647: learning rate 0.0000
[2019-04-07 11:56:55,170] A3C_AGENT_WORKER-Thread-17 INFO:Local step 16500, global step 266780: loss 6.6259
[2019-04-07 11:56:55,171] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 16500, global step 266780: learning rate 0.0000
[2019-04-07 11:56:55,452] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4585762e-31 5.7433359e-28 3.8801258e-26 2.2132062e-13 1.1536163e-24
 1.0000000e+00 9.9034660e-15 6.2097272e-18], sum to 1.0000
[2019-04-07 11:56:55,457] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2258
[2019-04-07 11:56:55,490] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 140.5, 3.0, 24.0, 24.48933303946509, 0.207941890964653, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4456800.0000, 
sim time next is 4458600.0000, 
raw observation next is [0.0, 88.5, 85.0, 0.0, 24.0, 24.68609926494238, 0.2164802193696688, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.885, 0.2833333333333333, 0.0, 0.5, 0.5571749387451984, 0.5721600731232229, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05557289], dtype=float32), -0.8223496]. 
=============================================
[2019-04-07 11:56:55,494] A3C_AGENT_WORKER-Thread-10 INFO:Local step 16500, global step 266873: loss 7.5111
[2019-04-07 11:56:55,496] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 16500, global step 266874: learning rate 0.0000
[2019-04-07 11:56:55,521] A3C_AGENT_WORKER-Thread-5 INFO:Local step 16500, global step 266883: loss 7.1125
[2019-04-07 11:56:55,523] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 16500, global step 266883: learning rate 0.0000
[2019-04-07 11:56:56,605] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:56:56,605] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:56:56,611] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run7
[2019-04-07 11:57:01,873] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:01,874] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:01,877] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run7
[2019-04-07 11:57:02,020] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.6587591e-30 2.6946331e-27 1.3942466e-25 3.1602481e-14 3.0568985e-23
 1.0000000e+00 4.1566969e-15 2.5749320e-16], sum to 1.0000
[2019-04-07 11:57:02,020] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3785
[2019-04-07 11:57:02,093] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 72.0, 100.0, 11.0, 24.0, 23.84018099679357, 0.02287966615407069, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4725000.0000, 
sim time next is 4726800.0000, 
raw observation next is [1.0, 72.0, 64.5, 19.5, 24.0, 23.9275570670901, 0.06399441192334372, 1.0, 1.0, 46275.983070702234], 
processed observation next is [1.0, 0.7391304347826086, 0.4903047091412743, 0.72, 0.215, 0.02154696132596685, 0.5, 0.4939630889241749, 0.5213314706411146, 1.0, 1.0, 0.22036182414620112], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.56917965], dtype=float32), 0.3529255]. 
=============================================
[2019-04-07 11:57:02,643] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.4106861e-30 3.8857629e-29 1.7246350e-26 1.1191055e-15 4.0961058e-25
 1.0000000e+00 7.4226143e-16 1.8075274e-17], sum to 1.0000
[2019-04-07 11:57:02,643] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0715
[2019-04-07 11:57:02,683] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 108.0, 0.0, 24.0, 24.48415609363064, 0.08925706047674287, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4710600.0000, 
sim time next is 4712400.0000, 
raw observation next is [1.0, 86.0, 121.5, 0.0, 24.0, 24.10357313415678, 0.02889693002651241, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4903047091412743, 0.86, 0.405, 0.0, 0.5, 0.5086310945130649, 0.5096323100088375, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45341676], dtype=float32), 1.0133812]. 
=============================================
[2019-04-07 11:57:04,047] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:04,047] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:04,051] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run7
[2019-04-07 11:57:09,697] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3236527e-21 3.5910722e-21 2.9372633e-19 5.3878046e-10 5.4218251e-17
 1.0000000e+00 2.0063073e-09 9.5480941e-12], sum to 1.0000
[2019-04-07 11:57:09,698] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2511
[2019-04-07 11:57:09,925] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 84.5, 124.0, 419.0, 24.0, 23.15025520121696, -0.002141639153067606, 0.0, 1.0, 75824.17933102956], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4782600.0000, 
sim time next is 4784400.0000, 
raw observation next is [-5.0, 77.0, 149.0, 420.0, 24.0, 23.83799231753955, 0.03861453006204978, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.32409972299168976, 0.77, 0.49666666666666665, 0.46408839779005523, 0.5, 0.48649935979496234, 0.5128715100206832, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.73254174], dtype=float32), 0.24609561]. 
=============================================
[2019-04-07 11:57:11,737] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5423078e-28 2.4646743e-26 6.2795822e-22 9.5626881e-13 6.2879237e-22
 1.0000000e+00 1.3214555e-12 8.5744637e-15], sum to 1.0000
[2019-04-07 11:57:11,737] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3227
[2019-04-07 11:57:11,774] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 24.0, 24.31736201546247, 0.2356310970272623, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4653000.0000, 
sim time next is 4654800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 24.0, 24.14705726349339, 0.202105711297573, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.52, 0.0, 0.0, 0.5, 0.5122547719577826, 0.5673685704325243, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31511983], dtype=float32), -0.14770335]. 
=============================================
[2019-04-07 11:57:16,218] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3681458e-28 3.1545783e-27 3.7701983e-26 2.1558994e-14 3.8145003e-23
 1.0000000e+00 6.4788415e-15 1.9033059e-16], sum to 1.0000
[2019-04-07 11:57:16,219] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0142
[2019-04-07 11:57:16,245] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:16,245] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:16,249] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run7
[2019-04-07 11:57:16,270] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 24.0, 26.37645384164122, 0.6344645962589137, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5079600.0000, 
sim time next is 5081400.0000, 
raw observation next is [10.5, 18.0, 0.0, 0.0, 24.0, 26.10534867060781, 0.5875273992881898, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7534626038781165, 0.18, 0.0, 0.0, 0.5, 0.6754457225506508, 0.6958424664293966, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2096456], dtype=float32), 0.47862864]. 
=============================================
[2019-04-07 11:57:17,709] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:17,709] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:17,713] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run7
[2019-04-07 11:57:18,739] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:18,739] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:18,743] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run7
[2019-04-07 11:57:20,925] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:20,926] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:20,930] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run7
[2019-04-07 11:57:23,225] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:23,225] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:23,229] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run7
[2019-04-07 11:57:23,607] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:23,607] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:23,622] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run7
[2019-04-07 11:57:24,929] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:24,937] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:24,942] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run7
[2019-04-07 11:57:26,569] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:26,569] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:26,573] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run7
[2019-04-07 11:57:27,794] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:27,794] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:27,798] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run7
[2019-04-07 11:57:28,693] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:28,693] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:28,697] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run7
[2019-04-07 11:57:28,793] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:28,793] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:28,797] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run7
[2019-04-07 11:57:28,979] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 11:57:28,979] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:57:28,983] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run7
[2019-04-07 11:57:55,011] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3048668e-23 1.7163174e-21 1.5410015e-18 3.0400799e-10 5.6544739e-18
 1.0000000e+00 3.2587235e-11 3.1908261e-12], sum to 1.0000
[2019-04-07 11:57:55,011] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4665
[2019-04-07 11:57:55,087] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.15, 68.5, 0.0, 0.0, 24.0, 21.88680674104263, -0.4480505105183565, 0.0, 1.0, 48462.61455799263], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 279000.0000, 
sim time next is 280800.0000, 
raw observation next is [-11.7, 70.0, 0.0, 0.0, 24.0, 21.77494779189567, -0.4783543307738649, 0.0, 1.0, 48554.20020900925], 
processed observation next is [1.0, 0.2608695652173913, 0.13850415512465375, 0.7, 0.0, 0.0, 0.5, 0.3145789826579725, 0.3405485564087117, 0.0, 1.0, 0.23121047718575832], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0845515], dtype=float32), 0.9514772]. 
=============================================
[2019-04-07 11:58:12,017] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7591509e-24 1.7117004e-22 6.3996506e-22 1.6729013e-10 8.9249634e-19
 1.0000000e+00 5.6553000e-12 7.1417522e-14], sum to 1.0000
[2019-04-07 11:58:12,018] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8767
[2019-04-07 11:58:12,642] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.25, 51.0, 58.0, 905.0, 24.0, 23.61827215982507, -0.0703915108026634, 1.0, 1.0, 66230.61601262067], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 390600.0000, 
sim time next is 392400.0000, 
raw observation next is [-11.7, 51.0, 56.5, 896.0, 24.0, 24.29598564578577, 0.07917551738873917, 1.0, 1.0, 84417.30631999251], 
processed observation next is [1.0, 0.5652173913043478, 0.13850415512465375, 0.51, 0.18833333333333332, 0.9900552486187846, 0.5, 0.5246654704821475, 0.5263918391295798, 1.0, 1.0, 0.40198717295234526], 
reward next is 0.8837, 
noisyNet noise sample is [array([-1.2467563], dtype=float32), 1.5741858]. 
=============================================
[2019-04-07 11:58:33,333] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.4709616e-24 4.5704147e-22 6.3846639e-19 3.3469515e-11 1.1344232e-17
 1.0000000e+00 1.3842114e-11 1.1408744e-13], sum to 1.0000
[2019-04-07 11:58:33,334] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5197
[2019-04-07 11:58:33,493] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 28.0, 124.0, 0.0, 24.0, 23.44845194809215, -0.2393393373644791, 1.0, 1.0, 21671.382259965594], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 478800.0000, 
sim time next is 480600.0000, 
raw observation next is [-0.8999999999999999, 31.5, 119.0, 0.0, 24.0, 23.40882165370054, -0.2189314597170699, 1.0, 1.0, 18687.72084896076], 
processed observation next is [1.0, 0.5652173913043478, 0.43767313019390586, 0.315, 0.39666666666666667, 0.0, 0.5, 0.4507351378083782, 0.4270228467609767, 1.0, 1.0, 0.08898914689981313], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05605614], dtype=float32), 0.57511175]. 
=============================================
[2019-04-07 11:58:43,497] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.4549183e-23 3.1436665e-22 2.7873081e-18 4.3790572e-11 1.5861623e-18
 1.0000000e+00 2.8653394e-11 2.7777834e-13], sum to 1.0000
[2019-04-07 11:58:43,497] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3112
[2019-04-07 11:58:43,682] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8, 81.0, 109.5, 265.5, 24.0, 22.97805734633155, -0.1205737343926593, 0.0, 1.0, 48574.055635972996], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 561600.0000, 
sim time next is 563400.0000, 
raw observation next is [-1.0, 80.5, 130.0, 396.0, 24.0, 23.10064522551386, -0.1172756605086516, 0.0, 1.0, 6237.027106562677], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.805, 0.43333333333333335, 0.4375690607734807, 0.5, 0.42505376879282153, 0.46090811316378283, 0.0, 1.0, 0.02970012907886989], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8620789], dtype=float32), -0.8987147]. 
=============================================
[2019-04-07 11:58:48,842] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.7248118e-22 1.6816840e-20 7.6400180e-18 1.0629314e-09 2.5545710e-17
 1.0000000e+00 6.3270340e-11 8.5028783e-13], sum to 1.0000
[2019-04-07 11:58:48,842] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1117
[2019-04-07 11:58:49,117] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.7, 61.0, 100.5, 69.0, 24.0, 23.00915538164015, -0.1989672203995456, 0.0, 1.0, 35905.48957018766], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 648000.0000, 
sim time next is 649800.0000, 
raw observation next is [-2.5, 60.0, 112.0, 100.0, 24.0, 23.01944662035378, -0.196205983220197, 0.0, 1.0, 32368.995669173437], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.6, 0.37333333333333335, 0.11049723756906077, 0.5, 0.41828721836281496, 0.43459800559326767, 0.0, 1.0, 0.15413807461511161], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3231331], dtype=float32), -0.016943863]. 
=============================================
[2019-04-07 11:58:50,731] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.4635344e-22 7.3424992e-22 9.2002316e-19 1.9678686e-10 7.2105262e-19
 1.0000000e+00 4.0230118e-11 8.6131070e-13], sum to 1.0000
[2019-04-07 11:58:50,731] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0456
[2019-04-07 11:58:50,906] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 54.0, 83.0, 38.0, 24.0, 23.01150883749151, -0.2093777652157949, 0.0, 1.0, 27284.8494429846], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 660600.0000, 
sim time next is 662400.0000, 
raw observation next is [-0.6, 54.0, 55.0, 26.5, 24.0, 22.99085416266085, -0.2100448317540773, 0.0, 1.0, 41270.61270259313], 
processed observation next is [0.0, 0.6956521739130435, 0.44598337950138506, 0.54, 0.18333333333333332, 0.029281767955801105, 0.5, 0.41590451355507074, 0.42998505608197424, 0.0, 1.0, 0.19652672715520536], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49330682], dtype=float32), -1.2875062]. 
=============================================
[2019-04-07 11:58:51,255] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-07 11:58:51,274] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:58:51,274] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:58:51,276] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run15
[2019-04-07 11:58:51,341] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 11:58:51,341] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:58:51,344] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run15
[2019-04-07 11:58:51,440] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 11:58:51,474] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 11:58:51,476] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run15
[2019-04-07 12:01:06,093] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 12:01:22,754] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 12:01:26,026] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 12:01:27,048] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 280000, evaluation results [280000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 12:01:35,065] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9059326e-27 1.7545763e-26 2.3200798e-24 6.4539059e-14 4.7937910e-23
 1.0000000e+00 2.1173010e-13 1.8213298e-16], sum to 1.0000
[2019-04-07 12:01:35,065] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7678
[2019-04-07 12:01:35,131] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 24.0, 23.4123610726655, 0.06013614147585156, 0.0, 1.0, 81839.4625047653], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1465200.0000, 
sim time next is 1467000.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 24.0, 23.58936840800723, 0.05488575564082327, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.5, 0.465780700667269, 0.5182952518802745, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.351763], dtype=float32), 0.044544265]. 
=============================================
[2019-04-07 12:01:35,141] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[115.61007 ]
 [114.86596 ]
 [115.24251 ]
 [114.71528 ]
 [116.091965]], R is [[115.20261383]
 [114.94659424]
 [114.77893066]
 [114.63114166]
 [114.48483276]].
[2019-04-07 12:01:44,163] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0970229e-26 5.6239687e-26 3.1531442e-21 9.2164882e-13 1.1663931e-21
 1.0000000e+00 8.1481799e-13 2.3826325e-15], sum to 1.0000
[2019-04-07 12:01:44,163] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1065
[2019-04-07 12:01:44,235] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.1, 77.5, 0.0, 0.0, 24.0, 23.77886444113878, 0.1436778260711853, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1053000.0000, 
sim time next is 1054800.0000, 
raw observation next is [13.8, 78.0, 0.0, 0.0, 24.0, 23.81427729562689, 0.1286878364546634, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.844875346260388, 0.78, 0.0, 0.0, 0.5, 0.4845231079689076, 0.5428959454848877, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4154689], dtype=float32), 0.29193646]. 
=============================================
[2019-04-07 12:02:04,624] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1358581e-29 3.5740893e-26 1.2209742e-24 3.6269295e-13 2.4066392e-22
 1.0000000e+00 2.4826620e-13 3.1655673e-15], sum to 1.0000
[2019-04-07 12:02:04,625] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3299
[2019-04-07 12:02:04,700] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 82.0, 0.0, 0.0, 24.0, 23.73726623292919, 0.1388353702136521, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1551600.0000, 
sim time next is 1553400.0000, 
raw observation next is [5.25, 82.0, 0.0, 0.0, 24.0, 23.73415229218964, 0.103882119737941, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.60803324099723, 0.82, 0.0, 0.0, 0.5, 0.47784602434913676, 0.5346273732459803, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.45989764], dtype=float32), 2.289644]. 
=============================================
[2019-04-07 12:02:20,609] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9054259e-26 7.9516745e-24 2.9686319e-21 1.3283899e-12 3.1723618e-22
 1.0000000e+00 5.4906671e-13 9.0938618e-15], sum to 1.0000
[2019-04-07 12:02:20,609] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6238
[2019-04-07 12:02:20,720] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 64.0, 130.0, 220.0, 24.0, 24.33361087577882, 0.002874464869297438, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2797200.0000, 
sim time next is 2799000.0000, 
raw observation next is [-4.5, 59.5, 152.0, 233.0, 24.0, 24.28757902515088, 0.01963097616992186, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3379501385041552, 0.595, 0.5066666666666667, 0.2574585635359116, 0.5, 0.5239649187625733, 0.5065436587233073, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2902011], dtype=float32), 0.043535482]. 
=============================================
[2019-04-07 12:02:20,723] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[106.84971 ]
 [105.385735]
 [101.91271 ]
 [100.11395 ]
 [ 96.35163 ]], R is [[109.60936737]
 [109.51327515]
 [109.41814423]
 [109.32396698]
 [108.77197266]].
[2019-04-07 12:02:22,438] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.63864307e-25 7.29397280e-24 6.39989499e-22 1.40309819e-10
 1.01555145e-20 1.00000000e+00 1.53393899e-12 1.81942203e-14], sum to 1.0000
[2019-04-07 12:02:22,438] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0991
[2019-04-07 12:02:22,485] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3, 46.5, 41.0, 0.0, 24.0, 23.44274524790061, -0.1225748263874524, 1.0, 1.0, 25826.156457601242], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2305800.0000, 
sim time next is 2307600.0000, 
raw observation next is [-0.6, 49.0, 23.0, 0.0, 24.0, 23.74314952539801, -0.06882896382687605, 1.0, 1.0, 26814.84389604366], 
processed observation next is [1.0, 0.7391304347826086, 0.44598337950138506, 0.49, 0.07666666666666666, 0.0, 0.5, 0.4785957937831675, 0.477057012057708, 1.0, 1.0, 0.12768973283830315], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0832664], dtype=float32), -0.7529904]. 
=============================================
[2019-04-07 12:02:26,358] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2250792e-24 8.6253389e-24 6.8879446e-20 1.7453972e-11 2.6303154e-19
 1.0000000e+00 4.3532808e-12 3.5786200e-14], sum to 1.0000
[2019-04-07 12:02:26,358] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2073
[2019-04-07 12:02:26,432] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.9, 85.0, 0.0, 0.0, 24.0, 22.9853445230684, -0.2130729493319187, 0.0, 1.0, 42336.091875721955], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2003400.0000, 
sim time next is 2005200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 24.0, 23.00605636558726, -0.2153922511566723, 0.0, 1.0, 42022.405673624315], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.5, 0.41717136379893827, 0.42820258294777586, 0.0, 1.0, 0.2001066936839253], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.4339135], dtype=float32), 2.5423713]. 
=============================================
[2019-04-07 12:02:27,406] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6993924e-20 1.4520837e-18 1.0273633e-17 4.8847397e-09 9.8096876e-16
 1.0000000e+00 5.9358507e-10 4.7122042e-11], sum to 1.0000
[2019-04-07 12:02:27,406] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9634
[2019-04-07 12:02:27,500] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.1, 67.0, 0.0, 0.0, 24.0, 22.98667259301833, -0.2046797227680671, 0.0, 1.0, 42110.62820513222], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2356200.0000, 
sim time next is 2358000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 24.0, 22.91106768706481, -0.2189703673053253, 0.0, 1.0, 42221.24580297598], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.0, 0.0, 0.5, 0.40925564058873426, 0.42700987756489156, 0.0, 1.0, 0.20105355144274276], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.91223085], dtype=float32), 1.748978]. 
=============================================
[2019-04-07 12:02:27,520] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.91915]
 [83.84601]
 [83.73294]
 [83.65638]
 [83.44929]], R is [[84.26093292]
 [84.41832733]
 [84.57414246]
 [84.72840118]
 [84.88111877]].
[2019-04-07 12:02:54,701] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.1430548e-20 1.0716632e-19 1.1618088e-16 6.2935412e-10 2.4835074e-15
 1.0000000e+00 9.2456638e-09 3.3314666e-12], sum to 1.0000
[2019-04-07 12:02:54,702] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8812
[2019-04-07 12:02:54,829] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 33.0, 0.0, 0.0, 24.0, 23.47795276457544, -0.1570659688489772, 0.0, 1.0, 21486.590117036732], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2498400.0000, 
sim time next is 2500200.0000, 
raw observation next is [-0.8999999999999999, 34.0, 0.0, 0.0, 24.0, 23.37943828022526, -0.1529613539270445, 0.0, 1.0, 67416.92410594819], 
processed observation next is [0.0, 0.9565217391304348, 0.43767313019390586, 0.34, 0.0, 0.0, 0.5, 0.4482865233521049, 0.4490128820243185, 0.0, 1.0, 0.3210329719330866], 
reward next is 0.9647, 
noisyNet noise sample is [array([0.00917147], dtype=float32), 0.7319877]. 
=============================================
[2019-04-07 12:03:23,192] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.3609206e-31 1.0609316e-29 1.3093588e-25 2.0385268e-14 4.8767338e-24
 1.0000000e+00 1.9008336e-14 1.3648091e-18], sum to 1.0000
[2019-04-07 12:03:23,192] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3129
[2019-04-07 12:03:23,304] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 92.5, 721.0, 24.0, 25.70356070881084, 0.5139779707682876, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3164400.0000, 
sim time next is 3166200.0000, 
raw observation next is [6.8, 99.5, 84.0, 679.0, 24.0, 25.92626628653815, 0.548272234397432, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6509695290858727, 0.995, 0.28, 0.7502762430939226, 0.5, 0.660522190544846, 0.6827574114658107, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09704187], dtype=float32), 0.086792514]. 
=============================================
[2019-04-07 12:03:29,177] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.7864306e-29 7.7543392e-29 2.5121571e-24 4.0362275e-13 1.1903047e-23
 1.0000000e+00 1.0513937e-13 5.8013048e-16], sum to 1.0000
[2019-04-07 12:03:29,177] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8968
[2019-04-07 12:03:29,305] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 85.5, 101.0, 769.0, 24.0, 25.05460079570672, 0.3771302604564126, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3249000.0000, 
sim time next is 3250800.0000, 
raw observation next is [-2.0, 71.0, 93.0, 727.5, 24.0, 23.92861332214373, 0.254751921174668, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.40720221606648205, 0.71, 0.31, 0.8038674033149171, 0.5, 0.49405111017864406, 0.5849173070582226, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.665194], dtype=float32), 0.35910892]. 
=============================================
[2019-04-07 12:03:43,549] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-07 12:03:43,550] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:03:43,550] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:03:43,551] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:03:43,552] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run16
[2019-04-07 12:03:43,571] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:03:43,572] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 12:03:43,572] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:03:43,589] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run16
[2019-04-07 12:03:43,650] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run16
[2019-04-07 12:05:57,781] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 12:06:15,217] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 12:06:17,283] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 12:06:18,305] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 300000, evaluation results [300000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 12:06:18,648] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0022986e-27 4.3786001e-25 4.6975392e-23 1.1319556e-12 5.7529361e-22
 1.0000000e+00 8.1860490e-13 4.7012332e-15], sum to 1.0000
[2019-04-07 12:06:18,649] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8043
[2019-04-07 12:06:18,729] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.8, 100.0, 0.0, 0.0, 24.0, 23.54230492936516, -0.1104667994449768, 0.0, 1.0, 23584.080353207886], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3126600.0000, 
sim time next is 3128400.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 24.0, 23.51914209649099, -0.0939715645478561, 0.0, 1.0, 45382.129891573735], 
processed observation next is [1.0, 0.21739130434782608, 0.5457063711911359, 1.0, 0.0, 0.0, 0.5, 0.45992850804091595, 0.46867614515071465, 0.0, 1.0, 0.21610538043606542], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8286453], dtype=float32), 0.44383132]. 
=============================================
[2019-04-07 12:06:21,032] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2498206e-22 7.9070106e-21 2.0842324e-17 7.2734041e-11 1.7058903e-17
 1.0000000e+00 9.4864706e-11 1.0279996e-11], sum to 1.0000
[2019-04-07 12:06:21,033] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6252
[2019-04-07 12:06:21,072] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 24.0, 23.35000584115166, -0.113958623754742, 0.0, 1.0, 41521.055777707574], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3024000.0000, 
sim time next is 3025800.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 24.0, 23.32669051637665, -0.1203556577712895, 0.0, 1.0, 42840.92071334513], 
processed observation next is [0.0, 0.0, 0.3379501385041552, 0.68, 0.0, 0.0, 0.5, 0.4438908763647209, 0.4598814474095702, 0.0, 1.0, 0.20400438434926252], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8666022], dtype=float32), 1.7511842]. 
=============================================
[2019-04-07 12:06:34,416] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9412965e-26 3.5280613e-25 2.3046631e-22 9.6230489e-13 3.3313498e-21
 1.0000000e+00 2.6402118e-12 2.2532684e-15], sum to 1.0000
[2019-04-07 12:06:34,416] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4566
[2019-04-07 12:06:34,460] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 100.0, 0.0, 0.0, 24.0, 23.58454392103748, -0.07650929576593789, 0.0, 1.0, 20411.105702104225], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3119400.0000, 
sim time next is 3121200.0000, 
raw observation next is [2.0, 100.0, 0.0, 0.0, 24.0, 23.59435583661636, -0.1305354483414201, 0.0, 1.0, 11195.557961621169], 
processed observation next is [1.0, 0.13043478260869565, 0.518005540166205, 1.0, 0.0, 0.0, 0.5, 0.4661963197180299, 0.45648818388619333, 0.0, 1.0, 0.053312180769624615], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37717518], dtype=float32), -0.51231873]. 
=============================================
[2019-04-07 12:06:37,052] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2989662e-28 1.5562238e-27 1.9723574e-23 5.2047366e-14 3.7413246e-21
 1.0000000e+00 7.7355934e-14 1.6164983e-16], sum to 1.0000
[2019-04-07 12:06:37,054] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5222
[2019-04-07 12:06:37,137] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 85.5, 101.0, 769.0, 24.0, 25.05460079570672, 0.3771302604564126, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3249000.0000, 
sim time next is 3250800.0000, 
raw observation next is [-2.0, 71.0, 93.0, 727.5, 24.0, 23.92861332214373, 0.254751921174668, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.40720221606648205, 0.71, 0.31, 0.8038674033149171, 0.5, 0.49405111017864406, 0.5849173070582226, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23653772], dtype=float32), -0.13446635]. 
=============================================
[2019-04-07 12:06:46,421] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4675389e-25 3.3425847e-24 9.2469786e-22 8.4944534e-12 2.8112152e-19
 1.0000000e+00 8.4376261e-13 5.4754161e-14], sum to 1.0000
[2019-04-07 12:06:46,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0929
[2019-04-07 12:06:46,467] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 24.0, 23.49830528596996, 0.008013932969226964, 0.0, 1.0, 28156.988876628486], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3891600.0000, 
sim time next is 3893400.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 24.0, 23.58720085669364, -0.009314006178385912, 0.0, 1.0, 12495.165078961296], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.5, 0.4656000713911368, 0.49689533127387137, 0.0, 1.0, 0.059500786090291885], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7342714], dtype=float32), 0.33180898]. 
=============================================
[2019-04-07 12:06:49,900] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.2393802e-20 3.6014606e-19 1.5036579e-17 2.0150437e-09 1.7939489e-15
 1.0000000e+00 2.2097604e-09 5.6028761e-11], sum to 1.0000
[2019-04-07 12:06:49,900] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1310
[2019-04-07 12:06:49,947] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 32.0, 118.0, 847.0, 24.0, 23.22032579566284, -0.03173521134365389, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4192200.0000, 
sim time next is 4194000.0000, 
raw observation next is [2.0, 34.0, 166.0, 758.0, 24.0, 23.24125776687294, -0.02436796574963554, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.34, 0.5533333333333333, 0.8375690607734807, 0.5, 0.4367714805727451, 0.49187734475012146, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3010038], dtype=float32), 0.63690746]. 
=============================================
[2019-04-07 12:06:49,951] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[83.38972 ]
 [83.15376 ]
 [83.0813  ]
 [83.19224 ]
 [83.011345]], R is [[84.31328583]
 [84.47015381]
 [84.62545013]
 [84.77919769]
 [84.93140411]].
[2019-04-07 12:06:51,949] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7408997e-21 6.8710404e-21 8.5684626e-19 2.5199193e-11 2.8989957e-17
 1.0000000e+00 9.3764458e-12 7.5146638e-13], sum to 1.0000
[2019-04-07 12:06:51,949] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5804
[2019-04-07 12:06:52,195] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.5, 70.0, 88.0, 425.0, 24.0, 23.28516169331595, -0.01135301936246198, 0.0, 1.0, 29903.4668461913], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3573000.0000, 
sim time next is 3574800.0000, 
raw observation next is [-6.0, 70.0, 94.0, 550.5, 24.0, 23.56989032116413, 0.004347794822492988, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.296398891966759, 0.7, 0.31333333333333335, 0.6082872928176796, 0.5, 0.46415752676367755, 0.501449264940831, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2686578], dtype=float32), 0.81877977]. 
=============================================
[2019-04-07 12:06:52,833] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:06:52,833] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:06:52,837] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run8
[2019-04-07 12:07:03,646] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2917384e-24 4.2058220e-23 3.7170576e-20 1.5640515e-10 1.0056673e-19
 1.0000000e+00 1.6673258e-11 2.9197985e-14], sum to 1.0000
[2019-04-07 12:07:03,646] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1595
[2019-04-07 12:07:03,703] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 24.0, 23.52609104651117, 0.0264490309344349, 0.0, 1.0, 31780.681654626696], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4577400.0000, 
sim time next is 4579200.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 24.0, 23.65005913473699, 0.02469880273102871, 0.0, 1.0, 6248.5942686292365], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.5, 0.47083826122808237, 0.5082329342436762, 0.0, 1.0, 0.029755210802996365], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37070477], dtype=float32), -0.90534276]. 
=============================================
[2019-04-07 12:07:04,150] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9667581e-25 2.3061205e-24 1.0141417e-21 5.7875016e-12 1.7223139e-19
 1.0000000e+00 1.7138522e-12 2.2572539e-13], sum to 1.0000
[2019-04-07 12:07:04,150] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1689
[2019-04-07 12:07:04,218] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 20.0, 96.5, 753.0, 24.0, 24.75618553303175, 0.2346549839424528, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4028400.0000, 
sim time next is 4030200.0000, 
raw observation next is [-1.5, 21.0, 89.0, 712.0, 24.0, 24.96622554883399, 0.2815774097787556, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4210526315789474, 0.21, 0.2966666666666667, 0.7867403314917127, 0.5, 0.5805187957361658, 0.5938591365929186, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6381343], dtype=float32), 0.1945156]. 
=============================================
[2019-04-07 12:07:07,248] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4342299e-19 3.1078763e-19 9.4628946e-17 2.5080016e-09 1.8910184e-15
 1.0000000e+00 5.2195498e-10 7.6461240e-11], sum to 1.0000
[2019-04-07 12:07:07,248] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9332
[2019-04-07 12:07:07,295] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 24.0, 23.51650748754967, -0.08263890629899019, 0.0, 1.0, 37064.206231036886], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4237200.0000, 
sim time next is 4239000.0000, 
raw observation next is [2.5, 46.5, 0.0, 0.0, 24.0, 23.50365517959596, -0.08256014465507468, 0.0, 1.0, 40258.28904043821], 
processed observation next is [0.0, 0.043478260869565216, 0.5318559556786704, 0.465, 0.0, 0.0, 0.5, 0.45863793163299665, 0.47247995178164176, 0.0, 1.0, 0.191706138287801], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10035473], dtype=float32), -1.8428035]. 
=============================================
[2019-04-07 12:07:07,327] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[86.63812]
 [88.0526 ]
 [87.87125]
 [88.42735]
 [88.54056]], R is [[85.71277618]
 [85.85565186]
 [85.9970932 ]
 [86.13712311]
 [86.27574921]].
[2019-04-07 12:07:07,473] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.3233187e-23 1.6996445e-21 1.8258008e-18 6.3326601e-11 9.9153158e-19
 1.0000000e+00 5.2746080e-12 9.9955858e-14], sum to 1.0000
[2019-04-07 12:07:07,474] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2328
[2019-04-07 12:07:07,521] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 36.0, 0.0, 0.0, 24.0, 23.42669419707013, -0.1369926359142909, 0.0, 1.0, 23931.237702469738], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4080600.0000, 
sim time next is 4082400.0000, 
raw observation next is [-4.0, 38.0, 0.0, 0.0, 24.0, 23.25293754022963, -0.1515243385877474, 0.0, 1.0, 61263.95326449856], 
processed observation next is [1.0, 0.2608695652173913, 0.3518005540166205, 0.38, 0.0, 0.0, 0.5, 0.43774479501913593, 0.44949188713741756, 0.0, 1.0, 0.2917331107833265], 
reward next is 0.9940, 
noisyNet noise sample is [array([1.4837283], dtype=float32), 0.120353565]. 
=============================================
[2019-04-07 12:07:13,196] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:13,196] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:13,200] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run8
[2019-04-07 12:07:25,014] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:25,014] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:25,017] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run8
[2019-04-07 12:07:25,103] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:25,104] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:25,107] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run8
[2019-04-07 12:07:28,530] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6346817e-26 4.0699587e-23 1.0721124e-20 2.4524445e-11 1.9065867e-20
 1.0000000e+00 1.6085295e-12 8.2775326e-14], sum to 1.0000
[2019-04-07 12:07:28,531] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5219
[2019-04-07 12:07:28,590] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 24.0, 23.6384831860826, -0.0008195432888232768, 0.0, 1.0, 9652.02065919921], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4498200.0000, 
sim time next is 4500000.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 24.0, 23.4157302574384, 0.002288536393709609, 0.0, 1.0, 93827.08147589261], 
processed observation next is [1.0, 0.08695652173913043, 0.44598337950138506, 0.73, 0.0, 0.0, 0.5, 0.45131085478653343, 0.5007628454645698, 0.0, 1.0, 0.44679562607567913], 
reward next is 0.8389, 
noisyNet noise sample is [array([1.1510409], dtype=float32), 1.4596064]. 
=============================================
[2019-04-07 12:07:28,593] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[104.47479 ]
 [105.56674 ]
 [106.86855 ]
 [107.716225]
 [109.570595]], R is [[103.62243652]
 [103.58621216]
 [103.550354  ]
 [103.47768402]
 [103.44290924]].
[2019-04-07 12:07:33,893] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:33,893] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:33,903] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run8
[2019-04-07 12:07:39,520] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:39,520] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:39,523] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run8
[2019-04-07 12:07:41,480] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:41,480] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:41,488] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run8
[2019-04-07 12:07:44,534] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:44,535] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:44,550] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run8
[2019-04-07 12:07:46,298] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:46,298] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:46,307] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run8
[2019-04-07 12:07:47,460] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:47,461] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:47,464] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run8
[2019-04-07 12:07:48,286] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:48,286] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:48,290] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run8
[2019-04-07 12:07:51,181] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:51,181] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:51,184] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run8
[2019-04-07 12:07:53,714] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:53,714] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:53,718] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run8
[2019-04-07 12:07:56,326] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:56,327] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:56,334] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run8
[2019-04-07 12:07:57,608] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:57,609] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:57,612] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run8
[2019-04-07 12:07:57,717] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:07:57,717] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:07:57,733] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run8
[2019-04-07 12:08:05,794] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6706535e-21 5.2195404e-21 2.7390332e-18 6.5981887e-11 4.1328339e-17
 1.0000000e+00 2.9506996e-12 3.3164330e-13], sum to 1.0000
[2019-04-07 12:08:05,794] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1935
[2019-04-07 12:08:05,898] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 24.0, 21.84979016360088, -0.4837773635803921, 0.0, 1.0, 49679.42974310452], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 354600.0000, 
sim time next is 356400.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 24.0, 21.61806791678144, -0.5323228641449304, 0.0, 1.0, 50031.51935413796], 
processed observation next is [1.0, 0.13043478260869565, 0.04709141274238226, 0.69, 0.0, 0.0, 0.5, 0.3015056597317865, 0.3225590452850232, 0.0, 1.0, 0.23824533025779981], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2410672], dtype=float32), -0.4015151]. 
=============================================
[2019-04-07 12:08:15,298] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.3410723e-26 4.4643133e-25 7.5371475e-23 5.2930915e-12 1.0532380e-21
 1.0000000e+00 1.1460871e-13 2.5611011e-15], sum to 1.0000
[2019-04-07 12:08:15,298] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7016
[2019-04-07 12:08:15,528] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 68.5, 153.0, 0.0, 24.0, 23.90983615766942, -0.105755535905208, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 214200.0000, 
sim time next is 216000.0000, 
raw observation next is [-5.0, 65.0, 141.0, 0.0, 24.0, 23.78711745744437, -0.1411336704134757, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.32409972299168976, 0.65, 0.47, 0.0, 0.5, 0.48225978812036424, 0.45295544319550807, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28538853], dtype=float32), 1.0124112]. 
=============================================
[2019-04-07 12:08:15,531] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[106.65661]
 [106.5311 ]
 [106.30909]
 [106.26745]
 [105.0704 ]], R is [[107.36631012]
 [107.29264832]
 [107.17279053]
 [107.10106659]
 [107.03005981]].
[2019-04-07 12:08:15,888] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2226188e-25 5.8707171e-24 4.4474093e-21 6.7415028e-12 8.8331690e-20
 1.0000000e+00 2.0722746e-12 3.7737825e-14], sum to 1.0000
[2019-04-07 12:08:15,888] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3288
[2019-04-07 12:08:15,934] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [19.4, 49.0, 92.0, 0.0, 24.0, 25.88813526283057, 0.5753741186144535, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1092600.0000, 
sim time next is 1094400.0000, 
raw observation next is [19.4, 49.0, 63.5, 0.0, 24.0, 26.11044756130483, 0.6122708971701861, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 1.0, 0.49, 0.21166666666666667, 0.0, 0.5, 0.6758706301087359, 0.7040902990567287, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3219254], dtype=float32), -0.28430703]. 
=============================================
[2019-04-07 12:08:22,111] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-07 12:08:22,115] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:08:22,116] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:08:22,118] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run17
[2019-04-07 12:08:22,213] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:08:22,213] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:08:22,215] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run17
[2019-04-07 12:08:22,255] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 12:08:22,256] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:08:22,258] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run17
[2019-04-07 12:10:32,956] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 12:10:50,565] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 12:10:54,648] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 12:10:55,670] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 320000, evaluation results [320000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 12:10:56,233] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.7063454e-28 1.9563710e-26 5.0856682e-23 8.8244394e-14 2.5616979e-22
 1.0000000e+00 2.2331048e-14 4.2343148e-16], sum to 1.0000
[2019-04-07 12:10:56,233] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4121
[2019-04-07 12:10:56,426] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 24.0, 23.0016408419403, -0.2328284263486727, 1.0, 1.0, 58982.0398880473], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 243000.0000, 
sim time next is 244800.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 24.0, 23.02039917676786, -0.1651399409682288, 0.0, 1.0, 96923.80047000722], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.5, 0.41836659806398835, 0.4449533530105904, 0.0, 1.0, 0.4615419070000344], 
reward next is 0.8242, 
noisyNet noise sample is [array([0.43521792], dtype=float32), -1.119636]. 
=============================================
[2019-04-07 12:11:06,480] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0487915e-24 1.7992529e-23 4.7286656e-21 2.6182133e-11 3.6945820e-19
 1.0000000e+00 7.1721969e-13 1.4634791e-13], sum to 1.0000
[2019-04-07 12:11:06,480] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2027
[2019-04-07 12:11:06,639] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.15, 61.5, 87.0, 512.0, 24.0, 23.88352419078864, -0.08747432777454438, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 297000.0000, 
sim time next is 298800.0000, 
raw observation next is [-10.6, 60.0, 96.5, 585.0, 24.0, 23.83136592708955, -0.09299692921417006, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.6, 0.32166666666666666, 0.6464088397790055, 0.5, 0.48594716059079585, 0.4690010235952766, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6616489], dtype=float32), 1.1443866]. 
=============================================
[2019-04-07 12:11:11,939] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3160751e-23 3.2391774e-23 6.4782847e-19 1.9272455e-11 2.2866279e-18
 1.0000000e+00 3.7399719e-12 5.9158131e-13], sum to 1.0000
[2019-04-07 12:11:11,939] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0506
[2019-04-07 12:11:12,206] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.55, 68.5, 30.0, 386.0, 24.0, 23.84345829796423, -0.08247579914918828, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 289800.0000, 
sim time next is 291600.0000, 
raw observation next is [-12.3, 67.0, 62.5, 384.5, 24.0, 23.9912551385333, -0.09008332035230737, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.12188365650969527, 0.67, 0.20833333333333334, 0.4248618784530387, 0.5, 0.49927126154444174, 0.46997222654923093, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48500702], dtype=float32), 0.48202977]. 
=============================================
[2019-04-07 12:11:18,180] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.3057797e-24 1.4326364e-22 4.7248815e-20 4.1464825e-11 2.5342438e-19
 1.0000000e+00 8.2056361e-13 1.6326989e-15], sum to 1.0000
[2019-04-07 12:11:18,181] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3695
[2019-04-07 12:11:18,231] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.0, 57.0, 0.0, 0.0, 24.0, 24.91994980498164, 0.3957106900758089, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1105200.0000, 
sim time next is 1107000.0000, 
raw observation next is [14.4, 58.5, 0.0, 0.0, 24.0, 24.65634358072843, 0.3502411521632067, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.585, 0.0, 0.0, 0.5, 0.5546952983940358, 0.616747050721069, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.553387], dtype=float32), 0.89898056]. 
=============================================
[2019-04-07 12:11:18,247] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[ 97.759186]
 [ 98.492355]
 [ 99.43347 ]
 [100.42175 ]
 [101.19398 ]], R is [[97.58544922]
 [97.60959625]
 [97.63349915]
 [97.65716553]
 [97.6805954 ]].
[2019-04-07 12:11:42,598] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1979233e-24 6.1410163e-24 1.0995017e-21 1.5640895e-11 8.8682777e-21
 1.0000000e+00 5.3535796e-14 1.6834920e-15], sum to 1.0000
[2019-04-07 12:11:42,598] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4344
[2019-04-07 12:11:42,757] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 53.0, 0.0, 0.0, 24.0, 23.24883185762052, -0.1076820458773353, 1.0, 1.0, 44905.6831833461], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 759600.0000, 
sim time next is 761400.0000, 
raw observation next is [-4.45, 55.5, 0.0, 0.0, 24.0, 23.21886493125773, -0.1510907761139799, 1.0, 1.0, 12563.05142077202], 
processed observation next is [1.0, 0.8260869565217391, 0.3393351800554017, 0.555, 0.0, 0.0, 0.5, 0.4349054109381442, 0.4496364079620067, 1.0, 1.0, 0.05982405438462867], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.4717438], dtype=float32), 1.1113147]. 
=============================================
[2019-04-07 12:12:04,196] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.0397147e-19 3.4840454e-18 1.2191972e-15 1.6860495e-08 1.4395057e-15
 1.0000000e+00 4.6804433e-10 9.2582539e-11], sum to 1.0000
[2019-04-07 12:12:04,196] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6874
[2019-04-07 12:12:04,222] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 24.0, 22.51735502700479, -0.09686503966195081, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1227600.0000, 
sim time next is 1229400.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 24.0, 22.46270373387445, -0.1011091377773739, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.5, 0.3718919778228707, 0.46629695407420874, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.827578], dtype=float32), 0.024752254]. 
=============================================
[2019-04-07 12:12:06,938] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9740628e-27 7.7951568e-27 5.0698789e-24 8.8428884e-14 4.6152465e-23
 1.0000000e+00 7.3931729e-15 2.2292694e-15], sum to 1.0000
[2019-04-07 12:12:06,938] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4388
[2019-04-07 12:12:07,089] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 24.0, 22.73698565607281, -0.04149020907115184, 1.0, 1.0, 101782.37808794303], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1708200.0000, 
sim time next is 1710000.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 24.0, 23.2734976500363, 0.04253672442824993, 1.0, 1.0, 40650.958148603866], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.5, 0.4394581375030251, 0.51417890814275, 1.0, 1.0, 0.19357599118382793], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.98996836], dtype=float32), 0.78775465]. 
=============================================
[2019-04-07 12:12:07,106] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[110.30869 ]
 [109.691895]
 [110.15677 ]
 [110.43616 ]
 [111.49347 ]], R is [[110.52497864]
 [110.22076416]
 [110.11856079]
 [110.01737976]
 [109.91720581]].
[2019-04-07 12:12:17,804] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3086408e-29 9.1903672e-28 4.8310522e-25 4.3876307e-13 2.8976158e-23
 1.0000000e+00 1.9275551e-14 4.8425509e-16], sum to 1.0000
[2019-04-07 12:12:17,805] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8827
[2019-04-07 12:12:17,849] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.55, 64.5, 200.0, 88.0, 24.0, 24.95292014975103, 0.2826435163520918, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1593000.0000, 
sim time next is 1594800.0000, 
raw observation next is [9.4, 61.0, 208.0, 168.5, 24.0, 24.99984499040062, 0.316233839204403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7229916897506927, 0.61, 0.6933333333333334, 0.1861878453038674, 0.5, 0.5833204158667185, 0.605411279734801, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.56877935], dtype=float32), 0.4858068]. 
=============================================
[2019-04-07 12:12:28,926] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0064942e-21 8.6528417e-20 1.0112894e-16 2.6553058e-09 4.7145563e-16
 1.0000000e+00 7.9674595e-10 9.2139924e-13], sum to 1.0000
[2019-04-07 12:12:28,926] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4455
[2019-04-07 12:12:29,022] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 80.5, 0.0, 0.0, 24.0, 22.90280793429552, -0.2473526244445009, 0.0, 1.0, 46038.17967635431], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1899000.0000, 
sim time next is 1900800.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 24.0, 22.81601008242322, -0.2649876647995604, 0.0, 1.0, 46051.758376965656], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.5, 0.40133417353526823, 0.41167077840014654, 0.0, 1.0, 0.21929408750936027], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05974165], dtype=float32), -0.20041977]. 
=============================================
[2019-04-07 12:12:58,282] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.4198699e-26 1.5684034e-22 2.5151432e-21 5.6612787e-12 5.3124863e-21
 1.0000000e+00 2.3332150e-13 2.7364382e-14], sum to 1.0000
[2019-04-07 12:12:58,282] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7826
[2019-04-07 12:12:58,378] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 91.0, 0.0, 0.0, 24.0, 23.18738018180871, -0.134144984199091, 0.0, 1.0, 44032.590372976825], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2077200.0000, 
sim time next is 2079000.0000, 
raw observation next is [-4.5, 88.5, 0.0, 0.0, 24.0, 23.15880714291628, -0.1376264302384556, 0.0, 1.0, 44000.38871572329], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.885, 0.0, 0.0, 0.5, 0.42990059524302343, 0.45412452325384817, 0.0, 1.0, 0.20952566055106328], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11640929], dtype=float32), -0.30771965]. 
=============================================
[2019-04-07 12:12:58,382] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[100.41935]
 [101.30734]
 [101.50707]
 [102.81203]
 [103.39073]], R is [[99.34269714]
 [99.34927368]
 [99.35578156]
 [99.36222839]
 [99.36860657]].
[2019-04-07 12:13:08,407] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.6227070e-27 1.1144180e-25 1.0292710e-22 1.8286225e-12 1.8676284e-20
 1.0000000e+00 6.5087943e-14 3.0147190e-14], sum to 1.0000
[2019-04-07 12:13:08,407] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6301
[2019-04-07 12:13:08,440] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 69.5, 293.0, 101.0, 24.0, 23.96142819519491, -0.01662687304721657, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2115000.0000, 
sim time next is 2116800.0000, 
raw observation next is [-6.7, 64.0, 222.0, 117.5, 24.0, 23.95279697074818, -0.02848681509276827, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2770083102493075, 0.64, 0.74, 0.1298342541436464, 0.5, 0.49606641422901515, 0.4905043949690773, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23364556], dtype=float32), 1.8321406]. 
=============================================
[2019-04-07 12:13:17,756] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4895679e-24 4.8733121e-24 2.6629241e-20 3.3083748e-11 1.3403215e-19
 1.0000000e+00 4.0579921e-11 1.5589235e-13], sum to 1.0000
[2019-04-07 12:13:17,756] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0368
[2019-04-07 12:13:18,014] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 51.0, 241.5, 71.5, 24.0, 23.34993079575428, -0.1022072772387223, 1.0, 1.0, 70063.24781038311], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2293200.0000, 
sim time next is 2295000.0000, 
raw observation next is [-1.15, 48.0, 221.0, 69.0, 24.0, 23.66632884396198, -0.04929560706743415, 1.0, 1.0, 18690.730496526638], 
processed observation next is [1.0, 0.5652173913043478, 0.4307479224376732, 0.48, 0.7366666666666667, 0.07624309392265194, 0.5, 0.4721940703301651, 0.48356813097752194, 1.0, 1.0, 0.08900347855488876], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12994125], dtype=float32), 0.81463355]. 
=============================================
[2019-04-07 12:13:18,017] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[106.09017 ]
 [105.64815 ]
 [105.268074]
 [105.049614]
 [105.09067 ]], R is [[105.52277374]
 [105.41962433]
 [105.36543274]
 [105.31178284]
 [105.25866699]].
[2019-04-07 12:13:30,465] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-07 12:13:30,485] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:13:30,486] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:13:30,488] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run18
[2019-04-07 12:13:30,605] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:13:30,605] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:13:30,607] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run18
[2019-04-07 12:13:30,673] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 12:13:30,673] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:13:30,676] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run18
[2019-04-07 12:15:43,039] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 12:16:01,369] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 12:16:03,534] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.3593 83805026.4189 32.8860
[2019-04-07 12:16:04,557] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 340000, evaluation results [340000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.3592590356807, 83805026.41892925, 32.88598103848735]
[2019-04-07 12:16:20,573] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2626281e-25 1.4905694e-23 2.7832558e-21 2.1692093e-11 1.2545410e-20
 1.0000000e+00 4.5171283e-13 1.3470241e-14], sum to 1.0000
[2019-04-07 12:16:20,574] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3266
[2019-04-07 12:16:20,653] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.75, 65.0, 0.0, 0.0, 24.0, 23.25753639654987, -0.1042014986745407, 0.0, 1.0, 43695.61489160464], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2593800.0000, 
sim time next is 2595600.0000, 
raw observation next is [-5.0, 68.0, 0.0, 0.0, 24.0, 23.43200633193377, -0.1030779995842715, 0.0, 1.0, 31301.48013715554], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.68, 0.0, 0.0, 0.5, 0.4526671943278142, 0.46564066680524285, 0.0, 1.0, 0.14905466731978828], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06027092], dtype=float32), 1.0036014]. 
=============================================
[2019-04-07 12:16:21,728] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3995836e-23 4.1504575e-21 1.5833814e-18 4.3754673e-11 7.6429080e-17
 1.0000000e+00 5.6705296e-12 2.2753147e-12], sum to 1.0000
[2019-04-07 12:16:21,728] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4889
[2019-04-07 12:16:21,778] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.4, 62.0, 24.0, 228.0, 24.0, 23.85254394853941, 0.02931780489551799, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4296600.0000, 
sim time next is 4298400.0000, 
raw observation next is [6.2, 64.0, 0.0, 0.0, 24.0, 23.73287273452377, -0.02069725291044318, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6343490304709142, 0.64, 0.0, 0.0, 0.5, 0.4777393945436475, 0.4931009156965189, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7916059], dtype=float32), 0.93130904]. 
=============================================
[2019-04-07 12:16:24,641] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7821374e-20 1.4415910e-19 2.4411246e-17 9.4004998e-11 1.3624125e-16
 1.0000000e+00 2.2199656e-10 1.3073262e-12], sum to 1.0000
[2019-04-07 12:16:24,641] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9303
[2019-04-07 12:16:24,681] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 24.0, 23.39197107620093, -0.05966738675658704, 0.0, 1.0, 19546.901569054105], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3609000.0000, 
sim time next is 3610800.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 24.0, 23.33934939641959, -0.07555562888770936, 0.0, 1.0, 18687.33707429482], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.5, 0.44494578303496574, 0.4748147903707636, 0.0, 1.0, 0.0889873194014039], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02457907], dtype=float32), -2.2184265]. 
=============================================
[2019-04-07 12:16:27,384] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5919767e-29 7.3074127e-27 2.1422062e-24 8.1070946e-14 2.7037819e-23
 1.0000000e+00 9.5169522e-15 5.3884636e-17], sum to 1.0000
[2019-04-07 12:16:27,384] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0265
[2019-04-07 12:16:27,425] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 149.0, 3.0, 24.0, 23.54415823763622, 0.08948224847956954, 1.0, 1.0, 81214.68117846512], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4453200.0000, 
sim time next is 4455000.0000, 
raw observation next is [0.0, 92.0, 196.0, 6.0, 24.0, 24.21935196021981, 0.184400245132168, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.46260387811634357, 0.92, 0.6533333333333333, 0.0066298342541436465, 0.5, 0.5182793300183176, 0.5614667483773893, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13007034], dtype=float32), -0.77796465]. 
=============================================
[2019-04-07 12:16:27,439] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[116.673996]
 [116.02924 ]
 [116.41241 ]
 [116.4866  ]
 [116.31533 ]], R is [[116.93649292]
 [116.66610718]
 [116.49945068]
 [116.3344574 ]
 [116.17111206]].
[2019-04-07 12:16:32,213] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6864787e-25 5.5657490e-25 1.2123388e-21 7.3554568e-13 1.2469025e-20
 1.0000000e+00 8.6599223e-13 9.9002176e-15], sum to 1.0000
[2019-04-07 12:16:32,214] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8863
[2019-04-07 12:16:32,242] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 28.0, 38.0, 61.0, 24.0, 23.60057200863181, -0.08951777305180732, 1.0, 1.0, 19987.003802883664], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2826000.0000, 
sim time next is 2827800.0000, 
raw observation next is [5.5, 29.0, 5.0, 46.0, 24.0, 23.84882208566151, -0.1005622327487004, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6149584487534627, 0.29, 0.016666666666666666, 0.05082872928176796, 0.5, 0.4874018404717925, 0.4664792557504332, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65359455], dtype=float32), 2.7961915]. 
=============================================
[2019-04-07 12:16:53,158] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.5503190e-24 1.0025627e-23 4.3440371e-21 7.4391590e-11 5.9269843e-20
 1.0000000e+00 1.0409012e-11 5.6475576e-13], sum to 1.0000
[2019-04-07 12:16:53,159] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3661
[2019-04-07 12:16:53,285] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 53.0, 95.5, 579.0, 24.0, 24.23087300627515, 0.1004928407787922, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3920400.0000, 
sim time next is 3922200.0000, 
raw observation next is [-7.5, 51.0, 100.0, 692.0, 24.0, 24.42287931777762, 0.1566894788587688, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2548476454293629, 0.51, 0.3333333333333333, 0.7646408839779005, 0.5, 0.5352399431481351, 0.5522298262862563, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6154488], dtype=float32), 0.7443826]. 
=============================================
[2019-04-07 12:16:56,405] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:16:56,405] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:16:56,408] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run9
[2019-04-07 12:17:00,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1042278e-25 2.2843020e-24 4.2072683e-22 1.5046659e-12 4.6992825e-21
 1.0000000e+00 3.3724716e-13 9.4703697e-17], sum to 1.0000
[2019-04-07 12:17:00,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8636
[2019-04-07 12:17:00,875] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 24.0, 23.78416798149084, 0.1053110427606442, 0.0, 1.0, 24154.908623658874], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3358800.0000, 
sim time next is 3360600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 24.0, 23.78456686790141, 0.07623194784654692, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.5, 0.48204723899178425, 0.5254106492821823, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4075344], dtype=float32), 0.09670841]. 
=============================================
[2019-04-07 12:17:05,468] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9383681e-22 1.3960277e-20 1.4346136e-18 8.0576573e-10 9.0918705e-18
 1.0000000e+00 2.2056579e-10 1.6620891e-12], sum to 1.0000
[2019-04-07 12:17:05,469] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6539
[2019-04-07 12:17:05,625] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 60.0, 0.0, 24.0, 22.54687812063861, -0.2668957315806915, 0.0, 1.0, 29581.169849390568], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 37800.0000, 
sim time next is 39600.0000, 
raw observation next is [7.7, 93.0, 67.5, 0.0, 24.0, 22.60556958809894, -0.2743104086866695, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.225, 0.0, 0.5, 0.38379746567491174, 0.4085631971044435, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7635642], dtype=float32), -1.0799406]. 
=============================================
[2019-04-07 12:17:12,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.57324182e-20 1.07906764e-19 3.03353394e-18 2.54464139e-09
 7.83897229e-17 1.00000000e+00 3.36045813e-09 2.81161292e-11], sum to 1.0000
[2019-04-07 12:17:12,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0130
[2019-04-07 12:17:12,488] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 46.0, 114.0, 812.0, 24.0, 23.58592578375457, 0.06469960641111282, 0.0, 1.0, 18695.594386662455], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3591000.0000, 
sim time next is 3592800.0000, 
raw observation next is [-1.0, 42.0, 108.0, 800.0, 24.0, 23.61151323434854, 0.07272270364587789, 0.0, 1.0, 12461.139030479933], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.36, 0.8839779005524862, 0.5, 0.46762610286237827, 0.5242409012152927, 0.0, 1.0, 0.059338757287999686], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6349346], dtype=float32), 0.7110764]. 
=============================================
[2019-04-07 12:17:23,091] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4675801e-24 4.0248579e-24 1.1303000e-21 1.4911046e-11 1.5039834e-19
 1.0000000e+00 2.1776732e-12 3.0755528e-13], sum to 1.0000
[2019-04-07 12:17:23,091] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9331
[2019-04-07 12:17:23,146] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 39.5, 19.0, 169.0, 24.0, 24.52095501487169, 0.08852755763101931, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3951000.0000, 
sim time next is 3952800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 24.0, 23.74432666885125, 0.03004599819448724, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.296398891966759, 0.41, 0.0, 0.0, 0.5, 0.4786938890709376, 0.5100153327314957, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.50947505], dtype=float32), -0.44317746]. 
=============================================
[2019-04-07 12:17:24,837] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:17:24,837] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:17:24,840] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run9
[2019-04-07 12:17:26,815] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.6210042e-23 1.1786754e-21 6.8657811e-19 1.4178589e-11 2.8098660e-18
 1.0000000e+00 5.3319429e-11 2.7505209e-13], sum to 1.0000
[2019-04-07 12:17:26,815] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6469
[2019-04-07 12:17:26,860] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.7, 71.0, 0.0, 0.0, 24.0, 23.24112588640982, -0.08879612905088437, 0.0, 1.0, 130811.93961286407], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4303800.0000, 
sim time next is 4305600.0000, 
raw observation next is [5.4, 73.0, 0.0, 0.0, 24.0, 23.43831224408289, 0.03367907143493132, 0.0, 1.0, 157960.71486956388], 
processed observation next is [0.0, 0.8695652173913043, 0.6121883656509697, 0.73, 0.0, 0.0, 0.5, 0.45319268700690757, 0.5112263571449771, 0.0, 1.0, 0.7521938803312566], 
reward next is 0.5335, 
noisyNet noise sample is [array([0.7951957], dtype=float32), -1.5881283]. 
=============================================
[2019-04-07 12:17:37,937] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:17:37,937] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:17:37,964] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run9
[2019-04-07 12:17:38,477] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1985285e-23 2.5387392e-22 5.1234133e-20 8.5924740e-11 2.9174309e-19
 1.0000000e+00 9.7806372e-12 4.4201375e-13], sum to 1.0000
[2019-04-07 12:17:38,477] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4681
[2019-04-07 12:17:38,518] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 24.0, 23.59871774527986, 0.03999365185018358, 0.0, 1.0, 64879.385301481896], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4672800.0000, 
sim time next is 4674600.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 24.0, 23.78471134079275, 0.07604956943646696, 0.0, 1.0, 26504.466154760576], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.5, 0.48205927839939583, 0.5253498564788223, 0.0, 1.0, 0.12621174359409798], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8443184], dtype=float32), 0.7983382]. 
=============================================
[2019-04-07 12:17:44,165] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:17:44,165] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:17:44,169] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run9
[2019-04-07 12:17:44,966] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1352595e-20 1.8885913e-20 1.4117801e-17 1.1327500e-09 1.3461650e-16
 1.0000000e+00 5.9471802e-11 2.4294720e-12], sum to 1.0000
[2019-04-07 12:17:44,967] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7563
[2019-04-07 12:17:45,101] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 24.0, 23.470481716833, -0.1063014611674857, 0.0, 1.0, 36219.020163349014], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4258800.0000, 
sim time next is 4260600.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 24.0, 23.46553807756678, -0.1065416492988902, 0.0, 1.0, 37201.58341066076], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 0.5, 0.4554615064638983, 0.46448611690036995, 0.0, 1.0, 0.17715039719362266], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5964783], dtype=float32), 1.2048528]. 
=============================================
[2019-04-07 12:17:51,924] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.4410464e-28 2.5445404e-27 7.4669974e-25 1.2063172e-12 1.2857651e-23
 1.0000000e+00 1.2108313e-15 8.1940459e-17], sum to 1.0000
[2019-04-07 12:17:51,925] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3169
[2019-04-07 12:17:52,063] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 78.0, 49.0, 0.0, 24.0, 24.46563997668365, 0.1555410036802132, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4465800.0000, 
sim time next is 4467600.0000, 
raw observation next is [0.0, 78.0, 37.0, 27.5, 24.0, 23.44862745770132, 0.04581162086127947, 1.0, 1.0, 44254.90677542796], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.78, 0.12333333333333334, 0.03038674033149171, 0.5, 0.4540522881417767, 0.5152705402870932, 1.0, 1.0, 0.21073765131156172], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.399259], dtype=float32), -0.12607145]. 
=============================================
[2019-04-07 12:17:56,021] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-07 12:17:56,046] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:17:56,046] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:17:56,047] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:17:56,047] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:17:56,047] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 12:17:56,048] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:17:56,103] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run19
[2019-04-07 12:17:56,135] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run19
[2019-04-07 12:17:56,189] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run19
[2019-04-07 12:20:09,041] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 12:20:26,408] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.1746 79463814.5229 95.0531
[2019-04-07 12:20:29,605] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 12:20:30,628] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 360000, evaluation results [360000.0, 2782.1745579556455, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 12:20:39,945] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:20:39,946] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:20:39,950] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run9
[2019-04-07 12:20:40,340] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:20:40,342] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:20:40,356] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run9
[2019-04-07 12:20:47,749] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:20:47,749] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:20:47,752] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run9
[2019-04-07 12:20:47,791] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.6948069e-24 3.0683610e-22 2.6431701e-19 3.4288784e-11 3.1614820e-19
 1.0000000e+00 2.5941839e-11 1.6319150e-14], sum to 1.0000
[2019-04-07 12:20:47,791] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1533
[2019-04-07 12:20:47,795] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:20:47,795] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:20:47,799] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run9
[2019-04-07 12:20:47,980] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.55, 68.5, 30.0, 386.0, 24.0, 23.84345829796423, -0.08247579914918828, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 289800.0000, 
sim time next is 291600.0000, 
raw observation next is [-12.3, 67.0, 62.5, 384.5, 24.0, 23.9912551385333, -0.09008332035230737, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.12188365650969527, 0.67, 0.20833333333333334, 0.4248618784530387, 0.5, 0.49927126154444174, 0.46997222654923093, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.56177324], dtype=float32), -0.1738289]. 
=============================================
[2019-04-07 12:20:49,174] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:20:49,174] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:20:49,188] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run9
[2019-04-07 12:20:51,753] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:20:51,753] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:20:51,758] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run9
[2019-04-07 12:20:52,217] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:20:52,217] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:20:52,221] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run9
[2019-04-07 12:20:55,494] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:20:55,494] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:20:55,506] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run9
[2019-04-07 12:20:56,133] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4296439e-23 1.0244138e-22 3.1945048e-19 2.3445888e-11 3.7368923e-19
 1.0000000e+00 2.6410631e-12 1.0158117e-11], sum to 1.0000
[2019-04-07 12:20:56,134] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7083
[2019-04-07 12:20:56,269] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:20:56,269] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:20:56,272] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run9
[2019-04-07 12:20:56,369] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 21.0, 0.0, 24.0, 21.9191293035185, -0.3492143463773412, 0.0, 1.0, 90934.28898703305], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 30600.0000, 
sim time next is 32400.0000, 
raw observation next is [7.7, 93.0, 29.5, 0.0, 24.0, 22.58013643996006, -0.2901820277045345, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.09833333333333333, 0.0, 0.5, 0.3816780366633384, 0.4032726574318219, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17782363], dtype=float32), 0.0055894735]. 
=============================================
[2019-04-07 12:20:56,409] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:20:56,409] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:20:56,412] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run9
[2019-04-07 12:20:56,831] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:20:56,831] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:20:56,835] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run9
[2019-04-07 12:20:58,944] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:20:58,944] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:20:58,947] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run9
[2019-04-07 12:21:02,120] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8047600e-22 5.4607536e-21 1.7019625e-19 1.4134935e-10 1.0171304e-18
 1.0000000e+00 6.9716552e-11 1.4394585e-12], sum to 1.0000
[2019-04-07 12:21:02,121] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9117
[2019-04-07 12:21:02,182] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.45, 94.5, 0.0, 0.0, 24.0, 20.75195243092939, -0.6874634758983964, 0.0, 1.0, 41825.57035195166], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 12600.0000, 
sim time next is 14400.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 24.0, 20.88710572992931, -0.6666360283300962, 0.0, 1.0, 41360.04505425665], 
processed observation next is [0.0, 0.17391304347826086, 0.6759002770083103, 0.93, 0.0, 0.0, 0.5, 0.24059214416077582, 0.2777879905566346, 0.0, 1.0, 0.19695259549646021], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3916715], dtype=float32), 0.5579539]. 
=============================================
[2019-04-07 12:21:18,469] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.06766746e-24 1.83703017e-23 1.23992984e-20 1.11441260e-12
 8.38326290e-20 1.00000000e+00 3.51288741e-13 7.93624471e-14], sum to 1.0000
[2019-04-07 12:21:18,469] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7318
[2019-04-07 12:21:18,534] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 24.0, 22.54635168236839, -0.2730862558903021, 0.0, 1.0, 45807.23814756623], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 172800.0000, 
sim time next is 174600.0000, 
raw observation next is [-8.65, 72.5, 0.0, 0.0, 24.0, 22.52368262434709, -0.2773421015053835, 0.0, 1.0, 45568.014004306366], 
processed observation next is [1.0, 0.0, 0.22299168975069253, 0.725, 0.0, 0.0, 0.5, 0.3769735520289241, 0.4075526328315388, 0.0, 1.0, 0.21699054287764935], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5593616], dtype=float32), 0.2741223]. 
=============================================
[2019-04-07 12:21:27,066] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8136838e-24 2.9140800e-21 3.4504602e-20 6.8386078e-12 3.1667447e-19
 1.0000000e+00 3.2714743e-13 2.9706774e-13], sum to 1.0000
[2019-04-07 12:21:27,066] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1133
[2019-04-07 12:21:27,279] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.7, 57.0, 0.0, 0.0, 24.0, 23.64281690501608, -0.02543424648211488, 1.0, 1.0, 152228.9442270752], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 324000.0000, 
sim time next is 325800.0000, 
raw observation next is [-12.0, 60.0, 0.0, 0.0, 24.0, 23.90391967683573, -0.0727189760550133, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.13019390581717452, 0.6, 0.0, 0.0, 0.5, 0.49199330640297756, 0.47576034131499556, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28635535], dtype=float32), -0.03601453]. 
=============================================
[2019-04-07 12:21:37,115] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1445957e-23 3.7372431e-22 5.0358135e-19 2.1023454e-11 1.1565053e-19
 1.0000000e+00 4.0408050e-12 7.3953057e-14], sum to 1.0000
[2019-04-07 12:21:37,144] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6828
[2019-04-07 12:21:37,241] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.65, 76.0, 0.0, 0.0, 24.0, 22.70817698976851, -0.261793579152117, 0.0, 1.0, 48403.48502198897], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 340200.0000, 
sim time next is 342000.0000, 
raw observation next is [-13.9, 70.0, 0.0, 0.0, 24.0, 22.56856721768349, -0.2983159613929023, 0.0, 1.0, 48295.11581031609], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.7, 0.0, 0.0, 0.5, 0.38071393480695753, 0.4005613462023659, 0.0, 1.0, 0.22997674195388615], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.988802], dtype=float32), -1.1932825]. 
=============================================
[2019-04-07 12:21:37,265] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[95.23369 ]
 [95.63042 ]
 [94.000946]
 [95.53575 ]
 [94.27965 ]], R is [[94.91763306]
 [94.96846008]
 [95.01877594]
 [95.06858826]
 [95.07774353]].
[2019-04-07 12:21:40,059] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.27353720e-23 4.55953121e-22 4.82793529e-19 4.93958430e-10
 1.92065445e-19 1.00000000e+00 1.31914445e-11 4.01764017e-13], sum to 1.0000
[2019-04-07 12:21:40,081] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7314
[2019-04-07 12:21:40,291] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 24.0, 23.45001549487514, -0.1255087753547393, 1.0, 1.0, 71526.24002011148], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 414000.0000, 
sim time next is 415800.0000, 
raw observation next is [-9.75, 41.0, 0.0, 0.0, 24.0, 23.42347391523294, -0.1197785301536521, 1.0, 1.0, 60667.17313736575], 
processed observation next is [1.0, 0.8260869565217391, 0.19252077562326872, 0.41, 0.0, 0.0, 0.5, 0.451956159602745, 0.460073823282116, 1.0, 1.0, 0.2888913006541226], 
reward next is 0.9968, 
noisyNet noise sample is [array([-1.1679057], dtype=float32), -0.9809425]. 
=============================================
[2019-04-07 12:21:54,061] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.7005067e-23 1.9684413e-23 2.9528664e-20 2.5837794e-11 6.0485430e-20
 1.0000000e+00 1.5519790e-12 3.7883288e-14], sum to 1.0000
[2019-04-07 12:21:54,061] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0386
[2019-04-07 12:21:54,130] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 87.0, 0.0, 0.0, 24.0, 22.78203412032366, -0.2591658211925887, 0.0, 1.0, 44610.15752545351], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2260800.0000, 
sim time next is 2262600.0000, 
raw observation next is [-8.65, 89.0, 0.0, 0.0, 24.0, 22.71125687555995, -0.2745687224074002, 0.0, 1.0, 44525.61376272383], 
processed observation next is [1.0, 0.17391304347826086, 0.22299168975069253, 0.89, 0.0, 0.0, 0.5, 0.39260473962999587, 0.40847709253086656, 0.0, 1.0, 0.2120267322034468], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03578432], dtype=float32), -0.5657245]. 
=============================================
[2019-04-07 12:22:01,153] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.5877090e-29 5.5168475e-28 2.2195519e-24 3.2259609e-14 7.4205211e-23
 1.0000000e+00 1.3415781e-14 3.0538310e-17], sum to 1.0000
[2019-04-07 12:22:01,153] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7860
[2019-04-07 12:22:01,194] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.55, 64.5, 200.0, 88.0, 24.0, 24.95292014975103, 0.2826435163520918, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1593000.0000, 
sim time next is 1594800.0000, 
raw observation next is [9.4, 61.0, 208.0, 168.5, 24.0, 24.99984499040062, 0.316233839204403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7229916897506927, 0.61, 0.6933333333333334, 0.1861878453038674, 0.5, 0.5833204158667185, 0.605411279734801, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01317037], dtype=float32), -0.3482459]. 
=============================================
[2019-04-07 12:22:01,953] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8644603e-28 1.9614758e-25 3.3977103e-24 3.5160800e-12 5.1933018e-22
 1.0000000e+00 7.0825398e-14 5.0494408e-16], sum to 1.0000
[2019-04-07 12:22:01,953] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1494
[2019-04-07 12:22:01,984] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.55, 70.0, 0.0, 0.0, 24.0, 24.21641742217381, 0.2190207874698238, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1625400.0000, 
sim time next is 1627200.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 24.0, 23.94439077088418, 0.1789770420408761, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6759002770083103, 0.74, 0.0, 0.0, 0.5, 0.49536589757368166, 0.5596590140136254, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03900832], dtype=float32), 1.7529585]. 
=============================================
[2019-04-07 12:22:08,173] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.48495650e-23 1.10431401e-22 1.03562534e-19 5.92806290e-11
 7.72845083e-19 1.00000000e+00 6.04974133e-13 7.03540633e-13], sum to 1.0000
[2019-04-07 12:22:08,174] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5180
[2019-04-07 12:22:08,314] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 87.0, 108.0, 0.0, 24.0, 23.04309172670231, -0.1003986566602454, 0.0, 1.0, 34109.18206184852], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1767600.0000, 
sim time next is 1769400.0000, 
raw observation next is [-2.3, 85.0, 119.0, 0.0, 24.0, 23.02156939917328, -0.08701930078651361, 0.0, 1.0, 51430.86993947084], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.85, 0.39666666666666667, 0.0, 0.5, 0.4184641165977734, 0.47099356640449547, 0.0, 1.0, 0.24490890447367067], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24896327], dtype=float32), 0.03369256]. 
=============================================
[2019-04-07 12:22:10,362] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0559456e-28 6.4726261e-27 4.0009643e-23 1.5909726e-12 1.3882887e-22
 1.0000000e+00 5.8466517e-15 2.1853081e-16], sum to 1.0000
[2019-04-07 12:22:10,364] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8456
[2019-04-07 12:22:10,413] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 78.0, 0.0, 0.0, 24.0, 23.81427729562689, 0.1286878364546634, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1054800.0000, 
sim time next is 1056600.0000, 
raw observation next is [13.55, 79.0, 0.0, 0.0, 24.0, 23.76697990760129, 0.1230991939454664, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.8379501385041552, 0.79, 0.0, 0.0, 0.5, 0.48058165896677413, 0.5410330646484888, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.50755876], dtype=float32), 1.3542724]. 
=============================================
[2019-04-07 12:22:15,471] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1908418e-21 6.5135433e-20 5.4836826e-18 3.5371844e-10 3.4255749e-16
 1.0000000e+00 7.3278779e-11 1.3748704e-11], sum to 1.0000
[2019-04-07 12:22:15,471] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7404
[2019-04-07 12:22:15,723] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.7, 100.0, 78.0, 0.0, 24.0, 22.37934695452884, 0.004148571123146219, 0.0, 1.0, 72737.69624129355], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1247400.0000, 
sim time next is 1249200.0000, 
raw observation next is [14.4, 100.0, 86.5, 0.0, 24.0, 23.11451879754863, 0.07183951664985282, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 1.0, 0.28833333333333333, 0.0, 0.5, 0.42620989979571916, 0.5239465055499509, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15717658], dtype=float32), 0.6227989]. 
=============================================
[2019-04-07 12:22:22,950] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3237917e-24 3.6079867e-22 8.7403561e-20 1.3590219e-10 6.2315315e-19
 1.0000000e+00 6.4835984e-12 9.3633770e-13], sum to 1.0000
[2019-04-07 12:22:22,950] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7085
[2019-04-07 12:22:23,002] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 80.5, 0.0, 0.0, 24.0, 23.88066138528933, 0.1270001830693181, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1560600.0000, 
sim time next is 1562400.0000, 
raw observation next is [5.0, 79.0, 0.0, 0.0, 24.0, 23.65186007587762, 0.0774395695927644, 0.0, 1.0, 34924.25550289363], 
processed observation next is [1.0, 0.08695652173913043, 0.6011080332409973, 0.79, 0.0, 0.0, 0.5, 0.47098833965646847, 0.5258131898642547, 0.0, 1.0, 0.16630597858520776], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3821352], dtype=float32), 0.61208487]. 
=============================================
[2019-04-07 12:22:42,835] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9924405e-25 6.7277006e-25 6.2237061e-22 5.0960187e-12 1.5412520e-21
 1.0000000e+00 8.6928667e-14 3.2278918e-15], sum to 1.0000
[2019-04-07 12:22:42,836] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5396
[2019-04-07 12:22:42,893] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 24.0, 23.59951397091974, 0.02843644259639349, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1409400.0000, 
sim time next is 1411200.0000, 
raw observation next is [-0.6, 100.0, 9.0, 0.0, 24.0, 23.44614192942466, -0.01535108529166244, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.03, 0.0, 0.5, 0.4538451607853882, 0.49488297156944583, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9093463], dtype=float32), 1.8026236]. 
=============================================
[2019-04-07 12:22:52,072] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.6264950e-23 2.6831005e-22 4.9449863e-21 2.7422773e-12 6.6230355e-20
 1.0000000e+00 1.6398929e-11 1.4297119e-12], sum to 1.0000
[2019-04-07 12:22:52,072] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7632
[2019-04-07 12:22:52,207] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 85.0, 0.0, 0.0, 24.0, 23.38185480697233, -0.01049315305444871, 0.0, 1.0, 46288.21397795829], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1744200.0000, 
sim time next is 1746000.0000, 
raw observation next is [-0.6, 83.0, 0.0, 0.0, 24.0, 23.37735778998062, -0.01372882197367658, 0.0, 1.0, 45892.553861669236], 
processed observation next is [0.0, 0.21739130434782608, 0.44598337950138506, 0.83, 0.0, 0.0, 0.5, 0.4481131491650518, 0.49542372600877443, 0.0, 1.0, 0.2185359707698535], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7587442], dtype=float32), -0.29320186]. 
=============================================
[2019-04-07 12:22:52,260] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[90.34745]
 [90.49172]
 [90.74108]
 [90.80515]
 [91.28393]], R is [[90.00051117]
 [90.10050964]
 [90.19950867]
 [90.29751587]
 [90.39453888]].
[2019-04-07 12:22:58,927] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7073835e-21 2.0621477e-21 1.1307502e-18 6.6577438e-11 1.1740513e-17
 1.0000000e+00 6.6491458e-13 4.5653563e-13], sum to 1.0000
[2019-04-07 12:22:58,927] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5592
[2019-04-07 12:22:59,218] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 85.0, 26.0, 0.0, 24.0, 23.4489374710716, -0.008445831017978708, 0.0, 1.0, 15066.488413980056], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1758600.0000, 
sim time next is 1760400.0000, 
raw observation next is [-1.7, 83.0, 45.5, 0.0, 24.0, 23.52397359511966, -0.03336155638123023, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.4155124653739613, 0.83, 0.15166666666666667, 0.0, 0.5, 0.46033113292663835, 0.4888794812062566, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1845397], dtype=float32), -1.2028943]. 
=============================================
[2019-04-07 12:22:59,413] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-07 12:22:59,414] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:22:59,414] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:22:59,418] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run20
[2019-04-07 12:22:59,516] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:22:59,516] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:22:59,522] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run20
[2019-04-07 12:22:59,561] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 12:22:59,564] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:22:59,577] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run20
[2019-04-07 12:25:15,817] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 12:25:34,143] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 12:25:38,466] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 12:25:39,488] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 380000, evaluation results [380000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 12:25:45,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6663072e-26 3.8815487e-25 3.7979790e-22 4.9693031e-14 5.9511097e-21
 1.0000000e+00 1.9221078e-13 3.5261823e-15], sum to 1.0000
[2019-04-07 12:25:45,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6768
[2019-04-07 12:25:45,864] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 87.0, 15.0, 0.0, 24.0, 23.22366489891439, -0.2014414853704705, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2016000.0000, 
sim time next is 2017800.0000, 
raw observation next is [-6.1, 86.5, 29.0, 0.0, 24.0, 23.40689115901016, -0.1549156306094019, 1.0, 1.0, 19400.124636486344], 
processed observation next is [1.0, 0.34782608695652173, 0.29362880886426596, 0.865, 0.09666666666666666, 0.0, 0.5, 0.45057426325084676, 0.4483614564635327, 1.0, 1.0, 0.09238154588803021], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2995241], dtype=float32), -0.5759312]. 
=============================================
[2019-04-07 12:25:53,891] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.40424693e-21 4.05538155e-20 2.37924110e-18 4.44501741e-10
 9.74858372e-16 1.00000000e+00 1.02881016e-10 4.07550469e-12], sum to 1.0000
[2019-04-07 12:25:53,891] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0285
[2019-04-07 12:25:54,164] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.75, 84.5, 0.0, 0.0, 24.0, 23.14072141314107, -0.1816702529490344, 0.0, 1.0, 43308.26801166913], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1877400.0000, 
sim time next is 1879200.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 24.0, 23.11800231915709, -0.1892267690860999, 0.0, 1.0, 44685.30450227655], 
processed observation next is [0.0, 0.782608695652174, 0.32409972299168976, 0.86, 0.0, 0.0, 0.5, 0.42650019326309074, 0.4369244103046334, 0.0, 1.0, 0.212787164296555], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06266712], dtype=float32), 0.32427222]. 
=============================================
[2019-04-07 12:27:01,975] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1080018e-27 6.7538262e-25 2.4821548e-21 1.5352678e-12 2.8041221e-21
 1.0000000e+00 1.5961207e-13 2.7413997e-15], sum to 1.0000
[2019-04-07 12:27:01,975] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2156
[2019-04-07 12:27:02,042] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.36230303e-24 7.46754192e-24 6.63615744e-21 1.25061905e-11
 4.69174388e-20 1.00000000e+00 2.86890737e-13 7.56549839e-14], sum to 1.0000
[2019-04-07 12:27:02,043] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9515
[2019-04-07 12:27:02,057] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 24.0, 23.40698692262767, 0.03978153417993907, 0.0, 1.0, 85076.17157495763], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3211200.0000, 
sim time next is 3213000.0000, 
raw observation next is [-1.5, 100.0, 0.0, 0.0, 24.0, 23.39026181034146, 0.07338177778775697, 0.0, 1.0, 78767.00004753216], 
processed observation next is [1.0, 0.17391304347826086, 0.4210526315789474, 1.0, 0.0, 0.0, 0.5, 0.4491884841951217, 0.524460592595919, 0.0, 1.0, 0.375080952607296], 
reward next is 0.9106, 
noisyNet noise sample is [array([-0.70553344], dtype=float32), 2.4064445]. 
=============================================
[2019-04-07 12:27:02,062] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[107.374695]
 [106.99046 ]
 [107.25688 ]
 [106.98005 ]
 [107.10462 ]], R is [[107.29618073]
 [107.10381317]
 [107.03277588]
 [106.96244812]
 [106.89282227]].
[2019-04-07 12:27:02,086] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 24.0, 23.42312766303703, -0.0773742745829732, 0.0, 1.0, 47039.49179977988], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3823200.0000, 
sim time next is 3825000.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 24.0, 23.31918178729608, -0.1068397877373567, 0.0, 1.0, 51177.729830775614], 
processed observation next is [1.0, 0.2608695652173913, 0.32409972299168976, 0.77, 0.0, 0.0, 0.5, 0.4432651489413401, 0.46438673742088116, 0.0, 1.0, 0.2437034753846458], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46393946], dtype=float32), -0.15319823]. 
=============================================
[2019-04-07 12:27:02,089] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[99.25121]
 [99.17077]
 [99.43534]
 [99.5425 ]
 [99.48462]], R is [[98.9345932 ]
 [98.94525146]
 [98.95580292]
 [98.96624756]
 [98.97658539]].
[2019-04-07 12:27:05,259] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:27:05,259] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:27:05,263] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run10
[2019-04-07 12:27:10,705] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9460597e-25 2.6715366e-24 7.1420191e-23 3.7889396e-12 1.5832936e-22
 1.0000000e+00 5.9511337e-13 2.9002929e-15], sum to 1.0000
[2019-04-07 12:27:10,705] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2943
[2019-04-07 12:27:10,775] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 115.5, 814.5, 24.0, 24.13619370923308, 0.1854157506233747, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3502800.0000, 
sim time next is 3504600.0000, 
raw observation next is [2.5, 50.5, 115.0, 806.0, 24.0, 24.79696535770502, 0.2804797610380711, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5318559556786704, 0.505, 0.38333333333333336, 0.8906077348066298, 0.5, 0.5664137798087516, 0.593493253679357, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2103169], dtype=float32), -0.81690645]. 
=============================================
[2019-04-07 12:27:13,458] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.0262110e-25 5.4680020e-23 3.6532089e-20 2.0613363e-11 1.0712749e-19
 1.0000000e+00 9.4671788e-12 1.4116019e-14], sum to 1.0000
[2019-04-07 12:27:13,459] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9737
[2019-04-07 12:27:13,668] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.0, 84.0, 44.0, 245.0, 24.0, 23.964480390723, 0.03466815818267265, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3312000.0000, 
sim time next is 3313800.0000, 
raw observation next is [-10.0, 80.5, 86.0, 396.0, 24.0, 24.08007756870574, 0.07933496541458518, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.18559556786703602, 0.805, 0.2866666666666667, 0.4375690607734807, 0.5, 0.5066731307254783, 0.5264449884715284, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15599798], dtype=float32), -0.15781207]. 
=============================================
[2019-04-07 12:27:22,984] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.1027916e-21 4.4138454e-20 7.7731304e-18 7.5224008e-11 9.8713153e-18
 1.0000000e+00 6.2901302e-11 2.7697099e-13], sum to 1.0000
[2019-04-07 12:27:22,985] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5427
[2019-04-07 12:27:23,118] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 24.0, 23.470481716833, -0.1063014611674857, 0.0, 1.0, 36219.020163349014], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4258800.0000, 
sim time next is 4260600.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 24.0, 23.46553807756678, -0.1065416492988902, 0.0, 1.0, 37201.58341066076], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 0.5, 0.4554615064638983, 0.46448611690036995, 0.0, 1.0, 0.17715039719362266], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06833284], dtype=float32), 2.4228218]. 
=============================================
[2019-04-07 12:27:24,238] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.1244998e-23 8.6920022e-22 6.1594460e-20 8.9938668e-11 4.3471727e-20
 1.0000000e+00 6.9373592e-13 5.5980394e-14], sum to 1.0000
[2019-04-07 12:27:24,238] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4035
[2019-04-07 12:27:24,383] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 24.0, 23.49686455923621, -0.01940151263524699, 0.0, 1.0, 52453.64349833984], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3807000.0000, 
sim time next is 3808800.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 24.0, 23.63290218226885, -0.02124577777095077, 0.0, 1.0, 15839.277382060907], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.77, 0.0, 0.0, 0.5, 0.46940851518907084, 0.49291807407634974, 0.0, 1.0, 0.07542513039076623], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08731931], dtype=float32), -0.71645564]. 
=============================================
[2019-04-07 12:27:31,675] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.6975282e-27 6.5936723e-26 1.2603444e-21 5.3313708e-12 6.8132763e-22
 1.0000000e+00 1.9582281e-13 3.5371890e-15], sum to 1.0000
[2019-04-07 12:27:31,675] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9319
[2019-04-07 12:27:31,726] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.4, 61.0, 0.0, 0.0, 24.0, 24.88886742673593, 0.3838910216827396, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4399200.0000, 
sim time next is 4401000.0000, 
raw observation next is [8.95, 61.5, 0.0, 0.0, 24.0, 24.57281854994812, 0.2700056712947128, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.7105263157894738, 0.615, 0.0, 0.0, 0.5, 0.5477348791623434, 0.5900018904315709, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3278236], dtype=float32), 1.1645588]. 
=============================================
[2019-04-07 12:27:31,766] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[108.68305 ]
 [109.115425]
 [109.187675]
 [109.8235  ]
 [110.402336]], R is [[108.74907684]
 [108.66159058]
 [108.57497406]
 [108.48922729]
 [108.40433502]].
[2019-04-07 12:27:50,573] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:27:50,653] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:27:50,663] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run10
[2019-04-07 12:27:57,733] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-07 12:27:57,737] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:27:57,737] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:27:57,739] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:27:57,740] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:27:57,742] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run21
[2019-04-07 12:27:57,794] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 12:27:57,794] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:27:57,796] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run21
[2019-04-07 12:27:57,857] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run21
[2019-04-07 12:29:05,060] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.10938515], dtype=float32), 0.12332559]
[2019-04-07 12:29:05,061] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [2.181712681, 89.02308879, 85.098394975, 0.0, 24.0, 23.10694303304332, -0.02003514922275602, 0.0, 1.0, 0.0]
[2019-04-07 12:29:05,061] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 12:29:05,062] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [1.8832421e-24 3.2613835e-23 1.0241414e-20 1.1630600e-11 8.2776022e-20
 1.0000000e+00 2.1149167e-12 4.5983792e-14], sampled 0.9192734836467255
[2019-04-07 12:30:07,825] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 12:30:25,908] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 12:30:28,820] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 12:30:29,843] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 400000, evaluation results [400000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 12:30:39,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0047567e-23 1.0129857e-21 4.9915190e-19 2.0523082e-12 3.1008553e-18
 1.0000000e+00 2.9917334e-13 2.0413458e-12], sum to 1.0000
[2019-04-07 12:30:39,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8899
[2019-04-07 12:30:39,989] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.5, 47.0, 0.0, 0.0, 24.0, 23.56669251751162, 0.05600304679707811, 0.0, 1.0, 51308.5600374758], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3965400.0000, 
sim time next is 3967200.0000, 
raw observation next is [-8.0, 49.0, 0.0, 0.0, 24.0, 23.52915584877078, 0.0445292234357762, 0.0, 1.0, 44567.51191756401], 
processed observation next is [1.0, 0.9565217391304348, 0.24099722991689754, 0.49, 0.0, 0.0, 0.5, 0.4607629873975651, 0.514843074478592, 0.0, 1.0, 0.21222624722649527], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.77185065], dtype=float32), -0.31148797]. 
=============================================
[2019-04-07 12:30:45,208] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:30:45,208] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:30:45,212] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run10
[2019-04-07 12:30:46,377] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:30:46,377] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:30:46,382] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run10
[2019-04-07 12:30:46,965] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2740014e-27 1.8966284e-26 2.0866445e-23 2.6117281e-12 3.5725425e-22
 1.0000000e+00 5.0174566e-14 2.8841186e-15], sum to 1.0000
[2019-04-07 12:30:46,966] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0975
[2019-04-07 12:30:47,058] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 78.0, 37.0, 27.5, 24.0, 23.44862745770132, 0.04581162086127947, 1.0, 1.0, 44254.90677542796], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4467600.0000, 
sim time next is 4469400.0000, 
raw observation next is [0.0, 75.0, 25.0, 55.0, 24.0, 24.09954656086408, 0.06493853772118806, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.75, 0.08333333333333333, 0.06077348066298342, 0.5, 0.5082955467386734, 0.521646179240396, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8294193], dtype=float32), 0.8826312]. 
=============================================
[2019-04-07 12:30:52,051] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8459964e-26 2.9404171e-24 5.8747139e-21 1.3024076e-13 2.1076223e-21
 1.0000000e+00 3.4043916e-13 4.3575771e-15], sum to 1.0000
[2019-04-07 12:30:52,051] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1225
[2019-04-07 12:30:52,197] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 69.0, 0.0, 0.0, 24.0, 23.55930655297082, -0.04427800697548803, 0.0, 1.0, 19187.662812931958], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4593600.0000, 
sim time next is 4595400.0000, 
raw observation next is [-1.75, 70.0, 0.0, 0.0, 24.0, 23.40617390774694, -0.03795088309289119, 0.0, 1.0, 72920.79962930513], 
processed observation next is [1.0, 0.17391304347826086, 0.4141274238227147, 0.7, 0.0, 0.0, 0.5, 0.450514492312245, 0.48734970563570296, 0.0, 1.0, 0.34724190299669105], 
reward next is 0.9385, 
noisyNet noise sample is [array([-0.34385288], dtype=float32), 1.5308264]. 
=============================================
[2019-04-07 12:31:09,081] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:31:09,081] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:31:09,084] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run10
[2019-04-07 12:31:09,263] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:31:09,263] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:31:09,267] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run10
[2019-04-07 12:31:11,853] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:31:11,853] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:31:11,858] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run10
[2019-04-07 12:31:13,605] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:31:13,605] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:31:13,613] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run10
[2019-04-07 12:31:14,917] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:31:14,917] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:31:14,922] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run10
[2019-04-07 12:31:15,477] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:31:15,477] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:31:15,503] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run10
[2019-04-07 12:31:16,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3028122e-25 6.8282005e-25 1.2188912e-21 6.0647602e-13 3.7679793e-21
 1.0000000e+00 2.3815173e-13 6.2438192e-16], sum to 1.0000
[2019-04-07 12:31:16,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6495
[2019-04-07 12:31:16,254] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.65, 96.0, 0.0, 0.0, 24.0, 22.89647259770059, 0.0756216996015368, 0.0, 1.0, 138309.1490886767], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1279800.0000, 
sim time next is 1281600.0000, 
raw observation next is [6.1, 96.0, 0.0, 0.0, 24.0, 23.4945586414144, 0.1996755008437875, 0.0, 1.0, 79953.54550694398], 
processed observation next is [0.0, 0.8695652173913043, 0.6315789473684211, 0.96, 0.0, 0.0, 0.5, 0.45787988678453334, 0.5665585002812624, 0.0, 1.0, 0.3807311690806856], 
reward next is 0.9050, 
noisyNet noise sample is [array([0.8142215], dtype=float32), -0.7844663]. 
=============================================
[2019-04-07 12:31:19,424] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.0081259e-24 1.0425837e-23 1.7799090e-20 2.1377712e-11 5.7883172e-20
 1.0000000e+00 7.1648823e-13 7.5898336e-15], sum to 1.0000
[2019-04-07 12:31:19,424] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5017
[2019-04-07 12:31:19,465] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 24.0, 24.12799298673521, 0.1496866050719168, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5004000.0000, 
sim time next is 5005800.0000, 
raw observation next is [3.0, 35.5, 0.0, 0.0, 24.0, 23.78352780995926, 0.02760465181939266, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.355, 0.0, 0.0, 0.5, 0.48196065082993833, 0.5092015506064642, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3408796], dtype=float32), 0.32212016]. 
=============================================
[2019-04-07 12:31:20,261] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:31:20,261] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:31:20,274] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run10
[2019-04-07 12:31:21,529] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:31:21,529] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:31:21,532] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run10
[2019-04-07 12:31:22,529] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:31:22,529] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:31:22,532] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run10
[2019-04-07 12:31:23,557] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:31:23,557] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:31:23,561] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run10
[2019-04-07 12:31:24,297] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:31:24,297] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:31:24,301] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run10
[2019-04-07 12:31:27,252] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:31:27,253] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:31:27,256] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run10
[2019-04-07 12:31:40,809] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2214804e-24 2.3816233e-23 2.8682898e-20 1.1074631e-11 1.4890210e-19
 1.0000000e+00 2.2337856e-11 1.5713975e-15], sum to 1.0000
[2019-04-07 12:31:40,821] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5796
[2019-04-07 12:31:41,022] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.95, 87.5, 0.0, 0.0, 24.0, 22.73537235287836, -0.2102347551463271, 0.0, 1.0, 28930.299042757204], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 63000.0000, 
sim time next is 64800.0000, 
raw observation next is [4.4, 89.0, 0.0, 0.0, 24.0, 22.73413845092564, -0.2001580577388831, 0.0, 1.0, 44047.32264158205], 
processed observation next is [0.0, 0.782608695652174, 0.5844875346260389, 0.89, 0.0, 0.0, 0.5, 0.3945115375771368, 0.4332806474203723, 0.0, 1.0, 0.209749155436105], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3516403], dtype=float32), -2.1932518]. 
=============================================
[2019-04-07 12:31:43,469] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7395924e-24 6.6674631e-23 3.3693330e-21 6.8602290e-12 1.7009491e-20
 1.0000000e+00 2.4185285e-12 1.1578988e-14], sum to 1.0000
[2019-04-07 12:31:43,469] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1070
[2019-04-07 12:31:43,811] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 17.0, 157.0, 24.0, 23.41624749455247, -0.1559444025204749, 1.0, 1.0, 53642.936141850834], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 201600.0000, 
sim time next is 203400.0000, 
raw observation next is [-8.65, 78.0, 34.0, 296.0, 24.0, 23.65645526464728, -0.1201095367562887, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.22299168975069253, 0.78, 0.11333333333333333, 0.3270718232044199, 0.5, 0.4713712720539401, 0.45996348774790374, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29693347], dtype=float32), -0.59203154]. 
=============================================
[2019-04-07 12:32:46,500] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3674358e-26 2.7102553e-24 1.4615121e-21 1.7911616e-12 9.6126586e-22
 1.0000000e+00 9.1043362e-13 1.0449052e-15], sum to 1.0000
[2019-04-07 12:32:46,500] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0620
[2019-04-07 12:32:46,551] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.05, 74.0, 0.0, 0.0, 24.0, 23.94779179250272, 0.211026177739967, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1125000.0000, 
sim time next is 1126800.0000, 
raw observation next is [10.5, 77.0, 0.0, 0.0, 24.0, 23.79545904142336, 0.1741675616895946, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.7534626038781165, 0.77, 0.0, 0.0, 0.5, 0.48295492011861335, 0.5580558538965316, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.8092284], dtype=float32), -1.7616224]. 
=============================================
[2019-04-07 12:33:01,319] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-07 12:33:01,337] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:33:01,337] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:33:01,340] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run22
[2019-04-07 12:33:01,408] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:33:01,408] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:33:01,432] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run22
[2019-04-07 12:33:01,487] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 12:33:01,487] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:33:01,489] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run22
[2019-04-07 12:35:18,716] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 12:35:37,133] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 12:35:40,010] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 12:35:41,032] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 420000, evaluation results [420000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 12:36:31,652] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6573094e-27 2.6813878e-25 1.1250679e-23 8.9558211e-13 1.5760143e-22
 1.0000000e+00 1.4405564e-13 1.5214066e-16], sum to 1.0000
[2019-04-07 12:36:31,653] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1867
[2019-04-07 12:36:31,809] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 24.0, 23.38085491732046, -0.07822555273060094, 1.0, 1.0, 23287.976488177086], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2055600.0000, 
sim time next is 2057400.0000, 
raw observation next is [-3.9, 84.0, 0.0, 0.0, 24.0, 23.28508138339079, -0.1022402428673094, 0.0, 1.0, 31241.06408099682], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.84, 0.0, 0.0, 0.5, 0.4404234486158991, 0.4659199190442302, 0.0, 1.0, 0.14876697181427057], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4359859], dtype=float32), 0.87928516]. 
=============================================
[2019-04-07 12:36:47,285] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.9711036e-26 2.0631507e-25 5.8671410e-22 2.9439241e-13 8.6022893e-20
 1.0000000e+00 8.2212080e-13 5.6445259e-15], sum to 1.0000
[2019-04-07 12:36:47,285] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7326
[2019-04-07 12:36:47,523] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.95, 89.0, 45.0, 16.0, 24.0, 23.76135805450556, -0.09166960434340238, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2277000.0000, 
sim time next is 2278800.0000, 
raw observation next is [-8.4, 87.0, 73.0, 27.5, 24.0, 24.00259332914349, -0.07529822568867035, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2299168975069252, 0.87, 0.24333333333333335, 0.03038674033149171, 0.5, 0.5002161107619575, 0.4749005914371099, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2530286], dtype=float32), -0.9019176]. 
=============================================
[2019-04-07 12:36:53,146] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.8578706e-20 1.0931590e-18 1.3288281e-16 6.3819936e-09 1.8095715e-15
 1.0000000e+00 8.6988722e-10 3.2648828e-11], sum to 1.0000
[2019-04-07 12:36:53,147] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2420
[2019-04-07 12:36:53,392] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.449999999999999, 46.5, 61.0, 665.0, 24.0, 23.51686045008995, -0.1460715454090017, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2453400.0000, 
sim time next is 2455200.0000, 
raw observation next is [-5.6, 43.0, 68.5, 721.0, 24.0, 23.35931444545297, -0.1638936643226483, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.43, 0.22833333333333333, 0.7966850828729282, 0.5, 0.4466095371210808, 0.4453687785591172, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6538053], dtype=float32), -1.030269]. 
=============================================
[2019-04-07 12:37:03,209] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1141808e-25 1.4749968e-24 8.4154237e-22 5.1082560e-10 2.0234526e-19
 1.0000000e+00 1.6243140e-13 2.9485041e-13], sum to 1.0000
[2019-04-07 12:37:03,209] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5619
[2019-04-07 12:37:03,274] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 62.5, 84.0, 704.0, 24.0, 25.18259042109601, 0.2160165984372022, 1.0, 1.0, 1245.360778015369], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3425400.0000, 
sim time next is 3427200.0000, 
raw observation next is [2.0, 67.0, 72.5, 608.5, 24.0, 24.61183829890578, 0.2336783938818884, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.24166666666666667, 0.6723756906077348, 0.5, 0.5509865249088151, 0.5778927979606295, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25133246], dtype=float32), 0.98777205]. 
=============================================
[2019-04-07 12:37:04,991] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.3129160e-27 2.2485588e-26 3.4081376e-22 2.0192179e-13 1.9768639e-21
 1.0000000e+00 7.1309170e-14 4.4585324e-15], sum to 1.0000
[2019-04-07 12:37:04,992] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7128
[2019-04-07 12:37:05,033] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 82.5, 0.0, 0.0, 24.0, 23.56805964710293, 0.02133949883365192, 0.0, 1.0, 41832.25364917484], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3457800.0000, 
sim time next is 3459600.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 24.0, 23.55033787020574, 0.01886289065654283, 0.0, 1.0, 38915.37819491825], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.79, 0.0, 0.0, 0.5, 0.4625281558504784, 0.5062876302188476, 0.0, 1.0, 0.18531132473770595], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.221599], dtype=float32), -1.0603826]. 
=============================================
[2019-04-07 12:37:11,962] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3690519e-26 1.1477081e-24 4.0114042e-22 5.2712061e-13 1.4172171e-20
 1.0000000e+00 4.0887209e-14 7.4263851e-15], sum to 1.0000
[2019-04-07 12:37:11,962] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8656
[2019-04-07 12:37:12,135] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 64.0, 108.0, 207.0, 24.0, 24.18199753867144, 0.008717532087352495, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2795400.0000, 
sim time next is 2797200.0000, 
raw observation next is [-6.0, 64.0, 130.0, 220.0, 24.0, 24.33361087577882, 0.002874464869297438, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.296398891966759, 0.64, 0.43333333333333335, 0.2430939226519337, 0.5, 0.5278009063149017, 0.5009581549564325, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00239999], dtype=float32), -1.0547488]. 
=============================================
[2019-04-07 12:37:28,947] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:37:28,947] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:37:28,950] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run11
[2019-04-07 12:37:35,088] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.94702328e-27 1.11959835e-27 2.16098525e-24 5.86886632e-14
 6.00361923e-22 1.00000000e+00 1.74544454e-14 2.54416503e-17], sum to 1.0000
[2019-04-07 12:37:35,088] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4185
[2019-04-07 12:37:35,153] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 72.0, 598.5, 24.0, 25.09800567947746, 0.3606935534990055, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3254400.0000, 
sim time next is 3256200.0000, 
raw observation next is [-3.5, 74.0, 59.0, 511.0, 24.0, 25.14168554688263, 0.3389224011446781, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.36565096952908593, 0.74, 0.19666666666666666, 0.5646408839779006, 0.5, 0.5951404622402192, 0.6129741337148927, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.008649], dtype=float32), 2.514096]. 
=============================================
[2019-04-07 12:37:46,032] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.8779585e-23 1.2272287e-21 4.3339451e-19 1.2561301e-10 7.7793888e-18
 1.0000000e+00 6.3206710e-13 3.5144539e-14], sum to 1.0000
[2019-04-07 12:37:46,032] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6377
[2019-04-07 12:37:46,106] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 24.0, 23.19416819773794, -0.08995344263812655, 0.0, 1.0, 41928.187143834984], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4764600.0000, 
sim time next is 4766400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 24.0, 23.08553588841091, -0.1115833476573287, 0.0, 1.0, 42048.57963657758], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.5, 0.42379465736757577, 0.4628055507808904, 0.0, 1.0, 0.2002313316027504], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3842213], dtype=float32), 0.18012297]. 
=============================================
[2019-04-07 12:37:47,330] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-07 12:37:47,386] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:37:47,387] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:37:47,390] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run23
[2019-04-07 12:37:47,415] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:37:47,416] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:37:47,418] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run23
[2019-04-07 12:37:47,436] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 12:37:47,436] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:37:47,438] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run23
[2019-04-07 12:38:45,719] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.10949571], dtype=float32), 0.124670886]
[2019-04-07 12:38:45,719] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [0.95, 59.5, 193.0, 244.0, 24.0, 24.02607805660272, 0.1576499227420297, 1.0, 1.0, 0.0]
[2019-04-07 12:38:45,719] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 12:38:45,721] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [4.6681535e-23 5.4326803e-22 9.0558971e-20 3.6276596e-11 1.1324726e-18
 1.0000000e+00 6.0736307e-12 2.5696079e-13], sampled 0.44791029512769653
[2019-04-07 12:39:51,058] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.10949571], dtype=float32), 0.124670886]
[2019-04-07 12:39:51,058] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-3.0, 84.0, 0.0, 0.0, 24.0, 23.20392650600022, -0.08636485750658608, 0.0, 1.0, 43746.82139864576]
[2019-04-07 12:39:51,058] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 12:39:51,059] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [1.0763732e-22 1.2501262e-21 2.4558424e-19 4.5962557e-11 1.6435257e-18
 1.0000000e+00 1.3622808e-11 4.2813260e-13], sampled 0.49962878365918284
[2019-04-07 12:40:03,048] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.10949571], dtype=float32), 0.124670886]
[2019-04-07 12:40:03,048] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-1.0, 42.0, 0.0, 0.0, 24.0, 23.33934939641959, -0.07555562888770936, 0.0, 1.0, 18687.33707429482]
[2019-04-07 12:40:03,048] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 12:40:03,049] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [5.0737007e-20 5.3024898e-19 4.7576431e-17 1.1819155e-09 2.6787623e-16
 1.0000000e+00 3.0935488e-10 1.1215460e-11], sampled 0.8635521094509511
[2019-04-07 12:40:05,114] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 12:40:22,947] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 12:40:24,837] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.3593 83805026.4189 32.8860
[2019-04-07 12:40:25,859] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 440000, evaluation results [440000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.3592590356807, 83805026.41892925, 32.88598103848735]
[2019-04-07 12:40:26,427] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.77325289e-21 2.86307566e-20 1.83951105e-17 1.00349715e-10
 1.27960447e-16 1.00000000e+00 5.58684522e-12 3.61336829e-13], sum to 1.0000
[2019-04-07 12:40:26,427] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9333
[2019-04-07 12:40:26,502] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 24.0, 23.46805355860836, -0.1053477941919925, 0.0, 1.0, 42389.05156264201], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4257000.0000, 
sim time next is 4258800.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 24.0, 23.470481716833, -0.1063014611674857, 0.0, 1.0, 36219.020163349014], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 0.5, 0.45587347640274994, 0.46456617961083807, 0.0, 1.0, 0.17247152458737625], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.131385], dtype=float32), 0.33713785]. 
=============================================
[2019-04-07 12:40:39,197] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:40:39,197] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:40:39,201] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run11
[2019-04-07 12:40:43,144] A3C_AGENT_WORKER-Thread-15 INFO:Local step 28500, global step 443544: loss 3.0238
[2019-04-07 12:40:43,147] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 28500, global step 443545: learning rate 0.0000
[2019-04-07 12:41:00,704] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:00,704] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:00,708] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run11
[2019-04-07 12:41:01,673] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:01,673] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:01,676] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run11
[2019-04-07 12:41:09,503] A3C_AGENT_WORKER-Thread-12 INFO:Local step 28500, global step 448281: loss 3.1828
[2019-04-07 12:41:09,503] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 28500, global step 448281: learning rate 0.0000
[2019-04-07 12:41:10,685] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3065375e-21 1.1956812e-20 2.7863831e-18 3.0943098e-10 8.0598692e-18
 1.0000000e+00 1.3396138e-11 8.3917994e-14], sum to 1.0000
[2019-04-07 12:41:10,685] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0636
[2019-04-07 12:41:10,737] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 45.0, 0.0, 0.0, 24.0, 23.61591179902994, -0.06386490474380559, 0.0, 1.0, 29366.591555302282], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4224600.0000, 
sim time next is 4226400.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 24.0, 23.5531708547421, -0.05558905249916202, 0.0, 1.0, 56418.75614158335], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.47, 0.0, 0.0, 0.5, 0.4627642378951749, 0.48147031583361266, 0.0, 1.0, 0.2686607435313493], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10541893], dtype=float32), 0.9107592]. 
=============================================
[2019-04-07 12:41:13,044] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3651601e-20 2.0586757e-19 9.5011942e-18 1.1082170e-08 7.4091170e-17
 1.0000000e+00 9.5965602e-10 2.1830172e-12], sum to 1.0000
[2019-04-07 12:41:13,047] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1426
[2019-04-07 12:41:13,080] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 32.0, 118.0, 847.0, 24.0, 23.22032579566284, -0.03173521134365389, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4192200.0000, 
sim time next is 4194000.0000, 
raw observation next is [2.0, 34.0, 166.0, 758.0, 24.0, 23.24125776687294, -0.02436796574963554, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.34, 0.5533333333333333, 0.8375690607734807, 0.5, 0.4367714805727451, 0.49187734475012146, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.55819017], dtype=float32), -0.71734536]. 
=============================================
[2019-04-07 12:41:13,111] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[81.08542]
 [81.13724]
 [81.42677]
 [82.17653]
 [82.94562]], R is [[81.79876709]
 [81.98078156]
 [82.1609726 ]
 [82.3393631 ]
 [82.51596832]].
[2019-04-07 12:41:24,412] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5540254e-20 3.3437012e-20 6.7120111e-18 2.6923768e-09 1.7266055e-16
 1.0000000e+00 6.6768827e-11 5.8068073e-12], sum to 1.0000
[2019-04-07 12:41:24,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3015
[2019-04-07 12:41:24,521] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 49.0, 0.0, 0.0, 24.0, 23.81339338594529, 0.0152907582906188, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4825800.0000, 
sim time next is 4827600.0000, 
raw observation next is [0.0, 51.0, 0.0, 0.0, 24.0, 23.71376737570707, -0.0242618392526832, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.46260387811634357, 0.51, 0.0, 0.0, 0.5, 0.47614728130892264, 0.4919127202491056, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.70300794], dtype=float32), -0.10756554]. 
=============================================
[2019-04-07 12:41:29,636] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:29,637] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:29,642] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run11
[2019-04-07 12:41:30,097] A3C_AGENT_WORKER-Thread-16 INFO:Local step 28500, global step 452577: loss 3.4227
[2019-04-07 12:41:30,097] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 28500, global step 452577: learning rate 0.0000
[2019-04-07 12:41:30,384] A3C_AGENT_WORKER-Thread-6 INFO:Local step 28500, global step 452634: loss 3.4359
[2019-04-07 12:41:30,385] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 28500, global step 452634: learning rate 0.0000
[2019-04-07 12:41:30,712] A3C_AGENT_WORKER-Thread-15 INFO:Local step 29000, global step 452718: loss 7.9702
[2019-04-07 12:41:30,713] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 29000, global step 452718: learning rate 0.0000
[2019-04-07 12:41:35,465] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:35,465] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:35,469] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run11
[2019-04-07 12:41:35,589] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:35,590] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:35,594] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run11
[2019-04-07 12:41:38,106] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:38,106] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:38,111] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run11
[2019-04-07 12:41:38,527] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:38,527] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:38,530] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run11
[2019-04-07 12:41:39,565] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:39,565] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:39,569] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run11
[2019-04-07 12:41:45,269] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:45,269] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:45,272] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run11
[2019-04-07 12:41:46,001] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:46,001] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:46,004] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run11
[2019-04-07 12:41:48,781] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:48,781] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:48,784] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run11
[2019-04-07 12:41:48,986] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:48,986] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:48,990] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run11
[2019-04-07 12:41:49,069] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:49,069] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:49,076] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run11
[2019-04-07 12:41:49,400] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:41:49,400] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:41:49,404] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run11
[2019-04-07 12:41:53,636] A3C_AGENT_WORKER-Thread-12 INFO:Local step 29000, global step 455978: loss 8.1072
[2019-04-07 12:41:53,636] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 29000, global step 455978: learning rate 0.0000
[2019-04-07 12:42:00,590] A3C_AGENT_WORKER-Thread-19 INFO:Local step 28500, global step 456771: loss 3.4034
[2019-04-07 12:42:00,592] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 28500, global step 456771: learning rate 0.0000
[2019-04-07 12:42:09,616] A3C_AGENT_WORKER-Thread-3 INFO:Local step 28500, global step 457664: loss 3.4798
[2019-04-07 12:42:09,617] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 28500, global step 457664: learning rate 0.0000
[2019-04-07 12:42:13,103] A3C_AGENT_WORKER-Thread-20 INFO:Local step 28500, global step 457959: loss 3.4753
[2019-04-07 12:42:13,104] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 28500, global step 457959: learning rate 0.0000
[2019-04-07 12:42:13,143] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8796998e-22 2.5253917e-21 2.0128613e-17 1.2183744e-11 1.2098096e-17
 1.0000000e+00 6.2290108e-12 3.0106536e-12], sum to 1.0000
[2019-04-07 12:42:13,143] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8774
[2019-04-07 12:42:13,281] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.9, 52.0, 0.0, 0.0, 24.0, 22.09860262423638, -0.4350366832222588, 0.0, 1.0, 46740.760959757645], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 441000.0000, 
sim time next is 442800.0000, 
raw observation next is [-10.6, 49.0, 0.0, 0.0, 24.0, 22.04272477203441, -0.4570458392370958, 0.0, 1.0, 46810.54773059485], 
processed observation next is [1.0, 0.13043478260869565, 0.1689750692520776, 0.49, 0.0, 0.0, 0.5, 0.3368937310028676, 0.3476513869209681, 0.0, 1.0, 0.22290737014568976], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.98012435], dtype=float32), -0.26458597]. 
=============================================
[2019-04-07 12:42:15,044] A3C_AGENT_WORKER-Thread-4 INFO:Local step 28500, global step 458140: loss 3.4300
[2019-04-07 12:42:15,044] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 28500, global step 458140: learning rate 0.0000
[2019-04-07 12:42:15,858] A3C_AGENT_WORKER-Thread-14 INFO:Local step 28500, global step 458210: loss 3.4390
[2019-04-07 12:42:15,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 28500, global step 458210: learning rate 0.0000
[2019-04-07 12:42:15,918] A3C_AGENT_WORKER-Thread-18 INFO:Local step 28500, global step 458214: loss 3.5108
[2019-04-07 12:42:15,919] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 28500, global step 458214: learning rate 0.0000
[2019-04-07 12:42:22,048] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6909125e-23 4.2738146e-23 4.1388544e-20 1.7774745e-11 9.5656588e-21
 1.0000000e+00 2.3380397e-13 6.2341388e-14], sum to 1.0000
[2019-04-07 12:42:22,048] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4762
[2019-04-07 12:42:22,208] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.1, 69.0, 0.0, 0.0, 24.0, 22.70667790682491, -0.2792771770628784, 0.0, 1.0, 46464.530783410264], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 268200.0000, 
sim time next is 270000.0000, 
raw observation next is [-8.9, 67.0, 0.0, 0.0, 24.0, 22.63238560105246, -0.3090695749970555, 0.0, 1.0, 46568.44196035645], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.67, 0.0, 0.0, 0.5, 0.3860321334210382, 0.3969768083343148, 0.0, 1.0, 0.22175448552550692], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1728994], dtype=float32), -1.3821808]. 
=============================================
[2019-04-07 12:42:22,234] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[93.228714]
 [93.880585]
 [94.525696]
 [95.47545 ]
 [96.86807 ]], R is [[92.69567871]
 [92.76872253]
 [92.84103394]
 [92.91262817]
 [92.98350525]].
[2019-04-07 12:42:23,486] A3C_AGENT_WORKER-Thread-2 INFO:Local step 28500, global step 458971: loss 3.4166
[2019-04-07 12:42:23,486] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 28500, global step 458971: learning rate 0.0000
[2019-04-07 12:42:25,600] A3C_AGENT_WORKER-Thread-6 INFO:Local step 29000, global step 459185: loss 8.1854
[2019-04-07 12:42:25,600] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 29000, global step 459185: learning rate 0.0000
[2019-04-07 12:42:25,736] A3C_AGENT_WORKER-Thread-11 INFO:Local step 28500, global step 459196: loss 3.3750
[2019-04-07 12:42:25,736] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 28500, global step 459196: learning rate 0.0000
[2019-04-07 12:42:26,074] A3C_AGENT_WORKER-Thread-16 INFO:Local step 29000, global step 459233: loss 8.2812
[2019-04-07 12:42:26,075] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 29000, global step 459233: learning rate 0.0000
[2019-04-07 12:42:26,276] A3C_AGENT_WORKER-Thread-15 INFO:Local step 29500, global step 459252: loss 2.6675
[2019-04-07 12:42:26,277] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 29500, global step 459252: learning rate 0.0000
[2019-04-07 12:42:29,333] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.1548922e-22 9.9913571e-21 1.1588951e-18 5.9605720e-10 2.7369678e-16
 1.0000000e+00 9.0764427e-12 1.7989326e-13], sum to 1.0000
[2019-04-07 12:42:29,334] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3844
[2019-04-07 12:42:29,764] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 49.0, 12.0, 123.0, 24.0, 23.83443391801812, 0.01898953694382678, 1.0, 1.0, 129475.6546757582], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 320400.0000, 
sim time next is 322200.0000, 
raw observation next is [-11.15, 53.0, 0.0, 0.0, 24.0, 24.21803916966133, -0.06379359194634687, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.15373961218836565, 0.53, 0.0, 0.0, 0.5, 0.518169930805111, 0.47873546935121775, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20745607], dtype=float32), 2.146418]. 
=============================================
[2019-04-07 12:42:29,950] A3C_AGENT_WORKER-Thread-17 INFO:Local step 28500, global step 459563: loss 3.4557
[2019-04-07 12:42:29,951] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 28500, global step 459563: learning rate 0.0000
[2019-04-07 12:42:30,343] A3C_AGENT_WORKER-Thread-5 INFO:Local step 28500, global step 459595: loss 3.4134
[2019-04-07 12:42:30,344] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 28500, global step 459595: learning rate 0.0000
[2019-04-07 12:42:30,891] A3C_AGENT_WORKER-Thread-13 INFO:Local step 28500, global step 459641: loss 3.3824
[2019-04-07 12:42:30,892] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 28500, global step 459641: learning rate 0.0000
[2019-04-07 12:42:33,468] A3C_AGENT_WORKER-Thread-10 INFO:Local step 28500, global step 459816: loss 3.3968
[2019-04-07 12:42:33,470] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 28500, global step 459816: learning rate 0.0000
[2019-04-07 12:42:35,642] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-07 12:42:35,645] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:42:35,645] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:42:35,647] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run24
[2019-04-07 12:42:35,726] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:42:35,726] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:42:35,728] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run24
[2019-04-07 12:42:35,765] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 12:42:35,765] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:42:35,768] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run24
[2019-04-07 12:44:53,540] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 12:45:11,268] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 12:45:14,274] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 12:45:15,297] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 460000, evaluation results [460000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 12:45:16,794] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7965585e-22 8.7736519e-22 5.3302103e-20 3.2206293e-11 3.7947138e-18
 1.0000000e+00 3.2197078e-11 1.4507677e-12], sum to 1.0000
[2019-04-07 12:45:16,795] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7469
[2019-04-07 12:45:16,963] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.95, 63.0, 71.0, 729.0, 24.0, 23.91509600848795, -0.1052254865608452, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 383400.0000, 
sim time next is 385200.0000, 
raw observation next is [-13.4, 60.0, 64.5, 746.5, 24.0, 23.81273392117757, -0.1280846958126492, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.09141274238227146, 0.6, 0.215, 0.8248618784530387, 0.5, 0.4843944934314643, 0.4573051013957836, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3816752], dtype=float32), -0.7536738]. 
=============================================
[2019-04-07 12:45:21,522] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7862951e-21 4.4048781e-21 3.0068627e-18 1.7214941e-10 7.8069769e-18
 1.0000000e+00 1.0248266e-10 1.0408218e-11], sum to 1.0000
[2019-04-07 12:45:21,522] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6410
[2019-04-07 12:45:21,671] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 83.0, 113.5, 270.0, 24.0, 23.13273101365264, -0.1036773107634101, 0.0, 1.0, 21293.64384140423], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 572400.0000, 
sim time next is 574200.0000, 
raw observation next is [-1.2, 83.0, 100.0, 73.0, 24.0, 23.07966411690581, -0.1145209109019648, 0.0, 1.0, 41268.961855148664], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.3333333333333333, 0.08066298342541436, 0.5, 0.42330534307548423, 0.46182636303267843, 0.0, 1.0, 0.1965188659768984], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9650873], dtype=float32), -0.7104764]. 
=============================================
[2019-04-07 12:45:30,330] A3C_AGENT_WORKER-Thread-12 INFO:Local step 29500, global step 461854: loss 2.3693
[2019-04-07 12:45:30,330] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 29500, global step 461854: learning rate 0.0000
[2019-04-07 12:45:37,092] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.5079656e-22 6.7334659e-21 8.2208104e-18 5.0984880e-12 9.9940737e-19
 1.0000000e+00 4.4840176e-10 6.4254992e-13], sum to 1.0000
[2019-04-07 12:45:37,092] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9596
[2019-04-07 12:45:37,169] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 75.0, 0.0, 0.0, 24.0, 22.73135572500144, -0.2749688890797136, 0.0, 1.0, 42147.12835878826], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 792000.0000, 
sim time next is 793800.0000, 
raw observation next is [-7.3, 73.0, 0.0, 0.0, 24.0, 22.66104220914791, -0.2899549540651938, 0.0, 1.0, 42285.62823498479], 
processed observation next is [1.0, 0.17391304347826086, 0.26038781163434904, 0.73, 0.0, 0.0, 0.5, 0.3884201840956593, 0.4033483486449354, 0.0, 1.0, 0.20136013445230852], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.68517405], dtype=float32), -1.1003814]. 
=============================================
[2019-04-07 12:45:41,331] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.3200341e-25 1.7479661e-22 1.1672171e-20 1.4696017e-10 1.1107756e-19
 1.0000000e+00 3.3602114e-11 7.6926118e-15], sum to 1.0000
[2019-04-07 12:45:41,332] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1821
[2019-04-07 12:45:41,373] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 24.0, 23.08381632569769, -0.2460823404846579, 0.0, 1.0, 42421.69852397421], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 711000.0000, 
sim time next is 712800.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 24.0, 23.15915211203254, -0.2257963600353633, 0.0, 1.0, 42508.469670993196], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 0.5, 0.4299293426693784, 0.4247345466548789, 0.0, 1.0, 0.20242128414758664], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3834802], dtype=float32), -0.81503356]. 
=============================================
[2019-04-07 12:45:44,234] A3C_AGENT_WORKER-Thread-19 INFO:Local step 29000, global step 463722: loss 8.1278
[2019-04-07 12:45:44,240] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 29000, global step 463722: learning rate 0.0000
[2019-04-07 12:45:51,911] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9616913e-19 1.3761964e-17 8.9870535e-16 1.8692567e-09 2.6160913e-15
 1.0000000e+00 2.5950873e-09 1.3772963e-10], sum to 1.0000
[2019-04-07 12:45:51,912] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6206
[2019-04-07 12:45:51,979] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.1, 80.0, 0.0, 0.0, 24.0, 22.91566412512486, -0.02182922266632959, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1213200.0000, 
sim time next is 1215000.0000, 
raw observation next is [16.1, 81.5, 0.0, 0.0, 24.0, 22.84230837326274, -0.03465221117108133, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.9085872576177286, 0.815, 0.0, 0.0, 0.5, 0.4035256977718949, 0.48844926294297286, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.86377645], dtype=float32), 0.08764582]. 
=============================================
[2019-04-07 12:45:52,045] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.62486]
 [75.40183]
 [75.20785]
 [75.89994]
 [75.75902]], R is [[74.73743439]
 [74.9900589 ]
 [75.24015808]
 [75.48775482]
 [75.73287964]].
[2019-04-07 12:45:52,229] A3C_AGENT_WORKER-Thread-3 INFO:Local step 29000, global step 465077: loss 8.0432
[2019-04-07 12:45:52,230] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 29000, global step 465077: learning rate 0.0000
[2019-04-07 12:45:53,615] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2853875e-23 3.7499797e-23 4.6124879e-21 2.9291184e-11 1.3279848e-19
 1.0000000e+00 2.6783337e-12 1.8051185e-14], sum to 1.0000
[2019-04-07 12:45:53,615] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6471
[2019-04-07 12:45:53,633] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.3, 62.0, 0.0, 0.0, 24.0, 24.04136081932736, 0.2402376743333795, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1112400.0000, 
sim time next is 1114200.0000, 
raw observation next is [13.0, 63.0, 0.0, 0.0, 24.0, 23.88891099459587, 0.209682396720875, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8227146814404434, 0.63, 0.0, 0.0, 0.5, 0.4907425828829893, 0.5698941322402916, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16151161], dtype=float32), -0.5849412]. 
=============================================
[2019-04-07 12:45:54,583] A3C_AGENT_WORKER-Thread-20 INFO:Local step 29000, global step 465552: loss 8.0683
[2019-04-07 12:45:54,585] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 29000, global step 465552: learning rate 0.0000
[2019-04-07 12:45:55,569] A3C_AGENT_WORKER-Thread-4 INFO:Local step 29000, global step 465809: loss 7.9543
[2019-04-07 12:45:55,581] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 29000, global step 465809: learning rate 0.0000
[2019-04-07 12:45:55,594] A3C_AGENT_WORKER-Thread-14 INFO:Local step 29000, global step 465816: loss 8.0111
[2019-04-07 12:45:55,595] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 29000, global step 465817: learning rate 0.0000
[2019-04-07 12:45:56,389] A3C_AGENT_WORKER-Thread-18 INFO:Local step 29000, global step 465998: loss 8.0423
[2019-04-07 12:45:56,400] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 29000, global step 465998: learning rate 0.0000
[2019-04-07 12:45:58,060] A3C_AGENT_WORKER-Thread-6 INFO:Local step 29500, global step 466380: loss 2.7029
[2019-04-07 12:45:58,061] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 29500, global step 466380: learning rate 0.0000
[2019-04-07 12:45:58,146] A3C_AGENT_WORKER-Thread-16 INFO:Local step 29500, global step 466403: loss 2.4449
[2019-04-07 12:45:58,147] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 29500, global step 466403: learning rate 0.0000
[2019-04-07 12:45:58,410] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.9963087e-25 1.6342562e-23 3.6283729e-22 3.3594147e-12 4.9965627e-21
 1.0000000e+00 8.7580599e-13 6.3974522e-15], sum to 1.0000
[2019-04-07 12:45:58,411] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1262
[2019-04-07 12:45:58,488] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 24.0, 23.63928295598618, 0.1746749887814976, 0.0, 1.0, 24878.957741019538], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1290600.0000, 
sim time next is 1292400.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 24.0, 23.648031668378, 0.1620284637468383, 0.0, 1.0, 18831.845292679955], 
processed observation next is [0.0, 1.0, 0.6149584487534627, 1.0, 0.0, 0.0, 0.5, 0.4706693056981666, 0.5540094879156128, 0.0, 1.0, 0.08967545377466644], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18588525], dtype=float32), 0.026087154]. 
=============================================
[2019-04-07 12:46:01,291] A3C_AGENT_WORKER-Thread-2 INFO:Local step 29000, global step 467197: loss 8.0247
[2019-04-07 12:46:01,295] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 29000, global step 467197: learning rate 0.0000
[2019-04-07 12:46:02,848] A3C_AGENT_WORKER-Thread-11 INFO:Local step 29000, global step 467591: loss 8.1012
[2019-04-07 12:46:02,883] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 29000, global step 467593: learning rate 0.0000
[2019-04-07 12:46:03,235] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.6214548e-20 1.1310442e-17 5.5596871e-15 2.8573850e-09 2.3140534e-15
 1.0000000e+00 1.1591341e-10 4.6097234e-11], sum to 1.0000
[2019-04-07 12:46:03,235] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7614
[2019-04-07 12:46:03,248] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.55, 64.0, 29.0, 0.0, 24.0, 23.59417794662427, 0.1277732859122716, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1182600.0000, 
sim time next is 1184400.0000, 
raw observation next is [18.3, 65.0, 14.5, 0.0, 24.0, 23.54659785985492, 0.1169992888834359, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.65, 0.04833333333333333, 0.0, 0.5, 0.46221648832124335, 0.5389997629611453, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05239076], dtype=float32), -1.3390044]. 
=============================================
[2019-04-07 12:46:03,341] A3C_AGENT_WORKER-Thread-17 INFO:Local step 29000, global step 467707: loss 8.2755
[2019-04-07 12:46:03,346] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 29000, global step 467708: learning rate 0.0000
[2019-04-07 12:46:04,161] A3C_AGENT_WORKER-Thread-5 INFO:Local step 29000, global step 467901: loss 8.2882
[2019-04-07 12:46:04,195] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 29000, global step 467909: learning rate 0.0000
[2019-04-07 12:46:04,898] A3C_AGENT_WORKER-Thread-15 INFO:Local step 30000, global step 468051: loss 1.2811
[2019-04-07 12:46:04,900] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 30000, global step 468051: learning rate 0.0000
[2019-04-07 12:46:05,018] A3C_AGENT_WORKER-Thread-13 INFO:Local step 29000, global step 468076: loss 8.2286
[2019-04-07 12:46:05,033] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 29000, global step 468078: learning rate 0.0000
[2019-04-07 12:46:05,866] A3C_AGENT_WORKER-Thread-10 INFO:Local step 29000, global step 468284: loss 8.3522
[2019-04-07 12:46:05,868] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 29000, global step 468285: learning rate 0.0000
[2019-04-07 12:46:12,368] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0186878e-26 3.3507920e-25 1.1731127e-22 2.4320823e-12 1.5817007e-21
 1.0000000e+00 4.9909913e-13 1.8574234e-15], sum to 1.0000
[2019-04-07 12:46:12,368] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4905
[2019-04-07 12:46:12,442] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 24.0, 23.78092402075714, 0.1070632273464786, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1461600.0000, 
sim time next is 1463400.0000, 
raw observation next is [1.35, 92.0, 0.0, 0.0, 24.0, 23.45828956030094, 0.01761432942728905, 0.0, 1.0, 63821.55021228755], 
processed observation next is [1.0, 0.9565217391304348, 0.5000000000000001, 0.92, 0.0, 0.0, 0.5, 0.4548574633584117, 0.5058714431424297, 0.0, 1.0, 0.30391214386803594], 
reward next is 0.9818, 
noisyNet noise sample is [array([-1.0019172], dtype=float32), 0.5391976]. 
=============================================
[2019-04-07 12:46:17,954] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7585704e-24 1.1941022e-23 3.3370366e-20 3.0274308e-10 1.2291654e-18
 1.0000000e+00 6.0225392e-12 2.1184846e-13], sum to 1.0000
[2019-04-07 12:46:17,955] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1000
[2019-04-07 12:46:18,138] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 24.0, 22.58097063486317, -0.2453294219875967, 1.0, 1.0, 150367.40320334263], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1927800.0000, 
sim time next is 1929600.0000, 
raw observation next is [-9.5, 91.0, 17.5, 11.0, 24.0, 23.62909488802488, -0.1723413572741507, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.1994459833795014, 0.91, 0.058333333333333334, 0.012154696132596685, 0.5, 0.46909124066874003, 0.4425528809086164, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0887543], dtype=float32), 0.58140206]. 
=============================================
[2019-04-07 12:46:21,834] A3C_AGENT_WORKER-Thread-12 INFO:Local step 30000, global step 471339: loss 1.0469
[2019-04-07 12:46:21,837] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 30000, global step 471339: learning rate 0.0000
[2019-04-07 12:46:31,820] A3C_AGENT_WORKER-Thread-19 INFO:Local step 29500, global step 472785: loss 2.6516
[2019-04-07 12:46:31,821] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 29500, global step 472785: learning rate 0.0000
[2019-04-07 12:46:39,181] A3C_AGENT_WORKER-Thread-3 INFO:Local step 29500, global step 473767: loss 2.0816
[2019-04-07 12:46:39,182] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 29500, global step 473767: learning rate 0.0000
[2019-04-07 12:46:41,869] A3C_AGENT_WORKER-Thread-14 INFO:Local step 29500, global step 474184: loss 2.5595
[2019-04-07 12:46:41,870] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 29500, global step 474184: learning rate 0.0000
[2019-04-07 12:46:41,992] A3C_AGENT_WORKER-Thread-20 INFO:Local step 29500, global step 474207: loss 2.3581
[2019-04-07 12:46:41,992] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 29500, global step 474207: learning rate 0.0000
[2019-04-07 12:46:42,807] A3C_AGENT_WORKER-Thread-15 INFO:Local step 30500, global step 474321: loss 3.4412
[2019-04-07 12:46:42,807] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 30500, global step 474321: learning rate 0.0000
[2019-04-07 12:46:44,290] A3C_AGENT_WORKER-Thread-18 INFO:Local step 29500, global step 474520: loss 2.2905
[2019-04-07 12:46:44,291] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 29500, global step 474520: learning rate 0.0000
[2019-04-07 12:46:44,298] A3C_AGENT_WORKER-Thread-4 INFO:Local step 29500, global step 474522: loss 2.6880
[2019-04-07 12:46:44,298] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 29500, global step 474522: learning rate 0.0000
[2019-04-07 12:46:49,078] A3C_AGENT_WORKER-Thread-16 INFO:Local step 30000, global step 475147: loss 1.0079
[2019-04-07 12:46:49,079] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 30000, global step 475147: learning rate 0.0000
[2019-04-07 12:46:49,291] A3C_AGENT_WORKER-Thread-2 INFO:Local step 29500, global step 475180: loss 2.4857
[2019-04-07 12:46:49,292] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 29500, global step 475180: learning rate 0.0000
[2019-04-07 12:46:49,394] A3C_AGENT_WORKER-Thread-6 INFO:Local step 30000, global step 475200: loss 1.0602
[2019-04-07 12:46:49,395] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 30000, global step 475200: learning rate 0.0000
[2019-04-07 12:46:51,640] A3C_AGENT_WORKER-Thread-11 INFO:Local step 29500, global step 475505: loss 2.3072
[2019-04-07 12:46:51,641] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 29500, global step 475505: learning rate 0.0000
[2019-04-07 12:46:51,667] A3C_AGENT_WORKER-Thread-13 INFO:Local step 29500, global step 475511: loss 2.2201
[2019-04-07 12:46:51,668] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 29500, global step 475511: learning rate 0.0000
[2019-04-07 12:46:52,089] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7672247e-23 1.7810278e-22 9.3863690e-21 1.5125365e-11 5.6295048e-19
 1.0000000e+00 5.8570938e-13 2.4684402e-13], sum to 1.0000
[2019-04-07 12:46:52,089] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1463
[2019-04-07 12:46:52,151] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 35.0, 109.0, 724.0, 24.0, 24.76229618361241, 0.1579385326377241, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4096800.0000, 
sim time next is 4098600.0000, 
raw observation next is [-1.5, 33.5, 114.0, 769.0, 24.0, 24.89686373556102, 0.1952630057724516, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4210526315789474, 0.335, 0.38, 0.8497237569060774, 0.5, 0.574738644630085, 0.5650876685908172, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4148839], dtype=float32), 1.0159875]. 
=============================================
[2019-04-07 12:46:52,420] A3C_AGENT_WORKER-Thread-17 INFO:Local step 29500, global step 475639: loss 2.2627
[2019-04-07 12:46:52,437] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 29500, global step 475639: learning rate 0.0000
[2019-04-07 12:46:52,840] A3C_AGENT_WORKER-Thread-5 INFO:Local step 29500, global step 475692: loss 2.2274
[2019-04-07 12:46:52,848] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 29500, global step 475692: learning rate 0.0000
[2019-04-07 12:46:54,745] A3C_AGENT_WORKER-Thread-10 INFO:Local step 29500, global step 475988: loss 2.4019
[2019-04-07 12:46:54,745] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 29500, global step 475988: learning rate 0.0000
[2019-04-07 12:47:00,543] A3C_AGENT_WORKER-Thread-12 INFO:Local step 30500, global step 476991: loss 3.5700
[2019-04-07 12:47:00,545] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 30500, global step 476992: learning rate 0.0000
[2019-04-07 12:47:17,322] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0560123e-24 1.8602601e-22 6.9287621e-21 2.3737004e-12 1.8056098e-19
 1.0000000e+00 3.9559428e-12 6.1454416e-14], sum to 1.0000
[2019-04-07 12:47:17,322] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2816
[2019-04-07 12:47:17,395] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 24.0, 24.43329471059008, 0.2234390819981325, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3520800.0000, 
sim time next is 3522600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 24.0, 24.42956051362993, 0.1848859843986877, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.5, 0.5357967094691608, 0.5616286614662293, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12370568], dtype=float32), 0.6818628]. 
=============================================
[2019-04-07 12:47:27,672] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1543984e-19 1.0268299e-18 3.8832804e-18 1.9729698e-09 1.1389767e-16
 1.0000000e+00 1.7636900e-10 5.3939919e-11], sum to 1.0000
[2019-04-07 12:47:27,672] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2696
[2019-04-07 12:47:27,843] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 27.0, 0.0, 0.0, 24.0, 23.64784471005614, -0.07182912458289538, 0.0, 1.0, 22990.382968229023], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3646800.0000, 
sim time next is 3648600.0000, 
raw observation next is [9.5, 26.0, 0.0, 0.0, 24.0, 23.65180461454467, -0.07818922135708627, 0.0, 1.0, 18727.428328083402], 
processed observation next is [0.0, 0.21739130434782608, 0.7257617728531857, 0.26, 0.0, 0.0, 0.5, 0.47098371787872245, 0.47393692621430455, 0.0, 1.0, 0.08917823013373048], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.59168124], dtype=float32), 0.8649151]. 
=============================================
[2019-04-07 12:47:31,161] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-07 12:47:31,170] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:47:31,171] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:47:31,172] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run25
[2019-04-07 12:47:31,250] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:47:31,251] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:47:31,262] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run25
[2019-04-07 12:47:31,357] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 12:47:31,358] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:47:31,377] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run25
[2019-04-07 12:49:50,388] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 12:50:09,349] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.10989568], dtype=float32), 0.12623458]
[2019-04-07 12:50:09,349] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [8.25, 27.5, 0.0, 0.0, 24.0, 24.29843887305709, 0.1802566063684911, 0.0, 1.0, 0.0]
[2019-04-07 12:50:09,349] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 12:50:09,350] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [4.4946336e-22 8.3128808e-21 1.1768801e-18 1.1312898e-10 6.9679478e-18
 1.0000000e+00 2.7392309e-11 7.5079445e-13], sampled 0.41937704551337585
[2019-04-07 12:50:09,358] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 12:50:10,986] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 12:50:12,008] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 480000, evaluation results [480000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 12:50:12,621] A3C_AGENT_WORKER-Thread-15 INFO:Local step 31000, global step 480122: loss 13.6268
[2019-04-07 12:50:12,625] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 31000, global step 480123: learning rate 0.0000
[2019-04-07 12:50:20,502] A3C_AGENT_WORKER-Thread-19 INFO:Local step 30000, global step 481477: loss 1.4451
[2019-04-07 12:50:20,503] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 30000, global step 481477: learning rate 0.0000
[2019-04-07 12:50:21,764] A3C_AGENT_WORKER-Thread-16 INFO:Local step 30500, global step 481664: loss 4.0632
[2019-04-07 12:50:21,765] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 30500, global step 481664: learning rate 0.0000
[2019-04-07 12:50:22,378] A3C_AGENT_WORKER-Thread-6 INFO:Local step 30500, global step 481760: loss 4.6205
[2019-04-07 12:50:22,378] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 30500, global step 481760: learning rate 0.0000
[2019-04-07 12:50:24,653] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:50:24,653] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:50:24,656] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run12
[2019-04-07 12:50:27,049] A3C_AGENT_WORKER-Thread-3 INFO:Local step 30000, global step 482433: loss 1.1946
[2019-04-07 12:50:27,050] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 30000, global step 482433: learning rate 0.0000
[2019-04-07 12:50:27,865] A3C_AGENT_WORKER-Thread-20 INFO:Local step 30000, global step 482561: loss 1.1881
[2019-04-07 12:50:27,870] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 30000, global step 482561: learning rate 0.0000
[2019-04-07 12:50:29,213] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.9751767e-27 2.2753963e-24 2.0225841e-23 1.4440197e-13 1.0097795e-21
 1.0000000e+00 2.9028139e-13 3.4882781e-16], sum to 1.0000
[2019-04-07 12:50:29,215] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3586
[2019-04-07 12:50:29,219] A3C_AGENT_WORKER-Thread-14 INFO:Local step 30000, global step 482768: loss 1.1446
[2019-04-07 12:50:29,220] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 30000, global step 482768: learning rate 0.0000
[2019-04-07 12:50:29,273] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 100.0, 63.5, 0.0, 24.0, 23.84176120446013, -0.07821420100206315, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2887200.0000, 
sim time next is 2889000.0000, 
raw observation next is [0.5, 96.5, 78.0, 0.0, 24.0, 23.85070817590197, -0.0771453514062142, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4764542936288089, 0.965, 0.26, 0.0, 0.5, 0.4875590146584976, 0.4742848828645953, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33313775], dtype=float32), 0.3443349]. 
=============================================
[2019-04-07 12:50:29,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[105.92928 ]
 [107.421684]
 [108.79243 ]
 [109.81618 ]
 [109.55888 ]], R is [[104.95033264]
 [104.90083313]
 [104.85182953]
 [104.80331421]
 [104.75527954]].
[2019-04-07 12:50:29,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3990712e-21 1.5851860e-21 2.5381801e-20 4.0196085e-10 2.3798324e-17
 1.0000000e+00 1.4882888e-11 7.0730531e-13], sum to 1.0000
[2019-04-07 12:50:29,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5877
[2019-04-07 12:50:29,615] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 39.0, 0.0, 0.0, 24.0, 23.40491794296771, -0.05079570233120771, 0.0, 1.0, 40934.735372176096], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4062600.0000, 
sim time next is 4064400.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 24.0, 23.45866545871316, -0.06366876593845036, 0.0, 1.0, 26241.041579453886], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.5, 0.45488878822609663, 0.47877707802051656, 0.0, 1.0, 0.12495734085454231], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.70977247], dtype=float32), -2.102373]. 
=============================================
[2019-04-07 12:50:29,881] A3C_AGENT_WORKER-Thread-18 INFO:Local step 30000, global step 482869: loss 1.4752
[2019-04-07 12:50:29,881] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 30000, global step 482869: learning rate 0.0000
[2019-04-07 12:50:30,990] A3C_AGENT_WORKER-Thread-4 INFO:Local step 30000, global step 483052: loss 1.3422
[2019-04-07 12:50:30,990] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 30000, global step 483052: learning rate 0.0000
[2019-04-07 12:50:31,641] A3C_AGENT_WORKER-Thread-12 INFO:Local step 31000, global step 483140: loss 14.2643
[2019-04-07 12:50:31,641] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 31000, global step 483140: learning rate 0.0000
[2019-04-07 12:50:36,616] A3C_AGENT_WORKER-Thread-2 INFO:Local step 30000, global step 484035: loss 1.3609
[2019-04-07 12:50:36,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 30000, global step 484035: learning rate 0.0000
[2019-04-07 12:50:38,702] A3C_AGENT_WORKER-Thread-13 INFO:Local step 30000, global step 484416: loss 1.1608
[2019-04-07 12:50:38,746] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 30000, global step 484416: learning rate 0.0000
[2019-04-07 12:50:39,484] A3C_AGENT_WORKER-Thread-17 INFO:Local step 30000, global step 484575: loss 1.3389
[2019-04-07 12:50:39,484] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 30000, global step 484575: learning rate 0.0000
[2019-04-07 12:50:39,755] A3C_AGENT_WORKER-Thread-11 INFO:Local step 30000, global step 484644: loss 1.3141
[2019-04-07 12:50:39,766] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 30000, global step 484644: learning rate 0.0000
[2019-04-07 12:50:40,139] A3C_AGENT_WORKER-Thread-5 INFO:Local step 30000, global step 484726: loss 1.5417
[2019-04-07 12:50:40,139] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 30000, global step 484726: learning rate 0.0000
[2019-04-07 12:50:41,702] A3C_AGENT_WORKER-Thread-10 INFO:Local step 30000, global step 484987: loss 1.2829
[2019-04-07 12:50:41,704] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 30000, global step 484987: learning rate 0.0000
[2019-04-07 12:50:42,397] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.3045597e-27 2.0794115e-25 4.9004759e-23 7.9558913e-13 4.0730649e-23
 1.0000000e+00 1.6744940e-14 3.6718404e-16], sum to 1.0000
[2019-04-07 12:50:42,397] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1060
[2019-04-07 12:50:42,460] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 24.0, 23.45516311299975, 0.04225273833632007, 0.0, 1.0, 55854.12784932493], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3277800.0000, 
sim time next is 3279600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 24.0, 23.45712120215219, 0.05274751552492746, 0.0, 1.0, 58359.405284084176], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.92, 0.0, 0.0, 0.5, 0.45476010017934926, 0.5175825051749758, 0.0, 1.0, 0.27790192992421037], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30142894], dtype=float32), 1.7978134]. 
=============================================
[2019-04-07 12:50:43,793] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:50:43,793] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:50:43,797] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run12
[2019-04-07 12:51:01,362] A3C_AGENT_WORKER-Thread-19 INFO:Local step 30500, global step 488703: loss 5.4853
[2019-04-07 12:51:01,363] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 30500, global step 488703: learning rate 0.0000
[2019-04-07 12:51:01,852] A3C_AGENT_WORKER-Thread-16 INFO:Local step 31000, global step 488808: loss 14.9067
[2019-04-07 12:51:01,853] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 31000, global step 488809: learning rate 0.0000
[2019-04-07 12:51:02,541] A3C_AGENT_WORKER-Thread-6 INFO:Local step 31000, global step 488962: loss 14.8303
[2019-04-07 12:51:02,542] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 31000, global step 488962: learning rate 0.0000
[2019-04-07 12:51:08,013] A3C_AGENT_WORKER-Thread-3 INFO:Local step 30500, global step 490003: loss 5.4843
[2019-04-07 12:51:08,013] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 30500, global step 490003: learning rate 0.0000
[2019-04-07 12:51:08,537] A3C_AGENT_WORKER-Thread-20 INFO:Local step 30500, global step 490120: loss 5.8382
[2019-04-07 12:51:08,539] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 30500, global step 490120: learning rate 0.0000
[2019-04-07 12:51:09,416] A3C_AGENT_WORKER-Thread-14 INFO:Local step 30500, global step 490309: loss 6.4041
[2019-04-07 12:51:09,417] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 30500, global step 490309: learning rate 0.0000
[2019-04-07 12:51:09,815] A3C_AGENT_WORKER-Thread-18 INFO:Local step 30500, global step 490395: loss 6.0886
[2019-04-07 12:51:09,817] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 30500, global step 490395: learning rate 0.0000
[2019-04-07 12:51:10,613] A3C_AGENT_WORKER-Thread-4 INFO:Local step 30500, global step 490573: loss 5.6305
[2019-04-07 12:51:10,614] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 30500, global step 490573: learning rate 0.0000
[2019-04-07 12:51:13,766] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:51:13,766] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:51:13,780] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run12
[2019-04-07 12:51:14,527] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:51:14,527] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:51:14,531] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run12
[2019-04-07 12:51:15,595] A3C_AGENT_WORKER-Thread-2 INFO:Local step 30500, global step 491424: loss 5.4963
[2019-04-07 12:51:15,596] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 30500, global step 491424: learning rate 0.0000
[2019-04-07 12:51:18,726] A3C_AGENT_WORKER-Thread-13 INFO:Local step 30500, global step 491951: loss 6.6397
[2019-04-07 12:51:18,727] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 30500, global step 491951: learning rate 0.0000
[2019-04-07 12:51:18,916] A3C_AGENT_WORKER-Thread-17 INFO:Local step 30500, global step 491979: loss 5.8971
[2019-04-07 12:51:18,917] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 30500, global step 491979: learning rate 0.0000
[2019-04-07 12:51:20,120] A3C_AGENT_WORKER-Thread-5 INFO:Local step 30500, global step 492149: loss 6.4412
[2019-04-07 12:51:20,120] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 30500, global step 492149: learning rate 0.0000
[2019-04-07 12:51:20,283] A3C_AGENT_WORKER-Thread-11 INFO:Local step 30500, global step 492174: loss 6.2676
[2019-04-07 12:51:20,283] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 30500, global step 492174: learning rate 0.0000
[2019-04-07 12:51:20,751] A3C_AGENT_WORKER-Thread-10 INFO:Local step 30500, global step 492252: loss 5.9751
[2019-04-07 12:51:20,751] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 30500, global step 492252: learning rate 0.0000
[2019-04-07 12:51:22,450] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7484928e-22 5.9169124e-19 2.3226914e-17 4.8764034e-11 6.7276770e-17
 1.0000000e+00 5.4312395e-11 7.5974600e-13], sum to 1.0000
[2019-04-07 12:51:22,450] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5569
[2019-04-07 12:51:22,483] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 24.0, 23.5778336567529, -0.08827611977900733, 0.0, 1.0, 25216.4743134172], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4248000.0000, 
sim time next is 4249800.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 24.0, 23.49794575428403, -0.1070506034110023, 0.0, 1.0, 40645.06951046922], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.5, 0.45816214619033574, 0.4643164655296659, 0.0, 1.0, 0.19354795004985342], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.47490022], dtype=float32), -0.30498803]. 
=============================================
[2019-04-07 12:51:37,614] A3C_AGENT_WORKER-Thread-19 INFO:Local step 31000, global step 495871: loss 15.1887
[2019-04-07 12:51:37,623] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 31000, global step 495871: learning rate 0.0000
[2019-04-07 12:51:43,637] A3C_AGENT_WORKER-Thread-3 INFO:Local step 31000, global step 497252: loss 14.9065
[2019-04-07 12:51:43,638] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 31000, global step 497252: learning rate 0.0000
[2019-04-07 12:51:44,321] A3C_AGENT_WORKER-Thread-20 INFO:Local step 31000, global step 497418: loss 15.0829
[2019-04-07 12:51:44,328] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 31000, global step 497419: learning rate 0.0000
[2019-04-07 12:51:44,663] A3C_AGENT_WORKER-Thread-14 INFO:Local step 31000, global step 497502: loss 15.4634
[2019-04-07 12:51:44,666] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 31000, global step 497502: learning rate 0.0000
[2019-04-07 12:51:44,700] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1955249e-22 2.2937448e-21 2.8343788e-18 4.6705889e-10 7.2215407e-18
 1.0000000e+00 3.5234513e-12 1.9371351e-13], sum to 1.0000
[2019-04-07 12:51:44,702] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7239
[2019-04-07 12:51:44,740] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-14.5, 69.0, 0.0, 0.0, 24.0, 22.03617747865524, -0.4144261935818901, 0.0, 1.0, 48602.49438641304], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 349200.0000, 
sim time next is 351000.0000, 
raw observation next is [-14.75, 69.0, 0.0, 0.0, 24.0, 22.09648445217096, -0.4216346286668751, 0.0, 1.0, 48741.789753793855], 
processed observation next is [1.0, 0.043478260869565216, 0.05401662049861495, 0.69, 0.0, 0.0, 0.5, 0.3413737043475799, 0.35945512377770833, 0.0, 1.0, 0.2321037607323517], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2885737], dtype=float32), 1.0505584]. 
=============================================
[2019-04-07 12:51:44,755] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.81082 ]
 [92.71026 ]
 [93.86675 ]
 [95.56925 ]
 [96.594505]], R is [[89.88339233]
 [89.98455811]
 [90.0847168 ]
 [90.18386841]
 [90.2820282 ]].
[2019-04-07 12:51:45,609] A3C_AGENT_WORKER-Thread-18 INFO:Local step 31000, global step 497716: loss 15.0956
[2019-04-07 12:51:45,610] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 31000, global step 497716: learning rate 0.0000
[2019-04-07 12:51:46,554] A3C_AGENT_WORKER-Thread-4 INFO:Local step 31000, global step 497878: loss 15.2068
[2019-04-07 12:51:46,555] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 31000, global step 497878: learning rate 0.0000
[2019-04-07 12:51:48,123] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:51:48,124] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:51:48,128] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run12
[2019-04-07 12:51:51,318] A3C_AGENT_WORKER-Thread-2 INFO:Local step 31000, global step 498928: loss 15.2920
[2019-04-07 12:51:51,321] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 31000, global step 498928: learning rate 0.0000
[2019-04-07 12:51:53,614] A3C_AGENT_WORKER-Thread-13 INFO:Local step 31000, global step 499421: loss 15.2240
[2019-04-07 12:51:53,616] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 31000, global step 499421: learning rate 0.0000
[2019-04-07 12:51:53,732] A3C_AGENT_WORKER-Thread-10 INFO:Local step 31000, global step 499460: loss 15.2682
[2019-04-07 12:51:53,733] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 31000, global step 499460: learning rate 0.0000
[2019-04-07 12:51:53,805] A3C_AGENT_WORKER-Thread-17 INFO:Local step 31000, global step 499481: loss 15.2076
[2019-04-07 12:51:53,808] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 31000, global step 499481: learning rate 0.0000
[2019-04-07 12:51:54,265] A3C_AGENT_WORKER-Thread-5 INFO:Local step 31000, global step 499609: loss 15.4482
[2019-04-07 12:51:54,267] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 31000, global step 499609: learning rate 0.0000
[2019-04-07 12:51:54,287] A3C_AGENT_WORKER-Thread-11 INFO:Local step 31000, global step 499611: loss 15.3557
[2019-04-07 12:51:54,287] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 31000, global step 499612: learning rate 0.0000
[2019-04-07 12:51:54,406] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:51:54,406] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:51:54,410] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run12
[2019-04-07 12:51:54,819] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:51:54,819] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:51:54,822] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run12
[2019-04-07 12:51:54,945] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:51:54,946] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:51:54,963] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run12
[2019-04-07 12:51:55,785] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:51:55,785] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:51:55,804] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run12
[2019-04-07 12:51:56,856] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-07 12:51:56,893] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:51:56,893] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:51:56,895] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run26
[2019-04-07 12:51:57,001] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:51:57,001] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:51:57,003] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run26
[2019-04-07 12:51:57,078] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:51:57,079] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:51:57,083] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run12
[2019-04-07 12:51:57,113] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 12:51:57,118] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:51:57,138] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run26
[2019-04-07 12:52:29,266] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.10980004], dtype=float32), 0.12665126]
[2019-04-07 12:52:29,266] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-15.6, 90.0, 32.0, 607.5, 24.0, 24.06733486484347, -0.1051515512457869, 1.0, 1.0, 0.0]
[2019-04-07 12:52:29,266] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 12:52:29,267] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [1.4496407e-21 9.2313340e-21 1.6584265e-18 8.7551126e-11 9.3328630e-18
 1.0000000e+00 2.4631094e-11 1.0851975e-12], sampled 0.7617643503207928
[2019-04-07 12:54:19,135] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.3337 70927233.8163 166.2180
[2019-04-07 12:54:35,494] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 12:54:41,210] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 12:54:42,232] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 500000, evaluation results [500000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.333747436773, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 12:54:48,477] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:54:48,477] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:54:48,495] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run12
[2019-04-07 12:54:51,325] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:54:51,325] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:54:51,328] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run12
[2019-04-07 12:54:51,505] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:54:51,505] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:54:51,508] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run12
[2019-04-07 12:54:51,601] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:54:51,601] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:54:51,605] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run12
[2019-04-07 12:54:51,753] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:54:51,753] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:54:51,757] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run12
[2019-04-07 12:54:52,113] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 12:54:52,113] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:54:52,118] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run12
[2019-04-07 12:55:08,357] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.7769090e-29 2.8938417e-28 3.3518404e-25 1.3291228e-15 1.6468405e-24
 1.0000000e+00 1.4376117e-16 3.6192841e-18], sum to 1.0000
[2019-04-07 12:55:08,357] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3510
[2019-04-07 12:55:08,434] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 93.0, 72.0, 0.0, 24.0, 24.14526967815019, 0.0075928328877026, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 918000.0000, 
sim time next is 919800.0000, 
raw observation next is [4.4, 93.0, 54.0, 0.0, 24.0, 23.57273010550238, -0.05952974109167149, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5844875346260389, 0.93, 0.18, 0.0, 0.5, 0.4643941754585317, 0.4801567529694428, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6836384], dtype=float32), 0.48077282]. 
=============================================
[2019-04-07 12:55:44,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.4577021e-25 1.0501998e-23 2.4705357e-21 2.6174516e-13 9.7816053e-20
 1.0000000e+00 1.1818866e-12 3.8182883e-14], sum to 1.0000
[2019-04-07 12:55:44,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0022
[2019-04-07 12:55:44,946] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 53.0, 0.0, 0.0, 24.0, 23.24883185762052, -0.1076820458773353, 1.0, 1.0, 44905.6831833461], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 759600.0000, 
sim time next is 761400.0000, 
raw observation next is [-4.45, 55.5, 0.0, 0.0, 24.0, 23.21886493125773, -0.1510907761139799, 1.0, 1.0, 12563.05142077202], 
processed observation next is [1.0, 0.8260869565217391, 0.3393351800554017, 0.555, 0.0, 0.0, 0.5, 0.4349054109381442, 0.4496364079620067, 1.0, 1.0, 0.05982405438462867], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1967027], dtype=float32), -1.359573]. 
=============================================
[2019-04-07 12:55:50,641] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0512775e-24 4.2934047e-24 5.2803818e-21 9.4493240e-11 6.3281122e-21
 1.0000000e+00 1.0820414e-12 1.3612235e-15], sum to 1.0000
[2019-04-07 12:55:50,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6554
[2019-04-07 12:55:50,749] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.05, 69.0, 0.0, 0.0, 24.0, 23.31154677002874, -0.07712209370434932, 0.0, 1.0, 91951.49188698914], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2673000.0000, 
sim time next is 2674800.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 24.0, 23.23217265541042, -0.08284903771760431, 0.0, 1.0, 56327.354524622926], 
processed observation next is [1.0, 1.0, 0.32409972299168976, 0.69, 0.0, 0.0, 0.5, 0.4360143879508683, 0.4723836540941319, 0.0, 1.0, 0.26822549773629967], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2548747], dtype=float32), -0.8637526]. 
=============================================
[2019-04-07 12:55:57,398] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7869965e-23 4.3121021e-23 8.5111883e-20 1.2106928e-11 7.8650491e-20
 1.0000000e+00 1.9337214e-12 8.3215287e-13], sum to 1.0000
[2019-04-07 12:55:57,398] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3717
[2019-04-07 12:55:57,562] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 54.5, 0.0, 0.0, 24.0, 23.04343304249935, -0.1558095048682821, 1.0, 1.0, 68025.3973584783], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 757800.0000, 
sim time next is 759600.0000, 
raw observation next is [-3.9, 53.0, 0.0, 0.0, 24.0, 23.24883185762052, -0.1076820458773353, 1.0, 1.0, 44905.6831833461], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.53, 0.0, 0.0, 0.5, 0.43740265480171, 0.4641059847075549, 1.0, 1.0, 0.21383658658736238], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01500321], dtype=float32), 1.4833635]. 
=============================================
[2019-04-07 12:56:08,858] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.9475606e-29 8.3596299e-28 3.8751743e-24 5.4393176e-14 3.9230697e-23
 1.0000000e+00 1.0944583e-15 2.8719840e-17], sum to 1.0000
[2019-04-07 12:56:08,859] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2006
[2019-04-07 12:56:08,929] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 24.0, 23.69278366552827, 0.03073887205469829, 0.0, 1.0, 27187.767014617613], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 972000.0000, 
sim time next is 973800.0000, 
raw observation next is [9.4, 83.0, 0.0, 0.0, 24.0, 23.73689540996254, 0.004711202484757065, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7229916897506927, 0.83, 0.0, 0.0, 0.5, 0.47807461749687824, 0.5015704008282523, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5891345], dtype=float32), -0.030862857]. 
=============================================
[2019-04-07 12:56:10,234] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0613391e-24 4.3192141e-23 6.4208269e-22 4.4336921e-12 1.5000979e-20
 1.0000000e+00 8.3217195e-13 1.5163123e-14], sum to 1.0000
[2019-04-07 12:56:10,235] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8005
[2019-04-07 12:56:10,279] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 24.0, 22.77733993371731, -0.2554803731515085, 0.0, 1.0, 42947.676969285974], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2178000.0000, 
sim time next is 2179800.0000, 
raw observation next is [-6.2, 77.0, 0.0, 0.0, 24.0, 22.65830140976488, -0.2729375328616368, 0.0, 1.0, 42862.96169247156], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.77, 0.0, 0.0, 0.5, 0.38819178414707345, 0.4090208223794544, 0.0, 1.0, 0.20410934139272172], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14222753], dtype=float32), -0.4532529]. 
=============================================
[2019-04-07 12:56:17,512] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5824314e-27 8.6959644e-26 1.9950371e-22 6.6915630e-13 1.0124453e-20
 1.0000000e+00 3.1727259e-14 3.8063742e-16], sum to 1.0000
[2019-04-07 12:56:17,512] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8102
[2019-04-07 12:56:17,539] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.55, 64.5, 200.0, 88.0, 24.0, 24.95292014975103, 0.2826435163520918, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1593000.0000, 
sim time next is 1594800.0000, 
raw observation next is [9.4, 61.0, 208.0, 168.5, 24.0, 24.99984499040062, 0.316233839204403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7229916897506927, 0.61, 0.6933333333333334, 0.1861878453038674, 0.5, 0.5833204158667185, 0.605411279734801, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19387382], dtype=float32), 0.038714297]. 
=============================================
[2019-04-07 12:56:19,150] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.63868470e-20 6.94027592e-19 1.21172174e-17 3.89567045e-10
 5.14673167e-17 1.00000000e+00 1.24726254e-11 1.63432861e-11], sum to 1.0000
[2019-04-07 12:56:19,150] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8473
[2019-04-07 12:56:19,210] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 24.0, 22.64287186233774, -0.07356598864994043, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1224000.0000, 
sim time next is 1225800.0000, 
raw observation next is [15.25, 94.5, 0.0, 0.0, 24.0, 22.58318927339551, -0.0844884195907624, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8850415512465375, 0.945, 0.0, 0.0, 0.5, 0.38193243944962596, 0.47183719346974584, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21358323], dtype=float32), -0.48081672]. 
=============================================
[2019-04-07 12:57:04,811] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-07 12:57:04,817] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:57:04,821] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:57:04,823] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run27
[2019-04-07 12:57:04,949] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 12:57:04,950] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:57:04,952] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run27
[2019-04-07 12:57:05,010] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 12:57:05,011] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 12:57:05,030] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run27
[2019-04-07 12:59:21,116] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11018809], dtype=float32), 0.127596]
[2019-04-07 12:59:21,116] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [7.840265818500001, 94.82198167499999, 17.4065193, 428.6047738, 24.0, 23.38892376128112, -0.09068139891205407, 1.0, 1.0, 0.0]
[2019-04-07 12:59:21,116] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 12:59:21,117] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [1.7722814e-26 2.6504016e-25 1.0988721e-22 3.5924155e-13 1.3402077e-21
 1.0000000e+00 7.3042423e-14 1.7870093e-15], sampled 0.8031858468851015
[2019-04-07 12:59:29,590] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 12:59:46,512] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 12:59:50,216] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 12:59:51,239] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 520000, evaluation results [520000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 12:59:53,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4039631e-27 2.9383399e-26 2.2366699e-24 7.2279530e-13 1.8062065e-22
 1.0000000e+00 2.4678491e-15 3.2733092e-17], sum to 1.0000
[2019-04-07 12:59:53,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5713
[2019-04-07 12:59:53,799] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.2, 84.0, 0.0, 0.0, 24.0, 23.37636473675245, -0.08897601739380527, 0.0, 1.0, 43305.36613176023], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2068200.0000, 
sim time next is 2070000.0000, 
raw observation next is [-4.5, 86.0, 0.0, 0.0, 24.0, 23.23206646878747, -0.1061129369464013, 0.0, 1.0, 51352.27556075033], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.86, 0.0, 0.0, 0.5, 0.4360055390656224, 0.46462902101786624, 0.0, 1.0, 0.24453464552738252], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23178127], dtype=float32), -0.658221]. 
=============================================
[2019-04-07 12:59:53,805] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[103.69622 ]
 [103.09561 ]
 [101.85506 ]
 [101.684074]
 [100.82487 ]], R is [[103.67337799]
 [103.63664246]
 [103.51928711]
 [103.48409271]
 [103.44924927]].
[2019-04-07 12:59:55,904] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.1033226e-24 1.7472718e-23 1.8236273e-20 9.3121587e-12 2.1663876e-20
 1.0000000e+00 7.6982006e-12 5.9177848e-14], sum to 1.0000
[2019-04-07 12:59:55,904] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3473
[2019-04-07 12:59:55,989] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.2, 91.0, 0.0, 0.0, 24.0, 22.33517293846827, -0.3524032570152227, 0.0, 1.0, 44039.018434824226], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2269800.0000, 
sim time next is 2271600.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 24.0, 22.22971589038708, -0.3861411782653451, 0.0, 1.0, 43938.21351197089], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.5, 0.35247632419892333, 0.37128627391155167, 0.0, 1.0, 0.20922958815224235], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.199412], dtype=float32), 0.77505827]. 
=============================================
[2019-04-07 12:59:56,543] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2454827e-25 6.8564161e-25 7.0226377e-22 2.1092153e-13 4.3813971e-22
 1.0000000e+00 1.6455490e-14 4.6184154e-16], sum to 1.0000
[2019-04-07 12:59:56,543] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2989
[2019-04-07 12:59:56,631] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 24.0, 22.89016210246292, -0.2006092287741056, 0.0, 1.0, 45279.007900975856], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2251800.0000, 
sim time next is 2253600.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 24.0, 22.84885637252431, -0.2213984326095102, 0.0, 1.0, 45122.77069256839], 
processed observation next is [1.0, 0.08695652173913043, 0.26038781163434904, 0.82, 0.0, 0.0, 0.5, 0.4040713643770258, 0.4262005224634966, 0.0, 1.0, 0.21487033663127805], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08737899], dtype=float32), 0.6529145]. 
=============================================
[2019-04-07 13:00:34,621] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:00:34,621] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:00:34,624] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run13
[2019-04-07 13:00:44,279] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5323545e-22 4.0684284e-20 7.2536187e-19 5.6590902e-11 2.3726700e-18
 1.0000000e+00 1.3068108e-11 1.0610769e-11], sum to 1.0000
[2019-04-07 13:00:44,279] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5074
[2019-04-07 13:00:44,370] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 70.5, 0.0, 0.0, 24.0, 22.53334364148149, -0.3201391073639568, 0.0, 1.0, 41526.80287240399], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3051000.0000, 
sim time next is 3052800.0000, 
raw observation next is [-6.0, 64.0, 42.0, 214.5, 24.0, 22.46807074778421, -0.2952928717585175, 0.0, 1.0, 41560.103032759536], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.14, 0.23701657458563535, 0.5, 0.37233922898201754, 0.4015690427471608, 0.0, 1.0, 0.19790525253695018], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8924972], dtype=float32), -1.8561591]. 
=============================================
[2019-04-07 13:00:51,069] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:00:51,069] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:00:51,074] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run13
[2019-04-07 13:00:56,242] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.9181606e-28 4.1154758e-27 8.3712263e-25 5.6405035e-14 4.2859657e-21
 1.0000000e+00 2.1001738e-14 1.9757918e-15], sum to 1.0000
[2019-04-07 13:00:56,243] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0138
[2019-04-07 13:00:56,322] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 67.0, 0.0, 0.0, 24.0, 23.84071493456909, 0.08996695849996621, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4417200.0000, 
sim time next is 4419000.0000, 
raw observation next is [4.75, 67.0, 0.0, 0.0, 24.0, 23.55780803721343, 0.1136343260329576, 0.0, 1.0, 124963.09081465188], 
processed observation next is [1.0, 0.13043478260869565, 0.5941828254847646, 0.67, 0.0, 0.0, 0.5, 0.46315066976778585, 0.5378781086776525, 0.0, 1.0, 0.595062337212628], 
reward next is 0.6907, 
noisyNet noise sample is [array([-0.6730511], dtype=float32), 0.4231833]. 
=============================================
[2019-04-07 13:00:56,326] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[106.53992 ]
 [107.08768 ]
 [107.000755]
 [107.17402 ]
 [107.113594]], R is [[106.97834015]
 [106.90855408]
 [106.83946991]
 [106.77108002]
 [106.70336914]].
[2019-04-07 13:00:59,247] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.29936039e-21 1.04562035e-20 9.63893219e-18 4.47417381e-11
 1.56892133e-18 1.00000000e+00 9.50510243e-12 9.67380473e-13], sum to 1.0000
[2019-04-07 13:00:59,248] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2249
[2019-04-07 13:00:59,358] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 31.0, 286.5, 24.0, 23.29919853190098, -0.06186572792643539, 0.0, 1.0, 18690.549946119885], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2998800.0000, 
sim time next is 3000600.0000, 
raw observation next is [-1.5, 57.5, 6.0, 99.0, 24.0, 23.19285900844684, -0.1017611103965916, 0.0, 1.0, 27522.97634314832], 
processed observation next is [0.0, 0.7391304347826086, 0.4210526315789474, 0.575, 0.02, 0.10939226519337017, 0.5, 0.4327382507039032, 0.46607962986780277, 0.0, 1.0, 0.13106179211023009], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4549171], dtype=float32), 1.2469077]. 
=============================================
[2019-04-07 13:01:09,161] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.8776128e-26 3.2510952e-25 7.0110436e-23 1.6173404e-12 5.8876810e-21
 1.0000000e+00 4.4790848e-14 4.3458906e-15], sum to 1.0000
[2019-04-07 13:01:09,161] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9992
[2019-04-07 13:01:09,208] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 100.0, 106.0, 790.5, 24.0, 24.85297921560971, 0.3415637316636987, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3247200.0000, 
sim time next is 3249000.0000, 
raw observation next is [-3.0, 85.5, 101.0, 769.0, 24.0, 25.05460079570672, 0.3771302604564126, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.855, 0.33666666666666667, 0.8497237569060774, 0.5, 0.5878833996422266, 0.6257100868188042, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43073332], dtype=float32), -1.7658086]. 
=============================================
[2019-04-07 13:01:09,223] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[106.694   ]
 [105.885254]
 [105.72598 ]
 [105.45944 ]
 [106.007065]], R is [[106.991745  ]
 [106.92182922]
 [106.85261536]
 [106.78408813]
 [106.71624756]].
[2019-04-07 13:01:20,168] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.8886706e-25 2.1079971e-24 1.0188297e-21 6.9127782e-13 2.8848691e-21
 1.0000000e+00 9.1482838e-14 3.9505680e-15], sum to 1.0000
[2019-04-07 13:01:20,188] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7638
[2019-04-07 13:01:20,246] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 56.0, 97.0, 533.0, 24.0, 23.68869793682354, 0.04722925609381442, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5041800.0000, 
sim time next is 5043600.0000, 
raw observation next is [1.0, 47.0, 104.5, 615.5, 24.0, 24.45510059295397, 0.1331378834420426, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4903047091412743, 0.47, 0.34833333333333333, 0.6801104972375691, 0.5, 0.5379250494128307, 0.5443792944806809, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8607882], dtype=float32), -0.69011223]. 
=============================================
[2019-04-07 13:01:22,916] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:01:22,916] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:01:22,920] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run13
[2019-04-07 13:01:22,955] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9312367e-22 1.6748850e-20 1.8457028e-18 7.8793838e-10 1.2865982e-17
 1.0000000e+00 9.6483481e-11 8.4202789e-13], sum to 1.0000
[2019-04-07 13:01:22,957] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6186
[2019-04-07 13:01:23,166] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 70.0, 45.5, 273.0, 24.0, 22.9866805940757, -0.1067377949563188, 0.0, 1.0, 42619.90208597912], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3571200.0000, 
sim time next is 3573000.0000, 
raw observation next is [-6.5, 70.0, 88.0, 425.0, 24.0, 23.28516169331595, -0.01135301936246198, 0.0, 1.0, 29903.4668461913], 
processed observation next is [0.0, 0.34782608695652173, 0.28254847645429365, 0.7, 0.29333333333333333, 0.4696132596685083, 0.5, 0.44043014110966244, 0.4962156602125127, 0.0, 1.0, 0.1423974611723395], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2652688], dtype=float32), -0.51652056]. 
=============================================
[2019-04-07 13:01:23,172] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[88.57727 ]
 [88.24301 ]
 [88.66406 ]
 [88.830124]
 [89.275406]], R is [[89.30876923]
 [89.41567993]
 [89.52152252]
 [89.62630463]
 [89.7300415 ]].
[2019-04-07 13:01:24,420] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.5403269e-22 2.7961536e-21 6.6293550e-20 1.8469076e-10 1.1463602e-17
 1.0000000e+00 5.3954705e-12 8.5726408e-13], sum to 1.0000
[2019-04-07 13:01:24,420] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5810
[2019-04-07 13:01:24,643] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.1, 55.5, 58.0, 764.0, 24.0, 23.67129928572353, -0.1283401515963921, 1.0, 1.0, 66008.96782495447], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 387000.0000, 
sim time next is 388800.0000, 
raw observation next is [-12.8, 51.0, 58.0, 834.5, 24.0, 24.20523908511404, 0.001453147175822409, 1.0, 1.0, 66409.44910805566], 
processed observation next is [1.0, 0.5217391304347826, 0.1080332409972299, 0.51, 0.19333333333333333, 0.9220994475138121, 0.5, 0.5171032570928368, 0.5004843823919408, 1.0, 1.0, 0.3162354719431222], 
reward next is 0.9695, 
noisyNet noise sample is [array([0.38462037], dtype=float32), -0.093005784]. 
=============================================
[2019-04-07 13:01:27,289] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:01:27,290] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:01:27,294] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run13
[2019-04-07 13:01:34,820] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3082255e-25 9.6764077e-24 8.0968364e-21 8.4266457e-12 6.7548228e-20
 1.0000000e+00 8.9987306e-13 2.3833235e-15], sum to 1.0000
[2019-04-07 13:01:34,820] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0932
[2019-04-07 13:01:34,868] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 74.0, 0.0, 0.0, 24.0, 22.65825511230607, -0.2596295367045433, 0.0, 1.0, 42327.98482409425], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 788400.0000, 
sim time next is 790200.0000, 
raw observation next is [-7.55, 74.5, 0.0, 0.0, 24.0, 22.75123753858189, -0.2651009660355584, 0.0, 1.0, 42138.02144136745], 
processed observation next is [1.0, 0.13043478260869565, 0.25346260387811637, 0.745, 0.0, 0.0, 0.5, 0.3959364615484908, 0.41163301132148056, 0.0, 1.0, 0.20065724495889262], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.930769], dtype=float32), 0.23444535]. 
=============================================
[2019-04-07 13:01:35,826] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0679571e-26 6.4793984e-26 5.0339236e-22 6.1188945e-13 1.4619311e-20
 1.0000000e+00 1.6858127e-14 2.6953369e-16], sum to 1.0000
[2019-04-07 13:01:35,831] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4456
[2019-04-07 13:01:35,882] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.15, 91.0, 0.0, 0.0, 24.0, 23.36218475361684, -0.09668601869525562, 0.0, 1.0, 41986.86118672096], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 91800.0000, 
sim time next is 93600.0000, 
raw observation next is [-1.7, 91.0, 0.0, 0.0, 24.0, 23.28309240609698, -0.1037605079569552, 0.0, 1.0, 42543.395456103404], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.91, 0.0, 0.0, 0.5, 0.44025770050808166, 0.46541316401434824, 0.0, 1.0, 0.20258759741001622], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6387093], dtype=float32), -1.1265105]. 
=============================================
[2019-04-07 13:01:46,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.34370476e-25 1.02786525e-24 2.09946154e-21 3.87441246e-13
 1.07403635e-20 1.00000000e+00 2.69563797e-14 1.32529596e-14], sum to 1.0000
[2019-04-07 13:01:46,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4928
[2019-04-07 13:01:46,441] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 48.0, 122.5, 0.0, 24.0, 24.60754208272948, 0.1430692097325678, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4536000.0000, 
sim time next is 4537800.0000, 
raw observation next is [2.0, 50.0, 127.0, 0.0, 24.0, 24.42179766714763, 0.08156098509939914, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.5, 0.42333333333333334, 0.0, 0.5, 0.5351498055956357, 0.527186995033133, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2837172], dtype=float32), -0.36061415]. 
=============================================
[2019-04-07 13:01:49,144] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-07 13:01:49,148] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:01:49,149] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:01:49,150] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run28
[2019-04-07 13:01:49,249] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:01:49,269] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:01:49,271] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run28
[2019-04-07 13:01:49,263] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 13:01:49,372] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:01:49,374] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run28
[2019-04-07 13:03:28,716] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11060411], dtype=float32), 0.1284778]
[2019-04-07 13:03:28,716] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-4.757927838500001, 74.51282595, 0.0, 0.0, 24.0, 22.71418593262763, -0.2643971035747106, 0.0, 1.0, 44541.21294771577]
[2019-04-07 13:03:28,716] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 13:03:28,717] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [3.4019791e-23 3.8158558e-22 7.2793665e-20 1.3618938e-11 4.9353804e-19
 1.0000000e+00 3.7569592e-12 1.2666105e-13], sampled 0.11053386797161424
[2019-04-07 13:04:11,832] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 13:04:27,161] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 13:04:31,551] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 13:04:32,574] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 540000, evaluation results [540000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 13:04:40,765] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.0852854e-25 1.6039985e-23 1.3959898e-21 1.3556867e-11 9.9120156e-21
 1.0000000e+00 2.5906916e-12 5.6106717e-15], sum to 1.0000
[2019-04-07 13:04:40,765] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0674
[2019-04-07 13:04:40,795] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.1, 73.0, 0.0, 0.0, 24.0, 24.12525201267306, 0.06452017571740315, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4309200.0000, 
sim time next is 4311000.0000, 
raw observation next is [4.949999999999999, 74.5, 0.0, 0.0, 24.0, 24.01876221850415, 0.03273125399449131, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.5997229916897507, 0.745, 0.0, 0.0, 0.5, 0.5015635182086792, 0.5109104179981637, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7454298], dtype=float32), -0.59362966]. 
=============================================
[2019-04-07 13:04:40,808] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[99.52995 ]
 [98.55652 ]
 [97.41553 ]
 [94.730644]
 [92.57476 ]], R is [[99.60064697]
 [99.60464478]
 [99.6085968 ]
 [99.14603424]
 [98.81737518]].
[2019-04-07 13:04:45,442] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.9340189e-28 1.8881182e-25 1.3208441e-22 1.2782623e-13 1.1210121e-21
 1.0000000e+00 8.3227111e-15 2.2753198e-15], sum to 1.0000
[2019-04-07 13:04:45,443] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6444
[2019-04-07 13:04:45,520] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 86.0, 208.0, 88.5, 24.0, 24.64271684968874, 0.2170020961454329, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4442400.0000, 
sim time next is 4444200.0000, 
raw observation next is [1.0, 86.0, 251.0, 146.0, 24.0, 24.7847157550035, 0.2558208927694862, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.8366666666666667, 0.16132596685082873, 0.5, 0.565392979583625, 0.5852736309231621, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.79418457], dtype=float32), -1.5047599]. 
=============================================
[2019-04-07 13:04:47,949] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.3391791e-23 1.4647381e-21 7.3099482e-20 2.9657761e-12 3.8323439e-20
 1.0000000e+00 4.6150497e-13 2.2626809e-13], sum to 1.0000
[2019-04-07 13:04:47,949] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5291
[2019-04-07 13:04:48,057] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4, 52.0, 291.5, 236.0, 24.0, 23.14202978843655, -0.1031981819114652, 0.0, 1.0, 14478.987122096902], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4878000.0000, 
sim time next is 4879800.0000, 
raw observation next is [0.3, 49.5, 283.0, 308.0, 24.0, 23.1325845830872, -0.0854921658422016, 0.0, 1.0, 21563.230657038443], 
processed observation next is [0.0, 0.4782608695652174, 0.47091412742382277, 0.495, 0.9433333333333334, 0.34033149171270716, 0.5, 0.42771538192393344, 0.4715026113859328, 0.0, 1.0, 0.10268205074780211], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9650242], dtype=float32), -0.05789016]. 
=============================================
[2019-04-07 13:04:51,209] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:04:51,209] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:04:51,219] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run13
[2019-04-07 13:04:54,442] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7807449e-27 5.0955113e-24 1.3211868e-22 1.5831024e-12 1.0654251e-22
 1.0000000e+00 1.1065323e-14 7.6364090e-17], sum to 1.0000
[2019-04-07 13:04:54,442] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1949
[2019-04-07 13:04:54,480] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.25, 98.0, 0.0, 0.0, 24.0, 22.95187181047656, 0.0183415538273567, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1272600.0000, 
sim time next is 1274400.0000, 
raw observation next is [8.3, 96.0, 0.0, 0.0, 24.0, 22.87996548154594, 0.01400284700622093, 0.0, 1.0, 44941.23112536912], 
processed observation next is [0.0, 0.782608695652174, 0.6925207756232689, 0.96, 0.0, 0.0, 0.5, 0.4066637901288284, 0.5046676156687403, 0.0, 1.0, 0.21400586250175774], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.77376395], dtype=float32), 0.6580278]. 
=============================================
[2019-04-07 13:04:54,901] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:04:54,901] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:04:54,912] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run13
[2019-04-07 13:04:55,320] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:04:55,320] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:04:55,324] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run13
[2019-04-07 13:04:55,844] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:04:55,845] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:04:55,849] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run13
[2019-04-07 13:04:56,809] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:04:56,809] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:04:56,812] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run13
[2019-04-07 13:04:57,341] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:04:57,341] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:04:57,344] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run13
[2019-04-07 13:05:00,971] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.6599896e-24 2.1095275e-22 8.8533803e-19 7.1465152e-12 2.8591477e-19
 1.0000000e+00 7.3936213e-12 5.3258391e-13], sum to 1.0000
[2019-04-07 13:05:00,972] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1007
[2019-04-07 13:05:01,165] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 80.0, 138.0, 595.0, 24.0, 23.09253185164343, -0.08809444564088664, 0.0, 1.0, 33183.814947998726], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 567000.0000, 
sim time next is 568800.0000, 
raw observation next is [-1.2, 80.0, 132.5, 531.0, 24.0, 23.12349837600605, -0.08580170502769409, 0.0, 1.0, 18708.03034560342], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.8, 0.44166666666666665, 0.5867403314917127, 0.5, 0.4269581980005042, 0.4713994316574353, 0.0, 1.0, 0.08908585878858771], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6827916], dtype=float32), 1.2093084]. 
=============================================
[2019-04-07 13:05:07,265] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:05:07,265] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:05:07,270] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run13
[2019-04-07 13:05:08,061] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:05:08,061] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:05:08,082] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run13
[2019-04-07 13:05:12,797] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:05:12,797] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:05:12,800] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run13
[2019-04-07 13:05:13,977] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:05:13,977] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:05:13,980] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run13
[2019-04-07 13:05:14,393] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:05:14,393] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:05:14,396] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run13
[2019-04-07 13:05:16,401] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:05:16,401] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:05:16,432] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run13
[2019-04-07 13:05:44,138] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9609970e-25 3.2529453e-24 7.9175104e-22 3.7604147e-12 4.5385708e-21
 1.0000000e+00 5.2276866e-15 5.0959985e-16], sum to 1.0000
[2019-04-07 13:05:44,139] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9599
[2019-04-07 13:05:44,172] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.1, 63.0, 161.0, 110.0, 24.0, 23.94316418499431, -0.07496627275703603, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2287800.0000, 
sim time next is 2289600.0000, 
raw observation next is [-3.2, 58.0, 211.5, 92.0, 24.0, 23.81927694275442, -0.0881167471444113, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.37396121883656513, 0.58, 0.705, 0.10165745856353592, 0.5, 0.48493974522953504, 0.4706277509518629, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5666709], dtype=float32), 2.036013]. 
=============================================
[2019-04-07 13:06:12,528] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.4045985e-25 1.3539314e-23 6.3617772e-22 7.3332676e-13 1.8533560e-20
 1.0000000e+00 1.8057634e-13 1.4432446e-15], sum to 1.0000
[2019-04-07 13:06:12,529] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6557
[2019-04-07 13:06:12,620] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 85.0, 0.0, 0.0, 24.0, 23.43189308175281, -0.1005287342408963, 0.0, 1.0, 27024.55374064621], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 592200.0000, 
sim time next is 594000.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 24.0, 23.38182563113594, -0.1067836364804331, 0.0, 1.0, 57863.5967515476], 
processed observation next is [0.0, 0.9130434782608695, 0.38504155124653744, 0.83, 0.0, 0.0, 0.5, 0.44848546926132826, 0.4644054545065223, 0.0, 1.0, 0.27554093691213144], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3122227], dtype=float32), 0.22813703]. 
=============================================
[2019-04-07 13:06:12,624] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[97.9932 ]
 [97.93522]
 [97.5791 ]
 [96.4719 ]
 [96.16078]], R is [[98.084198  ]
 [98.10335541]
 [98.12232208]
 [97.89028931]
 [97.91138458]].
[2019-04-07 13:06:46,133] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-07 13:06:46,133] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:06:46,134] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:06:46,136] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run29
[2019-04-07 13:06:46,188] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:06:46,188] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:06:46,190] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run29
[2019-04-07 13:06:46,299] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 13:06:46,299] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:06:46,338] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run29
[2019-04-07 13:09:13,336] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 13:09:22,328] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11109484], dtype=float32), 0.12945348]
[2019-04-07 13:09:22,329] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [1.39544121, 57.29191606, 116.235670075, 601.06234605, 24.0, 23.39540205006433, -0.02424195426332292, 0.0, 1.0, 6226.803890076847]
[2019-04-07 13:09:22,329] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 13:09:22,330] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [2.7022587e-21 3.5493448e-20 2.3368568e-18 1.4726123e-10 1.5584357e-17
 1.0000000e+00 3.5382017e-11 1.3844668e-12], sampled 0.7487498585971318
[2019-04-07 13:09:29,892] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 13:09:33,995] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 13:09:35,018] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 560000, evaluation results [560000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 13:10:08,901] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.63706403e-23 1.50725482e-22 3.81670596e-19 5.68587885e-11
 2.14558825e-20 1.00000000e+00 2.10074597e-11 1.03846104e-13], sum to 1.0000
[2019-04-07 13:10:08,902] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0660
[2019-04-07 13:10:09,109] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 79.0, 29.0, 0.0, 24.0, 23.14953879948241, -0.1713639935959526, 0.0, 1.0, 38734.28900587846], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1873800.0000, 
sim time next is 1875600.0000, 
raw observation next is [-4.5, 83.0, 15.0, 0.0, 24.0, 23.1202653424056, -0.1846006822009344, 0.0, 1.0, 39909.434026057264], 
processed observation next is [0.0, 0.7391304347826086, 0.3379501385041552, 0.83, 0.05, 0.0, 0.5, 0.4266887785337999, 0.43846643926635515, 0.0, 1.0, 0.19004492393360603], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00879271], dtype=float32), 1.1745527]. 
=============================================
[2019-04-07 13:10:12,196] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4729725e-25 6.4730557e-24 4.0416497e-21 2.8839084e-12 2.0039785e-20
 1.0000000e+00 1.2895638e-13 6.3043023e-15], sum to 1.0000
[2019-04-07 13:10:12,196] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0950
[2019-04-07 13:10:12,279] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 24.0, 22.43404184747966, -0.3832651528763155, 0.0, 1.0, 45946.08331233886], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1920600.0000, 
sim time next is 1922400.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 24.0, 22.26834370621, -0.4191334817690786, 0.0, 1.0, 45729.42892574455], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.82, 0.0, 0.0, 0.5, 0.3556953088508334, 0.36028883941030715, 0.0, 1.0, 0.21775918536068833], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01097051], dtype=float32), -0.6771608]. 
=============================================
[2019-04-07 13:10:22,914] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.0471787e-26 1.8678445e-25 1.8844451e-21 8.8881604e-13 7.4544291e-22
 1.0000000e+00 2.5698712e-14 8.9815012e-16], sum to 1.0000
[2019-04-07 13:10:22,914] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6988
[2019-04-07 13:10:22,978] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 24.0, 23.44792122695434, -0.07165269114308791, 0.0, 1.0, 26763.39826418497], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3380400.0000, 
sim time next is 3382200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 24.0, 23.39082376119416, -0.08319694579483099, 0.0, 1.0, 43844.606346347515], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.5, 0.4492353134328466, 0.47226768473505637, 0.0, 1.0, 0.20878383974451198], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29087186], dtype=float32), -0.742285]. 
=============================================
[2019-04-07 13:10:51,227] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:10:51,227] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:10:51,246] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run14
[2019-04-07 13:11:02,180] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:11:02,180] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:11:02,184] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run14
[2019-04-07 13:11:02,320] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7933503e-26 6.3461335e-25 2.8256379e-23 4.0001318e-12 1.5336854e-22
 1.0000000e+00 9.2638520e-15 6.2027524e-15], sum to 1.0000
[2019-04-07 13:11:02,323] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3721
[2019-04-07 13:11:02,391] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.55, 66.5, 0.0, 0.0, 24.0, 23.98279417242411, 0.1404853977869814, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4415400.0000, 
sim time next is 4417200.0000, 
raw observation next is [5.0, 67.0, 0.0, 0.0, 24.0, 23.84071493456909, 0.08996695849996621, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.6011080332409973, 0.67, 0.0, 0.0, 0.5, 0.4867262445474241, 0.5299889861666555, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7214581], dtype=float32), 1.4082397]. 
=============================================
[2019-04-07 13:11:29,857] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:11:29,857] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:11:29,862] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run14
[2019-04-07 13:11:34,697] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-07 13:11:34,698] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:11:34,698] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:11:34,700] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run30
[2019-04-07 13:11:34,724] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:11:34,725] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:11:34,726] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 13:11:34,726] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:11:34,730] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run30
[2019-04-07 13:11:34,730] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run30
[2019-04-07 13:13:56,888] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 13:14:16,832] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 13:14:18,887] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 13:14:19,909] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 580000, evaluation results [580000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 13:14:21,165] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:14:21,165] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:14:21,169] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run14
[2019-04-07 13:14:33,935] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6354520e-25 1.1594449e-23 1.1722997e-20 2.5909340e-12 2.0289537e-21
 1.0000000e+00 7.9899053e-14 7.3379337e-15], sum to 1.0000
[2019-04-07 13:14:33,935] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2639
[2019-04-07 13:14:34,028] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.8, 76.0, 0.0, 0.0, 24.0, 23.83937982176959, -0.008050274975340435, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4312800.0000, 
sim time next is 4314600.0000, 
raw observation next is [4.6, 75.5, 0.0, 0.0, 24.0, 23.63635769672375, -0.02448230097879215, 0.0, 1.0, 58291.53547054636], 
processed observation next is [0.0, 0.9565217391304348, 0.590027700831025, 0.755, 0.0, 0.0, 0.5, 0.4696964747269791, 0.4918392330070693, 0.0, 1.0, 0.27757874033593505], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29505402], dtype=float32), -1.0081]. 
=============================================
[2019-04-07 13:14:49,513] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.8752487e-22 9.1605043e-22 6.6914978e-21 2.0306366e-11 6.1372376e-19
 1.0000000e+00 1.7904956e-12 4.6235144e-14], sum to 1.0000
[2019-04-07 13:14:49,513] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0530
[2019-04-07 13:14:49,673] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.1, 48.5, 55.0, 887.0, 24.0, 24.68747162938312, 0.1263585400248482, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 394200.0000, 
sim time next is 396000.0000, 
raw observation next is [-10.5, 46.0, 51.5, 859.5, 24.0, 24.91474009412324, 0.1496921529763808, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.17174515235457063, 0.46, 0.17166666666666666, 0.9497237569060774, 0.5, 0.5762283411769366, 0.5498973843254603, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4928263], dtype=float32), -0.989126]. 
=============================================
[2019-04-07 13:14:49,681] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[95.20468 ]
 [95.176926]
 [94.13406 ]
 [94.012474]
 [94.27935 ]], R is [[95.21820831]
 [95.26602936]
 [95.19709778]
 [95.2154541 ]
 [95.23278046]].
[2019-04-07 13:14:53,329] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.8897258e-26 1.6498903e-24 1.2639885e-22 1.4394509e-11 3.1544436e-21
 1.0000000e+00 1.9712091e-13 2.1603040e-14], sum to 1.0000
[2019-04-07 13:14:53,330] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2763
[2019-04-07 13:14:53,470] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8575876e-27 8.6275018e-26 2.7839324e-23 1.0515882e-13 3.9713906e-22
 1.0000000e+00 2.6204660e-15 4.2216217e-16], sum to 1.0000
[2019-04-07 13:14:53,470] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3130
[2019-04-07 13:14:53,481] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 77.0, 0.0, 0.0, 24.0, 23.37892316101439, -0.06132885381333415, 0.0, 1.0, 55932.51957097074], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4604400.0000, 
sim time next is 4606200.0000, 
raw observation next is [-2.5, 74.0, 0.0, 0.0, 24.0, 23.56208028441969, -0.009644199157383904, 1.0, 1.0, 12017.765645259986], 
processed observation next is [1.0, 0.30434782608695654, 0.39335180055401664, 0.74, 0.0, 0.0, 0.5, 0.46350669036830744, 0.4967852669475387, 1.0, 1.0, 0.05722745545361898], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18798114], dtype=float32), -1.5974933]. 
=============================================
[2019-04-07 13:14:53,483] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 100.0, 64.0, 0.0, 24.0, 23.2272142811849, 0.08751351177457116, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1263600.0000, 
sim time next is 1265400.0000, 
raw observation next is [13.8, 100.0, 51.0, 0.0, 24.0, 23.19398003691138, 0.0813935374178306, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.17, 0.0, 0.5, 0.43283166974261505, 0.5271311791392769, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.52595556], dtype=float32), -1.3243202]. 
=============================================
[2019-04-07 13:14:54,371] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.4056955e-25 2.0289362e-23 8.1693062e-22 5.5139969e-13 2.6727971e-19
 1.0000000e+00 3.9128293e-13 1.9646292e-13], sum to 1.0000
[2019-04-07 13:14:54,373] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5407
[2019-04-07 13:14:54,398] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 35.5, 0.0, 0.0, 24.0, 24.98061974868747, 0.2790116232922088, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4127400.0000, 
sim time next is 4129200.0000, 
raw observation next is [3.0, 37.0, 0.0, 0.0, 24.0, 24.53832496178355, 0.2253583133157483, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5457063711911359, 0.37, 0.0, 0.0, 0.5, 0.5448604134819627, 0.5751194377719161, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4375176], dtype=float32), -0.09387092]. 
=============================================
[2019-04-07 13:14:57,109] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.7509073e-20 6.2201314e-19 3.2099838e-18 2.1183313e-09 4.4263464e-17
 1.0000000e+00 4.1438125e-10 2.2805937e-12], sum to 1.0000
[2019-04-07 13:14:57,110] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7011
[2019-04-07 13:14:57,157] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 32.5, 119.0, 822.0, 24.0, 23.26933309097819, -0.03824509026439608, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4188600.0000, 
sim time next is 4190400.0000, 
raw observation next is [1.0, 30.0, 118.5, 834.5, 24.0, 23.20986735313155, -0.03874885138751483, 0.0, 1.0, 6228.244042576333], 
processed observation next is [0.0, 0.5217391304347826, 0.4903047091412743, 0.3, 0.395, 0.9220994475138121, 0.5, 0.4341556127609625, 0.48708371620416174, 0.0, 1.0, 0.029658304964649208], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.61302865], dtype=float32), -1.4943993]. 
=============================================
[2019-04-07 13:15:06,703] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:15:06,703] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:15:06,711] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run14
[2019-04-07 13:15:12,281] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:15:12,281] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:15:12,285] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run14
[2019-04-07 13:15:12,409] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:15:12,409] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:15:12,413] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run14
[2019-04-07 13:15:13,322] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:15:13,323] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:15:13,338] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run14
[2019-04-07 13:15:14,161] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:15:14,161] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:15:14,164] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run14
[2019-04-07 13:15:15,701] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:15:15,701] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:15:15,704] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run14
[2019-04-07 13:15:24,137] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:15:24,137] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:15:24,141] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run14
[2019-04-07 13:15:25,349] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:15:25,349] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:15:25,353] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run14
[2019-04-07 13:15:26,240] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3850065e-24 2.6144445e-23 1.3217224e-20 4.5305999e-12 3.9394819e-20
 1.0000000e+00 4.4835148e-13 6.4844048e-15], sum to 1.0000
[2019-04-07 13:15:26,240] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2392
[2019-04-07 13:15:26,415] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.95, 87.5, 0.0, 0.0, 24.0, 22.73537235287836, -0.2102347551463271, 0.0, 1.0, 28930.299042757204], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 63000.0000, 
sim time next is 64800.0000, 
raw observation next is [4.4, 89.0, 0.0, 0.0, 24.0, 22.73413845092564, -0.2001580577388831, 0.0, 1.0, 44047.32264158205], 
processed observation next is [0.0, 0.782608695652174, 0.5844875346260389, 0.89, 0.0, 0.0, 0.5, 0.3945115375771368, 0.4332806474203723, 0.0, 1.0, 0.209749155436105], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9920729], dtype=float32), -0.028286444]. 
=============================================
[2019-04-07 13:15:32,289] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:15:32,289] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:15:32,292] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run14
[2019-04-07 13:15:35,017] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:15:35,017] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:15:35,022] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run14
[2019-04-07 13:15:35,479] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:15:35,479] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:15:35,482] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run14
[2019-04-07 13:15:35,869] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:15:35,869] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:15:35,873] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run14
[2019-04-07 13:15:51,537] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2227174e-24 2.4844422e-25 1.2191788e-22 7.3929404e-13 3.7404255e-21
 1.0000000e+00 1.1736082e-13 2.9127885e-15], sum to 1.0000
[2019-04-07 13:15:51,538] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5355
[2019-04-07 13:15:51,782] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.75, 65.0, 129.0, 0.0, 24.0, 22.95446375979964, -0.1812351537085542, 1.0, 1.0, 108247.69128613318], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 217800.0000, 
sim time next is 219600.0000, 
raw observation next is [-4.5, 65.0, 139.0, 0.0, 24.0, 23.72032921916288, -0.09886490835403106, 1.0, 1.0, 7425.66659559921], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.65, 0.4633333333333333, 0.0, 0.5, 0.4766941015969068, 0.46704503054865637, 1.0, 1.0, 0.035360317121901], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9883151], dtype=float32), 1.1100149]. 
=============================================
[2019-04-07 13:16:36,689] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-07 13:16:36,704] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:16:36,704] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:16:36,706] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run31
[2019-04-07 13:16:36,837] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:16:36,837] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:16:36,839] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run31
[2019-04-07 13:16:36,953] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 13:16:36,970] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:16:36,972] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run31
[2019-04-07 13:19:02,096] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 13:19:20,832] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 13:19:24,499] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 13:19:25,521] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 600000, evaluation results [600000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 13:19:29,605] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3195670e-31 4.3210492e-28 2.2196110e-24 2.6430545e-15 2.5483703e-25
 1.0000000e+00 1.8404316e-16 4.6818439e-18], sum to 1.0000
[2019-04-07 13:19:29,606] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2775
[2019-04-07 13:19:29,675] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 76.0, 0.0, 0.0, 24.0, 24.03562843607061, 0.2002088173843076, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1027800.0000, 
sim time next is 1029600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 24.0, 24.15487653527964, 0.2136864601211811, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.5, 0.5129063779399701, 0.5712288200403938, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9417225], dtype=float32), 0.33258337]. 
=============================================
[2019-04-07 13:19:33,017] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3570053e-27 1.7982220e-25 7.5320641e-22 5.9309606e-14 3.1818395e-22
 1.0000000e+00 1.2439115e-13 6.8677299e-16], sum to 1.0000
[2019-04-07 13:19:33,018] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9201
[2019-04-07 13:19:33,106] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.15, 81.5, 0.0, 0.0, 24.0, 23.67040553342981, 0.1765598348950739, 0.0, 1.0, 91510.6572493444], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1146600.0000, 
sim time next is 1148400.0000, 
raw observation next is [12.7, 80.0, 0.0, 0.0, 24.0, 23.84535296491872, 0.1881365925273412, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8, 0.0, 0.0, 0.5, 0.4871127470765601, 0.5627121975091137, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.61531764], dtype=float32), -1.2035505]. 
=============================================
[2019-04-07 13:19:35,800] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5681363e-27 2.5385320e-25 5.4998208e-23 1.5494938e-13 1.6797205e-22
 1.0000000e+00 4.1147180e-14 4.2698024e-17], sum to 1.0000
[2019-04-07 13:19:35,800] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9094
[2019-04-07 13:19:35,876] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 24.0, 23.19858989429019, -0.1560094068806831, 0.0, 1.0, 39978.3287552522], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 887400.0000, 
sim time next is 889200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 24.0, 23.36332817017618, -0.1307086725660131, 0.0, 1.0, 39452.62306644444], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.5, 0.4469440141813485, 0.45643044247799563, 0.0, 1.0, 0.1878696336497354], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43002465], dtype=float32), -1.1030186]. 
=============================================
[2019-04-07 13:19:39,512] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0219039e-30 8.1260175e-28 2.1118270e-25 1.0970588e-14 1.0442430e-24
 1.0000000e+00 4.1231830e-16 3.9810628e-17], sum to 1.0000
[2019-04-07 13:19:39,513] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1424
[2019-04-07 13:19:39,551] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.5, 93.0, 78.0, 0.0, 24.0, 24.52530499570075, 0.1633695649723842, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 986400.0000, 
sim time next is 988200.0000, 
raw observation next is [11.05, 89.5, 96.0, 0.0, 24.0, 24.71195535881979, 0.1963364487916653, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7686980609418284, 0.895, 0.32, 0.0, 0.5, 0.5593296132349824, 0.5654454829305551, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.73915225], dtype=float32), 0.49483007]. 
=============================================
[2019-04-07 13:20:13,093] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2871814e-26 1.0331673e-26 8.9956941e-24 1.3034408e-14 4.2590224e-22
 1.0000000e+00 7.8035053e-15 1.5512125e-16], sum to 1.0000
[2019-04-07 13:20:13,093] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0372
[2019-04-07 13:20:13,205] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.35, 84.5, 30.0, 0.0, 24.0, 23.06130366464045, -0.0330621547616796, 1.0, 1.0, 54049.409767982295], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1701000.0000, 
sim time next is 1702800.0000, 
raw observation next is [1.1, 88.0, 15.5, 0.0, 24.0, 23.84589938960486, 0.0575699558386282, 1.0, 1.0, 3113.4019450384226], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.051666666666666666, 0.0, 0.5, 0.48715828246707166, 0.5191899852795427, 1.0, 1.0, 0.014825723547802013], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.64676845], dtype=float32), -0.7666392]. 
=============================================
[2019-04-07 13:20:16,238] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.9238598e-25 3.0263355e-24 7.5374102e-22 1.2016297e-13 7.8420664e-21
 1.0000000e+00 3.6820994e-13 1.4652994e-14], sum to 1.0000
[2019-04-07 13:20:16,238] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7043
[2019-04-07 13:20:16,462] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 83.0, 122.5, 0.0, 24.0, 23.10609610226797, -0.0855699012124904, 0.0, 1.0, 27330.447137605035], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1774800.0000, 
sim time next is 1776600.0000, 
raw observation next is [-2.8, 83.0, 119.0, 0.0, 24.0, 23.07481886682897, -0.09190853370681873, 0.0, 1.0, 42339.62451428621], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.39666666666666667, 0.0, 0.5, 0.4229015722357475, 0.46936382209772703, 0.0, 1.0, 0.2016172595918391], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5822326], dtype=float32), 0.6389574]. 
=============================================
[2019-04-07 13:20:32,279] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9573343e-25 5.7971690e-23 1.5314771e-20 3.2019036e-12 8.5535944e-21
 1.0000000e+00 6.0572923e-13 4.5780915e-15], sum to 1.0000
[2019-04-07 13:20:32,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8390
[2019-04-07 13:20:32,339] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.65, 89.0, 0.0, 0.0, 24.0, 22.71125687555995, -0.2745687224074002, 0.0, 1.0, 44525.61376272383], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2262600.0000, 
sim time next is 2264400.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 24.0, 22.56604543351315, -0.3044592760154601, 0.0, 1.0, 44396.21157849796], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.5, 0.3805037861260958, 0.3985135746615133, 0.0, 1.0, 0.21141053132618076], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09886476], dtype=float32), -1.0286732]. 
=============================================
[2019-04-07 13:20:44,215] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1959706e-24 1.3067007e-23 3.3324936e-21 2.8113688e-13 3.8687973e-22
 1.0000000e+00 2.7473228e-13 2.3004640e-15], sum to 1.0000
[2019-04-07 13:20:44,215] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0341
[2019-04-07 13:20:44,367] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.6, 70.0, 0.0, 0.0, 24.0, 23.26353406613754, -0.0358054058989911, 0.0, 1.0, 93620.49416623893], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2228400.0000, 
sim time next is 2230200.0000, 
raw observation next is [-4.8, 70.5, 0.0, 0.0, 24.0, 23.59163409967162, -0.03919801824988153, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3296398891966759, 0.705, 0.0, 0.0, 0.5, 0.4659695083059683, 0.48693399391670616, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.89613974], dtype=float32), 0.8989121]. 
=============================================
[2019-04-07 13:20:44,917] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.8532734e-25 4.1283421e-25 8.4496823e-22 2.9740691e-12 1.2781578e-20
 1.0000000e+00 3.5168879e-14 1.0059993e-15], sum to 1.0000
[2019-04-07 13:20:44,917] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0169
[2019-04-07 13:20:44,960] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 68.0, 115.0, 822.0, 24.0, 24.93857010007013, 0.2297346724210362, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3756600.0000, 
sim time next is 3758400.0000, 
raw observation next is [-2.0, 65.0, 117.0, 825.5, 24.0, 25.0060093582718, 0.2454806368376509, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.65, 0.39, 0.9121546961325967, 0.5, 0.5838341131893167, 0.5818268789458837, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3236022], dtype=float32), 0.19314465]. 
=============================================
[2019-04-07 13:20:51,646] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2198907e-22 1.2633873e-20 6.8530412e-18 1.8076787e-10 9.2090489e-18
 1.0000000e+00 8.2677309e-11 4.9065987e-13], sum to 1.0000
[2019-04-07 13:20:51,646] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4174
[2019-04-07 13:20:51,757] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 44.0, 16.5, 93.5, 24.0, 23.18478954664035, -0.1402185983305223, 0.0, 1.0, 23240.48265095132], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4903200.0000, 
sim time next is 4905000.0000, 
raw observation next is [1.5, 45.5, 0.0, 0.0, 24.0, 23.05668482269783, -0.1487251107548145, 0.0, 1.0, 59035.419031385136], 
processed observation next is [0.0, 0.782608695652174, 0.5041551246537397, 0.455, 0.0, 0.0, 0.5, 0.42139040189148574, 0.4504249630817285, 0.0, 1.0, 0.2811210430065959], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4388161], dtype=float32), 0.15771036]. 
=============================================
[2019-04-07 13:20:51,774] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[87.483696]
 [87.43994 ]
 [87.529594]
 [87.4831  ]
 [87.372   ]], R is [[88.06674194]
 [88.1860733 ]
 [88.30421448]
 [88.4211731 ]
 [88.53696442]].
[2019-04-07 13:20:57,057] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.6881600e-25 5.8020558e-24 3.0054475e-21 3.6273724e-13 4.5865202e-21
 1.0000000e+00 4.5593408e-14 8.1930227e-15], sum to 1.0000
[2019-04-07 13:20:57,057] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6455
[2019-04-07 13:20:57,140] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.04999999999999999, 45.0, 164.0, 211.0, 24.0, 24.18013384126765, 0.04240738872275782, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2640600.0000, 
sim time next is 2642400.0000, 
raw observation next is [0.5, 43.0, 190.0, 170.5, 24.0, 24.43993362697081, 0.07472290824535839, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.43, 0.6333333333333333, 0.18839779005524862, 0.5, 0.5366611355809008, 0.5249076360817861, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.1193662], dtype=float32), -1.2585821]. 
=============================================
[2019-04-07 13:21:00,177] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:21:00,177] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:21:00,180] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run15
[2019-04-07 13:21:07,173] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.11668673e-21 2.59820429e-19 1.57189266e-18 2.29839845e-11
 1.24975675e-17 1.00000000e+00 8.54627914e-12 1.82573683e-12], sum to 1.0000
[2019-04-07 13:21:07,173] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4972
[2019-04-07 13:21:07,234] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 35.0, 0.0, 0.0, 24.0, 23.45803224223952, -0.1505318223142094, 0.0, 1.0, 23840.211414913505], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2502000.0000, 
sim time next is 2503800.0000, 
raw observation next is [-1.15, 36.5, 0.0, 0.0, 24.0, 23.40914313281164, -0.1620882777330266, 0.0, 1.0, 44580.54545131466], 
processed observation next is [0.0, 1.0, 0.4307479224376732, 0.365, 0.0, 0.0, 0.5, 0.4507619277343033, 0.44597057408899116, 0.0, 1.0, 0.21228831167292697], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8859548], dtype=float32), 1.2060916]. 
=============================================
[2019-04-07 13:21:10,785] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:21:10,785] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:21:10,788] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run15
[2019-04-07 13:21:21,476] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-07 13:21:21,484] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:21:21,484] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:21:21,486] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run32
[2019-04-07 13:21:21,502] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:21:21,503] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 13:21:21,503] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:21:21,504] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:21:21,506] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run32
[2019-04-07 13:21:21,528] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run32
[2019-04-07 13:23:38,416] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11173738], dtype=float32), 0.13151936]
[2019-04-07 13:23:38,416] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [6.6, 84.0, 0.0, 0.0, 24.0, 23.76684647522512, 0.08060782198595602, 0.0, 1.0, 0.0]
[2019-04-07 13:23:38,416] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 13:23:38,417] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [1.7193929e-26 2.9169101e-25 1.0181959e-22 1.5041199e-13 8.7177708e-22
 1.0000000e+00 4.7693982e-14 1.0519683e-15], sampled 0.1360095202480066
[2019-04-07 13:23:44,007] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 13:24:02,981] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 13:24:05,869] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 13:24:06,893] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 620000, evaluation results [620000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 13:24:10,456] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4924394e-27 6.4932793e-26 5.6232017e-24 3.6388735e-14 1.1863932e-22
 1.0000000e+00 3.4886075e-14 9.2255843e-16], sum to 1.0000
[2019-04-07 13:24:10,456] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0355
[2019-04-07 13:24:10,505] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 24.0, 23.41475660335237, -0.1042007194062623, 0.0, 1.0, 45613.02953718263], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2869200.0000, 
sim time next is 2871000.0000, 
raw observation next is [1.0, 96.5, 0.0, 0.0, 24.0, 23.51953402226139, -0.09561055514954314, 0.0, 1.0, 38895.00179503055], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.965, 0.0, 0.0, 0.5, 0.4599611685217824, 0.46812981495015227, 0.0, 1.0, 0.18521429426205022], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3060775], dtype=float32), 0.26731348]. 
=============================================
[2019-04-07 13:24:10,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[109.44697 ]
 [108.644585]
 [107.785286]
 [106.3697  ]
 [105.893135]], R is [[109.62492371]
 [109.52867889]
 [109.43339539]
 [109.20350647]
 [109.11147308]].
[2019-04-07 13:24:26,646] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4404843e-29 5.8052066e-29 9.6809069e-25 1.4223985e-15 1.6762695e-23
 1.0000000e+00 7.7948399e-17 3.9059947e-19], sum to 1.0000
[2019-04-07 13:24:26,676] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0428
[2019-04-07 13:24:26,711] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 100.0, 0.0, 0.0, 24.0, 23.54892627182743, -0.09655859297361842, 0.0, 1.0, 34148.77670086454], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3135600.0000, 
sim time next is 3137400.0000, 
raw observation next is [6.0, 100.0, 1.0, 82.0, 24.0, 23.58079677281312, -0.1020134746109918, 1.0, 1.0, 6245.262562227436], 
processed observation next is [1.0, 0.30434782608695654, 0.6288088642659281, 1.0, 0.0033333333333333335, 0.09060773480662983, 0.5, 0.46506639773442665, 0.46599550846300275, 1.0, 1.0, 0.029739345534416362], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1564442], dtype=float32), -1.8040183]. 
=============================================
[2019-04-07 13:24:28,531] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:24:28,531] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:24:28,543] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run15
[2019-04-07 13:24:31,935] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:24:31,935] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:24:31,939] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run15
[2019-04-07 13:24:42,698] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.1535800e-23 2.1794335e-21 3.0942235e-20 6.6925367e-12 1.2838514e-19
 1.0000000e+00 5.4079961e-13 6.6156573e-14], sum to 1.0000
[2019-04-07 13:24:42,699] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5802
[2019-04-07 13:24:42,741] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 43.5, 117.0, 829.0, 24.0, 23.71533353046011, 0.05643215543522698, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3673800.0000, 
sim time next is 3675600.0000, 
raw observation next is [5.0, 42.0, 115.0, 823.5, 24.0, 23.63000440459617, 0.05187720811838953, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6011080332409973, 0.42, 0.38333333333333336, 0.9099447513812154, 0.5, 0.4691670337163476, 0.5172924027061299, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38970226], dtype=float32), -0.37332717]. 
=============================================
[2019-04-07 13:24:50,854] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4996836e-23 3.3672259e-21 1.2167230e-19 3.3055111e-11 2.2100031e-18
 1.0000000e+00 4.2602957e-12 8.1822710e-13], sum to 1.0000
[2019-04-07 13:24:50,854] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9609
[2019-04-07 13:24:50,952] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 43.5, 117.0, 829.0, 24.0, 23.71533353046011, 0.05643215543522698, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3673800.0000, 
sim time next is 3675600.0000, 
raw observation next is [5.0, 42.0, 115.0, 823.5, 24.0, 23.63000440459617, 0.05187720811838953, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6011080332409973, 0.42, 0.38333333333333336, 0.9099447513812154, 0.5, 0.4691670337163476, 0.5172924027061299, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.96323276], dtype=float32), -0.4840866]. 
=============================================
[2019-04-07 13:24:55,523] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.2819212e-24 1.2586574e-22 8.7966797e-21 3.7582997e-12 1.4889755e-19
 1.0000000e+00 2.6402760e-13 4.0570473e-15], sum to 1.0000
[2019-04-07 13:24:55,523] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2455
[2019-04-07 13:24:55,581] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 24.0, 23.80610023512886, -0.0291602121804648, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4222800.0000, 
sim time next is 4224600.0000, 
raw observation next is [1.0, 45.0, 0.0, 0.0, 24.0, 23.61591179902994, -0.06386490474380559, 0.0, 1.0, 29366.591555302282], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.45, 0.0, 0.0, 0.5, 0.4679926499191618, 0.47871169841873146, 0.0, 1.0, 0.1398409121681061], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41164154], dtype=float32), -1.3892748]. 
=============================================
[2019-04-07 13:24:58,751] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8999086e-28 2.1635731e-26 5.0305234e-23 5.6530078e-14 4.3853432e-22
 1.0000000e+00 4.8466565e-15 1.2702424e-16], sum to 1.0000
[2019-04-07 13:24:58,752] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1821
[2019-04-07 13:24:58,785] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.3, 71.0, 0.0, 0.0, 24.0, 23.65687766120882, -0.04154808213969109, 0.0, 1.0, 15467.44631480718], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4341600.0000, 
sim time next is 4343400.0000, 
raw observation next is [3.1, 73.0, 0.0, 0.0, 24.0, 23.65570740180183, -0.07735776424946854, 0.0, 1.0, 11438.254247948322], 
processed observation next is [1.0, 0.2608695652173913, 0.5484764542936289, 0.73, 0.0, 0.0, 0.5, 0.47130895015015256, 0.4742140785835105, 0.0, 1.0, 0.054467877371182485], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5725816], dtype=float32), 0.8824192]. 
=============================================
[2019-04-07 13:25:11,533] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.5132711e-28 1.2010404e-25 7.3226638e-23 5.4657723e-15 4.0266742e-22
 1.0000000e+00 3.0195687e-15 5.5203731e-17], sum to 1.0000
[2019-04-07 13:25:11,533] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4648
[2019-04-07 13:25:11,600] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 24.0, 23.84013048852363, 0.06989004499406089, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4676400.0000, 
sim time next is 4678200.0000, 
raw observation next is [1.0, 77.0, 0.0, 0.0, 24.0, 23.80039948092097, 0.02510466357868377, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.77, 0.0, 0.0, 0.5, 0.4833666234100808, 0.5083682211928946, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17411876], dtype=float32), -0.030478572]. 
=============================================
[2019-04-07 13:25:13,773] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4465697e-24 2.5924877e-22 1.3756478e-21 2.6526847e-12 6.6876002e-19
 1.0000000e+00 1.6892907e-12 8.9613138e-15], sum to 1.0000
[2019-04-07 13:25:13,773] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0138
[2019-04-07 13:25:13,821] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 30.0, 116.0, 830.0, 24.0, 24.45378068219987, 0.233025848342336, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4109400.0000, 
sim time next is 4111200.0000, 
raw observation next is [3.0, 31.0, 111.0, 812.0, 24.0, 25.33777082025347, 0.3453693409070701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.31, 0.37, 0.8972375690607735, 0.5, 0.6114809016877892, 0.61512311363569, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22095911], dtype=float32), -0.39325136]. 
=============================================
[2019-04-07 13:25:21,963] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:25:21,963] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:25:22,010] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run15
[2019-04-07 13:25:22,220] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.7789441e-23 1.0814649e-21 3.8346982e-20 2.1336884e-12 1.7956304e-18
 1.0000000e+00 6.3112026e-13 9.3944451e-14], sum to 1.0000
[2019-04-07 13:25:22,221] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7523
[2019-04-07 13:25:22,278] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 41.5, 170.0, 787.0, 24.0, 23.20504576820193, -0.007422458410625545, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4797000.0000, 
sim time next is 4798800.0000, 
raw observation next is [2.0, 40.0, 195.0, 677.5, 24.0, 23.33493421604161, 0.009665874084013973, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.4, 0.65, 0.7486187845303868, 0.5, 0.44457785133680083, 0.5032219580280046, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.707088], dtype=float32), 1.1729916]. 
=============================================
[2019-04-07 13:25:25,768] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.95762444e-23 2.54125848e-22 3.89973977e-20 2.90752535e-12
 2.94506077e-19 1.00000000e+00 1.01736775e-13 3.38904828e-14], sum to 1.0000
[2019-04-07 13:25:25,768] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2247
[2019-04-07 13:25:25,925] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4, 52.0, 291.5, 236.0, 24.0, 23.14202978843655, -0.1031981819114652, 0.0, 1.0, 14478.987122096902], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4878000.0000, 
sim time next is 4879800.0000, 
raw observation next is [0.3, 49.5, 283.0, 308.0, 24.0, 23.1325845830872, -0.0854921658422016, 0.0, 1.0, 21563.230657038443], 
processed observation next is [0.0, 0.4782608695652174, 0.47091412742382277, 0.495, 0.9433333333333334, 0.34033149171270716, 0.5, 0.42771538192393344, 0.4715026113859328, 0.0, 1.0, 0.10268205074780211], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8019389], dtype=float32), 2.4721625]. 
=============================================
[2019-04-07 13:25:27,917] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1436600e-26 5.6824777e-25 3.9594383e-23 2.6064616e-13 1.8255037e-23
 1.0000000e+00 2.4060879e-15 6.1519675e-16], sum to 1.0000
[2019-04-07 13:25:27,940] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6122
[2019-04-07 13:25:27,997] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.6, 74.0, 0.0, 0.0, 24.0, 23.49999230475614, -0.03261936198624865, 0.0, 1.0, 30718.854517589007], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4600800.0000, 
sim time next is 4602600.0000, 
raw observation next is [-2.8, 75.5, 0.0, 0.0, 24.0, 23.40702047613824, -0.07491258984532875, 0.0, 1.0, 31290.308292639802], 
processed observation next is [1.0, 0.2608695652173913, 0.38504155124653744, 0.755, 0.0, 0.0, 0.5, 0.4505850396781866, 0.4750291367182238, 0.0, 1.0, 0.14900146806018955], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.86482114], dtype=float32), 0.7364306]. 
=============================================
[2019-04-07 13:25:29,370] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:25:29,370] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:25:29,381] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run15
[2019-04-07 13:25:29,437] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:25:29,437] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:25:29,446] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run15
[2019-04-07 13:25:29,873] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:25:29,873] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:25:29,878] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run15
[2019-04-07 13:25:30,241] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:25:30,241] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:25:30,245] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run15
[2019-04-07 13:25:34,541] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:25:34,541] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:25:34,545] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run15
[2019-04-07 13:25:41,013] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:25:41,013] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:25:41,016] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run15
[2019-04-07 13:25:41,881] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:25:41,881] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:25:41,885] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run15
[2019-04-07 13:25:48,057] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:25:48,057] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:25:48,060] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run15
[2019-04-07 13:25:49,933] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:25:49,933] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:25:49,936] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run15
[2019-04-07 13:25:52,050] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:25:52,051] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:25:52,079] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run15
[2019-04-07 13:25:53,121] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:25:53,121] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:25:53,138] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run15
[2019-04-07 13:26:02,141] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-07 13:26:02,142] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:26:02,142] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:26:02,143] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:26:02,144] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:26:02,157] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 13:26:02,159] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:26:02,159] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run33
[2019-04-07 13:26:02,160] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run33
[2019-04-07 13:26:02,207] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run33
[2019-04-07 13:26:47,812] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11176404], dtype=float32), 0.13209711]
[2019-04-07 13:26:47,813] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-2.3, 76.0, 0.0, 0.0, 24.0, 23.12682805649731, -0.2368736057143567, 0.0, 1.0, 42391.64447275194]
[2019-04-07 13:26:47,813] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 13:26:47,814] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [3.9992073e-24 4.6510895e-23 8.6408967e-21 1.7993042e-12 5.6890639e-20
 1.0000000e+00 5.5909140e-13 1.7586276e-14], sampled 0.8780974298869306
[2019-04-07 13:28:23,954] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 13:28:41,161] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 13:28:45,930] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 13:28:46,953] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 640000, evaluation results [640000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 13:28:56,531] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1761617e-26 2.2657722e-25 3.7759343e-22 1.2699408e-14 6.6882278e-22
 1.0000000e+00 6.5751509e-14 4.8945498e-17], sum to 1.0000
[2019-04-07 13:28:56,531] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7568
[2019-04-07 13:28:56,573] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 24.0, 22.87543435258983, -0.1899784575844363, 0.0, 1.0, 46978.521345671536], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 165600.0000, 
sim time next is 167400.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 24.0, 22.82690701781876, -0.2047717574014153, 0.0, 1.0, 46654.26369172938], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.5, 0.4022422514848965, 0.4317427475328615, 0.0, 1.0, 0.22216316043680656], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9045333], dtype=float32), 0.7694075]. 
=============================================
[2019-04-07 13:29:02,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.10305941e-22 2.71015474e-22 5.48746448e-20 3.23640420e-12
 1.70345000e-19 1.00000000e+00 7.53621211e-12 1.02462174e-13], sum to 1.0000
[2019-04-07 13:29:02,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7810
[2019-04-07 13:29:02,626] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.9, 66.0, 0.0, 0.0, 24.0, 22.30761453791205, -0.3612878928491468, 0.0, 1.0, 48333.95385626658], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 345600.0000, 
sim time next is 347400.0000, 
raw observation next is [-14.2, 67.5, 0.0, 0.0, 24.0, 22.13285170037691, -0.3997304151556322, 0.0, 1.0, 48467.76188503492], 
processed observation next is [1.0, 0.0, 0.06925207756232687, 0.675, 0.0, 0.0, 0.5, 0.34440430836474256, 0.36675652828145594, 0.0, 1.0, 0.23079886611921388], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5317537], dtype=float32), -0.8129361]. 
=============================================
[2019-04-07 13:29:12,420] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.9955266e-27 9.4050863e-27 3.0988845e-23 1.7524866e-14 3.1821066e-22
 1.0000000e+00 1.5132469e-14 1.1797722e-15], sum to 1.0000
[2019-04-07 13:29:12,420] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3057
[2019-04-07 13:29:12,604] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.1, 86.5, 29.0, 0.0, 24.0, 23.40689115901016, -0.1549156306094019, 1.0, 1.0, 19400.124636486344], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2017800.0000, 
sim time next is 2019600.0000, 
raw observation next is [-6.0, 86.0, 49.0, 0.0, 24.0, 23.77114445843691, -0.1267620961020357, 1.0, 1.0, 6242.311616848667], 
processed observation next is [1.0, 0.391304347826087, 0.296398891966759, 0.86, 0.16333333333333333, 0.0, 0.5, 0.4809287048697426, 0.4577459679659881, 1.0, 1.0, 0.02972529341356508], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04284043], dtype=float32), -0.3479438]. 
=============================================
[2019-04-07 13:29:37,545] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8965549e-23 1.1000759e-22 1.6692720e-20 4.2380183e-11 1.1033598e-19
 1.0000000e+00 2.7032829e-11 9.0086521e-14], sum to 1.0000
[2019-04-07 13:29:37,546] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7325
[2019-04-07 13:29:37,628] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 24.0, 23.3567383320234, -0.1178086891828206, 0.0, 1.0, 40173.15219807766], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2341800.0000, 
sim time next is 2343600.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 24.0, 23.25776625532065, -0.1348182392603572, 0.0, 1.0, 40858.48826669544], 
processed observation next is [0.0, 0.13043478260869565, 0.3988919667590028, 0.62, 0.0, 0.0, 0.5, 0.43814718794338753, 0.45506058691321427, 0.0, 1.0, 0.19456422984140687], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24763177], dtype=float32), 0.38947847]. 
=============================================
[2019-04-07 13:29:37,837] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8408954e-23 1.4701120e-21 7.8069694e-20 1.6941566e-12 5.2502362e-20
 1.0000000e+00 6.8284766e-13 2.3363134e-14], sum to 1.0000
[2019-04-07 13:29:37,839] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6790
[2019-04-07 13:29:37,978] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.05, 63.0, 89.0, 38.0, 24.0, 22.99331257121575, -0.2085391603915678, 0.0, 1.0, 40961.31077766824], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 646200.0000, 
sim time next is 648000.0000, 
raw observation next is [-2.7, 61.0, 100.5, 69.0, 24.0, 23.00915538164015, -0.1989672203995456, 0.0, 1.0, 35905.48957018766], 
processed observation next is [0.0, 0.5217391304347826, 0.38781163434903054, 0.61, 0.335, 0.07624309392265194, 0.5, 0.41742961513667903, 0.4336775932001515, 0.0, 1.0, 0.17097852176279837], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8427558], dtype=float32), -0.7226556]. 
=============================================
[2019-04-07 13:29:37,982] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[92.8558  ]
 [92.86341 ]
 [93.632034]
 [93.49236 ]
 [93.95622 ]], R is [[92.39597321]
 [92.47201538]
 [92.54729462]
 [92.53012848]
 [92.60482788]].
[2019-04-07 13:29:42,399] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1088432e-30 2.2481277e-28 3.9677485e-26 7.3560563e-14 1.2210860e-24
 1.0000000e+00 8.4224186e-16 7.6414465e-18], sum to 1.0000
[2019-04-07 13:29:42,400] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9946
[2019-04-07 13:29:42,427] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 81.0, 106.5, 0.0, 24.0, 24.82461929158074, 0.2649301979127704, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1000800.0000, 
sim time next is 1002600.0000, 
raw observation next is [14.4, 81.0, 94.0, 0.0, 24.0, 24.98345295309227, 0.2889734492284459, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.31333333333333335, 0.0, 0.5, 0.5819544127576893, 0.5963244830761486, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23270364], dtype=float32), 0.74812907]. 
=============================================
[2019-04-07 13:30:07,685] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9875937e-27 3.8914335e-27 1.2650305e-22 1.2916580e-14 4.0810321e-21
 1.0000000e+00 3.7129710e-15 5.0050998e-17], sum to 1.0000
[2019-04-07 13:30:07,689] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1338
[2019-04-07 13:30:07,808] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 80.5, 0.0, 0.0, 24.0, 23.17343868270534, -0.06477015092408668, 0.0, 1.0, 115945.8211321362], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1974600.0000, 
sim time next is 1976400.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 24.0, 23.4629624600413, -0.03165508875031064, 0.0, 1.0, 41424.74451326194], 
processed observation next is [1.0, 0.9130434782608695, 0.30747922437673136, 0.78, 0.0, 0.0, 0.5, 0.45524687167010836, 0.48944830374989645, 0.0, 1.0, 0.19726068815839018], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86443245], dtype=float32), -0.38817272]. 
=============================================
[2019-04-07 13:30:11,788] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5383532e-28 6.9029693e-26 1.3055248e-23 8.6132504e-14 7.9544048e-24
 1.0000000e+00 3.0335170e-14 2.0456379e-15], sum to 1.0000
[2019-04-07 13:30:11,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7737
[2019-04-07 13:30:12,051] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 88.0, 103.5, 0.0, 24.0, 23.27542292811013, -0.02199863861357612, 1.0, 1.0, 65741.18854561479], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1688400.0000, 
sim time next is 1690200.0000, 
raw observation next is [1.1, 88.0, 100.0, 0.0, 24.0, 23.53237720494948, 0.08601420687734508, 1.0, 1.0, 91417.37088191451], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3333333333333333, 0.0, 0.5, 0.46103143374579, 0.5286714022924484, 1.0, 1.0, 0.4353208137234024], 
reward next is 0.8504, 
noisyNet noise sample is [array([-0.61687464], dtype=float32), -0.42331186]. 
=============================================
[2019-04-07 13:30:13,184] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4774287e-25 7.6044629e-24 5.5681357e-22 6.1958919e-12 2.6246913e-20
 1.0000000e+00 4.3849619e-13 1.5692410e-15], sum to 1.0000
[2019-04-07 13:30:13,184] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5232
[2019-04-07 13:30:13,265] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 47.0, 119.0, 835.0, 24.0, 24.88013874669209, 0.2512048648892971, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3933000.0000, 
sim time next is 3934800.0000, 
raw observation next is [-6.0, 45.0, 117.5, 829.5, 24.0, 24.81333222411758, 0.1618711824122114, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.296398891966759, 0.45, 0.39166666666666666, 0.9165745856353591, 0.5, 0.5677776853431317, 0.5539570608040705, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4775518], dtype=float32), 0.0973771]. 
=============================================
[2019-04-07 13:30:15,892] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6250552e-28 1.3068668e-28 8.9445674e-25 1.0963308e-14 4.2095760e-24
 1.0000000e+00 8.4240933e-15 4.8678458e-18], sum to 1.0000
[2019-04-07 13:30:15,892] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4145
[2019-04-07 13:30:15,921] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.75, 52.5, 50.0, 37.0, 24.0, 25.29427078506336, 0.3769586204081925, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1614600.0000, 
sim time next is 1616400.0000, 
raw observation next is [12.2, 54.0, 25.5, 18.5, 24.0, 25.27543709141395, 0.3695986296956583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8005540166204987, 0.54, 0.085, 0.020441988950276244, 0.5, 0.6062864242844958, 0.6231995432318861, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.74927455], dtype=float32), 0.90237033]. 
=============================================
[2019-04-07 13:30:19,700] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0167975e-31 1.8162114e-29 1.1163224e-28 8.2668902e-16 2.0209084e-25
 1.0000000e+00 1.9392743e-18 2.4123521e-18], sum to 1.0000
[2019-04-07 13:30:19,700] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8082
[2019-04-07 13:30:19,722] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 92.5, 721.0, 24.0, 25.70356070881084, 0.5139779707682876, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3164400.0000, 
sim time next is 3166200.0000, 
raw observation next is [6.8, 99.5, 84.0, 679.0, 24.0, 25.92626628653815, 0.548272234397432, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6509695290858727, 0.995, 0.28, 0.7502762430939226, 0.5, 0.660522190544846, 0.6827574114658107, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6994499], dtype=float32), -0.7917468]. 
=============================================
[2019-04-07 13:30:22,407] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.1294273e-28 9.3385136e-28 1.7356000e-23 3.9840920e-14 1.5677096e-22
 1.0000000e+00 4.2240402e-15 3.5930936e-17], sum to 1.0000
[2019-04-07 13:30:22,407] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1481
[2019-04-07 13:30:22,453] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.7, 86.0, 109.0, 752.0, 24.0, 24.67796832685246, 0.2725110756355505, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3234600.0000, 
sim time next is 3236400.0000, 
raw observation next is [-2.4, 80.0, 111.0, 781.5, 24.0, 24.66840786267023, 0.1480053185032081, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.39612188365650974, 0.8, 0.37, 0.86353591160221, 0.5, 0.5557006552225191, 0.5493351061677361, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.99345815], dtype=float32), -0.52332693]. 
=============================================
[2019-04-07 13:30:26,199] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6837202e-26 3.4501864e-26 8.0768376e-23 1.1412880e-13 1.4854906e-21
 1.0000000e+00 1.1718696e-14 1.3978377e-15], sum to 1.0000
[2019-04-07 13:30:26,199] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9654
[2019-04-07 13:30:26,298] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 68.0, 0.0, 0.0, 24.0, 23.43451403400044, -0.01475694592450817, 0.0, 1.0, 54335.22686512515], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2235600.0000, 
sim time next is 2237400.0000, 
raw observation next is [-5.3, 69.5, 0.0, 0.0, 24.0, 23.5364759456575, -0.03486587117600491, 0.0, 1.0, 6248.45137465485], 
processed observation next is [1.0, 0.9130434782608695, 0.31578947368421056, 0.695, 0.0, 0.0, 0.5, 0.4613729954714583, 0.48837804294133175, 0.0, 1.0, 0.029754530355499288], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8148718], dtype=float32), 0.5842037]. 
=============================================
[2019-04-07 13:30:33,946] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8583783e-28 1.4569383e-26 2.5511825e-24 4.2807481e-16 3.4361487e-24
 1.0000000e+00 7.1176517e-15 1.6221187e-17], sum to 1.0000
[2019-04-07 13:30:33,947] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5324
[2019-04-07 13:30:33,993] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.3, 84.0, 142.5, 131.5, 24.0, 24.37876688552415, 0.1591773162295694, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4438800.0000, 
sim time next is 4440600.0000, 
raw observation next is [1.15, 85.0, 165.0, 31.0, 24.0, 24.44742969302773, 0.1907269515282792, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49445983379501385, 0.85, 0.55, 0.03425414364640884, 0.5, 0.5372858077523107, 0.5635756505094264, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08575446], dtype=float32), -0.07685538]. 
=============================================
[2019-04-07 13:30:34,639] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5278604e-26 3.5788115e-25 1.3971178e-23 4.4275523e-14 1.9181264e-21
 1.0000000e+00 8.8614402e-15 1.4780444e-16], sum to 1.0000
[2019-04-07 13:30:34,644] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9934
[2019-04-07 13:30:34,669] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 60.0, 104.0, 720.0, 24.0, 24.54185498762416, 0.1790937761301329, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3492000.0000, 
sim time next is 3493800.0000, 
raw observation next is [0.5, 60.5, 109.0, 770.0, 24.0, 24.64404561123834, 0.2019221301018516, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4764542936288089, 0.605, 0.36333333333333334, 0.850828729281768, 0.5, 0.553670467603195, 0.5673073767006173, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20630409], dtype=float32), 2.3682373]. 
=============================================
[2019-04-07 13:30:47,172] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0356143e-25 1.3090293e-25 1.3807941e-22 1.2804151e-12 2.0538731e-21
 1.0000000e+00 4.7833733e-14 1.9890714e-15], sum to 1.0000
[2019-04-07 13:30:47,173] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8311
[2019-04-07 13:30:47,219] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 68.0, 0.0, 0.0, 24.0, 23.43451403400044, -0.01475694592450817, 0.0, 1.0, 54335.22686512515], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2235600.0000, 
sim time next is 2237400.0000, 
raw observation next is [-5.3, 69.5, 0.0, 0.0, 24.0, 23.5364759456575, -0.03486587117600491, 0.0, 1.0, 6248.45137465485], 
processed observation next is [1.0, 0.9130434782608695, 0.31578947368421056, 0.695, 0.0, 0.0, 0.5, 0.4613729954714583, 0.48837804294133175, 0.0, 1.0, 0.029754530355499288], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2373048], dtype=float32), -1.8530498]. 
=============================================
[2019-04-07 13:30:56,625] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-07 13:30:56,626] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:30:56,626] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:30:56,628] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run34
[2019-04-07 13:30:56,694] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:30:56,694] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:30:56,697] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run34
[2019-04-07 13:30:56,781] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 13:30:56,782] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:30:56,784] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run34
[2019-04-07 13:33:06,185] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11173747], dtype=float32), 0.13251796]
[2019-04-07 13:33:06,185] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-2.127065374, 54.45631726, 0.0, 0.0, 24.0, 23.58059637030116, -0.05037510545740789, 0.0, 1.0, 28580.67552666475]
[2019-04-07 13:33:06,185] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 13:33:06,186] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [2.1324396e-23 2.8804470e-22 4.8822907e-20 5.8650897e-12 2.5477171e-19
 1.0000000e+00 1.3975067e-12 3.9338026e-14], sampled 0.7414226368008254
[2019-04-07 13:33:25,022] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 13:33:44,183] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 13:33:47,751] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 13:33:48,775] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 660000, evaluation results [660000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 13:33:55,223] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:33:55,224] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:33:55,227] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run16
[2019-04-07 13:34:05,102] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:34:05,102] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:34:05,107] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run16
[2019-04-07 13:34:05,914] A3C_AGENT_WORKER-Thread-15 INFO:Local step 42500, global step 662603: loss 2.5751
[2019-04-07 13:34:05,915] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 42500, global step 662603: learning rate 0.0000
[2019-04-07 13:34:06,069] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5473209e-24 1.5057484e-23 1.8639620e-21 5.5414649e-12 1.1579659e-20
 1.0000000e+00 1.0798327e-13 4.3926252e-15], sum to 1.0000
[2019-04-07 13:34:06,070] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0447
[2019-04-07 13:34:06,128] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.7, 29.0, 38.5, 83.5, 24.0, 24.218759878628, 0.002273360613668868, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2566800.0000, 
sim time next is 2568600.0000, 
raw observation next is [1.6, 32.0, 0.0, 0.0, 24.0, 24.01745515009603, -0.1312505969089656, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5069252077562327, 0.32, 0.0, 0.0, 0.5, 0.5014545958413358, 0.4562498010303448, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3734659], dtype=float32), -2.3133357]. 
=============================================
[2019-04-07 13:34:14,644] A3C_AGENT_WORKER-Thread-12 INFO:Local step 42500, global step 663949: loss 2.5857
[2019-04-07 13:34:14,645] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 42500, global step 663949: learning rate 0.0000
[2019-04-07 13:34:16,533] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1591214e-25 7.3532797e-25 3.2095993e-21 6.6656878e-14 2.3512753e-22
 1.0000000e+00 2.0019485e-14 2.0799764e-16], sum to 1.0000
[2019-04-07 13:34:16,533] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3510
[2019-04-07 13:34:16,597] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.75, 65.0, 0.0, 0.0, 24.0, 23.25753639654987, -0.1042014986745407, 0.0, 1.0, 43695.61489160464], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2593800.0000, 
sim time next is 2595600.0000, 
raw observation next is [-5.0, 68.0, 0.0, 0.0, 24.0, 23.43200633193377, -0.1030779995842715, 0.0, 1.0, 31301.48013715554], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.68, 0.0, 0.0, 0.5, 0.4526671943278142, 0.46564066680524285, 0.0, 1.0, 0.14905466731978828], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0037651], dtype=float32), -1.0401832]. 
=============================================
[2019-04-07 13:34:30,710] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.21115424e-23 6.34594611e-23 2.43053743e-20 2.53733861e-12
 2.10097788e-19 1.00000000e+00 2.90401004e-13 3.50545602e-14], sum to 1.0000
[2019-04-07 13:34:30,711] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0720
[2019-04-07 13:34:30,794] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 24.0, 23.07415046260944, -0.1150411789452542, 0.0, 1.0, 43927.12989325929], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2953800.0000, 
sim time next is 2955600.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 24.0, 23.00918889568232, -0.1246428738092991, 0.0, 1.0, 43901.487762038865], 
processed observation next is [0.0, 0.21739130434782608, 0.3795013850415513, 0.84, 0.0, 0.0, 0.5, 0.41743240797352676, 0.4584523753969003, 0.0, 1.0, 0.2090547036287565], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1217122], dtype=float32), 1.0281621]. 
=============================================
[2019-04-07 13:34:37,957] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:34:37,957] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:34:37,960] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run16
[2019-04-07 13:34:40,655] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1165215e-25 4.0375888e-25 1.7716343e-24 5.8429010e-13 4.2874232e-20
 1.0000000e+00 1.1854599e-13 2.1152564e-15], sum to 1.0000
[2019-04-07 13:34:40,662] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1205
[2019-04-07 13:34:40,734] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 60.0, 117.0, 823.5, 24.0, 24.89389196107415, 0.2337776034951436, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3762000.0000, 
sim time next is 3763800.0000, 
raw observation next is [-0.5, 60.0, 115.0, 818.0, 24.0, 24.85407390020132, 0.2664030356837588, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.44875346260387816, 0.6, 0.38333333333333336, 0.9038674033149171, 0.5, 0.5711728250167768, 0.5888010118945862, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.6321318], dtype=float32), -0.2620409]. 
=============================================
[2019-04-07 13:34:42,092] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:34:42,093] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:34:42,097] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run16
[2019-04-07 13:34:47,542] A3C_AGENT_WORKER-Thread-6 INFO:Local step 42500, global step 669348: loss 2.5254
[2019-04-07 13:34:47,543] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 42500, global step 669348: learning rate 0.0000
[2019-04-07 13:34:51,012] A3C_AGENT_WORKER-Thread-16 INFO:Local step 42500, global step 670011: loss 2.6419
[2019-04-07 13:34:51,012] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 42500, global step 670011: learning rate 0.0000
[2019-04-07 13:34:56,392] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.5054175e-26 1.0044839e-24 2.0207718e-23 4.5506464e-13 1.1272428e-21
 1.0000000e+00 1.3994318e-13 2.3929710e-15], sum to 1.0000
[2019-04-07 13:34:56,392] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8003
[2019-04-07 13:34:56,435] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 48.0, 106.0, 782.0, 24.0, 25.33911428611963, 0.3506447744745152, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3853800.0000, 
sim time next is 3855600.0000, 
raw observation next is [2.0, 48.0, 96.5, 749.5, 24.0, 25.50284671896991, 0.2828453993882348, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 0.48, 0.32166666666666666, 0.8281767955801105, 0.5, 0.6252372265808258, 0.5942817997960782, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36906675], dtype=float32), -1.870661]. 
=============================================
[2019-04-07 13:35:09,383] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43000, global step 673375: loss 4.9430
[2019-04-07 13:35:09,383] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43000, global step 673375: learning rate 0.0000
[2019-04-07 13:35:17,009] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43000, global step 674944: loss 4.9304
[2019-04-07 13:35:17,012] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43000, global step 674944: learning rate 0.0000
[2019-04-07 13:35:17,469] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.80755265e-24 3.44277832e-22 1.00967604e-20 3.22399572e-12
 2.15086682e-19 1.00000000e+00 2.87018835e-13 9.10620727e-14], sum to 1.0000
[2019-04-07 13:35:17,471] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4945
[2019-04-07 13:35:17,661] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 37.5, 0.0, 0.0, 24.0, 23.49112009438599, -0.121905275410895, 1.0, 1.0, 13719.774295521194], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4087800.0000, 
sim time next is 4089600.0000, 
raw observation next is [-4.0, 34.0, 46.0, 234.5, 24.0, 23.5394109762375, -0.09660602634085864, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.34, 0.15333333333333332, 0.2591160220994475, 0.5, 0.46161758135312514, 0.4677979912197138, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8238195], dtype=float32), -0.45285612]. 
=============================================
[2019-04-07 13:35:28,142] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.4158733e-27 9.7091255e-25 2.1123046e-23 1.5366887e-12 1.8957144e-21
 1.0000000e+00 1.0477542e-13 1.8159531e-16], sum to 1.0000
[2019-04-07 13:35:28,142] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6380
[2019-04-07 13:35:28,225] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.55, 73.0, 0.0, 0.0, 24.0, 23.43989072903774, 0.04946517128123575, 0.0, 1.0, 67805.22609667761], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4494600.0000, 
sim time next is 4496400.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 24.0, 23.77433260471673, 0.05728754581223572, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.5, 0.4811943837263941, 0.5190958486040785, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6125966], dtype=float32), 0.8256532]. 
=============================================
[2019-04-07 13:35:34,834] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:35:34,834] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:35:34,839] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run16
[2019-04-07 13:35:38,774] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-07 13:35:38,775] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:35:38,775] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:35:38,777] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run35
[2019-04-07 13:35:38,795] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:35:38,795] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:35:38,800] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run35
[2019-04-07 13:35:38,822] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 13:35:38,824] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:35:38,828] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run35
[2019-04-07 13:38:01,409] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 13:38:20,315] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 13:38:23,303] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 13:38:24,328] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 680000, evaluation results [680000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 13:38:25,985] A3C_AGENT_WORKER-Thread-19 INFO:Local step 42500, global step 680311: loss 2.5235
[2019-04-07 13:38:25,994] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 42500, global step 680311: learning rate 0.0000
[2019-04-07 13:38:27,975] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43500, global step 680677: loss 0.5586
[2019-04-07 13:38:27,975] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43500, global step 680677: learning rate 0.0000
[2019-04-07 13:38:30,783] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.8600283e-23 3.5599043e-22 3.8939569e-20 2.1420365e-11 5.2450142e-19
 1.0000000e+00 1.2075148e-11 4.7244902e-14], sum to 1.0000
[2019-04-07 13:38:30,797] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0355
[2019-04-07 13:38:30,881] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 44.5, 33.0, 187.0, 24.0, 23.2773774207347, -0.1022004600250471, 0.0, 1.0, 12453.607780153694], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4901400.0000, 
sim time next is 4903200.0000, 
raw observation next is [2.0, 44.0, 16.5, 93.5, 24.0, 23.18478954664035, -0.1402185983305223, 0.0, 1.0, 23240.48265095132], 
processed observation next is [0.0, 0.782608695652174, 0.518005540166205, 0.44, 0.055, 0.10331491712707182, 0.5, 0.43206579555336244, 0.4532604672231592, 0.0, 1.0, 0.11066896500453009], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0323389], dtype=float32), -1.3264444]. 
=============================================
[2019-04-07 13:38:30,895] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.56378986e-22 9.95315397e-23 9.46269935e-20 1.14874065e-11
 1.72510100e-20 1.00000000e+00 2.09752983e-12 9.33775643e-15], sum to 1.0000
[2019-04-07 13:38:30,895] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1759
[2019-04-07 13:38:30,923] A3C_AGENT_WORKER-Thread-6 INFO:Local step 43000, global step 681252: loss 4.7650
[2019-04-07 13:38:30,923] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 43000, global step 681252: learning rate 0.0000
[2019-04-07 13:38:30,977] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 51.0, 0.0, 0.0, 24.0, 23.71376737570707, -0.0242618392526832, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4827600.0000, 
sim time next is 4829400.0000, 
raw observation next is [-0.5, 53.0, 0.0, 0.0, 24.0, 23.5501028410164, -0.04138170463842877, 0.0, 1.0, 56853.88128760192], 
processed observation next is [0.0, 0.9130434782608695, 0.44875346260387816, 0.53, 0.0, 0.0, 0.5, 0.46250857008470003, 0.48620609845385704, 0.0, 1.0, 0.2707327680361996], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6526895], dtype=float32), -0.4510253]. 
=============================================
[2019-04-07 13:38:31,933] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:38:31,933] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:38:31,965] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run16
[2019-04-07 13:38:33,388] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:38:33,388] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:38:33,396] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run16
[2019-04-07 13:38:33,657] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:38:33,657] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:38:33,661] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run16
[2019-04-07 13:38:34,193] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:38:34,193] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:38:34,197] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run16
[2019-04-07 13:38:34,567] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43000, global step 681841: loss 4.8246
[2019-04-07 13:38:34,568] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43000, global step 681841: learning rate 0.0000
[2019-04-07 13:38:36,283] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43500, global step 682099: loss 0.5760
[2019-04-07 13:38:36,283] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43500, global step 682099: learning rate 0.0000
[2019-04-07 13:38:38,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2717710e-26 2.4903252e-25 4.8834776e-22 6.6793183e-14 3.5776839e-22
 1.0000000e+00 3.1353901e-15 4.0817565e-17], sum to 1.0000
[2019-04-07 13:38:38,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3137
[2019-04-07 13:38:38,276] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 37.0, 0.0, 0.0, 24.0, 23.70131950667876, 0.09132229193174868, 0.0, 1.0, 47682.49487485751], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5009400.0000, 
sim time next is 5011200.0000, 
raw observation next is [2.0, 40.0, 0.0, 0.0, 24.0, 23.80669531626796, 0.07189685118494107, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.4, 0.0, 0.0, 0.5, 0.48389127635566326, 0.523965617061647, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.500479], dtype=float32), -0.73733795]. 
=============================================
[2019-04-07 13:38:40,165] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:38:40,165] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:38:40,187] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run16
[2019-04-07 13:38:41,731] A3C_AGENT_WORKER-Thread-3 INFO:Local step 42500, global step 682859: loss 2.4425
[2019-04-07 13:38:41,733] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 42500, global step 682859: learning rate 0.0000
[2019-04-07 13:38:43,169] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:38:43,169] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:38:43,184] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run16
[2019-04-07 13:38:43,213] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:38:43,214] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:38:43,218] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run16
[2019-04-07 13:38:43,265] A3C_AGENT_WORKER-Thread-18 INFO:Local step 42500, global step 683108: loss 2.5503
[2019-04-07 13:38:43,292] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 42500, global step 683108: learning rate 0.0000
[2019-04-07 13:38:43,364] A3C_AGENT_WORKER-Thread-20 INFO:Local step 42500, global step 683116: loss 2.5077
[2019-04-07 13:38:43,365] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 42500, global step 683116: learning rate 0.0000
[2019-04-07 13:38:44,360] A3C_AGENT_WORKER-Thread-14 INFO:Local step 42500, global step 683244: loss 2.6622
[2019-04-07 13:38:44,361] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 42500, global step 683244: learning rate 0.0000
[2019-04-07 13:38:46,933] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:38:46,934] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:38:46,938] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run16
[2019-04-07 13:38:49,313] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:38:49,313] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:38:49,316] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run16
[2019-04-07 13:38:50,242] A3C_AGENT_WORKER-Thread-4 INFO:Local step 42500, global step 683927: loss 2.6457
[2019-04-07 13:38:50,242] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 42500, global step 683927: learning rate 0.0000
[2019-04-07 13:38:53,405] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:38:53,405] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:38:53,408] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run16
[2019-04-07 13:38:53,734] A3C_AGENT_WORKER-Thread-2 INFO:Local step 42500, global step 684284: loss 2.3977
[2019-04-07 13:38:53,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 42500, global step 684284: learning rate 0.0000
[2019-04-07 13:38:54,102] A3C_AGENT_WORKER-Thread-17 INFO:Local step 42500, global step 684323: loss 2.5947
[2019-04-07 13:38:54,103] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 42500, global step 684323: learning rate 0.0000
[2019-04-07 13:38:56,046] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.5901926e-30 4.8271214e-28 1.5080060e-25 4.8337846e-16 2.6390125e-24
 1.0000000e+00 5.7333267e-15 9.7377579e-18], sum to 1.0000
[2019-04-07 13:38:56,046] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0421
[2019-04-07 13:38:56,123] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 49.0, 145.0, 0.0, 24.0, 25.60470790582042, 0.3274439130572067, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1607400.0000, 
sim time next is 1609200.0000, 
raw observation next is [13.8, 49.0, 111.5, 0.0, 24.0, 24.88434976144732, 0.3335624882800312, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.844875346260388, 0.49, 0.37166666666666665, 0.0, 0.5, 0.5736958134539435, 0.6111874960933438, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.641006], dtype=float32), 0.27902296]. 
=============================================
[2019-04-07 13:38:56,977] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:38:56,977] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:38:56,981] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run16
[2019-04-07 13:38:58,122] A3C_AGENT_WORKER-Thread-10 INFO:Local step 42500, global step 684764: loss 2.5391
[2019-04-07 13:38:58,123] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 42500, global step 684764: learning rate 0.0000
[2019-04-07 13:38:59,510] A3C_AGENT_WORKER-Thread-11 INFO:Local step 42500, global step 684928: loss 2.4683
[2019-04-07 13:38:59,511] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 42500, global step 684928: learning rate 0.0000
[2019-04-07 13:39:01,524] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.4397407e-26 7.8695009e-25 1.5894601e-22 1.2412689e-13 3.8234348e-21
 1.0000000e+00 6.4121360e-15 7.9780458e-17], sum to 1.0000
[2019-04-07 13:39:01,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1512
[2019-04-07 13:39:01,833] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.1, 61.0, 130.0, 603.0, 24.0, 23.64351888213438, -0.03606688296584665, 1.0, 1.0, 65675.31588276669], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 131400.0000, 
sim time next is 133200.0000, 
raw observation next is [-7.8, 61.0, 134.5, 543.5, 24.0, 24.00963268211738, 0.05972549207653747, 1.0, 1.0, 49176.2538779974], 
processed observation next is [1.0, 0.5652173913043478, 0.24653739612188366, 0.61, 0.4483333333333333, 0.6005524861878453, 0.5, 0.5008027235097817, 0.5199084973588458, 1.0, 1.0, 0.23417263751427334], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27339765], dtype=float32), 0.8475962]. 
=============================================
[2019-04-07 13:39:03,186] A3C_AGENT_WORKER-Thread-5 INFO:Local step 42500, global step 685313: loss 2.5361
[2019-04-07 13:39:03,186] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 42500, global step 685313: learning rate 0.0000
[2019-04-07 13:39:08,137] A3C_AGENT_WORKER-Thread-13 INFO:Local step 42500, global step 685827: loss 2.5894
[2019-04-07 13:39:08,138] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 42500, global step 685827: learning rate 0.0000
[2019-04-07 13:39:09,063] A3C_AGENT_WORKER-Thread-6 INFO:Local step 43500, global step 685925: loss 0.9016
[2019-04-07 13:39:09,063] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 43500, global step 685925: learning rate 0.0000
[2019-04-07 13:39:12,969] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43500, global step 686367: loss 0.7308
[2019-04-07 13:39:12,969] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43500, global step 686367: learning rate 0.0000
[2019-04-07 13:39:22,287] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.1471505e-26 2.1912991e-26 1.3832990e-21 8.5535480e-13 2.4229336e-23
 1.0000000e+00 8.0306719e-13 1.5061509e-15], sum to 1.0000
[2019-04-07 13:39:22,287] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9326
[2019-04-07 13:39:22,326] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 79.0, 0.0, 0.0, 24.0, 22.84991609236818, -0.2308423921405384, 0.0, 1.0, 45471.01919857669], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 259200.0000, 
sim time next is 261000.0000, 
raw observation next is [-5.6, 73.0, 0.0, 0.0, 24.0, 22.81867154317765, -0.2315387007964116, 0.0, 1.0, 45482.88584391228], 
processed observation next is [1.0, 0.0, 0.30747922437673136, 0.73, 0.0, 0.0, 0.5, 0.40155596193147086, 0.4228204330678628, 0.0, 1.0, 0.21658517068529656], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4711584], dtype=float32), 0.30439308]. 
=============================================
[2019-04-07 13:39:22,349] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[104.19236 ]
 [105.25217 ]
 [105.20598 ]
 [104.590775]
 [103.95048 ]], R is [[103.9466629 ]
 [103.90719604]
 [103.86812592]
 [103.82944489]
 [103.79115295]].
[2019-04-07 13:39:31,167] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44000, global step 688335: loss 0.1501
[2019-04-07 13:39:31,168] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44000, global step 688335: learning rate 0.0000
[2019-04-07 13:39:34,435] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43000, global step 688714: loss 4.4662
[2019-04-07 13:39:34,436] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43000, global step 688714: learning rate 0.0000
[2019-04-07 13:39:38,962] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44000, global step 689305: loss 0.1568
[2019-04-07 13:39:38,964] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44000, global step 689305: learning rate 0.0000
[2019-04-07 13:39:39,098] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5717482e-26 1.7149797e-24 5.5756998e-23 6.3424087e-13 4.2045249e-22
 1.0000000e+00 2.4050017e-14 7.4415206e-17], sum to 1.0000
[2019-04-07 13:39:39,098] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2245
[2019-04-07 13:39:39,346] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 24.0, 23.08133714853161, -0.2552864703499348, 0.0, 1.0, 42767.014346446966], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 716400.0000, 
sim time next is 718200.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 24.0, 23.22942197735409, -0.1956893321549612, 1.0, 1.0, 14281.845538102525], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.0, 0.0, 0.5, 0.43578516477950746, 0.43477022261501297, 1.0, 1.0, 0.0680087882766787], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02107052], dtype=float32), 0.28504652]. 
=============================================
[2019-04-07 13:39:54,634] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43000, global step 691461: loss 4.3236
[2019-04-07 13:39:54,635] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43000, global step 691461: learning rate 0.0000
[2019-04-07 13:39:55,061] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43000, global step 691513: loss 4.3851
[2019-04-07 13:39:55,062] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43000, global step 691513: learning rate 0.0000
[2019-04-07 13:39:55,209] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43000, global step 691535: loss 4.4153
[2019-04-07 13:39:55,210] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43000, global step 691535: learning rate 0.0000
[2019-04-07 13:39:57,391] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43000, global step 691974: loss 4.2652
[2019-04-07 13:39:57,392] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43000, global step 691974: learning rate 0.0000
[2019-04-07 13:39:59,354] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43000, global step 692351: loss 4.3281
[2019-04-07 13:39:59,356] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43000, global step 692352: learning rate 0.0000
[2019-04-07 13:40:02,708] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43000, global step 693177: loss 4.3106
[2019-04-07 13:40:02,709] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43000, global step 693177: learning rate 0.0000
[2019-04-07 13:40:05,272] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43000, global step 693752: loss 4.3884
[2019-04-07 13:40:05,272] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43000, global step 693752: learning rate 0.0000
[2019-04-07 13:40:06,612] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.14037646e-24 7.25720226e-23 1.03753474e-20 3.54074686e-11
 1.64314866e-19 1.00000000e+00 2.13455853e-13 7.94197997e-15], sum to 1.0000
[2019-04-07 13:40:06,612] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8529
[2019-04-07 13:40:06,672] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.0, 83.0, 0.0, 0.0, 24.0, 21.93013261523448, -0.4187810807512407, 0.0, 1.0, 44307.39458723136], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2700000.0000, 
sim time next is 2701800.0000, 
raw observation next is [-15.5, 83.0, 0.0, 0.0, 24.0, 21.78692877617548, -0.4400877034776151, 0.0, 1.0, 44111.19721834703], 
processed observation next is [1.0, 0.2608695652173913, 0.033240997229916885, 0.83, 0.0, 0.0, 0.5, 0.31557739801462326, 0.35330409884079494, 0.0, 1.0, 0.2100533200873668], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2396429], dtype=float32), 1.6100407]. 
=============================================
[2019-04-07 13:40:07,772] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43500, global step 694313: loss 0.7285
[2019-04-07 13:40:07,775] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43500, global step 694313: learning rate 0.0000
[2019-04-07 13:40:08,244] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43000, global step 694399: loss 4.3758
[2019-04-07 13:40:08,245] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43000, global step 694399: learning rate 0.0000
[2019-04-07 13:40:08,321] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43000, global step 694412: loss 4.3539
[2019-04-07 13:40:08,322] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43000, global step 694412: learning rate 0.0000
[2019-04-07 13:40:08,836] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4831759e-27 3.8088238e-26 3.4047947e-24 1.6073204e-13 1.2864372e-23
 1.0000000e+00 1.3023814e-15 6.2229386e-17], sum to 1.0000
[2019-04-07 13:40:08,889] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1946
[2019-04-07 13:40:08,910] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 113.0, 0.0, 24.0, 24.2911945065968, 0.1476115641348721, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1341000.0000, 
sim time next is 1342800.0000, 
raw observation next is [1.1, 92.0, 109.5, 0.0, 24.0, 24.12980287363732, 0.1257292184704436, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.365, 0.0, 0.5, 0.5108169061364434, 0.5419097394901479, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5303544], dtype=float32), -0.7001066]. 
=============================================
[2019-04-07 13:40:09,098] A3C_AGENT_WORKER-Thread-6 INFO:Local step 44000, global step 694575: loss 0.1551
[2019-04-07 13:40:09,099] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 44000, global step 694575: learning rate 0.0000
[2019-04-07 13:40:09,854] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43000, global step 694742: loss 4.3938
[2019-04-07 13:40:09,854] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43000, global step 694742: learning rate 0.0000
[2019-04-07 13:40:12,215] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44000, global step 695282: loss 0.1593
[2019-04-07 13:40:12,215] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44000, global step 695282: learning rate 0.0000
[2019-04-07 13:40:13,357] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44500, global step 695585: loss 0.6509
[2019-04-07 13:40:13,358] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44500, global step 695585: learning rate 0.0000
[2019-04-07 13:40:14,192] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43000, global step 695812: loss 4.4006
[2019-04-07 13:40:14,192] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43000, global step 695812: learning rate 0.0000
[2019-04-07 13:40:14,886] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.1337144e-31 2.1592999e-28 5.1009903e-26 1.2306211e-16 3.6455848e-25
 1.0000000e+00 1.5533323e-16 1.3068136e-18], sum to 1.0000
[2019-04-07 13:40:14,915] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6614
[2019-04-07 13:40:14,977] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.25, 82.0, 0.0, 0.0, 24.0, 23.73415229218964, 0.103882119737941, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1553400.0000, 
sim time next is 1555200.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 24.0, 23.56032826239871, 0.1141233084583882, 0.0, 1.0, 99120.96523205434], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 0.5, 0.46336068853322576, 0.5380411028194627, 0.0, 1.0, 0.4720045963431159], 
reward next is 0.8137, 
noisyNet noise sample is [array([0.18433778], dtype=float32), -0.021370174]. 
=============================================
[2019-04-07 13:40:20,083] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.7849047e-27 5.3080571e-26 1.9348579e-23 6.3597554e-13 1.3506823e-22
 1.0000000e+00 1.1967646e-13 9.2164298e-18], sum to 1.0000
[2019-04-07 13:40:20,084] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9282
[2019-04-07 13:40:20,144] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 109.5, 0.0, 24.0, 24.12980287363732, 0.1257292184704436, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1342800.0000, 
sim time next is 1344600.0000, 
raw observation next is [1.1, 92.0, 106.0, 0.0, 24.0, 24.1122423287827, 0.1249316632190127, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.35333333333333333, 0.0, 0.5, 0.5093535273985582, 0.5416438877396709, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0582798], dtype=float32), 1.115459]. 
=============================================
[2019-04-07 13:40:21,732] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44500, global step 697721: loss 0.7343
[2019-04-07 13:40:21,732] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44500, global step 697721: learning rate 0.0000
[2019-04-07 13:40:22,246] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4237365e-29 5.7958059e-28 2.7181435e-25 2.3113890e-15 9.3093008e-23
 1.0000000e+00 1.3168635e-16 1.0990761e-17], sum to 1.0000
[2019-04-07 13:40:22,246] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1201
[2019-04-07 13:40:22,300] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 24.0, 23.50654437825035, 0.0311414705803309, 0.0, 1.0, 26476.19567625131], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1468800.0000, 
sim time next is 1470600.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 24.0, 23.45077562400641, 0.06723736698683284, 0.0, 1.0, 53764.525251115934], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.5, 0.45423130200053424, 0.5224124556622777, 0.0, 1.0, 0.2560215488148378], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22037816], dtype=float32), -0.04148441]. 
=============================================
[2019-04-07 13:40:27,061] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43500, global step 698713: loss 0.7505
[2019-04-07 13:40:27,061] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43500, global step 698713: learning rate 0.0000
[2019-04-07 13:40:27,076] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43500, global step 698717: loss 0.8003
[2019-04-07 13:40:27,077] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43500, global step 698717: learning rate 0.0000
[2019-04-07 13:40:27,618] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43500, global step 698819: loss 0.7531
[2019-04-07 13:40:27,620] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43500, global step 698819: learning rate 0.0000
[2019-04-07 13:40:29,428] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43500, global step 699157: loss 0.7003
[2019-04-07 13:40:29,430] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43500, global step 699157: learning rate 0.0000
[2019-04-07 13:40:29,448] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4957912e-24 3.7408781e-23 9.0436028e-21 4.3402039e-13 6.9959369e-19
 1.0000000e+00 1.3942977e-13 7.6181737e-15], sum to 1.0000
[2019-04-07 13:40:29,448] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5786
[2019-04-07 13:40:29,670] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 178.0, 62.0, 24.0, 23.14947723797735, -0.1472940253278607, 0.0, 1.0, 31610.814228909006], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1864800.0000, 
sim time next is 1866600.0000, 
raw observation next is [-4.5, 77.0, 186.0, 84.0, 24.0, 23.14121159867821, -0.1451453132832774, 0.0, 1.0, 36150.11066552001], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.77, 0.62, 0.09281767955801105, 0.5, 0.42843429988985093, 0.4516182289055742, 0.0, 1.0, 0.17214338412152386], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2813503], dtype=float32), -0.05921611]. 
=============================================
[2019-04-07 13:40:31,658] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43500, global step 699640: loss 0.6612
[2019-04-07 13:40:31,658] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43500, global step 699640: learning rate 0.0000
[2019-04-07 13:40:33,690] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-07 13:40:33,705] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:40:33,705] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:40:33,708] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run36
[2019-04-07 13:40:33,731] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:40:33,732] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:40:33,732] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 13:40:33,733] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:40:33,736] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run36
[2019-04-07 13:40:33,736] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run36
[2019-04-07 13:41:56,982] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11171189], dtype=float32), 0.13356042]
[2019-04-07 13:41:56,982] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [3.55, 87.0, 53.0, 77.0, 24.0, 23.6453150775214, 0.06853864162151606, 1.0, 1.0, 57201.65183093292]
[2019-04-07 13:41:56,982] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 13:41:56,983] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [7.9127802e-27 1.1838823e-25 3.0957653e-23 7.4339569e-14 4.6750528e-22
 1.0000000e+00 1.0389478e-14 3.5869328e-16], sampled 0.2527611308895592
[2019-04-07 13:43:03,641] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 13:43:21,083] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 13:43:26,320] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 13:43:27,417] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 700000, evaluation results [700000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 13:43:28,963] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6445993e-24 1.4892403e-22 2.0485454e-20 2.2788197e-12 2.6893247e-19
 1.0000000e+00 2.3849642e-12 9.6728764e-14], sum to 1.0000
[2019-04-07 13:43:28,963] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6886
[2019-04-07 13:43:29,143] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 83.0, 109.0, 0.0, 24.0, 23.16229663589679, -0.08421368822663135, 0.0, 1.0, 21858.387467205746], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1778400.0000, 
sim time next is 1780200.0000, 
raw observation next is [-2.8, 85.0, 99.0, 0.0, 24.0, 23.074919921732, -0.1008397493021423, 0.0, 1.0, 44878.54694840547], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.85, 0.33, 0.0, 0.5, 0.4229099934776667, 0.4663867502326193, 0.0, 1.0, 0.21370736642097843], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.61853063], dtype=float32), -0.036931988]. 
=============================================
[2019-04-07 13:43:29,727] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43500, global step 700369: loss 0.8759
[2019-04-07 13:43:29,728] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43500, global step 700369: learning rate 0.0000
[2019-04-07 13:43:32,878] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43500, global step 700813: loss 0.7173
[2019-04-07 13:43:32,879] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43500, global step 700813: learning rate 0.0000
[2019-04-07 13:43:35,697] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43500, global step 701195: loss 0.6384
[2019-04-07 13:43:35,698] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43500, global step 701195: learning rate 0.0000
[2019-04-07 13:43:35,766] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43500, global step 701208: loss 0.7524
[2019-04-07 13:43:35,766] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43500, global step 701208: learning rate 0.0000
[2019-04-07 13:43:36,598] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43500, global step 701324: loss 0.6287
[2019-04-07 13:43:36,600] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43500, global step 701324: learning rate 0.0000
[2019-04-07 13:43:42,343] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43500, global step 702089: loss 0.5822
[2019-04-07 13:43:42,344] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43500, global step 702089: learning rate 0.0000
[2019-04-07 13:43:43,083] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45000, global step 702204: loss 1.8543
[2019-04-07 13:43:43,084] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45000, global step 702204: learning rate 0.0000
[2019-04-07 13:43:44,404] A3C_AGENT_WORKER-Thread-6 INFO:Local step 44500, global step 702387: loss 0.7159
[2019-04-07 13:43:44,405] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 44500, global step 702387: learning rate 0.0000
[2019-04-07 13:43:48,713] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44500, global step 703016: loss 0.6194
[2019-04-07 13:43:48,714] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44500, global step 703016: learning rate 0.0000
[2019-04-07 13:43:49,939] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.6234936e-27 1.0572573e-25 2.7774726e-23 2.9420703e-14 1.5386194e-22
 1.0000000e+00 6.3033405e-15 2.8942553e-17], sum to 1.0000
[2019-04-07 13:43:49,939] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0460
[2019-04-07 13:43:49,984] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 24.0, 23.87227256895463, 0.05633470301993926, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3783600.0000, 
sim time next is 3785400.0000, 
raw observation next is [-2.0, 68.0, 0.0, 0.0, 24.0, 23.44129927440989, 0.0327393279317467, 0.0, 1.0, 120131.3572161504], 
processed observation next is [1.0, 0.8260869565217391, 0.40720221606648205, 0.68, 0.0, 0.0, 0.5, 0.45344160620082413, 0.5109131093105822, 0.0, 1.0, 0.5720540819816685], 
reward next is 0.7137, 
noisyNet noise sample is [array([-1.1721557], dtype=float32), -0.2868076]. 
=============================================
[2019-04-07 13:43:50,300] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4810111e-27 1.3766108e-25 3.7983394e-23 3.1423588e-15 5.4550668e-22
 1.0000000e+00 1.4609161e-13 2.8805436e-16], sum to 1.0000
[2019-04-07 13:43:50,300] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0966
[2019-04-07 13:43:50,592] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 79.0, 36.5, 18.5, 24.0, 23.49414900012665, -0.1046845016572593, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2102400.0000, 
sim time next is 2104200.0000, 
raw observation next is [-7.55, 80.5, 72.0, 37.0, 24.0, 23.57344261708887, -0.08889460230991397, 1.0, 1.0, 6245.9931522377565], 
processed observation next is [1.0, 0.34782608695652173, 0.25346260387811637, 0.805, 0.24, 0.04088397790055249, 0.5, 0.4644535514240724, 0.47036846589669534, 1.0, 1.0, 0.029742824534465508], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20932718], dtype=float32), 1.8775157]. 
=============================================
[2019-04-07 13:43:51,423] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.1511954e-26 3.4995262e-24 1.5523873e-20 3.1754943e-13 4.0180682e-21
 1.0000000e+00 1.6778286e-14 1.1298422e-15], sum to 1.0000
[2019-04-07 13:43:51,423] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7844
[2019-04-07 13:43:51,484] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.1, 86.5, 0.0, 0.0, 24.0, 22.87927236540075, -0.2424726883329111, 0.0, 1.0, 44693.50272509931], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2259000.0000, 
sim time next is 2260800.0000, 
raw observation next is [-8.4, 87.0, 0.0, 0.0, 24.0, 22.78203412032366, -0.2591658211925887, 0.0, 1.0, 44610.15752545351], 
processed observation next is [1.0, 0.17391304347826086, 0.2299168975069252, 0.87, 0.0, 0.0, 0.5, 0.3985028433603051, 0.4136113929358038, 0.0, 1.0, 0.21242932154977864], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1258297], dtype=float32), -0.90401703]. 
=============================================
[2019-04-07 13:43:54,707] A3C_AGENT_WORKER-Thread-12 INFO:Local step 45000, global step 703866: loss 2.0989
[2019-04-07 13:43:54,709] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 45000, global step 703866: learning rate 0.0000
[2019-04-07 13:43:57,386] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44000, global step 704285: loss 0.1501
[2019-04-07 13:43:57,394] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44000, global step 704285: learning rate 0.0000
[2019-04-07 13:44:08,397] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:44:08,397] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:44:08,400] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run17
[2019-04-07 13:44:19,256] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9745686e-25 6.4940077e-23 1.4158546e-21 1.3201675e-13 2.1518669e-20
 1.0000000e+00 1.2987933e-13 1.9690126e-15], sum to 1.0000
[2019-04-07 13:44:19,257] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1647
[2019-04-07 13:44:19,292] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 57.0, 0.0, 0.0, 24.0, 23.35690521611665, -0.2066952609579135, 0.0, 1.0, 45530.71233544103], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2527200.0000, 
sim time next is 2529000.0000, 
raw observation next is [-2.55, 55.5, 0.0, 0.0, 24.0, 23.31847645605481, -0.2093982937844294, 0.0, 1.0, 45154.633708544134], 
processed observation next is [1.0, 0.2608695652173913, 0.3919667590027701, 0.555, 0.0, 0.0, 0.5, 0.44320637133790086, 0.4302005687385235, 0.0, 1.0, 0.2150220652787816], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.100684], dtype=float32), -0.91057354]. 
=============================================
[2019-04-07 13:44:19,318] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[98.96788]
 [98.23554]
 [97.758  ]
 [97.0786 ]
 [96.36331]], R is [[99.18008423]
 [99.18828583]
 [99.1964035 ]
 [99.20443726]
 [99.21239471]].
[2019-04-07 13:44:20,323] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:44:20,324] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:44:20,327] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run17
[2019-04-07 13:44:20,569] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44000, global step 707717: loss 0.1489
[2019-04-07 13:44:20,569] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44000, global step 707717: learning rate 0.0000
[2019-04-07 13:44:20,797] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44000, global step 707753: loss 0.1752
[2019-04-07 13:44:20,798] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44000, global step 707753: learning rate 0.0000
[2019-04-07 13:44:21,496] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44000, global step 707845: loss 0.1718
[2019-04-07 13:44:21,497] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44000, global step 707845: learning rate 0.0000
[2019-04-07 13:44:23,368] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44000, global step 708136: loss 0.1598
[2019-04-07 13:44:23,392] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44000, global step 708138: learning rate 0.0000
[2019-04-07 13:44:23,664] A3C_AGENT_WORKER-Thread-6 INFO:Local step 45000, global step 708185: loss 1.9232
[2019-04-07 13:44:23,665] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 45000, global step 708185: learning rate 0.0000
[2019-04-07 13:44:25,620] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44000, global step 708485: loss 0.1473
[2019-04-07 13:44:25,620] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44000, global step 708485: learning rate 0.0000
[2019-04-07 13:44:29,370] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6224462e-26 2.4665378e-25 1.6825110e-21 1.4298272e-11 3.3106492e-21
 1.0000000e+00 1.2149040e-13 1.9657320e-16], sum to 1.0000
[2019-04-07 13:44:29,370] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8015
[2019-04-07 13:44:29,437] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 52.0, 114.0, 800.0, 24.0, 24.14655081693791, 0.1739756233357644, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3331800.0000, 
sim time next is 3333600.0000, 
raw observation next is [-4.0, 50.0, 110.0, 776.0, 24.0, 24.68772445312408, 0.2315830748560349, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3518005540166205, 0.5, 0.36666666666666664, 0.8574585635359117, 0.5, 0.5573103710936733, 0.5771943582853449, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.284946], dtype=float32), 0.37242302]. 
=============================================
[2019-04-07 13:44:29,438] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45000, global step 709072: loss 2.1759
[2019-04-07 13:44:29,450] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45000, global step 709072: learning rate 0.0000
[2019-04-07 13:44:31,400] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44000, global step 709391: loss 0.1525
[2019-04-07 13:44:31,401] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44000, global step 709391: learning rate 0.0000
[2019-04-07 13:44:34,989] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44000, global step 709932: loss 0.1504
[2019-04-07 13:44:34,990] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44000, global step 709932: learning rate 0.0000
[2019-04-07 13:44:35,243] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44000, global step 709968: loss 0.1554
[2019-04-07 13:44:35,244] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44000, global step 709968: learning rate 0.0000
[2019-04-07 13:44:37,102] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44000, global step 710225: loss 0.1495
[2019-04-07 13:44:37,102] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44000, global step 710225: learning rate 0.0000
[2019-04-07 13:44:38,215] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44000, global step 710398: loss 0.1531
[2019-04-07 13:44:38,215] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44000, global step 710398: learning rate 0.0000
[2019-04-07 13:44:43,562] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44000, global step 711242: loss 0.1461
[2019-04-07 13:44:43,563] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44000, global step 711242: learning rate 0.0000
[2019-04-07 13:44:43,825] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44500, global step 711300: loss 0.7843
[2019-04-07 13:44:43,826] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44500, global step 711300: learning rate 0.0000
[2019-04-07 13:44:48,369] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:44:48,369] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:44:48,373] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run17
[2019-04-07 13:44:53,502] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.4539309e-29 1.2266124e-26 3.9484990e-25 9.4603280e-14 2.5422171e-23
 1.0000000e+00 1.2509982e-15 8.3843636e-17], sum to 1.0000
[2019-04-07 13:44:53,502] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4249
[2019-04-07 13:44:53,576] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 100.0, 1.0, 82.0, 24.0, 23.58079677281312, -0.1020134746109918, 1.0, 1.0, 6245.262562227436], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3137400.0000, 
sim time next is 3139200.0000, 
raw observation next is [6.0, 100.0, 42.0, 237.0, 24.0, 23.53033995452802, -0.08355902812721892, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6288088642659281, 1.0, 0.14, 0.261878453038674, 0.5, 0.46086166287733504, 0.47214699062426035, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39740577], dtype=float32), 0.7756088]. 
=============================================
[2019-04-07 13:44:54,085] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:44:54,085] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:44:54,088] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run17
[2019-04-07 13:45:05,373] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6733439e-30 2.4741849e-29 6.1595443e-26 7.0340538e-16 3.1399719e-24
 1.0000000e+00 2.6093326e-16 1.8101348e-18], sum to 1.0000
[2019-04-07 13:45:05,374] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0494
[2019-04-07 13:45:05,403] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 101.0, 763.0, 24.0, 25.30525577612528, 0.4485193313660427, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3162600.0000, 
sim time next is 3164400.0000, 
raw observation next is [7.0, 100.0, 92.5, 721.0, 24.0, 25.70356070881084, 0.5139779707682876, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6565096952908588, 1.0, 0.30833333333333335, 0.7966850828729282, 0.5, 0.6419633924009034, 0.6713259902560959, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39727232], dtype=float32), 0.28171864]. 
=============================================
[2019-04-07 13:45:06,261] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44500, global step 715168: loss 0.5565
[2019-04-07 13:45:06,261] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44500, global step 715168: learning rate 0.0000
[2019-04-07 13:45:07,011] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44500, global step 715316: loss 0.4744
[2019-04-07 13:45:07,020] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44500, global step 715316: learning rate 0.0000
[2019-04-07 13:45:07,041] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44500, global step 715321: loss 0.4081
[2019-04-07 13:45:07,041] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44500, global step 715321: learning rate 0.0000
[2019-04-07 13:45:07,086] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44500, global step 715326: loss 0.5461
[2019-04-07 13:45:07,087] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44500, global step 715326: learning rate 0.0000
[2019-04-07 13:45:09,581] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44500, global step 715882: loss 0.5964
[2019-04-07 13:45:09,582] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44500, global step 715882: learning rate 0.0000
[2019-04-07 13:45:13,863] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44500, global step 716756: loss 0.5500
[2019-04-07 13:45:13,864] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44500, global step 716756: learning rate 0.0000
[2019-04-07 13:45:16,539] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44500, global step 717276: loss 0.5834
[2019-04-07 13:45:16,547] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44500, global step 717276: learning rate 0.0000
[2019-04-07 13:45:19,164] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44500, global step 717771: loss 0.5386
[2019-04-07 13:45:19,165] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44500, global step 717771: learning rate 0.0000
[2019-04-07 13:45:19,881] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44500, global step 717917: loss 0.6062
[2019-04-07 13:45:19,883] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44500, global step 717917: learning rate 0.0000
[2019-04-07 13:45:21,468] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45000, global step 718240: loss 1.7613
[2019-04-07 13:45:21,469] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45000, global step 718240: learning rate 0.0000
[2019-04-07 13:45:21,665] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44500, global step 718275: loss 0.5914
[2019-04-07 13:45:21,666] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44500, global step 718275: learning rate 0.0000
[2019-04-07 13:45:24,697] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44500, global step 718829: loss 0.5264
[2019-04-07 13:45:24,698] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44500, global step 718829: learning rate 0.0000
[2019-04-07 13:45:29,965] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-07 13:45:29,966] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:45:29,966] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:45:29,966] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:45:29,966] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 13:45:29,966] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:45:29,967] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:45:29,970] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run37
[2019-04-07 13:45:29,989] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run37
[2019-04-07 13:45:29,994] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run37
[2019-04-07 13:46:44,222] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11220075], dtype=float32), 0.13446169]
[2019-04-07 13:46:44,222] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [4.9, 70.0, 25.0, 225.0, 24.0, 24.04962871646204, 0.1647882618708621, 1.0, 1.0, 0.0]
[2019-04-07 13:46:44,222] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 13:46:44,224] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [1.2976629e-26 1.8569675e-25 4.4877268e-23 8.3687824e-14 6.6465921e-22
 1.0000000e+00 1.2311342e-14 4.7521312e-16], sampled 0.23883176340929646
[2019-04-07 13:47:56,340] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11220075], dtype=float32), 0.13446169]
[2019-04-07 13:47:56,341] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [5.0, 83.0, 20.5, 114.0, 24.0, 24.93631252618598, 0.2973688024868282, 1.0, 1.0, 0.0]
[2019-04-07 13:47:56,341] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 13:47:56,341] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [9.5586580e-28 1.5012496e-26 4.7770671e-24 2.2344170e-14 7.4215403e-23
 1.0000000e+00 3.1044228e-15 9.9350897e-17], sampled 0.38831597417946573
[2019-04-07 13:47:57,316] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 13:48:11,426] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 13:48:17,298] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 13:48:18,320] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 720000, evaluation results [720000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 13:48:30,008] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45000, global step 722446: loss 1.8364
[2019-04-07 13:48:30,008] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45000, global step 722446: learning rate 0.0000
[2019-04-07 13:48:31,035] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45000, global step 722675: loss 1.7886
[2019-04-07 13:48:31,037] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45000, global step 722675: learning rate 0.0000
[2019-04-07 13:48:31,403] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45000, global step 722762: loss 2.1920
[2019-04-07 13:48:31,404] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45000, global step 722762: learning rate 0.0000
[2019-04-07 13:48:32,256] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45000, global step 722952: loss 1.9904
[2019-04-07 13:48:32,257] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45000, global step 722952: learning rate 0.0000
[2019-04-07 13:48:33,384] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.7470246e-30 2.2803870e-29 3.9767794e-26 3.4924455e-16 7.2453793e-26
 1.0000000e+00 6.4356299e-17 9.1460275e-19], sum to 1.0000
[2019-04-07 13:48:33,384] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0760
[2019-04-07 13:48:33,438] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 24.0, 23.49652608631182, 0.06485692652543042, 0.0, 1.0, 11396.294387386455], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1380600.0000, 
sim time next is 1382400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 24.0, 23.43672946815675, 0.06657635145173278, 0.0, 1.0, 56641.03253506609], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.5, 0.4530607890130624, 0.5221921171505776, 0.0, 1.0, 0.2697192025479338], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07072804], dtype=float32), -0.22225763]. 
=============================================
[2019-04-07 13:48:33,493] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:48:33,493] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:48:33,499] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run17
[2019-04-07 13:48:34,205] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45000, global step 723344: loss 1.9058
[2019-04-07 13:48:34,206] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45000, global step 723344: learning rate 0.0000
[2019-04-07 13:48:37,505] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45000, global step 724010: loss 1.9371
[2019-04-07 13:48:37,505] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45000, global step 724010: learning rate 0.0000
[2019-04-07 13:48:42,356] A3C_AGENT_WORKER-Thread-11 INFO:Local step 45000, global step 724955: loss 2.0869
[2019-04-07 13:48:42,360] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 45000, global step 724956: learning rate 0.0000
[2019-04-07 13:48:43,807] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5025085e-28 3.2381207e-27 1.4876992e-25 2.1188341e-15 3.7666208e-25
 1.0000000e+00 4.2294234e-16 2.6387287e-18], sum to 1.0000
[2019-04-07 13:48:43,807] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2539
[2019-04-07 13:48:43,872] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 24.0, 23.75133747826289, 0.06179791958475173, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4433400.0000, 
sim time next is 4435200.0000, 
raw observation next is [2.0, 80.0, 60.0, 116.0, 24.0, 23.69918060336527, 0.05030024471767986, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.518005540166205, 0.8, 0.2, 0.1281767955801105, 0.5, 0.4749317169471059, 0.5167667482392266, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8613009], dtype=float32), 1.938104]. 
=============================================
[2019-04-07 13:48:43,995] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4301709e-26 1.1011448e-25 2.3586225e-23 8.3743708e-14 1.4398417e-21
 1.0000000e+00 4.7210559e-15 6.1680172e-16], sum to 1.0000
[2019-04-07 13:48:43,995] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9323
[2019-04-07 13:48:44,110] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45000, global step 725309: loss 1.8592
[2019-04-07 13:48:44,119] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45000, global step 725310: learning rate 0.0000
[2019-04-07 13:48:44,146] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4, 72.5, 111.0, 66.0, 24.0, 23.73709916165909, 0.04007817017201808, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4523400.0000, 
sim time next is 4525200.0000, 
raw observation next is [0.0, 72.0, 117.0, 33.0, 24.0, 24.26097911048069, 0.09762764891329412, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.72, 0.39, 0.036464088397790057, 0.5, 0.5217482592067242, 0.5325425496377647, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12704128], dtype=float32), 1.2209833]. 
=============================================
[2019-04-07 13:48:44,505] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45000, global step 725392: loss 1.9230
[2019-04-07 13:48:44,505] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45000, global step 725392: learning rate 0.0000
[2019-04-07 13:48:47,705] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45000, global step 725990: loss 2.2249
[2019-04-07 13:48:47,706] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45000, global step 725990: learning rate 0.0000
[2019-04-07 13:48:48,084] A3C_AGENT_WORKER-Thread-13 INFO:Local step 45000, global step 726062: loss 1.8358
[2019-04-07 13:48:48,085] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 45000, global step 726062: learning rate 0.0000
[2019-04-07 13:48:53,633] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:48:53,633] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:48:53,638] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run17
[2019-04-07 13:48:55,205] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:48:55,205] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:48:55,209] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run17
[2019-04-07 13:48:55,589] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:48:55,589] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:48:55,617] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run17
[2019-04-07 13:48:57,283] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:48:57,283] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:48:57,287] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run17
[2019-04-07 13:48:58,029] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:48:58,029] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:48:58,033] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run17
[2019-04-07 13:49:01,625] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:49:01,625] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:49:01,629] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run17
[2019-04-07 13:49:06,265] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:49:06,265] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:49:06,274] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run17
[2019-04-07 13:49:06,369] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2211453e-26 1.8924097e-24 7.7315014e-23 2.0726332e-13 1.0606729e-21
 1.0000000e+00 2.7333032e-14 8.8777053e-17], sum to 1.0000
[2019-04-07 13:49:06,369] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3524
[2019-04-07 13:49:06,430] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 24.0, 23.5167866987965, -0.05582574656619604, 0.0, 1.0, 33357.715582477445], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5036400.0000, 
sim time next is 5038200.0000, 
raw observation next is [-2.5, 65.0, 59.0, 101.0, 24.0, 23.43946247889821, -0.05472353358722652, 1.0, 1.0, 6413.791113272273], 
processed observation next is [1.0, 0.30434782608695654, 0.39335180055401664, 0.65, 0.19666666666666666, 0.11160220994475138, 0.5, 0.45328853990818424, 0.4817588221375912, 1.0, 1.0, 0.030541862444153682], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.411475], dtype=float32), -0.845283]. 
=============================================
[2019-04-07 13:49:08,637] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:49:08,637] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:49:08,649] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run17
[2019-04-07 13:49:09,463] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2404684e-27 1.4416408e-26 8.8373938e-25 3.2295747e-14 4.1364490e-23
 1.0000000e+00 6.4924491e-15 9.1728010e-17], sum to 1.0000
[2019-04-07 13:49:09,463] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5107
[2019-04-07 13:49:09,542] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.0, 26.0, 53.0, 472.5, 24.0, 26.39196005735602, 0.6136894331029074, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4986000.0000, 
sim time next is 4987800.0000, 
raw observation next is [7.0, 25.5, 34.0, 304.0, 24.0, 25.99657253132318, 0.5060970312919008, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6565096952908588, 0.255, 0.11333333333333333, 0.33591160220994476, 0.5, 0.6663810442769318, 0.6686990104306335, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3143], dtype=float32), 0.109109156]. 
=============================================
[2019-04-07 13:49:09,567] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.0715495e-26 6.0413098e-25 1.6737902e-23 3.0311471e-13 3.1080154e-21
 1.0000000e+00 7.9542289e-15 1.3392152e-15], sum to 1.0000
[2019-04-07 13:49:09,567] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0245
[2019-04-07 13:49:09,575] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:49:09,575] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:49:09,578] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run17
[2019-04-07 13:49:09,682] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 67.5, 45.0, 0.0, 24.0, 23.88477838469396, -0.06940095662072605, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 120600.0000, 
sim time next is 122400.0000, 
raw observation next is [-7.8, 74.0, 117.5, 18.0, 24.0, 23.82904192824545, -0.08950276894946241, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.74, 0.39166666666666666, 0.019889502762430938, 0.5, 0.48575349402045404, 0.47016574368351255, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.92474043], dtype=float32), -0.6321647]. 
=============================================
[2019-04-07 13:49:12,891] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:49:12,891] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:49:12,901] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run17
[2019-04-07 13:49:14,197] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:49:14,197] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:49:14,200] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run17
[2019-04-07 13:49:14,700] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0757320e-27 3.3778827e-26 3.0205241e-23 5.5512523e-15 3.0150804e-22
 1.0000000e+00 4.1413131e-15 2.6685401e-18], sum to 1.0000
[2019-04-07 13:49:14,700] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1360
[2019-04-07 13:49:14,910] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 62.5, 61.0, 45.0, 24.0, 23.98636164911574, 0.0461763761610753, 1.0, 1.0, 85131.26064295774], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 142200.0000, 
sim time next is 144000.0000, 
raw observation next is [-6.7, 64.0, 44.0, 24.0, 24.0, 24.31283175702432, 0.06255596385217267, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.2770083102493075, 0.64, 0.14666666666666667, 0.026519337016574586, 0.5, 0.52606931308536, 0.5208519879507242, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42567143], dtype=float32), 2.5631573]. 
=============================================
[2019-04-07 13:49:14,931] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[ 99.248116]
 [ 98.501564]
 [ 98.05497 ]
 [ 99.41594 ]
 [100.369125]], R is [[99.14144897]
 [99.03035736]
 [99.01403046]
 [99.02388763]
 [99.03365326]].
[2019-04-07 13:50:12,787] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9141130e-25 1.3333890e-22 5.6560833e-21 1.3886896e-13 8.1235049e-21
 1.0000000e+00 1.6709789e-13 4.0727398e-15], sum to 1.0000
[2019-04-07 13:50:12,788] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6282
[2019-04-07 13:50:12,843] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 56.0, 474.0, 24.0, 23.32794733765231, -0.03341957962355759, 0.0, 1.0, 18691.078768987965], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2997000.0000, 
sim time next is 2998800.0000, 
raw observation next is [-1.0, 55.0, 31.0, 286.5, 24.0, 23.29919853190098, -0.06186572792643539, 0.0, 1.0, 18690.549946119885], 
processed observation next is [0.0, 0.7391304347826086, 0.4349030470914128, 0.55, 0.10333333333333333, 0.3165745856353591, 0.5, 0.4415998776584151, 0.4793780906911882, 0.0, 1.0, 0.08900261879104707], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06366494], dtype=float32), 1.0843245]. 
=============================================
[2019-04-07 13:50:21,352] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.4837401e-29 2.0160345e-25 2.2712240e-25 4.8538351e-15 5.6306501e-24
 1.0000000e+00 1.9783173e-16 2.8813650e-18], sum to 1.0000
[2019-04-07 13:50:21,355] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6077
[2019-04-07 13:50:21,412] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 96.0, 0.0, 0.0, 24.0, 23.55732925641994, 0.1535636735095673, 0.0, 1.0, 46226.44643479217], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1296000.0000, 
sim time next is 1297800.0000, 
raw observation next is [4.1, 94.5, 0.0, 0.0, 24.0, 23.60881308049898, 0.1613592480693193, 0.0, 1.0, 31494.300110609285], 
processed observation next is [1.0, 0.0, 0.5761772853185596, 0.945, 0.0, 0.0, 0.5, 0.46740109004158165, 0.5537864160231064, 0.0, 1.0, 0.14997285766956803], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3216673], dtype=float32), -2.7076797]. 
=============================================
[2019-04-07 13:50:22,995] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9909413e-27 1.7784588e-25 3.5616936e-24 4.5504101e-14 1.7190301e-21
 1.0000000e+00 1.2607432e-14 4.9245338e-17], sum to 1.0000
[2019-04-07 13:50:22,995] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7269
[2019-04-07 13:50:23,038] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.1, 98.0, 101.0, 0.0, 24.0, 23.26018680537511, 0.08946227154787693, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1254600.0000, 
sim time next is 1256400.0000, 
raw observation next is [13.8, 100.0, 98.0, 0.0, 24.0, 23.24493069255851, 0.08971445133765875, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.32666666666666666, 0.0, 0.5, 0.43707755771320905, 0.5299048171125529, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2656007], dtype=float32), 0.8555365]. 
=============================================
[2019-04-07 13:50:27,794] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-07 13:50:27,795] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:50:27,795] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:50:27,797] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run38
[2019-04-07 13:50:27,875] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:50:27,875] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:50:27,878] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run38
[2019-04-07 13:50:27,941] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 13:50:27,942] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:50:27,958] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run38
[2019-04-07 13:52:42,820] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11249055], dtype=float32), 0.13533114]
[2019-04-07 13:52:42,820] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [2.0, 100.0, 78.0, 27.0, 24.0, 23.75648365712848, -0.1254735339879913, 1.0, 1.0, 12453.607780153694]
[2019-04-07 13:52:42,820] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 13:52:42,821] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [1.57809785e-27 2.13841564e-26 7.00630836e-24 1.89029090e-14
 9.82062344e-23 1.00000000e+00 2.78171464e-15 9.23871607e-17], sampled 0.09939540871265862
[2019-04-07 13:52:58,087] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 13:53:15,242] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 13:53:18,994] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 13:53:20,016] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 740000, evaluation results [740000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 13:53:21,265] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9763193e-30 1.9328898e-29 1.5405250e-26 1.0822193e-15 1.3900080e-26
 1.0000000e+00 3.1651673e-16 1.4540563e-19], sum to 1.0000
[2019-04-07 13:53:21,266] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6697
[2019-04-07 13:53:21,361] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 73.5, 0.0, 24.0, 24.28204161477503, 0.1636206047246589, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1332000.0000, 
sim time next is 1333800.0000, 
raw observation next is [0.8, 92.0, 102.0, 0.0, 24.0, 24.35653691876969, 0.1739832363859483, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4847645429362882, 0.92, 0.34, 0.0, 0.5, 0.5297114098974743, 0.5579944121286494, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43879694], dtype=float32), -1.3433905]. 
=============================================
[2019-04-07 13:53:27,551] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2703654e-28 7.5422190e-28 2.6495018e-25 1.3504295e-15 5.7751600e-24
 1.0000000e+00 4.3568768e-16 2.0301436e-16], sum to 1.0000
[2019-04-07 13:53:27,551] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8622
[2019-04-07 13:53:27,592] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 85.5, 0.0, 0.0, 24.0, 23.726650974542, 0.06233304257661761, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1567800.0000, 
sim time next is 1569600.0000, 
raw observation next is [4.6, 85.0, 0.0, 0.0, 24.0, 23.66391514447429, 0.08729229950318516, 0.0, 1.0, 63337.63785253605], 
processed observation next is [1.0, 0.17391304347826086, 0.590027700831025, 0.85, 0.0, 0.0, 0.5, 0.471992928706191, 0.5290974331677284, 0.0, 1.0, 0.3016077992977907], 
reward next is 0.9841, 
noisyNet noise sample is [array([1.4238749], dtype=float32), -1.2221797]. 
=============================================
[2019-04-07 13:53:40,391] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1921472e-27 4.6348914e-27 2.4709453e-25 7.2187739e-14 4.7231803e-23
 1.0000000e+00 2.0428307e-15 8.5015314e-17], sum to 1.0000
[2019-04-07 13:53:40,391] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1785
[2019-04-07 13:53:40,439] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 76.0, 0.0, 0.0, 24.0, 23.65428377819058, 0.2285644160847062, 0.0, 1.0, 147120.98559588054], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1630800.0000, 
sim time next is 1632600.0000, 
raw observation next is [6.9, 81.0, 0.0, 0.0, 24.0, 23.95073393357181, 0.2497629406829608, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6537396121883658, 0.81, 0.0, 0.0, 0.5, 0.4958944944643176, 0.583254313560987, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8980547], dtype=float32), 1.6624383]. 
=============================================
[2019-04-07 13:53:46,596] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9325580e-26 4.1550609e-24 1.0213902e-21 2.1326162e-14 4.2827766e-22
 1.0000000e+00 7.1522454e-15 1.0707874e-14], sum to 1.0000
[2019-04-07 13:53:46,597] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5803
[2019-04-07 13:53:46,667] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 81.0, 0.0, 0.0, 24.0, 23.73774225785608, 0.1479439651291026, 0.0, 1.0, 23595.139203521274], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3272400.0000, 
sim time next is 3274200.0000, 
raw observation next is [-5.5, 86.5, 0.0, 0.0, 24.0, 23.65393525557087, 0.1104616392845608, 0.0, 1.0, 16671.73812439156], 
processed observation next is [1.0, 0.9130434782608695, 0.3102493074792244, 0.865, 0.0, 0.0, 0.5, 0.4711612712975726, 0.5368205464281869, 0.0, 1.0, 0.07938922916376932], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.66016376], dtype=float32), -0.7121286]. 
=============================================
[2019-04-07 13:53:49,169] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2132065e-25 2.2684975e-24 5.9906935e-22 2.1305295e-15 8.0620666e-22
 1.0000000e+00 2.0151422e-14 4.6148054e-16], sum to 1.0000
[2019-04-07 13:53:49,169] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6067
[2019-04-07 13:53:49,481] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 24.0, 22.06122896975347, -0.452561051099803, 0.0, 1.0, 45320.17520198002], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1926000.0000, 
sim time next is 1927800.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 24.0, 22.58097063486317, -0.2453294219875967, 1.0, 1.0, 150367.40320334263], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.5, 0.38174755290526424, 0.4182235260041344, 1.0, 1.0, 0.7160352533492507], 
reward next is 0.5697, 
noisyNet noise sample is [array([0.39175746], dtype=float32), 1.5395848]. 
=============================================
[2019-04-07 13:54:23,501] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:54:23,501] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:54:23,509] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run18
[2019-04-07 13:54:37,579] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:54:37,579] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:54:37,583] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run18
[2019-04-07 13:54:58,600] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:54:58,600] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:54:58,604] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run18
[2019-04-07 13:55:03,873] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.4366740e-25 2.4630038e-24 1.1423541e-22 9.0761784e-13 7.1165512e-19
 1.0000000e+00 8.6627428e-14 2.3769407e-15], sum to 1.0000
[2019-04-07 13:55:03,874] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9603
[2019-04-07 13:55:03,920] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 50.0, 149.5, 635.5, 24.0, 24.58014619621733, 0.1193903769118063, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2804400.0000, 
sim time next is 2806200.0000, 
raw observation next is [0.5, 47.0, 125.0, 763.0, 24.0, 24.23648215488494, 0.05517709681664865, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4764542936288089, 0.47, 0.4166666666666667, 0.8430939226519337, 0.5, 0.5197068462404116, 0.5183923656055496, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51056945], dtype=float32), 1.2747029]. 
=============================================
[2019-04-07 13:55:08,409] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:55:08,409] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:55:08,415] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run18
[2019-04-07 13:55:12,600] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3500422e-24 3.6435550e-25 8.3439163e-22 2.0565225e-14 6.3502843e-22
 1.0000000e+00 6.4148062e-14 4.4543822e-16], sum to 1.0000
[2019-04-07 13:55:12,600] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0889
[2019-04-07 13:55:12,675] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 24.0, 23.31095150600218, -0.07597871312864793, 0.0, 1.0, 58042.30496580177], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3376800.0000, 
sim time next is 3378600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 24.0, 23.36624137152024, -0.07089552089262756, 0.0, 1.0, 43330.70380132997], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 0.5, 0.4471867809600199, 0.47636815970245744, 0.0, 1.0, 0.20633668476823794], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5321369], dtype=float32), 0.16377072]. 
=============================================
[2019-04-07 13:55:14,710] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.5663469e-32 3.6693637e-30 5.3731828e-28 1.8716191e-16 2.0622704e-24
 1.0000000e+00 1.5984633e-17 4.3095784e-19], sum to 1.0000
[2019-04-07 13:55:14,710] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5645
[2019-04-07 13:55:14,759] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.3, 99.5, 58.0, 499.0, 24.0, 25.95363038527099, 0.565171530175771, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3169800.0000, 
sim time next is 3171600.0000, 
raw observation next is [6.0, 100.0, 33.0, 307.5, 24.0, 26.12751205998503, 0.5678615355154385, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.11, 0.3397790055248619, 0.5, 0.6772926716654192, 0.6892871785051461, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5481057], dtype=float32), 2.295286]. 
=============================================
[2019-04-07 13:55:18,501] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.0460691e-28 2.8440188e-25 3.7537702e-24 2.3374857e-14 6.0688229e-24
 1.0000000e+00 6.3019700e-15 5.4970506e-16], sum to 1.0000
[2019-04-07 13:55:18,501] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0975
[2019-04-07 13:55:18,638] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 75.0, 0.0, 24.0, 22.5255920540464, -0.2784979184862819, 0.0, 1.0, 39763.7904082284], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 41400.0000, 
sim time next is 43200.0000, 
raw observation next is [7.7, 93.0, 85.5, 0.0, 24.0, 22.55183898852694, -0.2529608295538143, 0.0, 1.0, 36045.414142885835], 
processed observation next is [0.0, 0.5217391304347826, 0.6759002770083103, 0.93, 0.285, 0.0, 0.5, 0.3793199157105782, 0.4156797234820619, 0.0, 1.0, 0.1716448292518373], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.93462694], dtype=float32), -0.096280545]. 
=============================================
[2019-04-07 13:55:20,687] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-07 13:55:20,697] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:55:20,697] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:55:20,699] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run39
[2019-04-07 13:55:20,723] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 13:55:20,725] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:55:20,726] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 13:55:20,726] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:55:20,730] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run39
[2019-04-07 13:55:20,743] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run39
[2019-04-07 13:57:43,423] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 13:58:00,945] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 13:58:04,727] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 13:58:05,750] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 760000, evaluation results [760000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 13:58:45,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.3358852e-24 2.0041885e-24 3.4733710e-21 3.1044554e-13 7.2113754e-20
 1.0000000e+00 6.6347508e-14 6.6989756e-15], sum to 1.0000
[2019-04-07 13:58:45,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2865
[2019-04-07 13:58:45,187] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 58.0, 93.0, 444.0, 24.0, 24.28998441257087, 0.07592843829721262, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4005000.0000, 
sim time next is 4006800.0000, 
raw observation next is [-11.0, 53.0, 97.0, 571.0, 24.0, 24.7355146728252, 0.1344055840244933, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.15789473684210528, 0.53, 0.3233333333333333, 0.630939226519337, 0.5, 0.5612928894021, 0.5448018613414978, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22357358], dtype=float32), 0.85563415]. 
=============================================
[2019-04-07 13:58:45,218] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.6584460e-29 4.2964454e-27 1.2278021e-25 1.4741642e-13 1.5377671e-24
 1.0000000e+00 9.0622742e-16 3.1985060e-18], sum to 1.0000
[2019-04-07 13:58:45,218] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3519
[2019-04-07 13:58:45,249] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.0, 50.0, 0.0, 0.0, 24.0, 26.15312139529736, 0.6023411553991692, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4388400.0000, 
sim time next is 4390200.0000, 
raw observation next is [11.5, 54.0, 0.0, 0.0, 24.0, 25.89780229871617, 0.5580405285652975, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7811634349030472, 0.54, 0.0, 0.0, 0.5, 0.6581501915596807, 0.6860135095217658, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.611197], dtype=float32), -0.46619853]. 
=============================================
[2019-04-07 13:58:45,471] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:58:45,472] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:58:45,475] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run18
[2019-04-07 13:58:50,294] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8790342e-24 2.0107064e-23 1.2598254e-21 4.9112895e-13 8.2949203e-21
 1.0000000e+00 1.6194657e-14 6.0176727e-15], sum to 1.0000
[2019-04-07 13:58:50,295] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8642
[2019-04-07 13:58:50,356] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 35.0, 109.0, 724.0, 24.0, 24.76229618361241, 0.1579385326377241, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4096800.0000, 
sim time next is 4098600.0000, 
raw observation next is [-1.5, 33.5, 114.0, 769.0, 24.0, 24.89686373556102, 0.1952630057724516, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4210526315789474, 0.335, 0.38, 0.8497237569060774, 0.5, 0.574738644630085, 0.5650876685908172, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2217535], dtype=float32), 1.2627304]. 
=============================================
[2019-04-07 13:58:54,356] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2287114e-27 2.8557789e-27 8.0090287e-24 2.3594240e-15 3.8640900e-23
 1.0000000e+00 8.5605364e-16 7.0834104e-16], sum to 1.0000
[2019-04-07 13:58:54,356] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4216
[2019-04-07 13:58:54,404] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.95, 70.5, 0.0, 0.0, 24.0, 23.77683643165969, -0.02784272407932454, 0.0, 1.0, 13925.138591911762], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4332600.0000, 
sim time next is 4334400.0000, 
raw observation next is [3.9, 70.0, 0.0, 0.0, 24.0, 23.62430783908896, -0.03436933952065236, 0.0, 1.0, 22520.66090404839], 
processed observation next is [1.0, 0.17391304347826086, 0.5706371191135734, 0.7, 0.0, 0.0, 0.5, 0.46869231992408, 0.4885435534931159, 0.0, 1.0, 0.10724124240023042], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9053461], dtype=float32), -1.5374789]. 
=============================================
[2019-04-07 13:59:03,306] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2983897e-25 1.5037107e-23 4.8165595e-22 8.0651333e-13 1.6098051e-21
 1.0000000e+00 3.1701445e-13 1.3487415e-14], sum to 1.0000
[2019-04-07 13:59:03,306] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9390
[2019-04-07 13:59:03,404] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 24.0, 23.09112701207183, -0.1409866267338252, 0.0, 1.0, 51241.54797238888], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 162000.0000, 
sim time next is 163800.0000, 
raw observation next is [-8.4, 69.5, 0.0, 0.0, 24.0, 22.97169766204649, -0.1681505151701038, 0.0, 1.0, 47867.40296404795], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.695, 0.0, 0.0, 0.5, 0.41430813850387427, 0.4439498282766321, 0.0, 1.0, 0.22794001411451403], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07311424], dtype=float32), -0.24006099]. 
=============================================
[2019-04-07 13:59:10,227] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:59:10,227] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:59:10,231] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run18
[2019-04-07 13:59:10,713] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:59:10,713] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:59:10,717] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run18
[2019-04-07 13:59:11,609] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:59:11,609] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:59:11,616] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run18
[2019-04-07 13:59:11,764] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:59:11,765] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:59:11,769] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run18
[2019-04-07 13:59:13,743] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4445041e-27 1.6084567e-26 7.3363607e-24 6.3007230e-14 9.5059612e-22
 1.0000000e+00 3.2786379e-14 3.1084738e-16], sum to 1.0000
[2019-04-07 13:59:13,744] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9931
[2019-04-07 13:59:13,821] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.3, 88.5, 0.0, 0.0, 24.0, 23.08757610523422, -0.1799898961783381, 0.0, 1.0, 44366.29697888405], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2086200.0000, 
sim time next is 2088000.0000, 
raw observation next is [-5.6, 91.0, 0.0, 0.0, 24.0, 23.06206772599649, -0.1672994363810815, 0.0, 1.0, 44654.81668902221], 
processed observation next is [1.0, 0.17391304347826086, 0.30747922437673136, 0.91, 0.0, 0.0, 0.5, 0.42183897716637403, 0.44423352120630616, 0.0, 1.0, 0.2126419842334391], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0635319], dtype=float32), 0.04410709]. 
=============================================
[2019-04-07 13:59:13,825] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[104.60448 ]
 [104.84689 ]
 [104.945114]
 [104.93299 ]
 [104.745766]], R is [[104.42314911]
 [104.37892151]
 [104.33513641]
 [104.29178619]
 [104.24887085]].
[2019-04-07 13:59:14,432] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:59:14,432] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:59:14,436] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run18
[2019-04-07 13:59:16,187] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.05740405e-22 1.11579425e-21 1.80944350e-19 5.97360876e-12
 3.38688792e-19 1.00000000e+00 1.12337648e-11 4.92683388e-14], sum to 1.0000
[2019-04-07 13:59:16,187] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3410
[2019-04-07 13:59:16,252] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 39.0, 0.0, 0.0, 24.0, 23.54498527398878, -0.08728223286750718, 0.0, 1.0, 34854.655260899635], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4921200.0000, 
sim time next is 4923000.0000, 
raw observation next is [0.5, 39.5, 0.0, 0.0, 24.0, 23.49487263920319, -0.08785416010481796, 0.0, 1.0, 49186.78561259238], 
processed observation next is [0.0, 1.0, 0.4764542936288089, 0.395, 0.0, 0.0, 0.5, 0.45790605326693257, 0.4707152799650607, 0.0, 1.0, 0.2342227886313923], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.072709], dtype=float32), -0.9187762]. 
=============================================
[2019-04-07 13:59:16,256] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[90.0492  ]
 [90.50483 ]
 [91.08586 ]
 [91.74974 ]
 [92.373314]], R is [[90.26209259]
 [90.35947418]
 [90.45587921]
 [90.55132294]
 [90.64581299]].
[2019-04-07 13:59:22,753] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:59:22,753] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:59:22,757] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run18
[2019-04-07 13:59:24,420] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:59:24,421] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:59:24,424] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run18
[2019-04-07 13:59:26,184] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:59:26,184] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:59:26,187] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run18
[2019-04-07 13:59:26,495] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:59:26,495] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:59:26,499] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run18
[2019-04-07 13:59:27,397] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4195061e-27 1.5700174e-25 5.0281608e-24 1.5672286e-14 2.2478689e-22
 1.0000000e+00 2.2278836e-15 6.8493857e-17], sum to 1.0000
[2019-04-07 13:59:27,397] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7790
[2019-04-07 13:59:27,442] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.75, 86.0, 0.0, 0.0, 24.0, 23.20956209426955, -0.1476207986671536, 0.0, 1.0, 43920.77737012369], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2082600.0000, 
sim time next is 2084400.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 24.0, 23.10302353289319, -0.1660350492539037, 0.0, 1.0, 44116.837007623144], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.86, 0.0, 0.0, 0.5, 0.42525196107443247, 0.4446549835820321, 0.0, 1.0, 0.21008017622677688], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7108432], dtype=float32), -0.092435025]. 
=============================================
[2019-04-07 13:59:33,709] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:59:33,709] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:59:33,712] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run18
[2019-04-07 13:59:33,789] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 13:59:33,789] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 13:59:33,792] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run18
[2019-04-07 14:00:00,419] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9570890e-23 7.2059780e-22 4.9171469e-21 1.7793256e-12 1.8626986e-19
 1.0000000e+00 4.5903785e-14 1.6407133e-14], sum to 1.0000
[2019-04-07 14:00:00,419] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4702
[2019-04-07 14:00:00,587] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 26.5, 122.0, 0.0, 24.0, 23.6081645991028, -0.2165146023662298, 1.0, 1.0, 33885.880025338935], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 473400.0000, 
sim time next is 475200.0000, 
raw observation next is [-1.7, 25.0, 125.5, 0.0, 24.0, 23.64480898887171, -0.2011077320009066, 1.0, 1.0, 20656.058867009648], 
processed observation next is [1.0, 0.5217391304347826, 0.4155124653739613, 0.25, 0.41833333333333333, 0.0, 0.5, 0.47040074907264245, 0.43296408933303115, 1.0, 1.0, 0.09836218508099832], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48911333], dtype=float32), 1.7385768]. 
=============================================
[2019-04-07 14:00:03,292] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7467520e-23 2.0200570e-21 1.6425345e-19 6.9421769e-13 1.2414362e-18
 1.0000000e+00 9.6035582e-13 5.0614217e-14], sum to 1.0000
[2019-04-07 14:00:03,292] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5003
[2019-04-07 14:00:03,402] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 24.0, 21.17522402702668, -0.6321945935293597, 0.0, 1.0, 50023.17126511858], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 363600.0000, 
sim time next is 365400.0000, 
raw observation next is [-15.9, 75.5, 0.0, 0.0, 24.0, 21.07635123378418, -0.657679550899894, 0.0, 1.0, 49392.51866252679], 
processed observation next is [1.0, 0.21739130434782608, 0.02216066481994457, 0.755, 0.0, 0.0, 0.5, 0.25636260281534834, 0.28077348303336863, 0.0, 1.0, 0.23520246982155613], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.79777867], dtype=float32), -0.36185032]. 
=============================================
[2019-04-07 14:00:10,188] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-07 14:00:10,188] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:00:10,188] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:00:10,190] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run40
[2019-04-07 14:00:10,210] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:00:10,212] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:00:10,214] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run40
[2019-04-07 14:00:10,231] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 14:00:10,234] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:00:10,236] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run40
[2019-04-07 14:02:36,970] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 14:02:53,033] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 14:02:57,165] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11240056], dtype=float32), 0.13629168]
[2019-04-07 14:02:57,165] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [8.0218707275, 20.23360849, 256.082284, 663.4296137, 24.0, 25.78824327560526, 0.5147548494818764, 1.0, 1.0, 0.0]
[2019-04-07 14:02:57,165] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 14:02:57,166] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [1.1541967e-25 1.7185554e-24 1.9802366e-22 1.9775281e-13 2.4082109e-21
 1.0000000e+00 2.9593708e-14 1.3894327e-15], sampled 0.9226125332012338
[2019-04-07 14:02:57,430] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 14:02:58,452] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 780000, evaluation results [780000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 14:03:04,138] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6106810e-24 4.8126061e-24 5.0851304e-21 2.0424519e-12 6.8435773e-20
 1.0000000e+00 1.6995801e-13 1.5205022e-15], sum to 1.0000
[2019-04-07 14:03:04,138] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9969
[2019-04-07 14:03:04,197] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 24.0, 23.16053909063046, -0.2087458999618189, 0.0, 1.0, 42644.40980501642], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 689400.0000, 
sim time next is 691200.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 24.0, 23.09852817569197, -0.2149365941226256, 0.0, 1.0, 42355.761860824576], 
processed observation next is [1.0, 0.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.5, 0.4248773479743309, 0.42835446862579146, 0.0, 1.0, 0.20169410409916463], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10546342], dtype=float32), -0.1482106]. 
=============================================
[2019-04-07 14:03:14,044] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.38301134e-25 3.85356136e-25 1.14205294e-23 1.26992872e-14
 1.48089634e-21 1.00000000e+00 9.48067567e-16 1.26666657e-16], sum to 1.0000
[2019-04-07 14:03:14,045] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6677
[2019-04-07 14:03:14,266] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 87.0, 110.5, 122.0, 24.0, 22.98650079792435, -0.1684653162117973, 0.0, 1.0, 32256.542208607007], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 554400.0000, 
sim time next is 556200.0000, 
raw observation next is [-0.6, 85.0, 77.0, 141.0, 24.0, 22.94582462241683, -0.1437807200793975, 0.0, 1.0, 56541.336746930436], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.85, 0.25666666666666665, 0.1558011049723757, 0.5, 0.4121520518680691, 0.4520730933068675, 0.0, 1.0, 0.26924446069966873], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48384628], dtype=float32), -0.5171772]. 
=============================================
[2019-04-07 14:03:16,448] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4215167e-26 2.8784216e-25 6.0883007e-24 8.0948533e-15 7.4590414e-21
 1.0000000e+00 1.1575890e-15 7.6450737e-15], sum to 1.0000
[2019-04-07 14:03:16,448] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6092
[2019-04-07 14:03:16,630] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.55, 87.0, 0.0, 0.0, 24.0, 23.08873152192992, -0.1577588831951436, 0.0, 1.0, 31708.787528857098], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 585000.0000, 
sim time next is 586800.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 24.0, 23.0447426051274, -0.1610066030139835, 0.0, 1.0, 50241.77019099947], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.5, 0.42039521709395, 0.4463311323286721, 0.0, 1.0, 0.23924652471904512], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04822752], dtype=float32), 1.1953988]. 
=============================================
[2019-04-07 14:03:19,458] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.6293699e-30 3.2000259e-29 6.1263628e-26 2.3364470e-14 1.1049160e-23
 1.0000000e+00 3.5096874e-16 8.0751332e-18], sum to 1.0000
[2019-04-07 14:03:19,460] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9040
[2019-04-07 14:03:19,505] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.7, 92.5, 27.0, 0.0, 24.0, 23.70317791488016, 0.03685987011209494, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 981000.0000, 
sim time next is 982800.0000, 
raw observation next is [10.0, 92.0, 43.5, 0.0, 24.0, 24.08100736262349, 0.09123930356604472, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.739612188365651, 0.92, 0.145, 0.0, 0.5, 0.5067506135519576, 0.5304131011886816, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.720949], dtype=float32), -0.97938305]. 
=============================================
[2019-04-07 14:03:31,957] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.22909765e-29 5.71952383e-29 1.03104120e-26 4.81256791e-17
 2.03863204e-25 1.00000000e+00 6.86471069e-18 1.63718527e-20], sum to 1.0000
[2019-04-07 14:03:31,958] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9709
[2019-04-07 14:03:31,971] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.5, 44.0, 0.0, 24.0, 24.04478170811883, 0.0979771579975421, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1351800.0000, 
sim time next is 1353600.0000, 
raw observation next is [1.1, 93.0, 31.0, 0.0, 24.0, 24.02802309863277, 0.09029376876507274, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.93, 0.10333333333333333, 0.0, 0.5, 0.5023352582193974, 0.5300979229216909, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.73003274], dtype=float32), 1.8196272]. 
=============================================
[2019-04-07 14:03:42,147] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3294221e-28 1.1749817e-27 6.3318694e-24 2.6235969e-15 4.9524189e-23
 1.0000000e+00 2.7465602e-15 1.1419389e-17], sum to 1.0000
[2019-04-07 14:03:42,147] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7267
[2019-04-07 14:03:42,205] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 84.0, 95.0, 0.0, 24.0, 23.98867372067635, 0.06827724507915041, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1684800.0000, 
sim time next is 1686600.0000, 
raw observation next is [1.1, 86.0, 107.0, 0.0, 24.0, 23.89504078958651, 0.03861936001006961, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.86, 0.3566666666666667, 0.0, 0.5, 0.49125339913220917, 0.5128731200033566, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11899092], dtype=float32), 0.7315141]. 
=============================================
[2019-04-07 14:03:44,254] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3298887e-30 1.3394969e-29 4.6024025e-27 6.5398944e-17 5.8389103e-25
 1.0000000e+00 5.0251135e-16 2.4294832e-18], sum to 1.0000
[2019-04-07 14:03:44,254] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1552
[2019-04-07 14:03:44,300] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.05, 97.0, 0.0, 0.0, 24.0, 23.6437400188055, 0.08326062355111519, 0.0, 1.0, 59329.60028476579], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1661400.0000, 
sim time next is 1663200.0000, 
raw observation next is [5.5, 97.0, 0.0, 0.0, 24.0, 23.58439315339641, 0.1124532484890368, 0.0, 1.0, 64043.60200299073], 
processed observation next is [1.0, 0.2608695652173913, 0.6149584487534627, 0.97, 0.0, 0.0, 0.5, 0.4653660961163674, 0.5374844161630122, 0.0, 1.0, 0.30496953334757493], 
reward next is 0.9807, 
noisyNet noise sample is [array([-0.839014], dtype=float32), -1.1933761]. 
=============================================
[2019-04-07 14:03:57,104] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3598645e-23 2.0015078e-23 2.2424572e-21 2.1357081e-12 3.5284339e-20
 1.0000000e+00 1.9885764e-13 1.6332198e-14], sum to 1.0000
[2019-04-07 14:03:57,105] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4733
[2019-04-07 14:03:57,157] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 24.0, 22.43404184747966, -0.3832651528763155, 0.0, 1.0, 45946.08331233886], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1920600.0000, 
sim time next is 1922400.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 24.0, 22.26834370621, -0.4191334817690786, 0.0, 1.0, 45729.42892574455], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.82, 0.0, 0.0, 0.5, 0.3556953088508334, 0.36028883941030715, 0.0, 1.0, 0.21775918536068833], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6916998], dtype=float32), -0.21619557]. 
=============================================
[2019-04-07 14:04:00,488] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.6958372e-23 1.1194395e-21 8.7639560e-21 1.3421752e-13 5.2439083e-21
 1.0000000e+00 6.2515027e-13 1.6676995e-13], sum to 1.0000
[2019-04-07 14:04:00,492] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3307
[2019-04-07 14:04:00,698] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.5, 70.0, 88.0, 425.0, 24.0, 23.28516169331595, -0.01135301936246198, 0.0, 1.0, 29903.4668461913], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3573000.0000, 
sim time next is 3574800.0000, 
raw observation next is [-6.0, 70.0, 94.0, 550.5, 24.0, 23.56989032116413, 0.004347794822492988, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.296398891966759, 0.7, 0.31333333333333335, 0.6082872928176796, 0.5, 0.46415752676367755, 0.501449264940831, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1002175], dtype=float32), 1.054765]. 
=============================================
[2019-04-07 14:04:07,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0355545e-26 1.7305994e-26 3.1820929e-23 8.6382412e-14 1.6477848e-22
 1.0000000e+00 3.3938268e-15 2.1799865e-17], sum to 1.0000
[2019-04-07 14:04:07,548] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6291
[2019-04-07 14:04:07,793] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.1, 86.5, 29.0, 0.0, 24.0, 23.40689115901016, -0.1549156306094019, 1.0, 1.0, 19400.124636486344], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2017800.0000, 
sim time next is 2019600.0000, 
raw observation next is [-6.0, 86.0, 49.0, 0.0, 24.0, 23.77114445843691, -0.1267620961020357, 1.0, 1.0, 6242.311616848667], 
processed observation next is [1.0, 0.391304347826087, 0.296398891966759, 0.86, 0.16333333333333333, 0.0, 0.5, 0.4809287048697426, 0.4577459679659881, 1.0, 1.0, 0.02972529341356508], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3785809], dtype=float32), -1.1274368]. 
=============================================
[2019-04-07 14:04:24,654] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4364433e-26 1.7317691e-25 1.9427485e-24 1.0910859e-15 2.1586548e-23
 1.0000000e+00 6.7540374e-16 1.8691931e-16], sum to 1.0000
[2019-04-07 14:04:24,654] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8697
[2019-04-07 14:04:24,754] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 24.0, 22.96461909966193, -0.1719392002098655, 0.0, 1.0, 45648.04400637979], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2246400.0000, 
sim time next is 2248200.0000, 
raw observation next is [-6.7, 76.5, 0.0, 0.0, 24.0, 22.94023324602855, -0.167155288977619, 0.0, 1.0, 45671.84358221727], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.765, 0.0, 0.0, 0.5, 0.41168610383571264, 0.4442815703407937, 0.0, 1.0, 0.21748496943912984], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.265353], dtype=float32), -0.39195797]. 
=============================================
[2019-04-07 14:04:34,736] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:04:34,737] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:04:34,743] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run19
[2019-04-07 14:04:46,189] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:04:46,189] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:04:46,193] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run19
[2019-04-07 14:04:58,787] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-07 14:04:58,791] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:04:58,792] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:04:58,794] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run41
[2019-04-07 14:04:58,815] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:04:58,818] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 14:04:58,818] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:04:58,821] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run41
[2019-04-07 14:04:58,840] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:04:58,845] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run41
[2019-04-07 14:07:13,655] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.1125303], dtype=float32), 0.13698532]
[2019-04-07 14:07:13,655] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-2.85321138, 95.439579725, 0.0, 0.0, 24.0, 23.47550357631269, -0.0106001696167282, 0.0, 1.0, 27426.486179333206]
[2019-04-07 14:07:13,655] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 14:07:13,656] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [1.3287555e-26 1.8412307e-25 5.2183007e-23 4.5751695e-14 3.6284282e-22
 1.0000000e+00 8.4367319e-15 2.0270172e-16], sampled 0.33222794015621426
[2019-04-07 14:07:25,534] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 14:07:43,138] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 14:07:46,889] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 14:07:47,916] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 800000, evaluation results [800000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 14:07:51,568] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7007069e-30 1.1971693e-27 6.1173482e-26 2.1483864e-15 3.8550043e-24
 1.0000000e+00 4.1448986e-16 2.8536126e-19], sum to 1.0000
[2019-04-07 14:07:51,568] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6551
[2019-04-07 14:07:51,680] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 96.5, 0.0, 0.0, 24.0, 23.26286400344668, -0.1096928121765715, 0.0, 1.0, 63666.98256688778], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2874600.0000, 
sim time next is 2876400.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 24.0, 23.45480626102453, -0.1070505083442991, 0.0, 1.0, 29429.97451867085], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.0, 0.0, 0.5, 0.4545671884187108, 0.46431649721856694, 0.0, 1.0, 0.14014273580319453], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5529659], dtype=float32), -0.8891403]. 
=============================================
[2019-04-07 14:07:54,511] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:07:54,511] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:07:54,514] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run19
[2019-04-07 14:08:03,885] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:08:03,886] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:08:03,889] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run19
[2019-04-07 14:08:39,752] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7433438e-23 2.1335555e-22 4.3617048e-21 9.5613014e-13 2.5086969e-19
 1.0000000e+00 1.2316522e-12 5.3425807e-15], sum to 1.0000
[2019-04-07 14:08:39,752] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8889
[2019-04-07 14:08:39,886] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 46.0, 0.0, 0.0, 24.0, 23.6586114529078, -0.04766932771637857, 0.0, 1.0, 14711.32129847726], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3619800.0000, 
sim time next is 3621600.0000, 
raw observation next is [-2.0, 50.0, 0.0, 0.0, 24.0, 23.51690756802455, -0.04677051164345619, 0.0, 1.0, 72617.59431017286], 
processed observation next is [0.0, 0.9565217391304348, 0.40720221606648205, 0.5, 0.0, 0.0, 0.5, 0.45974229733537914, 0.48440982945218125, 0.0, 1.0, 0.3457980681436803], 
reward next is 0.9399, 
noisyNet noise sample is [array([1.5013646], dtype=float32), -1.6139123]. 
=============================================
[2019-04-07 14:08:55,118] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5812971e-25 4.0820111e-22 1.2403314e-21 5.9757819e-13 1.8423736e-20
 1.0000000e+00 4.2735245e-14 1.0314404e-15], sum to 1.0000
[2019-04-07 14:08:55,121] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3278
[2019-04-07 14:08:55,181] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 24.0, 23.53404184153937, -0.07156969558533413, 0.0, 1.0, 32276.42554212052], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4233600.0000, 
sim time next is 4235400.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 24.0, 23.52761588118975, -0.07640907048768532, 0.0, 1.0, 34310.17071894071], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.5, 0.46063465676581244, 0.4745303098374382, 0.0, 1.0, 0.1633817653282891], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6439124], dtype=float32), -0.1270985]. 
=============================================
[2019-04-07 14:08:56,407] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3020103e-28 1.3041633e-27 1.1587186e-25 6.4624269e-15 1.6968119e-23
 1.0000000e+00 2.5346838e-15 2.9035526e-19], sum to 1.0000
[2019-04-07 14:08:56,407] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5109
[2019-04-07 14:08:56,467] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.15, 85.0, 165.0, 31.0, 24.0, 24.44742969302773, 0.1907269515282792, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4440600.0000, 
sim time next is 4442400.0000, 
raw observation next is [1.0, 86.0, 208.0, 88.5, 24.0, 24.64271684968874, 0.2170020961454329, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.6933333333333334, 0.09779005524861878, 0.5, 0.5535597374740616, 0.5723340320484777, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29433236], dtype=float32), -0.5971849]. 
=============================================
[2019-04-07 14:08:56,688] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3431341e-30 2.8085336e-28 4.4886447e-26 1.4901491e-15 8.1271495e-24
 1.0000000e+00 6.2734955e-16 1.1897902e-18], sum to 1.0000
[2019-04-07 14:08:56,688] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7203
[2019-04-07 14:08:56,727] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.7, 84.0, 16.0, 0.5, 24.0, 23.88260015068639, 0.1700524380007096, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1152000.0000, 
sim time next is 1153800.0000, 
raw observation next is [14.1, 79.5, 31.0, 0.0, 24.0, 23.78739937182349, 0.169198072761452, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8531855955678671, 0.795, 0.10333333333333333, 0.0, 0.5, 0.48228328098529083, 0.5563993575871506, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5084801], dtype=float32), -0.91721773]. 
=============================================
[2019-04-07 14:08:57,136] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:08:57,136] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:08:57,140] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run19
[2019-04-07 14:09:06,766] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0552269e-26 2.8294197e-26 1.4991973e-23 1.5724348e-15 1.2653822e-23
 1.0000000e+00 1.0300288e-15 2.2960429e-17], sum to 1.0000
[2019-04-07 14:09:06,768] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9453
[2019-04-07 14:09:06,837] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 59.0, 0.0, 0.0, 24.0, 24.06957886006229, 0.1474203244678009, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4570200.0000, 
sim time next is 4572000.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 24.0, 24.04788953131608, 0.1298814001070718, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.61, 0.0, 0.0, 0.5, 0.5039907942763401, 0.5432938000356906, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.70754707], dtype=float32), 0.8113124]. 
=============================================
[2019-04-07 14:09:06,885] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[103.33177 ]
 [102.90678 ]
 [101.64355 ]
 [100.87829 ]
 [100.800835]], R is [[103.54024506]
 [103.50484467]
 [103.38687134]
 [102.91864014]
 [102.8894577 ]].
[2019-04-07 14:09:23,977] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:09:23,977] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:09:23,997] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run19
[2019-04-07 14:09:26,139] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6101617e-28 4.2830432e-27 1.3360498e-25 2.9353330e-14 2.4378042e-23
 1.0000000e+00 1.2292417e-16 7.5716614e-18], sum to 1.0000
[2019-04-07 14:09:26,140] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3597
[2019-04-07 14:09:26,188] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.6, 52.0, 76.0, 570.5, 24.0, 25.26880336410662, 0.4107532581087797, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1522800.0000, 
sim time next is 1524600.0000, 
raw observation next is [11.9, 51.0, 77.0, 478.0, 24.0, 25.34555458759705, 0.4232704433396051, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7922437673130196, 0.51, 0.25666666666666665, 0.5281767955801105, 0.5, 0.6121295489664208, 0.6410901477798684, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17890684], dtype=float32), -1.3508973]. 
=============================================
[2019-04-07 14:09:27,452] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:09:27,453] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:09:27,457] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run19
[2019-04-07 14:09:27,773] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:09:27,773] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:09:27,777] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run19
[2019-04-07 14:09:28,949] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:09:28,950] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:09:28,953] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run19
[2019-04-07 14:09:30,746] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:09:30,747] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:09:30,750] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run19
[2019-04-07 14:09:35,532] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1296593e-04 1.2523080e-04 2.2470587e-04 4.7211135e-03 5.4992310e-04
 9.8584694e-01 6.2952111e-03 2.1238651e-03], sum to 1.0000
[2019-04-07 14:09:35,532] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9076
[2019-04-07 14:09:35,603] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 18.7323782676697, -0.75, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 0.0000, 
sim time next is 1800.0000, 
raw observation next is [3.6, 95.5, 0.0, 0.0, 24.0, 18.90128101974305, -0.9919784346224004, 0.0, 1.0, 164399.77376409786], 
processed observation next is [0.0, 0.0, 0.5623268698060943, 0.955, 0.0, 0.0, 0.5, 0.07510675164525409, 0.1693405217925332, 0.0, 1.0, 0.7828560655433231], 
reward next is 0.5029, 
noisyNet noise sample is [array([-1.0839074], dtype=float32), 1.1660075]. 
=============================================
[2019-04-07 14:09:37,030] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:09:37,030] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:09:37,035] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run19
[2019-04-07 14:09:38,221] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:09:38,221] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:09:38,225] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run19
[2019-04-07 14:09:38,311] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-07 14:09:38,313] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:09:38,313] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:09:38,315] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run42
[2019-04-07 14:09:38,346] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:09:38,347] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:09:38,349] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run42
[2019-04-07 14:09:38,370] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 14:09:38,372] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:09:38,378] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run42
[2019-04-07 14:11:59,753] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 14:12:17,059] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 14:12:21,894] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 14:12:22,916] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 820000, evaluation results [820000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 14:12:24,313] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:12:24,313] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:12:24,317] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run19
[2019-04-07 14:12:27,201] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:12:27,201] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:12:27,204] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run19
[2019-04-07 14:12:33,444] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:12:33,444] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:12:33,447] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run19
[2019-04-07 14:12:33,949] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:12:33,949] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:12:33,953] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run19
[2019-04-07 14:12:37,852] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.4248055e-24 6.3444182e-24 1.7986312e-20 1.1456281e-13 3.7141799e-20
 1.0000000e+00 4.1326433e-14 1.7838755e-16], sum to 1.0000
[2019-04-07 14:12:37,852] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3452
[2019-04-07 14:12:37,914] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 24.0, 22.32076404809455, -0.3549962237800166, 0.0, 1.0, 45075.52045408633], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 183600.0000, 
sim time next is 185400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 24.0, 22.22996072903443, -0.3805283462806794, 0.0, 1.0, 44928.98113527224], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.5, 0.3524967274195359, 0.3731572179064402, 0.0, 1.0, 0.21394752921558208], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3333154], dtype=float32), -0.31158122]. 
=============================================
[2019-04-07 14:12:39,590] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.6897161e-27 8.9699214e-25 2.0323069e-22 3.3143064e-14 3.3382556e-22
 1.0000000e+00 8.9967008e-14 2.6312041e-15], sum to 1.0000
[2019-04-07 14:12:39,590] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1163
[2019-04-07 14:12:39,669] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 24.0, 23.20712529732336, -0.1026820210692559, 0.0, 1.0, 41028.397891483306], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 86400.0000, 
sim time next is 88200.0000, 
raw observation next is [-0.3, 93.0, 0.0, 0.0, 24.0, 23.23991433731215, -0.08717335844315806, 0.0, 1.0, 41076.176410226384], 
processed observation next is [1.0, 0.0, 0.4542936288088643, 0.93, 0.0, 0.0, 0.5, 0.4366595281093459, 0.4709422138522807, 0.0, 1.0, 0.19560084004869707], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32684052], dtype=float32), 0.2557265]. 
=============================================
[2019-04-07 14:12:48,688] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3109588e-24 1.3481239e-22 4.2686211e-21 2.4189574e-12 7.6831779e-20
 1.0000000e+00 1.4312466e-14 1.0256380e-14], sum to 1.0000
[2019-04-07 14:12:48,688] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4532
[2019-04-07 14:12:48,785] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2480991e-24 3.0758482e-23 1.1028501e-20 7.4747234e-13 6.8954629e-20
 1.0000000e+00 3.1799249e-13 1.5082819e-14], sum to 1.0000
[2019-04-07 14:12:48,785] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2775
[2019-04-07 14:12:48,865] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.1, 79.5, 0.0, 0.0, 24.0, 22.95745490288165, -0.2053041714892045, 0.0, 1.0, 50664.84830057161], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 336600.0000, 
sim time next is 338400.0000, 
raw observation next is [-13.4, 82.0, 0.0, 0.0, 24.0, 22.82357769935841, -0.236755901592478, 0.0, 1.0, 48556.052128359384], 
processed observation next is [1.0, 0.9565217391304348, 0.09141274238227146, 0.82, 0.0, 0.0, 0.5, 0.40196480827986747, 0.42108136613584063, 0.0, 1.0, 0.2312192958493304], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07596844], dtype=float32), -0.75044847]. 
=============================================
[2019-04-07 14:12:48,893] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 54.5, 106.0, 658.0, 24.0, 23.68225611433198, -0.08343167409417136, 1.0, 1.0, 65674.4865351123], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 300600.0000, 
sim time next is 302400.0000, 
raw observation next is [-10.6, 49.0, 94.5, 708.0, 24.0, 23.81732224080217, -0.06059233582031864, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.1689750692520776, 0.49, 0.315, 0.7823204419889502, 0.5, 0.48477685340018084, 0.47980255472656047, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.258898], dtype=float32), 0.027827533]. 
=============================================
[2019-04-07 14:13:40,945] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.9530447e-22 9.1826208e-21 3.3703391e-19 1.8462586e-11 9.3142222e-19
 1.0000000e+00 3.5230396e-13 1.6028200e-13], sum to 1.0000
[2019-04-07 14:13:40,946] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6720
[2019-04-07 14:13:40,995] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.3, 65.0, 128.0, 0.0, 24.0, 23.60460955410249, 0.1417620702120957, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1175400.0000, 
sim time next is 1177200.0000, 
raw observation next is [18.3, 65.0, 104.0, 0.0, 24.0, 23.59164762655909, 0.1423501710101056, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.9695290858725764, 0.65, 0.3466666666666667, 0.0, 0.5, 0.4659706355465909, 0.5474500570033686, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.78702396], dtype=float32), -2.012022]. 
=============================================
[2019-04-07 14:14:01,116] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.3993540e-30 5.4944329e-30 4.2864945e-26 3.7077766e-15 1.5193762e-24
 1.0000000e+00 1.5820072e-15 1.9834234e-18], sum to 1.0000
[2019-04-07 14:14:01,119] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4419
[2019-04-07 14:14:01,170] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.25, 94.5, 0.0, 0.0, 24.0, 23.80381246280891, 0.1112200837807936, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1665000.0000, 
sim time next is 1666800.0000, 
raw observation next is [5.0, 92.0, 0.0, 0.0, 24.0, 23.74505391876795, 0.09456555702512341, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.6011080332409973, 0.92, 0.0, 0.0, 0.5, 0.4787544932306626, 0.5315218523417078, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04828673], dtype=float32), 0.20928043]. 
=============================================
[2019-04-07 14:14:32,914] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-07 14:14:32,915] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:14:32,915] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:14:32,917] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run43
[2019-04-07 14:14:32,933] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:14:32,937] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:14:32,940] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run43
[2019-04-07 14:14:32,936] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 14:14:32,959] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:14:32,962] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run43
[2019-04-07 14:17:03,327] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 14:17:20,564] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 14:17:23,174] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 14:17:24,198] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 840000, evaluation results [840000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 14:17:34,406] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:17:34,406] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:17:34,438] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run20
[2019-04-07 14:17:34,994] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.2801611e-23 7.0245097e-23 1.9387102e-20 6.8485116e-13 3.8634479e-20
 1.0000000e+00 1.3625114e-12 3.5019008e-14], sum to 1.0000
[2019-04-07 14:17:34,994] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3080
[2019-04-07 14:17:35,065] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.1, 67.0, 0.0, 0.0, 24.0, 22.9529455269741, -0.1988154919649829, 0.0, 1.0, 42008.078437491], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2352600.0000, 
sim time next is 2354400.0000, 
raw observation next is [-2.8, 65.0, 0.0, 0.0, 24.0, 22.90510373190674, -0.2052380926932525, 0.0, 1.0, 42117.03361158205], 
processed observation next is [0.0, 0.2608695652173913, 0.38504155124653744, 0.65, 0.0, 0.0, 0.5, 0.4087586443255615, 0.4315873024355825, 0.0, 1.0, 0.20055730291229545], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2093229], dtype=float32), -0.40712404]. 
=============================================
[2019-04-07 14:17:37,915] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0017711e-24 7.5598667e-23 5.1619815e-20 7.2674343e-13 2.2682079e-20
 1.0000000e+00 5.0097041e-13 2.9497778e-15], sum to 1.0000
[2019-04-07 14:17:37,915] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2636
[2019-04-07 14:17:38,155] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.15, 44.5, 0.0, 0.0, 24.0, 23.05677331799215, -0.1763931699318679, 0.0, 1.0, 37596.79445239987], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2395800.0000, 
sim time next is 2397600.0000, 
raw observation next is [-1.7, 44.0, 0.0, 0.0, 24.0, 23.04672325830552, -0.1772787935150685, 0.0, 1.0, 44444.61946221217], 
processed observation next is [0.0, 0.782608695652174, 0.4155124653739613, 0.44, 0.0, 0.0, 0.5, 0.42056027152545994, 0.4409070688283105, 0.0, 1.0, 0.21164104505815318], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09007462], dtype=float32), 1.6524228]. 
=============================================
[2019-04-07 14:17:43,530] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.2036547e-26 3.2910498e-25 4.8625998e-23 2.9862245e-14 1.1666868e-21
 1.0000000e+00 7.2857742e-15 5.7286426e-17], sum to 1.0000
[2019-04-07 14:17:43,530] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4478
[2019-04-07 14:17:43,600] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 50.0, 110.0, 776.0, 24.0, 24.68772445312408, 0.2315830748560349, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3333600.0000, 
sim time next is 3335400.0000, 
raw observation next is [-3.5, 50.0, 106.0, 752.0, 24.0, 24.42061655758008, 0.1795424589715984, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.36565096952908593, 0.5, 0.35333333333333333, 0.830939226519337, 0.5, 0.5350513797983399, 0.5598474863238662, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.75737864], dtype=float32), -0.06429712]. 
=============================================
[2019-04-07 14:17:43,743] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:17:43,743] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:17:43,749] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run20
[2019-04-07 14:17:44,104] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.2307833e-29 7.2638620e-27 9.5934897e-24 2.0923093e-14 4.3246219e-23
 1.0000000e+00 1.2511993e-14 5.8028950e-17], sum to 1.0000
[2019-04-07 14:17:44,105] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7643
[2019-04-07 14:17:44,155] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 24.0, 23.96636150396602, 0.061107199277527, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4687200.0000, 
sim time next is 4689000.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 24.0, 23.79873286771944, 0.01878627527532266, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 0.5, 0.48322773897662, 0.506262091758441, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44452465], dtype=float32), 0.55540967]. 
=============================================
[2019-04-07 14:17:44,170] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[108.0635  ]
 [107.053604]
 [104.75668 ]
 [103.76725 ]
 [102.931366]], R is [[108.4756012 ]
 [108.39084625]
 [108.01544952]
 [107.9352951 ]
 [107.85594177]].
[2019-04-07 14:17:44,873] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.2446708e-29 2.1528762e-28 4.7646502e-26 2.2691560e-16 6.6972587e-24
 1.0000000e+00 1.4442014e-17 1.1456639e-19], sum to 1.0000
[2019-04-07 14:17:44,873] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5061
[2019-04-07 14:17:44,889] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 208.0, 6.0, 24.0, 24.56960669817781, 0.1451443139625295, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4703400.0000, 
sim time next is 4705200.0000, 
raw observation next is [0.0, 92.0, 210.5, 6.0, 24.0, 24.65841360944879, 0.1535820992264459, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.92, 0.7016666666666667, 0.0066298342541436465, 0.5, 0.5548678007873992, 0.551194033075482, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1010189], dtype=float32), -1.7433019]. 
=============================================
[2019-04-07 14:18:03,272] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:18:03,272] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:18:03,288] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run20
[2019-04-07 14:18:14,492] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1241494e-24 1.3524249e-22 7.1419178e-21 2.3173696e-12 1.5281331e-20
 1.0000000e+00 3.5728842e-14 5.7776954e-16], sum to 1.0000
[2019-04-07 14:18:14,492] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8047
[2019-04-07 14:18:14,581] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 24.0, 23.28292550794952, -0.13530174295897, 0.0, 1.0, 39539.75627421837], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3027600.0000, 
sim time next is 3029400.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 24.0, 23.25799023238572, -0.1457529220772104, 0.0, 1.0, 39515.59047310208], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.5, 0.4381658526988099, 0.45141569264092984, 0.0, 1.0, 0.18816947844334322], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4435436], dtype=float32), 1.9121743]. 
=============================================
[2019-04-07 14:18:14,748] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1040905e-26 2.1863238e-25 2.3415266e-23 9.3523720e-15 7.3594046e-23
 1.0000000e+00 1.4319245e-13 2.6781786e-17], sum to 1.0000
[2019-04-07 14:18:14,748] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4894
[2019-04-07 14:18:14,877] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:18:14,877] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:18:14,881] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run20
[2019-04-07 14:18:14,905] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 86.0, 64.5, 0.0, 24.0, 22.6778715962926, -0.2245489912633737, 0.0, 1.0, 17343.513406929444], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 54000.0000, 
sim time next is 55800.0000, 
raw observation next is [6.9, 84.0, 50.0, 0.0, 24.0, 22.64236069763369, -0.2127045426628814, 0.0, 1.0, 44191.19028386329], 
processed observation next is [0.0, 0.6521739130434783, 0.6537396121883658, 0.84, 0.16666666666666666, 0.0, 0.5, 0.3868633914694743, 0.4290984857790396, 0.0, 1.0, 0.21043423944696804], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12371465], dtype=float32), -0.87580407]. 
=============================================
[2019-04-07 14:18:16,793] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3739307e-25 3.4616593e-24 1.4513712e-23 9.7524243e-14 9.4528761e-22
 1.0000000e+00 4.6033991e-14 3.1785737e-16], sum to 1.0000
[2019-04-07 14:18:16,793] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6793
[2019-04-07 14:18:16,940] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.0, 72.5, 0.0, 0.0, 24.0, 23.0185545019453, -0.1678581188983196, 0.0, 1.0, 45652.091169951775], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2683800.0000, 
sim time next is 2685600.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 24.0, 22.78674549253355, -0.1941231697803409, 0.0, 1.0, 45646.469180643886], 
processed observation next is [1.0, 0.08695652173913043, 0.15789473684210528, 0.76, 0.0, 0.0, 0.5, 0.3988954577111293, 0.43529227673988635, 0.0, 1.0, 0.21736413895544707], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26147765], dtype=float32), -0.3012061]. 
=============================================
[2019-04-07 14:18:42,505] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.5338834e-27 2.9692715e-25 1.5344813e-22 2.7991518e-14 1.4552811e-21
 1.0000000e+00 4.3318713e-15 3.5093779e-17], sum to 1.0000
[2019-04-07 14:18:42,506] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1136
[2019-04-07 14:18:42,551] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.2, 80.5, 0.0, 0.0, 24.0, 22.93606488084302, -0.2141805308447834, 0.0, 1.0, 45494.09310444561], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 257400.0000, 
sim time next is 259200.0000, 
raw observation next is [-4.5, 79.0, 0.0, 0.0, 24.0, 22.84991609236818, -0.2308423921405384, 0.0, 1.0, 45471.01919857669], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.79, 0.0, 0.0, 0.5, 0.40415967436401495, 0.42305253595315384, 0.0, 1.0, 0.21652866285036518], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86683005], dtype=float32), 1.4032028]. 
=============================================
[2019-04-07 14:18:43,372] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9117618e-23 2.5534355e-21 7.8725235e-20 2.8818024e-12 2.9331204e-19
 1.0000000e+00 9.6831137e-12 4.5215825e-14], sum to 1.0000
[2019-04-07 14:18:43,372] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7653
[2019-04-07 14:18:43,413] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 24.0, 23.63777518917008, -0.06484368289878166, 0.0, 1.0, 30849.74921855249], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3636000.0000, 
sim time next is 3637800.0000, 
raw observation next is [8.6, 26.0, 0.0, 0.0, 24.0, 23.64454640907707, -0.07128994634209297, 0.0, 1.0, 21972.486924839915], 
processed observation next is [0.0, 0.08695652173913043, 0.700831024930748, 0.26, 0.0, 0.0, 0.5, 0.4703788674230891, 0.4762366845526356, 0.0, 1.0, 0.10463089011828532], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.183185], dtype=float32), 1.4491603]. 
=============================================
[2019-04-07 14:18:55,521] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2953997e-23 4.0513088e-23 1.6078420e-20 4.4670170e-12 6.1632536e-20
 1.0000000e+00 1.2122218e-12 1.8327723e-14], sum to 1.0000
[2019-04-07 14:18:55,523] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1486
[2019-04-07 14:18:55,576] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 24.0, 23.47130676972274, -0.06218520927750651, 0.0, 1.0, 34607.51997424161], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4066200.0000, 
sim time next is 4068000.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 24.0, 23.44554128257178, -0.09492725794788982, 0.0, 1.0, 32113.56672952896], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.41, 0.0, 0.0, 0.5, 0.4537951068809818, 0.4683575806840367, 0.0, 1.0, 0.1529217463310903], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8732429], dtype=float32), 0.823829]. 
=============================================
[2019-04-07 14:18:55,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[89.266396]
 [89.23988 ]
 [89.57405 ]
 [89.4187  ]
 [90.39721 ]], R is [[89.49456024]
 [89.599617  ]
 [89.70362091]
 [89.80658722]
 [89.90852356]].
[2019-04-07 14:19:01,529] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:19:01,530] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:19:01,534] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run20
[2019-04-07 14:19:13,990] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.1264960e-27 2.4639518e-25 1.6976609e-22 5.4535467e-15 1.6541205e-22
 1.0000000e+00 9.1525027e-15 8.9904434e-17], sum to 1.0000
[2019-04-07 14:19:13,990] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3888
[2019-04-07 14:19:14,068] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 24.0, 23.65005913473699, 0.02469880273102871, 0.0, 1.0, 6248.5942686292365], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4579200.0000, 
sim time next is 4581000.0000, 
raw observation next is [0.7, 62.0, 0.0, 0.0, 24.0, 23.57799797983006, 0.007263054014500087, 0.0, 1.0, 25212.94722258326], 
processed observation next is [1.0, 0.0, 0.4819944598337951, 0.62, 0.0, 0.0, 0.5, 0.4648331649858382, 0.5024210180048333, 0.0, 1.0, 0.12006165344087268], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34360832], dtype=float32), -0.33089766]. 
=============================================
[2019-04-07 14:19:14,081] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[102.41597]
 [103.77674]
 [103.842  ]
 [102.94896]
 [103.61462]], R is [[102.40037537]
 [102.37637329]
 [102.35260773]
 [102.13185883]
 [102.1105423 ]].
[2019-04-07 14:19:21,141] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-07 14:19:21,143] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:19:21,143] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:19:21,146] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run44
[2019-04-07 14:19:21,172] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:19:21,172] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:19:21,175] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run44
[2019-04-07 14:19:21,198] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 14:19:21,199] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:19:21,201] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run44
[2019-04-07 14:21:46,672] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 14:21:50,229] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11247837], dtype=float32), 0.13841932]
[2019-04-07 14:21:50,230] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-1.284062269, 60.65848521, 159.2664086, 806.19817715, 24.0, 24.25213882692096, 0.1315846572313083, 1.0, 1.0, 0.0]
[2019-04-07 14:21:50,230] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 14:21:50,230] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [1.1507632e-25 1.2150223e-24 1.3053064e-22 9.5619713e-14 1.7914404e-21
 1.0000000e+00 1.3525491e-14 6.6571432e-16], sampled 0.26384888648947213
[2019-04-07 14:22:05,175] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 14:22:08,943] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 14:22:09,966] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 860000, evaluation results [860000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 14:22:13,626] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3327513e-25 3.5120971e-24 5.2302210e-22 1.0197782e-14 2.5813084e-21
 1.0000000e+00 4.1833098e-15 3.7634780e-16], sum to 1.0000
[2019-04-07 14:22:13,627] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1442
[2019-04-07 14:22:13,673] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 39.0, 0.0, 0.0, 24.0, 23.5256441591222, 0.000901250893392283, 0.0, 1.0, 31451.157029570062], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4150800.0000, 
sim time next is 4152600.0000, 
raw observation next is [-1.5, 42.5, 0.0, 0.0, 24.0, 23.47912885047186, -0.007356715435244386, 0.0, 1.0, 45814.34745460638], 
processed observation next is [0.0, 0.043478260869565216, 0.4210526315789474, 0.425, 0.0, 0.0, 0.5, 0.45659407087265497, 0.49754776152158525, 0.0, 1.0, 0.21816355930764944], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00953928], dtype=float32), -1.6025382]. 
=============================================
[2019-04-07 14:22:23,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4957976e-24 4.3396933e-23 1.4524258e-20 3.8650165e-13 2.7573975e-20
 1.0000000e+00 3.6589035e-14 5.3667859e-15], sum to 1.0000
[2019-04-07 14:22:23,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2268
[2019-04-07 14:22:23,230] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 53.0, 0.0, 0.0, 24.0, 23.5501028410164, -0.04138170463842877, 0.0, 1.0, 56853.88128760192], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4829400.0000, 
sim time next is 4831200.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 24.0, 23.50489271282122, -0.03676641860959991, 0.0, 1.0, 53844.47183923682], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.5, 0.4587410594017684, 0.48774452713013333, 0.0, 1.0, 0.2564022468535087], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0262465], dtype=float32), -0.740472]. 
=============================================
[2019-04-07 14:22:26,436] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:22:26,436] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:22:26,439] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run20
[2019-04-07 14:22:30,155] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:22:30,155] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:22:30,158] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run20
[2019-04-07 14:22:31,385] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:22:31,385] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:22:31,408] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run20
[2019-04-07 14:22:32,865] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:22:32,865] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:22:32,874] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run20
[2019-04-07 14:22:33,913] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:22:33,913] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:22:33,933] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run20
[2019-04-07 14:22:35,372] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6842584e-23 1.8777767e-23 2.4371653e-21 2.9502776e-12 1.8683196e-20
 1.0000000e+00 1.4366847e-13 5.3988389e-15], sum to 1.0000
[2019-04-07 14:22:35,373] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9529
[2019-04-07 14:22:35,427] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 24.0, 23.30090775903318, -0.1372393776493357, 0.0, 1.0, 42725.609425066265], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4860000.0000, 
sim time next is 4861800.0000, 
raw observation next is [-3.5, 62.5, 0.0, 0.0, 24.0, 23.26521207012803, -0.1457051661092915, 0.0, 1.0, 40829.17386649973], 
processed observation next is [0.0, 0.2608695652173913, 0.36565096952908593, 0.625, 0.0, 0.0, 0.5, 0.4387676725106691, 0.45143161129690285, 0.0, 1.0, 0.19442463745952254], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38681695], dtype=float32), 0.6537167]. 
=============================================
[2019-04-07 14:22:35,445] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:22:35,445] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:22:35,448] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run20
[2019-04-07 14:22:37,323] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:22:37,324] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:22:37,330] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run20
[2019-04-07 14:22:37,866] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2059417e-27 6.2914827e-27 2.0979831e-26 3.8026989e-14 2.1179688e-23
 1.0000000e+00 5.6904865e-16 8.7304678e-18], sum to 1.0000
[2019-04-07 14:22:37,866] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5350
[2019-04-07 14:22:37,974] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 24.0, 23.54563371922191, 0.04617764716547018, 0.0, 1.0, 35852.709751496], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4744800.0000, 
sim time next is 4746600.0000, 
raw observation next is [-3.0, 80.5, 0.0, 0.0, 24.0, 23.39127970456011, -0.004373416609856726, 0.0, 1.0, 81966.32301669155], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.805, 0.0, 0.0, 0.5, 0.4492733087133424, 0.49854219446338105, 0.0, 1.0, 0.3903158238890074], 
reward next is 0.8954, 
noisyNet noise sample is [array([0.91554135], dtype=float32), 1.0101266]. 
=============================================
[2019-04-07 14:22:42,568] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:22:42,568] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:22:42,587] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run20
[2019-04-07 14:22:46,423] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:22:46,423] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:22:46,438] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run20
[2019-04-07 14:22:50,885] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1419783e-24 1.3981247e-24 1.0856598e-22 5.1316660e-13 8.2184736e-22
 1.0000000e+00 5.5297712e-14 8.0699946e-17], sum to 1.0000
[2019-04-07 14:22:50,885] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9794
[2019-04-07 14:22:51,099] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 62.5, 61.0, 45.0, 24.0, 23.98636164911574, 0.0461763761610753, 1.0, 1.0, 85131.26064295774], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 142200.0000, 
sim time next is 144000.0000, 
raw observation next is [-6.7, 64.0, 44.0, 24.0, 24.0, 24.31283175702432, 0.06255596385217267, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.2770083102493075, 0.64, 0.14666666666666667, 0.026519337016574586, 0.5, 0.52606931308536, 0.5208519879507242, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.70908076], dtype=float32), 1.5882597]. 
=============================================
[2019-04-07 14:22:51,124] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[ 99.4691  ]
 [ 98.81284 ]
 [ 98.402016]
 [ 99.61574 ]
 [100.46105 ]], R is [[99.29192352]
 [99.17932892]
 [99.16151428]
 [99.16989899]
 [99.17819977]].
[2019-04-07 14:22:53,025] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:22:53,025] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:22:53,414] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run20
[2019-04-07 14:22:54,440] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:22:54,440] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:22:54,461] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run20
[2019-04-07 14:23:10,966] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1970622e-22 6.3998950e-22 8.8425750e-20 3.2544980e-12 4.0963704e-18
 1.0000000e+00 1.9725668e-13 8.3855550e-15], sum to 1.0000
[2019-04-07 14:23:10,975] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0410
[2019-04-07 14:23:11,057] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 24.0, 20.882840428321, -0.7103223688418812, 0.0, 1.0, 49419.028030816524], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 367200.0000, 
sim time next is 369000.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 24.0, 20.80287327214417, -0.7248471803414183, 0.0, 1.0, 49679.41053902535], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.5, 0.2335727726786807, 0.2583842732195272, 0.0, 1.0, 0.23656862161440645], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0490547], dtype=float32), 0.1797937]. 
=============================================
[2019-04-07 14:23:11,080] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.27934 ]
 [85.49706 ]
 [85.849976]
 [86.18786 ]
 [86.63788 ]], R is [[85.1787262 ]
 [85.32694244]
 [85.47367096]
 [85.61893463]
 [85.76274872]].
[2019-04-07 14:23:36,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7335625e-30 2.8160002e-28 4.9469004e-25 1.9688528e-17 1.8618751e-26
 1.0000000e+00 2.6033446e-18 2.6031036e-20], sum to 1.0000
[2019-04-07 14:23:36,022] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1779
[2019-04-07 14:23:36,116] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 100.0, 0.0, 0.0, 24.0, 23.58454392103748, -0.07650929576593789, 0.0, 1.0, 20411.105702104225], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3119400.0000, 
sim time next is 3121200.0000, 
raw observation next is [2.0, 100.0, 0.0, 0.0, 24.0, 23.59435583661636, -0.1305354483414201, 0.0, 1.0, 11195.557961621169], 
processed observation next is [1.0, 0.13043478260869565, 0.518005540166205, 1.0, 0.0, 0.0, 0.5, 0.4661963197180299, 0.45648818388619333, 0.0, 1.0, 0.053312180769624615], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23230503], dtype=float32), 0.55204654]. 
=============================================
[2019-04-07 14:24:03,060] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2801630e-28 1.6202419e-28 1.0121184e-25 9.9090110e-17 5.2060330e-23
 1.0000000e+00 2.9528598e-17 1.2971789e-17], sum to 1.0000
[2019-04-07 14:24:03,060] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9380
[2019-04-07 14:24:03,101] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 72.0, 0.0, 24.0, 24.39951159761215, 0.1238775163071178, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1420200.0000, 
sim time next is 1422000.0000, 
raw observation next is [0.0, 95.0, 81.0, 0.0, 24.0, 24.3437150342033, 0.1147824483171626, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.27, 0.0, 0.5, 0.5286429195169416, 0.5382608161057209, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1515505], dtype=float32), 0.05785794]. 
=============================================
[2019-04-07 14:24:03,111] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[110.258736]
 [110.33794 ]
 [110.22927 ]
 [110.37535 ]
 [110.64523 ]], R is [[110.01712799]
 [109.91695404]
 [109.81778717]
 [109.71961212]
 [109.62241364]].
[2019-04-07 14:24:13,564] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.5417340e-29 2.8035053e-26 2.8902511e-24 1.8680224e-14 5.2830737e-24
 1.0000000e+00 4.9240667e-16 1.4649870e-17], sum to 1.0000
[2019-04-07 14:24:13,567] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5062
[2019-04-07 14:24:13,589] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 114.5, 0.0, 24.0, 24.40297112130542, 0.1829145425673463, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1335600.0000, 
sim time next is 1337400.0000, 
raw observation next is [1.1, 92.0, 127.0, 0.0, 24.0, 24.43966252938922, 0.1878565277811722, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.42333333333333334, 0.0, 0.5, 0.5366385441157684, 0.5626188425937241, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.437489], dtype=float32), 0.45545214]. 
=============================================
[2019-04-07 14:24:16,642] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6986921e-28 2.1405142e-25 1.3761443e-24 2.3230040e-15 5.1273379e-22
 1.0000000e+00 4.0998940e-15 1.0024281e-17], sum to 1.0000
[2019-04-07 14:24:16,642] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0185
[2019-04-07 14:24:16,683] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 24.0, 23.40779831364794, -0.02745006615074001, 0.0, 1.0, 71341.48612958506], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3902400.0000, 
sim time next is 3904200.0000, 
raw observation next is [-3.5, 74.0, 0.0, 0.0, 24.0, 23.58840227815704, -0.04219458800607508, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.36565096952908593, 0.74, 0.0, 0.0, 0.5, 0.4657001898464201, 0.48593513733130833, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35156062], dtype=float32), -1.4911124]. 
=============================================
[2019-04-07 14:24:18,612] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-07 14:24:18,612] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:24:18,627] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:24:18,632] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:24:18,633] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:24:18,633] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 14:24:18,634] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:24:18,638] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run45
[2019-04-07 14:24:18,655] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run45
[2019-04-07 14:24:18,679] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run45
[2019-04-07 14:26:48,566] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 14:27:03,185] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 14:27:09,533] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 14:27:10,556] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 880000, evaluation results [880000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 14:27:11,634] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2047979e-26 3.9459525e-26 2.2089440e-23 6.0827570e-15 8.2395914e-24
 1.0000000e+00 1.9438396e-15 1.0081227e-17], sum to 1.0000
[2019-04-07 14:27:11,634] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4053
[2019-04-07 14:27:11,656] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.35, 84.5, 53.0, 0.0, 24.0, 23.94303336877333, 0.07856244638545697, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1697400.0000, 
sim time next is 1699200.0000, 
raw observation next is [1.6, 81.0, 41.5, 0.0, 24.0, 24.097800487338, 0.09477808598795257, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5069252077562327, 0.81, 0.13833333333333334, 0.0, 0.5, 0.5081500406114999, 0.5315926953293175, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0182736], dtype=float32), 0.8429849]. 
=============================================
[2019-04-07 14:27:40,584] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8345484e-26 1.5661889e-25 1.9769911e-22 2.1871243e-14 3.3096499e-22
 1.0000000e+00 1.5991762e-14 6.4733291e-16], sum to 1.0000
[2019-04-07 14:27:40,585] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4904
[2019-04-07 14:27:40,653] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 24.0, 23.41814901077947, -0.02037420492676975, 0.0, 1.0, 67887.9228587797], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3895200.0000, 
sim time next is 3897000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 24.0, 23.95950461009096, 0.03958337337646554, 0.0, 1.0, 19735.223687145193], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.5, 0.4966253841742467, 0.5131944577921551, 0.0, 1.0, 0.09397725565307236], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49280503], dtype=float32), -1.6467565]. 
=============================================
[2019-04-07 14:27:40,675] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[99.58952 ]
 [98.85344 ]
 [98.93513 ]
 [99.007385]
 [98.567955]], R is [[99.34976196]
 [99.3187027 ]
 [99.32551575]
 [99.33226013]
 [99.33893585]].
[2019-04-07 14:27:49,539] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9394842e-33 3.6804104e-30 5.3438344e-26 5.8741237e-16 5.8799449e-26
 1.0000000e+00 1.1722599e-19 5.7574676e-19], sum to 1.0000
[2019-04-07 14:27:49,539] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8465
[2019-04-07 14:27:49,601] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.5, 96.5, 113.0, 823.0, 24.0, 25.44408440216921, 0.3924286899282334, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3155400.0000, 
sim time next is 3157200.0000, 
raw observation next is [7.0, 100.0, 112.5, 814.5, 24.0, 25.41095985399549, 0.4073643350673801, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.375, 0.9, 0.5, 0.6175799878329574, 0.6357881116891267, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31280854], dtype=float32), -0.15712011]. 
=============================================
[2019-04-07 14:27:50,996] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:27:50,996] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:27:51,018] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run21
[2019-04-07 14:27:59,642] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:27:59,643] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:27:59,646] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run21
[2019-04-07 14:28:01,582] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.6650335e-26 2.1912010e-24 2.5243226e-21 1.7904170e-12 1.4669525e-21
 1.0000000e+00 5.5347203e-14 5.2049793e-16], sum to 1.0000
[2019-04-07 14:28:01,583] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2129
[2019-04-07 14:28:01,870] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 163.5, 575.5, 24.0, 23.69051226993184, 0.02744617446743249, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4788000.0000, 
sim time next is 4789800.0000, 
raw observation next is [-2.5, 55.5, 153.0, 730.0, 24.0, 23.51336058134284, 0.01349845908906531, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.39335180055401664, 0.555, 0.51, 0.8066298342541437, 0.5, 0.45944671511190344, 0.5044994863630218, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5245482], dtype=float32), 0.68545955]. 
=============================================
[2019-04-07 14:28:16,665] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:28:16,665] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:28:16,670] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run21
[2019-04-07 14:28:24,030] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.21677493e-25 1.49548385e-24 4.94085421e-22 2.18173813e-13
 4.93219446e-21 1.00000000e+00 2.44789371e-14 4.44025021e-16], sum to 1.0000
[2019-04-07 14:28:24,030] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5566
[2019-04-07 14:28:24,082] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 24.0, 23.24315791830077, -0.1023838118434974, 0.0, 1.0, 44975.72430650791], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2766600.0000, 
sim time next is 2768400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 24.0, 23.40295251971755, -0.0934409230015864, 0.0, 1.0, 38697.388992241635], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.5, 0.45024604330979595, 0.46885302566613785, 0.0, 1.0, 0.18427328091543635], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1837214], dtype=float32), 0.97067755]. 
=============================================
[2019-04-07 14:28:35,301] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:28:35,301] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:28:35,304] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run21
[2019-04-07 14:28:50,185] A3C_AGENT_WORKER-Thread-15 INFO:Local step 57000, global step 895066: loss 0.2746
[2019-04-07 14:28:50,190] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 57000, global step 895066: learning rate 0.0000
[2019-04-07 14:28:58,182] A3C_AGENT_WORKER-Thread-12 INFO:Local step 57000, global step 896439: loss 0.2329
[2019-04-07 14:28:58,183] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 57000, global step 896439: learning rate 0.0000
[2019-04-07 14:29:09,297] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.3732663e-30 6.2888402e-28 4.9321254e-26 9.7340861e-17 4.3900255e-24
 1.0000000e+00 9.7998412e-18 3.6343367e-18], sum to 1.0000
[2019-04-07 14:29:09,297] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1301
[2019-04-07 14:29:09,324] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.7, 92.5, 27.0, 0.0, 24.0, 23.70317791488016, 0.03685987011209494, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 981000.0000, 
sim time next is 982800.0000, 
raw observation next is [10.0, 92.0, 43.5, 0.0, 24.0, 24.08100736262349, 0.09123930356604472, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.739612188365651, 0.92, 0.145, 0.0, 0.5, 0.5067506135519576, 0.5304131011886816, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19133517], dtype=float32), -0.8342017]. 
=============================================
[2019-04-07 14:29:11,017] A3C_AGENT_WORKER-Thread-6 INFO:Local step 57000, global step 898998: loss 0.2058
[2019-04-07 14:29:11,018] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 57000, global step 898998: learning rate 0.0000
[2019-04-07 14:29:13,904] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:29:13,904] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:29:13,910] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run21
[2019-04-07 14:29:15,788] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-07 14:29:15,793] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:29:15,793] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:29:15,797] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run46
[2019-04-07 14:29:15,821] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:29:15,822] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 14:29:15,822] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:29:15,822] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:29:15,828] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run46
[2019-04-07 14:29:15,844] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run46
[2019-04-07 14:29:30,971] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11244605], dtype=float32), 0.13931659]
[2019-04-07 14:29:30,971] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-13.897248545, 73.14581632, 0.0, 0.0, 24.0, 22.8609269711743, -0.2572619267041666, 0.0, 1.0, 47424.003940758674]
[2019-04-07 14:29:30,971] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 14:29:30,972] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [4.9480003e-23 5.4471237e-22 5.0917519e-20 2.2717804e-12 2.1816566e-19
 1.0000000e+00 3.6244814e-13 1.4670360e-14], sampled 0.9101954263880097
[2019-04-07 14:30:58,235] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11244605], dtype=float32), 0.13931659]
[2019-04-07 14:30:58,235] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [-9.45, 62.0, 0.0, 0.0, 24.0, 22.22097510948948, -0.361958703674636, 0.0, 1.0, 44535.87273240231]
[2019-04-07 14:30:58,235] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 14:30:58,237] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [3.9138489e-22 3.6796273e-21 2.2878221e-19 4.0810974e-12 9.7433263e-19
 1.0000000e+00 1.3468817e-12 6.8417582e-14], sampled 0.615143121970168
[2019-04-07 14:31:40,357] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 14:31:55,326] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 14:32:00,122] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 14:32:01,160] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 900000, evaluation results [900000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 14:32:05,965] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.52522289e-25 4.98570190e-24 5.19744640e-21 1.03352205e-11
 2.62669459e-20 1.00000000e+00 3.53569340e-14 5.66395939e-15], sum to 1.0000
[2019-04-07 14:32:05,966] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3662
[2019-04-07 14:32:06,024] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.9, 51.5, 0.0, 0.0, 24.0, 25.86478263987483, 0.4987419524008751, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1099800.0000, 
sim time next is 1101600.0000, 
raw observation next is [16.1, 53.0, 0.0, 0.0, 24.0, 25.05957370379096, 0.4313136101957747, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.9085872576177286, 0.53, 0.0, 0.0, 0.5, 0.5882978086492466, 0.6437712033985915, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1914994], dtype=float32), -2.2001412]. 
=============================================
[2019-04-07 14:32:12,325] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.9288078e-23 5.1562887e-22 3.1473175e-20 7.5510139e-13 3.8391644e-19
 1.0000000e+00 1.0092022e-13 3.2359424e-15], sum to 1.0000
[2019-04-07 14:32:12,325] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6594
[2019-04-07 14:32:12,333] A3C_AGENT_WORKER-Thread-16 INFO:Local step 57000, global step 902171: loss 0.2362
[2019-04-07 14:32:12,334] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 57000, global step 902171: learning rate 0.0000
[2019-04-07 14:32:12,365] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.0, 69.0, 0.0, 0.0, 24.0, 22.61718773518722, -0.2686980109553114, 0.0, 1.0, 44786.08956846624], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3992400.0000, 
sim time next is 3994200.0000, 
raw observation next is [-13.0, 66.0, 0.0, 0.0, 24.0, 22.47932043052196, -0.300858826312834, 0.0, 1.0, 44781.35782675769], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.66, 0.0, 0.0, 0.5, 0.3732767025434966, 0.3997137245623887, 0.0, 1.0, 0.21324456107979853], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08835109], dtype=float32), 1.4264493]. 
=============================================
[2019-04-07 14:32:15,495] A3C_AGENT_WORKER-Thread-15 INFO:Local step 57500, global step 902821: loss 0.6456
[2019-04-07 14:32:15,496] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 57500, global step 902821: learning rate 0.0000
[2019-04-07 14:32:23,171] A3C_AGENT_WORKER-Thread-12 INFO:Local step 57500, global step 904529: loss 0.6431
[2019-04-07 14:32:23,171] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 57500, global step 904529: learning rate 0.0000
[2019-04-07 14:32:27,610] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0781428e-26 3.8716260e-25 2.9641591e-23 5.2910194e-13 1.5007946e-22
 1.0000000e+00 1.6769072e-14 8.0655324e-17], sum to 1.0000
[2019-04-07 14:32:27,611] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5070
[2019-04-07 14:32:27,662] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.6, 74.0, 0.0, 0.0, 24.0, 23.49999230475614, -0.03261936198624865, 0.0, 1.0, 30718.854517589007], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4600800.0000, 
sim time next is 4602600.0000, 
raw observation next is [-2.8, 75.5, 0.0, 0.0, 24.0, 23.40702047613824, -0.07491258984532875, 0.0, 1.0, 31290.308292639802], 
processed observation next is [1.0, 0.2608695652173913, 0.38504155124653744, 0.755, 0.0, 0.0, 0.5, 0.4505850396781866, 0.4750291367182238, 0.0, 1.0, 0.14900146806018955], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7456949], dtype=float32), 1.1614474]. 
=============================================
[2019-04-07 14:32:32,063] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4846241e-31 1.7422689e-28 4.4648001e-28 1.3526657e-17 4.1230185e-27
 1.0000000e+00 7.8017892e-20 8.4148267e-19], sum to 1.0000
[2019-04-07 14:32:32,064] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5999
[2019-04-07 14:32:32,086] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.5, 76.5, 25.0, 0.0, 24.0, 22.86546364839213, 0.1004469497534423, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1009800.0000, 
sim time next is 1011600.0000, 
raw observation next is [15.5, 78.0, 12.5, 0.0, 24.0, 24.77910569447652, 0.2456049432441601, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8919667590027703, 0.78, 0.041666666666666664, 0.0, 0.5, 0.56492547453971, 0.58186831441472, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2988574], dtype=float32), -2.1229699]. 
=============================================
[2019-04-07 14:32:35,078] A3C_AGENT_WORKER-Thread-6 INFO:Local step 57500, global step 906944: loss 0.6631
[2019-04-07 14:32:35,099] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 57500, global step 906944: learning rate 0.0000
[2019-04-07 14:32:35,624] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.8543355e-28 6.9152250e-26 2.4267235e-25 1.8123900e-15 6.3930073e-24
 1.0000000e+00 2.7615809e-17 4.6471487e-17], sum to 1.0000
[2019-04-07 14:32:35,624] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7606
[2019-04-07 14:32:35,675] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8, 71.0, 0.0, 0.0, 24.0, 23.61728629100338, -9.754851074379271e-05, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4510800.0000, 
sim time next is 4512600.0000, 
raw observation next is [-0.9, 71.0, 0.0, 0.0, 24.0, 23.45847340729465, -0.03781203567752531, 0.0, 1.0, 40949.40336650216], 
processed observation next is [1.0, 0.21739130434782608, 0.43767313019390586, 0.71, 0.0, 0.0, 0.5, 0.45487278394122094, 0.4873959881074916, 0.0, 1.0, 0.19499715888810554], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54353464], dtype=float32), 1.0538572]. 
=============================================
[2019-04-07 14:32:40,327] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.73725826e-28 1.51733505e-27 3.46173859e-24 4.52056503e-14
 1.12019383e-24 1.00000000e+00 2.92018553e-16 1.06144465e-16], sum to 1.0000
[2019-04-07 14:32:40,327] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9604
[2019-04-07 14:32:40,370] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 89.0, 213.0, 6.0, 24.0, 24.58625585193493, 0.1431004105201555, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4707000.0000, 
sim time next is 4708800.0000, 
raw observation next is [1.0, 86.0, 160.5, 3.0, 24.0, 24.57032526824844, 0.1283347272880007, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.535, 0.0033149171270718232, 0.5, 0.54752710568737, 0.5427782424293336, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11348674], dtype=float32), 1.4652756]. 
=============================================
[2019-04-07 14:32:40,785] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.1305975e-25 1.4627313e-24 7.2972082e-22 1.6725892e-13 3.3523457e-21
 1.0000000e+00 5.9170596e-15 5.1989095e-15], sum to 1.0000
[2019-04-07 14:32:40,785] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3648
[2019-04-07 14:32:40,905] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 43.5, 0.0, 0.0, 24.0, 23.20006743499979, -0.09644478721349048, 0.0, 1.0, 138628.0062428896], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4908600.0000, 
sim time next is 4910400.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 24.0, 23.66504822696265, -0.00281346655498986, 0.0, 1.0, 59636.40518577205], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.4, 0.0, 0.0, 0.5, 0.4720873522468875, 0.49906217781500334, 0.0, 1.0, 0.28398288183700976], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4899707], dtype=float32), 0.20578787]. 
=============================================
[2019-04-07 14:32:44,042] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:32:44,043] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:32:44,046] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run21
[2019-04-07 14:32:44,642] A3C_AGENT_WORKER-Thread-19 INFO:Local step 57000, global step 908871: loss 0.2945
[2019-04-07 14:32:44,642] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 57000, global step 908871: learning rate 0.0000
[2019-04-07 14:32:48,937] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:32:48,937] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:32:48,940] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run21
[2019-04-07 14:32:48,992] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:32:48,993] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:32:48,997] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run21
[2019-04-07 14:32:50,101] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0446262e-27 3.5609312e-25 7.9655490e-24 7.0091206e-15 7.4056788e-22
 1.0000000e+00 9.0562566e-17 2.1494335e-17], sum to 1.0000
[2019-04-07 14:32:50,101] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7848
[2019-04-07 14:32:50,177] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 50.5, 122.0, 833.0, 24.0, 25.26770051244191, 0.2316336024480952, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4620600.0000, 
sim time next is 4622400.0000, 
raw observation next is [3.0, 49.0, 121.0, 846.0, 24.0, 25.43669993087832, 0.4091822794112023, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.49, 0.4033333333333333, 0.9348066298342541, 0.5, 0.6197249942398599, 0.6363940931370674, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7424738], dtype=float32), -0.6571628]. 
=============================================
[2019-04-07 14:32:51,133] A3C_AGENT_WORKER-Thread-16 INFO:Local step 57500, global step 909928: loss 0.6756
[2019-04-07 14:32:51,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 57500, global step 909928: learning rate 0.0000
[2019-04-07 14:32:51,625] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:32:51,625] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:32:51,628] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run21
[2019-04-07 14:32:52,357] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:32:52,357] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:32:52,361] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run21
[2019-04-07 14:32:55,641] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:32:55,641] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:32:55,644] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run21
[2019-04-07 14:32:58,237] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:32:58,237] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:32:58,241] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run21
[2019-04-07 14:33:00,806] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:33:00,806] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:33:00,809] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run21
[2019-04-07 14:33:03,873] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:33:03,873] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:33:03,876] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run21
[2019-04-07 14:33:10,173] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6578198e-25 7.9351298e-24 2.6929856e-22 8.7238203e-12 3.0139206e-21
 1.0000000e+00 1.2801288e-13 2.9658774e-16], sum to 1.0000
[2019-04-07 14:33:10,174] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3810
[2019-04-07 14:33:10,408] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 64.0, 0.0, 0.0, 24.0, 23.18331715698164, -0.1215808071626905, 1.0, 1.0, 59074.84025017405], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 154800.0000, 
sim time next is 156600.0000, 
raw observation next is [-8.1, 66.0, 0.0, 0.0, 24.0, 23.15969124386004, -0.1223449497299035, 1.0, 1.0, 65428.304772413256], 
processed observation next is [1.0, 0.8260869565217391, 0.23822714681440446, 0.66, 0.0, 0.0, 0.5, 0.42997427032166985, 0.4592183500900322, 1.0, 1.0, 0.31156335605911073], 
reward next is 0.9742, 
noisyNet noise sample is [array([-1.8932526], dtype=float32), 1.1834006]. 
=============================================
[2019-04-07 14:33:11,630] A3C_AGENT_WORKER-Thread-15 INFO:Local step 58000, global step 912326: loss 0.3687
[2019-04-07 14:33:11,630] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 58000, global step 912326: learning rate 0.0000
[2019-04-07 14:33:12,901] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:33:12,901] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:33:12,957] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run21
[2019-04-07 14:33:15,597] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:33:15,597] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:33:15,601] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run21
[2019-04-07 14:33:20,628] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7391316e-24 1.7175143e-23 6.4494006e-22 3.3355832e-14 8.9021335e-22
 1.0000000e+00 4.8682804e-15 7.1648304e-16], sum to 1.0000
[2019-04-07 14:33:20,628] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2520
[2019-04-07 14:33:20,710] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 24.0, 22.63813109408801, -0.2743200981091619, 0.0, 1.0, 45319.123784556105], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2615400.0000, 
sim time next is 2617200.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 24.0, 22.6837774763049, -0.2809545880962885, 0.0, 1.0, 45641.14336312675], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.79, 0.0, 0.0, 0.5, 0.39031478969207506, 0.4063484706345705, 0.0, 1.0, 0.2173387779196512], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.397188], dtype=float32), 0.37069744]. 
=============================================
[2019-04-07 14:33:20,779] A3C_AGENT_WORKER-Thread-12 INFO:Local step 58000, global step 913345: loss 0.3157
[2019-04-07 14:33:20,779] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 58000, global step 913345: learning rate 0.0000
[2019-04-07 14:33:26,333] A3C_AGENT_WORKER-Thread-19 INFO:Local step 57500, global step 913934: loss 0.6156
[2019-04-07 14:33:26,335] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 57500, global step 913934: learning rate 0.0000
[2019-04-07 14:33:32,952] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3649930e-23 3.9408137e-23 8.5994965e-21 1.0145914e-12 2.7347916e-20
 1.0000000e+00 2.8280031e-13 1.1941167e-14], sum to 1.0000
[2019-04-07 14:33:32,953] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4207
[2019-04-07 14:33:33,002] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.95, 43.0, 0.0, 0.0, 24.0, 23.40553207336481, -0.1410264286886768, 0.0, 1.0, 33890.991398210805], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2410200.0000, 
sim time next is 2412000.0000, 
raw observation next is [-4.5, 44.0, 0.0, 0.0, 24.0, 23.32005425829166, -0.1409931016691199, 0.0, 1.0, 56662.907995288784], 
processed observation next is [0.0, 0.9565217391304348, 0.3379501385041552, 0.44, 0.0, 0.0, 0.5, 0.44333785485763827, 0.4530022994436267, 0.0, 1.0, 0.26982337140613705], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9574572], dtype=float32), -1.202711]. 
=============================================
[2019-04-07 14:33:33,025] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[88.99349 ]
 [89.595276]
 [89.38522 ]
 [89.92575 ]
 [90.09594 ]], R is [[88.91309357]
 [89.02396393]
 [89.09012604]
 [89.19922638]
 [89.30723572]].
[2019-04-07 14:33:34,250] A3C_AGENT_WORKER-Thread-6 INFO:Local step 58000, global step 914884: loss 0.3565
[2019-04-07 14:33:34,264] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 58000, global step 914884: learning rate 0.0000
[2019-04-07 14:33:44,853] A3C_AGENT_WORKER-Thread-14 INFO:Local step 57000, global step 916096: loss 0.3201
[2019-04-07 14:33:44,854] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 57000, global step 916096: learning rate 0.0000
[2019-04-07 14:33:49,508] A3C_AGENT_WORKER-Thread-3 INFO:Local step 57000, global step 916686: loss 0.3276
[2019-04-07 14:33:49,508] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 57000, global step 916686: learning rate 0.0000
[2019-04-07 14:33:49,691] A3C_AGENT_WORKER-Thread-18 INFO:Local step 57000, global step 916706: loss 0.3959
[2019-04-07 14:33:49,693] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 57000, global step 916706: learning rate 0.0000
[2019-04-07 14:33:51,686] A3C_AGENT_WORKER-Thread-16 INFO:Local step 58000, global step 916987: loss 0.3049
[2019-04-07 14:33:51,686] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 58000, global step 916987: learning rate 0.0000
[2019-04-07 14:33:53,768] A3C_AGENT_WORKER-Thread-20 INFO:Local step 57000, global step 917302: loss 0.3523
[2019-04-07 14:33:53,769] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 57000, global step 917302: learning rate 0.0000
[2019-04-07 14:33:54,051] A3C_AGENT_WORKER-Thread-2 INFO:Local step 57000, global step 917344: loss 0.3043
[2019-04-07 14:33:54,051] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 57000, global step 917344: learning rate 0.0000
[2019-04-07 14:33:55,317] A3C_AGENT_WORKER-Thread-11 INFO:Local step 57000, global step 917492: loss 0.2874
[2019-04-07 14:33:55,317] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 57000, global step 917492: learning rate 0.0000
[2019-04-07 14:33:58,665] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.7974686e-29 6.6618564e-27 1.0470346e-25 3.7348050e-15 9.7068340e-24
 1.0000000e+00 4.1270857e-16 5.9338455e-18], sum to 1.0000
[2019-04-07 14:33:58,665] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0600
[2019-04-07 14:33:58,701] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.5, 49.0, 0.0, 24.0, 23.74070156477827, -0.07704307986367968, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2885400.0000, 
sim time next is 2887200.0000, 
raw observation next is [0.0, 100.0, 63.5, 0.0, 24.0, 23.84176120446013, -0.07821420100206315, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 1.0, 0.21166666666666667, 0.0, 0.5, 0.4868134337050109, 0.47392859966597894, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20709264], dtype=float32), 0.55956876]. 
=============================================
[2019-04-07 14:34:00,217] A3C_AGENT_WORKER-Thread-4 INFO:Local step 57000, global step 918196: loss 0.2953
[2019-04-07 14:34:00,218] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 57000, global step 918196: learning rate 0.0000
[2019-04-07 14:34:00,732] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.85649023e-26 1.58252874e-27 1.11598464e-23 5.11436545e-15
 3.96495501e-23 1.00000000e+00 1.63743378e-14 4.72223978e-17], sum to 1.0000
[2019-04-07 14:34:00,733] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1281
[2019-04-07 14:34:00,743] A3C_AGENT_WORKER-Thread-15 INFO:Local step 58500, global step 918274: loss 0.7140
[2019-04-07 14:34:00,743] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 58500, global step 918274: learning rate 0.0000
[2019-04-07 14:34:00,798] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 24.0, 23.4173075513352, -0.03223728514474519, 0.0, 1.0, 31675.849921006156], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2937600.0000, 
sim time next is 2939400.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 24.0, 23.31601813455737, -0.04037628385375944, 0.0, 1.0, 54765.47827327005], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.5, 0.44300151121311426, 0.48654123871541355, 0.0, 1.0, 0.2607879917774764], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43561566], dtype=float32), 0.38573858]. 
=============================================
[2019-04-07 14:34:02,615] A3C_AGENT_WORKER-Thread-17 INFO:Local step 57000, global step 918536: loss 0.2712
[2019-04-07 14:34:02,616] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 57000, global step 918536: learning rate 0.0000
[2019-04-07 14:34:04,872] A3C_AGENT_WORKER-Thread-5 INFO:Local step 57000, global step 918863: loss 0.3461
[2019-04-07 14:34:04,897] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 57000, global step 918863: learning rate 0.0000
[2019-04-07 14:34:09,254] A3C_AGENT_WORKER-Thread-12 INFO:Local step 58500, global step 919567: loss 0.6523
[2019-04-07 14:34:09,255] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 58500, global step 919567: learning rate 0.0000
[2019-04-07 14:34:11,541] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-07 14:34:11,542] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:34:11,543] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:34:11,550] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:34:11,551] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:34:11,553] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run47
[2019-04-07 14:34:11,572] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 14:34:11,580] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:34:11,582] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run47
[2019-04-07 14:34:11,599] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run47
[2019-04-07 14:36:34,008] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11279281], dtype=float32), 0.14015883]
[2019-04-07 14:36:34,009] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-11.86923402, 68.78331352, 0.0, 0.0, 24.0, 22.2707034948254, -0.2823932309208252, 0.0, 1.0, 45978.241296191256]
[2019-04-07 14:36:34,009] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 14:36:34,010] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [1.09934536e-23 1.20504398e-22 9.84662957e-21 6.56822253e-13
 4.60443884e-20 1.00000000e+00 1.42811408e-13 6.08208450e-15], sampled 0.7920957476800271
[2019-04-07 14:36:39,719] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 14:36:56,300] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 14:37:00,339] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 14:37:01,361] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 920000, evaluation results [920000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 14:37:03,071] A3C_AGENT_WORKER-Thread-10 INFO:Local step 57000, global step 920329: loss 0.3248
[2019-04-07 14:37:03,077] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 57000, global step 920329: learning rate 0.0000
[2019-04-07 14:37:03,920] A3C_AGENT_WORKER-Thread-13 INFO:Local step 57000, global step 920501: loss 0.3409
[2019-04-07 14:37:03,924] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 57000, global step 920502: learning rate 0.0000
[2019-04-07 14:37:10,597] A3C_AGENT_WORKER-Thread-6 INFO:Local step 58500, global step 921998: loss 0.6907
[2019-04-07 14:37:10,597] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 58500, global step 921998: learning rate 0.0000
[2019-04-07 14:37:12,783] A3C_AGENT_WORKER-Thread-19 INFO:Local step 58000, global step 922504: loss 0.2963
[2019-04-07 14:37:12,783] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 58000, global step 922504: learning rate 0.0000
[2019-04-07 14:37:15,043] A3C_AGENT_WORKER-Thread-14 INFO:Local step 57500, global step 922974: loss 0.6421
[2019-04-07 14:37:15,044] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 57500, global step 922974: learning rate 0.0000
[2019-04-07 14:37:19,504] A3C_AGENT_WORKER-Thread-3 INFO:Local step 57500, global step 923999: loss 0.6318
[2019-04-07 14:37:19,506] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 57500, global step 923999: learning rate 0.0000
[2019-04-07 14:37:20,519] A3C_AGENT_WORKER-Thread-18 INFO:Local step 57500, global step 924215: loss 0.6787
[2019-04-07 14:37:20,520] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 57500, global step 924215: learning rate 0.0000
[2019-04-07 14:37:23,168] A3C_AGENT_WORKER-Thread-2 INFO:Local step 57500, global step 924800: loss 0.6398
[2019-04-07 14:37:23,172] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 57500, global step 924800: learning rate 0.0000
[2019-04-07 14:37:23,627] A3C_AGENT_WORKER-Thread-11 INFO:Local step 57500, global step 924922: loss 0.6471
[2019-04-07 14:37:23,631] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 57500, global step 924922: learning rate 0.0000
[2019-04-07 14:37:24,383] A3C_AGENT_WORKER-Thread-20 INFO:Local step 57500, global step 925090: loss 0.6525
[2019-04-07 14:37:24,384] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 57500, global step 925090: learning rate 0.0000
[2019-04-07 14:37:25,564] A3C_AGENT_WORKER-Thread-16 INFO:Local step 58500, global step 925353: loss 0.7385
[2019-04-07 14:37:25,564] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 58500, global step 925353: learning rate 0.0000
[2019-04-07 14:37:26,814] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.6521993e-29 4.6865911e-28 4.2839933e-26 6.7841198e-16 8.0814564e-24
 1.0000000e+00 1.0477746e-16 1.1956914e-18], sum to 1.0000
[2019-04-07 14:37:26,814] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3998
[2019-04-07 14:37:26,866] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 94.0, 0.0, 0.0, 24.0, 23.53039086267809, 0.007163942698893829, 0.0, 1.0, 24773.71597038143], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1479600.0000, 
sim time next is 1481400.0000, 
raw observation next is [2.2, 95.0, 0.0, 0.0, 24.0, 23.55481266233096, 0.03715990927240905, 0.0, 1.0, 51436.45873840485], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.95, 0.0, 0.0, 0.5, 0.4629010551942467, 0.5123866364241364, 0.0, 1.0, 0.24493551780192785], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.86974484], dtype=float32), -0.5579528]. 
=============================================
[2019-04-07 14:37:28,897] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0093708e-22 4.9029863e-21 2.9071767e-19 1.6902318e-12 1.0680968e-18
 1.0000000e+00 6.5952251e-13 3.9254604e-14], sum to 1.0000
[2019-04-07 14:37:28,898] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0716
[2019-04-07 14:37:28,954] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.1, 83.0, 0.0, 0.0, 24.0, 22.78886061268053, -0.04391297069923421, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1216800.0000, 
sim time next is 1218600.0000, 
raw observation next is [15.8, 88.0, 0.0, 0.0, 24.0, 22.81855099048718, -0.04314008577746372, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9002770083102495, 0.88, 0.0, 0.0, 0.5, 0.40154591587393157, 0.48561997140751206, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2844834], dtype=float32), 0.88584495]. 
=============================================
[2019-04-07 14:37:28,993] A3C_AGENT_WORKER-Thread-15 INFO:Local step 59000, global step 926063: loss 0.2199
[2019-04-07 14:37:29,000] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 59000, global step 926063: learning rate 0.0000
[2019-04-07 14:37:29,637] A3C_AGENT_WORKER-Thread-4 INFO:Local step 57500, global step 926190: loss 0.6541
[2019-04-07 14:37:29,648] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 57500, global step 926190: learning rate 0.0000
[2019-04-07 14:37:31,041] A3C_AGENT_WORKER-Thread-17 INFO:Local step 57500, global step 926436: loss 0.6242
[2019-04-07 14:37:31,073] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 57500, global step 926441: learning rate 0.0000
[2019-04-07 14:37:32,531] A3C_AGENT_WORKER-Thread-5 INFO:Local step 57500, global step 926733: loss 0.6442
[2019-04-07 14:37:32,532] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 57500, global step 926733: learning rate 0.0000
[2019-04-07 14:37:34,189] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9180145e-28 3.7068489e-27 4.3546780e-26 4.0436980e-15 3.1563060e-23
 1.0000000e+00 3.9199851e-17 2.6138130e-18], sum to 1.0000
[2019-04-07 14:37:34,189] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7934
[2019-04-07 14:37:34,300] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 86.0, 107.0, 0.0, 24.0, 23.89504078958651, 0.03861936001006961, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1686600.0000, 
sim time next is 1688400.0000, 
raw observation next is [1.1, 88.0, 103.5, 0.0, 24.0, 23.27542292811013, -0.02199863861357612, 1.0, 1.0, 65741.18854561479], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.345, 0.0, 0.5, 0.439618577342511, 0.4926671204621413, 1.0, 1.0, 0.31305327878864186], 
reward next is 0.9727, 
noisyNet noise sample is [array([-1.3646305], dtype=float32), 2.7612596]. 
=============================================
[2019-04-07 14:37:35,253] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.1970883e-29 2.8371459e-27 1.4340019e-24 1.8941734e-15 2.9287859e-24
 1.0000000e+00 7.5805415e-16 3.6869533e-18], sum to 1.0000
[2019-04-07 14:37:35,254] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4629
[2019-04-07 14:37:35,349] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.35, 84.5, 30.0, 0.0, 24.0, 23.06130366464045, -0.0330621547616796, 1.0, 1.0, 54049.409767982295], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1701000.0000, 
sim time next is 1702800.0000, 
raw observation next is [1.1, 88.0, 15.5, 0.0, 24.0, 23.84589938960486, 0.0575699558386282, 1.0, 1.0, 3113.4019450384226], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.051666666666666666, 0.0, 0.5, 0.48715828246707166, 0.5191899852795427, 1.0, 1.0, 0.014825723547802013], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.5200193], dtype=float32), 1.2408521]. 
=============================================
[2019-04-07 14:37:38,661] A3C_AGENT_WORKER-Thread-12 INFO:Local step 59000, global step 927757: loss 0.2140
[2019-04-07 14:37:38,665] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 59000, global step 927757: learning rate 0.0000
[2019-04-07 14:37:42,152] A3C_AGENT_WORKER-Thread-13 INFO:Local step 57500, global step 928279: loss 0.6277
[2019-04-07 14:37:42,182] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 57500, global step 928279: learning rate 0.0000
[2019-04-07 14:37:42,220] A3C_AGENT_WORKER-Thread-10 INFO:Local step 57500, global step 928287: loss 0.6180
[2019-04-07 14:37:42,221] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 57500, global step 928287: learning rate 0.0000
[2019-04-07 14:37:50,268] A3C_AGENT_WORKER-Thread-6 INFO:Local step 59000, global step 929551: loss 0.2239
[2019-04-07 14:37:50,269] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 59000, global step 929551: learning rate 0.0000
[2019-04-07 14:37:50,987] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9405049e-26 2.2222194e-25 9.5923553e-24 3.7569964e-15 1.1491419e-22
 1.0000000e+00 1.2794788e-14 5.6543903e-17], sum to 1.0000
[2019-04-07 14:37:50,990] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9615
[2019-04-07 14:37:51,174] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 102.0, 0.0, 24.0, 23.75714627800347, -0.1331307084141234, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2025000.0000, 
sim time next is 2026800.0000, 
raw observation next is [-5.6, 83.0, 124.5, 0.0, 24.0, 23.70258464824114, -0.1088827872406592, 1.0, 1.0, 47522.701166689396], 
processed observation next is [1.0, 0.4782608695652174, 0.30747922437673136, 0.83, 0.415, 0.0, 0.5, 0.4752153873534282, 0.4637057375864469, 1.0, 1.0, 0.22629857698423522], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36784288], dtype=float32), -1.2326207]. 
=============================================
[2019-04-07 14:37:51,434] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.8058886e-25 9.8973787e-24 8.9598855e-21 9.2297297e-14 1.0399003e-20
 1.0000000e+00 2.5524269e-14 2.0658860e-15], sum to 1.0000
[2019-04-07 14:37:51,434] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2695
[2019-04-07 14:37:51,505] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 79.0, 0.0, 0.0, 24.0, 22.99928992485471, -0.228413327128776, 0.0, 1.0, 46079.79786444249], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1897200.0000, 
sim time next is 1899000.0000, 
raw observation next is [-7.3, 80.5, 0.0, 0.0, 24.0, 22.90280793429552, -0.2473526244445009, 0.0, 1.0, 46038.17967635431], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.805, 0.0, 0.0, 0.5, 0.40856732785796, 0.4175491251851664, 0.0, 1.0, 0.2192294270302586], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1347448], dtype=float32), -0.22925392]. 
=============================================
[2019-04-07 14:37:51,515] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[91.82065]
 [92.66934]
 [93.46139]
 [94.20429]
 [94.68953]], R is [[91.28536224]
 [91.37251282]
 [91.45878601]
 [91.54419708]
 [91.62875366]].
[2019-04-07 14:37:54,569] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4480282e-25 5.7847925e-25 1.3643690e-22 6.8258557e-14 5.7033124e-22
 1.0000000e+00 7.5968725e-15 3.0943951e-16], sum to 1.0000
[2019-04-07 14:37:54,569] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6531
[2019-04-07 14:37:54,630] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 65.5, 0.0, 0.0, 24.0, 23.35485556602731, -0.1305444069801985, 0.0, 1.0, 49042.197293770725], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4858200.0000, 
sim time next is 4860000.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 24.0, 23.30090775903318, -0.1372393776493357, 0.0, 1.0, 42725.609425066265], 
processed observation next is [0.0, 0.2608695652173913, 0.3795013850415513, 0.6, 0.0, 0.0, 0.5, 0.441742313252765, 0.4542535407835548, 0.0, 1.0, 0.20345528297650603], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01622041], dtype=float32), 0.48367223]. 
=============================================
[2019-04-07 14:37:54,653] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[92.05965 ]
 [92.02319 ]
 [91.924835]
 [91.93219 ]
 [92.13034 ]], R is [[91.97039032]
 [92.0506897 ]
 [92.13018036]
 [92.20887756]
 [92.28678894]].
[2019-04-07 14:38:00,029] A3C_AGENT_WORKER-Thread-19 INFO:Local step 58500, global step 930962: loss 0.7020
[2019-04-07 14:38:00,069] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 58500, global step 930962: learning rate 0.0000
[2019-04-07 14:38:03,596] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6600097e-28 1.7407551e-27 8.3566452e-25 1.5723740e-16 1.4542966e-24
 1.0000000e+00 1.0802966e-16 1.2352667e-18], sum to 1.0000
[2019-04-07 14:38:03,596] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5411
[2019-04-07 14:38:03,640] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.5, 17.0, 36.0, 292.0, 24.0, 27.18146776325986, 0.7316410498899925, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5074200.0000, 
sim time next is 5076000.0000, 
raw observation next is [11.0, 17.0, 18.0, 146.0, 24.0, 26.89857220996055, 0.7178419215706858, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.06, 0.16132596685082873, 0.5, 0.7415476841633794, 0.739280640523562, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39502388], dtype=float32), 0.79450697]. 
=============================================
[2019-04-07 14:38:03,674] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[107.589325]
 [107.57832 ]
 [107.184006]
 [106.633804]
 [105.76986 ]], R is [[107.18100739]
 [107.10919952]
 [107.03810883]
 [106.96772766]
 [106.8980484 ]].
[2019-04-07 14:38:05,485] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:38:05,485] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:38:05,489] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run22
[2019-04-07 14:38:06,317] A3C_AGENT_WORKER-Thread-16 INFO:Local step 59000, global step 931867: loss 0.2248
[2019-04-07 14:38:06,318] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 59000, global step 931867: learning rate 0.0000
[2019-04-07 14:38:11,277] A3C_AGENT_WORKER-Thread-14 INFO:Local step 58000, global step 932546: loss 0.3243
[2019-04-07 14:38:11,277] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 58000, global step 932546: learning rate 0.0000
[2019-04-07 14:38:15,693] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:38:15,693] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:38:15,699] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run22
[2019-04-07 14:38:16,749] A3C_AGENT_WORKER-Thread-3 INFO:Local step 58000, global step 933311: loss 0.2889
[2019-04-07 14:38:16,749] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 58000, global step 933311: learning rate 0.0000
[2019-04-07 14:38:19,141] A3C_AGENT_WORKER-Thread-18 INFO:Local step 58000, global step 933614: loss 0.2663
[2019-04-07 14:38:19,177] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 58000, global step 933617: learning rate 0.0000
[2019-04-07 14:38:21,236] A3C_AGENT_WORKER-Thread-2 INFO:Local step 58000, global step 933878: loss 0.2960
[2019-04-07 14:38:21,236] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 58000, global step 933878: learning rate 0.0000
[2019-04-07 14:38:22,702] A3C_AGENT_WORKER-Thread-11 INFO:Local step 58000, global step 934101: loss 0.2675
[2019-04-07 14:38:22,712] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 58000, global step 934101: learning rate 0.0000
[2019-04-07 14:38:24,025] A3C_AGENT_WORKER-Thread-20 INFO:Local step 58000, global step 934319: loss 0.2328
[2019-04-07 14:38:24,027] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 58000, global step 934319: learning rate 0.0000
[2019-04-07 14:38:27,358] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:38:27,358] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:38:27,362] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run22
[2019-04-07 14:38:29,221] A3C_AGENT_WORKER-Thread-4 INFO:Local step 58000, global step 935037: loss 0.3295
[2019-04-07 14:38:29,222] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 58000, global step 935037: learning rate 0.0000
[2019-04-07 14:38:29,410] A3C_AGENT_WORKER-Thread-17 INFO:Local step 58000, global step 935069: loss 0.3254
[2019-04-07 14:38:29,410] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 58000, global step 935069: learning rate 0.0000
[2019-04-07 14:38:31,170] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3772290e-24 3.9591667e-23 1.1060215e-22 1.2683559e-13 7.0193472e-21
 1.0000000e+00 8.4449957e-14 8.2138648e-15], sum to 1.0000
[2019-04-07 14:38:31,170] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7565
[2019-04-07 14:38:31,213] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 24.0, 22.0751518314103, -0.3656284148473676, 0.0, 1.0, 45023.87912415794], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2696400.0000, 
sim time next is 2698200.0000, 
raw observation next is [-15.5, 83.0, 0.0, 0.0, 24.0, 22.05018199442735, -0.3859859223279781, 0.0, 1.0, 44607.5736191322], 
processed observation next is [1.0, 0.21739130434782608, 0.033240997229916885, 0.83, 0.0, 0.0, 0.5, 0.3375151662022793, 0.37133802589067394, 0.0, 1.0, 0.21241701723396286], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5780804], dtype=float32), -1.1134772]. 
=============================================
[2019-04-07 14:38:32,057] A3C_AGENT_WORKER-Thread-5 INFO:Local step 58000, global step 935431: loss 0.3108
[2019-04-07 14:38:32,060] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 58000, global step 935431: learning rate 0.0000
[2019-04-07 14:38:37,318] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.58482938e-26 1.09429104e-23 1.32640827e-22 5.47956645e-13
 5.77580478e-22 1.00000000e+00 1.38421193e-14 1.45892087e-15], sum to 1.0000
[2019-04-07 14:38:37,319] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2902
[2019-04-07 14:38:37,370] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.2, 60.5, 0.0, 0.0, 24.0, 23.30460751739169, -0.09724213867076931, 0.0, 1.0, 45132.81813892839], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2590200.0000, 
sim time next is 2592000.0000, 
raw observation next is [-4.5, 62.0, 0.0, 0.0, 24.0, 23.30654092042915, -0.1082997091070617, 0.0, 1.0, 43795.08218922827], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.62, 0.0, 0.0, 0.5, 0.4422117433690958, 0.4639000969643128, 0.0, 1.0, 0.2085480104248965], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7943783], dtype=float32), 0.7602149]. 
=============================================
[2019-04-07 14:38:37,374] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[96.94131]
 [96.90889]
 [96.46701]
 [95.95194]
 [95.20621]], R is [[96.2711792 ]
 [96.30847168]
 [96.29190826]
 [96.32898712]
 [96.36569977]].
[2019-04-07 14:38:38,049] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5928082e-25 7.4767127e-26 1.5201481e-23 2.7578056e-14 3.6483467e-21
 1.0000000e+00 1.6998135e-13 1.7998095e-15], sum to 1.0000
[2019-04-07 14:38:38,050] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9766
[2019-04-07 14:38:38,091] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 24.0, 24.70659634722935, 0.347115090892961, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4993200.0000, 
sim time next is 4995000.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 24.0, 24.91532728261967, 0.3369576284633819, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.6288088642659281, 0.23, 0.0, 0.0, 0.5, 0.5762772735516393, 0.612319209487794, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6310924], dtype=float32), -1.3620749]. 
=============================================
[2019-04-07 14:38:38,113] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[102.01722]
 [103.32402]
 [104.53839]
 [105.3476 ]
 [105.82523]], R is [[99.98529816]
 [99.98544312]
 [99.98558807]
 [99.98573303]
 [99.98587799]].
[2019-04-07 14:38:41,088] A3C_AGENT_WORKER-Thread-13 INFO:Local step 58000, global step 936746: loss 0.3612
[2019-04-07 14:38:41,089] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 58000, global step 936746: learning rate 0.0000
[2019-04-07 14:38:42,256] A3C_AGENT_WORKER-Thread-10 INFO:Local step 58000, global step 936913: loss 0.2479
[2019-04-07 14:38:42,257] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 58000, global step 936913: learning rate 0.0000
[2019-04-07 14:38:42,374] A3C_AGENT_WORKER-Thread-19 INFO:Local step 59000, global step 936929: loss 0.1246
[2019-04-07 14:38:42,375] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 59000, global step 936929: learning rate 0.0000
[2019-04-07 14:38:42,893] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:38:42,893] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:38:42,896] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run22
[2019-04-07 14:38:48,577] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5567638e-25 3.6643394e-24 2.0417249e-22 5.5665791e-13 3.5763352e-21
 1.0000000e+00 4.9528761e-14 4.4879164e-15], sum to 1.0000
[2019-04-07 14:38:48,577] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7660
[2019-04-07 14:38:48,662] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 76.0, 0.0, 0.0, 24.0, 22.37539435312499, -0.3254222952481883, 0.0, 1.0, 45058.939703420845], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 181800.0000, 
sim time next is 183600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 24.0, 22.32076404809455, -0.3549962237800166, 0.0, 1.0, 45075.52045408633], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.5, 0.36006367067454575, 0.3816679254066611, 0.0, 1.0, 0.21464533549564918], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43844712], dtype=float32), 0.9590967]. 
=============================================
[2019-04-07 14:38:52,569] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1225537e-24 3.2748620e-22 4.1034666e-20 1.8767874e-13 1.0032916e-19
 1.0000000e+00 7.5716950e-13 2.9541711e-14], sum to 1.0000
[2019-04-07 14:38:52,570] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1743
[2019-04-07 14:38:52,698] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 73.5, 0.0, 0.0, 24.0, 22.58842393773816, -0.2986324586104189, 0.0, 1.0, 41134.77282271455], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3047400.0000, 
sim time next is 3049200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 24.0, 22.6196096617487, -0.3013521410111343, 0.0, 1.0, 41276.759986502904], 
processed observation next is [0.0, 0.30434782608695654, 0.296398891966759, 0.77, 0.0, 0.0, 0.5, 0.3849674718123917, 0.3995492863296219, 0.0, 1.0, 0.19655599993572812], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3351853], dtype=float32), -1.4386117]. 
=============================================
[2019-04-07 14:38:58,398] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.03057045e-24 1.04428334e-23 9.23893899e-22 1.09933357e-14
 4.80496374e-21 1.00000000e+00 2.03781524e-14 4.89102446e-16], sum to 1.0000
[2019-04-07 14:38:58,398] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1090
[2019-04-07 14:38:58,475] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 73.5, 0.0, 0.0, 24.0, 22.71540219626197, -0.269698762372358, 0.0, 1.0, 41085.770503366024], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3043800.0000, 
sim time next is 3045600.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 24.0, 22.65489570911155, -0.2852471026731074, 0.0, 1.0, 41062.28424592698], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.5, 0.3879079757592958, 0.40491763244229756, 0.0, 1.0, 0.19553468688536657], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7206126], dtype=float32), 1.2546508]. 
=============================================
[2019-04-07 14:38:58,605] A3C_AGENT_WORKER-Thread-14 INFO:Local step 58500, global step 939396: loss 0.6288
[2019-04-07 14:38:58,612] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 58500, global step 939396: learning rate 0.0000
[2019-04-07 14:39:02,146] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-07 14:39:02,146] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:39:02,147] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:39:02,149] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run48
[2019-04-07 14:39:02,170] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:39:02,172] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:39:02,172] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 14:39:02,175] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run48
[2019-04-07 14:39:02,194] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:39:02,198] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run48
[2019-04-07 14:41:24,355] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 14:41:41,501] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 14:41:44,279] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 14:41:45,302] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 940000, evaluation results [940000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 14:41:48,735] A3C_AGENT_WORKER-Thread-3 INFO:Local step 58500, global step 940608: loss 0.7104
[2019-04-07 14:41:48,769] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 58500, global step 940608: learning rate 0.0000
[2019-04-07 14:41:50,866] A3C_AGENT_WORKER-Thread-18 INFO:Local step 58500, global step 940982: loss 0.6853
[2019-04-07 14:41:50,866] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 58500, global step 940982: learning rate 0.0000
[2019-04-07 14:41:53,745] A3C_AGENT_WORKER-Thread-2 INFO:Local step 58500, global step 941470: loss 0.6659
[2019-04-07 14:41:53,758] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 58500, global step 941470: learning rate 0.0000
[2019-04-07 14:41:54,476] A3C_AGENT_WORKER-Thread-20 INFO:Local step 58500, global step 941600: loss 0.7044
[2019-04-07 14:41:54,478] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 58500, global step 941600: learning rate 0.0000
[2019-04-07 14:41:55,528] A3C_AGENT_WORKER-Thread-11 INFO:Local step 58500, global step 941779: loss 0.6605
[2019-04-07 14:41:55,528] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 58500, global step 941779: learning rate 0.0000
[2019-04-07 14:41:58,204] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1820273e-26 5.1379836e-24 1.3721409e-22 7.2743366e-14 3.6569998e-21
 1.0000000e+00 7.6462437e-14 5.8592441e-15], sum to 1.0000
[2019-04-07 14:41:58,204] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2471
[2019-04-07 14:41:58,269] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.5, 67.0, 111.0, 740.0, 24.0, 24.74207393733368, 0.2046653610023572, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3321000.0000, 
sim time next is 3322800.0000, 
raw observation next is [-7.0, 64.0, 113.5, 769.0, 24.0, 24.8132107484019, 0.2243841986288489, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.2686980609418283, 0.64, 0.37833333333333335, 0.8497237569060774, 0.5, 0.567767562366825, 0.5747947328762829, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6796076], dtype=float32), 0.9489977]. 
=============================================
[2019-04-07 14:41:59,246] A3C_AGENT_WORKER-Thread-4 INFO:Local step 58500, global step 942420: loss 0.7064
[2019-04-07 14:41:59,247] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 58500, global step 942420: learning rate 0.0000
[2019-04-07 14:42:01,309] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:42:01,309] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:42:01,319] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run22
[2019-04-07 14:42:01,349] A3C_AGENT_WORKER-Thread-17 INFO:Local step 58500, global step 942799: loss 0.6945
[2019-04-07 14:42:01,351] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 58500, global step 942800: learning rate 0.0000
[2019-04-07 14:42:02,189] A3C_AGENT_WORKER-Thread-5 INFO:Local step 58500, global step 942936: loss 0.6909
[2019-04-07 14:42:02,197] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 58500, global step 942936: learning rate 0.0000
[2019-04-07 14:42:10,194] A3C_AGENT_WORKER-Thread-13 INFO:Local step 58500, global step 944408: loss 0.6715
[2019-04-07 14:42:10,195] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 58500, global step 944408: learning rate 0.0000
[2019-04-07 14:42:12,623] A3C_AGENT_WORKER-Thread-10 INFO:Local step 58500, global step 944876: loss 0.6769
[2019-04-07 14:42:12,623] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 58500, global step 944876: learning rate 0.0000
[2019-04-07 14:42:16,918] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5778934e-25 6.2839129e-24 1.0985411e-22 5.8077207e-14 1.5590153e-21
 1.0000000e+00 6.2205240e-15 1.3010648e-16], sum to 1.0000
[2019-04-07 14:42:16,918] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0820
[2019-04-07 14:42:16,955] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 85.0, 0.0, 0.0, 24.0, 23.43189308175281, -0.1005287342408963, 0.0, 1.0, 27024.55374064621], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 592200.0000, 
sim time next is 594000.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 24.0, 23.38182563113594, -0.1067836364804331, 0.0, 1.0, 57863.5967515476], 
processed observation next is [0.0, 0.9130434782608695, 0.38504155124653744, 0.83, 0.0, 0.0, 0.5, 0.44848546926132826, 0.4644054545065223, 0.0, 1.0, 0.27554093691213144], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.79484785], dtype=float32), 0.58968896]. 
=============================================
[2019-04-07 14:42:16,958] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[99.44912 ]
 [99.612236]
 [99.461525]
 [98.50383 ]
 [98.31987 ]], R is [[99.55445862]
 [99.55891418]
 [99.56332397]
 [99.31687927]
 [99.32371521]].
[2019-04-07 14:42:21,684] A3C_AGENT_WORKER-Thread-14 INFO:Local step 59000, global step 946600: loss 0.1861
[2019-04-07 14:42:21,685] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 59000, global step 946600: learning rate 0.0000
[2019-04-07 14:42:26,643] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7397288e-24 3.2953244e-23 1.4755780e-22 1.9950396e-12 7.4739982e-20
 1.0000000e+00 7.6155877e-15 1.7443486e-15], sum to 1.0000
[2019-04-07 14:42:26,644] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2789
[2019-04-07 14:42:26,695] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 29.0, 116.0, 835.5, 24.0, 24.38696325031299, 0.164195394845628, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4021200.0000, 
sim time next is 4023000.0000, 
raw observation next is [-3.5, 27.5, 114.0, 830.0, 24.0, 24.44564599173941, 0.1976982248514879, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.36565096952908593, 0.275, 0.38, 0.9171270718232044, 0.5, 0.5371371659782843, 0.5658994082838293, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0254667], dtype=float32), 0.057760186]. 
=============================================
[2019-04-07 14:42:26,701] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[93.70035 ]
 [93.830795]
 [93.59251 ]
 [93.11833 ]
 [92.711365]], R is [[93.63955688]
 [93.70316315]
 [93.76613617]
 [93.82847595]
 [93.89019012]].
[2019-04-07 14:42:28,742] A3C_AGENT_WORKER-Thread-3 INFO:Local step 59000, global step 947981: loss 0.1930
[2019-04-07 14:42:28,743] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 59000, global step 947981: learning rate 0.0000
[2019-04-07 14:42:32,055] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6419393e-27 2.7344911e-26 4.7810958e-24 9.0791649e-15 1.1999791e-23
 1.0000000e+00 1.2655509e-16 1.2175234e-16], sum to 1.0000
[2019-04-07 14:42:32,056] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9287
[2019-04-07 14:42:32,097] A3C_AGENT_WORKER-Thread-18 INFO:Local step 59000, global step 948641: loss 0.1462
[2019-04-07 14:42:32,097] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 59000, global step 948641: learning rate 0.0000
[2019-04-07 14:42:32,222] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.9, 72.0, 0.0, 0.0, 24.0, 23.49317660479851, -0.04282111465504116, 1.0, 1.0, 12904.204287834558], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4519800.0000, 
sim time next is 4521600.0000, 
raw observation next is [-0.8, 73.0, 55.5, 33.0, 24.0, 23.4932473773906, -0.03744910280446483, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4404432132963989, 0.73, 0.185, 0.036464088397790057, 0.5, 0.45777061478254993, 0.48751696573184505, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7414302], dtype=float32), -0.49596083]. 
=============================================
[2019-04-07 14:42:33,577] A3C_AGENT_WORKER-Thread-2 INFO:Local step 59000, global step 948949: loss 0.1486
[2019-04-07 14:42:33,579] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 59000, global step 948949: learning rate 0.0000
[2019-04-07 14:42:35,338] A3C_AGENT_WORKER-Thread-20 INFO:Local step 59000, global step 949350: loss 0.1778
[2019-04-07 14:42:35,339] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 59000, global step 949350: learning rate 0.0000
[2019-04-07 14:42:36,038] A3C_AGENT_WORKER-Thread-11 INFO:Local step 59000, global step 949483: loss 0.1390
[2019-04-07 14:42:36,039] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 59000, global step 949483: learning rate 0.0000
[2019-04-07 14:42:38,354] A3C_AGENT_WORKER-Thread-4 INFO:Local step 59000, global step 949992: loss 0.1951
[2019-04-07 14:42:38,356] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 59000, global step 949993: learning rate 0.0000
[2019-04-07 14:42:41,293] A3C_AGENT_WORKER-Thread-17 INFO:Local step 59000, global step 950660: loss 0.1659
[2019-04-07 14:42:41,295] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 59000, global step 950660: learning rate 0.0000
[2019-04-07 14:42:42,833] A3C_AGENT_WORKER-Thread-5 INFO:Local step 59000, global step 950977: loss 0.1425
[2019-04-07 14:42:42,834] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 59000, global step 950977: learning rate 0.0000
[2019-04-07 14:42:44,928] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.0587346e-23 7.4005957e-22 9.8611820e-20 1.6537620e-13 6.7973438e-20
 1.0000000e+00 1.0214587e-13 3.9878935e-14], sum to 1.0000
[2019-04-07 14:42:44,928] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4833
[2019-04-07 14:42:44,967] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.25, 94.5, 0.0, 0.0, 24.0, 22.58318927339551, -0.0844884195907624, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1225800.0000, 
sim time next is 1227600.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 24.0, 22.51735502700479, -0.09686503966195081, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.5, 0.3764462522503991, 0.4677116534460164, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5169229], dtype=float32), -0.33332992]. 
=============================================
[2019-04-07 14:42:46,970] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9550132e-26 6.5919621e-26 2.4273556e-23 1.8937011e-14 1.0779674e-22
 1.0000000e+00 6.3238569e-16 3.1091363e-17], sum to 1.0000
[2019-04-07 14:42:46,970] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2020
[2019-04-07 14:42:47,045] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 24.0, 23.76364188381744, 0.0152002696096904, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4573800.0000, 
sim time next is 4575600.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 24.0, 23.45152771437547, 0.0158485152401201, 0.0, 1.0, 101417.6704872455], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.5, 0.4542939761979558, 0.5052828384133734, 0.0, 1.0, 0.48294128803450237], 
reward next is 0.8028, 
noisyNet noise sample is [array([-0.8096664], dtype=float32), -2.0977197]. 
=============================================
[2019-04-07 14:42:47,433] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.5351896e-26 2.7676639e-26 1.0581067e-21 1.7727631e-13 2.0767590e-22
 1.0000000e+00 4.3363540e-14 9.3839538e-17], sum to 1.0000
[2019-04-07 14:42:47,434] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3516
[2019-04-07 14:42:47,515] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.4, 63.0, 0.0, 0.0, 24.0, 23.54747983230766, -0.004891265456536838, 0.0, 1.0, 34632.017311252166], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4582800.0000, 
sim time next is 4584600.0000, 
raw observation next is [0.1, 64.0, 0.0, 0.0, 24.0, 23.55938374297265, -0.006342750981502175, 0.0, 1.0, 33824.51350030176], 
processed observation next is [1.0, 0.043478260869565216, 0.4653739612188367, 0.64, 0.0, 0.0, 0.5, 0.4632819785810541, 0.4978857496728326, 0.0, 1.0, 0.16106911190619885], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8227606], dtype=float32), -0.17916255]. 
=============================================
[2019-04-07 14:42:50,568] A3C_AGENT_WORKER-Thread-13 INFO:Local step 59000, global step 952611: loss 0.1763
[2019-04-07 14:42:50,568] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 59000, global step 952611: learning rate 0.0000
[2019-04-07 14:42:51,588] A3C_AGENT_WORKER-Thread-10 INFO:Local step 59000, global step 952830: loss 0.1412
[2019-04-07 14:42:51,612] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 59000, global step 952836: learning rate 0.0000
[2019-04-07 14:42:55,642] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5184698e-26 8.3373171e-25 5.7127191e-22 4.3492237e-15 2.5502427e-21
 1.0000000e+00 1.9137771e-14 3.8745690e-16], sum to 1.0000
[2019-04-07 14:42:55,643] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2149
[2019-04-07 14:42:55,665] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:42:55,665] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:42:55,669] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run22
[2019-04-07 14:42:55,695] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 24.0, 23.34720283240604, -0.0419226085312254, 0.0, 1.0, 48345.40777962766], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4759200.0000, 
sim time next is 4761000.0000, 
raw observation next is [-5.0, 81.5, 0.0, 0.0, 24.0, 23.35426962401434, -0.0474065465924707, 0.0, 1.0, 43821.75088820622], 
processed observation next is [0.0, 0.08695652173913043, 0.32409972299168976, 0.815, 0.0, 0.0, 0.5, 0.44618913533452825, 0.48419781780250976, 0.0, 1.0, 0.20867500422955343], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1705974], dtype=float32), 0.41982296]. 
=============================================
[2019-04-07 14:42:55,707] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[ 95.04002 ]
 [ 96.701164]
 [ 97.803894]
 [ 98.73889 ]
 [100.84407 ]], R is [[95.19792938]
 [95.24594879]
 [95.29348755]
 [95.34055328]
 [95.387146  ]].
[2019-04-07 14:43:03,659] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:43:03,659] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:43:03,676] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run22
[2019-04-07 14:43:04,640] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5474798e-28 3.3075986e-26 3.7538561e-24 4.7684016e-16 6.8787661e-24
 1.0000000e+00 1.2827867e-15 3.9967944e-18], sum to 1.0000
[2019-04-07 14:43:04,641] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5023
[2019-04-07 14:43:04,753] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.1, 64.0, 0.0, 0.0, 24.0, 23.55938374297265, -0.006342750981502175, 0.0, 1.0, 33824.51350030176], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4584600.0000, 
sim time next is 4586400.0000, 
raw observation next is [-0.2, 65.0, 0.0, 0.0, 24.0, 23.57257301308768, -0.002907918362704371, 0.0, 1.0, 26132.26609658479], 
processed observation next is [1.0, 0.08695652173913043, 0.4570637119113574, 0.65, 0.0, 0.0, 0.5, 0.46438108442397336, 0.49903069387909854, 0.0, 1.0, 0.12443936236468947], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02792495], dtype=float32), -0.19143447]. 
=============================================
[2019-04-07 14:43:06,257] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:43:06,257] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:43:06,261] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run22
[2019-04-07 14:43:08,251] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:43:08,251] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:43:08,255] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run22
[2019-04-07 14:43:11,730] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:43:11,730] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:43:11,734] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run22
[2019-04-07 14:43:11,754] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:43:11,755] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:43:11,770] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run22
[2019-04-07 14:43:12,297] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:43:12,297] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:43:12,301] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run22
[2019-04-07 14:43:16,137] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:43:16,145] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:43:16,148] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run22
[2019-04-07 14:43:19,563] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:43:19,563] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:43:19,567] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run22
[2019-04-07 14:43:26,521] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:43:26,521] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:43:26,524] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run22
[2019-04-07 14:43:30,325] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:43:30,325] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:43:30,328] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run22
[2019-04-07 14:43:43,390] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1855566e-25 9.1092658e-25 2.8362983e-22 2.6562869e-14 2.9747761e-21
 1.0000000e+00 1.1241421e-14 3.2126207e-16], sum to 1.0000
[2019-04-07 14:43:43,390] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3032
[2019-04-07 14:43:43,534] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 61.0, 41.0, 4.5, 24.0, 23.89826829710873, -0.06782267939304877, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 118800.0000, 
sim time next is 120600.0000, 
raw observation next is [-7.8, 67.5, 45.0, 0.0, 24.0, 23.88477838469396, -0.06940095662072605, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.675, 0.15, 0.0, 0.5, 0.4903981987244966, 0.4768663477930913, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9810288], dtype=float32), -0.30079395]. 
=============================================
[2019-04-07 14:43:44,533] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-07 14:43:44,539] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:43:44,539] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:43:44,542] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run49
[2019-04-07 14:43:44,562] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:43:44,563] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:43:44,564] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 14:43:44,567] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:43:44,567] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run49
[2019-04-07 14:43:44,570] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run49
[2019-04-07 14:44:08,957] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11303644], dtype=float32), 0.14135043]
[2019-04-07 14:44:08,957] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [1.1, 96.0, 0.0, 0.0, 24.0, 22.94991468450925, -0.2251486106711371, 1.0, 1.0, 40130.09982181786]
[2019-04-07 14:44:08,957] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 14:44:08,958] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [5.5606713e-27 6.8886853e-26 1.3674948e-23 6.0902334e-15 9.7053582e-23
 1.0000000e+00 9.0149346e-16 3.2261281e-17], sampled 0.3693922502714585
[2019-04-07 14:46:02,452] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11303644], dtype=float32), 0.14135043]
[2019-04-07 14:46:02,452] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [6.35, 86.5, 0.0, 0.0, 24.0, 23.66885056744732, 0.07659960440026896, 0.0, 1.0, 51937.52762081382]
[2019-04-07 14:46:02,452] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 14:46:02,453] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [1.5692425e-28 3.2700973e-27 5.6816538e-25 1.0917062e-15 3.8901780e-24
 1.0000000e+00 2.1121993e-16 5.2065229e-18], sampled 0.21557008805028555
[2019-04-07 14:46:08,358] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 14:46:24,284] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 14:46:27,123] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 14:46:28,159] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 960000, evaluation results [960000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 14:46:28,718] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2159432e-28 1.6250655e-26 2.2608434e-24 5.2042048e-16 1.8191014e-23
 1.0000000e+00 1.8506468e-16 6.1465893e-18], sum to 1.0000
[2019-04-07 14:46:28,719] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4869
[2019-04-07 14:46:28,752] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 89.0, 0.0, 0.0, 24.0, 23.42052219987976, -0.1034240629055504, 0.0, 1.0, 30182.63735706089], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 522000.0000, 
sim time next is 523800.0000, 
raw observation next is [4.65, 88.5, 0.0, 0.0, 24.0, 23.47992242958206, -0.1205881515296683, 0.0, 1.0, 15677.387422987456], 
processed observation next is [0.0, 0.043478260869565216, 0.5914127423822716, 0.885, 0.0, 0.0, 0.5, 0.45666020246517175, 0.45980394949011055, 0.0, 1.0, 0.0746542258237498], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.598527], dtype=float32), 0.3160857]. 
=============================================
[2019-04-07 14:46:36,118] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2130830e-22 5.3347030e-21 3.6480741e-18 1.4553246e-11 2.7018271e-20
 1.0000000e+00 6.0538189e-12 3.1061240e-14], sum to 1.0000
[2019-04-07 14:46:36,118] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6671
[2019-04-07 14:46:36,233] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 24.0, 22.43038477516919, -0.3378988085034417, 0.0, 1.0, 46203.384439204354], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 432000.0000, 
sim time next is 433800.0000, 
raw observation next is [-11.45, 54.5, 0.0, 0.0, 24.0, 22.41087565998714, -0.3603598095759447, 0.0, 1.0, 46310.953071152384], 
processed observation next is [1.0, 0.0, 0.14542936288088645, 0.545, 0.0, 0.0, 0.5, 0.3675729716655951, 0.37988006347468506, 0.0, 1.0, 0.2205283479578685], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3050057], dtype=float32), -1.3526796]. 
=============================================
[2019-04-07 14:46:54,752] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1653056e-25 2.0381121e-24 1.7712577e-22 2.4606933e-13 8.8602928e-22
 1.0000000e+00 1.6390016e-15 3.0087603e-15], sum to 1.0000
[2019-04-07 14:46:54,752] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0222
[2019-04-07 14:46:54,813] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8, 90.0, 0.0, 0.0, 24.0, 23.41205170450707, -0.1094859958265952, 0.0, 1.0, 34452.78328270225], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 541800.0000, 
sim time next is 543600.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 24.0, 23.3884554840365, -0.1346585211624166, 0.0, 1.0, 32853.86991318947], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 0.5, 0.44903795700304155, 0.45511382627919444, 0.0, 1.0, 0.1564469995866165], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9471704], dtype=float32), -2.1314242]. 
=============================================
[2019-04-07 14:47:01,801] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9189418e-27 1.6910075e-25 1.2033529e-23 2.2951607e-15 6.9285174e-23
 1.0000000e+00 1.2288947e-16 5.5244151e-18], sum to 1.0000
[2019-04-07 14:47:01,801] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2490
[2019-04-07 14:47:01,816] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5144323e-28 6.7447342e-26 2.6536602e-24 4.6990817e-15 8.0141364e-23
 1.0000000e+00 1.6617728e-15 6.9871657e-18], sum to 1.0000
[2019-04-07 14:47:01,817] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6180
[2019-04-07 14:47:01,847] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 65.5, 99.0, 670.0, 24.0, 24.10488994744014, 0.1107768550736364, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3490200.0000, 
sim time next is 3492000.0000, 
raw observation next is [0.0, 60.0, 104.0, 720.0, 24.0, 24.54185498762416, 0.1790937761301329, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.6, 0.3466666666666667, 0.7955801104972375, 0.5, 0.5451545823020133, 0.559697925376711, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49484816], dtype=float32), -0.60027635]. 
=============================================
[2019-04-07 14:47:01,851] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[103.454956]
 [103.69441 ]
 [103.81969 ]
 [103.921524]
 [104.34225 ]], R is [[103.18434906]
 [103.15250397]
 [103.12097931]
 [103.08976746]
 [103.05886841]].
[2019-04-07 14:47:02,047] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 24.0, 23.19281996927643, -0.1200100651892626, 1.0, 1.0, 74282.25426930534], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 846000.0000, 
sim time next is 847800.0000, 
raw observation next is [-3.65, 84.5, 0.0, 0.0, 24.0, 23.25946239318016, -0.1627789530836755, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3614958448753463, 0.845, 0.0, 0.0, 0.5, 0.43828853276501345, 0.4457403489721082, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7102015], dtype=float32), 0.3984093]. 
=============================================
[2019-04-07 14:47:19,059] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9666838e-29 6.8281311e-30 9.7371189e-27 4.2922267e-16 7.8499783e-26
 1.0000000e+00 3.6679868e-18 1.2913375e-18], sum to 1.0000
[2019-04-07 14:47:19,060] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2685
[2019-04-07 14:47:19,109] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.0, 60.0, 0.0, 0.0, 24.0, 25.09744092529318, 0.3759640568145155, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1533600.0000, 
sim time next is 1535400.0000, 
raw observation next is [9.7, 60.5, 0.0, 0.0, 24.0, 25.02133733929897, 0.3473985316511617, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7313019390581719, 0.605, 0.0, 0.0, 0.5, 0.5851114449415808, 0.6157995105503872, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17589664], dtype=float32), 1.5690174]. 
=============================================
[2019-04-07 14:47:22,512] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.9924789e-31 3.0392854e-29 3.8452902e-27 9.6676552e-18 2.4440485e-26
 1.0000000e+00 1.2827896e-18 2.9515841e-19], sum to 1.0000
[2019-04-07 14:47:22,512] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5943
[2019-04-07 14:47:22,565] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.7, 92.5, 27.0, 0.0, 24.0, 23.70317791488016, 0.03685987011209494, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 981000.0000, 
sim time next is 982800.0000, 
raw observation next is [10.0, 92.0, 43.5, 0.0, 24.0, 24.08100736262349, 0.09123930356604472, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.739612188365651, 0.92, 0.145, 0.0, 0.5, 0.5067506135519576, 0.5304131011886816, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30225718], dtype=float32), 1.8168538]. 
=============================================
[2019-04-07 14:47:25,303] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.13046784e-29 4.03052148e-27 6.29979512e-26 5.21891410e-17
 3.83139856e-25 1.00000000e+00 3.21323378e-16 1.07347633e-18], sum to 1.0000
[2019-04-07 14:47:25,303] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8979
[2019-04-07 14:47:25,333] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.2, 100.0, 9.5, 0.0, 24.0, 23.05341723815702, 0.04496645151482357, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1270800.0000, 
sim time next is 1272600.0000, 
raw observation next is [10.25, 98.0, 0.0, 0.0, 24.0, 22.95187181047656, 0.0183415538273567, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.7465373961218837, 0.98, 0.0, 0.0, 0.5, 0.4126559842063801, 0.5061138512757856, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7520106], dtype=float32), -0.2173542]. 
=============================================
[2019-04-07 14:47:51,228] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.5106165e-26 3.1802937e-25 1.7245860e-22 2.9850058e-14 4.5734517e-21
 1.0000000e+00 1.1290284e-14 3.1614869e-16], sum to 1.0000
[2019-04-07 14:47:51,228] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7729
[2019-04-07 14:47:51,282] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.45, 87.0, 0.0, 0.0, 24.0, 23.30929909980819, -0.03475461679472395, 0.0, 1.0, 48411.4692514652], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1751400.0000, 
sim time next is 1753200.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 24.0, 23.2814897905749, -0.04003206957656433, 0.0, 1.0, 45296.26042230174], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 0.5, 0.4401241492145749, 0.48665597680781186, 0.0, 1.0, 0.21569647820143686], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1245283], dtype=float32), -0.7402618]. 
=============================================
[2019-04-07 14:48:02,291] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.9490743e-26 2.1104499e-25 1.2055399e-23 8.4793859e-15 4.9308963e-21
 1.0000000e+00 6.1208342e-16 2.7019755e-17], sum to 1.0000
[2019-04-07 14:48:02,291] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5777
[2019-04-07 14:48:02,543] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 79.0, 0.0, 0.0, 24.0, 23.89112217801281, 0.004234608990552113, 1.0, 1.0, 63693.164101985494], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1965600.0000, 
sim time next is 1967400.0000, 
raw observation next is [-4.75, 75.0, 0.0, 0.0, 24.0, 23.87963726919256, -0.03850199985125019, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3310249307479225, 0.75, 0.0, 0.0, 0.5, 0.4899697724327134, 0.48716600004958327, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.033289], dtype=float32), -0.21132706]. 
=============================================
[2019-04-07 14:48:10,625] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:48:10,625] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:48:10,633] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run23
[2019-04-07 14:48:16,932] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.1500188e-27 1.6906214e-24 1.3767968e-22 1.4744165e-14 7.7075351e-22
 1.0000000e+00 1.6267804e-15 2.5119853e-16], sum to 1.0000
[2019-04-07 14:48:16,933] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8689
[2019-04-07 14:48:17,059] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.75, 66.5, 86.0, 0.0, 24.0, 23.62267295896489, -0.02061226611841474, 1.0, 1.0, 84571.75848368973], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2129400.0000, 
sim time next is 2131200.0000, 
raw observation next is [-4.5, 65.0, 56.0, 0.0, 24.0, 24.24204638766066, -0.06280381027676359, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.65, 0.18666666666666668, 0.0, 0.5, 0.5201705323050551, 0.4790653965744121, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12599826], dtype=float32), -2.010383]. 
=============================================
[2019-04-07 14:48:22,109] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:48:22,109] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:48:22,113] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run23
[2019-04-07 14:48:33,123] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-07 14:48:33,128] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:48:33,128] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:48:33,128] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:48:33,128] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 14:48:33,128] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:48:33,129] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:48:33,132] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run50
[2019-04-07 14:48:33,153] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run50
[2019-04-07 14:48:33,170] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run50
[2019-04-07 14:51:02,089] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 14:51:17,916] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 14:51:23,223] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 14:51:24,248] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 980000, evaluation results [980000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 14:51:26,245] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:51:26,245] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:51:26,255] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run23
[2019-04-07 14:51:29,513] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.2033820e-27 1.3055428e-26 1.8073831e-24 2.3434095e-14 3.3911988e-23
 1.0000000e+00 4.2764576e-15 1.4144269e-17], sum to 1.0000
[2019-04-07 14:51:29,513] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0795
[2019-04-07 14:51:29,552] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 73.0, 0.0, 0.0, 24.0, 22.81867154317765, -0.2315387007964116, 0.0, 1.0, 45482.88584391228], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 261000.0000, 
sim time next is 262800.0000, 
raw observation next is [-6.7, 67.0, 0.0, 0.0, 24.0, 22.90402263414954, -0.2342958115961475, 0.0, 1.0, 45689.625495868015], 
processed observation next is [1.0, 0.043478260869565216, 0.2770083102493075, 0.67, 0.0, 0.0, 0.5, 0.4086685528457951, 0.42190139613461747, 0.0, 1.0, 0.21756964521841912], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.67879707], dtype=float32), -1.4214106]. 
=============================================
[2019-04-07 14:51:35,973] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:51:35,973] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:51:35,976] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run23
[2019-04-07 14:52:07,325] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:52:07,325] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:52:07,332] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run23
[2019-04-07 14:52:32,163] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.8682510e-27 3.2945904e-26 3.7048348e-23 8.7242225e-15 6.1884793e-22
 1.0000000e+00 1.2172268e-15 1.6391070e-16], sum to 1.0000
[2019-04-07 14:52:32,164] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2586
[2019-04-07 14:52:32,200] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 24.0, 23.39494547449193, -0.07518035795563441, 0.0, 1.0, 91724.8537215871], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3722400.0000, 
sim time next is 3724200.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 24.0, 23.53647952339132, -0.07915880407931751, 0.0, 1.0, 12557.500490716426], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.5, 0.4613732936159434, 0.4736137319735609, 0.0, 1.0, 0.05979762138436393], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5160251], dtype=float32), -0.15485218]. 
=============================================
[2019-04-07 14:52:33,634] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.0045262e-27 1.9729358e-24 1.2425708e-22 1.4189985e-14 2.1068576e-22
 1.0000000e+00 3.3116556e-16 1.4033877e-17], sum to 1.0000
[2019-04-07 14:52:33,636] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0481
[2019-04-07 14:52:33,730] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 47.0, 119.0, 835.0, 24.0, 24.88013874669209, 0.2512048648892971, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3933000.0000, 
sim time next is 3934800.0000, 
raw observation next is [-6.0, 45.0, 117.5, 829.5, 24.0, 24.81333222411758, 0.1618711824122114, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.296398891966759, 0.45, 0.39166666666666666, 0.9165745856353591, 0.5, 0.5677776853431317, 0.5539570608040705, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9969348], dtype=float32), -1.6931341]. 
=============================================
[2019-04-07 14:53:04,467] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.27453763e-27 1.48025509e-25 4.43321054e-23 2.42751363e-15
 1.07452645e-23 1.00000000e+00 8.02661325e-17 9.86996043e-18], sum to 1.0000
[2019-04-07 14:53:04,467] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0715
[2019-04-07 14:53:04,548] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 68.0, 120.0, 58.5, 24.0, 23.90650939564675, -0.1222357789607966, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 727200.0000, 
sim time next is 729000.0000, 
raw observation next is [-1.15, 67.0, 139.0, 68.0, 24.0, 23.89336529886913, -0.1194065237878235, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4307479224376732, 0.67, 0.4633333333333333, 0.07513812154696133, 0.5, 0.49111377490576097, 0.46019782540405885, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7716876], dtype=float32), -2.3551114]. 
=============================================
[2019-04-07 14:53:04,567] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[ 99.58905]
 [ 99.89545]
 [ 99.90037]
 [100.10403]
 [100.55221]], R is [[99.14221954]
 [99.15079498]
 [99.1592865 ]
 [99.16769409]
 [99.17601776]].
[2019-04-07 14:53:07,635] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7451486e-27 1.5092144e-25 4.4346945e-25 6.1810218e-15 2.4493817e-23
 1.0000000e+00 2.9224254e-16 3.7008487e-17], sum to 1.0000
[2019-04-07 14:53:07,635] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7032
[2019-04-07 14:53:07,665] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.0, 19.0, 96.0, 745.0, 24.0, 25.86546485395928, 0.6286937779966325, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5067000.0000, 
sim time next is 5068800.0000, 
raw observation next is [12.0, 19.0, 86.0, 665.0, 24.0, 27.00292468609986, 0.7517892085021275, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.19, 0.2866666666666667, 0.7348066298342542, 0.5, 0.7502437238416549, 0.7505964028340425, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44584617], dtype=float32), -1.9035039]. 
=============================================
[2019-04-07 14:53:09,442] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:53:09,443] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:53:09,446] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run23
[2019-04-07 14:53:17,587] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-07 14:53:17,608] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:53:17,609] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:53:17,613] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:53:17,613] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:53:17,617] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run51
[2019-04-07 14:53:17,617] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 14:53:17,623] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:53:17,645] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run51
[2019-04-07 14:53:18,533] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run51
[2019-04-07 14:54:46,272] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11298813], dtype=float32), 0.14214566]
[2019-04-07 14:54:46,272] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-5.4335914975, 71.82931635, 50.7546182, 0.0, 24.0, 23.43381167713404, -0.05293482472735272, 1.0, 1.0, 103946.95409371595]
[2019-04-07 14:54:46,273] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 14:54:46,274] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [9.4477068e-25 1.0522771e-23 8.0642809e-22 1.1809892e-13 7.8287063e-21
 1.0000000e+00 1.2591331e-14 7.2149123e-16], sampled 0.7598095342599179
[2019-04-07 14:55:41,731] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 14:55:59,744] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 14:56:03,288] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 14:56:04,310] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1000000, evaluation results [1000000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 14:56:04,676] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6075943e-26 2.4100359e-24 1.2448005e-22 2.7417109e-13 1.0357552e-20
 1.0000000e+00 1.6205841e-14 2.3858887e-15], sum to 1.0000
[2019-04-07 14:56:04,676] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7496
[2019-04-07 14:56:04,740] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 26.0, 0.0, 0.0, 24.0, 24.70165224356602, 0.276339413030836, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4998600.0000, 
sim time next is 5000400.0000, 
raw observation next is [4.0, 29.0, 0.0, 0.0, 24.0, 24.51579408555131, 0.2336742872371512, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5734072022160666, 0.29, 0.0, 0.0, 0.5, 0.5429828404626091, 0.5778914290790503, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7514734], dtype=float32), 0.35547212]. 
=============================================
[2019-04-07 14:56:07,942] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:56:07,942] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:56:07,946] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run23
[2019-04-07 14:56:08,797] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:56:08,803] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:56:08,807] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run23
[2019-04-07 14:56:09,080] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:56:09,081] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:56:09,084] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run23
[2019-04-07 14:56:15,962] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:56:15,962] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:56:15,967] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run23
[2019-04-07 14:56:17,681] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:56:17,681] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:56:17,685] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run23
[2019-04-07 14:56:17,965] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:56:17,966] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:56:18,025] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run23
[2019-04-07 14:56:19,214] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:56:19,214] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:56:19,218] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run23
[2019-04-07 14:56:20,858] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:56:20,858] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:56:20,862] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run23
[2019-04-07 14:56:32,762] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:56:32,762] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:56:32,791] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run23
[2019-04-07 14:56:34,922] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 14:56:34,922] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:56:34,927] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run23
[2019-04-07 14:57:07,797] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4819199e-27 2.1045823e-24 2.3902153e-23 1.2149497e-14 2.5360471e-23
 1.0000000e+00 3.6000653e-16 8.8356381e-18], sum to 1.0000
[2019-04-07 14:57:07,797] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7008
[2019-04-07 14:57:07,852] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 24.0, 23.18851593805394, -0.1704840335302016, 0.0, 1.0, 43491.77961364759], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2170800.0000, 
sim time next is 2172600.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 24.0, 23.09745719061349, -0.2046534149800219, 0.0, 1.0, 43283.95749830861], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.5, 0.4247880992177908, 0.4317821950066594, 0.0, 1.0, 0.2061140833252791], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8381467], dtype=float32), -2.039334]. 
=============================================
[2019-04-07 14:57:25,980] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7849702e-29 1.3949467e-28 2.4522001e-27 1.2209465e-16 1.3668108e-24
 1.0000000e+00 3.3519579e-16 1.9562381e-17], sum to 1.0000
[2019-04-07 14:57:25,980] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7744
[2019-04-07 14:57:26,009] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 57.5, 0.0, 24.0, 24.076611357374, 0.1034317215313917, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1350000.0000, 
sim time next is 1351800.0000, 
raw observation next is [1.1, 92.5, 44.0, 0.0, 24.0, 24.04478170811883, 0.0979771579975421, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.925, 0.14666666666666667, 0.0, 0.5, 0.5037318090099024, 0.5326590526658473, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1044706], dtype=float32), 0.73112917]. 
=============================================
[2019-04-07 14:57:30,679] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.5439286e-27 2.5688277e-25 2.2574823e-24 2.1865403e-14 8.3646588e-23
 1.0000000e+00 1.7114070e-14 6.2602723e-17], sum to 1.0000
[2019-04-07 14:57:30,679] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0810
[2019-04-07 14:57:30,700] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 60.0, 104.0, 720.0, 24.0, 24.54185498762416, 0.1790937761301329, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3492000.0000, 
sim time next is 3493800.0000, 
raw observation next is [0.5, 60.5, 109.0, 770.0, 24.0, 24.64404561123834, 0.2019221301018516, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4764542936288089, 0.605, 0.36333333333333334, 0.850828729281768, 0.5, 0.553670467603195, 0.5673073767006173, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37259707], dtype=float32), -0.0649726]. 
=============================================
[2019-04-07 14:57:31,776] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7182015e-30 1.6496100e-28 2.1461862e-27 5.5442235e-16 7.7767757e-26
 1.0000000e+00 2.0992747e-16 1.6434128e-18], sum to 1.0000
[2019-04-07 14:57:31,792] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8504
[2019-04-07 14:57:31,809] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 24.0, 23.93045652169253, 0.149154173106618, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1038600.0000, 
sim time next is 1040400.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 24.0, 23.86766537404801, 0.134152428621878, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.8614958448753465, 0.75, 0.0, 0.0, 0.5, 0.48897211450400074, 0.5447174762072927, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7988352], dtype=float32), 0.9202396]. 
=============================================
[2019-04-07 14:57:56,817] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8808746e-29 1.2966485e-26 5.3228414e-25 2.1733959e-15 2.7309197e-23
 1.0000000e+00 1.2329571e-15 2.5417482e-18], sum to 1.0000
[2019-04-07 14:57:56,822] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7015
[2019-04-07 14:57:56,877] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 24.0, 23.58936840800723, 0.05488575564082327, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1467000.0000, 
sim time next is 1468800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 24.0, 23.50654437825035, 0.0311414705803309, 0.0, 1.0, 26476.19567625131], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.5, 0.4588786981875292, 0.5103804901934437, 0.0, 1.0, 0.12607712226786338], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.73008], dtype=float32), 0.84939075]. 
=============================================
[2019-04-07 14:58:00,721] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3228886e-26 1.5821500e-25 2.3529878e-23 3.4645947e-15 6.4973279e-23
 1.0000000e+00 1.0130539e-16 3.7145277e-16], sum to 1.0000
[2019-04-07 14:58:00,721] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4106
[2019-04-07 14:58:00,800] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 24.0, 23.58545346129764, -0.06814201127278852, 0.0, 1.0, 25645.57637128237], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3727800.0000, 
sim time next is 3729600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 24.0, 23.53196764771628, -0.1017415951727405, 0.0, 1.0, 13190.727277309661], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.5, 0.4609973039763566, 0.46608613494241985, 0.0, 1.0, 0.06281298703480791], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6505757], dtype=float32), -0.9956689]. 
=============================================
[2019-04-07 14:58:14,950] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-07 14:58:14,970] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:58:14,970] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:58:14,974] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run52
[2019-04-07 14:58:14,997] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 14:58:15,017] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:58:14,999] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 14:58:15,021] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 14:58:15,025] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run52
[2019-04-07 14:58:15,044] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run52
[2019-04-07 14:58:37,260] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11235152], dtype=float32), 0.14201605]
[2019-04-07 14:58:37,261] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [1.12405648, 86.64441679000001, 0.0, 0.0, 24.0, 23.26039057632434, -0.1551287921088154, 0.0, 1.0, 41358.47641588694]
[2019-04-07 14:58:37,261] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 14:58:37,262] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [3.8577787e-27 6.3364784e-26 8.4270121e-24 4.0405831e-15 4.4389134e-23
 1.0000000e+00 8.7904968e-16 2.5366354e-17], sampled 0.1318859284600663
[2019-04-07 14:59:31,830] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11235152], dtype=float32), 0.14201605]
[2019-04-07 14:59:31,830] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [0.0, 56.0, 155.5, 238.0, 24.0, 23.12727948594852, -0.08242864782409658, 0.0, 1.0, 22941.576116689674]
[2019-04-07 14:59:31,830] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 14:59:31,831] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [1.8894088e-24 3.5398202e-23 1.8312737e-21 2.1996970e-13 9.4635973e-21
 1.0000000e+00 3.1923506e-14 1.2157186e-15], sampled 0.2712517137574286
[2019-04-07 15:00:43,027] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 15:01:01,316] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 15:01:04,071] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 15:01:05,094] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 1020000, evaluation results [1020000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 15:01:10,815] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3755417e-25 3.6059724e-25 4.2636189e-24 9.1150084e-16 1.7064717e-22
 1.0000000e+00 1.6877487e-15 1.2330876e-17], sum to 1.0000
[2019-04-07 15:01:10,816] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9190
[2019-04-07 15:01:10,913] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 82.0, 191.0, 89.0, 24.0, 24.1362861724198, -0.005163306562964959, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2109600.0000, 
sim time next is 2111400.0000, 
raw observation next is [-7.55, 78.5, 208.0, 60.0, 24.0, 24.12473102748535, -0.01848035371844551, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.25346260387811637, 0.785, 0.6933333333333334, 0.06629834254143646, 0.5, 0.5103942522904458, 0.4938398820938515, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0373082], dtype=float32), -0.21041837]. 
=============================================
[2019-04-07 15:01:16,765] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:01:16,765] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:01:16,768] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run24
[2019-04-07 15:01:18,196] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6616177e-25 4.9866126e-25 3.5304343e-23 3.1902446e-14 1.7057110e-21
 1.0000000e+00 3.1222611e-15 3.4802633e-16], sum to 1.0000
[2019-04-07 15:01:18,197] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1366
[2019-04-07 15:01:18,374] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 79.0, 152.0, 0.0, 24.0, 22.94532650544621, -0.1054836295775713, 1.0, 1.0, 124821.07017393876], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2034000.0000, 
sim time next is 2035800.0000, 
raw observation next is [-4.2, 79.0, 148.0, 0.0, 24.0, 23.9644813783783, -0.00961388724149211, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.34626038781163443, 0.79, 0.49333333333333335, 0.0, 0.5, 0.4970401148648582, 0.4967953709195026, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.68871605], dtype=float32), -1.6680402]. 
=============================================
[2019-04-07 15:01:23,099] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.0217318e-27 2.6154283e-26 1.3507949e-23 5.6523474e-15 2.2851136e-23
 1.0000000e+00 1.3240515e-16 7.9660901e-18], sum to 1.0000
[2019-04-07 15:01:23,099] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5239
[2019-04-07 15:01:23,166] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.0, 26.0, 113.0, 839.5, 24.0, 25.80349602391754, 0.4734891214096524, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4975200.0000, 
sim time next is 4977000.0000, 
raw observation next is [8.0, 26.0, 109.0, 819.0, 24.0, 26.30549461158981, 0.5468777664607072, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.36333333333333334, 0.9049723756906077, 0.5, 0.6921245509658176, 0.6822925888202357, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.089173], dtype=float32), 1.9177772]. 
=============================================
[2019-04-07 15:01:23,213] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[102.84874 ]
 [101.91513 ]
 [101.03683 ]
 [ 99.9193  ]
 [ 98.737076]], R is [[103.75767517]
 [103.7201004 ]
 [103.68289948]
 [103.64607239]
 [103.60961151]].
[2019-04-07 15:01:28,453] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:01:28,453] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:01:28,457] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run24
[2019-04-07 15:01:44,369] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:01:44,369] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:01:44,373] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run24
[2019-04-07 15:01:50,913] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:01:50,913] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:01:50,916] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run24
[2019-04-07 15:02:19,861] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:02:19,861] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:02:19,864] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run24
[2019-04-07 15:02:32,364] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.89464933e-31 1.16939111e-29 8.90425052e-27 1.11037855e-17
 1.01761578e-25 1.00000000e+00 1.02485808e-17 2.59176986e-19], sum to 1.0000
[2019-04-07 15:02:32,365] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5953
[2019-04-07 15:02:32,400] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 24.0, 23.48883630173475, 0.08045552234600417, 0.0, 1.0, 82330.59380453083], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3204000.0000, 
sim time next is 3205800.0000, 
raw observation next is [-0.5, 100.0, 0.0, 0.0, 24.0, 23.59478663175226, 0.132306906706659, 0.0, 1.0, 51939.21377177449], 
processed observation next is [1.0, 0.08695652173913043, 0.44875346260387816, 1.0, 0.0, 0.0, 0.5, 0.4662322193126884, 0.544102302235553, 0.0, 1.0, 0.24732958938940233], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.95606285], dtype=float32), -0.3652498]. 
=============================================
[2019-04-07 15:02:37,586] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3112461e-24 3.4483098e-23 1.9484524e-20 5.3857515e-13 2.4746964e-20
 1.0000000e+00 8.7917297e-14 9.1501988e-15], sum to 1.0000
[2019-04-07 15:02:37,605] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0433
[2019-04-07 15:02:37,676] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 46.0, 114.0, 812.0, 24.0, 23.58592578375457, 0.06469960641111282, 0.0, 1.0, 18695.594386662455], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3591000.0000, 
sim time next is 3592800.0000, 
raw observation next is [-1.0, 42.0, 108.0, 800.0, 24.0, 23.61151323434854, 0.07272270364587789, 0.0, 1.0, 12461.139030479933], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.36, 0.8839779005524862, 0.5, 0.46762610286237827, 0.5242409012152927, 0.0, 1.0, 0.059338757287999686], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7583423], dtype=float32), -1.2414167]. 
=============================================
[2019-04-07 15:02:50,976] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.9812724e-24 1.4728785e-22 1.1588138e-21 2.8199345e-13 2.6302340e-20
 1.0000000e+00 3.1161927e-15 1.0733463e-16], sum to 1.0000
[2019-04-07 15:02:50,977] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0712
[2019-04-07 15:02:51,030] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 24.0, 23.39197107620093, -0.05966738675658704, 0.0, 1.0, 19546.901569054105], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3609000.0000, 
sim time next is 3610800.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 24.0, 23.33934939641959, -0.07555562888770936, 0.0, 1.0, 18687.33707429482], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.5, 0.44494578303496574, 0.4748147903707636, 0.0, 1.0, 0.0889873194014039], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.67521256], dtype=float32), -0.49781203]. 
=============================================
[2019-04-07 15:02:57,744] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.1344074e-28 9.9613811e-26 1.8252878e-23 6.1595321e-15 1.9371410e-22
 1.0000000e+00 6.1550458e-15 3.1212932e-17], sum to 1.0000
[2019-04-07 15:02:57,745] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8734
[2019-04-07 15:02:58,009] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 24.0, 23.36332817017618, -0.1307086725660131, 0.0, 1.0, 39452.62306644444], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 889200.0000, 
sim time next is 891000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 24.0, 23.74796928189979, -0.08599892729912133, 1.0, 1.0, 8496.30788556198], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.5, 0.4789974401583157, 0.4713336909002929, 1.0, 1.0, 0.04045860897886657], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.92270035], dtype=float32), -0.86660725]. 
=============================================
[2019-04-07 15:02:58,013] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[ 99.89752 ]
 [ 99.80983 ]
 [ 99.90203 ]
 [ 99.90708 ]
 [100.135735]], R is [[100.31505585]
 [100.31190491]
 [100.30878448]
 [100.30569458]
 [100.30263519]].
[2019-04-07 15:03:06,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3117472e-27 2.3146596e-25 1.0894802e-23 1.1166796e-14 1.2285067e-22
 1.0000000e+00 1.6386951e-15 3.5671199e-16], sum to 1.0000
[2019-04-07 15:03:06,826] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0845
[2019-04-07 15:03:06,868] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 47.0, 95.5, 740.5, 24.0, 23.79356128673279, 0.07685472990484601, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3682800.0000, 
sim time next is 3684600.0000, 
raw observation next is [5.5, 48.5, 87.0, 705.0, 24.0, 23.8267145034328, 0.08697864747780258, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6149584487534627, 0.485, 0.29, 0.7790055248618785, 0.5, 0.4855595419527334, 0.5289928824926008, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.91360015], dtype=float32), -0.35164797]. 
=============================================
[2019-04-07 15:03:09,647] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-07 15:03:09,647] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:03:09,657] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:03:09,657] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 15:03:09,658] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:03:09,658] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:03:09,658] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:03:09,669] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run53
[2019-04-07 15:03:09,696] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run53
[2019-04-07 15:03:09,723] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run53
[2019-04-07 15:05:35,623] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 15:05:51,815] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 15:05:56,435] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 15:05:57,457] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1040000, evaluation results [1040000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 15:06:09,346] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:06:09,346] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:06:09,366] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run24
[2019-04-07 15:06:27,265] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:06:27,265] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:06:27,271] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run24
[2019-04-07 15:06:27,848] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:06:27,848] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:06:27,852] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run24
[2019-04-07 15:06:29,297] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:06:29,297] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:06:29,300] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run24
[2019-04-07 15:06:34,899] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3411676e-25 6.3491361e-25 1.1507123e-22 8.5282604e-14 8.5221605e-22
 1.0000000e+00 3.3200293e-16 1.9785739e-16], sum to 1.0000
[2019-04-07 15:06:34,899] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7535
[2019-04-07 15:06:35,047] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 48.0, 0.0, 0.0, 24.0, 23.67768286930597, -0.08689680607402628, 1.0, 1.0, 15385.101009804586], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4951800.0000, 
sim time next is 4953600.0000, 
raw observation next is [-2.0, 46.0, 46.5, 280.0, 24.0, 23.81070303907723, -0.06927915644661746, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.46, 0.155, 0.30939226519337015, 0.5, 0.48422525325643573, 0.47690694785112747, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39115748], dtype=float32), -1.516555]. 
=============================================
[2019-04-07 15:06:38,317] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:06:38,317] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:06:38,321] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run24
[2019-04-07 15:06:38,470] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7844394e-31 3.3441848e-28 1.4110838e-27 8.2252656e-18 1.2804335e-25
 1.0000000e+00 4.9220150e-18 5.9113170e-19], sum to 1.0000
[2019-04-07 15:06:38,470] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2141
[2019-04-07 15:06:38,592] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 24.0, 23.4774743629754, 0.01168818779231323, 0.0, 1.0, 39783.76068118694], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1486800.0000, 
sim time next is 1488600.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 24.0, 23.51167501290994, 0.01464737786911219, 0.0, 1.0, 31435.75600559005], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.5, 0.45930625107582834, 0.5048824592897041, 0.0, 1.0, 0.1496940762170955], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15651241], dtype=float32), 0.1052922]. 
=============================================
[2019-04-07 15:06:38,675] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:06:38,675] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:06:38,679] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run24
[2019-04-07 15:06:40,713] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:06:40,713] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:06:40,718] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run24
[2019-04-07 15:06:41,244] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:06:41,244] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:06:41,256] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run24
[2019-04-07 15:06:41,333] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:06:41,333] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:06:41,337] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run24
[2019-04-07 15:06:51,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.7151780e-28 4.2743461e-26 2.4973090e-24 3.4787652e-15 3.0663221e-24
 1.0000000e+00 1.3208883e-16 2.0472439e-17], sum to 1.0000
[2019-04-07 15:06:51,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3150
[2019-04-07 15:06:51,677] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 24.0, 21.31438586249307, -0.5485331347517629, 0.0, 1.0, 40273.58792176733], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 25200.0000, 
sim time next is 27000.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 24.0, 21.38567164288263, -0.5295393678796717, 0.0, 1.0, 40268.12057780949], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 0.5, 0.2821393035735526, 0.3234868773734428, 0.0, 1.0, 0.19175295513242613], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2115728], dtype=float32), -0.44325566]. 
=============================================
[2019-04-07 15:06:51,682] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[103.11825 ]
 [102.82462 ]
 [102.557625]
 [102.27011 ]
 [101.97294 ]], R is [[103.31135559]
 [103.27824402]
 [103.24546051]
 [103.21300507]
 [103.18087769]].
[2019-04-07 15:06:52,721] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:06:52,721] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:06:52,724] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run24
[2019-04-07 15:06:55,345] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:06:55,345] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:06:55,349] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run24
[2019-04-07 15:07:00,821] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.54322405e-16 4.13453908e-16 1.25093205e-14 2.54637955e-09
 1.88889350e-13 1.00000000e+00 4.24021956e-10 1.48492954e-10], sum to 1.0000
[2019-04-07 15:07:00,821] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5403
[2019-04-07 15:07:00,895] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 24.0, 20.05475781599167, -0.8306253765656183, 0.0, 1.0, 45157.934975250006], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5400.0000, 
sim time next is 7200.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 24.0, 20.30782857125094, -0.7885634995344163, 0.0, 1.0, 43894.09716376298], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 0.5, 0.19231904760424504, 0.23714550015519456, 0.0, 1.0, 0.20901951030363322], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1614126], dtype=float32), 0.6244278]. 
=============================================
[2019-04-07 15:07:08,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4947788e-26 9.6829336e-26 2.3435193e-23 1.2911586e-13 4.1292012e-21
 1.0000000e+00 5.6250484e-16 2.3203194e-16], sum to 1.0000
[2019-04-07 15:07:08,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3788
[2019-04-07 15:07:08,313] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.0, 64.0, 112.5, 790.0, 24.0, 24.36881514598242, 0.09087688662136788, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2721600.0000, 
sim time next is 2723400.0000, 
raw observation next is [-7.0, 61.5, 113.0, 799.0, 24.0, 24.33389516667464, -8.807402004478344e-05, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2686980609418283, 0.615, 0.37666666666666665, 0.8828729281767956, 0.5, 0.5278245972228867, 0.49997064199331837, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2266827], dtype=float32), -1.4466456]. 
=============================================
[2019-04-07 15:07:14,997] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4870379e-25 3.3982029e-24 2.5255542e-22 6.7149316e-14 1.4867435e-21
 1.0000000e+00 3.5629602e-15 5.8167055e-16], sum to 1.0000
[2019-04-07 15:07:14,997] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7361
[2019-04-07 15:07:15,078] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 24.0, 22.65489570911155, -0.2852471026731074, 0.0, 1.0, 41062.28424592698], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3045600.0000, 
sim time next is 3047400.0000, 
raw observation next is [-6.0, 73.5, 0.0, 0.0, 24.0, 22.58842393773816, -0.2986324586104189, 0.0, 1.0, 41134.77282271455], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.735, 0.0, 0.0, 0.5, 0.38236866147818, 0.4004558471298603, 0.0, 1.0, 0.195879870584355], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11473511], dtype=float32), 1.0886184]. 
=============================================
[2019-04-07 15:07:34,555] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1092405e-27 4.7440471e-26 3.5223882e-24 9.6561290e-16 1.2297016e-23
 1.0000000e+00 7.2043436e-18 5.4392030e-18], sum to 1.0000
[2019-04-07 15:07:34,556] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2128
[2019-04-07 15:07:34,704] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 72.0, 101.0, 49.0, 24.0, 23.7847689705755, -0.1331860196682433, 1.0, 1.0, 12465.98499671688], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 725400.0000, 
sim time next is 727200.0000, 
raw observation next is [-1.7, 68.0, 120.0, 58.5, 24.0, 23.90650939564675, -0.1222357789607966, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4155124653739613, 0.68, 0.4, 0.06464088397790055, 0.5, 0.4922091163038959, 0.4592547403464011, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20674963], dtype=float32), -0.7064858]. 
=============================================
[2019-04-07 15:08:07,275] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-07 15:08:07,279] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:08:07,279] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:08:07,280] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:08:07,280] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:08:07,282] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 15:08:07,283] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:08:07,289] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run54
[2019-04-07 15:08:07,289] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run54
[2019-04-07 15:08:07,323] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run54
[2019-04-07 15:09:56,757] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11301122], dtype=float32), 0.14347488]
[2019-04-07 15:09:56,758] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [-6.0, 62.0, 214.0, 184.0, 24.0, 24.16043884339937, 0.1534456193577394, 1.0, 1.0, 0.0]
[2019-04-07 15:09:56,758] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 15:09:56,759] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [3.2752819e-25 4.0464569e-24 2.5868370e-22 4.9055859e-14 2.7589323e-21
 1.0000000e+00 3.9625671e-15 2.6866525e-16], sampled 0.33725512722167594
[2019-04-07 15:10:14,857] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11301122], dtype=float32), 0.14347488]
[2019-04-07 15:10:14,857] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [0.154617358, 62.37252144, 0.0, 0.0, 24.0, 23.23992061763799, -0.124472012536878, 0.0, 1.0, 0.0]
[2019-04-07 15:10:14,857] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 15:10:14,859] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [9.8615699e-25 1.5645961e-23 1.0030381e-21 8.1733299e-14 4.7560365e-21
 1.0000000e+00 1.5068845e-14 5.2816845e-16], sampled 0.5245052453684725
[2019-04-07 15:10:16,676] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11301122], dtype=float32), 0.14347488]
[2019-04-07 15:10:16,676] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-1.0, 100.0, 0.0, 0.0, 24.0, 23.40698692262767, 0.03978153417993907, 0.0, 1.0, 85076.17157495763]
[2019-04-07 15:10:16,676] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 15:10:16,677] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [1.9530657e-28 3.7767891e-27 5.2775519e-25 5.6797731e-16 3.2504645e-24
 1.0000000e+00 1.0395613e-16 2.7268772e-18], sampled 0.03551678698788452
[2019-04-07 15:10:34,552] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 15:10:54,683] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 15:10:57,974] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 15:10:58,997] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1060000, evaluation results [1060000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 15:11:12,648] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3694366e-28 3.5743485e-26 6.2727251e-25 2.0187383e-15 1.5363013e-24
 1.0000000e+00 1.4807928e-16 9.4708706e-18], sum to 1.0000
[2019-04-07 15:11:12,648] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4247
[2019-04-07 15:11:12,705] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 24.0, 23.45152771437547, 0.0158485152401201, 0.0, 1.0, 101417.6704872455], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4575600.0000, 
sim time next is 4577400.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 24.0, 23.52609104651117, 0.0264490309344349, 0.0, 1.0, 31780.681654626696], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.5, 0.4605075872092641, 0.5088163436448117, 0.0, 1.0, 0.15133657930774616], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00508666], dtype=float32), 0.4360978]. 
=============================================
[2019-04-07 15:11:36,021] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:11:36,022] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:11:36,026] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run25
[2019-04-07 15:11:47,644] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:11:47,644] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:11:47,648] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run25
[2019-04-07 15:12:04,257] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:12:04,257] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:12:04,261] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run25
[2019-04-07 15:12:08,402] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:12:08,402] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:12:08,410] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run25
[2019-04-07 15:12:12,394] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4621691e-27 4.7333457e-26 1.0557521e-21 9.2636985e-14 3.6595815e-21
 1.0000000e+00 1.1422450e-15 4.1784254e-17], sum to 1.0000
[2019-04-07 15:12:12,395] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6955
[2019-04-07 15:12:12,468] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.3, 69.5, 0.0, 0.0, 24.0, 23.5364759456575, -0.03486587117600491, 0.0, 1.0, 6248.45137465485], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2237400.0000, 
sim time next is 2239200.0000, 
raw observation next is [-5.6, 71.0, 0.0, 0.0, 24.0, 23.37948825332222, -0.04017188285913493, 0.0, 1.0, 75427.8637547338], 
processed observation next is [1.0, 0.9565217391304348, 0.30747922437673136, 0.71, 0.0, 0.0, 0.5, 0.44829068777685155, 0.4866093723802884, 0.0, 1.0, 0.35918030359397046], 
reward next is 0.9265, 
noisyNet noise sample is [array([0.1574304], dtype=float32), -0.22920007]. 
=============================================
[2019-04-07 15:12:33,268] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2836261e-25 4.5541699e-25 1.3609226e-22 3.3234727e-14 8.0603791e-21
 1.0000000e+00 1.1034916e-13 1.9446926e-15], sum to 1.0000
[2019-04-07 15:12:33,268] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6376
[2019-04-07 15:12:33,316] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 35.5, 0.0, 0.0, 24.0, 23.78352780995926, 0.02760465181939266, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5005800.0000, 
sim time next is 5007600.0000, 
raw observation next is [3.0, 34.0, 0.0, 0.0, 24.0, 23.50753847955265, 0.05200704211839605, 0.0, 1.0, 196676.43051584437], 
processed observation next is [1.0, 1.0, 0.5457063711911359, 0.34, 0.0, 0.0, 0.5, 0.4589615399627209, 0.517335680706132, 0.0, 1.0, 0.9365544310278303], 
reward next is 0.3492, 
noisyNet noise sample is [array([-0.02851455], dtype=float32), -1.3381615]. 
=============================================
[2019-04-07 15:12:37,469] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:12:37,469] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:12:37,474] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run25
[2019-04-07 15:12:49,521] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9506851e-26 3.6162764e-25 2.5642679e-23 4.3923175e-14 8.4637475e-22
 1.0000000e+00 1.2008534e-16 3.2565683e-18], sum to 1.0000
[2019-04-07 15:12:49,521] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5766
[2019-04-07 15:12:49,758] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 84.0, 49.0, 0.0, 24.0, 23.95918118028792, -0.06003834002058162, 1.0, 1.0, 20210.031686621], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 833400.0000, 
sim time next is 835200.0000, 
raw observation next is [-3.9, 82.0, 39.0, 0.0, 24.0, 23.66956157104298, -0.1203906088848645, 1.0, 1.0, 30257.165584245948], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.13, 0.0, 0.5, 0.47246346425358166, 0.4598697970383785, 1.0, 1.0, 0.14408174087736167], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7299818], dtype=float32), -0.19416992]. 
=============================================
[2019-04-07 15:12:51,948] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4945047e-28 2.6158859e-27 2.7837292e-24 7.9389471e-16 1.5856371e-24
 1.0000000e+00 1.9090204e-17 2.0897555e-19], sum to 1.0000
[2019-04-07 15:12:51,951] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9745
[2019-04-07 15:12:51,990] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 24.0, 23.20712529732336, -0.1026820210692559, 0.0, 1.0, 41028.397891483306], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 86400.0000, 
sim time next is 88200.0000, 
raw observation next is [-0.3, 93.0, 0.0, 0.0, 24.0, 23.23991433731215, -0.08717335844315806, 0.0, 1.0, 41076.176410226384], 
processed observation next is [1.0, 0.0, 0.4542936288088643, 0.93, 0.0, 0.0, 0.5, 0.4366595281093459, 0.4709422138522807, 0.0, 1.0, 0.19560084004869707], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4494369], dtype=float32), 2.493543]. 
=============================================
[2019-04-07 15:13:02,040] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.8343768e-25 6.1284521e-23 1.0013368e-21 1.9273171e-13 2.7255209e-21
 1.0000000e+00 1.0162986e-14 3.7340907e-16], sum to 1.0000
[2019-04-07 15:13:02,040] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9334
[2019-04-07 15:13:02,081] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 24.0, 23.17394176972111, -0.1247406691210969, 0.0, 1.0, 16533.301955401326], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3697200.0000, 
sim time next is 3699000.0000, 
raw observation next is [3.5, 61.0, 0.0, 0.0, 24.0, 23.03588558510716, -0.1023201232851954, 0.0, 1.0, 148925.8238624904], 
processed observation next is [0.0, 0.8260869565217391, 0.5595567867036012, 0.61, 0.0, 0.0, 0.5, 0.4196571320922633, 0.4658932922382682, 0.0, 1.0, 0.7091705898213828], 
reward next is 0.5765, 
noisyNet noise sample is [array([-0.6816227], dtype=float32), -0.41712105]. 
=============================================
[2019-04-07 15:13:02,113] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[96.44722 ]
 [96.71026 ]
 [97.16406 ]
 [97.712616]
 [98.0281  ]], R is [[97.53135681]
 [97.55604553]
 [97.58048248]
 [97.60467529]
 [97.62863159]].
[2019-04-07 15:13:04,971] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-07 15:13:04,972] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:13:04,972] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:13:04,976] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run55
[2019-04-07 15:13:05,012] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:13:05,012] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 15:13:05,012] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:13:05,013] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:13:05,018] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run55
[2019-04-07 15:13:05,044] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run55
[2019-04-07 15:15:15,150] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11281137], dtype=float32), 0.14362915]
[2019-04-07 15:15:15,150] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-3.0, 55.0, 0.0, 0.0, 24.0, 23.39063651727228, 0.0180948816609291, 1.0, 1.0, 78549.16571634008]
[2019-04-07 15:15:15,150] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 15:15:15,152] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [5.3962582e-25 7.6889381e-24 4.5136541e-22 5.1068626e-14 4.1943065e-21
 1.0000000e+00 7.0629351e-15 3.1368362e-16], sampled 0.07255471064010954
[2019-04-07 15:15:24,232] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 15:15:43,694] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.3174 79463814.5229 95.0531
[2019-04-07 15:15:45,948] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 15:15:46,971] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1080000, evaluation results [1080000.0, 2782.3174150985024, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 15:16:06,397] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2524615e-24 7.2703322e-24 1.1286713e-21 9.9280583e-14 4.8280432e-20
 1.0000000e+00 7.0336051e-14 2.1574688e-16], sum to 1.0000
[2019-04-07 15:16:06,397] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8143
[2019-04-07 15:16:06,441] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 27.0, 0.0, 0.0, 24.0, 23.64784471005614, -0.07182912458289538, 0.0, 1.0, 22990.382968229023], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3646800.0000, 
sim time next is 3648600.0000, 
raw observation next is [9.5, 26.0, 0.0, 0.0, 24.0, 23.65180461454467, -0.07818922135708627, 0.0, 1.0, 18727.428328083402], 
processed observation next is [0.0, 0.21739130434782608, 0.7257617728531857, 0.26, 0.0, 0.0, 0.5, 0.47098371787872245, 0.47393692621430455, 0.0, 1.0, 0.08917823013373048], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8613229], dtype=float32), 1.7946644]. 
=============================================
[2019-04-07 15:16:15,867] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3621379e-31 4.9651320e-30 1.1394806e-24 7.5060354e-18 5.5494855e-26
 1.0000000e+00 5.8586287e-19 8.5783024e-20], sum to 1.0000
[2019-04-07 15:16:15,867] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4408
[2019-04-07 15:16:15,943] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.95, 61.5, 0.0, 0.0, 24.0, 24.57281854994812, 0.2700056712947128, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4401000.0000, 
sim time next is 4402800.0000, 
raw observation next is [8.5, 62.0, 0.0, 0.0, 24.0, 24.23568456248068, 0.2137396966239927, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.698060941828255, 0.62, 0.0, 0.0, 0.5, 0.5196403802067234, 0.5712465655413309, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4101783], dtype=float32), -0.18990701]. 
=============================================
[2019-04-07 15:16:20,970] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.02853546e-25 8.65566774e-23 3.62810793e-23 6.96550429e-14
 9.16998371e-21 1.00000000e+00 1.77466345e-14 4.97335660e-16], sum to 1.0000
[2019-04-07 15:16:20,970] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7573
[2019-04-07 15:16:21,014] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.55, 74.5, 0.0, 0.0, 24.0, 22.75123753858189, -0.2651009660355584, 0.0, 1.0, 42138.02144136745], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 790200.0000, 
sim time next is 792000.0000, 
raw observation next is [-7.3, 75.0, 0.0, 0.0, 24.0, 22.73135572500144, -0.2749688890797136, 0.0, 1.0, 42147.12835878826], 
processed observation next is [1.0, 0.17391304347826086, 0.26038781163434904, 0.75, 0.0, 0.0, 0.5, 0.3942796437501199, 0.4083437036400954, 0.0, 1.0, 0.20070061123232502], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03568509], dtype=float32), -1.2023994]. 
=============================================
[2019-04-07 15:16:21,098] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[92.900764]
 [92.98179 ]
 [93.093094]
 [93.19137 ]
 [93.126015]], R is [[93.00292206]
 [93.07289124]
 [93.14216614]
 [93.21074677]
 [93.27864075]].
[2019-04-07 15:16:24,451] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:16:24,452] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:16:24,455] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run25
[2019-04-07 15:16:40,504] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1112707e-27 9.1672675e-26 1.1014184e-24 2.5962377e-15 3.0458770e-22
 1.0000000e+00 7.8907215e-18 3.1989821e-18], sum to 1.0000
[2019-04-07 15:16:40,504] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9906
[2019-04-07 15:16:40,580] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.45, 76.5, 0.0, 0.0, 24.0, 23.07234521930419, -0.1487172872259057, 0.0, 1.0, 45781.85661876141], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2244600.0000, 
sim time next is 2246400.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 24.0, 22.96461909966193, -0.1719392002098655, 0.0, 1.0, 45648.04400637979], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.78, 0.0, 0.0, 0.5, 0.4137182583051609, 0.4426869332633782, 0.0, 1.0, 0.21737163812561802], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9736997], dtype=float32), 0.30996215]. 
=============================================
[2019-04-07 15:16:42,869] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:16:42,869] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:16:42,873] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run25
[2019-04-07 15:16:43,365] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:16:43,365] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:16:43,369] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run25
[2019-04-07 15:16:45,661] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:16:45,674] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:16:45,679] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run25
[2019-04-07 15:16:56,260] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.8909403e-31 1.0494322e-28 8.9248974e-26 4.1859395e-17 3.4590966e-25
 1.0000000e+00 4.3166718e-18 1.4252852e-19], sum to 1.0000
[2019-04-07 15:16:56,265] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0982
[2019-04-07 15:16:56,326] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.5, 17.0, 36.0, 292.0, 24.0, 27.18146776325986, 0.7316410498899925, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5074200.0000, 
sim time next is 5076000.0000, 
raw observation next is [11.0, 17.0, 18.0, 146.0, 24.0, 26.89857220996055, 0.7178419215706858, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.06, 0.16132596685082873, 0.5, 0.7415476841633794, 0.739280640523562, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0961341], dtype=float32), 1.1937798]. 
=============================================
[2019-04-07 15:16:56,339] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[108.45291]
 [108.54922]
 [108.23737]
 [107.71096]
 [106.87713]], R is [[107.90575409]
 [107.8266983 ]
 [107.74843597]
 [107.67095184]
 [107.59424591]].
[2019-04-07 15:16:57,849] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:16:57,849] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:16:57,852] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run25
[2019-04-07 15:16:57,950] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:16:57,951] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:16:57,955] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run25
[2019-04-07 15:16:58,837] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:16:58,837] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:16:58,840] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run25
[2019-04-07 15:16:59,285] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:16:59,285] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:16:59,302] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run25
[2019-04-07 15:16:59,639] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:16:59,639] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:16:59,643] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run25
[2019-04-07 15:17:07,885] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:17:07,885] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:17:07,888] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run25
[2019-04-07 15:17:14,309] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:17:14,309] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:17:14,313] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run25
[2019-04-07 15:17:47,368] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7462200e-22 2.2483159e-21 4.1894178e-19 6.3654386e-12 4.9695936e-19
 1.0000000e+00 1.3885712e-12 5.6584881e-14], sum to 1.0000
[2019-04-07 15:17:47,369] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2101
[2019-04-07 15:17:47,436] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.45, 54.5, 0.0, 0.0, 24.0, 22.41087565998714, -0.3603598095759447, 0.0, 1.0, 46310.953071152384], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 433800.0000, 
sim time next is 435600.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 24.0, 22.35713967742182, -0.3718342882051174, 0.0, 1.0, 46421.703970391805], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 0.5, 0.363094973118485, 0.3760552372649609, 0.0, 1.0, 0.22105573319234192], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22139433], dtype=float32), -0.56360567]. 
=============================================
[2019-04-07 15:17:48,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.82013182e-24 4.82720977e-22 4.71924311e-20 1.01186204e-13
 1.46233270e-20 1.00000000e+00 3.38555952e-14 1.53425636e-14], sum to 1.0000
[2019-04-07 15:17:48,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7248
[2019-04-07 15:17:48,801] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 24.0, 23.28013573228503, 0.05830213140542146, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1197000.0000, 
sim time next is 1198800.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 24.0, 23.28453563229891, 0.0564796600645393, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.9529085872576178, 0.67, 0.0, 0.0, 0.5, 0.44037796935824236, 0.5188265533548464, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.5282717], dtype=float32), -2.165711]. 
=============================================
[2019-04-07 15:17:53,593] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-07 15:17:53,606] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:17:53,606] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:17:53,626] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run56
[2019-04-07 15:17:53,644] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:17:53,645] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:17:53,646] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 15:17:53,646] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:17:53,651] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run56
[2019-04-07 15:17:53,674] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run56
[2019-04-07 15:18:44,279] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.1130918], dtype=float32), 0.14433949]
[2019-04-07 15:18:44,279] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [-0.5, 66.0, 137.0, 203.5, 24.0, 23.06256026941551, -0.1749169695983179, 0.0, 1.0, 6237.5021607183035]
[2019-04-07 15:18:44,280] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 15:18:44,281] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [2.8152634e-25 4.6018481e-24 2.9622388e-22 2.9017716e-14 1.2607147e-21
 1.0000000e+00 5.4715088e-15 1.9495791e-16], sampled 0.49976086547715126
[2019-04-07 15:20:19,598] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 15:20:35,786] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 15:20:40,461] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 15:20:41,484] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1100000, evaluation results [1100000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 15:21:33,865] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9846233e-29 3.1095325e-29 4.2230702e-24 3.0556385e-16 1.9693341e-24
 1.0000000e+00 3.2873868e-17 1.1258048e-18], sum to 1.0000
[2019-04-07 15:21:33,868] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9038
[2019-04-07 15:21:33,986] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.35, 84.5, 30.0, 0.0, 24.0, 23.06130366464045, -0.0330621547616796, 1.0, 1.0, 54049.409767982295], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1701000.0000, 
sim time next is 1702800.0000, 
raw observation next is [1.1, 88.0, 15.5, 0.0, 24.0, 23.84589938960486, 0.0575699558386282, 1.0, 1.0, 3113.4019450384226], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.051666666666666666, 0.0, 0.5, 0.48715828246707166, 0.5191899852795427, 1.0, 1.0, 0.014825723547802013], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24682012], dtype=float32), -0.94827086]. 
=============================================
[2019-04-07 15:21:41,720] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3851972e-25 2.3806956e-24 9.2659709e-23 2.2204342e-15 1.6943501e-20
 1.0000000e+00 1.7144054e-15 8.2300090e-17], sum to 1.0000
[2019-04-07 15:21:41,720] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3655
[2019-04-07 15:21:41,828] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 24.0, 23.42341617974338, -0.06721732295337692, 0.0, 1.0, 47532.90100768785], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1800000.0000, 
sim time next is 1801800.0000, 
raw observation next is [-4.75, 84.5, 0.0, 0.0, 24.0, 23.38077205742006, -0.08479449582538079, 0.0, 1.0, 46684.45892218533], 
processed observation next is [0.0, 0.8695652173913043, 0.3310249307479225, 0.845, 0.0, 0.0, 0.5, 0.4483976714516717, 0.4717351680582064, 0.0, 1.0, 0.22230694724850159], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23892938], dtype=float32), 0.83773214]. 
=============================================
[2019-04-07 15:21:45,057] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:21:45,058] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:21:45,062] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run26
[2019-04-07 15:21:46,580] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.2684672e-25 1.4627649e-24 8.2297993e-22 7.2123053e-14 9.0180319e-23
 1.0000000e+00 4.1675889e-15 5.9392415e-16], sum to 1.0000
[2019-04-07 15:21:46,580] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5308
[2019-04-07 15:21:46,703] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 24.0, 23.11702328104316, -0.1984116286809835, 0.0, 1.0, 34737.67198238016], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1882800.0000, 
sim time next is 1884600.0000, 
raw observation next is [-5.05, 84.5, 0.0, 0.0, 24.0, 23.10545734434969, -0.166474366645841, 0.0, 1.0, 108330.14487676551], 
processed observation next is [0.0, 0.8260869565217391, 0.32271468144044324, 0.845, 0.0, 0.0, 0.5, 0.4254547786958076, 0.4445085444513863, 0.0, 1.0, 0.5158578327465024], 
reward next is 0.7699, 
noisyNet noise sample is [array([-1.1927494], dtype=float32), -1.705795]. 
=============================================
[2019-04-07 15:21:47,682] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1695756e-24 7.8826042e-24 8.8481378e-21 3.6168505e-13 2.9778302e-21
 1.0000000e+00 3.6407949e-15 1.9947083e-16], sum to 1.0000
[2019-04-07 15:21:47,682] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3933
[2019-04-07 15:21:47,752] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 24.0, 23.37408975184137, -0.08812684661153099, 0.0, 1.0, 42585.44954611281], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1805400.0000, 
sim time next is 1807200.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 24.0, 23.36852745854031, -0.09805779653537239, 0.0, 1.0, 49573.81280286069], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.86, 0.0, 0.0, 0.5, 0.4473772882116925, 0.46731406782154256, 0.0, 1.0, 0.23606577525171757], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6083527], dtype=float32), 0.73270446]. 
=============================================
[2019-04-07 15:21:57,301] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:21:57,301] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:21:57,388] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run26
[2019-04-07 15:22:12,957] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:22:12,957] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:22:12,983] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run26
[2019-04-07 15:22:14,987] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.4051211e-23 2.3872834e-21 1.7913115e-20 3.7122670e-13 1.2097992e-19
 1.0000000e+00 1.1913873e-13 7.7136809e-15], sum to 1.0000
[2019-04-07 15:22:14,987] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3432
[2019-04-07 15:22:15,145] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 31.0, 88.0, 837.0, 24.0, 23.24859200193053, -0.1462962096729993, 0.0, 1.0, 3118.5490331740325], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2462400.0000, 
sim time next is 2464200.0000, 
raw observation next is [0.5, 29.5, 90.0, 845.0, 24.0, 23.13787215946996, -0.164004134051151, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4764542936288089, 0.295, 0.3, 0.9337016574585635, 0.5, 0.42815601328916336, 0.44533195531628306, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2260305], dtype=float32), 1.2242255]. 
=============================================
[2019-04-07 15:22:17,825] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:22:17,825] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:22:17,829] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run26
[2019-04-07 15:22:22,103] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71000, global step 1116801: loss 0.7574
[2019-04-07 15:22:22,104] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71000, global step 1116801: learning rate 0.0000
[2019-04-07 15:22:32,194] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.7201685e-25 3.1422351e-25 1.2393085e-23 9.6925218e-15 5.8983784e-22
 1.0000000e+00 1.0250303e-13 3.0058452e-16], sum to 1.0000
[2019-04-07 15:22:32,194] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7331
[2019-04-07 15:22:32,271] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 57.0, 0.0, 0.0, 24.0, 23.44839627829124, -0.1855488328405818, 0.0, 1.0, 23895.444609107824], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2525400.0000, 
sim time next is 2527200.0000, 
raw observation next is [-2.3, 57.0, 0.0, 0.0, 24.0, 23.35690521611665, -0.2066952609579135, 0.0, 1.0, 45530.71233544103], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.57, 0.0, 0.0, 0.5, 0.4464087680097209, 0.43110157968069546, 0.0, 1.0, 0.21681291588305251], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6884716], dtype=float32), -0.993022]. 
=============================================
[2019-04-07 15:22:34,061] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71000, global step 1118461: loss 0.7276
[2019-04-07 15:22:34,061] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71000, global step 1118461: learning rate 0.0000
[2019-04-07 15:22:44,249] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:22:44,249] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:22:44,252] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run26
[2019-04-07 15:22:44,288] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-07 15:22:44,288] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:22:44,289] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:22:44,293] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run57
[2019-04-07 15:22:44,316] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:22:44,320] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:22:44,324] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run57
[2019-04-07 15:22:44,347] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 15:22:44,347] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:22:44,351] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run57
[2019-04-07 15:24:00,402] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11319084], dtype=float32), 0.14486215]
[2019-04-07 15:24:00,403] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-6.2, 87.0, 0.0, 0.0, 24.0, 22.99209384573037, -0.186456608075525, 0.0, 1.0, 43362.298651584155]
[2019-04-07 15:24:00,403] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 15:24:00,403] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [6.0984753e-26 1.1214081e-24 1.2441549e-22 1.1660769e-14 3.7833310e-22
 1.0000000e+00 1.9025883e-15 7.0432867e-17], sampled 0.48890525085387426
[2019-04-07 15:24:34,671] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11319084], dtype=float32), 0.14486215]
[2019-04-07 15:24:34,672] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-6.0, 59.0, 111.0, 793.5, 24.0, 23.62489910904625, 0.06570767437130122, 1.0, 1.0, 100136.10214338783]
[2019-04-07 15:24:34,672] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 15:24:34,673] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [5.4352013e-25 6.3405709e-24 2.4231288e-22 4.0001737e-14 2.5341928e-21
 1.0000000e+00 4.1275445e-15 2.7279723e-16], sampled 0.9842878832335227
[2019-04-07 15:25:09,153] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 15:25:26,170] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 15:25:28,137] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 15:25:29,161] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1120000, evaluation results [1120000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 15:25:32,894] A3C_AGENT_WORKER-Thread-6 INFO:Local step 71000, global step 1120531: loss 0.5932
[2019-04-07 15:25:32,895] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 71000, global step 1120531: learning rate 0.0000
[2019-04-07 15:25:37,605] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3498925e-32 6.4682905e-32 2.7611757e-28 5.7918096e-19 2.0408710e-27
 1.0000000e+00 5.1558979e-21 2.4711859e-21], sum to 1.0000
[2019-04-07 15:25:37,605] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1099
[2019-04-07 15:25:37,658] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 111.0, 775.5, 24.0, 25.16900654563204, 0.2985241495303741, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3150000.0000, 
sim time next is 3151800.0000, 
raw observation next is [7.5, 96.5, 114.0, 805.0, 24.0, 25.23488998019677, 0.3339345121260206, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6703601108033241, 0.965, 0.38, 0.8895027624309392, 0.5, 0.6029074983497308, 0.6113115040420068, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.64882153], dtype=float32), -0.19791298]. 
=============================================
[2019-04-07 15:25:39,808] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71000, global step 1121625: loss 0.6274
[2019-04-07 15:25:39,809] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71000, global step 1121625: learning rate 0.0000
[2019-04-07 15:25:57,609] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71500, global step 1124603: loss 70.5985
[2019-04-07 15:25:57,610] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71500, global step 1124603: learning rate 0.0000
[2019-04-07 15:25:58,511] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71000, global step 1124781: loss 0.6597
[2019-04-07 15:25:58,512] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71000, global step 1124781: learning rate 0.0000
[2019-04-07 15:26:01,834] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1613143e-28 1.4389588e-27 5.0538689e-25 1.5104548e-15 2.6286439e-24
 1.0000000e+00 2.6501305e-16 8.4397409e-19], sum to 1.0000
[2019-04-07 15:26:01,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9310
[2019-04-07 15:26:01,929] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 24.0, 23.21855893227372, 0.016167998331971, 1.0, 1.0, 6227.178609817587], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1364400.0000, 
sim time next is 1366200.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 24.0, 23.08519785562408, 0.002677786110153883, 0.0, 1.0, 50068.06987440297], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.5, 0.4237664879686734, 0.5008925953700513, 0.0, 1.0, 0.23841938035429988], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14212647], dtype=float32), 0.07875261]. 
=============================================
[2019-04-07 15:26:07,792] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71500, global step 1126637: loss 71.3425
[2019-04-07 15:26:07,794] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71500, global step 1126638: learning rate 0.0000
[2019-04-07 15:26:21,173] A3C_AGENT_WORKER-Thread-6 INFO:Local step 71500, global step 1129300: loss 69.2438
[2019-04-07 15:26:21,173] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 71500, global step 1129300: learning rate 0.0000
[2019-04-07 15:26:21,423] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3900028e-26 4.1212929e-25 1.0081620e-23 3.1340405e-14 1.1994207e-23
 1.0000000e+00 3.6678226e-16 3.5467787e-17], sum to 1.0000
[2019-04-07 15:26:21,423] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9317
[2019-04-07 15:26:21,488] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 24.0, 23.29926359311625, -0.05216985317372739, 0.0, 1.0, 88494.22130036833], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3812400.0000, 
sim time next is 3814200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 24.0, 23.47173175328187, -0.04150546559550296, 0.0, 1.0, 32083.054899188177], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.5, 0.4559776461068224, 0.486164844801499, 0.0, 1.0, 0.1527764519008961], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3572423], dtype=float32), 1.4913735]. 
=============================================
[2019-04-07 15:26:25,057] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2219859e-23 3.6744455e-24 2.5772040e-22 5.5599579e-13 1.6304718e-21
 1.0000000e+00 6.9941632e-15 7.3966951e-16], sum to 1.0000
[2019-04-07 15:26:25,058] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8810
[2019-04-07 15:26:25,088] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 39.5, 19.0, 169.0, 24.0, 24.52095501487169, 0.08852755763101931, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3951000.0000, 
sim time next is 3952800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 24.0, 23.74432666885125, 0.03004599819448724, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.296398891966759, 0.41, 0.0, 0.0, 0.5, 0.4786938890709376, 0.5100153327314957, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31058288], dtype=float32), -0.9635515]. 
=============================================
[2019-04-07 15:26:28,953] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71500, global step 1130832: loss 70.4265
[2019-04-07 15:26:28,955] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71500, global step 1130832: learning rate 0.0000
[2019-04-07 15:26:31,376] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2682559e-26 1.0526339e-24 5.3574803e-22 1.9191432e-14 2.9918810e-21
 1.0000000e+00 2.9466023e-14 1.3806613e-16], sum to 1.0000
[2019-04-07 15:26:31,376] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3544
[2019-04-07 15:26:31,629] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 34.0, 46.0, 234.5, 24.0, 23.5394109762375, -0.09660602634085864, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4089600.0000, 
sim time next is 4091400.0000, 
raw observation next is [-3.5, 36.0, 92.0, 469.0, 24.0, 23.71097304548309, -0.009446381551175586, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.36565096952908593, 0.36, 0.30666666666666664, 0.518232044198895, 0.5, 0.47591442045692417, 0.49685120614960815, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2580965], dtype=float32), 0.06343279]. 
=============================================
[2019-04-07 15:26:37,253] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:26:37,253] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:26:37,257] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run26
[2019-04-07 15:26:38,444] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6493235e-32 2.8717570e-30 6.8841372e-27 3.3401477e-18 1.9930923e-27
 1.0000000e+00 9.8879359e-18 1.3218535e-20], sum to 1.0000
[2019-04-07 15:26:38,444] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3648
[2019-04-07 15:26:38,510] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.95, 79.5, 0.0, 0.0, 24.0, 24.61529320971017, 0.1869219350464975, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1013400.0000, 
sim time next is 1015200.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 24.0, 24.21620251887738, 0.1203270049309181, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.5, 0.5180168765731151, 0.5401090016436394, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25972134], dtype=float32), -1.7617133]. 
=============================================
[2019-04-07 15:26:45,276] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72000, global step 1134133: loss 3.8069
[2019-04-07 15:26:45,277] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72000, global step 1134133: learning rate 0.0000
[2019-04-07 15:26:45,350] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71500, global step 1134153: loss 70.6153
[2019-04-07 15:26:45,354] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71500, global step 1134154: learning rate 0.0000
[2019-04-07 15:26:54,386] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7701995e-26 8.1651812e-25 1.4105385e-22 2.0098480e-15 4.0306392e-22
 1.0000000e+00 7.3320257e-16 2.7908313e-17], sum to 1.0000
[2019-04-07 15:26:54,386] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1086
[2019-04-07 15:26:54,463] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3, 46.5, 41.0, 0.0, 24.0, 23.44274524790061, -0.1225748263874524, 1.0, 1.0, 25826.156457601242], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2305800.0000, 
sim time next is 2307600.0000, 
raw observation next is [-0.6, 49.0, 23.0, 0.0, 24.0, 23.74314952539801, -0.06882896382687605, 1.0, 1.0, 26814.84389604366], 
processed observation next is [1.0, 0.7391304347826086, 0.44598337950138506, 0.49, 0.07666666666666666, 0.0, 0.5, 0.4785957937831675, 0.477057012057708, 1.0, 1.0, 0.12768973283830315], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.232662], dtype=float32), 0.05062124]. 
=============================================
[2019-04-07 15:26:55,331] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72000, global step 1136009: loss 3.9425
[2019-04-07 15:26:55,332] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72000, global step 1136009: learning rate 0.0000
[2019-04-07 15:27:02,981] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:27:02,981] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:27:02,996] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run26
[2019-04-07 15:27:05,091] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:27:05,093] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:27:05,096] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run26
[2019-04-07 15:27:05,134] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:27:05,135] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:27:05,140] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run26
[2019-04-07 15:27:10,083] A3C_AGENT_WORKER-Thread-6 INFO:Local step 72000, global step 1138418: loss 3.8389
[2019-04-07 15:27:10,084] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 72000, global step 1138418: learning rate 0.0000
[2019-04-07 15:27:10,763] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71000, global step 1138510: loss 0.6513
[2019-04-07 15:27:10,765] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71000, global step 1138510: learning rate 0.0000
[2019-04-07 15:27:13,133] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:27:13,133] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:27:13,136] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run26
[2019-04-07 15:27:13,825] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:27:13,825] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:27:13,828] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run26
[2019-04-07 15:27:13,862] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:27:13,865] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:27:13,869] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run26
[2019-04-07 15:27:16,887] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:27:16,887] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:27:16,891] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run26
[2019-04-07 15:27:17,602] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72000, global step 1139388: loss 4.2597
[2019-04-07 15:27:17,602] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72000, global step 1139388: learning rate 0.0000
[2019-04-07 15:27:17,997] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4415012e-24 2.8255611e-24 1.1499230e-21 4.3367155e-15 3.0066728e-22
 1.0000000e+00 1.1158274e-15 6.7617939e-17], sum to 1.0000
[2019-04-07 15:27:17,997] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5052
[2019-04-07 15:27:18,008] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:27:18,008] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:27:18,011] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run26
[2019-04-07 15:27:18,078] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.75, 77.0, 0.0, 0.0, 24.0, 23.08749661688043, -0.2100305694663042, 0.0, 1.0, 46112.849033075094], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1895400.0000, 
sim time next is 1897200.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 24.0, 22.99928992485471, -0.228413327128776, 0.0, 1.0, 46079.79786444249], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.79, 0.0, 0.0, 0.5, 0.4166074937378926, 0.423862224290408, 0.0, 1.0, 0.21942760887829757], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6848782], dtype=float32), -0.21167886]. 
=============================================
[2019-04-07 15:27:20,721] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:27:20,721] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:27:20,724] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run26
[2019-04-07 15:27:24,233] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-07 15:27:24,235] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:27:24,235] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:27:24,237] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:27:24,239] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:27:24,239] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run58
[2019-04-07 15:27:24,266] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 15:27:24,269] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:27:24,274] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run58
[2019-04-07 15:27:24,299] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run58
[2019-04-07 15:29:47,360] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 15:30:04,852] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 15:30:07,771] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 15:30:08,795] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1140000, evaluation results [1140000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 15:30:08,811] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.6346516e-25 4.5568314e-24 7.0165894e-21 1.7876419e-13 1.8980745e-20
 1.0000000e+00 1.0656976e-14 1.6978621e-14], sum to 1.0000
[2019-04-07 15:30:08,842] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8117
[2019-04-07 15:30:08,958] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 76.0, 0.0, 0.0, 24.0, 22.37539435312499, -0.3254222952481883, 0.0, 1.0, 45058.939703420845], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 181800.0000, 
sim time next is 183600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 24.0, 22.32076404809455, -0.3549962237800166, 0.0, 1.0, 45075.52045408633], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.5, 0.36006367067454575, 0.3816679254066611, 0.0, 1.0, 0.21464533549564918], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17648852], dtype=float32), -0.9256116]. 
=============================================
[2019-04-07 15:30:13,053] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:30:13,053] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:30:13,056] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run26
[2019-04-07 15:30:23,636] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72000, global step 1141694: loss 3.5453
[2019-04-07 15:30:23,637] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72000, global step 1141694: learning rate 0.0000
[2019-04-07 15:30:23,973] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72500, global step 1141736: loss 0.1323
[2019-04-07 15:30:23,973] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72500, global step 1141736: learning rate 0.0000
[2019-04-07 15:30:24,659] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71000, global step 1141826: loss 0.8151
[2019-04-07 15:30:24,660] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71000, global step 1141826: learning rate 0.0000
[2019-04-07 15:30:27,326] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71000, global step 1142109: loss 0.7261
[2019-04-07 15:30:27,327] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71000, global step 1142109: learning rate 0.0000
[2019-04-07 15:30:27,819] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71000, global step 1142170: loss 0.6417
[2019-04-07 15:30:27,819] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71000, global step 1142170: learning rate 0.0000
[2019-04-07 15:30:36,240] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71000, global step 1143201: loss 0.7732
[2019-04-07 15:30:36,241] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71000, global step 1143201: learning rate 0.0000
[2019-04-07 15:30:36,582] A3C_AGENT_WORKER-Thread-5 INFO:Local step 71000, global step 1143245: loss 0.7373
[2019-04-07 15:30:36,582] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 71000, global step 1143245: learning rate 0.0000
[2019-04-07 15:30:36,630] A3C_AGENT_WORKER-Thread-4 INFO:Local step 71000, global step 1143249: loss 0.7557
[2019-04-07 15:30:36,632] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 71000, global step 1143249: learning rate 0.0000
[2019-04-07 15:30:36,878] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72500, global step 1143280: loss 0.1309
[2019-04-07 15:30:36,878] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72500, global step 1143280: learning rate 0.0000
[2019-04-07 15:30:37,898] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71000, global step 1143396: loss 0.7895
[2019-04-07 15:30:37,899] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71000, global step 1143396: learning rate 0.0000
[2019-04-07 15:30:39,141] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71000, global step 1143544: loss 0.6705
[2019-04-07 15:30:39,142] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71000, global step 1143544: learning rate 0.0000
[2019-04-07 15:30:39,586] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71000, global step 1143598: loss 0.6379
[2019-04-07 15:30:39,587] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71000, global step 1143598: learning rate 0.0000
[2019-04-07 15:30:43,129] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7609656e-27 4.8369694e-26 2.8912214e-24 8.7847979e-16 7.5061267e-24
 1.0000000e+00 2.9123759e-16 5.6625898e-18], sum to 1.0000
[2019-04-07 15:30:43,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1731
[2019-04-07 15:30:43,269] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 81.5, 127.0, 467.0, 24.0, 23.12083209370496, -0.08288043091897622, 0.0, 1.0, 21330.345031704743], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 570600.0000, 
sim time next is 572400.0000, 
raw observation next is [-1.2, 83.0, 113.5, 270.0, 24.0, 23.13273101365264, -0.1036773107634101, 0.0, 1.0, 21293.64384140423], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.37833333333333335, 0.2983425414364641, 0.5, 0.42772758447105347, 0.4654408964121966, 0.0, 1.0, 0.10139830400668681], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00206462], dtype=float32), 0.17362854]. 
=============================================
[2019-04-07 15:30:47,045] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71500, global step 1144698: loss 71.1522
[2019-04-07 15:30:47,050] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71500, global step 1144698: learning rate 0.0000
[2019-04-07 15:30:51,561] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71000, global step 1145344: loss 0.7127
[2019-04-07 15:30:51,586] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71000, global step 1145344: learning rate 0.0000
[2019-04-07 15:30:52,185] A3C_AGENT_WORKER-Thread-6 INFO:Local step 72500, global step 1145430: loss 0.1336
[2019-04-07 15:30:52,185] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 72500, global step 1145430: learning rate 0.0000
[2019-04-07 15:30:58,770] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72500, global step 1146371: loss 0.1336
[2019-04-07 15:30:58,771] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72500, global step 1146371: learning rate 0.0000
[2019-04-07 15:31:03,758] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73000, global step 1147071: loss 0.9658
[2019-04-07 15:31:03,758] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73000, global step 1147071: learning rate 0.0000
[2019-04-07 15:31:05,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0615440e-27 3.0406136e-25 3.3946010e-24 1.5019490e-14 3.8969230e-23
 1.0000000e+00 3.7158336e-15 3.5772038e-16], sum to 1.0000
[2019-04-07 15:31:05,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1740
[2019-04-07 15:31:05,814] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 24.0, 23.30300449648002, -0.09604748668256258, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3695400.0000, 
sim time next is 3697200.0000, 
raw observation next is [4.0, 59.0, 0.0, 0.0, 24.0, 23.17394176972111, -0.1247406691210969, 0.0, 1.0, 16533.301955401326], 
processed observation next is [0.0, 0.8260869565217391, 0.5734072022160666, 0.59, 0.0, 0.0, 0.5, 0.4311618141434259, 0.4584197769596344, 0.0, 1.0, 0.07873000931143488], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2874271], dtype=float32), -1.9948661]. 
=============================================
[2019-04-07 15:31:17,303] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72500, global step 1149264: loss 0.1285
[2019-04-07 15:31:17,304] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72500, global step 1149264: learning rate 0.0000
[2019-04-07 15:31:18,347] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73000, global step 1149474: loss 0.7914
[2019-04-07 15:31:18,369] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73000, global step 1149474: learning rate 0.0000
[2019-04-07 15:31:18,872] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71500, global step 1149593: loss 70.8022
[2019-04-07 15:31:18,880] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71500, global step 1149593: learning rate 0.0000
[2019-04-07 15:31:19,290] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71500, global step 1149684: loss 69.5087
[2019-04-07 15:31:19,290] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71500, global step 1149684: learning rate 0.0000
[2019-04-07 15:31:22,540] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71500, global step 1150513: loss 69.9864
[2019-04-07 15:31:22,540] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71500, global step 1150513: learning rate 0.0000
[2019-04-07 15:31:26,675] A3C_AGENT_WORKER-Thread-5 INFO:Local step 71500, global step 1151599: loss 70.4360
[2019-04-07 15:31:26,676] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 71500, global step 1151600: learning rate 0.0000
[2019-04-07 15:31:27,365] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71500, global step 1151768: loss 70.3202
[2019-04-07 15:31:27,366] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71500, global step 1151768: learning rate 0.0000
[2019-04-07 15:31:27,880] A3C_AGENT_WORKER-Thread-4 INFO:Local step 71500, global step 1151899: loss 69.9188
[2019-04-07 15:31:27,881] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 71500, global step 1151899: learning rate 0.0000
[2019-04-07 15:31:28,581] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71500, global step 1152054: loss 69.7444
[2019-04-07 15:31:28,607] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71500, global step 1152054: learning rate 0.0000
[2019-04-07 15:31:28,797] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71500, global step 1152107: loss 69.9290
[2019-04-07 15:31:28,800] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71500, global step 1152107: learning rate 0.0000
[2019-04-07 15:31:30,027] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71500, global step 1152409: loss 69.7731
[2019-04-07 15:31:30,027] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71500, global step 1152409: learning rate 0.0000
[2019-04-07 15:31:30,197] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0378659e-29 1.1855267e-26 1.3805339e-25 2.5165891e-16 1.8212242e-25
 1.0000000e+00 8.6508658e-19 2.0332027e-18], sum to 1.0000
[2019-04-07 15:31:30,197] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7184
[2019-04-07 15:31:30,227] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.35, 90.5, 0.0, 0.0, 24.0, 23.84009413984059, 0.1301517321874159, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1459800.0000, 
sim time next is 1461600.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 24.0, 23.78092402075714, 0.1070632273464786, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.49307479224376743, 0.92, 0.0, 0.0, 0.5, 0.4817436683964284, 0.5356877424488262, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05247591], dtype=float32), 0.5882995]. 
=============================================
[2019-04-07 15:31:32,733] A3C_AGENT_WORKER-Thread-6 INFO:Local step 73000, global step 1152990: loss 0.9262
[2019-04-07 15:31:32,755] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 73000, global step 1152990: learning rate 0.0000
[2019-04-07 15:31:34,827] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.9222087e-29 3.4842288e-27 1.1066576e-24 7.4951129e-16 3.0644876e-23
 1.0000000e+00 3.8931591e-18 2.1483667e-18], sum to 1.0000
[2019-04-07 15:31:34,828] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9314
[2019-04-07 15:31:34,897] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.35, 84.5, 30.0, 0.0, 24.0, 23.06130366464045, -0.0330621547616796, 1.0, 1.0, 54049.409767982295], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1701000.0000, 
sim time next is 1702800.0000, 
raw observation next is [1.1, 88.0, 15.5, 0.0, 24.0, 23.84589938960486, 0.0575699558386282, 1.0, 1.0, 3113.4019450384226], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.051666666666666666, 0.0, 0.5, 0.48715828246707166, 0.5191899852795427, 1.0, 1.0, 0.014825723547802013], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7985833], dtype=float32), 1.8119226]. 
=============================================
[2019-04-07 15:31:34,984] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72000, global step 1153469: loss 3.7716
[2019-04-07 15:31:34,989] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72000, global step 1153469: learning rate 0.0000
[2019-04-07 15:31:38,206] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73000, global step 1154116: loss 0.8535
[2019-04-07 15:31:38,210] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73000, global step 1154116: learning rate 0.0000
[2019-04-07 15:31:39,737] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8512333e-23 3.6105891e-22 1.2439812e-19 6.5007824e-13 3.5866655e-20
 1.0000000e+00 1.4132649e-13 3.2131005e-15], sum to 1.0000
[2019-04-07 15:31:39,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0630
[2019-04-07 15:31:39,795] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 24.0, 22.69587834404956, -0.06510113590565834, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1222200.0000, 
sim time next is 1224000.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 24.0, 22.64287186233774, -0.07356598864994043, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8919667590027703, 0.93, 0.0, 0.0, 0.5, 0.3869059885281449, 0.4754780037833532, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7797038], dtype=float32), 0.46840054]. 
=============================================
[2019-04-07 15:31:39,814] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[87.29146]
 [86.95524]
 [86.69149]
 [86.51751]
 [86.15227]], R is [[87.72234344]
 [87.84512329]
 [87.9666748 ]
 [88.08700562]
 [88.20613861]].
[2019-04-07 15:31:41,184] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71500, global step 1154693: loss 70.1126
[2019-04-07 15:31:41,185] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71500, global step 1154693: learning rate 0.0000
[2019-04-07 15:31:41,299] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73500, global step 1154721: loss 0.0123
[2019-04-07 15:31:41,299] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73500, global step 1154721: learning rate 0.0000
[2019-04-07 15:31:52,446] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:31:52,447] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:31:52,465] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run27
[2019-04-07 15:31:55,678] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73500, global step 1157202: loss 0.0256
[2019-04-07 15:31:55,680] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73500, global step 1157202: learning rate 0.0000
[2019-04-07 15:31:55,785] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73000, global step 1157217: loss 0.7011
[2019-04-07 15:31:55,787] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73000, global step 1157217: learning rate 0.0000
[2019-04-07 15:32:06,493] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:32:06,493] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:32:06,497] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run27
[2019-04-07 15:32:06,651] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72000, global step 1158773: loss 3.9294
[2019-04-07 15:32:06,652] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72000, global step 1158773: learning rate 0.0000
[2019-04-07 15:32:06,822] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72000, global step 1158804: loss 3.9796
[2019-04-07 15:32:06,822] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72000, global step 1158804: learning rate 0.0000
[2019-04-07 15:32:10,179] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72000, global step 1159220: loss 3.5258
[2019-04-07 15:32:10,180] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72000, global step 1159220: learning rate 0.0000
[2019-04-07 15:32:12,073] A3C_AGENT_WORKER-Thread-6 INFO:Local step 73500, global step 1159449: loss 0.0123
[2019-04-07 15:32:12,073] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 73500, global step 1159449: learning rate 0.0000
[2019-04-07 15:32:15,282] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.2717276e-13 2.4819383e-12 1.2196320e-11 7.9243023e-08 1.1643316e-11
 9.9999976e-01 1.6820478e-07 4.4119335e-09], sum to 1.0000
[2019-04-07 15:32:15,283] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6828
[2019-04-07 15:32:15,336] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 24.0, 19.64114642579297, -0.8903611708336857, 0.0, 1.0, 55291.1053854383], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3600.0000, 
sim time next is 5400.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 24.0, 20.05475781599167, -0.8306253765656183, 0.0, 1.0, 45157.934975250006], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.5, 0.17122981799930592, 0.22312487447812723, 0.0, 1.0, 0.2150377855964286], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6134797], dtype=float32), 1.094133]. 
=============================================
[2019-04-07 15:32:15,926] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-07 15:32:15,929] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:32:15,929] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:32:15,933] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run59
[2019-04-07 15:32:15,961] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:32:15,965] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:32:15,965] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73500, global step 1160000: loss 0.0118
[2019-04-07 15:32:15,963] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 15:32:15,966] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:32:15,966] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73500, global step 1160000: learning rate 0.0000
[2019-04-07 15:32:15,977] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run59
[2019-04-07 15:32:16,001] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run59
[2019-04-07 15:33:06,199] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11326226], dtype=float32), 0.1458122]
[2019-04-07 15:33:06,199] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [0.5, 96.0, 0.0, 0.0, 24.0, 23.08519785562408, 0.002677786110153883, 0.0, 1.0, 50068.06987440297]
[2019-04-07 15:33:06,199] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 15:33:06,201] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [2.8006592e-28 3.4646397e-27 6.9105606e-25 2.3980964e-16 4.2708949e-24
 1.0000000e+00 2.5588720e-17 1.1060237e-18], sampled 0.18811552853569402
[2019-04-07 15:33:13,047] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11326226], dtype=float32), 0.1458122]
[2019-04-07 15:33:13,048] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [3.0, 78.0, 0.0, 0.0, 24.0, 23.42667555335942, -0.00377502136410493, 0.0, 1.0, 64278.62308334986]
[2019-04-07 15:33:13,048] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 15:33:13,049] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [5.3489276e-28 9.7314005e-27 1.0791349e-24 4.1818239e-16 5.2963916e-24
 1.0000000e+00 8.3915309e-17 2.4665372e-18], sampled 0.034206406477320206
[2019-04-07 15:34:43,987] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 15:35:02,233] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 15:35:02,916] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 15:35:03,939] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 1160000, evaluation results [1160000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 15:35:06,367] A3C_AGENT_WORKER-Thread-5 INFO:Local step 72000, global step 1160313: loss 4.0740
[2019-04-07 15:35:06,368] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 72000, global step 1160313: learning rate 0.0000
[2019-04-07 15:35:06,417] A3C_AGENT_WORKER-Thread-4 INFO:Local step 72000, global step 1160318: loss 4.2814
[2019-04-07 15:35:06,418] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 72000, global step 1160318: learning rate 0.0000
[2019-04-07 15:35:06,711] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72000, global step 1160368: loss 3.8356
[2019-04-07 15:35:06,711] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72000, global step 1160368: learning rate 0.0000
[2019-04-07 15:35:07,507] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72000, global step 1160504: loss 3.9418
[2019-04-07 15:35:07,508] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72000, global step 1160504: learning rate 0.0000
[2019-04-07 15:35:07,585] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72000, global step 1160519: loss 4.3789
[2019-04-07 15:35:07,585] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72000, global step 1160519: learning rate 0.0000
[2019-04-07 15:35:10,042] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72000, global step 1160852: loss 3.4833
[2019-04-07 15:35:10,044] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72000, global step 1160852: learning rate 0.0000
[2019-04-07 15:35:10,957] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:35:10,957] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:35:10,960] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run27
[2019-04-07 15:35:15,009] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:35:15,009] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:35:15,012] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run27
[2019-04-07 15:35:15,068] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2405809e-23 2.0529556e-22 2.3178186e-21 2.1525354e-13 5.2346975e-20
 1.0000000e+00 1.3861949e-12 4.8053268e-15], sum to 1.0000
[2019-04-07 15:35:15,069] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9077
[2019-04-07 15:35:15,231] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.05, 43.5, 0.0, 0.0, 24.0, 23.07289455261877, -0.1812888143762579, 0.0, 1.0, 33412.41908200205], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2399400.0000, 
sim time next is 2401200.0000, 
raw observation next is [-2.4, 43.0, 0.0, 0.0, 24.0, 23.06283836417953, -0.186096072626063, 0.0, 1.0, 41210.84291334496], 
processed observation next is [0.0, 0.8260869565217391, 0.39612188365650974, 0.43, 0.0, 0.0, 0.5, 0.4219031970149609, 0.43796797579131236, 0.0, 1.0, 0.1962421091111665], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04108763], dtype=float32), 1.9427751]. 
=============================================
[2019-04-07 15:35:17,085] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72500, global step 1161685: loss 0.1213
[2019-04-07 15:35:17,085] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72500, global step 1161685: learning rate 0.0000
[2019-04-07 15:35:20,767] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9611393e-26 7.5337765e-25 1.5397754e-23 1.0278706e-14 2.1847359e-21
 1.0000000e+00 1.1473626e-16 1.3507379e-16], sum to 1.0000
[2019-04-07 15:35:20,767] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2572
[2019-04-07 15:35:20,785] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.2500000000000001, 43.0, 232.0, 71.0, 24.0, 24.10662259530881, -0.06191243925486737, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2547000.0000, 
sim time next is 2548800.0000, 
raw observation next is [1.1, 39.0, 225.0, 46.5, 24.0, 24.1809606140202, -0.04910731694917619, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.39, 0.75, 0.05138121546961326, 0.5, 0.5150800511683501, 0.48363089435027456, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.53614575], dtype=float32), -0.39161757]. 
=============================================
[2019-04-07 15:35:21,393] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72000, global step 1162252: loss 3.8880
[2019-04-07 15:35:21,394] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72000, global step 1162252: learning rate 0.0000
[2019-04-07 15:35:21,434] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73500, global step 1162257: loss 0.0122
[2019-04-07 15:35:21,435] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73500, global step 1162257: learning rate 0.0000
[2019-04-07 15:35:32,484] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:35:32,484] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:35:32,487] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run27
[2019-04-07 15:35:39,852] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1815251e-25 7.4237093e-26 1.5534224e-22 3.5600070e-14 6.8278463e-22
 1.0000000e+00 6.9758012e-16 1.4454202e-16], sum to 1.0000
[2019-04-07 15:35:39,852] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6413
[2019-04-07 15:35:39,933] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 39.5, 178.0, 685.0, 24.0, 23.811005810541, 0.006274312221600159, 1.0, 1.0, 32180.89451357593], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2809800.0000, 
sim time next is 2811600.0000, 
raw observation next is [4.0, 35.0, 213.5, 429.0, 24.0, 24.01589252103852, 0.02168451355114055, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5734072022160666, 0.35, 0.7116666666666667, 0.4740331491712707, 0.5, 0.50132437675321, 0.5072281711837135, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.253234], dtype=float32), -0.9241923]. 
=============================================
[2019-04-07 15:35:42,376] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5979684e-25 3.4536662e-24 4.1717413e-22 1.4868083e-14 6.9276255e-21
 1.0000000e+00 2.0168437e-16 2.0503948e-16], sum to 1.0000
[2019-04-07 15:35:42,376] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5721
[2019-04-07 15:35:42,505] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.0, 83.5, 97.0, 612.0, 24.0, 24.24288561647624, 0.03262719647473614, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2712600.0000, 
sim time next is 2714400.0000, 
raw observation next is [-12.0, 76.0, 107.0, 643.0, 24.0, 24.36044422854103, 0.05941165900395107, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.13019390581717452, 0.76, 0.3566666666666667, 0.7104972375690608, 0.5, 0.5300370190450859, 0.5198038863346504, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6278594], dtype=float32), -0.8884977]. 
=============================================
[2019-04-07 15:35:42,884] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1882083e-26 2.4286404e-25 5.3777100e-22 5.6992658e-15 6.8580973e-23
 1.0000000e+00 5.0278721e-17 5.1596271e-17], sum to 1.0000
[2019-04-07 15:35:42,884] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3488
[2019-04-07 15:35:42,935] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.0, 69.0, 0.0, 0.0, 24.0, 23.13233893793799, -0.1261536745272148, 0.0, 1.0, 45742.969404266274], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2682000.0000, 
sim time next is 2683800.0000, 
raw observation next is [-10.0, 72.5, 0.0, 0.0, 24.0, 23.0185545019453, -0.1678581188983196, 0.0, 1.0, 45652.091169951775], 
processed observation next is [1.0, 0.043478260869565216, 0.18559556786703602, 0.725, 0.0, 0.0, 0.5, 0.4182128751621083, 0.4440472937005601, 0.0, 1.0, 0.21739091033310368], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0321257], dtype=float32), -0.37320992]. 
=============================================
[2019-04-07 15:35:43,849] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.9456347e-24 1.8478971e-22 1.4704597e-21 8.8288169e-14 9.3070380e-21
 1.0000000e+00 2.7236236e-14 3.3932832e-15], sum to 1.0000
[2019-04-07 15:35:43,849] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6715
[2019-04-07 15:35:44,017] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.75, 59.5, 0.0, 0.0, 24.0, 22.9938089235397, -0.2278203113768078, 0.0, 1.0, 44952.48478664904], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 671400.0000, 
sim time next is 673200.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 24.0, 23.02249597120933, -0.2293410157365606, 0.0, 1.0, 38669.55462629627], 
processed observation next is [0.0, 0.8260869565217391, 0.3988919667590028, 0.62, 0.0, 0.0, 0.5, 0.4185413309341109, 0.42355299475447983, 0.0, 1.0, 0.18414073631569652], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6587477], dtype=float32), 0.27918434]. 
=============================================
[2019-04-07 15:35:50,851] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72500, global step 1166338: loss 0.1382
[2019-04-07 15:35:50,851] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72500, global step 1166338: learning rate 0.0000
[2019-04-07 15:35:50,970] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72500, global step 1166359: loss 0.1360
[2019-04-07 15:35:50,971] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72500, global step 1166359: learning rate 0.0000
[2019-04-07 15:35:54,997] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72500, global step 1166971: loss 0.1374
[2019-04-07 15:35:54,998] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72500, global step 1166971: learning rate 0.0000
[2019-04-07 15:35:58,095] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73000, global step 1167405: loss 0.8117
[2019-04-07 15:35:58,096] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73000, global step 1167405: learning rate 0.0000
[2019-04-07 15:36:01,466] A3C_AGENT_WORKER-Thread-5 INFO:Local step 72500, global step 1167914: loss 0.1221
[2019-04-07 15:36:01,467] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 72500, global step 1167914: learning rate 0.0000
[2019-04-07 15:36:02,026] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72500, global step 1168005: loss 0.1152
[2019-04-07 15:36:02,026] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72500, global step 1168005: learning rate 0.0000
[2019-04-07 15:36:02,724] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72500, global step 1168115: loss 0.1519
[2019-04-07 15:36:02,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72500, global step 1168115: learning rate 0.0000
[2019-04-07 15:36:03,212] A3C_AGENT_WORKER-Thread-4 INFO:Local step 72500, global step 1168192: loss 0.1491
[2019-04-07 15:36:03,237] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 72500, global step 1168192: learning rate 0.0000
[2019-04-07 15:36:04,121] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72500, global step 1168330: loss 0.1212
[2019-04-07 15:36:04,137] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72500, global step 1168330: learning rate 0.0000
[2019-04-07 15:36:04,913] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7010128e-29 1.1506028e-29 2.4608695e-26 2.9349702e-17 4.1471887e-25
 1.0000000e+00 5.3016461e-17 1.2822992e-20], sum to 1.0000
[2019-04-07 15:36:04,913] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4700
[2019-04-07 15:36:05,021] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 24.0, 23.51914209649099, -0.0939715645478561, 0.0, 1.0, 45382.129891573735], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3128400.0000, 
sim time next is 3130200.0000, 
raw observation next is [3.5, 100.0, 0.0, 0.0, 24.0, 23.65475876901603, -0.1138777577011351, 0.0, 1.0, 12485.700859416092], 
processed observation next is [1.0, 0.21739130434782608, 0.5595567867036012, 1.0, 0.0, 0.0, 0.5, 0.47122989741800253, 0.4620407474329549, 0.0, 1.0, 0.059455718378171867], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3308963], dtype=float32), -1.6894262]. 
=============================================
[2019-04-07 15:36:06,822] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72500, global step 1168798: loss 0.1351
[2019-04-07 15:36:06,835] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72500, global step 1168798: learning rate 0.0000
[2019-04-07 15:36:08,525] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4228188e-31 1.8786514e-30 2.5843830e-28 2.2032124e-16 7.8780568e-26
 1.0000000e+00 1.3556094e-18 5.4229723e-21], sum to 1.0000
[2019-04-07 15:36:08,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1780
[2019-04-07 15:36:08,593] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 103.5, 696.5, 24.0, 24.75520054655412, 0.1862628073286657, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3146400.0000, 
sim time next is 3148200.0000, 
raw observation next is [7.0, 100.0, 108.0, 746.0, 24.0, 25.008828146498, 0.251008187061988, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.36, 0.8243093922651934, 0.5, 0.5840690122081668, 0.5836693956873293, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06847036], dtype=float32), 1.0599504]. 
=============================================
[2019-04-07 15:36:11,810] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3412820e-28 6.5841642e-28 4.7047615e-25 4.7205288e-17 7.8074217e-26
 1.0000000e+00 1.7438270e-18 1.0453930e-20], sum to 1.0000
[2019-04-07 15:36:11,811] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5433
[2019-04-07 15:36:11,880] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 72.0, 0.0, 0.0, 24.0, 23.76922733751539, 0.03011184072505957, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3537000.0000, 
sim time next is 3538800.0000, 
raw observation next is [-1.0, 66.0, 0.0, 0.0, 24.0, 23.31460672896235, 0.06390055814658181, 0.0, 1.0, 172826.70171012584], 
processed observation next is [1.0, 1.0, 0.4349030470914128, 0.66, 0.0, 0.0, 0.5, 0.4428838940801958, 0.5213001860488606, 0.0, 1.0, 0.8229842938577421], 
reward next is 0.4627, 
noisyNet noise sample is [array([-0.43807548], dtype=float32), 0.51420856]. 
=============================================
[2019-04-07 15:36:14,369] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72500, global step 1170242: loss 0.1275
[2019-04-07 15:36:14,369] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72500, global step 1170242: learning rate 0.0000
[2019-04-07 15:36:22,503] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.4112619e-28 7.3577377e-26 1.2408223e-23 1.0734779e-15 1.3870804e-24
 1.0000000e+00 1.8981632e-18 6.1493067e-19], sum to 1.0000
[2019-04-07 15:36:22,503] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6302
[2019-04-07 15:36:22,546] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 74.0, 0.0, 0.0, 24.0, 23.78958965911031, 0.1039511120087825, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3792600.0000, 
sim time next is 3794400.0000, 
raw observation next is [-3.0, 77.0, 0.0, 0.0, 24.0, 23.64684892865994, 0.07944236340980147, 0.0, 1.0, 39536.66716542375], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.77, 0.0, 0.0, 0.5, 0.4705707440549949, 0.5264807878032671, 0.0, 1.0, 0.188269843644875], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32079783], dtype=float32), 0.057867438]. 
=============================================
[2019-04-07 15:36:29,990] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73000, global step 1173344: loss 0.6753
[2019-04-07 15:36:29,991] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73000, global step 1173344: learning rate 0.0000
[2019-04-07 15:36:30,259] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73000, global step 1173397: loss 0.7293
[2019-04-07 15:36:30,262] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73000, global step 1173397: learning rate 0.0000
[2019-04-07 15:36:31,231] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1758024e-28 1.6777574e-26 2.7437189e-25 1.9725149e-16 3.6941940e-23
 1.0000000e+00 1.0701160e-17 4.1673599e-18], sum to 1.0000
[2019-04-07 15:36:31,232] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0734
[2019-04-07 15:36:31,316] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 55.5, 117.0, 835.0, 24.0, 24.75352381043262, 0.2439718401687331, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3846600.0000, 
sim time next is 3848400.0000, 
raw observation next is [1.0, 51.0, 115.0, 829.5, 24.0, 24.88443578533125, 0.1744785163004362, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4903047091412743, 0.51, 0.38333333333333336, 0.9165745856353591, 0.5, 0.5737029821109374, 0.5581595054334788, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14341033], dtype=float32), 1.067487]. 
=============================================
[2019-04-07 15:36:35,891] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73000, global step 1174567: loss 0.7040
[2019-04-07 15:36:35,891] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73000, global step 1174567: learning rate 0.0000
[2019-04-07 15:36:35,900] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73500, global step 1174570: loss 0.0248
[2019-04-07 15:36:35,903] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73500, global step 1174570: learning rate 0.0000
[2019-04-07 15:36:37,787] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1642913e-27 9.3644523e-27 4.8400013e-24 6.6243630e-16 2.7233151e-23
 1.0000000e+00 5.6262443e-18 5.3971628e-18], sum to 1.0000
[2019-04-07 15:36:37,787] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1971
[2019-04-07 15:36:37,883] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 24.0, 23.42093513655264, 0.05975283277794585, 1.0, 1.0, 55309.80622865477], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3787200.0000, 
sim time next is 3789000.0000, 
raw observation next is [-2.5, 68.0, 0.0, 0.0, 24.0, 23.45711355278931, 0.1096064000975583, 0.0, 1.0, 135041.4655344532], 
processed observation next is [1.0, 0.8695652173913043, 0.39335180055401664, 0.68, 0.0, 0.0, 0.5, 0.45475946273244244, 0.5365354666991861, 0.0, 1.0, 0.6430545977831105], 
reward next is 0.6427, 
noisyNet noise sample is [array([-0.5658415], dtype=float32), 0.10219004]. 
=============================================
[2019-04-07 15:36:37,887] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[103.238464]
 [103.1101  ]
 [103.25411 ]
 [104.12987 ]
 [104.760124]], R is [[103.50488281]
 [103.46983337]
 [103.14879608]
 [103.11730957]
 [103.08613586]].
[2019-04-07 15:36:41,564] A3C_AGENT_WORKER-Thread-5 INFO:Local step 73000, global step 1175739: loss 0.6991
[2019-04-07 15:36:41,565] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 73000, global step 1175739: learning rate 0.0000
[2019-04-07 15:36:41,838] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73000, global step 1175798: loss 0.6875
[2019-04-07 15:36:41,842] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73000, global step 1175798: learning rate 0.0000
[2019-04-07 15:36:42,030] A3C_AGENT_WORKER-Thread-4 INFO:Local step 73000, global step 1175838: loss 0.7362
[2019-04-07 15:36:42,033] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 73000, global step 1175838: learning rate 0.0000
[2019-04-07 15:36:42,632] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73000, global step 1175955: loss 0.7032
[2019-04-07 15:36:42,633] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73000, global step 1175955: learning rate 0.0000
[2019-04-07 15:36:43,096] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73000, global step 1176041: loss 0.6704
[2019-04-07 15:36:43,097] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73000, global step 1176041: learning rate 0.0000
[2019-04-07 15:36:45,762] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73000, global step 1176537: loss 0.7760
[2019-04-07 15:36:45,763] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73000, global step 1176537: learning rate 0.0000
[2019-04-07 15:36:46,770] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:36:46,771] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:36:46,774] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run27
[2019-04-07 15:36:48,350] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.9262351e-28 5.0960865e-27 3.1471773e-25 7.6140443e-16 4.0346346e-24
 1.0000000e+00 1.9014980e-17 2.0152025e-19], sum to 1.0000
[2019-04-07 15:36:48,350] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3951
[2019-04-07 15:36:48,414] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 48.0, 109.5, 803.0, 24.0, 25.08049057322735, 0.3106977926567023, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3852000.0000, 
sim time next is 3853800.0000, 
raw observation next is [2.0, 48.0, 106.0, 782.0, 24.0, 25.33911428611963, 0.3506447744745152, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.35333333333333333, 0.8640883977900552, 0.5, 0.6115928571766359, 0.616881591491505, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9047377], dtype=float32), -2.6863115]. 
=============================================
[2019-04-07 15:36:53,616] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73000, global step 1178021: loss 0.7088
[2019-04-07 15:36:53,617] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73000, global step 1178021: learning rate 0.0000
[2019-04-07 15:37:02,840] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-07 15:37:02,840] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:37:02,841] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:37:02,845] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run60
[2019-04-07 15:37:02,865] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:37:02,869] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:37:02,876] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 15:37:02,877] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:37:02,877] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run60
[2019-04-07 15:37:02,916] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run60
[2019-04-07 15:39:23,930] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 15:39:47,186] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 15:39:50,468] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 15:39:51,491] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1180000, evaluation results [1180000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 15:39:55,729] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73500, global step 1180796: loss 0.0124
[2019-04-07 15:39:55,731] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73500, global step 1180796: learning rate 0.0000
[2019-04-07 15:39:55,940] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73500, global step 1180835: loss 0.0117
[2019-04-07 15:39:55,942] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73500, global step 1180835: learning rate 0.0000
[2019-04-07 15:40:00,721] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73500, global step 1181707: loss 0.0168
[2019-04-07 15:40:00,723] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73500, global step 1181707: learning rate 0.0000
[2019-04-07 15:40:01,723] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.8559176e-29 2.7512398e-28 6.0983361e-26 2.4474931e-17 4.6297009e-25
 1.0000000e+00 2.1106276e-17 1.0268964e-18], sum to 1.0000
[2019-04-07 15:40:01,724] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5573
[2019-04-07 15:40:01,789] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 24.0, 23.75133747826289, 0.06179791958475173, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4433400.0000, 
sim time next is 4435200.0000, 
raw observation next is [2.0, 80.0, 60.0, 116.0, 24.0, 23.69918060336527, 0.05030024471767986, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.518005540166205, 0.8, 0.2, 0.1281767955801105, 0.5, 0.4749317169471059, 0.5167667482392266, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0956216], dtype=float32), 0.7533485]. 
=============================================
[2019-04-07 15:40:03,811] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2505325e-27 3.6439997e-25 3.6244680e-24 1.4181779e-15 3.0572484e-23
 1.0000000e+00 5.0789369e-17 1.3098581e-18], sum to 1.0000
[2019-04-07 15:40:03,812] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6946
[2019-04-07 15:40:03,934] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 24.0, 22.98805696469357, -0.1982826853367437, 0.0, 1.0, 42845.819899614085], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1994400.0000, 
sim time next is 1996200.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 24.0, 23.05814520612277, -0.1846900881154061, 0.0, 1.0, 42752.23293112017], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.5, 0.42151210051023086, 0.43843663729486465, 0.0, 1.0, 0.2035820615767627], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.792283], dtype=float32), -1.3127568]. 
=============================================
[2019-04-07 15:40:06,653] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:40:06,653] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:40:06,658] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run27
[2019-04-07 15:40:06,705] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:40:06,705] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:40:06,711] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run27
[2019-04-07 15:40:08,482] A3C_AGENT_WORKER-Thread-5 INFO:Local step 73500, global step 1183029: loss 0.0114
[2019-04-07 15:40:08,495] A3C_AGENT_WORKER-Thread-4 INFO:Local step 73500, global step 1183032: loss 0.0112
[2019-04-07 15:40:08,505] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 73500, global step 1183029: learning rate 0.0000
[2019-04-07 15:40:08,517] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 73500, global step 1183032: learning rate 0.0000
[2019-04-07 15:40:09,470] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73500, global step 1183156: loss 0.0114
[2019-04-07 15:40:09,471] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73500, global step 1183156: learning rate 0.0000
[2019-04-07 15:40:09,560] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73500, global step 1183167: loss 0.0196
[2019-04-07 15:40:09,562] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73500, global step 1183167: learning rate 0.0000
[2019-04-07 15:40:10,227] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73500, global step 1183249: loss 0.0126
[2019-04-07 15:40:10,228] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73500, global step 1183249: learning rate 0.0000
[2019-04-07 15:40:11,949] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:40:11,949] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:40:11,952] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run27
[2019-04-07 15:40:12,863] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73500, global step 1183630: loss 0.0133
[2019-04-07 15:40:12,866] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73500, global step 1183630: learning rate 0.0000
[2019-04-07 15:40:16,347] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5678305e-28 6.0775742e-26 3.1726477e-25 1.5348593e-14 1.2893008e-24
 1.0000000e+00 4.6200343e-17 1.5668818e-18], sum to 1.0000
[2019-04-07 15:40:16,353] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7099
[2019-04-07 15:40:16,389] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 82.0, 707.5, 24.0, 26.33447592373914, 0.6129949556743549, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4982400.0000, 
sim time next is 4984200.0000, 
raw observation next is [8.5, 25.5, 72.0, 641.0, 24.0, 26.85533356541201, 0.6768549328552659, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.698060941828255, 0.255, 0.24, 0.7082872928176795, 0.5, 0.7379444637843342, 0.7256183109517553, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43141943], dtype=float32), 0.8474988]. 
=============================================
[2019-04-07 15:40:19,973] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:40:19,973] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:40:19,977] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run27
[2019-04-07 15:40:20,668] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:40:20,668] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:40:20,685] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run27
[2019-04-07 15:40:20,929] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:40:20,929] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:40:20,951] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run27
[2019-04-07 15:40:21,013] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:40:21,013] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:40:21,017] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run27
[2019-04-07 15:40:21,056] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73500, global step 1184808: loss 0.0114
[2019-04-07 15:40:21,067] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73500, global step 1184808: learning rate 0.0000
[2019-04-07 15:40:21,329] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2913225e-26 2.5714945e-25 1.2368391e-22 2.2578457e-15 1.3681111e-22
 1.0000000e+00 6.8678871e-16 1.2022370e-17], sum to 1.0000
[2019-04-07 15:40:21,329] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3152
[2019-04-07 15:40:21,501] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 75.0, 0.0, 0.0, 24.0, 22.78437925607204, -0.1703814026874041, 1.0, 1.0, 107830.83980270247], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2187000.0000, 
sim time next is 2188800.0000, 
raw observation next is [-5.6, 75.0, 21.5, 131.0, 24.0, 23.79999811068044, -0.06931083282991657, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.30747922437673136, 0.75, 0.07166666666666667, 0.14475138121546963, 0.5, 0.48333317589003677, 0.47689638905669446, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7311812], dtype=float32), 0.9530632]. 
=============================================
[2019-04-07 15:40:21,507] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:40:21,507] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:40:21,511] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run27
[2019-04-07 15:40:23,690] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:40:23,690] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:40:23,702] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run27
[2019-04-07 15:40:31,024] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.7120444e-27 5.2946948e-24 9.3344818e-22 7.7148286e-15 5.9748712e-24
 1.0000000e+00 7.7785958e-16 1.9346424e-17], sum to 1.0000
[2019-04-07 15:40:31,024] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8465
[2019-04-07 15:40:31,240] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 78.0, 56.5, 148.0, 24.0, 23.74151481971752, -0.1354433045960898, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 205200.0000, 
sim time next is 207000.0000, 
raw observation next is [-7.85, 76.5, 79.0, 0.0, 24.0, 23.82285367325645, -0.1360638053982433, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24515235457063714, 0.765, 0.2633333333333333, 0.0, 0.5, 0.4852378061047042, 0.4546453982005856, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5299919], dtype=float32), -0.10547155]. 
=============================================
[2019-04-07 15:40:31,245] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[95.40068 ]
 [95.276825]
 [94.29625 ]
 [92.44295 ]
 [91.12235 ]], R is [[95.44248199]
 [95.48806   ]
 [95.53318024]
 [95.57785034]
 [95.19116211]].
[2019-04-07 15:40:31,293] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:40:31,293] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:40:31,321] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run27
[2019-04-07 15:40:35,328] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.1360914e-28 3.9110799e-26 8.3088057e-25 6.7737765e-16 2.4318043e-23
 1.0000000e+00 3.1512643e-16 1.3320546e-18], sum to 1.0000
[2019-04-07 15:40:35,329] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8531
[2019-04-07 15:40:35,368] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 24.0, 23.12682805649731, -0.2368736057143567, 0.0, 1.0, 42391.64447275194], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 709200.0000, 
sim time next is 711000.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 24.0, 23.08381632569769, -0.2460823404846579, 0.0, 1.0, 42421.69852397421], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.5, 0.4236513604748075, 0.41797255317178067, 0.0, 1.0, 0.20200808820940103], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25228986], dtype=float32), -0.9597198]. 
=============================================
[2019-04-07 15:40:35,373] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[97.450806]
 [97.25948 ]
 [97.09077 ]
 [96.8794  ]
 [96.68339 ]], R is [[97.5858078 ]
 [97.6099472 ]
 [97.6338501 ]
 [97.65750885]
 [97.68093109]].
[2019-04-07 15:41:07,287] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3656504e-26 3.8045294e-24 2.4037950e-22 7.3528118e-15 5.6236367e-22
 1.0000000e+00 4.7102242e-16 8.4001415e-18], sum to 1.0000
[2019-04-07 15:41:07,287] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1657
[2019-04-07 15:41:07,590] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 24.0, 22.64200611983163, -0.2100238452368271, 0.0, 1.0, 44988.55719763532], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3308400.0000, 
sim time next is 3310200.0000, 
raw observation next is [-11.0, 80.0, 2.0, 94.0, 24.0, 22.9837577004455, -0.0506824198381452, 1.0, 1.0, 122528.60071866006], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.8, 0.006666666666666667, 0.10386740331491713, 0.5, 0.4153131417037918, 0.48310586005395156, 1.0, 1.0, 0.5834695272317146], 
reward next is 0.7022, 
noisyNet noise sample is [array([0.3306063], dtype=float32), -0.50247]. 
=============================================
[2019-04-07 15:41:16,984] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.6836265e-25 2.5647361e-24 1.3901707e-21 4.8005013e-14 5.7697271e-22
 1.0000000e+00 7.1124128e-15 4.5800782e-17], sum to 1.0000
[2019-04-07 15:41:16,984] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0438
[2019-04-07 15:41:17,100] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.55, 71.0, 0.0, 0.0, 24.0, 22.91857617234864, -0.210479463978923, 0.0, 1.0, 42622.96788328788], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 783000.0000, 
sim time next is 784800.0000, 
raw observation next is [-7.8, 71.0, 0.0, 0.0, 24.0, 22.82209804927463, -0.2363141061101405, 0.0, 1.0, 42550.25822710064], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.71, 0.0, 0.0, 0.5, 0.4018415041062191, 0.4212286312966198, 0.0, 1.0, 0.20262027727190782], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2570805], dtype=float32), -0.72404474]. 
=============================================
[2019-04-07 15:41:31,321] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8464823e-25 4.0591476e-25 1.2006710e-22 1.8049439e-13 5.5896544e-21
 1.0000000e+00 2.5479848e-15 1.4523013e-16], sum to 1.0000
[2019-04-07 15:41:31,322] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0836
[2019-04-07 15:41:31,360] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 24.0, 22.94384994481113, -0.1986913914999209, 0.0, 1.0, 42711.5107627023], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 781200.0000, 
sim time next is 783000.0000, 
raw observation next is [-7.55, 71.0, 0.0, 0.0, 24.0, 22.91857617234864, -0.210479463978923, 0.0, 1.0, 42622.96788328788], 
processed observation next is [1.0, 0.043478260869565216, 0.25346260387811637, 0.71, 0.0, 0.0, 0.5, 0.40988134769572, 0.42984017867369234, 0.0, 1.0, 0.2029665137299423], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36373973], dtype=float32), -1.549172]. 
=============================================
[2019-04-07 15:41:31,363] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[93.82545]
 [93.82922]
 [93.49579]
 [94.00696]
 [93.88154]], R is [[94.15738678]
 [94.21581268]
 [94.27365875]
 [94.33092499]
 [94.38761902]].
[2019-04-07 15:41:45,042] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3936443e-27 9.9348545e-26 6.8999783e-23 9.0127380e-15 9.4456261e-23
 1.0000000e+00 3.6769545e-17 7.3049106e-18], sum to 1.0000
[2019-04-07 15:41:45,043] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4740
[2019-04-07 15:41:45,091] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.55, 55.0, 171.0, 0.0, 24.0, 25.81695306432577, 0.5554897801688282, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1085400.0000, 
sim time next is 1087200.0000, 
raw observation next is [18.8, 54.0, 155.5, 0.0, 24.0, 25.17512269473329, 0.508436822403126, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.9833795013850417, 0.54, 0.5183333333333333, 0.0, 0.5, 0.5979268912277741, 0.6694789408010421, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.77313477], dtype=float32), -1.5115104]. 
=============================================
[2019-04-07 15:42:01,052] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-07 15:42:01,053] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:42:01,054] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:42:01,054] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:42:01,055] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 15:42:01,056] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:42:01,056] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:42:01,061] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run61
[2019-04-07 15:42:01,085] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run61
[2019-04-07 15:42:01,122] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run61
[2019-04-07 15:42:50,997] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11347288], dtype=float32), 0.1468923]
[2019-04-07 15:42:50,997] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [13.45780404, 49.16540252, 115.412090965, 0.0, 24.0, 25.11248614458806, 0.3577539929122687, 1.0, 1.0, 0.0]
[2019-04-07 15:42:50,997] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 15:42:50,998] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [8.7754288e-29 1.8105685e-27 1.8199533e-25 1.2806836e-16 1.7116203e-24
 1.0000000e+00 1.4492567e-17 5.8618258e-19], sampled 0.18848819726902222
[2019-04-07 15:44:04,030] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11347288], dtype=float32), 0.1468923]
[2019-04-07 15:44:04,030] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [-1.7, 74.0, 0.0, 0.0, 24.0, 23.69655606593474, 0.05390188348222055, 1.0, 1.0, 65405.96980643203]
[2019-04-07 15:44:04,030] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 15:44:04,032] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [1.20806544e-26 1.56445144e-25 1.17614112e-23 1.87641684e-15
 9.98967002e-23 1.00000000e+00 2.11046769e-16 1.06705058e-17], sampled 0.9003344533644245
[2019-04-07 15:44:26,143] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 15:44:45,095] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 15:44:47,909] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 15:44:48,931] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1200000, evaluation results [1200000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 15:44:53,191] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5598528e-28 7.6605321e-26 1.3558129e-23 3.8421906e-15 2.7920473e-23
 1.0000000e+00 1.4183556e-16 1.4023968e-18], sum to 1.0000
[2019-04-07 15:44:53,191] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8728
[2019-04-07 15:44:53,214] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 75.5, 634.0, 24.0, 25.4130419487668, 0.3799369760984398, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3859200.0000, 
sim time next is 3861000.0000, 
raw observation next is [3.0, 43.0, 64.0, 551.0, 24.0, 25.54222357873307, 0.2521732872542625, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.43, 0.21333333333333335, 0.6088397790055249, 0.5, 0.628518631561089, 0.5840577624180875, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.63835585], dtype=float32), -0.0252363]. 
=============================================
[2019-04-07 15:44:53,217] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[103.37701 ]
 [103.22011 ]
 [103.102715]
 [102.948204]
 [102.88532 ]], R is [[103.1769104 ]
 [103.1451416 ]
 [103.11369324]
 [103.08255768]
 [103.05173492]].
[2019-04-07 15:44:58,837] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:44:58,837] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:44:58,840] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run28
[2019-04-07 15:45:13,953] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:45:13,953] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:45:13,956] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run28
[2019-04-07 15:45:15,591] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3541130e-28 1.6270123e-27 6.0386829e-25 1.8574729e-15 4.0097173e-24
 1.0000000e+00 9.1123262e-17 6.6473377e-19], sum to 1.0000
[2019-04-07 15:45:15,591] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4575
[2019-04-07 15:45:15,667] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.6, 69.0, 0.0, 0.0, 24.0, 23.57821798450555, -0.0686054131142431, 0.0, 1.0, 58137.23195745057], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4338000.0000, 
sim time next is 4339800.0000, 
raw observation next is [3.45, 70.0, 0.0, 0.0, 24.0, 23.59249215669078, -0.05074150986796037, 0.0, 1.0, 45613.308087428486], 
processed observation next is [1.0, 0.21739130434782608, 0.5581717451523546, 0.7, 0.0, 0.0, 0.5, 0.4660410130575651, 0.4830861633773465, 0.0, 1.0, 0.21720622898775468], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0662687], dtype=float32), 0.66600907]. 
=============================================
[2019-04-07 15:45:28,776] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:45:28,776] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:45:28,785] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run28
[2019-04-07 15:45:33,860] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:45:33,861] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:45:33,874] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run28
[2019-04-07 15:45:50,237] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:45:50,238] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:45:50,241] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run28
[2019-04-07 15:45:56,041] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.09307502e-27 3.86640131e-25 1.16265594e-23 2.40774070e-15
 5.14834810e-22 1.00000000e+00 2.00510783e-15 3.44926772e-17], sum to 1.0000
[2019-04-07 15:45:56,042] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4092
[2019-04-07 15:45:56,090] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.3, 76.0, 0.0, 0.0, 24.0, 23.2635156889927, -0.1632677012892943, 0.0, 1.0, 43555.4944858687], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2604600.0000, 
sim time next is 2606400.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 24.0, 23.17123919339666, -0.1776423569788426, 0.0, 1.0, 43612.81234287087], 
processed observation next is [1.0, 0.17391304347826086, 0.30747922437673136, 0.78, 0.0, 0.0, 0.5, 0.43093659944972157, 0.44078588100705246, 0.0, 1.0, 0.20768005877557558], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5506209], dtype=float32), 0.44178402]. 
=============================================
[2019-04-07 15:46:24,733] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1879084e-25 2.2989884e-24 4.7923671e-22 1.9643360e-14 1.4049971e-21
 1.0000000e+00 3.6369202e-16 8.8709350e-17], sum to 1.0000
[2019-04-07 15:46:24,734] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1201
[2019-04-07 15:46:24,922] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 52.5, 114.0, 817.0, 24.0, 23.21334052957353, -0.06998749501544312, 0.0, 1.0, 12471.398867642616], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3069000.0000, 
sim time next is 3070800.0000, 
raw observation next is [-2.0, 50.0, 111.5, 811.5, 24.0, 23.24085187425528, -0.06980898658742646, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.40720221606648205, 0.5, 0.37166666666666665, 0.8966850828729281, 0.5, 0.4367376561879401, 0.47673033780419116, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14151166], dtype=float32), -0.5581813]. 
=============================================
[2019-04-07 15:46:37,962] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4110791e-26 2.6545194e-25 3.5217720e-23 1.0460300e-15 8.9990324e-22
 1.0000000e+00 7.3999366e-17 4.2833736e-18], sum to 1.0000
[2019-04-07 15:46:37,962] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6786
[2019-04-07 15:46:37,999] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 24.0, 23.61766544967751, 0.04845189507487161, 0.0, 1.0, 27834.534029068865], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3362400.0000, 
sim time next is 3364200.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 24.0, 23.50281155441027, -0.01697170616622258, 0.0, 1.0, 65700.94546560582], 
processed observation next is [1.0, 0.9565217391304348, 0.3379501385041552, 0.68, 0.0, 0.0, 0.5, 0.458567629534189, 0.4943427646112591, 0.0, 1.0, 0.31286164507431347], 
reward next is 0.9729, 
noisyNet noise sample is [array([1.3289222], dtype=float32), -1.2200865]. 
=============================================
[2019-04-07 15:46:47,560] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1523121e-28 3.8736183e-26 1.7697970e-24 6.2544272e-16 2.4734458e-24
 1.0000000e+00 2.1399596e-17 1.1495826e-18], sum to 1.0000
[2019-04-07 15:46:47,560] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7120
[2019-04-07 15:46:47,611] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 60.0, 64.0, 539.0, 24.0, 25.28479282286366, 0.3368115836632102, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3774600.0000, 
sim time next is 3776400.0000, 
raw observation next is [0.0, 60.0, 40.5, 343.0, 24.0, 25.0602658637444, 0.3015458085874542, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.6, 0.135, 0.37900552486187844, 0.5, 0.5883554886453668, 0.6005152695291515, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5810651], dtype=float32), -0.96320593]. 
=============================================
[2019-04-07 15:46:56,835] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-07 15:46:56,841] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:46:56,842] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:46:56,845] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:46:56,845] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 15:46:56,846] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:46:56,846] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run62
[2019-04-07 15:46:56,846] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:46:56,870] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run62
[2019-04-07 15:46:56,892] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run62
[2019-04-07 15:48:04,837] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11338276], dtype=float32), 0.14712512]
[2019-04-07 15:48:04,837] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-2.111305289, 73.4805348, 2.9099657645, 9.67154464, 24.0, 23.34668383138553, -0.1975263135284763, 1.0, 1.0, 43313.23710906005]
[2019-04-07 15:48:04,837] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 15:48:04,839] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [3.0814588e-25 3.2047618e-24 2.2262362e-22 9.3448478e-15 1.5535061e-21
 1.0000000e+00 1.2274524e-15 7.4058393e-17], sampled 0.09023037517000965
[2019-04-07 15:48:36,328] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11338276], dtype=float32), 0.14712512]
[2019-04-07 15:48:36,328] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [9.4, 66.0, 0.0, 0.0, 24.0, 23.65368751890194, 0.1580780818793014, 0.0, 1.0, 80496.256076939]
[2019-04-07 15:48:36,328] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 15:48:36,330] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [1.9290006e-29 4.3416528e-28 5.6977340e-26 3.8230925e-17 2.8010481e-25
 1.0000000e+00 7.5320221e-18 1.9290987e-19], sampled 0.9548782222815008
[2019-04-07 15:49:18,630] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 15:49:38,148] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 15:49:41,545] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 15:49:42,567] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 1220000, evaluation results [1220000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 15:49:46,118] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5928656e-29 1.1435546e-27 2.4783083e-25 1.4171226e-16 1.5910957e-25
 1.0000000e+00 2.5895798e-16 2.2774179e-18], sum to 1.0000
[2019-04-07 15:49:46,119] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3529
[2019-04-07 15:49:46,170] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 24.0, 23.65221072684479, 0.08713347307284093, 0.0, 1.0, 32149.87846895642], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1317600.0000, 
sim time next is 1319400.0000, 
raw observation next is [1.35, 92.0, 0.0, 0.0, 24.0, 23.44589407447208, 0.09417920724804009, 0.0, 1.0, 88374.46255984646], 
processed observation next is [1.0, 0.2608695652173913, 0.5000000000000001, 0.92, 0.0, 0.0, 0.5, 0.4538245062060066, 0.53139306908268, 0.0, 1.0, 0.420830774094507], 
reward next is 0.8649, 
noisyNet noise sample is [array([-0.58714885], dtype=float32), -0.010248273]. 
=============================================
[2019-04-07 15:49:47,153] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:49:47,153] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:49:47,156] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run28
[2019-04-07 15:49:54,910] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8891779e-27 3.4628595e-25 4.2534493e-24 1.1229029e-15 2.3507382e-21
 1.0000000e+00 2.8164147e-16 4.0747559e-17], sum to 1.0000
[2019-04-07 15:49:54,910] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9157
[2019-04-07 15:49:54,980] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.05, 79.0, 147.0, 0.0, 24.0, 23.81796027092204, -0.1029596681455562, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2028600.0000, 
sim time next is 2030400.0000, 
raw observation next is [-4.5, 75.0, 151.5, 0.0, 24.0, 23.81127586652951, -0.1085397937254431, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.75, 0.505, 0.0, 0.5, 0.4842729888774591, 0.46382006875818566, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.598914], dtype=float32), -0.15260899]. 
=============================================
[2019-04-07 15:49:57,826] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7884247e-26 7.0954129e-25 1.0563755e-23 4.7062548e-16 5.8173092e-22
 1.0000000e+00 3.1106327e-16 5.4251560e-17], sum to 1.0000
[2019-04-07 15:49:57,826] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1924
[2019-04-07 15:49:57,990] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 36.0, 92.0, 469.0, 24.0, 23.71097304548309, -0.009446381551175586, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4091400.0000, 
sim time next is 4093200.0000, 
raw observation next is [-3.0, 38.0, 98.0, 574.0, 24.0, 24.35154047869202, 0.05039779804583009, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.38, 0.32666666666666666, 0.6342541436464089, 0.5, 0.5292950398910016, 0.5167992660152767, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65117073], dtype=float32), 0.6745504]. 
=============================================
[2019-04-07 15:50:26,477] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:50:26,477] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:50:26,481] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run28
[2019-04-07 15:50:29,063] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:50:29,064] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:50:29,067] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run28
[2019-04-07 15:50:31,377] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:50:31,377] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:50:31,382] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run28
[2019-04-07 15:50:38,534] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:50:38,534] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:50:38,540] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run28
[2019-04-07 15:50:38,735] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:50:38,736] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:50:38,741] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run28
[2019-04-07 15:50:39,152] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:50:39,152] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:50:39,156] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run28
[2019-04-07 15:50:39,681] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:50:39,681] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:50:39,684] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run28
[2019-04-07 15:50:40,460] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:50:40,460] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:50:40,479] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run28
[2019-04-07 15:50:47,365] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:50:47,365] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:50:47,369] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run28
[2019-04-07 15:50:47,934] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:50:47,934] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:50:47,937] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run28
[2019-04-07 15:50:48,039] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9325988e-25 5.5863579e-26 7.5294457e-23 2.2428831e-15 3.0142277e-23
 1.0000000e+00 6.9314231e-16 2.6929525e-16], sum to 1.0000
[2019-04-07 15:50:48,039] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0091
[2019-04-07 15:50:48,137] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 24.0, 21.31438586249307, -0.5485331347517629, 0.0, 1.0, 40273.58792176733], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 25200.0000, 
sim time next is 27000.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 24.0, 21.38567164288263, -0.5295393678796717, 0.0, 1.0, 40268.12057780949], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 0.5, 0.2821393035735526, 0.3234868773734428, 0.0, 1.0, 0.19175295513242613], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1090156], dtype=float32), -0.79160047]. 
=============================================
[2019-04-07 15:50:48,141] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[103.03362 ]
 [102.73939 ]
 [102.465866]
 [102.17591 ]
 [101.87149 ]], R is [[103.22854614]
 [103.19625854]
 [103.16429901]
 [103.13265991]
 [103.10133362]].
[2019-04-07 15:50:48,886] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.9730096e-05 9.3844363e-05 1.2581954e-04 3.7452071e-03 2.2251961e-04
 9.9166119e-01 2.8457791e-03 1.2258695e-03], sum to 1.0000
[2019-04-07 15:50:48,887] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4207
[2019-04-07 15:50:48,938] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 18.7323782676697, -0.75, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 0.0000, 
sim time next is 1800.0000, 
raw observation next is [3.6, 95.5, 0.0, 0.0, 24.0, 18.90128101974305, -0.9919784346224004, 0.0, 1.0, 164399.77376409786], 
processed observation next is [0.0, 0.0, 0.5623268698060943, 0.955, 0.0, 0.0, 0.5, 0.07510675164525409, 0.1693405217925332, 0.0, 1.0, 0.7828560655433231], 
reward next is 0.5029, 
noisyNet noise sample is [array([0.36153185], dtype=float32), 0.13098934]. 
=============================================
[2019-04-07 15:51:04,309] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1163755e-25 6.8535920e-25 4.7672755e-23 1.3589769e-15 1.7369190e-22
 1.0000000e+00 7.8566647e-17 9.2444928e-18], sum to 1.0000
[2019-04-07 15:51:04,309] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2145
[2019-04-07 15:51:04,403] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 24.0, 23.09112701207183, -0.1409866267338252, 0.0, 1.0, 51241.54797238888], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 162000.0000, 
sim time next is 163800.0000, 
raw observation next is [-8.4, 69.5, 0.0, 0.0, 24.0, 22.97169766204649, -0.1681505151701038, 0.0, 1.0, 47867.40296404795], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.695, 0.0, 0.0, 0.5, 0.41430813850387427, 0.4439498282766321, 0.0, 1.0, 0.22794001411451403], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0918489], dtype=float32), 0.9317092]. 
=============================================
[2019-04-07 15:51:10,374] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9088157e-25 3.2757333e-24 2.9106345e-22 9.9492700e-14 1.5914882e-20
 1.0000000e+00 5.4562936e-15 2.8450193e-16], sum to 1.0000
[2019-04-07 15:51:10,375] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9766
[2019-04-07 15:51:10,425] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 24.0, 23.01755094446171, -0.2005295473985785, 0.0, 1.0, 40312.220762523786], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3034800.0000, 
sim time next is 3036600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 24.0, 22.96612308457762, -0.2134273689470805, 0.0, 1.0, 40692.56967958141], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.5, 0.4138435903814684, 0.4288575436843065, 0.0, 1.0, 0.19377414133134002], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.89406556], dtype=float32), 0.91160184]. 
=============================================
[2019-04-07 15:51:32,447] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3205566e-30 9.0937575e-30 2.5979583e-25 1.0153637e-16 5.7876592e-26
 1.0000000e+00 1.4798588e-19 1.5092191e-19], sum to 1.0000
[2019-04-07 15:51:32,448] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6063
[2019-04-07 15:51:32,487] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.6, 100.0, 0.0, 0.0, 24.0, 23.44889385565595, -0.108519712128563, 0.0, 1.0, 57583.7190807271], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3124800.0000, 
sim time next is 3126600.0000, 
raw observation next is [2.8, 100.0, 0.0, 0.0, 24.0, 23.54230492936516, -0.1104667994449768, 0.0, 1.0, 23584.080353207886], 
processed observation next is [1.0, 0.17391304347826086, 0.5401662049861496, 1.0, 0.0, 0.0, 0.5, 0.4618587441137632, 0.4631777335183411, 0.0, 1.0, 0.11230514453908517], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4633907], dtype=float32), 0.79440767]. 
=============================================
[2019-04-07 15:51:33,226] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3676022e-26 5.6569377e-24 1.8140497e-23 1.2236748e-15 3.6717312e-22
 1.0000000e+00 3.4291311e-16 8.4962143e-17], sum to 1.0000
[2019-04-07 15:51:33,226] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2863
[2019-04-07 15:51:33,325] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 24.0, 23.34019497039953, -0.1725892906214115, 0.0, 1.0, 43582.670641225624], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 684000.0000, 
sim time next is 685800.0000, 
raw observation next is [-3.65, 70.0, 0.0, 0.0, 24.0, 23.27112145968214, -0.1813020490171193, 0.0, 1.0, 43214.09434190096], 
processed observation next is [0.0, 0.9565217391304348, 0.3614958448753463, 0.7, 0.0, 0.0, 0.5, 0.4392601216401782, 0.4395659836609602, 0.0, 1.0, 0.20578140162809982], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1002984], dtype=float32), -0.44265196]. 
=============================================
[2019-04-07 15:51:42,379] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.3542450e-26 1.3655236e-24 2.3997168e-23 1.0318620e-14 2.7294734e-22
 1.0000000e+00 1.0768740e-15 7.8187792e-18], sum to 1.0000
[2019-04-07 15:51:42,379] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6471
[2019-04-07 15:51:42,449] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 24.0, 24.38030384211754, 0.1480418223887586, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3349800.0000, 
sim time next is 3351600.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 24.0, 23.98850794243369, 0.1015874909276506, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.5, 0.4990423285361407, 0.5338624969758835, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9588417], dtype=float32), 1.0488914]. 
=============================================
[2019-04-07 15:51:44,656] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.9083123e-24 1.8166050e-23 2.5568666e-21 4.3068389e-13 1.1679960e-21
 1.0000000e+00 8.5482709e-15 2.1036998e-16], sum to 1.0000
[2019-04-07 15:51:44,656] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9597
[2019-04-07 15:51:44,861] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 28.5, 0.0, 0.0, 24.0, 23.38696718685491, 0.02532778536215228, 1.0, 1.0, 64018.27805267187], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4041000.0000, 
sim time next is 4042800.0000, 
raw observation next is [-4.0, 31.0, 0.0, 0.0, 24.0, 23.78858658479458, 0.1205603638649787, 1.0, 1.0, 63302.37931425681], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.31, 0.0, 0.0, 0.5, 0.4823822153995483, 0.540186787954993, 1.0, 1.0, 0.301439901496461], 
reward next is 0.9843, 
noisyNet noise sample is [array([1.0388557], dtype=float32), -0.27538097]. 
=============================================
[2019-04-07 15:51:55,178] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-07 15:51:55,179] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:51:55,180] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:51:55,180] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:51:55,182] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:51:55,185] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run63
[2019-04-07 15:51:55,185] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 15:51:55,206] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:51:55,210] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run63
[2019-04-07 15:51:55,231] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run63
[2019-04-07 15:53:04,056] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11393287], dtype=float32), 0.14808828]
[2019-04-07 15:53:04,056] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-6.2, 87.0, 0.0, 0.0, 24.0, 23.00605636558726, -0.2153922511566723, 0.0, 1.0, 42022.405673624315]
[2019-04-07 15:53:04,056] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 15:53:04,057] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [3.2173997e-26 4.2098627e-25 3.1666896e-23 2.0551020e-15 1.2684905e-22
 1.0000000e+00 4.4771583e-16 1.7475238e-17], sampled 0.39478916701712696
[2019-04-07 15:54:17,545] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 15:54:39,476] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 15:54:42,119] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 15:54:43,144] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 1240000, evaluation results [1240000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 15:54:54,391] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.6970816e-27 6.0976583e-27 2.6033056e-25 2.8437295e-15 7.2984856e-23
 1.0000000e+00 4.6309035e-17 6.4218653e-19], sum to 1.0000
[2019-04-07 15:54:54,391] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8992
[2019-04-07 15:54:54,482] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 68.0, 157.0, 285.0, 24.0, 23.85754900740536, 0.02121152719295561, 1.0, 1.0, 62600.05751884459], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2215800.0000, 
sim time next is 2217600.0000, 
raw observation next is [-3.9, 68.0, 96.0, 142.5, 24.0, 24.33517750587576, 0.03834579341810049, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.68, 0.32, 0.1574585635359116, 0.5, 0.5279314588229799, 0.5127819311393668, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.86356777], dtype=float32), 0.74201113]. 
=============================================
[2019-04-07 15:55:03,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2272375e-31 9.2054149e-29 1.3380580e-27 1.1705893e-18 2.8080405e-28
 1.0000000e+00 8.2994191e-20 1.5719451e-20], sum to 1.0000
[2019-04-07 15:55:03,518] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7144
[2019-04-07 15:55:03,559] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 24.0, 23.75748268672869, 0.08438198848479143, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1652400.0000, 
sim time next is 1654200.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 24.0, 23.62303025105721, 0.1258261556984716, 0.0, 1.0, 111676.14757926842], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.5, 0.4685858542547674, 0.5419420518994905, 0.0, 1.0, 0.5317911789488973], 
reward next is 0.7539, 
noisyNet noise sample is [array([-0.23858365], dtype=float32), 0.23988347]. 
=============================================
[2019-04-07 15:55:13,576] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1488333e-25 2.0651124e-24 1.3748024e-22 3.9358720e-15 1.1433008e-21
 1.0000000e+00 1.5097800e-16 8.4482854e-17], sum to 1.0000
[2019-04-07 15:55:13,576] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3528
[2019-04-07 15:55:13,613] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 24.0, 23.65062803152453, 0.02834580914021587, 0.0, 1.0, 52577.6234917276], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4824000.0000, 
sim time next is 4825800.0000, 
raw observation next is [0.5, 49.0, 0.0, 0.0, 24.0, 23.81339338594529, 0.0152907582906188, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4764542936288089, 0.49, 0.0, 0.0, 0.5, 0.4844494488287741, 0.5050969194302063, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0333025], dtype=float32), 0.03617045]. 
=============================================
[2019-04-07 15:55:15,508] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:55:15,508] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:55:15,512] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run29
[2019-04-07 15:55:26,024] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:55:26,024] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:55:26,028] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run29
[2019-04-07 15:55:26,639] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6139741e-28 4.3726699e-27 4.3254259e-25 3.4329398e-16 1.3694882e-24
 1.0000000e+00 8.4171464e-17 7.5994655e-19], sum to 1.0000
[2019-04-07 15:55:26,640] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5846
[2019-04-07 15:55:26,750] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.35, 49.0, 139.0, 813.0, 24.0, 25.72333956081928, 0.483529082516235, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4627800.0000, 
sim time next is 4629600.0000, 
raw observation next is [4.7, 49.0, 171.0, 706.0, 24.0, 24.9094957767478, 0.4222584446461985, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.592797783933518, 0.49, 0.57, 0.7801104972375691, 0.5, 0.5757913147289834, 0.6407528148820661, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2941812], dtype=float32), -0.67380005]. 
=============================================
[2019-04-07 15:55:35,392] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5800776e-25 8.3667564e-25 7.5911927e-23 1.7121189e-14 3.4231999e-22
 1.0000000e+00 2.8241408e-15 7.1591436e-17], sum to 1.0000
[2019-04-07 15:55:35,392] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0596
[2019-04-07 15:55:35,444] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.58973198178835, -0.05740398368041468, 0.0, 1.0, 19491.242059150343], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4838400.0000, 
sim time next is 4840200.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.43119191569092, -0.08348199372088527, 0.0, 1.0, 60508.13932287068], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.5, 0.4525993263075767, 0.4721726687597049, 0.0, 1.0, 0.28813399677557466], 
reward next is 0.9976, 
noisyNet noise sample is [array([-0.9142312], dtype=float32), -0.002942694]. 
=============================================
[2019-04-07 15:55:46,293] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1022404e-25 1.5885917e-24 2.2433189e-23 7.9239403e-16 8.2916532e-23
 1.0000000e+00 4.3901437e-16 2.9584409e-17], sum to 1.0000
[2019-04-07 15:55:46,294] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4918
[2019-04-07 15:55:46,362] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 24.0, 22.9766374771572, -0.1819644006317934, 0.0, 1.0, 43762.35294913186], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2167200.0000, 
sim time next is 2169000.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 24.0, 23.18328820834455, -0.1767577236396035, 0.0, 1.0, 43630.38345488868], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 0.5, 0.4319406840287125, 0.44108075878679887, 0.0, 1.0, 0.20776373073756516], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4713737], dtype=float32), 0.27193764]. 
=============================================
[2019-04-07 15:55:46,382] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[97.06945]
 [97.1673 ]
 [97.34969]
 [97.75707]
 [97.95994]], R is [[97.03632355]
 [97.06596375]
 [97.0953064 ]
 [97.1243515 ]
 [97.15310669]].
[2019-04-07 15:55:47,141] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:55:47,141] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:55:47,144] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run29
[2019-04-07 15:55:47,898] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:55:47,898] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:55:47,904] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run29
[2019-04-07 15:55:53,127] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3420583e-23 2.4352669e-23 3.1761900e-21 2.2995483e-14 4.2321423e-20
 1.0000000e+00 6.2539056e-15 3.0620680e-16], sum to 1.0000
[2019-04-07 15:55:53,127] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9648
[2019-04-07 15:55:53,241] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.9, 68.0, 0.0, 0.0, 24.0, 22.40905286769505, -0.3350494618038556, 0.0, 1.0, 48287.41644001284], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 343800.0000, 
sim time next is 345600.0000, 
raw observation next is [-13.9, 66.0, 0.0, 0.0, 24.0, 22.30761453791205, -0.3612878928491468, 0.0, 1.0, 48333.95385626658], 
processed observation next is [1.0, 0.0, 0.07756232686980608, 0.66, 0.0, 0.0, 0.5, 0.35896787815933734, 0.37957070238361773, 0.0, 1.0, 0.23016168502984083], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7776194], dtype=float32), -1.9937633]. 
=============================================
[2019-04-07 15:56:08,309] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:56:08,309] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:56:08,313] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run29
[2019-04-07 15:56:12,969] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3804583e-25 1.5757722e-24 2.6933554e-22 1.0686409e-14 1.3146668e-21
 1.0000000e+00 5.0775860e-15 8.1423491e-18], sum to 1.0000
[2019-04-07 15:56:12,970] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8269
[2019-04-07 15:56:12,995] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.65, 34.5, 218.0, 22.0, 24.0, 24.20639484559186, -0.05850100105503181, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2550600.0000, 
sim time next is 2552400.0000, 
raw observation next is [2.2, 30.0, 194.5, 252.0, 24.0, 24.07925180883654, -0.07095318667787624, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5235457063711911, 0.3, 0.6483333333333333, 0.27845303867403315, 0.5, 0.5066043174030449, 0.47634893777404125, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00725587], dtype=float32), -0.43942735]. 
=============================================
[2019-04-07 15:56:21,001] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6735833e-25 4.2534816e-24 2.5516830e-21 7.3653221e-14 3.8724020e-21
 1.0000000e+00 1.4549200e-14 3.0231499e-17], sum to 1.0000
[2019-04-07 15:56:21,001] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9507
[2019-04-07 15:56:21,052] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.5, 83.0, 0.0, 0.0, 24.0, 21.78692877617548, -0.4400877034776151, 0.0, 1.0, 44111.19721834703], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2701800.0000, 
sim time next is 2703600.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 24.0, 21.65922433945151, -0.482345261367944, 0.0, 1.0, 44052.323161073844], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.5, 0.3049353616209591, 0.33921824621068536, 0.0, 1.0, 0.20977296743368498], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4713808], dtype=float32), 1.8005496]. 
=============================================
[2019-04-07 15:56:38,754] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8405030e-27 1.9313155e-26 1.1831057e-25 2.0982179e-16 3.9002971e-24
 1.0000000e+00 1.2484607e-17 1.5735005e-18], sum to 1.0000
[2019-04-07 15:56:38,755] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2598
[2019-04-07 15:56:38,822] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 24.0, 23.20392650600022, -0.08636485750658608, 0.0, 1.0, 43746.82139864576], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2950200.0000, 
sim time next is 2952000.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 24.0, 23.12094015663507, -0.1048122286624172, 0.0, 1.0, 43771.430366826105], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.84, 0.0, 0.0, 0.5, 0.4267450130529224, 0.46506259044586096, 0.0, 1.0, 0.20843538269917192], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3731868], dtype=float32), -1.6408529]. 
=============================================
[2019-04-07 15:56:38,842] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[100.063644]
 [100.35937 ]
 [ 99.89501 ]
 [ 99.5908  ]
 [100.357506]], R is [[99.74804688]
 [99.75056458]
 [99.75305939]
 [99.75553131]
 [99.75798035]].
[2019-04-07 15:56:47,551] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-07 15:56:47,565] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:56:47,565] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:56:47,569] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run64
[2019-04-07 15:56:47,590] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 15:56:47,597] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:56:47,601] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run64
[2019-04-07 15:56:47,625] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 15:56:47,627] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:56:47,632] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run64
[2019-04-07 15:59:05,262] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 15:59:23,551] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 15:59:28,910] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 15:59:29,932] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1260000, evaluation results [1260000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 15:59:53,567] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.1428204e-24 1.1135050e-24 4.1778110e-21 2.2220325e-14 1.5709380e-20
 1.0000000e+00 1.7696778e-14 3.7327522e-16], sum to 1.0000
[2019-04-07 15:59:53,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7572
[2019-04-07 15:59:53,619] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 24.0, 23.60016150839085, 0.0590345336281228, 0.0, 1.0, 37001.98311625476], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3963600.0000, 
sim time next is 3965400.0000, 
raw observation next is [-7.5, 47.0, 0.0, 0.0, 24.0, 23.56669251751162, 0.05600304679707811, 0.0, 1.0, 51308.5600374758], 
processed observation next is [1.0, 0.9130434782608695, 0.2548476454293629, 0.47, 0.0, 0.0, 0.5, 0.46389104312596824, 0.5186676822656927, 0.0, 1.0, 0.24432647636893237], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01934193], dtype=float32), -0.20984046]. 
=============================================
[2019-04-07 15:59:55,776] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 15:59:55,776] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 15:59:55,779] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run29
[2019-04-07 15:59:57,755] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4344435e-25 4.9298379e-23 1.7959760e-22 1.9127153e-14 6.2904430e-22
 1.0000000e+00 3.2378271e-14 1.0833386e-15], sum to 1.0000
[2019-04-07 15:59:57,755] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6775
[2019-04-07 15:59:57,806] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 42.5, 86.0, 699.0, 24.0, 23.76487465648191, 0.1125136761791824, 0.0, 1.0, 6229.5960233948135], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3598200.0000, 
sim time next is 3600000.0000, 
raw observation next is [0.0, 43.0, 74.5, 607.0, 24.0, 23.84556010945287, 0.1024442096367569, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.43, 0.24833333333333332, 0.6707182320441989, 0.5, 0.4871300091210724, 0.5341480698789189, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0022862], dtype=float32), 0.699524]. 
=============================================
[2019-04-07 15:59:57,814] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[92.76001]
 [92.96279]
 [93.08904]
 [93.12912]
 [93.21013]], R is [[92.52683258]
 [92.6015625 ]
 [92.67554474]
 [92.74878693]
 [92.82129669]].
[2019-04-07 16:00:10,454] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9975428e-25 7.8456153e-24 1.2964929e-22 4.0962047e-15 2.1107280e-21
 1.0000000e+00 4.8328439e-16 2.0900297e-16], sum to 1.0000
[2019-04-07 16:00:10,455] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2776
[2019-04-07 16:00:10,568] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 35.0, 114.0, 774.0, 24.0, 23.41088855737374, -0.01993515364389331, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4185000.0000, 
sim time next is 4186800.0000, 
raw observation next is [-1.0, 35.0, 116.5, 798.0, 24.0, 23.32014898013743, -0.02723953815310351, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4349030470914128, 0.35, 0.3883333333333333, 0.881767955801105, 0.5, 0.44334574834478574, 0.49092015394896554, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5738201], dtype=float32), -0.9018048]. 
=============================================
[2019-04-07 16:00:19,419] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3178977e-25 9.0101870e-24 7.6469712e-22 9.1026768e-15 9.9650495e-22
 1.0000000e+00 2.5266040e-15 8.8968264e-17], sum to 1.0000
[2019-04-07 16:00:19,419] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7817
[2019-04-07 16:00:19,476] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 45.0, 0.0, 0.0, 24.0, 23.61591179902994, -0.06386490474380559, 0.0, 1.0, 29366.591555302282], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4224600.0000, 
sim time next is 4226400.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 24.0, 23.5531708547421, -0.05558905249916202, 0.0, 1.0, 56418.75614158335], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.47, 0.0, 0.0, 0.5, 0.4627642378951749, 0.48147031583361266, 0.0, 1.0, 0.2686607435313493], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6495883], dtype=float32), -1.3448224]. 
=============================================
[2019-04-07 16:00:24,697] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.8508229e-28 4.7506729e-27 8.0032217e-25 2.5700829e-17 6.6691166e-23
 1.0000000e+00 1.1966183e-17 3.4685283e-19], sum to 1.0000
[2019-04-07 16:00:24,698] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3846
[2019-04-07 16:00:24,803] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 24.0, 23.27854774911933, -0.02853223235408448, 1.0, 1.0, 25406.176172017163], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4735800.0000, 
sim time next is 4737600.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 24.0, 23.08658583009058, -0.03180030565351619, 0.0, 1.0, 62853.74485474941], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.78, 0.0, 0.0, 0.5, 0.4238821525075484, 0.4893998981154946, 0.0, 1.0, 0.29930354692737815], 
reward next is 0.9864, 
noisyNet noise sample is [array([-1.2627933], dtype=float32), -0.6035978]. 
=============================================
[2019-04-07 16:00:25,089] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3179342e-28 2.4209490e-29 6.9180444e-27 1.1594080e-18 1.6598456e-24
 1.0000000e+00 5.0290751e-19 1.2920402e-21], sum to 1.0000
[2019-04-07 16:00:25,116] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7116
[2019-04-07 16:00:25,210] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 72.0, 123.5, 5.5, 24.0, 24.29729965139924, 0.09886342524691304, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4723200.0000, 
sim time next is 4725000.0000, 
raw observation next is [1.0, 72.0, 100.0, 11.0, 24.0, 23.84018099679357, 0.02287966615407069, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.72, 0.3333333333333333, 0.012154696132596685, 0.5, 0.48668174973279754, 0.5076265553846903, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7700602], dtype=float32), 0.8152338]. 
=============================================
[2019-04-07 16:00:25,255] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[104.702705]
 [105.62625 ]
 [105.91869 ]
 [106.58402 ]
 [107.11748 ]], R is [[103.93816376]
 [103.89878082]
 [103.85979462]
 [103.82119751]
 [103.7829895 ]].
[2019-04-07 16:00:39,728] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.8673529e-25 2.3263011e-24 3.2171853e-22 1.1278742e-15 1.4923119e-21
 1.0000000e+00 7.4928259e-16 5.8070392e-16], sum to 1.0000
[2019-04-07 16:00:39,728] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7164
[2019-04-07 16:00:39,812] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 45.0, 0.0, 0.0, 24.0, 23.18432387544205, -0.0518837748524126, 0.0, 1.0, 145302.92592218707], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4822200.0000, 
sim time next is 4824000.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 24.0, 23.65062803152453, 0.02834580914021587, 0.0, 1.0, 52577.6234917276], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.47, 0.0, 0.0, 0.5, 0.47088566929371084, 0.5094486030467386, 0.0, 1.0, 0.25036963567489334], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6871707], dtype=float32), 1.7298716]. 
=============================================
[2019-04-07 16:00:39,847] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[92.22716]
 [90.69559]
 [90.49594]
 [90.89186]
 [91.30075]], R is [[93.05336761]
 [92.71662903]
 [92.78946686]
 [92.86157227]
 [92.93296051]].
[2019-04-07 16:00:41,191] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:00:41,191] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:00:41,199] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run29
[2019-04-07 16:00:42,553] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:00:42,553] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:00:42,557] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run29
[2019-04-07 16:00:43,868] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1520205e-23 5.6133489e-22 5.1770398e-23 7.2479532e-14 4.5044884e-21
 1.0000000e+00 1.4924482e-14 4.6402604e-16], sum to 1.0000
[2019-04-07 16:00:43,869] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4129
[2019-04-07 16:00:43,937] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.9, 27.5, 87.0, 832.0, 24.0, 23.08301294770494, -0.1537152346476797, 0.0, 1.0, 12463.528354547205], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2467800.0000, 
sim time next is 2469600.0000, 
raw observation next is [2.2, 27.0, 82.5, 808.0, 24.0, 23.09085857784311, -0.1512209523232629, 0.0, 1.0, 12459.647591599898], 
processed observation next is [0.0, 0.6086956521739131, 0.5235457063711911, 0.27, 0.275, 0.8928176795580111, 0.5, 0.42423821482025925, 0.4495930158922457, 0.0, 1.0, 0.05933165519809475], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42916256], dtype=float32), -0.015450412]. 
=============================================
[2019-04-07 16:00:48,021] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:00:48,021] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:00:48,024] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run29
[2019-04-07 16:00:52,073] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:00:52,073] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:00:52,097] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run29
[2019-04-07 16:00:53,818] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:00:53,818] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:00:53,822] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run29
[2019-04-07 16:00:54,992] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1686191e-30 1.0960633e-29 4.5149514e-27 2.9961913e-18 2.8787165e-26
 1.0000000e+00 2.0997612e-18 8.6659843e-21], sum to 1.0000
[2019-04-07 16:00:54,993] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9467
[2019-04-07 16:00:55,222] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.1, 87.5, 0.0, 0.0, 24.0, 22.75608941333962, -0.197900456247739, 0.0, 1.0, 32655.397033876827], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 66600.0000, 
sim time next is 68400.0000, 
raw observation next is [3.8, 86.0, 0.0, 0.0, 24.0, 22.76415128936222, -0.1960315733194058, 0.0, 1.0, 35215.74422032802], 
processed observation next is [0.0, 0.8260869565217391, 0.5678670360110805, 0.86, 0.0, 0.0, 0.5, 0.3970126074468518, 0.4346561422268647, 0.0, 1.0, 0.1676940200968001], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48439613], dtype=float32), -0.46073607]. 
=============================================
[2019-04-07 16:00:55,813] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:00:55,813] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:00:55,818] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run29
[2019-04-07 16:00:56,557] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:00:56,558] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:00:56,561] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run29
[2019-04-07 16:00:57,497] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:00:57,497] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:00:57,500] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run29
[2019-04-07 16:00:58,194] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5307707e-27 3.9339735e-26 1.4298559e-24 1.7589336e-17 5.2856767e-23
 1.0000000e+00 8.7526813e-17 9.6446331e-18], sum to 1.0000
[2019-04-07 16:00:58,194] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1759
[2019-04-07 16:00:58,418] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 24.0, 24.0953166175642, -0.07186854908548154, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2050200.0000, 
sim time next is 2052000.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 24.0, 23.25616273655161, -0.08131047674451229, 1.0, 1.0, 51963.3812872353], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.5, 0.43801356137930075, 0.47289650775182923, 1.0, 1.0, 0.24744467279635857], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.167921], dtype=float32), 2.7909768]. 
=============================================
[2019-04-07 16:00:58,422] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[101.70036 ]
 [101.838844]
 [101.74422 ]
 [100.576454]
 [100.86891 ]], R is [[101.64279938]
 [101.62637329]
 [101.61010742]
 [101.43731689]
 [101.42294312]].
[2019-04-07 16:01:02,381] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:01:02,381] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:01:02,384] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run29
[2019-04-07 16:01:02,889] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:01:02,889] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:01:02,893] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run29
[2019-04-07 16:01:05,010] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.9236146e-24 2.9444596e-23 6.0328215e-22 2.7108548e-13 8.9213503e-20
 1.0000000e+00 4.3962967e-15 8.6268220e-16], sum to 1.0000
[2019-04-07 16:01:05,010] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6714
[2019-04-07 16:01:05,102] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 61.0, 0.0, 0.0, 24.0, 21.91134359473809, -0.4771824298225114, 0.0, 1.0, 44862.890199724934], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2444400.0000, 
sim time next is 2446200.0000, 
raw observation next is [-9.5, 59.5, 0.0, 0.0, 24.0, 21.82266396609261, -0.4964186357717622, 0.0, 1.0, 44961.20221348965], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.595, 0.0, 0.0, 0.5, 0.3185553305077174, 0.3345271214094126, 0.0, 1.0, 0.21410096292137928], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3215632], dtype=float32), 1.3626326]. 
=============================================
[2019-04-07 16:01:16,295] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.47152981e-25 1.26292696e-24 1.36120758e-23 1.97202789e-14
 1.16646045e-20 1.00000000e+00 3.19971752e-16 1.09543515e-16], sum to 1.0000
[2019-04-07 16:01:16,296] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9279
[2019-04-07 16:01:16,562] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 61.0, 104.5, 75.5, 24.0, 23.51630057880474, -0.04772401474458243, 1.0, 1.0, 65465.562942386074], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 140400.0000, 
sim time next is 142200.0000, 
raw observation next is [-6.7, 62.5, 61.0, 45.0, 24.0, 23.98636164911574, 0.0461763761610753, 1.0, 1.0, 85131.26064295774], 
processed observation next is [1.0, 0.6521739130434783, 0.2770083102493075, 0.625, 0.20333333333333334, 0.049723756906077346, 0.5, 0.49886347075964493, 0.5153921253870252, 1.0, 1.0, 0.4053869554426559], 
reward next is 0.8803, 
noisyNet noise sample is [array([0.98814636], dtype=float32), 0.5002936]. 
=============================================
[2019-04-07 16:01:21,045] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2083840e-25 7.7997458e-25 1.6274945e-22 1.2630483e-15 2.9571824e-21
 1.0000000e+00 6.4333259e-16 1.0162466e-17], sum to 1.0000
[2019-04-07 16:01:21,045] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3487
[2019-04-07 16:01:21,118] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 24.0, 22.73374998847608, -0.229197639983223, 0.0, 1.0, 46337.35280507108], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 169200.0000, 
sim time next is 171000.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 24.0, 22.63762688865307, -0.2509347552135462, 0.0, 1.0, 46067.92760776743], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.5, 0.38646890738775586, 0.4163550815954846, 0.0, 1.0, 0.21937108384651158], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.379533], dtype=float32), 0.54820406]. 
=============================================
[2019-04-07 16:01:21,123] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[91.419785]
 [91.37782 ]
 [91.5214  ]
 [91.70613 ]
 [92.045395]], R is [[91.14626312]
 [91.23480225]
 [91.32245636]
 [91.40923309]
 [91.49514008]].
[2019-04-07 16:01:31,912] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-07 16:01:31,912] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:01:31,913] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:01:31,916] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run65
[2019-04-07 16:01:31,940] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:01:31,940] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:01:31,943] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 16:01:31,943] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:01:31,950] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run65
[2019-04-07 16:01:31,975] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run65
[2019-04-07 16:03:33,647] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11394516], dtype=float32), 0.14893246]
[2019-04-07 16:03:33,647] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-6.5, 88.0, 0.0, 0.0, 24.0, 23.4324468317476, 0.02759547169777271, 0.0, 1.0, 36469.89874637019]
[2019-04-07 16:03:33,647] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 16:03:33,648] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [1.8270257e-27 3.6041141e-26 3.4766268e-24 4.3308612e-16 1.1962176e-23
 1.0000000e+00 7.1666306e-17 1.7594762e-18], sampled 0.8318939366031164
[2019-04-07 16:03:54,010] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 16:04:11,711] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 16:04:15,386] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 16:04:16,410] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1280000, evaluation results [1280000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 16:04:16,429] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5710495e-23 7.0658942e-24 5.5496686e-21 2.6431768e-14 2.3875761e-20
 1.0000000e+00 1.1233957e-15 8.9793772e-16], sum to 1.0000
[2019-04-07 16:04:16,430] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3167
[2019-04-07 16:04:16,573] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.0, 42.0, 0.0, 0.0, 24.0, 23.34412565878754, -0.1352676713896013, 0.0, 1.0, 54852.50122157626], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 417600.0000, 
sim time next is 419400.0000, 
raw observation next is [-10.3, 44.5, 0.0, 0.0, 24.0, 23.29686304917654, -0.1436610011428048, 0.0, 1.0, 75755.58865926276], 
processed observation next is [1.0, 0.8695652173913043, 0.1772853185595568, 0.445, 0.0, 0.0, 0.5, 0.44140525409804504, 0.4521129996190651, 0.0, 1.0, 0.3607408983774417], 
reward next is 0.9250, 
noisyNet noise sample is [array([1.091975], dtype=float32), 0.098470464]. 
=============================================
[2019-04-07 16:04:46,791] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1415292e-25 1.3706119e-24 7.4398804e-23 2.1723100e-15 2.8023136e-22
 1.0000000e+00 4.1447109e-15 1.5420322e-16], sum to 1.0000
[2019-04-07 16:04:46,797] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6723
[2019-04-07 16:04:46,848] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 73.5, 0.0, 0.0, 24.0, 22.71540219626197, -0.269698762372358, 0.0, 1.0, 41085.770503366024], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3043800.0000, 
sim time next is 3045600.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 24.0, 22.65489570911155, -0.2852471026731074, 0.0, 1.0, 41062.28424592698], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.5, 0.3879079757592958, 0.40491763244229756, 0.0, 1.0, 0.19553468688536657], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5294623], dtype=float32), -0.027535133]. 
=============================================
[2019-04-07 16:04:57,881] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7112104e-30 9.6926379e-29 1.5711781e-27 1.9890293e-19 2.1535696e-25
 1.0000000e+00 3.4800473e-18 9.9913571e-21], sum to 1.0000
[2019-04-07 16:04:57,881] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8337
[2019-04-07 16:04:57,935] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.15, 81.0, 0.0, 0.0, 24.0, 23.64693547121273, -0.006627228555247384, 0.0, 1.0, 6364.220790626364], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 959400.0000, 
sim time next is 961200.0000, 
raw observation next is [7.7, 80.0, 0.0, 0.0, 24.0, 23.50750071384315, -0.005550045009981209, 0.0, 1.0, 59436.42077483015], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.8, 0.0, 0.0, 0.5, 0.4589583928202625, 0.4981499849966729, 0.0, 1.0, 0.2830305751182388], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2591763], dtype=float32), 2.746153]. 
=============================================
[2019-04-07 16:05:01,089] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0793844e-28 4.5883263e-27 1.4612813e-24 2.5372655e-16 4.4383037e-23
 1.0000000e+00 5.8197655e-17 9.6704176e-19], sum to 1.0000
[2019-04-07 16:05:01,089] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3889
[2019-04-07 16:05:01,137] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 24.0, 24.29359319215518, 0.133717956311152, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3781800.0000, 
sim time next is 3783600.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 24.0, 23.87227256895463, 0.05633470301993926, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.40720221606648205, 0.71, 0.0, 0.0, 0.5, 0.48935604741288596, 0.5187782343399797, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.55150807], dtype=float32), 0.43609244]. 
=============================================
[2019-04-07 16:05:20,026] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.3799796e-25 9.5120355e-26 2.3032820e-23 5.6742102e-15 5.4295491e-23
 1.0000000e+00 2.1745890e-16 4.2999108e-17], sum to 1.0000
[2019-04-07 16:05:20,026] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3882
[2019-04-07 16:05:20,075] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 24.0, 23.50489271282122, -0.03676641860959991, 0.0, 1.0, 53844.47183923682], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4831200.0000, 
sim time next is 4833000.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 24.0, 23.52149161772119, -0.04719528359102401, 0.0, 1.0, 30640.752536235756], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.5, 0.46012430147676575, 0.48426823880299197, 0.0, 1.0, 0.14590834541064646], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1998669], dtype=float32), 1.0099633]. 
=============================================
[2019-04-07 16:05:20,078] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[93.23664 ]
 [93.21258 ]
 [93.01984 ]
 [93.215614]
 [93.032875]], R is [[93.21005249]
 [93.2779541 ]
 [93.3451767 ]
 [93.41172791]
 [93.47760773]].
[2019-04-07 16:05:20,817] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9535334e-26 1.3171747e-24 2.7007836e-22 9.4617074e-16 4.0182040e-22
 1.0000000e+00 4.0993446e-16 4.3741271e-17], sum to 1.0000
[2019-04-07 16:05:20,818] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6544
[2019-04-07 16:05:20,860] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 81.0, 0.0, 0.0, 24.0, 22.6043586126985, -0.2728422504956386, 0.0, 1.0, 47803.7263514625], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1830600.0000, 
sim time next is 1832400.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 24.0, 22.51876535757698, -0.2898031404646753, 0.0, 1.0, 47880.86253628895], 
processed observation next is [0.0, 0.21739130434782608, 0.2908587257617729, 0.79, 0.0, 0.0, 0.5, 0.3765637797980818, 0.4033989531784416, 0.0, 1.0, 0.22800410731566167], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.71081066], dtype=float32), -1.5458606]. 
=============================================
[2019-04-07 16:05:24,183] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:05:24,183] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:05:24,198] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run30
[2019-04-07 16:05:32,449] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:05:32,449] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:05:32,452] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run30
[2019-04-07 16:05:33,454] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.2113926e-25 8.1568691e-25 5.0137243e-22 3.0273769e-15 1.8434271e-21
 1.0000000e+00 3.3563850e-16 8.9409198e-17], sum to 1.0000
[2019-04-07 16:05:33,454] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9466
[2019-04-07 16:05:33,605] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.0, 33.5, 0.0, 0.0, 24.0, 22.93616344531996, -0.174049524809362, 1.0, 1.0, 121942.81577336685], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2831400.0000, 
sim time next is 2833200.0000, 
raw observation next is [3.0, 37.0, 0.0, 0.0, 24.0, 22.85534812928762, -0.1449197999831049, 1.0, 1.0, 88504.79302426695], 
processed observation next is [1.0, 0.8260869565217391, 0.5457063711911359, 0.37, 0.0, 0.0, 0.5, 0.4046123441073017, 0.4516934000056317, 1.0, 1.0, 0.42145139535365217], 
reward next is 0.8643, 
noisyNet noise sample is [array([-0.40407106], dtype=float32), -0.69433576]. 
=============================================
[2019-04-07 16:05:41,592] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7696619e-24 7.8471381e-25 3.3429979e-21 2.2848070e-13 1.4677984e-20
 1.0000000e+00 1.0577109e-14 1.0325303e-16], sum to 1.0000
[2019-04-07 16:05:41,604] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7193
[2019-04-07 16:05:41,738] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 145.0, 20.0, 24.0, 23.08689000027113, -0.1617581789251946, 0.0, 1.0, 44799.04488544567], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1861200.0000, 
sim time next is 1863000.0000, 
raw observation next is [-4.5, 71.0, 170.0, 40.0, 24.0, 23.11548651784694, -0.1547528735696784, 0.0, 1.0, 33398.2867340357], 
processed observation next is [0.0, 0.5652173913043478, 0.3379501385041552, 0.71, 0.5666666666666667, 0.04419889502762431, 0.5, 0.4262905431539116, 0.4484157088101072, 0.0, 1.0, 0.1590394606382652], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2913916], dtype=float32), -0.8557372]. 
=============================================
[2019-04-07 16:05:41,744] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[93.45927 ]
 [93.364555]
 [93.73404 ]
 [93.97284 ]
 [93.54274 ]], R is [[93.61911774]
 [93.68292999]
 [93.74610138]
 [93.80863953]
 [93.87055206]].
[2019-04-07 16:05:57,922] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:05:57,922] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:05:57,941] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run30
[2019-04-07 16:05:58,385] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:05:58,385] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:05:58,393] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run30
[2019-04-07 16:06:05,088] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.01561466e-25 2.38601433e-24 8.23346875e-23 1.63279932e-14
 7.84011821e-22 1.00000000e+00 1.93838031e-17 3.14374657e-17], sum to 1.0000
[2019-04-07 16:06:05,088] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6540
[2019-04-07 16:06:05,296] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 68.0, 105.5, 0.0, 24.0, 24.1471951436504, -0.08957231435626657, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2127600.0000, 
sim time next is 2129400.0000, 
raw observation next is [-4.75, 66.5, 86.0, 0.0, 24.0, 23.62267295896489, -0.02061226611841474, 1.0, 1.0, 84571.75848368973], 
processed observation next is [1.0, 0.6521739130434783, 0.3310249307479225, 0.665, 0.2866666666666667, 0.0, 0.5, 0.46855607991374093, 0.4931292446271951, 1.0, 1.0, 0.40272265944614155], 
reward next is 0.8830, 
noisyNet noise sample is [array([3.753547], dtype=float32), -0.15785043]. 
=============================================
[2019-04-07 16:06:16,601] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:06:16,601] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:06:16,604] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run30
[2019-04-07 16:06:22,626] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-07 16:06:22,645] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:06:22,645] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:06:22,649] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run66
[2019-04-07 16:06:22,683] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:06:22,684] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:06:22,685] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 16:06:22,685] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:06:22,690] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run66
[2019-04-07 16:06:22,716] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run66
[2019-04-07 16:08:46,613] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 16:09:05,741] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 16:09:08,811] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 16:09:09,835] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 1300000, evaluation results [1300000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 16:09:39,944] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.8399853e-28 8.2935067e-26 3.7846695e-24 9.2075128e-16 1.9082791e-23
 1.0000000e+00 7.7166157e-17 9.7842645e-18], sum to 1.0000
[2019-04-07 16:09:39,945] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0761
[2019-04-07 16:09:40,021] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 24.0, 23.51794952147753, -0.03249194960935935, 0.0, 1.0, 26483.925061323203], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4518000.0000, 
sim time next is 4519800.0000, 
raw observation next is [-0.9, 72.0, 0.0, 0.0, 24.0, 23.49317660479851, -0.04282111465504116, 1.0, 1.0, 12904.204287834558], 
processed observation next is [1.0, 0.30434782608695654, 0.43767313019390586, 0.72, 0.0, 0.0, 0.5, 0.4577647170665425, 0.4857262951149863, 1.0, 1.0, 0.06144859184683123], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06437846], dtype=float32), -1.6832322]. 
=============================================
[2019-04-07 16:09:40,092] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.3598928e-28 1.3157177e-25 2.7217673e-23 5.5414748e-16 7.4758725e-22
 1.0000000e+00 4.6852791e-16 1.8565430e-17], sum to 1.0000
[2019-04-07 16:09:40,092] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4753
[2019-04-07 16:09:40,147] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.5, 86.5, 0.0, 0.0, 24.0, 23.65393525557087, 0.1104616392845608, 0.0, 1.0, 16671.73812439156], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3274200.0000, 
sim time next is 3276000.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 24.0, 23.5345357736723, 0.1107485447733379, 0.0, 1.0, 69775.0091327239], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.5, 0.46121131447269165, 0.5369161815911127, 0.0, 1.0, 0.3322619482510662], 
reward next is 0.9535, 
noisyNet noise sample is [array([0.5228416], dtype=float32), -0.7863218]. 
=============================================
[2019-04-07 16:09:40,150] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[100.42528 ]
 [100.34724 ]
 [100.20953 ]
 [100.027176]
 [101.199295]], R is [[101.42544556]
 [101.41119385]
 [101.39707947]
 [101.20417786]
 [101.19213867]].
[2019-04-07 16:10:05,999] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:10:06,007] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:10:06,012] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run30
[2019-04-07 16:10:07,231] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8722449e-27 3.2101299e-26 4.3603506e-24 2.1283119e-15 1.8793687e-22
 1.0000000e+00 2.7655462e-16 6.9834086e-18], sum to 1.0000
[2019-04-07 16:10:07,232] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2966
[2019-04-07 16:10:07,320] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 63.0, 0.0, 0.0, 24.0, 23.46459133585084, 0.02808254720646851, 0.0, 1.0, 120392.39518573503], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3700800.0000, 
sim time next is 3702600.0000, 
raw observation next is [2.5, 62.5, 0.0, 0.0, 24.0, 23.9805298649683, 0.05539889014823082, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.5318559556786704, 0.625, 0.0, 0.0, 0.5, 0.4983774887473584, 0.5184662967160769, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17001465], dtype=float32), -0.47704914]. 
=============================================
[2019-04-07 16:10:36,824] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2178890e-27 6.5522972e-27 6.9872779e-24 3.0402098e-16 7.1318643e-23
 1.0000000e+00 4.6823844e-16 9.5565549e-18], sum to 1.0000
[2019-04-07 16:10:36,825] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0485
[2019-04-07 16:10:36,888] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.2, 64.0, 0.0, 0.0, 24.0, 23.73287273452377, -0.02069725291044318, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4298400.0000, 
sim time next is 4300200.0000, 
raw observation next is [6.1, 66.5, 0.0, 0.0, 24.0, 23.55404928147806, -0.06088859153318471, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6315789473684211, 0.665, 0.0, 0.0, 0.5, 0.4628374401231718, 0.47970380282227176, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05791883], dtype=float32), 1.4673933]. 
=============================================
[2019-04-07 16:10:52,714] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.8810090e-28 1.3121919e-28 3.8551788e-25 1.4186250e-14 8.6416369e-25
 1.0000000e+00 3.8831048e-17 4.1901847e-19], sum to 1.0000
[2019-04-07 16:10:52,714] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6066
[2019-04-07 16:10:52,785] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 125.5, 800.0, 24.0, 25.38825155976026, 0.2255330771819329, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4618800.0000, 
sim time next is 4620600.0000, 
raw observation next is [2.5, 50.5, 122.0, 833.0, 24.0, 25.26770051244191, 0.2316336024480952, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5318559556786704, 0.505, 0.4066666666666667, 0.9204419889502763, 0.5, 0.605641709370159, 0.5772112008160317, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.69214135], dtype=float32), -0.70655644]. 
=============================================
[2019-04-07 16:10:53,890] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9057083e-26 5.0308638e-25 2.8319953e-22 3.4397696e-16 1.5932247e-21
 1.0000000e+00 5.8760063e-16 2.2710598e-17], sum to 1.0000
[2019-04-07 16:10:53,920] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5806
[2019-04-07 16:10:53,971] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.41874303814862, -0.07077523793925554, 0.0, 1.0, 57677.822319924504], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4842000.0000, 
sim time next is 4843800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.45174695487147, -0.0822457935546174, 0.0, 1.0, 23611.12396683109], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.5, 0.45431224623928923, 0.4725847354817942, 0.0, 1.0, 0.11243392365157662], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.7781146], dtype=float32), -0.17181484]. 
=============================================
[2019-04-07 16:10:56,951] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:10:56,951] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:10:56,955] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run30
[2019-04-07 16:10:57,163] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:10:57,163] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:10:57,178] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run30
[2019-04-07 16:11:02,679] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-07 16:11:02,679] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:11:02,679] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:11:02,682] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:11:02,682] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:11:02,692] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 16:11:02,692] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:11:02,694] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run67
[2019-04-07 16:11:02,726] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run67
[2019-04-07 16:11:02,728] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run67
[2019-04-07 16:13:18,346] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 16:13:44,745] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.7460 79463814.5229 95.0531
[2019-04-07 16:13:49,325] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 16:13:50,368] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1320000, evaluation results [1320000.0, 2782.745986527074, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 16:13:52,315] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.8420162e-26 3.9938847e-25 1.2452036e-23 2.8486588e-15 4.3015401e-22
 1.0000000e+00 1.1223200e-16 1.0813681e-17], sum to 1.0000
[2019-04-07 16:13:52,317] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9523
[2019-04-07 16:13:52,392] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 130.0, 0.0, 24.0, 24.06144890752602, -0.06285641147013508, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2199600.0000, 
sim time next is 2201400.0000, 
raw observation next is [-4.2, 69.5, 143.0, 0.0, 24.0, 23.80784583505428, -0.08852622248060564, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.34626038781163443, 0.695, 0.4766666666666667, 0.0, 0.5, 0.48398715292119004, 0.47049125917313145, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5656214], dtype=float32), -0.86236054]. 
=============================================
[2019-04-07 16:13:53,020] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:13:53,021] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:13:53,025] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run30
[2019-04-07 16:13:55,981] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:13:55,981] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:13:55,985] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run30
[2019-04-07 16:13:56,699] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0702993e-26 7.3382282e-26 2.9809186e-23 2.9077802e-16 2.8448031e-23
 1.0000000e+00 5.3879908e-17 2.8219719e-18], sum to 1.0000
[2019-04-07 16:13:56,699] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1033
[2019-04-07 16:13:56,709] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:13:56,709] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:13:56,712] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run30
[2019-04-07 16:13:56,883] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:13:56,883] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:13:56,887] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run30
[2019-04-07 16:13:56,945] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 42.5, 93.0, 560.0, 24.0, 23.84434108628028, -0.01083477910610232, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4955400.0000, 
sim time next is 4957200.0000, 
raw observation next is [-1.0, 39.0, 100.5, 638.5, 24.0, 24.33119154573241, 0.07382673247291034, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.39, 0.335, 0.7055248618784531, 0.5, 0.5275992954777008, 0.5246089108243034, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1225569], dtype=float32), 0.60293806]. 
=============================================
[2019-04-07 16:13:57,438] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:13:57,439] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:13:57,443] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run30
[2019-04-07 16:14:03,220] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:14:03,220] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:14:03,230] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run30
[2019-04-07 16:14:05,237] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:14:05,237] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:14:05,240] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run30
[2019-04-07 16:14:06,909] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:14:06,909] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:14:06,928] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run30
[2019-04-07 16:14:28,424] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3122692e-24 2.0215042e-23 1.9174899e-21 2.7248809e-14 1.5384157e-20
 1.0000000e+00 4.5357310e-14 3.6112869e-17], sum to 1.0000
[2019-04-07 16:14:28,424] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4272
[2019-04-07 16:14:28,515] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 24.0, 23.18253932137628, -0.184446690391469, 0.0, 1.0, 41954.64194407708], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2782800.0000, 
sim time next is 2784600.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 24.0, 23.12534465356388, -0.1975360805383754, 0.0, 1.0, 42133.37382467243], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 0.5, 0.4271120544636566, 0.43415463982054153, 0.0, 1.0, 0.2006351134508211], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12690403], dtype=float32), -0.45077375]. 
=============================================
[2019-04-07 16:14:34,253] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9903003e-25 1.5152500e-23 1.8991897e-20 2.6018358e-14 1.6478422e-21
 1.0000000e+00 7.7724895e-15 1.4743784e-16], sum to 1.0000
[2019-04-07 16:14:34,253] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5336
[2019-04-07 16:14:34,297] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.4, 82.0, 0.0, 0.0, 24.0, 22.82357769935841, -0.236755901592478, 0.0, 1.0, 48556.052128359384], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 338400.0000, 
sim time next is 340200.0000, 
raw observation next is [-13.65, 76.0, 0.0, 0.0, 24.0, 22.70817698976851, -0.261793579152117, 0.0, 1.0, 48403.48502198897], 
processed observation next is [1.0, 0.9565217391304348, 0.08448753462603877, 0.76, 0.0, 0.0, 0.5, 0.39234808248070924, 0.412735473615961, 0.0, 1.0, 0.2304927858189951], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0773479], dtype=float32), 1.4063772]. 
=============================================
[2019-04-07 16:14:37,333] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4417596e-29 5.6930601e-27 2.4159536e-25 5.8478752e-16 3.2942796e-24
 1.0000000e+00 8.5267630e-18 5.6881527e-20], sum to 1.0000
[2019-04-07 16:14:37,333] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9877
[2019-04-07 16:14:37,560] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 93.0, 6.0, 41.0, 24.0, 22.90956459630621, -0.1496034417261138, 1.0, 1.0, 75570.36285320675], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2914200.0000, 
sim time next is 2916000.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 24.0, 23.51847344049241, -0.00194402583854388, 1.0, 1.0, 47587.14448217727], 
processed observation next is [1.0, 0.782608695652174, 0.4903047091412743, 0.93, 0.0, 0.0, 0.5, 0.4598727867077009, 0.499351991387152, 1.0, 1.0, 0.22660544991512985], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10385567], dtype=float32), -0.62499696]. 
=============================================
[2019-04-07 16:14:37,568] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[108.91295 ]
 [108.84584 ]
 [109.032036]
 [109.13165 ]
 [109.105415]], R is [[108.79026031]
 [108.6282196 ]
 [108.54193878]
 [108.45652008]
 [108.37195587]].
[2019-04-07 16:14:45,034] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2443916e-28 7.5754035e-29 2.2976686e-28 2.8125576e-18 1.2920510e-27
 1.0000000e+00 1.6128871e-18 1.6270670e-21], sum to 1.0000
[2019-04-07 16:14:45,034] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7330
[2019-04-07 16:14:45,090] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.05, 87.0, 0.0, 0.0, 24.0, 23.38629546234757, -0.1124391400351835, 0.0, 1.0, 41115.87440960854], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 527400.0000, 
sim time next is 529200.0000, 
raw observation next is [3.8, 86.0, 0.0, 0.0, 24.0, 23.39327114902493, -0.1156261276794406, 0.0, 1.0, 35786.62920959788], 
processed observation next is [0.0, 0.13043478260869565, 0.5678670360110805, 0.86, 0.0, 0.0, 0.5, 0.44943926241874416, 0.4614579574401865, 0.0, 1.0, 0.17041252004570417], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48031172], dtype=float32), -0.7519077]. 
=============================================
[2019-04-07 16:14:53,851] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.2005865e-27 6.5693409e-27 4.3004836e-25 4.1779172e-16 7.0509768e-24
 1.0000000e+00 1.2665844e-16 1.6909426e-18], sum to 1.0000
[2019-04-07 16:14:53,852] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0432
[2019-04-07 16:14:54,097] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 87.0, 20.5, 22.5, 24.0, 23.09133202185987, -0.1347151869522553, 0.0, 1.0, 38600.926992613364], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 579600.0000, 
sim time next is 581400.0000, 
raw observation next is [-2.0, 87.0, 0.0, 0.0, 24.0, 23.05315466175015, -0.1501315077342354, 0.0, 1.0, 41860.44640095138], 
processed observation next is [0.0, 0.7391304347826086, 0.40720221606648205, 0.87, 0.0, 0.0, 0.5, 0.42109622181251244, 0.44995616408858824, 0.0, 1.0, 0.19933545905214944], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49133474], dtype=float32), -1.1502019]. 
=============================================
[2019-04-07 16:15:08,495] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.2106775e-28 2.0399857e-25 1.6696452e-23 6.8077406e-15 5.1231146e-22
 1.0000000e+00 2.4161016e-17 7.1993949e-19], sum to 1.0000
[2019-04-07 16:15:08,495] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9057
[2019-04-07 16:15:08,557] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [19.4, 49.0, 63.5, 0.0, 24.0, 26.11044756130483, 0.6122708971701861, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1094400.0000, 
sim time next is 1096200.0000, 
raw observation next is [18.55, 49.5, 35.0, 0.0, 24.0, 26.4086927159909, 0.6527427997937295, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.976454293628809, 0.495, 0.11666666666666667, 0.0, 0.5, 0.7007243929992416, 0.7175809332645765, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43729255], dtype=float32), -0.34047106]. 
=============================================
[2019-04-07 16:15:11,209] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6765134e-28 2.9454100e-26 1.1296258e-24 2.0622473e-16 7.5819532e-25
 1.0000000e+00 3.4613530e-18 2.2331716e-19], sum to 1.0000
[2019-04-07 16:15:11,209] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6872
[2019-04-07 16:15:11,474] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 68.0, 138.5, 142.5, 24.0, 24.09587652967073, -0.1003720429688266, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2214000.0000, 
sim time next is 2215800.0000, 
raw observation next is [-3.9, 68.0, 157.0, 285.0, 24.0, 23.85754900740536, 0.02121152719295561, 1.0, 1.0, 62600.05751884459], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.68, 0.5233333333333333, 0.3149171270718232, 0.5, 0.4881290839504467, 0.5070705090643185, 1.0, 1.0, 0.298095511994498], 
reward next is 0.9876, 
noisyNet noise sample is [array([1.4123843], dtype=float32), -0.013049749]. 
=============================================
[2019-04-07 16:15:25,671] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5539291e-31 2.5499100e-30 2.2038064e-26 5.0308164e-21 1.1658874e-26
 1.0000000e+00 3.8182476e-20 4.9081487e-22], sum to 1.0000
[2019-04-07 16:15:25,671] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1256
[2019-04-07 16:15:25,700] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.45, 73.5, 0.0, 0.0, 24.0, 24.02673425066932, 0.1940490977904379, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1542600.0000, 
sim time next is 1544400.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 24.0, 23.82529693696402, 0.1733784965813298, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6759002770083103, 0.74, 0.0, 0.0, 0.5, 0.48544141141366826, 0.5577928321937766, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.91502863], dtype=float32), -0.0077720014]. 
=============================================
[2019-04-07 16:15:35,657] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.7041657e-29 3.6917635e-27 1.8125250e-26 4.8440080e-17 4.6863835e-25
 1.0000000e+00 1.1231068e-18 1.5815161e-18], sum to 1.0000
[2019-04-07 16:15:35,657] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1088
[2019-04-07 16:15:35,703] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 24.0, 23.648031668378, 0.1620284637468383, 0.0, 1.0, 18831.845292679955], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1292400.0000, 
sim time next is 1294200.0000, 
raw observation next is [4.95, 98.0, 0.0, 0.0, 24.0, 23.60936490905709, 0.1550359278754434, 0.0, 1.0, 35209.016279207404], 
processed observation next is [0.0, 1.0, 0.5997229916897507, 0.98, 0.0, 0.0, 0.5, 0.46744707575475736, 0.5516786426251478, 0.0, 1.0, 0.16766198228194001], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3732848], dtype=float32), 0.1267165]. 
=============================================
[2019-04-07 16:15:43,441] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:15:43,441] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:15:43,445] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run31
[2019-04-07 16:15:49,093] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:15:49,093] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:15:49,098] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run31
[2019-04-07 16:15:56,926] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85000, global step 1339589: loss 0.2941
[2019-04-07 16:15:56,933] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85000, global step 1339590: learning rate 0.0000
[2019-04-07 16:15:59,617] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-07 16:15:59,632] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:15:59,632] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:15:59,636] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run68
[2019-04-07 16:15:59,664] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:15:59,665] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:15:59,669] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run68
[2019-04-07 16:15:59,689] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 16:15:59,692] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:15:59,696] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run68
[2019-04-07 16:16:28,980] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11328871], dtype=float32), 0.14963782]
[2019-04-07 16:16:28,980] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-2.3222192285, 86.526979875, 0.0, 0.0, 24.0, 23.22552202070415, -0.1726882703517603, 0.0, 1.0, 39912.310726780255]
[2019-04-07 16:16:28,980] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 16:16:28,981] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [1.4399582e-27 2.9491651e-26 3.0732780e-24 2.5470990e-16 7.3923769e-24
 1.0000000e+00 4.5395149e-17 1.2629741e-18], sampled 0.10895995898938027
[2019-04-07 16:18:22,163] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 16:18:23,432] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11328871], dtype=float32), 0.14963782]
[2019-04-07 16:18:23,433] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-6.0, 41.0, 0.0, 0.0, 24.0, 23.74432666885125, 0.03004599819448724, 1.0, 1.0, 0.0]
[2019-04-07 16:18:23,433] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 16:18:23,434] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [4.1367467e-24 3.8144277e-23 1.6597911e-21 1.9578350e-14 8.8525273e-21
 1.0000000e+00 2.9863371e-15 1.9976858e-16], sampled 0.3333170145695512
[2019-04-07 16:18:42,411] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 16:18:44,821] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 16:18:45,845] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1340000, evaluation results [1340000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 16:18:50,760] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85000, global step 1340671: loss 0.2908
[2019-04-07 16:18:50,761] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85000, global step 1340671: learning rate 0.0000
[2019-04-07 16:19:01,181] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:19:01,181] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:19:01,184] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run31
[2019-04-07 16:19:02,313] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:19:02,313] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:19:02,317] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run31
[2019-04-07 16:19:02,890] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.2779838e-26 2.2355525e-24 1.5623119e-23 1.8793683e-15 5.0877469e-22
 1.0000000e+00 2.0956981e-16 1.7227878e-18], sum to 1.0000
[2019-04-07 16:19:02,891] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8247
[2019-04-07 16:19:03,048] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 86.0, 87.5, 0.0, 24.0, 24.25257901839727, -0.1089643999968851, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2041200.0000, 
sim time next is 2043000.0000, 
raw observation next is [-4.2, 84.0, 71.0, 0.0, 24.0, 23.60599683626941, -0.05930944357024478, 1.0, 1.0, 70716.18533967304], 
processed observation next is [1.0, 0.6521739130434783, 0.34626038781163443, 0.84, 0.23666666666666666, 0.0, 0.5, 0.4671664030224507, 0.4802301854765851, 1.0, 1.0, 0.3367437397127288], 
reward next is 0.9490, 
noisyNet noise sample is [array([0.3533925], dtype=float32), -2.8830893]. 
=============================================
[2019-04-07 16:19:03,068] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[99.65241 ]
 [99.601944]
 [99.669624]
 [99.499405]
 [99.57407 ]], R is [[100.21578217]
 [100.21362305]
 [100.21148682]
 [100.20937347]
 [100.20728302]].
[2019-04-07 16:19:05,364] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8430339e-26 1.1561759e-24 7.6819977e-22 3.1473960e-16 1.7435251e-21
 1.0000000e+00 5.2371846e-16 1.6433321e-17], sum to 1.0000
[2019-04-07 16:19:05,364] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8155
[2019-04-07 16:19:05,515] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 24.0, 23.16036804614557, -0.1571558166810728, 0.0, 1.0, 53488.91509729713], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 248400.0000, 
sim time next is 250200.0000, 
raw observation next is [-3.65, 70.0, 0.0, 0.0, 24.0, 23.12929066535279, -0.1687991943205082, 0.0, 1.0, 46661.08196865474], 
processed observation next is [1.0, 0.9130434782608695, 0.3614958448753463, 0.7, 0.0, 0.0, 0.5, 0.42744088877939923, 0.4437336018931639, 0.0, 1.0, 0.22219562842216542], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.50011104], dtype=float32), 0.09415208]. 
=============================================
[2019-04-07 16:19:13,295] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7352192e-27 6.0299788e-26 1.9559860e-23 1.5357450e-16 1.1480240e-23
 1.0000000e+00 2.7037902e-17 3.3160278e-17], sum to 1.0000
[2019-04-07 16:19:13,295] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3123
[2019-04-07 16:19:13,346] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 24.0, 23.53101882593745, -0.04112198093626658, 0.0, 1.0, 74619.2016461321], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3708000.0000, 
sim time next is 3709800.0000, 
raw observation next is [-1.5, 68.5, 0.0, 0.0, 24.0, 23.4937084192445, -0.03280996480182814, 0.0, 1.0, 59165.116932465055], 
processed observation next is [0.0, 0.9565217391304348, 0.4210526315789474, 0.685, 0.0, 0.0, 0.5, 0.45780903493704156, 0.4890633450660573, 0.0, 1.0, 0.2817386520593574], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6514864], dtype=float32), -1.7311031]. 
=============================================
[2019-04-07 16:19:15,312] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85000, global step 1343776: loss 0.2738
[2019-04-07 16:19:15,314] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85000, global step 1343776: learning rate 0.0000
[2019-04-07 16:19:16,470] A3C_AGENT_WORKER-Thread-6 INFO:Local step 85000, global step 1343939: loss 0.3093
[2019-04-07 16:19:16,470] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 85000, global step 1343939: learning rate 0.0000
[2019-04-07 16:19:18,629] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:19:18,633] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:19:18,644] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run31
[2019-04-07 16:19:32,548] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85000, global step 1346054: loss 0.2863
[2019-04-07 16:19:32,551] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85000, global step 1346054: learning rate 0.0000
[2019-04-07 16:19:49,516] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85500, global step 1348536: loss 0.3428
[2019-04-07 16:19:49,516] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85500, global step 1348536: learning rate 0.0000
[2019-04-07 16:19:56,566] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85500, global step 1349694: loss 0.3821
[2019-04-07 16:19:56,567] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85500, global step 1349694: learning rate 0.0000
[2019-04-07 16:19:56,680] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.04517648e-27 7.04938677e-25 1.10886825e-23 1.84328515e-15
 5.00701313e-23 1.00000000e+00 1.61243362e-16 2.08501180e-17], sum to 1.0000
[2019-04-07 16:19:56,681] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0965
[2019-04-07 16:19:56,911] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 217.0, 154.0, 24.0, 23.09351890082257, -0.08791961736953052, 0.0, 1.0, 18726.984762859676], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2977200.0000, 
sim time next is 2979000.0000, 
raw observation next is [-3.0, 65.0, 256.0, 284.0, 24.0, 23.10155065482063, -0.07264395444182577, 0.0, 1.0, 26915.937386482376], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.8533333333333334, 0.3138121546961326, 0.5, 0.4251292212350526, 0.4757853485193914, 0.0, 1.0, 0.12817113041182085], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31192747], dtype=float32), -0.1710659]. 
=============================================
[2019-04-07 16:19:56,915] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[94.68764 ]
 [95.011986]
 [95.27741 ]
 [95.65957 ]
 [96.24103 ]], R is [[95.06151581]
 [95.11090088]
 [95.14572144]
 [95.19426727]
 [95.24232483]].
[2019-04-07 16:20:05,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3313870e-29 4.6928549e-27 1.0456610e-26 9.5232923e-18 6.9962890e-25
 1.0000000e+00 2.2750660e-17 2.0036126e-19], sum to 1.0000
[2019-04-07 16:20:05,314] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8954
[2019-04-07 16:20:05,407] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 65.5, 99.0, 670.0, 24.0, 24.10488994744014, 0.1107768550736364, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3490200.0000, 
sim time next is 3492000.0000, 
raw observation next is [0.0, 60.0, 104.0, 720.0, 24.0, 24.54185498762416, 0.1790937761301329, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.6, 0.3466666666666667, 0.7955801104972375, 0.5, 0.5451545823020133, 0.559697925376711, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.99747247], dtype=float32), 0.7302781]. 
=============================================
[2019-04-07 16:20:05,422] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[101.3096  ]
 [101.361755]
 [101.23469 ]
 [101.17617 ]
 [101.24163 ]], R is [[101.2756424 ]
 [101.26288605]
 [101.2502594 ]
 [101.23775482]
 [101.22537994]].
[2019-04-07 16:20:07,497] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.0227252e-30 2.4002509e-28 3.5235153e-25 2.3224522e-17 2.0657811e-25
 1.0000000e+00 6.0443051e-18 4.1578461e-19], sum to 1.0000
[2019-04-07 16:20:07,497] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2845
[2019-04-07 16:20:07,539] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 24.0, 23.58753399128562, 0.09080964538450836, 0.0, 1.0, 12210.221543341808], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1321200.0000, 
sim time next is 1323000.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 24.0, 23.52269983324109, 0.08602452228708889, 1.0, 1.0, 13969.50428650214], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.0, 0.0, 0.5, 0.46022498610342427, 0.5286748407623629, 1.0, 1.0, 0.06652144898334353], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0576247], dtype=float32), 0.31569642]. 
=============================================
[2019-04-07 16:20:07,612] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[108.69354]
 [109.0194 ]
 [108.71127]
 [108.87355]
 [109.47358]], R is [[108.99471283]
 [108.9047699 ]
 [108.68060303]
 [108.59379578]
 [108.50785828]].
[2019-04-07 16:20:11,716] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6423236e-24 1.4907329e-25 1.1501418e-22 3.9512292e-16 4.1244291e-22
 1.0000000e+00 4.5865822e-17 6.8592922e-18], sum to 1.0000
[2019-04-07 16:20:11,716] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2752
[2019-04-07 16:20:11,855] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 79.0, 95.0, 0.0, 24.0, 23.69747045181288, -0.1010582376586873, 1.0, 1.0, 43477.06093211695], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 824400.0000, 
sim time next is 826200.0000, 
raw observation next is [-4.2, 79.0, 91.0, 0.0, 24.0, 23.9113745436435, -0.05742622731871446, 1.0, 1.0, 9742.822258738643], 
processed observation next is [1.0, 0.5652173913043478, 0.34626038781163443, 0.79, 0.30333333333333334, 0.0, 0.5, 0.49261454530362503, 0.48085792422709517, 1.0, 1.0, 0.04639439170827925], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0380867], dtype=float32), -0.48435283]. 
=============================================
[2019-04-07 16:20:13,529] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0527075e-29 3.5255669e-28 9.5636276e-26 1.3430200e-17 9.2375158e-25
 1.0000000e+00 1.0170067e-17 3.4378996e-20], sum to 1.0000
[2019-04-07 16:20:13,529] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7713
[2019-04-07 16:20:13,607] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.4, 80.0, 111.0, 781.5, 24.0, 24.66840786267023, 0.1480053185032081, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3236400.0000, 
sim time next is 3238200.0000, 
raw observation next is [-2.2, 75.5, 113.0, 811.0, 24.0, 24.52651398189222, 0.1338181423062676, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4016620498614959, 0.755, 0.37666666666666665, 0.8961325966850828, 0.5, 0.5438761651576849, 0.5446060474354225, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9104379], dtype=float32), 1.5053419]. 
=============================================
[2019-04-07 16:20:13,610] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2064151e-25 4.5823313e-25 1.9289990e-23 5.5330708e-15 6.7186797e-23
 1.0000000e+00 3.6701336e-15 4.4757218e-17], sum to 1.0000
[2019-04-07 16:20:13,610] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6688
[2019-04-07 16:20:13,699] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 48.0, 87.0, 674.0, 24.0, 24.59522857675741, 0.2161679467134738, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3339000.0000, 
sim time next is 3340800.0000, 
raw observation next is [-2.0, 46.0, 73.5, 587.5, 24.0, 24.86313340655321, 0.1584910080073035, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.46, 0.245, 0.649171270718232, 0.5, 0.5719277838794342, 0.5528303360024345, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7637501], dtype=float32), -1.1113917]. 
=============================================
[2019-04-07 16:20:16,349] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:20:16,349] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:20:16,353] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run31
[2019-04-07 16:20:17,982] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85500, global step 1353659: loss 0.4128
[2019-04-07 16:20:17,998] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85500, global step 1353659: learning rate 0.0000
[2019-04-07 16:20:19,556] A3C_AGENT_WORKER-Thread-6 INFO:Local step 85500, global step 1353958: loss 0.3923
[2019-04-07 16:20:19,558] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 85500, global step 1353958: learning rate 0.0000
[2019-04-07 16:20:22,144] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.3833455e-32 2.1911201e-29 2.7552231e-26 9.2458992e-19 1.6033476e-26
 1.0000000e+00 1.3920620e-18 2.9824684e-21], sum to 1.0000
[2019-04-07 16:20:22,144] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3584
[2019-04-07 16:20:22,188] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 24.0, 23.85187331752235, 0.1326941693029464, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1036800.0000, 
sim time next is 1038600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 24.0, 23.93045652169253, 0.149154173106618, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.5, 0.4942047101410442, 0.5497180577022059, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40667868], dtype=float32), 2.4910018]. 
=============================================
[2019-04-07 16:20:23,432] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4233186e-26 7.8828972e-26 1.5254044e-24 1.7744832e-15 6.1062567e-24
 1.0000000e+00 1.1711890e-16 3.8767383e-18], sum to 1.0000
[2019-04-07 16:20:23,432] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0862
[2019-04-07 16:20:23,468] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.0, 34.5, 116.0, 816.0, 24.0, 24.0122751266417, 0.09528048019333686, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3670200.0000, 
sim time next is 3672000.0000, 
raw observation next is [4.0, 45.0, 116.5, 822.5, 24.0, 23.88346466822971, 0.07064183810456791, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5734072022160666, 0.45, 0.3883333333333333, 0.9088397790055248, 0.5, 0.49028872235247584, 0.5235472793681893, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6402719], dtype=float32), -0.662186]. 
=============================================
[2019-04-07 16:20:23,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[99.18965 ]
 [98.68547 ]
 [98.276886]
 [97.8319  ]
 [97.26394 ]], R is [[99.49051666]
 [99.4956131 ]
 [99.50065613]
 [99.50565338]
 [99.51059723]].
[2019-04-07 16:20:27,309] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86000, global step 1355496: loss 0.4503
[2019-04-07 16:20:27,310] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86000, global step 1355496: learning rate 0.0000
[2019-04-07 16:20:27,916] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85000, global step 1355609: loss 0.3046
[2019-04-07 16:20:27,916] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85000, global step 1355609: learning rate 0.0000
[2019-04-07 16:20:29,159] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.9607283e-27 3.0168331e-27 4.3547986e-24 7.2430641e-17 6.7227531e-24
 1.0000000e+00 7.9718051e-18 9.2218758e-19], sum to 1.0000
[2019-04-07 16:20:29,159] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0117
[2019-04-07 16:20:29,205] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 42.0, 115.0, 823.5, 24.0, 23.63000440459617, 0.05187720811838953, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3675600.0000, 
sim time next is 3677400.0000, 
raw observation next is [5.5, 42.5, 113.0, 818.0, 24.0, 23.6160515191607, 0.05370392505310487, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6149584487534627, 0.425, 0.37666666666666665, 0.9038674033149171, 0.5, 0.4680042932633916, 0.517901308351035, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.67672986], dtype=float32), -1.2907642]. 
=============================================
[2019-04-07 16:20:34,351] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86000, global step 1356782: loss 0.4666
[2019-04-07 16:20:34,352] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86000, global step 1356782: learning rate 0.0000
[2019-04-07 16:20:35,546] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85500, global step 1357029: loss 0.3798
[2019-04-07 16:20:35,547] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85500, global step 1357029: learning rate 0.0000
[2019-04-07 16:20:41,009] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0635341e-27 3.6021349e-26 1.7845839e-23 3.2635872e-15 4.1370800e-23
 1.0000000e+00 7.2283793e-17 5.1899033e-19], sum to 1.0000
[2019-04-07 16:20:41,025] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4251
[2019-04-07 16:20:41,091] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 83.0, 0.0, 0.0, 24.0, 22.78394442449601, -0.2411948522857048, 0.0, 1.0, 44431.15597310449], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2095200.0000, 
sim time next is 2097000.0000, 
raw observation next is [-6.7, 80.5, 0.0, 0.0, 24.0, 22.66073220190565, -0.2688667600537725, 0.0, 1.0, 44533.70291541475], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.805, 0.0, 0.0, 0.5, 0.3883943501588041, 0.41037774664874255, 0.0, 1.0, 0.2120652519781655], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27699476], dtype=float32), 0.4767209]. 
=============================================
[2019-04-07 16:20:41,107] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[96.842186]
 [97.412   ]
 [97.89755 ]
 [98.28513 ]
 [98.608925]], R is [[96.11386871]
 [96.15273285]
 [96.19120789]
 [96.22929382]
 [96.26699829]].
[2019-04-07 16:20:51,610] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-07 16:20:51,610] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:20:51,610] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:20:51,614] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run69
[2019-04-07 16:20:51,638] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:20:51,641] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:20:51,644] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run69
[2019-04-07 16:20:51,672] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 16:20:51,675] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:20:51,680] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run69
[2019-04-07 16:21:25,151] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11361388], dtype=float32), 0.15030602]
[2019-04-07 16:21:25,151] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [10.0, 83.0, 0.0, 0.0, 24.0, 23.59500352314948, 0.006930557408892692, 0.0, 1.0, 56374.897004382445]
[2019-04-07 16:21:25,152] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 16:21:25,154] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [5.0180285e-30 1.0800222e-28 1.3149493e-26 5.2165823e-18 5.8525098e-26
 1.0000000e+00 1.1769070e-18 2.7750937e-20], sampled 0.7951585706638895
[2019-04-07 16:23:12,437] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 16:23:35,470] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 16:23:40,190] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 16:23:41,214] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1360000, evaluation results [1360000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 16:23:45,859] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86000, global step 1360857: loss 0.4656
[2019-04-07 16:23:45,860] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86000, global step 1360857: learning rate 0.0000
[2019-04-07 16:23:47,526] A3C_AGENT_WORKER-Thread-6 INFO:Local step 86000, global step 1361213: loss 0.4480
[2019-04-07 16:23:47,527] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 86000, global step 1361213: learning rate 0.0000
[2019-04-07 16:23:58,125] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4041399e-25 6.8289559e-25 4.6362804e-23 2.6226063e-15 2.6728034e-22
 1.0000000e+00 5.9850469e-16 8.9012012e-18], sum to 1.0000
[2019-04-07 16:23:58,125] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9852
[2019-04-07 16:23:58,278] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 60.0, 112.0, 100.0, 24.0, 23.01944662035378, -0.196205983220197, 0.0, 1.0, 32368.995669173437], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 649800.0000, 
sim time next is 651600.0000, 
raw observation next is [-2.3, 59.0, 147.0, 96.5, 24.0, 23.0295992025819, -0.1869028324791652, 0.0, 1.0, 33897.445473511965], 
processed observation next is [0.0, 0.5652173913043478, 0.3988919667590028, 0.59, 0.49, 0.10662983425414364, 0.5, 0.4191332668818249, 0.43769905584027824, 0.0, 1.0, 0.16141640701672363], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.151003], dtype=float32), -1.0609726]. 
=============================================
[2019-04-07 16:24:02,293] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86000, global step 1364001: loss 0.4898
[2019-04-07 16:24:02,293] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86000, global step 1364001: learning rate 0.0000
[2019-04-07 16:24:05,069] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:24:05,069] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:24:05,077] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run31
[2019-04-07 16:24:05,409] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:24:05,409] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:24:05,412] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run31
[2019-04-07 16:24:10,380] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86500, global step 1365278: loss 0.1547
[2019-04-07 16:24:10,381] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86500, global step 1365278: learning rate 0.0000
[2019-04-07 16:24:15,341] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:24:15,341] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:24:15,344] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run31
[2019-04-07 16:24:16,695] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:24:16,695] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:24:16,700] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run31
[2019-04-07 16:24:17,602] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85500, global step 1366452: loss 0.3854
[2019-04-07 16:24:17,603] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85500, global step 1366452: learning rate 0.0000
[2019-04-07 16:24:17,801] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85000, global step 1366476: loss 0.2673
[2019-04-07 16:24:17,809] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85000, global step 1366476: learning rate 0.0000
[2019-04-07 16:24:18,390] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86500, global step 1366571: loss 0.2188
[2019-04-07 16:24:18,406] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86500, global step 1366575: learning rate 0.0000
[2019-04-07 16:24:18,564] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85000, global step 1366594: loss 0.2740
[2019-04-07 16:24:18,565] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85000, global step 1366594: learning rate 0.0000
[2019-04-07 16:24:18,685] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:24:18,685] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:24:18,688] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run31
[2019-04-07 16:24:19,889] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:24:19,889] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:24:19,892] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run31
[2019-04-07 16:24:21,641] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:24:21,641] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:24:21,644] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run31
[2019-04-07 16:24:22,187] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9201068e-32 3.5841598e-29 1.3698734e-26 1.0955798e-19 1.4357896e-27
 1.0000000e+00 1.9791748e-19 2.9355673e-22], sum to 1.0000
[2019-04-07 16:24:22,187] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4415
[2019-04-07 16:24:22,227] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 24.0, 23.67820830824496, 0.1874535313347347, 0.0, 1.0, 87069.8358688476], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1137600.0000, 
sim time next is 1139400.0000, 
raw observation next is [11.35, 77.0, 0.0, 0.0, 24.0, 23.8357673337444, 0.1922136500556563, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.7770083102493075, 0.77, 0.0, 0.0, 0.5, 0.48631394447869997, 0.5640712166852188, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2409407], dtype=float32), -0.3859618]. 
=============================================
[2019-04-07 16:24:24,569] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:24:24,569] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:24:24,572] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run31
[2019-04-07 16:24:27,669] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:24:27,669] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:24:27,673] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run31
[2019-04-07 16:24:29,517] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85000, global step 1367868: loss 0.2646
[2019-04-07 16:24:29,519] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85000, global step 1367869: learning rate 0.0000
[2019-04-07 16:24:29,943] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85000, global step 1367926: loss 0.2453
[2019-04-07 16:24:29,945] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85000, global step 1367926: learning rate 0.0000
[2019-04-07 16:24:30,232] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:24:30,232] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:24:30,238] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run31
[2019-04-07 16:24:32,601] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85000, global step 1368214: loss 0.2806
[2019-04-07 16:24:32,609] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85000, global step 1368214: learning rate 0.0000
[2019-04-07 16:24:34,168] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85000, global step 1368385: loss 0.3169
[2019-04-07 16:24:34,168] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85000, global step 1368385: learning rate 0.0000
[2019-04-07 16:24:35,608] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85000, global step 1368575: loss 0.3100
[2019-04-07 16:24:35,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85000, global step 1368575: learning rate 0.0000
[2019-04-07 16:24:39,309] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85000, global step 1369002: loss 0.2645
[2019-04-07 16:24:39,310] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85000, global step 1369002: learning rate 0.0000
[2019-04-07 16:24:42,693] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85000, global step 1369446: loss 0.2978
[2019-04-07 16:24:42,694] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85000, global step 1369446: learning rate 0.0000
[2019-04-07 16:24:43,351] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86500, global step 1369531: loss 0.1183
[2019-04-07 16:24:43,352] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86500, global step 1369531: learning rate 0.0000
[2019-04-07 16:24:44,615] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85000, global step 1369705: loss 0.3109
[2019-04-07 16:24:44,615] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85000, global step 1369705: learning rate 0.0000
[2019-04-07 16:24:46,737] A3C_AGENT_WORKER-Thread-6 INFO:Local step 86500, global step 1369964: loss 0.1271
[2019-04-07 16:24:46,745] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 86500, global step 1369964: learning rate 0.0000
[2019-04-07 16:24:54,023] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7976149e-23 3.0378109e-23 3.9575868e-20 3.4715503e-13 5.1381513e-20
 1.0000000e+00 3.6167087e-15 2.7442561e-15], sum to 1.0000
[2019-04-07 16:24:54,024] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3504
[2019-04-07 16:24:54,255] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 35.0, 106.5, 0.0, 24.0, 22.78547162477815, -0.2524478763175897, 1.0, 1.0, 78187.24384656434], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 482400.0000, 
sim time next is 484200.0000, 
raw observation next is [-0.3, 36.0, 94.0, 0.0, 24.0, 23.7047574624192, -0.1559501086064394, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4542936288088643, 0.36, 0.31333333333333335, 0.0, 0.5, 0.4753964552015999, 0.44801663046452017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23410788], dtype=float32), 0.26482573]. 
=============================================
[2019-04-07 16:24:55,012] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87000, global step 1370966: loss 2.5000
[2019-04-07 16:24:55,012] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87000, global step 1370966: learning rate 0.0000
[2019-04-07 16:24:55,886] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86000, global step 1371062: loss 0.4100
[2019-04-07 16:24:55,887] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86000, global step 1371062: learning rate 0.0000
[2019-04-07 16:24:58,826] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9975394e-25 8.8012312e-24 7.2339643e-21 4.8454545e-15 3.7057150e-21
 1.0000000e+00 1.9068679e-15 8.8235463e-18], sum to 1.0000
[2019-04-07 16:24:58,827] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5589
[2019-04-07 16:24:58,906] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 24.0, 22.53475042465359, -0.3638768405011761, 0.0, 1.0, 46098.57527488144], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1918800.0000, 
sim time next is 1920600.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 24.0, 22.43404184747966, -0.3832651528763155, 0.0, 1.0, 45946.08331233886], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.82, 0.0, 0.0, 0.5, 0.36950348728997157, 0.3722449490412281, 0.0, 1.0, 0.21879087291589933], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.73405886], dtype=float32), -1.543163]. 
=============================================
[2019-04-07 16:25:03,517] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86500, global step 1372004: loss 0.1461
[2019-04-07 16:25:03,517] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86500, global step 1372004: learning rate 0.0000
[2019-04-07 16:25:05,188] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87000, global step 1372219: loss 2.5217
[2019-04-07 16:25:05,205] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87000, global step 1372219: learning rate 0.0000
[2019-04-07 16:25:13,123] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.6965849e-26 4.7907465e-27 2.8378877e-23 4.3077441e-16 4.9416409e-24
 1.0000000e+00 5.3694630e-17 2.0810615e-18], sum to 1.0000
[2019-04-07 16:25:13,123] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5947
[2019-04-07 16:25:13,193] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 24.0, 23.43345222339078, -0.08206731902284159, 0.0, 1.0, 46366.49315153077], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 590400.0000, 
sim time next is 592200.0000, 
raw observation next is [-2.8, 85.0, 0.0, 0.0, 24.0, 23.43189308175281, -0.1005287342408963, 0.0, 1.0, 27024.55374064621], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.85, 0.0, 0.0, 0.5, 0.4526577568127343, 0.46649042191970125, 0.0, 1.0, 0.12868835114593433], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7997527], dtype=float32), -1.7050288]. 
=============================================
[2019-04-07 16:25:22,845] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5469101e-24 2.3648105e-24 1.9001870e-23 6.4843771e-16 1.4220306e-21
 1.0000000e+00 2.5541031e-17 1.9277627e-16], sum to 1.0000
[2019-04-07 16:25:22,845] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2932
[2019-04-07 16:25:22,987] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.0, 44.0, 106.5, 739.5, 24.0, 25.03649799907537, 0.2007561096936195, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4010400.0000, 
sim time next is 4012200.0000, 
raw observation next is [-8.5, 42.0, 112.0, 781.0, 24.0, 25.0929514234993, 0.2238340978308543, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.22714681440443216, 0.42, 0.37333333333333335, 0.8629834254143647, 0.5, 0.5910792852916084, 0.5746113659436182, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30941883], dtype=float32), -0.75107765]. 
=============================================
[2019-04-07 16:25:26,827] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85500, global step 1375250: loss 0.3662
[2019-04-07 16:25:26,828] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85500, global step 1375250: learning rate 0.0000
[2019-04-07 16:25:27,250] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85500, global step 1375326: loss 0.3526
[2019-04-07 16:25:27,251] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85500, global step 1375326: learning rate 0.0000
[2019-04-07 16:25:28,359] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87000, global step 1375535: loss 2.3743
[2019-04-07 16:25:28,365] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87000, global step 1375535: learning rate 0.0000
[2019-04-07 16:25:34,044] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87500, global step 1376526: loss 1.3731
[2019-04-07 16:25:34,046] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87500, global step 1376527: learning rate 0.0000
[2019-04-07 16:25:34,507] A3C_AGENT_WORKER-Thread-6 INFO:Local step 87000, global step 1376622: loss 2.2831
[2019-04-07 16:25:34,508] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 87000, global step 1376622: learning rate 0.0000
[2019-04-07 16:25:36,897] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85500, global step 1377054: loss 0.3870
[2019-04-07 16:25:36,898] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85500, global step 1377054: learning rate 0.0000
[2019-04-07 16:25:37,708] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85500, global step 1377218: loss 0.3390
[2019-04-07 16:25:37,736] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85500, global step 1377218: learning rate 0.0000
[2019-04-07 16:25:39,144] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.03505684e-26 1.61976082e-27 2.00153733e-24 2.37563413e-16
 2.01769068e-23 1.00000000e+00 1.00114397e-17 2.03729436e-18], sum to 1.0000
[2019-04-07 16:25:39,145] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4550
[2019-04-07 16:25:39,230] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.35, 73.0, 87.0, 0.0, 24.0, 24.13288299837289, -0.0815501044239972, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 815400.0000, 
sim time next is 817200.0000, 
raw observation next is [-4.5, 71.0, 98.5, 0.0, 24.0, 24.01251184051635, -0.09544933166081798, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.3283333333333333, 0.0, 0.5, 0.5010426533763624, 0.4681835561130607, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01333906], dtype=float32), 1.4065998]. 
=============================================
[2019-04-07 16:25:41,290] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85500, global step 1377920: loss 0.3533
[2019-04-07 16:25:41,291] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85500, global step 1377920: learning rate 0.0000
[2019-04-07 16:25:42,486] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85500, global step 1378160: loss 0.3278
[2019-04-07 16:25:42,486] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85500, global step 1378160: learning rate 0.0000
[2019-04-07 16:25:42,533] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85500, global step 1378168: loss 0.3195
[2019-04-07 16:25:42,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85500, global step 1378168: learning rate 0.0000
[2019-04-07 16:25:45,449] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87500, global step 1378875: loss 1.3811
[2019-04-07 16:25:45,452] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87500, global step 1378875: learning rate 0.0000
[2019-04-07 16:25:45,596] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85500, global step 1378909: loss 0.3768
[2019-04-07 16:25:45,596] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85500, global step 1378909: learning rate 0.0000
[2019-04-07 16:25:47,043] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87000, global step 1379256: loss 2.3041
[2019-04-07 16:25:47,044] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87000, global step 1379256: learning rate 0.0000
[2019-04-07 16:25:47,677] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85500, global step 1379428: loss 0.3105
[2019-04-07 16:25:47,679] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85500, global step 1379429: learning rate 0.0000
[2019-04-07 16:25:47,851] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4173123e-24 7.3507670e-22 4.4043236e-21 1.4456643e-14 2.1193863e-20
 1.0000000e+00 2.1464830e-14 1.3496673e-15], sum to 1.0000
[2019-04-07 16:25:47,856] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4321
[2019-04-07 16:25:47,891] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.55, 64.0, 171.0, 0.0, 24.0, 23.53403583369346, 0.1284882576090416, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1168200.0000, 
sim time next is 1170000.0000, 
raw observation next is [18.3, 65.0, 165.0, 0.0, 24.0, 23.55373708748808, 0.1291558200480368, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.55, 0.0, 0.5, 0.46281142395734004, 0.5430519400160122, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0293336], dtype=float32), -1.2032831]. 
=============================================
[2019-04-07 16:25:47,947] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[ 89.10791]
 [ 92.09868]
 [ 95.43123]
 [ 98.53512]
 [102.38313]], R is [[89.17559814]
 [89.28384399]
 [89.39100647]
 [89.49710083]
 [89.60212708]].
[2019-04-07 16:25:48,557] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85500, global step 1379698: loss 0.3293
[2019-04-07 16:25:48,561] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85500, global step 1379698: learning rate 0.0000
[2019-04-07 16:25:49,625] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-07 16:25:49,629] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:25:49,629] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:25:49,633] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run70
[2019-04-07 16:25:49,670] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:25:49,670] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:25:49,678] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 16:25:49,679] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:25:49,679] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run70
[2019-04-07 16:25:49,703] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run70
[2019-04-07 16:28:02,557] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11403506], dtype=float32), 0.1511538]
[2019-04-07 16:28:02,558] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [2.5, 48.5, 109.0, 764.0, 24.0, 24.88230762084509, 0.2208015222862647, 1.0, 1.0, 0.0]
[2019-04-07 16:28:02,558] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 16:28:02,559] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [3.0753373e-27 4.2166754e-26 2.1043975e-24 2.1295777e-16 1.4510943e-23
 1.0000000e+00 3.1666754e-17 1.8144556e-18], sampled 0.05888609837259129
[2019-04-07 16:28:12,730] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 16:28:34,692] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 16:28:39,213] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 16:28:40,236] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1380000, evaluation results [1380000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 16:28:43,719] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86500, global step 1380855: loss 0.1833
[2019-04-07 16:28:43,719] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86500, global step 1380855: learning rate 0.0000
[2019-04-07 16:28:46,464] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:28:46,464] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:28:46,468] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run32
[2019-04-07 16:28:54,053] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86000, global step 1382850: loss 0.4474
[2019-04-07 16:28:54,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86000, global step 1382851: learning rate 0.0000
[2019-04-07 16:28:55,229] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86000, global step 1383067: loss 0.4376
[2019-04-07 16:28:55,230] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86000, global step 1383067: learning rate 0.0000
[2019-04-07 16:28:56,233] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87500, global step 1383274: loss 1.5452
[2019-04-07 16:28:56,234] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87500, global step 1383274: learning rate 0.0000
[2019-04-07 16:28:57,917] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:28:57,917] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:28:57,921] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run32
[2019-04-07 16:29:01,745] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.1967716e-31 1.6021285e-29 4.1299445e-27 6.1939741e-17 3.4140401e-25
 1.0000000e+00 5.9989806e-19 5.3368199e-21], sum to 1.0000
[2019-04-07 16:29:01,746] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1580
[2019-04-07 16:29:01,806] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 84.0, 95.0, 0.0, 24.0, 23.98867372067635, 0.06827724507915041, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1684800.0000, 
sim time next is 1686600.0000, 
raw observation next is [1.1, 86.0, 107.0, 0.0, 24.0, 23.89504078958651, 0.03861936001006961, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.86, 0.3566666666666667, 0.0, 0.5, 0.49125339913220917, 0.5128731200033566, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39408028], dtype=float32), 0.6821746]. 
=============================================
[2019-04-07 16:29:03,106] A3C_AGENT_WORKER-Thread-6 INFO:Local step 87500, global step 1384424: loss 1.5727
[2019-04-07 16:29:03,108] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 87500, global step 1384424: learning rate 0.0000
[2019-04-07 16:29:05,144] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86000, global step 1384703: loss 0.4870
[2019-04-07 16:29:05,147] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86000, global step 1384703: learning rate 0.0000
[2019-04-07 16:29:06,582] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86000, global step 1384906: loss 0.5326
[2019-04-07 16:29:06,583] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86000, global step 1384906: learning rate 0.0000
[2019-04-07 16:29:07,960] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86000, global step 1385139: loss 0.4331
[2019-04-07 16:29:07,960] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86000, global step 1385139: learning rate 0.0000
[2019-04-07 16:29:08,018] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2066524e-27 4.9605228e-26 2.4820576e-23 1.3232587e-16 5.3938120e-24
 1.0000000e+00 9.6139249e-18 2.9838827e-19], sum to 1.0000
[2019-04-07 16:29:08,018] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7024
[2019-04-07 16:29:08,136] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 86.0, 0.0, 0.0, 24.0, 23.33262021354145, -0.1487387014740344, 0.0, 1.0, 54744.61810216985], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1886400.0000, 
sim time next is 1888200.0000, 
raw observation next is [-5.6, 84.5, 0.0, 0.0, 24.0, 23.36722010137248, -0.1490886032087078, 0.0, 1.0, 46511.96268956975], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.845, 0.0, 0.0, 0.5, 0.44726834178104014, 0.45030379893043077, 0.0, 1.0, 0.22148553661699882], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7194683], dtype=float32), 0.69901055]. 
=============================================
[2019-04-07 16:29:11,214] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86000, global step 1385551: loss 0.3890
[2019-04-07 16:29:11,215] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86000, global step 1385551: learning rate 0.0000
[2019-04-07 16:29:11,442] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86000, global step 1385580: loss 0.4889
[2019-04-07 16:29:11,442] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86000, global step 1385580: learning rate 0.0000
[2019-04-07 16:29:15,067] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.7636668e-28 2.0748159e-25 1.9865229e-23 2.6022637e-17 1.3797456e-23
 1.0000000e+00 1.8974837e-17 1.8088715e-18], sum to 1.0000
[2019-04-07 16:29:15,067] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2519
[2019-04-07 16:29:15,161] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87500, global step 1386105: loss 1.5792
[2019-04-07 16:29:15,162] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87500, global step 1386105: learning rate 0.0000
[2019-04-07 16:29:15,306] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 86.0, 49.0, 0.0, 24.0, 23.77114445843691, -0.1267620961020357, 1.0, 1.0, 6242.311616848667], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2019600.0000, 
sim time next is 2021400.0000, 
raw observation next is [-5.8, 84.5, 69.0, 0.0, 24.0, 23.70331178215571, -0.1191141995695083, 1.0, 1.0, 19948.20677150168], 
processed observation next is [1.0, 0.391304347826087, 0.30193905817174516, 0.845, 0.23, 0.0, 0.5, 0.47527598184630904, 0.4602952668101639, 1.0, 1.0, 0.09499146081667466], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.80672055], dtype=float32), 0.95646226]. 
=============================================
[2019-04-07 16:29:15,869] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86000, global step 1386196: loss 0.4949
[2019-04-07 16:29:15,869] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86000, global step 1386196: learning rate 0.0000
[2019-04-07 16:29:17,377] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86000, global step 1386384: loss 0.5467
[2019-04-07 16:29:17,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86000, global step 1386384: learning rate 0.0000
[2019-04-07 16:29:17,504] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86000, global step 1386405: loss 0.4589
[2019-04-07 16:29:17,505] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86000, global step 1386405: learning rate 0.0000
[2019-04-07 16:29:19,661] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:29:19,662] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:29:19,666] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run32
[2019-04-07 16:29:26,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6922603e-24 1.7603781e-23 3.5863197e-22 2.0883572e-15 1.0562597e-21
 1.0000000e+00 3.3183306e-17 1.4893008e-17], sum to 1.0000
[2019-04-07 16:29:26,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3479
[2019-04-07 16:29:26,193] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.2, 39.0, 37.0, 707.0, 24.0, 23.53220836191604, 0.003085757472952524, 1.0, 1.0, 65798.3802086579], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 401400.0000, 
sim time next is 403200.0000, 
raw observation next is [-8.9, 38.0, 29.0, 555.0, 24.0, 24.55222001951562, 0.09583554836953062, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.38, 0.09666666666666666, 0.6132596685082873, 0.5, 0.5460183349596349, 0.5319451827898435, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37492305], dtype=float32), -1.5160815]. 
=============================================
[2019-04-07 16:29:26,369] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:29:26,369] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:29:26,377] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run32
[2019-04-07 16:29:27,336] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87000, global step 1387656: loss 2.3908
[2019-04-07 16:29:27,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87000, global step 1387656: learning rate 0.0000
[2019-04-07 16:29:39,393] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:29:39,393] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:29:39,396] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run32
[2019-04-07 16:29:47,050] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2820158e-27 4.5101190e-25 2.3123144e-23 7.0015807e-16 1.6312176e-22
 1.0000000e+00 3.6431946e-17 5.0461207e-19], sum to 1.0000
[2019-04-07 16:29:47,050] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6767
[2019-04-07 16:29:47,329] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 87.0, 73.0, 27.5, 24.0, 24.00259332914349, -0.07529822568867035, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2278800.0000, 
sim time next is 2280600.0000, 
raw observation next is [-7.550000000000001, 82.5, 101.0, 39.0, 24.0, 23.98086364396066, -0.08045110422273201, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.25346260387811637, 0.825, 0.33666666666666667, 0.0430939226519337, 0.5, 0.49840530366338837, 0.4731829652590893, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.08259], dtype=float32), 0.84087855]. 
=============================================
[2019-04-07 16:29:52,053] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3834308e-24 4.3458702e-24 1.0008098e-21 5.5341291e-14 1.7623629e-20
 1.0000000e+00 8.9969028e-15 2.0910345e-16], sum to 1.0000
[2019-04-07 16:29:52,054] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7377
[2019-04-07 16:29:52,225] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 26.5, 0.0, 0.0, 24.0, 22.9870303899495, -0.2327649351636267, 0.0, 1.0, 52307.361548136534], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2482200.0000, 
sim time next is 2484000.0000, 
raw observation next is [1.1, 28.0, 0.0, 0.0, 24.0, 23.0112831950516, -0.2111499065967455, 0.0, 1.0, 51270.8466486945], 
processed observation next is [0.0, 0.782608695652174, 0.49307479224376743, 0.28, 0.0, 0.0, 0.5, 0.4176069329209667, 0.42961669780108486, 0.0, 1.0, 0.24414688880330715], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0443095], dtype=float32), -1.3920413]. 
=============================================
[2019-04-07 16:29:52,303] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[88.23737 ]
 [88.71313 ]
 [89.42999 ]
 [89.907776]
 [90.300125]], R is [[88.304039  ]
 [88.42099762]
 [88.53678894]
 [88.65142059]
 [88.76490784]].
[2019-04-07 16:29:53,099] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4507130e-26 9.4373233e-27 4.9826032e-24 4.5874395e-17 8.5958426e-24
 1.0000000e+00 2.0494563e-16 3.3743675e-18], sum to 1.0000
[2019-04-07 16:29:53,099] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8147
[2019-04-07 16:29:53,324] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.55, 87.0, 0.0, 0.0, 24.0, 23.08873152192992, -0.1577588831951436, 0.0, 1.0, 31708.787528857098], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 585000.0000, 
sim time next is 586800.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 24.0, 23.0447426051274, -0.1610066030139835, 0.0, 1.0, 50241.77019099947], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.5, 0.42039521709395, 0.4463311323286721, 0.0, 1.0, 0.23924652471904512], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02316237], dtype=float32), 0.263727]. 
=============================================
[2019-04-07 16:29:53,917] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86500, global step 1390920: loss 0.1999
[2019-04-07 16:29:53,917] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86500, global step 1390920: learning rate 0.0000
[2019-04-07 16:29:56,288] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86500, global step 1391246: loss 0.1392
[2019-04-07 16:29:56,303] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86500, global step 1391246: learning rate 0.0000
[2019-04-07 16:30:04,944] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86500, global step 1392557: loss 0.1577
[2019-04-07 16:30:04,945] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86500, global step 1392557: learning rate 0.0000
[2019-04-07 16:30:07,915] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87500, global step 1392983: loss 1.2417
[2019-04-07 16:30:07,925] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87500, global step 1392983: learning rate 0.0000
[2019-04-07 16:30:08,338] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.01364869e-32 2.49519453e-32 1.63636686e-28 1.05916180e-19
 1.76247675e-28 1.00000000e+00 1.11354545e-20 1.72935370e-24], sum to 1.0000
[2019-04-07 16:30:08,338] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6155
[2019-04-07 16:30:08,367] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 24.0, 24.01441955804582, 0.159885547296804, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1035000.0000, 
sim time next is 1036800.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 24.0, 23.85187331752235, 0.1326941693029464, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.5, 0.48765610979352925, 0.5442313897676488, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7887748], dtype=float32), -0.97002566]. 
=============================================
[2019-04-07 16:30:09,006] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86500, global step 1393153: loss 0.1727
[2019-04-07 16:30:09,020] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86500, global step 1393153: learning rate 0.0000
[2019-04-07 16:30:10,215] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86500, global step 1393319: loss 0.1535
[2019-04-07 16:30:10,215] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86500, global step 1393319: learning rate 0.0000
[2019-04-07 16:30:12,855] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86500, global step 1393746: loss 0.2104
[2019-04-07 16:30:12,856] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86500, global step 1393746: learning rate 0.0000
[2019-04-07 16:30:13,852] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86500, global step 1393906: loss 0.1958
[2019-04-07 16:30:13,854] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86500, global step 1393906: learning rate 0.0000
[2019-04-07 16:30:16,504] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86500, global step 1394325: loss 0.2237
[2019-04-07 16:30:16,505] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86500, global step 1394325: learning rate 0.0000
[2019-04-07 16:30:16,582] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86500, global step 1394338: loss 0.1884
[2019-04-07 16:30:16,583] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86500, global step 1394338: learning rate 0.0000
[2019-04-07 16:30:18,944] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86500, global step 1394732: loss 0.1597
[2019-04-07 16:30:18,944] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86500, global step 1394732: learning rate 0.0000
[2019-04-07 16:30:25,281] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1881039e-24 1.5748055e-23 1.2737953e-22 4.8655674e-16 1.2036342e-21
 1.0000000e+00 1.1411779e-15 2.7742498e-18], sum to 1.0000
[2019-04-07 16:30:25,282] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6177
[2019-04-07 16:30:25,292] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.8, 63.0, 165.5, 0.0, 24.0, 23.53497864378522, 0.1221501822108193, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1166400.0000, 
sim time next is 1168200.0000, 
raw observation next is [18.55, 64.0, 171.0, 0.0, 24.0, 23.53403583369346, 0.1284882576090416, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.976454293628809, 0.64, 0.57, 0.0, 0.5, 0.4611696528077882, 0.5428294192030139, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9880841], dtype=float32), 0.8192282]. 
=============================================
[2019-04-07 16:30:31,293] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:30:31,293] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:30:31,308] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run32
[2019-04-07 16:30:38,201] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87000, global step 1398039: loss 2.3281
[2019-04-07 16:30:38,202] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87000, global step 1398039: learning rate 0.0000
[2019-04-07 16:30:41,765] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87000, global step 1398735: loss 2.0978
[2019-04-07 16:30:41,765] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87000, global step 1398735: learning rate 0.0000
[2019-04-07 16:30:48,052] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-07 16:30:48,053] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:30:48,053] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:30:48,054] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:30:48,055] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:30:48,059] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run71
[2019-04-07 16:30:48,082] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 16:30:48,084] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:30:48,089] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run71
[2019-04-07 16:30:48,118] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run71
[2019-04-07 16:33:08,170] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 16:33:28,427] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 16:33:32,898] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.1134038], dtype=float32), 0.15091744]
[2019-04-07 16:33:32,898] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [12.65740788, 19.54984646, 47.763304845, 454.82819465, 24.0, 27.30994495562629, 0.7717949438075676, 1.0, 1.0, 0.0]
[2019-04-07 16:33:32,898] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 16:33:32,899] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [1.72934275e-29 3.98692199e-28 2.89169291e-26 1.03842716e-17
 2.06619866e-25 1.00000000e+00 1.40072873e-18 5.86466337e-20], sampled 0.10854160457625894
[2019-04-07 16:33:33,063] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 16:33:34,085] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 1400000, evaluation results [1400000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 16:33:35,956] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5707670e-24 1.4714001e-24 1.1737936e-21 3.0627193e-14 1.7633717e-20
 1.0000000e+00 1.6297112e-16 2.2305345e-17], sum to 1.0000
[2019-04-07 16:33:35,956] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2082
[2019-04-07 16:33:35,999] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 69.5, 0.0, 0.0, 24.0, 22.97169766204649, -0.1681505151701038, 0.0, 1.0, 47867.40296404795], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 163800.0000, 
sim time next is 165600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 24.0, 22.87543435258983, -0.1899784575844363, 0.0, 1.0, 46978.521345671536], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.5, 0.4062861960491526, 0.43667384747185456, 0.0, 1.0, 0.2237072445031978], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03445963], dtype=float32), -0.47607476]. 
=============================================
[2019-04-07 16:33:36,720] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87000, global step 1400494: loss 2.2008
[2019-04-07 16:33:36,732] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87000, global step 1400494: learning rate 0.0000
[2019-04-07 16:33:38,674] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0527076e-27 2.4456153e-26 8.1233642e-25 1.1053927e-15 9.7025815e-23
 1.0000000e+00 1.2710900e-17 2.0382504e-18], sum to 1.0000
[2019-04-07 16:33:38,674] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5744
[2019-04-07 16:33:38,740] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 24.0, 23.04278585536452, -0.2205445335672655, 0.0, 1.0, 41964.5878340914], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2007000.0000, 
sim time next is 2008800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 24.0, 22.95464763466078, -0.2329373862185663, 0.0, 1.0, 41976.644796071596], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.5, 0.41288730288839837, 0.4223542045938113, 0.0, 1.0, 0.19988878474319807], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17117022], dtype=float32), 0.21118304]. 
=============================================
[2019-04-07 16:33:38,979] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87000, global step 1400939: loss 2.1952
[2019-04-07 16:33:38,981] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87000, global step 1400939: learning rate 0.0000
[2019-04-07 16:33:39,524] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87000, global step 1401045: loss 2.1599
[2019-04-07 16:33:39,525] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87000, global step 1401046: learning rate 0.0000
[2019-04-07 16:33:42,332] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87000, global step 1401555: loss 2.2251
[2019-04-07 16:33:42,334] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87000, global step 1401555: learning rate 0.0000
[2019-04-07 16:33:42,358] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87000, global step 1401558: loss 2.2672
[2019-04-07 16:33:42,358] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87000, global step 1401558: learning rate 0.0000
[2019-04-07 16:33:44,399] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4403293e-26 4.4150637e-25 2.3145913e-23 2.5282417e-16 4.8090299e-23
 1.0000000e+00 1.5951634e-15 2.8941561e-17], sum to 1.0000
[2019-04-07 16:33:44,400] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3772
[2019-04-07 16:33:44,427] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.0, 25.0, 0.0, 0.0, 24.0, 23.64665903091439, -0.07325040768247056, 0.0, 1.0, 24834.580764789414], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3650400.0000, 
sim time next is 3652200.0000, 
raw observation next is [9.5, 26.0, 0.0, 0.0, 24.0, 23.67148066518728, -0.08095048558813832, 0.0, 1.0, 18723.6695793788], 
processed observation next is [0.0, 0.2608695652173913, 0.7257617728531857, 0.26, 0.0, 0.0, 0.5, 0.47262338876560656, 0.4730165048039539, 0.0, 1.0, 0.08916033133037524], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6082923], dtype=float32), -2.0975816]. 
=============================================
[2019-04-07 16:33:45,111] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2393022e-33 7.7772863e-32 1.4430203e-28 3.7191708e-20 8.1488330e-28
 1.0000000e+00 6.0208922e-21 1.1253093e-21], sum to 1.0000
[2019-04-07 16:33:45,111] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2593
[2019-04-07 16:33:45,113] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87000, global step 1402121: loss 2.1458
[2019-04-07 16:33:45,114] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87000, global step 1402121: learning rate 0.0000
[2019-04-07 16:33:45,144] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.3, 80.0, 107.0, 117.0, 24.0, 25.17689373690236, 0.4026331519210018, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1072800.0000, 
sim time next is 1074600.0000, 
raw observation next is [14.4, 75.0, 114.0, 0.0, 24.0, 25.42791377561531, 0.4360699778824317, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8614958448753465, 0.75, 0.38, 0.0, 0.5, 0.6189928146346091, 0.6453566592941439, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9200813], dtype=float32), -1.1311146]. 
=============================================
[2019-04-07 16:33:45,866] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87000, global step 1402280: loss 2.1554
[2019-04-07 16:33:45,868] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87000, global step 1402280: learning rate 0.0000
[2019-04-07 16:33:47,532] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.2531123e-28 1.4655840e-26 8.1863893e-25 1.3846013e-16 4.6106163e-24
 1.0000000e+00 7.9256291e-17 2.6985839e-19], sum to 1.0000
[2019-04-07 16:33:47,560] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9455
[2019-04-07 16:33:47,619] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 24.0, 23.45987683895984, 0.02836839891575685, 0.0, 1.0, 68408.41417206104], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3798000.0000, 
sim time next is 3799800.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 24.0, 23.54551232866446, 0.01589707485175531, 0.0, 1.0, 18748.517821988044], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.5, 0.4621260273887051, 0.5052990249505851, 0.0, 1.0, 0.08927865629518116], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.94964725], dtype=float32), 0.8603852]. 
=============================================
[2019-04-07 16:33:48,279] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87000, global step 1402771: loss 2.2531
[2019-04-07 16:33:48,280] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87000, global step 1402772: learning rate 0.0000
[2019-04-07 16:33:59,043] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5082058e-27 2.5519587e-26 3.6703391e-25 2.0013852e-16 3.4930060e-23
 1.0000000e+00 3.4806180e-18 1.6493164e-18], sum to 1.0000
[2019-04-07 16:33:59,044] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0028
[2019-04-07 16:33:59,284] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.5, 61.0, 6.0, 163.0, 24.0, 23.63027310696353, -0.04602657052244929, 1.0, 1.0, 15479.622978024097], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3915000.0000, 
sim time next is 3916800.0000, 
raw observation next is [-8.0, 58.0, 48.5, 314.5, 24.0, 23.71156410504513, -0.0197063992490836, 1.0, 1.0, 3123.3262232534084], 
processed observation next is [1.0, 0.34782608695652173, 0.24099722991689754, 0.58, 0.16166666666666665, 0.3475138121546961, 0.5, 0.47596367542042756, 0.49343120025030546, 1.0, 1.0, 0.014872982015492421], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0123962], dtype=float32), 1.6511546]. 
=============================================
[2019-04-07 16:34:01,207] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87500, global step 1405153: loss 1.5030
[2019-04-07 16:34:01,208] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87500, global step 1405153: learning rate 0.0000
[2019-04-07 16:34:05,382] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87500, global step 1405903: loss 1.5996
[2019-04-07 16:34:05,384] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87500, global step 1405903: learning rate 0.0000
[2019-04-07 16:34:12,693] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7859488e-27 2.1381720e-25 3.9395322e-25 1.4926972e-16 1.3251230e-23
 1.0000000e+00 2.6857494e-17 8.0019682e-18], sum to 1.0000
[2019-04-07 16:34:12,720] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8949
[2019-04-07 16:34:12,872] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 65.0, 182.0, 2.0, 24.0, 23.76178940529542, -0.08488883627074228, 1.0, 1.0, 53619.01153801213], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1947600.0000, 
sim time next is 1949400.0000, 
raw observation next is [-3.65, 63.5, 137.0, 0.0, 24.0, 23.91190009671133, -0.06498204355255972, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3614958448753463, 0.635, 0.45666666666666667, 0.0, 0.5, 0.49265834139261094, 0.47833931881581343, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2694725], dtype=float32), -0.36798468]. 
=============================================
[2019-04-07 16:34:13,254] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87500, global step 1407489: loss 1.5773
[2019-04-07 16:34:13,255] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87500, global step 1407489: learning rate 0.0000
[2019-04-07 16:34:17,554] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87500, global step 1408326: loss 1.4589
[2019-04-07 16:34:17,557] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87500, global step 1408326: learning rate 0.0000
[2019-04-07 16:34:17,617] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87500, global step 1408339: loss 1.6357
[2019-04-07 16:34:17,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87500, global step 1408339: learning rate 0.0000
[2019-04-07 16:34:20,124] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87500, global step 1408798: loss 1.5021
[2019-04-07 16:34:20,129] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87500, global step 1408799: learning rate 0.0000
[2019-04-07 16:34:20,496] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87500, global step 1408851: loss 1.4451
[2019-04-07 16:34:20,496] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87500, global step 1408851: learning rate 0.0000
[2019-04-07 16:34:22,631] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87500, global step 1409247: loss 1.5294
[2019-04-07 16:34:22,684] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87500, global step 1409247: learning rate 0.0000
[2019-04-07 16:34:24,345] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2830053e-25 1.7904198e-24 7.7290837e-23 8.9174898e-16 4.4672477e-23
 1.0000000e+00 2.7496737e-15 1.1621894e-17], sum to 1.0000
[2019-04-07 16:34:24,346] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1216
[2019-04-07 16:34:24,382] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 37.0, 33.0, 185.0, 24.0, 23.45669397637472, -0.02577545843756971, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4815000.0000, 
sim time next is 4816800.0000, 
raw observation next is [2.0, 40.0, 16.5, 92.5, 24.0, 23.34474321623599, -0.07395053532933285, 0.0, 1.0, 12453.607780153694], 
processed observation next is [0.0, 0.782608695652174, 0.518005540166205, 0.4, 0.055, 0.10220994475138122, 0.5, 0.44539526801966584, 0.47534982155688904, 0.0, 1.0, 0.059302894191208065], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08668738], dtype=float32), -0.1586628]. 
=============================================
[2019-04-07 16:34:24,454] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:34:24,454] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:34:24,458] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run32
[2019-04-07 16:34:25,043] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87500, global step 1409694: loss 1.4583
[2019-04-07 16:34:25,044] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87500, global step 1409694: learning rate 0.0000
[2019-04-07 16:34:27,738] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87500, global step 1410116: loss 1.5466
[2019-04-07 16:34:27,738] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87500, global step 1410116: learning rate 0.0000
[2019-04-07 16:34:29,302] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:34:29,303] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:34:29,315] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run32
[2019-04-07 16:34:33,432] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.1785814e-26 1.5249322e-25 1.2599970e-23 3.6773213e-16 4.1736333e-23
 1.0000000e+00 9.2025522e-18 2.8870864e-18], sum to 1.0000
[2019-04-07 16:34:33,433] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1385
[2019-04-07 16:34:33,532] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 50.0, 0.0, 0.0, 24.0, 23.4634673320809, -0.1261527452305043, 0.0, 1.0, 42423.68746884785], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4937400.0000, 
sim time next is 4939200.0000, 
raw observation next is [-2.0, 50.0, 0.0, 0.0, 24.0, 23.44765651870108, -0.1410312966247059, 0.0, 1.0, 30840.738113392727], 
processed observation next is [1.0, 0.17391304347826086, 0.40720221606648205, 0.5, 0.0, 0.0, 0.5, 0.4539713765584234, 0.45298956779176475, 0.0, 1.0, 0.1468606576828225], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5356795], dtype=float32), 1.5237263]. 
=============================================
[2019-04-07 16:34:37,526] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:34:37,526] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:34:37,529] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run32
[2019-04-07 16:34:40,608] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:34:40,608] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:34:40,612] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run32
[2019-04-07 16:34:42,137] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:34:42,137] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:34:42,140] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run32
[2019-04-07 16:34:43,937] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:34:43,937] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:34:43,973] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run32
[2019-04-07 16:34:44,113] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:34:44,113] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:34:44,117] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run32
[2019-04-07 16:34:47,420] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:34:47,421] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:34:47,432] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run32
[2019-04-07 16:34:47,860] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:34:47,860] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:34:47,864] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run32
[2019-04-07 16:34:52,145] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:34:52,146] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:34:52,237] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run32
[2019-04-07 16:35:14,517] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.1437507e-31 3.5383938e-28 3.6386918e-26 1.0561041e-18 5.6561549e-26
 1.0000000e+00 3.7249372e-19 1.2818443e-20], sum to 1.0000
[2019-04-07 16:35:14,517] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1540
[2019-04-07 16:35:14,573] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.7, 92.0, 0.0, 0.0, 24.0, 23.34195971914093, -0.1386444461369805, 0.0, 1.0, 42945.58772004904], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 511200.0000, 
sim time next is 513000.0000, 
raw observation next is [3.0, 94.0, 0.0, 0.0, 24.0, 23.3786391141081, -0.1308737825482239, 0.0, 1.0, 41189.344687752244], 
processed observation next is [1.0, 0.9565217391304348, 0.5457063711911359, 0.94, 0.0, 0.0, 0.5, 0.448219926175675, 0.4563754058172587, 0.0, 1.0, 0.19613973660834402], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5067008], dtype=float32), 0.95979357]. 
=============================================
[2019-04-07 16:35:14,592] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[107.96425]
 [107.50669]
 [106.88314]
 [106.1235 ]
 [104.71857]], R is [[107.79135132]
 [107.71343994]
 [107.63630676]
 [107.55994415]
 [107.26747894]].
[2019-04-07 16:35:24,962] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.3677119e-25 9.6285767e-24 1.3784626e-22 1.7491129e-15 1.6924500e-21
 1.0000000e+00 9.0143491e-16 2.4785656e-16], sum to 1.0000
[2019-04-07 16:35:24,963] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2221
[2019-04-07 16:35:25,194] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.4, 60.0, 64.5, 746.5, 24.0, 23.81273392117757, -0.1280846958126492, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 385200.0000, 
sim time next is 387000.0000, 
raw observation next is [-13.1, 55.5, 58.0, 764.0, 24.0, 23.67129928572353, -0.1283401515963921, 1.0, 1.0, 66008.96782495447], 
processed observation next is [1.0, 0.4782608695652174, 0.0997229916897507, 0.555, 0.19333333333333333, 0.8441988950276244, 0.5, 0.4726082738102943, 0.4572199494678693, 1.0, 1.0, 0.31432841821406887], 
reward next is 0.9714, 
noisyNet noise sample is [array([-1.6071671], dtype=float32), -1.835663]. 
=============================================
[2019-04-07 16:35:25,200] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[90.29668 ]
 [90.19915 ]
 [90.48387 ]
 [90.46644 ]
 [89.426765]], R is [[90.09522247]
 [90.1942749 ]
 [90.29233551]
 [90.38941193]
 [90.48551941]].
[2019-04-07 16:35:38,085] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.8290527e-29 6.6754119e-28 1.2936108e-25 1.1458973e-16 1.5581679e-25
 1.0000000e+00 1.0124082e-17 1.7937801e-20], sum to 1.0000
[2019-04-07 16:35:38,085] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4263
[2019-04-07 16:35:38,112] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 48.5, 109.0, 764.0, 24.0, 24.88230762084509, 0.2208015222862647, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3407400.0000, 
sim time next is 3409200.0000, 
raw observation next is [3.0, 49.0, 112.0, 784.0, 24.0, 25.01612421089506, 0.2284104519901944, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.49, 0.37333333333333335, 0.8662983425414365, 0.5, 0.5846770175745885, 0.5761368173300648, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7969978], dtype=float32), 1.2883838]. 
=============================================
[2019-04-07 16:35:41,929] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-07 16:35:41,930] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:35:41,930] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:35:41,934] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:35:41,934] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run72
[2019-04-07 16:35:41,955] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 16:35:41,957] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:35:41,957] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:35:41,962] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run72
[2019-04-07 16:35:41,989] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run72
[2019-04-07 16:36:39,610] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11402985], dtype=float32), 0.15194179]
[2019-04-07 16:36:39,610] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [5.0, 86.0, 195.5, 154.0, 24.0, 23.74003713352279, 0.1470064489408854, 1.0, 1.0, 12453.607780153694]
[2019-04-07 16:36:39,610] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 16:36:39,611] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [4.6172199e-30 8.9151857e-29 8.1007348e-27 3.5031831e-18 6.9020479e-26
 1.0000000e+00 3.7834810e-19 1.4855989e-20], sampled 0.5604769359764218
[2019-04-07 16:38:05,614] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 16:38:25,633] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 16:38:29,018] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 16:38:30,041] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 1420000, evaluation results [1420000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 16:38:30,629] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.4524046e-28 1.7827363e-27 4.5191625e-24 1.1713633e-16 1.1777439e-23
 1.0000000e+00 2.2947897e-18 2.1992290e-19], sum to 1.0000
[2019-04-07 16:38:30,629] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8938
[2019-04-07 16:38:30,706] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 24.0, 23.09852817569197, -0.2149365941226256, 0.0, 1.0, 42355.761860824576], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 691200.0000, 
sim time next is 693000.0000, 
raw observation next is [-3.65, 71.5, 0.0, 0.0, 24.0, 23.08567818155402, -0.2249084685119493, 0.0, 1.0, 42082.39568254092], 
processed observation next is [1.0, 0.0, 0.3614958448753463, 0.715, 0.0, 0.0, 0.5, 0.4238065151295016, 0.42503051049601687, 0.0, 1.0, 0.200392360393052], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22459649], dtype=float32), 1.3450707]. 
=============================================
[2019-04-07 16:38:30,734] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[95.30951]
 [94.11021]
 [94.26857]
 [94.41302]
 [94.42183]], R is [[96.82505798]
 [96.85681152]
 [96.88824463]
 [96.91936493]
 [96.95017242]].
[2019-04-07 16:38:46,123] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.1335097e-27 2.1025499e-26 1.3101490e-24 1.4230164e-16 1.3110602e-22
 1.0000000e+00 4.7709077e-18 4.3095643e-18], sum to 1.0000
[2019-04-07 16:38:46,123] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6585
[2019-04-07 16:38:46,306] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 84.0, 0.0, 0.0, 24.0, 23.03300370802211, -0.1961313121532292, 1.0, 1.0, 70993.66613067372], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 844200.0000, 
sim time next is 846000.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 24.0, 23.19281996927643, -0.1200100651892626, 1.0, 1.0, 74282.25426930534], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.86, 0.0, 0.0, 0.5, 0.4327349974397026, 0.45999664493691245, 1.0, 1.0, 0.35372502033002545], 
reward next is 0.9320, 
noisyNet noise sample is [array([-0.22126973], dtype=float32), -0.1985961]. 
=============================================
[2019-04-07 16:38:46,310] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[99.009094]
 [98.61992 ]
 [98.92042 ]
 [98.44714 ]
 [98.47291 ]], R is [[99.4209671 ]
 [99.37440491]
 [99.38066101]
 [99.38555145]
 [99.39169312]].
[2019-04-07 16:38:53,198] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4244978e-29 6.7332398e-27 1.8179974e-24 3.8274867e-16 5.1392777e-24
 1.0000000e+00 2.5354454e-17 6.2301325e-18], sum to 1.0000
[2019-04-07 16:38:53,198] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3336
[2019-04-07 16:38:53,334] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.0, 39.0, 0.0, 24.0, 23.66956157104298, -0.1203906088848645, 1.0, 1.0, 30257.165584245948], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 835200.0000, 
sim time next is 837000.0000, 
raw observation next is [-3.9, 84.0, 29.0, 0.0, 24.0, 23.84007035113995, -0.05501485067481429, 1.0, 1.0, 33146.08441743008], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.84, 0.09666666666666666, 0.0, 0.5, 0.4866725292616625, 0.48166171644172856, 1.0, 1.0, 0.15783849722585752], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21827435], dtype=float32), 0.32029566]. 
=============================================
[2019-04-07 16:38:53,337] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[98.716064]
 [98.634926]
 [98.27447 ]
 [97.699455]
 [97.50538 ]], R is [[99.16864014]
 [99.17695618]
 [99.18518829]
 [99.19333649]
 [99.20140076]].
[2019-04-07 16:38:58,302] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1137407e-25 7.8029340e-24 1.1240003e-22 1.8360839e-16 4.8260841e-23
 1.0000000e+00 1.0399817e-16 7.9730516e-18], sum to 1.0000
[2019-04-07 16:38:58,302] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5813
[2019-04-07 16:38:58,310] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.1, 83.0, 0.0, 0.0, 24.0, 22.78886061268053, -0.04391297069923421, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1216800.0000, 
sim time next is 1218600.0000, 
raw observation next is [15.8, 88.0, 0.0, 0.0, 24.0, 22.81855099048718, -0.04314008577746372, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9002770083102495, 0.88, 0.0, 0.0, 0.5, 0.40154591587393157, 0.48561997140751206, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44431564], dtype=float32), -0.4489486]. 
=============================================
[2019-04-07 16:38:59,531] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:38:59,531] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:38:59,534] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run33
[2019-04-07 16:39:03,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.24450235e-28 1.40674262e-25 5.43669683e-24 1.15209434e-17
 6.93335214e-24 1.00000000e+00 3.92086721e-17 4.38068493e-19], sum to 1.0000
[2019-04-07 16:39:03,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0331
[2019-04-07 16:39:04,109] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 83.0, 119.0, 0.0, 24.0, 23.07481886682897, -0.09190853370681873, 0.0, 1.0, 42339.62451428621], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1776600.0000, 
sim time next is 1778400.0000, 
raw observation next is [-2.8, 83.0, 109.0, 0.0, 24.0, 23.16229663589679, -0.08421368822663135, 0.0, 1.0, 21858.387467205746], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.83, 0.36333333333333334, 0.0, 0.5, 0.4301913863247326, 0.47192877059112287, 0.0, 1.0, 0.10408755936764641], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.45929936], dtype=float32), -0.378181]. 
=============================================
[2019-04-07 16:39:13,254] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:39:13,254] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:39:13,257] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run33
[2019-04-07 16:39:19,307] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2434407e-26 1.8186754e-26 2.3936618e-25 7.6472861e-16 4.6206369e-23
 1.0000000e+00 3.2908120e-17 2.3050645e-17], sum to 1.0000
[2019-04-07 16:39:19,328] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4996
[2019-04-07 16:39:19,345] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 52.5, 0.0, 0.0, 24.0, 24.86110634090346, 0.3324987201755977, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4649400.0000, 
sim time next is 4651200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 24.0, 24.5732901822326, 0.2811917909083656, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.5, 0.5477741818527168, 0.5937305969694552, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8831073], dtype=float32), -0.6290955]. 
=============================================
[2019-04-07 16:39:35,685] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:39:35,685] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:39:35,694] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run33
[2019-04-07 16:39:39,841] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:39:39,841] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:39:39,854] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run33
[2019-04-07 16:39:53,817] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:39:53,817] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:39:53,821] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run33
[2019-04-07 16:40:13,024] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3528866e-25 2.9262079e-23 1.1394606e-21 2.5968533e-14 6.9545765e-22
 1.0000000e+00 3.1787921e-16 3.3369528e-17], sum to 1.0000
[2019-04-07 16:40:13,024] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9880
[2019-04-07 16:40:13,097] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.1, 67.0, 0.0, 0.0, 24.0, 22.98667259301833, -0.2046797227680671, 0.0, 1.0, 42110.62820513222], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2356200.0000, 
sim time next is 2358000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 24.0, 22.91106768706481, -0.2189703673053253, 0.0, 1.0, 42221.24580297598], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.0, 0.0, 0.5, 0.40925564058873426, 0.42700987756489156, 0.0, 1.0, 0.20105355144274276], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8320397], dtype=float32), 0.17775838]. 
=============================================
[2019-04-07 16:40:13,101] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[94.23572 ]
 [94.343185]
 [94.4995  ]
 [94.61586 ]
 [94.62822 ]], R is [[94.26725006]
 [94.32457733]
 [94.3813324 ]
 [94.43752289]
 [94.4931488 ]].
[2019-04-07 16:40:23,322] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2880922e-27 2.9028001e-26 7.7591522e-24 2.3659480e-16 3.3547986e-23
 1.0000000e+00 2.8558030e-18 1.0563902e-18], sum to 1.0000
[2019-04-07 16:40:23,322] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0301
[2019-04-07 16:40:23,392] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 72.0, 0.0, 0.0, 24.0, 23.18906412661108, -0.1076188229679178, 0.0, 1.0, 46017.38959258851], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2678400.0000, 
sim time next is 2680200.0000, 
raw observation next is [-8.0, 70.5, 0.0, 0.0, 24.0, 23.1736575129584, -0.1191878635515144, 0.0, 1.0, 45876.37581700442], 
processed observation next is [1.0, 0.0, 0.24099722991689754, 0.705, 0.0, 0.0, 0.5, 0.4311381260798666, 0.46027071214949516, 0.0, 1.0, 0.2184589324619258], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35004008], dtype=float32), -1.0038362]. 
=============================================
[2019-04-07 16:40:25,222] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.0792764e-27 1.1732138e-25 9.7339393e-24 5.4301281e-16 9.8536086e-23
 1.0000000e+00 2.9824504e-18 2.1038340e-18], sum to 1.0000
[2019-04-07 16:40:25,222] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2372
[2019-04-07 16:40:25,242] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 54.0, 94.0, 673.5, 24.0, 24.7294096789593, 0.221802393157065, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2732400.0000, 
sim time next is 2734200.0000, 
raw observation next is [-3.5, 52.0, 86.0, 614.0, 24.0, 25.03449637919768, 0.2635085583028803, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.36565096952908593, 0.52, 0.2866666666666667, 0.6784530386740332, 0.5, 0.5862080315998067, 0.5878361861009601, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6883594], dtype=float32), -0.8548151]. 
=============================================
[2019-04-07 16:40:35,087] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-07 16:40:35,088] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:40:35,088] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:40:35,099] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:40:35,100] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:40:35,103] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 16:40:35,104] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run73
[2019-04-07 16:40:35,119] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:40:35,122] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run73
[2019-04-07 16:40:35,143] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run73
[2019-04-07 16:42:52,751] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 16:43:12,973] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 16:43:17,550] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 16:43:18,573] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1440000, evaluation results [1440000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 16:43:27,225] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:43:27,225] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:43:27,228] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run33
[2019-04-07 16:43:33,976] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4460819e-26 4.9855664e-25 9.7899098e-24 2.1829824e-17 1.5385366e-23
 1.0000000e+00 4.5084513e-17 9.4006409e-19], sum to 1.0000
[2019-04-07 16:43:33,977] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0037
[2019-04-07 16:43:34,057] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.2, 75.0, 0.0, 0.0, 24.0, 23.0145333471522, -0.2121105584017276, 0.0, 1.0, 44037.59125541764], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 617400.0000, 
sim time next is 619200.0000, 
raw observation next is [-4.5, 75.0, 0.0, 0.0, 24.0, 22.8873416100486, -0.2365051541903805, 0.0, 1.0, 44610.01318162852], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.75, 0.0, 0.0, 0.5, 0.4072784675040501, 0.4211649486032065, 0.0, 1.0, 0.21242863419823105], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36346036], dtype=float32), -0.011043371]. 
=============================================
[2019-04-07 16:43:34,356] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9972522e-27 4.1123415e-25 1.9022538e-23 7.0256603e-16 1.5129635e-22
 1.0000000e+00 5.8329484e-16 2.5313753e-18], sum to 1.0000
[2019-04-07 16:43:34,356] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9226
[2019-04-07 16:43:34,587] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 62.5, 0.0, 0.0, 24.0, 23.11739718236412, -0.06867312502068515, 0.0, 1.0, 130383.42448703363], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3007800.0000, 
sim time next is 3009600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 24.0, 23.62817576806341, -0.03663619424266645, 0.0, 1.0, 28001.190747657238], 
processed observation next is [0.0, 0.8695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.5, 0.4690146473386176, 0.4877879352524445, 0.0, 1.0, 0.13333900356027256], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26941413], dtype=float32), 0.48306522]. 
=============================================
[2019-04-07 16:43:40,922] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.4463903e-29 3.4325506e-29 2.5382111e-26 1.5206109e-17 1.6039838e-26
 1.0000000e+00 6.7040528e-18 2.1664218e-19], sum to 1.0000
[2019-04-07 16:43:40,923] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6026
[2019-04-07 16:43:41,037] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 100.0, 0.0, 0.0, 24.0, 23.50000930918636, -0.08807792893597331, 0.0, 1.0, 32711.35725977499], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3105000.0000, 
sim time next is 3106800.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 24.0, 23.52350246339093, -0.09746112994716372, 0.0, 1.0, 27590.03581786828], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.5, 0.46029187194924415, 0.4675129566842788, 0.0, 1.0, 0.1313811229422299], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2663229], dtype=float32), 1.4583919]. 
=============================================
[2019-04-07 16:43:56,654] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0972209e-33 2.2190033e-30 1.1119110e-28 2.1156717e-19 1.1745169e-25
 1.0000000e+00 4.6208179e-21 1.6850931e-21], sum to 1.0000
[2019-04-07 16:43:56,655] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4871
[2019-04-07 16:43:56,697] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 24.0, 23.90844198010087, 0.08010360918928737, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1017000.0000, 
sim time next is 1018800.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 24.0, 23.73007279837715, 0.06918740803858696, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.81, 0.0, 0.0, 0.5, 0.47750606653142924, 0.5230624693461957, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7829877], dtype=float32), 0.9372867]. 
=============================================
[2019-04-07 16:44:14,446] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.6685698e-27 1.7898044e-25 1.6686391e-23 2.5432543e-16 6.5084630e-24
 1.0000000e+00 1.5311355e-16 2.9651619e-18], sum to 1.0000
[2019-04-07 16:44:14,446] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4397
[2019-04-07 16:44:14,518] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 24.0, 23.55895990808847, 0.01634602596011092, 0.0, 1.0, 28048.37105189668], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4147200.0000, 
sim time next is 4149000.0000, 
raw observation next is [-1.0, 40.5, 0.0, 0.0, 24.0, 23.50813627772776, 0.01148645493347068, 0.0, 1.0, 48038.63227495891], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.405, 0.0, 0.0, 0.5, 0.45901135647731345, 0.5038288183111569, 0.0, 1.0, 0.22875539178551862], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.57191324], dtype=float32), 0.24287632]. 
=============================================
[2019-04-07 16:44:14,522] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[93.73394 ]
 [95.864365]
 [96.58182 ]
 [95.79535 ]
 [96.24174 ]], R is [[92.47779846]
 [92.55302429]
 [92.62749481]
 [92.46219635]
 [92.53757477]].
[2019-04-07 16:44:21,352] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3903535e-25 1.3694614e-25 3.3811381e-22 3.3126679e-15 5.8286124e-23
 1.0000000e+00 1.0754166e-15 2.2667755e-17], sum to 1.0000
[2019-04-07 16:44:21,354] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4206
[2019-04-07 16:44:21,352] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.1464270e-25 2.4550586e-24 5.1845327e-22 3.5137600e-16 6.6116187e-22
 1.0000000e+00 1.1461028e-16 7.9083802e-18], sum to 1.0000
[2019-04-07 16:44:21,407] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9133
[2019-04-07 16:44:21,435] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 42.0, 187.0, 89.0, 24.0, 23.25856766070465, -0.0737403129237212, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4199400.0000, 
sim time next is 4201200.0000, 
raw observation next is [2.0, 44.0, 173.5, 313.5, 24.0, 23.16226209406786, -0.05134242630778572, 0.0, 1.0, 27225.882072209923], 
processed observation next is [0.0, 0.6521739130434783, 0.518005540166205, 0.44, 0.5783333333333334, 0.34640883977900555, 0.5, 0.43018850783898827, 0.48288585789740474, 0.0, 1.0, 0.12964705748671393], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00814171], dtype=float32), -2.3769364]. 
=============================================
[2019-04-07 16:44:21,515] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 54.0, 55.0, 26.5, 24.0, 22.99085416266085, -0.2100448317540773, 0.0, 1.0, 41270.61270259313], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 662400.0000, 
sim time next is 664200.0000, 
raw observation next is [-0.8999999999999999, 55.5, 27.0, 15.0, 24.0, 23.00119296218458, -0.2207267056395305, 0.0, 1.0, 35258.28361134026], 
processed observation next is [0.0, 0.6956521739130435, 0.43767313019390586, 0.555, 0.09, 0.016574585635359115, 0.5, 0.41676608018204825, 0.4264244314534898, 0.0, 1.0, 0.1678965886254298], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5774965], dtype=float32), -1.3572567]. 
=============================================
[2019-04-07 16:44:39,509] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:44:39,509] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:44:39,513] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run33
[2019-04-07 16:44:43,283] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:44:43,283] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:44:43,287] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run33
[2019-04-07 16:44:53,933] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.77406713e-26 6.53976476e-26 1.11252896e-23 1.80131271e-16
 6.99264103e-23 1.00000000e+00 3.37474238e-17 1.51018737e-18], sum to 1.0000
[2019-04-07 16:44:53,934] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6710
[2019-04-07 16:44:53,964] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 89.5, 638.0, 24.0, 23.46865754037896, 0.02097536772250336, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4809600.0000, 
sim time next is 4811400.0000, 
raw observation next is [3.0, 35.5, 82.0, 549.0, 24.0, 23.49467085287927, 0.02258166508152306, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.355, 0.2733333333333333, 0.6066298342541436, 0.5, 0.4578892377399392, 0.507527221693841, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.79063565], dtype=float32), 0.0732994]. 
=============================================
[2019-04-07 16:44:55,079] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:44:55,079] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:44:55,107] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run33
[2019-04-07 16:44:55,677] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:44:55,677] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:44:55,681] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run33
[2019-04-07 16:44:59,198] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:44:59,198] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:44:59,202] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run33
[2019-04-07 16:44:59,874] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:44:59,874] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:44:59,878] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run33
[2019-04-07 16:45:01,665] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:45:01,665] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:45:01,670] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run33
[2019-04-07 16:45:02,570] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:45:02,570] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:45:02,588] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run33
[2019-04-07 16:45:03,837] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.0227373e-26 2.4104853e-25 4.1010796e-24 4.6370776e-15 6.1507958e-23
 1.0000000e+00 2.0619485e-16 3.3756807e-18], sum to 1.0000
[2019-04-07 16:45:03,837] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5112
[2019-04-07 16:45:03,960] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 48.5, 155.0, 206.0, 24.0, 24.36085943020235, 0.06602469630598919, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2647800.0000, 
sim time next is 2649600.0000, 
raw observation next is [0.5, 50.0, 115.0, 165.0, 24.0, 23.12552473357525, -0.0480427320947661, 1.0, 1.0, 52352.182114844], 
processed observation next is [1.0, 0.6956521739130435, 0.4764542936288089, 0.5, 0.38333333333333336, 0.18232044198895028, 0.5, 0.4271270611312709, 0.4839857559684113, 1.0, 1.0, 0.24929610530878096], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.280224], dtype=float32), 0.5252987]. 
=============================================
[2019-04-07 16:45:04,880] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0590080e-27 1.9244308e-27 4.8943248e-25 1.3568369e-16 1.8007481e-25
 1.0000000e+00 3.6503154e-18 3.1397522e-19], sum to 1.0000
[2019-04-07 16:45:04,880] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5473
[2019-04-07 16:45:04,906] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 25.0, 122.5, 855.0, 24.0, 25.71328253203415, 0.3839023311675804, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4968000.0000, 
sim time next is 4969800.0000, 
raw observation next is [6.5, 24.5, 123.0, 865.0, 24.0, 25.80747807178243, 0.4047296920940502, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6426592797783934, 0.245, 0.41, 0.9558011049723757, 0.5, 0.6506231726485359, 0.6349098973646834, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3284392], dtype=float32), 0.6107718]. 
=============================================
[2019-04-07 16:45:06,197] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:45:06,197] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:45:06,200] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run33
[2019-04-07 16:45:09,565] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9552946e-24 3.2700291e-23 9.4137607e-22 1.0867286e-15 6.0410660e-22
 1.0000000e+00 7.1740706e-17 7.2301409e-18], sum to 1.0000
[2019-04-07 16:45:09,566] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3591
[2019-04-07 16:45:09,698] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 65.0, 95.0, 383.0, 24.0, 23.904145956371, -0.08732575653347106, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 293400.0000, 
sim time next is 295200.0000, 
raw observation next is [-11.7, 63.0, 91.0, 447.5, 24.0, 23.91951035889454, -0.08764788882319831, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.13850415512465375, 0.63, 0.30333333333333334, 0.494475138121547, 0.5, 0.4932925299078785, 0.47078403705893385, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4143639], dtype=float32), -0.46000257]. 
=============================================
[2019-04-07 16:45:10,205] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:45:10,205] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:45:10,208] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run33
[2019-04-07 16:45:15,576] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-07 16:45:15,584] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:45:15,584] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:45:15,585] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:45:15,585] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:45:15,586] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 16:45:15,586] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:45:15,600] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run74
[2019-04-07 16:45:15,628] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run74
[2019-04-07 16:45:15,648] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run74
[2019-04-07 16:47:37,988] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 16:47:55,879] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 16:47:57,994] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 16:47:59,019] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 1460000, evaluation results [1460000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 16:48:04,872] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.2187447e-30 1.0733638e-29 1.3059013e-26 3.6191460e-18 1.0100048e-25
 1.0000000e+00 9.5089488e-19 7.5386220e-21], sum to 1.0000
[2019-04-07 16:48:04,872] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4336
[2019-04-07 16:48:04,945] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.15, 91.0, 0.0, 0.0, 24.0, 23.36218475361684, -0.09668601869525562, 0.0, 1.0, 41986.86118672096], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 91800.0000, 
sim time next is 93600.0000, 
raw observation next is [-1.7, 91.0, 0.0, 0.0, 24.0, 23.28309240609698, -0.1037605079569552, 0.0, 1.0, 42543.395456103404], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.91, 0.0, 0.0, 0.5, 0.44025770050808166, 0.46541316401434824, 0.0, 1.0, 0.20258759741001622], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45370382], dtype=float32), 1.1112154]. 
=============================================
[2019-04-07 16:48:31,454] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.9191298e-26 1.0862303e-24 6.1765387e-24 3.1429925e-16 4.1393713e-22
 1.0000000e+00 2.2717010e-17 9.5091351e-18], sum to 1.0000
[2019-04-07 16:48:31,455] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0071
[2019-04-07 16:48:31,633] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.5, 73.5, 104.0, 615.0, 24.0, 24.57752834123074, 0.1563649328808045, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3317400.0000, 
sim time next is 3319200.0000, 
raw observation next is [-8.0, 70.0, 107.5, 677.5, 24.0, 24.70497567756417, 0.1921252510215557, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.24099722991689754, 0.7, 0.35833333333333334, 0.7486187845303868, 0.5, 0.5587479731303476, 0.5640417503405185, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0426056], dtype=float32), -0.54961985]. 
=============================================
[2019-04-07 16:48:53,700] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.2398481e-29 3.5553083e-27 3.0017420e-24 3.5895728e-17 1.3288892e-23
 1.0000000e+00 9.6677985e-19 1.9672667e-19], sum to 1.0000
[2019-04-07 16:48:53,702] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6878
[2019-04-07 16:48:53,773] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 24.0, 23.45870797865874, -0.004414667091912544, 0.0, 1.0, 50075.39831976261], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3801600.0000, 
sim time next is 3803400.0000, 
raw observation next is [-3.5, 74.0, 0.0, 0.0, 24.0, 23.4465723569768, 0.01624147187025874, 0.0, 1.0, 54137.24523209431], 
processed observation next is [1.0, 0.0, 0.36565096952908593, 0.74, 0.0, 0.0, 0.5, 0.4538810297480668, 0.5054138239567529, 0.0, 1.0, 0.2577964058671158], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05646618], dtype=float32), -0.18176515]. 
=============================================
[2019-04-07 16:48:56,397] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3252858e-29 4.8588073e-27 1.0930925e-25 6.5960639e-18 1.6198736e-25
 1.0000000e+00 8.3459665e-19 1.0355779e-19], sum to 1.0000
[2019-04-07 16:48:56,397] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1027
[2019-04-07 16:48:56,428] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 45.0, 75.5, 634.0, 24.0, 25.4130419487668, 0.3799369760984398, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3859200.0000, 
sim time next is 3861000.0000, 
raw observation next is [3.0, 43.0, 64.0, 551.0, 24.0, 25.54222357873307, 0.2521732872542625, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.43, 0.21333333333333335, 0.6088397790055249, 0.5, 0.628518631561089, 0.5840577624180875, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.73405284], dtype=float32), 1.75279]. 
=============================================
[2019-04-07 16:48:56,436] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[101.98649 ]
 [101.86688 ]
 [101.742714]
 [101.591965]
 [101.5149  ]], R is [[101.76110077]
 [101.74349213]
 [101.72605896]
 [101.70880127]
 [101.69171143]].
[2019-04-07 16:48:56,609] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.7193552e-27 8.1214655e-27 7.5064916e-26 1.6546998e-16 4.3251827e-23
 1.0000000e+00 1.3303845e-17 3.7478867e-18], sum to 1.0000
[2019-04-07 16:48:56,609] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9162
[2019-04-07 16:48:56,654] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 66.0, 111.5, 423.5, 24.0, 23.9355021345288, -0.09091840296460053, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 730800.0000, 
sim time next is 732600.0000, 
raw observation next is [-0.6, 61.5, 84.0, 779.0, 24.0, 23.97262068390554, -0.04529984015954969, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.615, 0.28, 0.8607734806629834, 0.5, 0.4977183903254616, 0.4849000532801501, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6643099], dtype=float32), -0.48113203]. 
=============================================
[2019-04-07 16:49:04,746] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:49:04,747] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:49:04,776] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run34
[2019-04-07 16:49:20,610] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:49:20,610] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:49:20,615] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run34
[2019-04-07 16:49:43,261] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:49:43,261] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:49:43,265] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run34
[2019-04-07 16:49:48,417] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:49:48,417] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:49:48,444] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run34
[2019-04-07 16:49:48,725] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2122988e-26 8.9232337e-25 1.3111495e-23 2.5158117e-16 9.6581213e-23
 1.0000000e+00 3.4248808e-17 6.0811237e-18], sum to 1.0000
[2019-04-07 16:49:48,726] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8733
[2019-04-07 16:49:48,908] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 77.0, 186.0, 84.0, 24.0, 23.14121159867821, -0.1451453132832774, 0.0, 1.0, 36150.11066552001], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1866600.0000, 
sim time next is 1868400.0000, 
raw observation next is [-4.5, 83.0, 129.0, 42.0, 24.0, 23.16982319464328, -0.1454574978229903, 0.0, 1.0, 33348.16083146027], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.83, 0.43, 0.04640883977900553, 0.5, 0.4308185995536065, 0.4515141673923366, 0.0, 1.0, 0.15880076586409653], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.65268946], dtype=float32), -0.59105194]. 
=============================================
[2019-04-07 16:49:52,442] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.4462629e-28 6.4893107e-28 1.0336803e-25 1.1304747e-17 5.6091664e-25
 1.0000000e+00 3.9578005e-19 9.4048634e-21], sum to 1.0000
[2019-04-07 16:49:52,442] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4947
[2019-04-07 16:49:52,515] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 91.0, 0.0, 0.0, 24.0, 23.10914457441788, -0.1343807839508896, 0.0, 1.0, 44326.20134606568], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2073600.0000, 
sim time next is 2075400.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 24.0, 23.17947872299828, -0.1224393263884614, 0.0, 1.0, 44361.92461295754], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 0.5, 0.4316232269165233, 0.4591868912038462, 0.0, 1.0, 0.21124726006170258], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.4951458], dtype=float32), -1.5082386]. 
=============================================
[2019-04-07 16:49:55,538] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7680273e-26 2.1849970e-26 3.1777242e-24 5.9123186e-16 1.6127239e-22
 1.0000000e+00 3.1674106e-18 1.5405196e-18], sum to 1.0000
[2019-04-07 16:49:55,540] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1617
[2019-04-07 16:49:55,583] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 62.0, 93.0, 0.0, 24.0, 24.00305743191608, -0.09222161783234918, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1954800.0000, 
sim time next is 1956600.0000, 
raw observation next is [-2.8, 62.0, 74.0, 0.0, 24.0, 23.89652084507836, -0.1179575648649945, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.24666666666666667, 0.0, 0.5, 0.49137673708986335, 0.4606808117116685, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1032088], dtype=float32), 0.5494478]. 
=============================================
[2019-04-07 16:49:59,201] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6138610e-27 4.5232283e-26 9.2370218e-25 2.1302278e-16 5.1398319e-22
 1.0000000e+00 2.3043602e-18 6.9352710e-20], sum to 1.0000
[2019-04-07 16:49:59,201] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9552
[2019-04-07 16:49:59,370] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 68.0, 137.0, 0.0, 24.0, 24.03236933910837, 0.009436458746792165, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2124000.0000, 
sim time next is 2125800.0000, 
raw observation next is [-5.3, 68.0, 125.0, 0.0, 24.0, 24.18435337837495, 0.01014853597624707, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.31578947368421056, 0.68, 0.4166666666666667, 0.0, 0.5, 0.5153627815312459, 0.5033828453254157, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04840296], dtype=float32), -0.9275279]. 
=============================================
[2019-04-07 16:50:06,845] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:50:06,845] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:50:06,848] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run34
[2019-04-07 16:50:08,145] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-07 16:50:08,149] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:50:08,149] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:50:08,153] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run75
[2019-04-07 16:50:08,178] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:50:08,179] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:50:08,188] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 16:50:08,190] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:50:08,190] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run75
[2019-04-07 16:50:08,219] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run75
[2019-04-07 16:50:50,896] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11388168], dtype=float32), 0.15313618]
[2019-04-07 16:50:50,897] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [15.0, 96.0, 0.0, 0.0, 24.0, 22.3303378401837, -0.1326931488227205, 0.0, 0.0, 0.0]
[2019-04-07 16:50:50,897] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 16:50:50,898] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [5.8544336e-25 7.7507509e-24 3.8262488e-22 1.8590607e-15 1.0886754e-21
 1.0000000e+00 5.6064968e-16 2.7982829e-17], sampled 0.9646469108995949
[2019-04-07 16:52:30,515] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 16:52:42,206] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11388168], dtype=float32), 0.15313618]
[2019-04-07 16:52:42,206] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-1.0, 35.0, 116.5, 798.0, 24.0, 23.32014898013743, -0.02723953815310351, 0.0, 1.0, 0.0]
[2019-04-07 16:52:42,206] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 16:52:42,207] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [4.9128058e-25 7.6666788e-24 2.1021531e-22 2.6439924e-15 5.8790151e-22
 1.0000000e+00 5.4847554e-16 2.8511672e-17], sampled 0.44680059348229884
[2019-04-07 16:52:53,405] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 16:52:57,815] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 16:52:58,839] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1480000, evaluation results [1480000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 16:53:32,056] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3747800e-24 5.9055745e-25 1.5283182e-22 2.8198993e-15 2.5234451e-22
 1.0000000e+00 4.9578400e-17 4.0997179e-17], sum to 1.0000
[2019-04-07 16:53:32,056] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6742
[2019-04-07 16:53:32,146] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 37.0, 21.0, 403.0, 24.0, 24.4377277630239, 0.03806997048365877, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 405000.0000, 
sim time next is 406800.0000, 
raw observation next is [-8.9, 36.0, 10.5, 210.0, 24.0, 24.15693027100203, -0.1547327299759501, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.21606648199445982, 0.36, 0.035, 0.23204419889502761, 0.5, 0.5130775225835024, 0.44842242334135, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29823366], dtype=float32), 0.5290942]. 
=============================================
[2019-04-07 16:53:38,032] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.9475012e-27 4.3544620e-26 7.0049720e-24 2.4512691e-16 2.8934408e-23
 1.0000000e+00 8.9497534e-18 3.8805537e-19], sum to 1.0000
[2019-04-07 16:53:38,033] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2944
[2019-04-07 16:53:38,158] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.5, 70.0, 117.0, 674.0, 24.0, 24.4295354471131, 0.07622527949898629, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2716200.0000, 
sim time next is 2718000.0000, 
raw observation next is [-9.0, 64.0, 114.5, 727.5, 24.0, 24.48118614871373, 0.09160561117466494, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.21329639889196678, 0.64, 0.38166666666666665, 0.8038674033149171, 0.5, 0.5400988457261441, 0.5305352037248884, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0389346], dtype=float32), 0.36806205]. 
=============================================
[2019-04-07 16:53:38,162] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[93.708664]
 [93.44108 ]
 [93.05074 ]
 [92.09275 ]
 [90.7105  ]], R is [[94.18415833]
 [94.2423172 ]
 [94.29989624]
 [94.35689545]
 [94.41333008]].
[2019-04-07 16:53:39,061] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:53:39,061] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:53:39,064] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run34
[2019-04-07 16:53:46,912] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0003454e-26 1.9904968e-24 2.3056907e-23 5.3369531e-17 2.3653723e-22
 1.0000000e+00 9.8073630e-17 8.3188215e-19], sum to 1.0000
[2019-04-07 16:53:46,927] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7255
[2019-04-07 16:53:46,997] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 24.0, 23.32669051637665, -0.1203556577712895, 0.0, 1.0, 42840.92071334513], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3025800.0000, 
sim time next is 3027600.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 24.0, 23.28292550794952, -0.13530174295897, 0.0, 1.0, 39539.75627421837], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.5, 0.44024379232912675, 0.45489941901367664, 0.0, 1.0, 0.18828455368675415], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49163142], dtype=float32), -0.09341901]. 
=============================================
[2019-04-07 16:53:57,174] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6248255e-24 6.3992046e-24 6.8985047e-23 2.5326929e-15 1.1725316e-21
 1.0000000e+00 2.0088581e-16 1.0482704e-16], sum to 1.0000
[2019-04-07 16:53:57,174] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6094
[2019-04-07 16:53:57,194] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 40.5, 99.0, 775.0, 24.0, 23.24117060740338, -0.06554309329062774, 0.0, 1.0, 12459.310822383495], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3076200.0000, 
sim time next is 3078000.0000, 
raw observation next is [0.0, 39.0, 91.5, 724.0, 24.0, 23.22980340935333, -0.06628323115277486, 0.0, 1.0, 12457.003623602941], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.39, 0.305, 0.8, 0.5, 0.4358169507794442, 0.47790558961574175, 0.0, 1.0, 0.05931906487429972], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5044954], dtype=float32), 0.2667648]. 
=============================================
[2019-04-07 16:53:57,245] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[90.311676]
 [90.53656 ]
 [90.87814 ]
 [90.93053 ]
 [90.915436]], R is [[90.09952545]
 [90.1985321 ]
 [90.29654694]
 [90.39358521]
 [90.48964691]].
[2019-04-07 16:54:04,564] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2532587e-28 3.7775240e-27 4.4889529e-26 1.3679945e-17 1.5697119e-25
 1.0000000e+00 1.8564146e-18 2.3197382e-21], sum to 1.0000
[2019-04-07 16:54:04,565] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0380
[2019-04-07 16:54:04,677] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 77.0, 34.0, 307.5, 24.0, 24.85749572836338, 0.2827603693703704, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3258000.0000, 
sim time next is 3259800.0000, 
raw observation next is [-4.0, 71.0, 9.0, 104.0, 24.0, 24.17622251784135, 0.1393556696127338, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.71, 0.03, 0.11491712707182321, 0.5, 0.5146852098201125, 0.5464518898709113, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9298539], dtype=float32), 0.960567]. 
=============================================
[2019-04-07 16:54:06,465] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.6746733e-28 1.7818736e-26 1.9946241e-24 1.9691759e-17 4.6564306e-24
 1.0000000e+00 5.9809555e-18 1.5260136e-19], sum to 1.0000
[2019-04-07 16:54:06,466] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2597
[2019-04-07 16:54:06,527] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.45, 77.0, 0.0, 0.0, 24.0, 23.22007954852882, -0.05770009091748019, 0.0, 1.0, 48307.35493595419], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3295800.0000, 
sim time next is 3297600.0000, 
raw observation next is [-8.9, 77.0, 0.0, 0.0, 24.0, 23.33805654973057, -0.06384760224833493, 0.0, 1.0, 45301.38711919588], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.77, 0.0, 0.0, 0.5, 0.4448380458108809, 0.4787174659172217, 0.0, 1.0, 0.21572089104378991], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03997825], dtype=float32), -2.2151198]. 
=============================================
[2019-04-07 16:54:17,886] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.95950753e-31 8.89123765e-30 3.17124068e-27 8.50648605e-19
 2.03130296e-26 1.00000000e+00 7.33290201e-21 1.20478775e-20], sum to 1.0000
[2019-04-07 16:54:17,886] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8641
[2019-04-07 16:54:17,926] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 92.0, 59.5, 0.0, 24.0, 24.03730743083444, 0.09401449998169127, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1677600.0000, 
sim time next is 1679400.0000, 
raw observation next is [1.3, 92.0, 66.0, 0.0, 24.0, 24.06464065094459, 0.0782298724618339, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49861495844875353, 0.92, 0.22, 0.0, 0.5, 0.5053867209120492, 0.5260766241539446, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48322526], dtype=float32), -1.355639]. 
=============================================
[2019-04-07 16:54:24,354] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9714528e-25 8.9733827e-24 2.1499857e-22 3.5635040e-15 2.4906424e-22
 1.0000000e+00 6.8404073e-16 1.1278515e-17], sum to 1.0000
[2019-04-07 16:54:24,354] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7630
[2019-04-07 16:54:24,437] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 42.5, 0.0, 0.0, 24.0, 23.50884013499575, -0.04731918962916165, 0.0, 1.0, 34971.0203655484], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3627000.0000, 
sim time next is 3628800.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 24.0, 23.67017272898448, -0.03078457424299566, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.7119113573407203, 0.25, 0.0, 0.0, 0.5, 0.47251439408203993, 0.48973847525233477, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18299338], dtype=float32), -1.1231782]. 
=============================================
[2019-04-07 16:54:24,677] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.4645325e-27 5.2837940e-26 1.7929496e-23 1.7199332e-16 7.3102440e-23
 1.0000000e+00 3.9487108e-17 6.6434080e-19], sum to 1.0000
[2019-04-07 16:54:24,677] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2477
[2019-04-07 16:54:24,723] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.6, 26.0, 0.0, 0.0, 24.0, 23.64454640907707, -0.07128994634209297, 0.0, 1.0, 21972.486924839915], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3637800.0000, 
sim time next is 3639600.0000, 
raw observation next is [8.2, 27.0, 0.0, 0.0, 24.0, 23.61633639823481, -0.07548949455810754, 0.0, 1.0, 32786.984693845916], 
processed observation next is [0.0, 0.13043478260869565, 0.6897506925207757, 0.27, 0.0, 0.0, 0.5, 0.46802803318623426, 0.4748368351472975, 0.0, 1.0, 0.1561284985421234], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.97265995], dtype=float32), 1.1680158]. 
=============================================
[2019-04-07 16:54:24,929] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3616276e-30 1.1519429e-28 3.0559014e-27 1.6069362e-18 3.2862575e-25
 1.0000000e+00 1.2781738e-18 5.0339458e-21], sum to 1.0000
[2019-04-07 16:54:24,929] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5176
[2019-04-07 16:54:25,021] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 140.5, 3.0, 24.0, 24.48933303946509, 0.207941890964653, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4456800.0000, 
sim time next is 4458600.0000, 
raw observation next is [0.0, 88.5, 85.0, 0.0, 24.0, 24.68609926494238, 0.2164802193696688, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.885, 0.2833333333333333, 0.0, 0.5, 0.5571749387451984, 0.5721600731232229, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.56290305], dtype=float32), 1.1102909]. 
=============================================
[2019-04-07 16:54:26,110] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1170216e-26 1.9609820e-25 5.2052814e-22 2.4166388e-15 4.6052023e-24
 1.0000000e+00 2.3148645e-16 1.7133903e-17], sum to 1.0000
[2019-04-07 16:54:26,110] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9793
[2019-04-07 16:54:26,165] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 24.0, 23.53196764771628, -0.1017415951727405, 0.0, 1.0, 13190.727277309661], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3729600.0000, 
sim time next is 3731400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 24.0, 23.47507082667667, -0.1071034703255761, 0.0, 1.0, 55323.825163573616], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.5, 0.4562559022230559, 0.4642988432248079, 0.0, 1.0, 0.2634467864932077], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8736839], dtype=float32), -0.59124]. 
=============================================
[2019-04-07 16:54:30,455] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.3487566e-30 1.4507966e-29 6.9956686e-27 1.9124368e-19 8.4905014e-27
 1.0000000e+00 7.5993713e-21 8.6240980e-22], sum to 1.0000
[2019-04-07 16:54:30,455] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9424
[2019-04-07 16:54:30,500] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.5, 72.0, 0.0, 0.0, 24.0, 23.9096064535081, -0.02192069638029305, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4327200.0000, 
sim time next is 4329000.0000, 
raw observation next is [4.25, 71.5, 0.0, 0.0, 24.0, 23.69199926027574, -0.06366612014177137, 0.0, 1.0, 30237.607215095853], 
processed observation next is [1.0, 0.08695652173913043, 0.5803324099722993, 0.715, 0.0, 0.0, 0.5, 0.47433327168964495, 0.47877795995274286, 0.0, 1.0, 0.14398860578617073], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.31481], dtype=float32), 2.1448252]. 
=============================================
[2019-04-07 16:54:30,506] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[105.18649 ]
 [104.98506 ]
 [105.471344]
 [105.83933 ]
 [104.48244 ]], R is [[104.11164093]
 [104.07052612]
 [104.0298233 ]
 [103.98952484]
 [103.94963074]].
[2019-04-07 16:54:40,689] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6350589e-25 1.3469094e-24 4.0687966e-22 9.5592263e-16 1.4784749e-21
 1.0000000e+00 9.1944864e-17 2.1889437e-18], sum to 1.0000
[2019-04-07 16:54:40,690] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2470
[2019-04-07 16:54:40,792] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 24.0, 23.07153507125205, -0.1126077564239715, 0.0, 1.0, 58067.9945000218], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4820400.0000, 
sim time next is 4822200.0000, 
raw observation next is [1.0, 45.0, 0.0, 0.0, 24.0, 23.18432387544205, -0.0518837748524126, 0.0, 1.0, 145302.92592218707], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.45, 0.0, 0.0, 0.5, 0.4320269896201709, 0.4827054083825291, 0.0, 1.0, 0.6919186948675575], 
reward next is 0.5938, 
noisyNet noise sample is [array([1.3470157], dtype=float32), -0.9158716]. 
=============================================
[2019-04-07 16:54:49,236] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3544253e-26 3.1380646e-26 4.8141483e-24 5.7340862e-17 5.2903164e-23
 1.0000000e+00 4.7612016e-17 1.3255916e-18], sum to 1.0000
[2019-04-07 16:54:49,236] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6404
[2019-04-07 16:54:49,308] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 24.0, 23.6425667624127, -0.01015778678812672, 0.0, 1.0, 26815.43030084703], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 5023800.0000, 
sim time next is 5025600.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 24.0, 23.55546788939997, -0.0292954205006536, 0.0, 1.0, 44578.14789713463], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.55, 0.0, 0.0, 0.5, 0.46295565744999756, 0.49023485983311543, 0.0, 1.0, 0.21227689474826014], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00452595], dtype=float32), 1.2781006]. 
=============================================
[2019-04-07 16:54:52,817] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:54:52,818] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:54:52,822] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run34
[2019-04-07 16:54:54,880] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0026978e-28 1.0693143e-28 3.9354594e-26 3.0527944e-17 4.4327494e-25
 1.0000000e+00 7.8051826e-20 5.2219130e-20], sum to 1.0000
[2019-04-07 16:54:54,883] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8467
[2019-04-07 16:54:54,943] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 74.0, 0.0, 0.0, 24.0, 23.75604281505299, 0.09950483280539431, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4429800.0000, 
sim time next is 4431600.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 24.0, 23.84651606009406, 0.08152096451433599, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.5, 0.4872096716745051, 0.527173654838112, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03041403], dtype=float32), 0.04633594]. 
=============================================
[2019-04-07 16:54:55,454] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-07 16:54:55,466] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:54:55,481] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:54:55,485] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run76
[2019-04-07 16:54:55,528] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:54:55,539] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 16:54:55,545] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:54:55,587] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:54:57,243] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run76
[2019-04-07 16:54:57,435] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run76
[2019-04-07 16:55:35,309] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11424045], dtype=float32), 0.15385836]
[2019-04-07 16:55:35,309] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [8.442819590500001, 88.8882777, 0.0, 0.0, 24.0, 23.53744481361757, -0.02360626576722454, 0.0, 1.0, 38601.91151708579]
[2019-04-07 16:55:35,309] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 16:55:35,310] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [9.0154502e-31 1.8908389e-29 2.2740826e-27 5.2259808e-19 8.8699420e-27
 1.0000000e+00 1.1839039e-19 3.3734227e-21], sampled 0.46113544085290514
[2019-04-07 16:56:04,393] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11424045], dtype=float32), 0.15385836]
[2019-04-07 16:56:04,393] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-6.466362682, 70.90133063, 0.0, 0.0, 24.0, 22.59532200334362, -0.3163928964383569, 0.0, 1.0, 46524.48123419604]
[2019-04-07 16:56:04,393] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 16:56:04,394] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [8.6248696e-26 1.3302885e-24 8.8057362e-23 8.2664486e-16 1.7855585e-22
 1.0000000e+00 1.6883017e-16 8.0164191e-18], sampled 0.04343618625672774
[2019-04-07 16:57:20,900] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 16:57:38,927] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 16:57:44,333] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 16:57:45,358] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1500000, evaluation results [1500000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 16:57:53,085] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6112992e-27 1.8041024e-26 1.7693178e-24 1.2996016e-16 5.9571002e-23
 1.0000000e+00 4.1621487e-18 5.4946940e-19], sum to 1.0000
[2019-04-07 16:57:53,085] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7824
[2019-04-07 16:57:53,126] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.4, 55.0, 102.0, 733.0, 24.0, 24.69818270207914, 0.2032684320864317, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2730600.0000, 
sim time next is 2732400.0000, 
raw observation next is [-4.0, 54.0, 94.0, 673.5, 24.0, 24.7294096789593, 0.221802393157065, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.54, 0.31333333333333335, 0.7441988950276243, 0.5, 0.560784139913275, 0.573934131052355, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.88284516], dtype=float32), 0.0053593433]. 
=============================================
[2019-04-07 16:57:53,285] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:57:53,285] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:57:53,288] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run34
[2019-04-07 16:57:54,689] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7625241e-28 1.2025932e-27 1.4018417e-24 6.3835990e-17 4.2893614e-24
 1.0000000e+00 3.5491185e-18 5.8706009e-18], sum to 1.0000
[2019-04-07 16:57:54,689] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5475
[2019-04-07 16:57:54,771] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 46.5, 195.0, 129.0, 24.0, 26.10478030364112, 0.5617625562493627, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4635000.0000, 
sim time next is 4636800.0000, 
raw observation next is [6.0, 43.0, 156.0, 138.0, 24.0, 25.96766184398201, 0.5552124190750676, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6288088642659281, 0.43, 0.52, 0.15248618784530388, 0.5, 0.6639718203318342, 0.6850708063583558, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03033055], dtype=float32), -1.4222534]. 
=============================================
[2019-04-07 16:57:58,363] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.2040005e-26 2.5373992e-25 1.1657742e-22 1.1329194e-15 8.5077670e-23
 1.0000000e+00 1.8598534e-17 1.4888237e-17], sum to 1.0000
[2019-04-07 16:57:58,363] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6952
[2019-04-07 16:57:58,572] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3, 46.0, 79.0, 58.0, 24.0, 23.0574881759484, -0.1571896080946062, 0.0, 1.0, 42566.816847692324], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2392200.0000, 
sim time next is 2394000.0000, 
raw observation next is [-0.6, 45.0, 42.5, 37.0, 24.0, 23.0598651451444, -0.1610923869346757, 0.0, 1.0, 36419.7183435918], 
processed observation next is [0.0, 0.7391304347826086, 0.44598337950138506, 0.45, 0.14166666666666666, 0.04088397790055249, 0.5, 0.4216554287620333, 0.44630253768844147, 0.0, 1.0, 0.17342723020758], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9439381], dtype=float32), 0.06590456]. 
=============================================
[2019-04-07 16:57:58,576] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[91.05972 ]
 [91.125435]
 [91.37519 ]
 [91.065315]
 [90.5786  ]], R is [[90.68682098]
 [90.779953  ]
 [90.87215424]
 [90.96343231]
 [91.05380249]].
[2019-04-07 16:58:01,029] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:58:01,029] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:58:01,033] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run34
[2019-04-07 16:58:03,473] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:58:03,473] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:58:03,477] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run34
[2019-04-07 16:58:06,822] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:58:06,823] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:58:06,826] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run34
[2019-04-07 16:58:09,257] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:58:09,257] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:58:09,260] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run34
[2019-04-07 16:58:09,801] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:58:09,801] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:58:09,804] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run34
[2019-04-07 16:58:15,005] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:58:15,005] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:58:15,008] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run34
[2019-04-07 16:58:15,693] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:58:15,693] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:58:15,704] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run34
[2019-04-07 16:58:16,547] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0902308e-31 1.1283089e-30 1.5859727e-28 9.6307589e-21 2.9983813e-29
 1.0000000e+00 2.4496820e-22 3.0652124e-23], sum to 1.0000
[2019-04-07 16:58:16,548] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6898
[2019-04-07 16:58:16,598] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.0, 100.0, 111.0, 775.5, 24.0, 25.16900654563204, 0.2985241495303741, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3150000.0000, 
sim time next is 3151800.0000, 
raw observation next is [7.5, 96.5, 114.0, 805.0, 24.0, 25.23488998019677, 0.3339345121260206, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6703601108033241, 0.965, 0.38, 0.8895027624309392, 0.5, 0.6029074983497308, 0.6113115040420068, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25162748], dtype=float32), 2.2411005]. 
=============================================
[2019-04-07 16:58:19,675] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:58:19,675] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:58:19,678] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run34
[2019-04-07 16:58:32,306] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.6238455e-27 9.4104065e-25 7.2003967e-23 2.9625320e-15 4.5837347e-23
 1.0000000e+00 4.5991619e-17 3.6774597e-17], sum to 1.0000
[2019-04-07 16:58:32,306] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0376
[2019-04-07 16:58:32,670] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 24.0, 22.41064020263096, -0.2958585613224826, 0.0, 1.0, 46307.29609582811], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 111600.0000, 
sim time next is 113400.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 24.0, 22.66311628968957, -0.1255384268563754, 1.0, 1.0, 148725.33109778128], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.5, 0.3885930241407974, 0.4581538577145416, 1.0, 1.0, 0.708215862370387], 
reward next is 0.5775, 
noisyNet noise sample is [array([0.2542893], dtype=float32), 0.02204993]. 
=============================================
[2019-04-07 16:58:41,573] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5165080e-24 1.4296439e-23 5.1817448e-22 1.5909406e-14 1.3674656e-20
 1.0000000e+00 9.6556875e-16 4.4888559e-16], sum to 1.0000
[2019-04-07 16:58:41,573] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8525
[2019-04-07 16:58:41,662] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.45, 26.5, 129.0, 0.0, 24.0, 23.65163247464021, -0.2091387937993445, 1.0, 1.0, 15289.121743835303], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 477000.0000, 
sim time next is 478800.0000, 
raw observation next is [-1.2, 28.0, 124.0, 0.0, 24.0, 23.44845194809215, -0.2393393373644791, 1.0, 1.0, 21671.382259965594], 
processed observation next is [1.0, 0.5652173913043478, 0.42936288088642666, 0.28, 0.41333333333333333, 0.0, 0.5, 0.45403766234101245, 0.4202202208785069, 1.0, 1.0, 0.10319705838078855], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86331064], dtype=float32), 1.2011493]. 
=============================================
[2019-04-07 16:59:25,683] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8725743e-33 5.7584177e-32 1.4519815e-29 2.9603764e-21 4.5174793e-29
 1.0000000e+00 2.5939741e-20 4.5777409e-23], sum to 1.0000
[2019-04-07 16:59:25,683] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0386
[2019-04-07 16:59:25,702] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.2, 83.0, 61.0, 151.5, 24.0, 24.64221157777231, 0.2864168482766056, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1069200.0000, 
sim time next is 1071000.0000, 
raw observation next is [12.75, 81.5, 100.0, 234.0, 24.0, 24.8547150342392, 0.3458283040423863, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.8157894736842106, 0.815, 0.3333333333333333, 0.2585635359116022, 0.5, 0.5712262528532666, 0.6152761013474621, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3094231], dtype=float32), 0.3384898]. 
=============================================
[2019-04-07 16:59:25,735] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[117.15697 ]
 [116.82978 ]
 [116.840614]
 [116.57672 ]
 [115.91807 ]], R is [[117.60855103]
 [117.4324646 ]
 [117.25814056]
 [117.08556366]
 [116.914711  ]].
[2019-04-07 16:59:27,943] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:59:27,944] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:59:27,952] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run35
[2019-04-07 16:59:31,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.1622812e-33 1.8601736e-31 2.9161697e-29 6.7795238e-21 1.3220714e-26
 1.0000000e+00 4.2165596e-20 3.0270196e-22], sum to 1.0000
[2019-04-07 16:59:31,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3099
[2019-04-07 16:59:31,437] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [12.2, 83.0, 22.0, 69.0, 24.0, 24.16159491584239, 0.2295144389946727, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1067400.0000, 
sim time next is 1069200.0000, 
raw observation next is [12.2, 83.0, 61.0, 151.5, 24.0, 24.64221157777231, 0.2864168482766056, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.8005540166204987, 0.83, 0.20333333333333334, 0.16740331491712707, 0.5, 0.553517631481026, 0.5954722827588685, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22868669], dtype=float32), 0.053368375]. 
=============================================
[2019-04-07 16:59:40,939] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4713209e-25 5.8086550e-24 1.7671601e-23 2.2970866e-16 6.6862376e-22
 1.0000000e+00 4.6012851e-17 7.5057202e-18], sum to 1.0000
[2019-04-07 16:59:40,942] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6268
[2019-04-07 16:59:40,994] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.1, 81.5, 0.0, 0.0, 24.0, 22.84230837326274, -0.03465221117108133, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1215000.0000, 
sim time next is 1216800.0000, 
raw observation next is [16.1, 83.0, 0.0, 0.0, 24.0, 22.78886061268053, -0.04391297069923421, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9085872576177286, 0.83, 0.0, 0.0, 0.5, 0.3990717177233775, 0.4853623431002552, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00694419], dtype=float32), 1.4295865]. 
=============================================
[2019-04-07 16:59:41,764] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 16:59:41,764] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:59:41,768] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run35
[2019-04-07 16:59:44,423] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4369656e-28 3.1943701e-27 1.8549357e-25 3.8719353e-18 2.1303571e-24
 1.0000000e+00 5.5285261e-18 2.5259627e-19], sum to 1.0000
[2019-04-07 16:59:44,423] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2759
[2019-04-07 16:59:44,552] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 86.0, 49.0, 0.0, 24.0, 23.77114445843691, -0.1267620961020357, 1.0, 1.0, 6242.311616848667], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2019600.0000, 
sim time next is 2021400.0000, 
raw observation next is [-5.8, 84.5, 69.0, 0.0, 24.0, 23.70331178215571, -0.1191141995695083, 1.0, 1.0, 19948.20677150168], 
processed observation next is [1.0, 0.391304347826087, 0.30193905817174516, 0.845, 0.23, 0.0, 0.5, 0.47527598184630904, 0.4602952668101639, 1.0, 1.0, 0.09499146081667466], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3010664], dtype=float32), 2.0087004]. 
=============================================
[2019-04-07 16:59:51,806] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-07 16:59:51,806] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:59:51,806] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:59:51,810] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run77
[2019-04-07 16:59:51,832] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 16:59:51,833] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:59:51,837] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run77
[2019-04-07 16:59:51,868] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 16:59:51,869] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 16:59:51,877] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run77
[2019-04-07 17:01:14,386] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11397856], dtype=float32), 0.15407757]
[2019-04-07 17:01:14,386] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [1.9, 75.0, 89.5, 303.0, 24.0, 22.88585553628456, -0.2020012347223914, 1.0, 1.0, 56011.22708601805]
[2019-04-07 17:01:14,386] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 17:01:14,388] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [2.0041167e-27 2.4501162e-26 1.5749910e-24 4.7056601e-17 7.4240852e-24
 1.0000000e+00 9.4009680e-18 4.6913704e-19], sampled 0.051783106017039726
[2019-04-07 17:02:18,075] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 17:02:36,127] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 17:02:38,316] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 17:02:39,338] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1520000, evaluation results [1520000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 17:02:48,633] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.38009756e-24 1.27129885e-24 9.02849602e-23 4.55354854e-16
 4.96053307e-22 1.00000000e+00 7.93479924e-16 9.23582623e-17], sum to 1.0000
[2019-04-07 17:02:48,634] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2981
[2019-04-07 17:02:48,723] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 24.0, 23.12342766997299, -0.193633931768012, 0.0, 1.0, 42544.40885213826], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1998000.0000, 
sim time next is 1999800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 24.0, 23.1256989250085, -0.1935268740615446, 0.0, 1.0, 42339.918985846685], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 0.5, 0.42714157708404166, 0.4354910419794851, 0.0, 1.0, 0.20161866183736515], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1927524], dtype=float32), -2.672609]. 
=============================================
[2019-04-07 17:02:55,700] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:02:55,700] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:02:55,703] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run35
[2019-04-07 17:02:56,065] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:02:56,065] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:02:56,069] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run35
[2019-04-07 17:02:57,048] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5224956e-27 5.6545185e-25 9.9196463e-23 2.2644270e-15 8.1056354e-23
 1.0000000e+00 5.6879010e-17 3.9755650e-19], sum to 1.0000
[2019-04-07 17:02:57,049] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5484
[2019-04-07 17:02:57,085] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 24.0, 23.54551232866446, 0.01589707485175531, 0.0, 1.0, 18748.517821988044], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3799800.0000, 
sim time next is 3801600.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 24.0, 23.45870797865874, -0.004414667091912544, 0.0, 1.0, 50075.39831976261], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.5, 0.454892331554895, 0.49852844430269583, 0.0, 1.0, 0.23845427771315528], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7508161], dtype=float32), -1.517451]. 
=============================================
[2019-04-07 17:03:08,497] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:03:08,497] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:03:08,501] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run35
[2019-04-07 17:03:56,582] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:03:56,583] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:03:56,586] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run35
[2019-04-07 17:04:06,833] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9610742e-30 2.1423910e-27 1.3273876e-26 2.7412163e-19 2.9812095e-26
 1.0000000e+00 1.7648799e-19 6.3179370e-20], sum to 1.0000
[2019-04-07 17:04:06,834] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0377
[2019-04-07 17:04:06,885] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 24.0, 23.55431297089023, 0.005271822586583541, 0.0, 1.0, 8021.022400578513], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1483200.0000, 
sim time next is 1485000.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 24.0, 23.47598714787343, 0.00750651284925891, 0.0, 1.0, 51183.11739328469], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.5, 0.45633226232278573, 0.5025021709497529, 0.0, 1.0, 0.2437291304442128], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23439129], dtype=float32), -1.285652]. 
=============================================
[2019-04-07 17:04:06,909] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[108.581985]
 [108.51352 ]
 [107.953926]
 [107.84671 ]
 [107.766426]], R is [[108.79210663]
 [108.70418549]
 [108.61714172]
 [108.53096771]
 [108.44565582]].
[2019-04-07 17:04:19,294] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5667498e-25 8.6170842e-24 4.1578262e-21 2.9383562e-15 8.0873858e-22
 1.0000000e+00 3.6892081e-16 1.2557837e-16], sum to 1.0000
[2019-04-07 17:04:19,295] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7176
[2019-04-07 17:04:19,342] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 50.0, 111.5, 811.5, 24.0, 23.24085187425528, -0.06980898658742646, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3070800.0000, 
sim time next is 3072600.0000, 
raw observation next is [-1.5, 46.0, 109.0, 806.0, 24.0, 23.22681280628676, -0.06834886774349754, 0.0, 1.0, 18696.326171162968], 
processed observation next is [0.0, 0.5652173913043478, 0.4210526315789474, 0.46, 0.36333333333333334, 0.8906077348066298, 0.5, 0.4355677338572299, 0.4772170440855008, 0.0, 1.0, 0.08903012462458557], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9798876], dtype=float32), -0.002009049]. 
=============================================
[2019-04-07 17:04:21,127] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.7727216e-28 1.0464836e-25 2.6327797e-23 7.5391872e-16 1.7800421e-23
 1.0000000e+00 2.5088143e-17 1.2329787e-18], sum to 1.0000
[2019-04-07 17:04:21,127] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9203
[2019-04-07 17:04:21,158] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.9, 51.5, 0.0, 0.0, 24.0, 25.86478263987483, 0.4987419524008751, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1099800.0000, 
sim time next is 1101600.0000, 
raw observation next is [16.1, 53.0, 0.0, 0.0, 24.0, 25.05957370379096, 0.4313136101957747, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.9085872576177286, 0.53, 0.0, 0.0, 0.5, 0.5882978086492466, 0.6437712033985915, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.174734], dtype=float32), 0.1284907]. 
=============================================
[2019-04-07 17:04:36,951] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.0829492e-32 1.1914771e-30 3.8852907e-28 1.8321770e-19 9.0322575e-29
 1.0000000e+00 6.3862895e-21 7.1662023e-22], sum to 1.0000
[2019-04-07 17:04:36,951] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7621
[2019-04-07 17:04:36,993] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.2, 59.0, 0.0, 0.0, 24.0, 25.21552819521877, 0.4489996960315516, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4395600.0000, 
sim time next is 4397400.0000, 
raw observation next is [9.8, 60.0, 0.0, 0.0, 24.0, 25.06080741463829, 0.4164205080910472, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7340720221606649, 0.6, 0.0, 0.0, 0.5, 0.5884006178865242, 0.638806836030349, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1779174], dtype=float32), -0.41991138]. 
=============================================
[2019-04-07 17:04:42,698] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.40748691e-30 1.34156861e-28 1.24874555e-26 1.52808301e-17
 1.90363846e-25 1.00000000e+00 7.68825398e-19 2.80275623e-19], sum to 1.0000
[2019-04-07 17:04:42,699] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3068
[2019-04-07 17:04:42,946] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 24.0, 23.43936001561104, 0.02420074728006477, 0.0, 1.0, 50494.20016842248], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1728000.0000, 
sim time next is 1729800.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 24.0, 23.45212699307942, 0.02165158443279206, 0.0, 1.0, 39923.42823632017], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.5, 0.4543439160899518, 0.5072171948109306, 0.0, 1.0, 0.19011156303009605], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00140384], dtype=float32), 1.4338857]. 
=============================================
[2019-04-07 17:04:46,451] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0215396e-26 2.7188071e-25 4.8660672e-24 6.2677786e-16 2.0813341e-23
 1.0000000e+00 5.0396237e-17 2.7312391e-18], sum to 1.0000
[2019-04-07 17:04:46,451] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9783
[2019-04-07 17:04:46,568] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 54.5, 114.0, 816.0, 24.0, 23.33582879717972, 0.01393577915710843, 0.0, 1.0, 18706.862632847275], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3583800.0000, 
sim time next is 3585600.0000, 
raw observation next is [-3.0, 55.0, 116.0, 819.5, 24.0, 23.39603666220753, 0.02749921385375594, 0.0, 1.0, 18704.633052294335], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.55, 0.38666666666666666, 0.905524861878453, 0.5, 0.44966972185062737, 0.5091664046179186, 0.0, 1.0, 0.0890696812014016], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1023884], dtype=float32), 0.30930343]. 
=============================================
[2019-04-07 17:04:48,607] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-07 17:04:48,616] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:04:48,616] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:04:48,620] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run78
[2019-04-07 17:04:48,652] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:04:48,653] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:04:48,659] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run78
[2019-04-07 17:04:48,659] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 17:04:48,684] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:04:48,697] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run78
[2019-04-07 17:07:13,217] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 17:07:20,903] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11397058], dtype=float32), 0.15440065]
[2019-04-07 17:07:20,904] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [-2.028027944, 18.40187167, 117.6124089, 645.5810151999999, 24.0, 24.86664485924259, 0.2214918083486274, 1.0, 1.0, 0.0]
[2019-04-07 17:07:20,904] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 17:07:20,904] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [2.6793953e-24 2.5901831e-23 7.5503026e-22 5.0126139e-15 3.6639258e-21
 1.0000000e+00 7.2657322e-16 6.8401166e-17], sampled 0.09741859240840012
[2019-04-07 17:07:30,037] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 17:07:35,023] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 17:07:36,046] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1540000, evaluation results [1540000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 17:07:38,597] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.9991805e-28 1.4802205e-26 9.8890706e-25 5.6068572e-17 4.1896284e-25
 1.0000000e+00 2.4527715e-19 1.5143286e-19], sum to 1.0000
[2019-04-07 17:07:38,598] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5820
[2019-04-07 17:07:38,682] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 68.0, 0.0, 0.0, 24.0, 23.44678193242982, -0.05189080346593265, 0.0, 1.0, 51546.95968050998], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3713400.0000, 
sim time next is 3715200.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 24.0, 23.48312891501744, -0.04939311061997727, 0.0, 1.0, 34890.41999720124], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.5, 0.45692740958478656, 0.4835356297933409, 0.0, 1.0, 0.16614485712952973], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02831546], dtype=float32), -0.96996003]. 
=============================================
[2019-04-07 17:07:54,873] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:07:54,873] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:07:54,876] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run35
[2019-04-07 17:08:01,022] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.0019801e-30 2.6233792e-28 3.4519767e-26 2.7688486e-17 3.9800312e-24
 1.0000000e+00 7.0701220e-18 9.3730278e-20], sum to 1.0000
[2019-04-07 17:08:01,022] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8846
[2019-04-07 17:08:01,117] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.6, 69.0, 0.0, 0.0, 24.0, 23.57821798450555, -0.0686054131142431, 0.0, 1.0, 58137.23195745057], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4338000.0000, 
sim time next is 4339800.0000, 
raw observation next is [3.45, 70.0, 0.0, 0.0, 24.0, 23.59249215669078, -0.05074150986796037, 0.0, 1.0, 45613.308087428486], 
processed observation next is [1.0, 0.21739130434782608, 0.5581717451523546, 0.7, 0.0, 0.0, 0.5, 0.4660410130575651, 0.4830861633773465, 0.0, 1.0, 0.21720622898775468], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.74332666], dtype=float32), 1.2199455]. 
=============================================
[2019-04-07 17:08:02,294] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.7695804e-27 2.7709824e-25 7.0145985e-24 1.6617910e-16 6.8615813e-22
 1.0000000e+00 2.1500322e-17 1.8205013e-18], sum to 1.0000
[2019-04-07 17:08:02,294] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6756
[2019-04-07 17:08:02,374] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 24.0, 23.18328820834455, -0.1767577236396035, 0.0, 1.0, 43630.38345488868], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2169000.0000, 
sim time next is 2170800.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 24.0, 23.18851593805394, -0.1704840335302016, 0.0, 1.0, 43491.77961364759], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.5, 0.4323763281711616, 0.44317198882326614, 0.0, 1.0, 0.2071037124459409], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.665635], dtype=float32), -2.0379214]. 
=============================================
[2019-04-07 17:08:02,502] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4936506e-28 3.1122650e-27 9.0493082e-26 1.6456909e-17 5.2042261e-23
 1.0000000e+00 1.3091887e-18 6.2791571e-20], sum to 1.0000
[2019-04-07 17:08:02,502] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7178
[2019-04-07 17:08:02,550] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.95, 73.0, 0.0, 0.0, 24.0, 23.48625092413194, -0.01151414827025352, 0.0, 1.0, 36060.56463871949], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4505400.0000, 
sim time next is 4507200.0000, 
raw observation next is [-0.9, 73.0, 0.0, 0.0, 24.0, 23.49819778985158, -0.01506070390992409, 0.0, 1.0, 45069.326777483715], 
processed observation next is [1.0, 0.17391304347826086, 0.43767313019390586, 0.73, 0.0, 0.0, 0.5, 0.4581831491542984, 0.4949797653633586, 0.0, 1.0, 0.2146158417975415], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0914265], dtype=float32), 2.3845253]. 
=============================================
[2019-04-07 17:08:03,721] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3633746e-26 1.6078625e-25 3.6381539e-24 2.1064542e-16 7.5138370e-23
 1.0000000e+00 1.4114085e-17 5.8695004e-19], sum to 1.0000
[2019-04-07 17:08:03,721] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0664
[2019-04-07 17:08:03,780] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 24.0, 23.91539120355822, 0.007740301922800156, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4221000.0000, 
sim time next is 4222800.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 24.0, 23.80610023512886, -0.0291602121804648, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.43, 0.0, 0.0, 0.5, 0.48384168626073826, 0.49027992927317837, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6973677], dtype=float32), 1.1766022]. 
=============================================
[2019-04-07 17:08:07,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3376015e-29 5.8429506e-30 2.6145004e-26 3.3988986e-19 5.1394909e-25
 1.0000000e+00 1.3273019e-18 9.7056694e-20], sum to 1.0000
[2019-04-07 17:08:07,478] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8729
[2019-04-07 17:08:07,554] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.3, 38.0, 115.0, 780.0, 24.0, 25.60306895809724, 0.3706578231683508, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4357800.0000, 
sim time next is 4359600.0000, 
raw observation next is [12.6, 34.0, 117.5, 804.0, 24.0, 25.82589782317541, 0.4323959376032904, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8116343490304709, 0.34, 0.39166666666666666, 0.8883977900552487, 0.5, 0.6521581519312841, 0.6441319792010968, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9113463], dtype=float32), 0.35088488]. 
=============================================
[2019-04-07 17:08:14,493] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:08:14,493] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:08:14,523] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run35
[2019-04-07 17:08:19,290] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:08:19,291] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:08:19,294] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run35
[2019-04-07 17:08:21,732] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6984291e-25 3.7301300e-25 1.5654738e-23 1.2463588e-15 2.9406431e-23
 1.0000000e+00 1.0464120e-17 3.8284152e-19], sum to 1.0000
[2019-04-07 17:08:21,732] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6639
[2019-04-07 17:08:21,887] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 39.0, 100.5, 638.5, 24.0, 24.33119154573241, 0.07382673247291034, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4957200.0000, 
sim time next is 4959000.0000, 
raw observation next is [0.0, 34.5, 108.0, 717.0, 24.0, 24.72279369871124, 0.1379489901465012, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.345, 0.36, 0.7922651933701658, 0.5, 0.5602328082259366, 0.5459829967155004, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3774849], dtype=float32), -1.1948837]. 
=============================================
[2019-04-07 17:08:21,891] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[93.507774]
 [93.065765]
 [92.596146]
 [91.91963 ]
 [91.45567 ]], R is [[93.79394531]
 [93.85601044]
 [93.91744995]
 [93.97827911]
 [94.03849792]].
[2019-04-07 17:08:24,501] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:08:24,501] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:08:24,504] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run35
[2019-04-07 17:08:24,847] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2959774e-24 7.5773017e-24 3.8860884e-22 2.1453534e-14 1.5813621e-22
 1.0000000e+00 2.7119210e-16 3.2297344e-18], sum to 1.0000
[2019-04-07 17:08:24,847] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0577
[2019-04-07 17:08:25,237] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 64.0, 42.0, 214.5, 24.0, 22.46807074778421, -0.2952928717585175, 0.0, 1.0, 41560.103032759536], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3052800.0000, 
sim time next is 3054600.0000, 
raw observation next is [-6.0, 61.5, 83.0, 359.0, 24.0, 22.97738684234859, -0.09682443326394163, 0.0, 1.0, 81231.03112238603], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.615, 0.27666666666666667, 0.3966850828729282, 0.5, 0.4147822368623825, 0.4677251889120195, 0.0, 1.0, 0.38681443391612397], 
reward next is 0.8989, 
noisyNet noise sample is [array([1.2489672], dtype=float32), 0.36935872]. 
=============================================
[2019-04-07 17:08:27,753] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:08:27,753] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:08:27,756] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run35
[2019-04-07 17:08:28,997] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:08:28,997] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:08:29,010] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run35
[2019-04-07 17:08:31,281] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:08:31,281] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:08:31,285] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run35
[2019-04-07 17:08:34,177] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:08:34,177] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:08:34,181] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run35
[2019-04-07 17:08:35,068] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2507494e-25 1.1018891e-24 1.3343194e-23 4.5085399e-16 1.0166246e-21
 1.0000000e+00 1.1770654e-16 1.8759416e-18], sum to 1.0000
[2019-04-07 17:08:35,068] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1748
[2019-04-07 17:08:35,126] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 24.0, 22.54635168236839, -0.2730862558903021, 0.0, 1.0, 45807.23814756623], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 172800.0000, 
sim time next is 174600.0000, 
raw observation next is [-8.65, 72.5, 0.0, 0.0, 24.0, 22.52368262434709, -0.2773421015053835, 0.0, 1.0, 45568.014004306366], 
processed observation next is [1.0, 0.0, 0.22299168975069253, 0.725, 0.0, 0.0, 0.5, 0.3769735520289241, 0.4075526328315388, 0.0, 1.0, 0.21699054287764935], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3019756], dtype=float32), -1.0542457]. 
=============================================
[2019-04-07 17:08:39,569] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.3625906e-28 8.1676655e-27 1.3327773e-24 5.2801482e-18 3.7537399e-25
 1.0000000e+00 3.3643421e-18 3.7135748e-18], sum to 1.0000
[2019-04-07 17:08:39,569] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0101
[2019-04-07 17:08:39,614] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.5, 26.0, 111.0, 763.0, 24.0, 23.89561474646973, 0.0692391600053541, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3666600.0000, 
sim time next is 3668400.0000, 
raw observation next is [12.0, 24.0, 113.5, 789.5, 24.0, 23.97773976561648, 0.09310726085339742, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.7950138504155125, 0.24, 0.37833333333333335, 0.8723756906077348, 0.5, 0.49814498046804, 0.5310357536177991, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8771511], dtype=float32), -0.6552627]. 
=============================================
[2019-04-07 17:08:40,842] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:08:40,842] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:08:40,847] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run35
[2019-04-07 17:08:41,430] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:08:41,430] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:08:41,462] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run35
[2019-04-07 17:09:00,126] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6866599e-33 1.1958289e-32 2.4710426e-30 7.8405154e-20 1.0202412e-29
 1.0000000e+00 1.5128424e-22 8.0183830e-24], sum to 1.0000
[2019-04-07 17:09:00,126] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0549
[2019-04-07 17:09:00,190] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.3, 99.5, 58.0, 499.0, 24.0, 25.95363038527099, 0.565171530175771, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3169800.0000, 
sim time next is 3171600.0000, 
raw observation next is [6.0, 100.0, 33.0, 307.5, 24.0, 26.12751205998503, 0.5678615355154385, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.11, 0.3397790055248619, 0.5, 0.6772926716654192, 0.6892871785051461, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7173619], dtype=float32), 0.6358989]. 
=============================================
[2019-04-07 17:09:04,121] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0419227e-25 1.5157292e-24 7.8435544e-23 1.0193460e-15 3.1682502e-22
 1.0000000e+00 1.3125654e-16 4.0908114e-18], sum to 1.0000
[2019-04-07 17:09:04,146] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6557
[2019-04-07 17:09:04,539] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 24.0, 21.69390951502522, -0.5078483011437408, 0.0, 1.0, 45644.18380063553], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 198000.0000, 
sim time next is 199800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 24.0, 21.95564212908399, -0.3457037775896326, 0.0, 1.0, 150490.13950192672], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.0, 0.0, 0.5, 0.32963684409033256, 0.3847654074701225, 0.0, 1.0, 0.7166197119139368], 
reward next is 0.5691, 
noisyNet noise sample is [array([-2.2787292], dtype=float32), 0.037259188]. 
=============================================
[2019-04-07 17:09:09,399] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2732748e-28 6.0546850e-27 2.2228491e-23 5.3016662e-17 2.7409270e-24
 1.0000000e+00 9.2866955e-18 1.0169093e-18], sum to 1.0000
[2019-04-07 17:09:09,400] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5603
[2019-04-07 17:09:09,471] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 24.0, 23.39698658081269, -0.04103273283952182, 0.0, 1.0, 46771.377394660325], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3371400.0000, 
sim time next is 3373200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 24.0, 23.57245147189493, -0.02942840887542461, 0.0, 1.0, 15413.889599133932], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.5, 0.4643709559912441, 0.49019053037485844, 0.0, 1.0, 0.07339947428159016], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02943766], dtype=float32), 0.61113745]. 
=============================================
[2019-04-07 17:09:43,275] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:09:43,275] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:09:43,325] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run36
[2019-04-07 17:09:44,532] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-07 17:09:44,545] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:09:44,545] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:09:44,546] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:09:44,546] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 17:09:44,546] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:09:44,546] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:09:44,552] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run79
[2019-04-07 17:09:44,585] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run79
[2019-04-07 17:09:44,607] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run79
[2019-04-07 17:09:53,910] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11428714], dtype=float32), 0.15509622]
[2019-04-07 17:09:53,910] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [5.75, 89.0, 0.0, 0.0, 24.0, 23.46466486155549, 0.02535893348373503, 0.0, 1.0, 46931.27139791262]
[2019-04-07 17:09:53,910] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 17:09:53,910] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [8.9153131e-31 1.7260079e-29 2.0001146e-27 2.7335933e-19 7.5590439e-27
 1.0000000e+00 6.4763292e-20 1.9747986e-21], sampled 0.2186052278068682
[2019-04-07 17:12:13,787] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 17:12:19,077] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11428714], dtype=float32), 0.15509622]
[2019-04-07 17:12:19,078] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [14.2, 31.5, 195.0, 629.0, 24.0, 26.86292005127353, 0.7402492300450066, 1.0, 1.0, 0.0]
[2019-04-07 17:12:19,078] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 17:12:19,078] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [1.0537510e-31 2.7606673e-30 2.1223064e-28 1.0936464e-19 1.6960577e-27
 1.0000000e+00 1.4289374e-20 5.9404240e-22], sampled 0.13869006765004166
[2019-04-07 17:12:19,182] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11428714], dtype=float32), 0.15509622]
[2019-04-07 17:12:19,182] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [13.0, 38.0, 24.0, 0.0, 24.0, 26.75694801879422, 0.6712633004062637, 1.0, 1.0, 0.0]
[2019-04-07 17:12:19,182] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 17:12:19,183] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [2.8178549e-31 6.4786192e-30 6.8151270e-28 1.5446321e-19 4.9860177e-27
 1.0000000e+00 2.3776874e-20 8.7671956e-22], sampled 0.04093130567004044
[2019-04-07 17:12:28,189] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 17:12:33,380] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 17:12:34,404] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1560000, evaluation results [1560000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 17:12:46,709] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:12:46,709] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:12:46,713] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run36
[2019-04-07 17:12:50,239] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4791533e-30 1.5051128e-28 5.1541125e-27 1.3567942e-18 2.4175947e-25
 1.0000000e+00 2.4870669e-19 7.1554710e-21], sum to 1.0000
[2019-04-07 17:12:50,239] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8959
[2019-04-07 17:12:50,293] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 127.0, 0.0, 24.0, 24.43966252938922, 0.1878565277811722, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1337400.0000, 
sim time next is 1339200.0000, 
raw observation next is [1.1, 92.0, 120.0, 0.0, 24.0, 24.39733510228491, 0.1775684207343826, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.4, 0.0, 0.5, 0.5331112585237424, 0.5591894735781275, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51448625], dtype=float32), 1.4304057]. 
=============================================
[2019-04-07 17:12:55,931] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.8295925e-30 2.9268688e-29 1.5272893e-26 3.5466279e-18 9.6869235e-26
 1.0000000e+00 3.0865738e-19 6.6827187e-22], sum to 1.0000
[2019-04-07 17:12:55,931] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0991
[2019-04-07 17:12:55,975] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 121.0, 846.0, 24.0, 25.43669993087832, 0.4091822794112023, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4622400.0000, 
sim time next is 4624200.0000, 
raw observation next is [3.5, 49.0, 120.0, 859.0, 24.0, 25.29595454949046, 0.3979780640893957, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5595567867036012, 0.49, 0.4, 0.949171270718232, 0.5, 0.6079962124575383, 0.6326593546964653, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3819655], dtype=float32), -0.4810188]. 
=============================================
[2019-04-07 17:12:59,438] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.72159070e-27 1.99478949e-26 2.28680299e-25 1.88380210e-17
 1.56576490e-25 1.00000000e+00 2.12244090e-16 1.06473596e-19], sum to 1.0000
[2019-04-07 17:12:59,439] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5374
[2019-04-07 17:12:59,563] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.55, 83.0, 126.0, 0.0, 24.0, 23.05572354281031, -0.08598751575729306, 0.0, 1.0, 40485.92297825631], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1773000.0000, 
sim time next is 1774800.0000, 
raw observation next is [-2.8, 83.0, 122.5, 0.0, 24.0, 23.10609610226797, -0.0855699012124904, 0.0, 1.0, 27330.447137605035], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.4083333333333333, 0.0, 0.5, 0.4255080085223308, 0.47147669959583655, 0.0, 1.0, 0.1301449863695478], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26736695], dtype=float32), 0.14196019]. 
=============================================
[2019-04-07 17:13:06,963] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7484153e-30 4.7914519e-30 2.0349715e-26 1.5535255e-19 2.2137067e-26
 1.0000000e+00 1.8228423e-19 5.3371811e-23], sum to 1.0000
[2019-04-07 17:13:06,963] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5568
[2019-04-07 17:13:07,030] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.6, 52.0, 76.0, 570.5, 24.0, 25.26880336410662, 0.4107532581087797, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1522800.0000, 
sim time next is 1524600.0000, 
raw observation next is [11.9, 51.0, 77.0, 478.0, 24.0, 25.34555458759705, 0.4232704433396051, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7922437673130196, 0.51, 0.25666666666666665, 0.5281767955801105, 0.5, 0.6121295489664208, 0.6410901477798684, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4350101], dtype=float32), 0.23073757]. 
=============================================
[2019-04-07 17:13:15,415] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:13:15,415] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:13:15,432] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run36
[2019-04-07 17:13:17,500] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:13:17,500] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:13:17,507] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run36
[2019-04-07 17:13:23,552] A3C_AGENT_WORKER-Thread-15 INFO:Local step 99500, global step 1568699: loss 0.5872
[2019-04-07 17:13:23,555] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 99500, global step 1568699: learning rate 0.0000
[2019-04-07 17:13:27,217] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:13:27,217] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:13:27,242] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run36
[2019-04-07 17:13:39,036] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.1583023e-25 3.4854191e-23 9.4715734e-22 3.9457937e-15 5.5234344e-22
 1.0000000e+00 3.3971670e-16 6.8024382e-17], sum to 1.0000
[2019-04-07 17:13:39,036] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3091
[2019-04-07 17:13:39,056] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 40.5, 160.0, 538.0, 24.0, 23.35169559423557, -0.007845644389558768, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4203000.0000, 
sim time next is 4204800.0000, 
raw observation next is [3.0, 37.0, 114.0, 544.0, 24.0, 23.42003770954984, -0.002038109278561758, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.37, 0.38, 0.6011049723756906, 0.5, 0.4516698091291535, 0.4993206302404794, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.59222746], dtype=float32), -1.6122147]. 
=============================================
[2019-04-07 17:13:47,260] A3C_AGENT_WORKER-Thread-12 INFO:Local step 99500, global step 1571612: loss 0.5940
[2019-04-07 17:13:47,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 99500, global step 1571612: learning rate 0.0000
[2019-04-07 17:13:57,937] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9962510e-26 1.3325682e-25 8.2262485e-23 1.0729288e-16 3.3339398e-23
 1.0000000e+00 9.1164290e-17 5.7883421e-19], sum to 1.0000
[2019-04-07 17:13:57,937] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2971
[2019-04-07 17:13:58,105] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.05, 43.5, 0.0, 0.0, 24.0, 23.07289455261877, -0.1812888143762579, 0.0, 1.0, 33412.41908200205], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2399400.0000, 
sim time next is 2401200.0000, 
raw observation next is [-2.4, 43.0, 0.0, 0.0, 24.0, 23.06283836417953, -0.186096072626063, 0.0, 1.0, 41210.84291334496], 
processed observation next is [0.0, 0.8260869565217391, 0.39612188365650974, 0.43, 0.0, 0.0, 0.5, 0.4219031970149609, 0.43796797579131236, 0.0, 1.0, 0.1962421091111665], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9410172], dtype=float32), 0.528166]. 
=============================================
[2019-04-07 17:14:03,881] A3C_AGENT_WORKER-Thread-15 INFO:Local step 100000, global step 1573971: loss 0.2922
[2019-04-07 17:14:03,882] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 100000, global step 1573971: learning rate 0.0000
[2019-04-07 17:14:09,526] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6169949e-26 1.6878690e-25 2.7569186e-24 1.0026886e-16 4.6188746e-23
 1.0000000e+00 1.8053844e-17 7.4907284e-19], sum to 1.0000
[2019-04-07 17:14:09,526] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5249
[2019-04-07 17:14:09,636] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.45, 80.5, 0.0, 0.0, 24.0, 22.82770207615837, -0.2488531630929794, 0.0, 1.0, 44326.13392509493], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2611800.0000, 
sim time next is 2613600.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 24.0, 22.67079412033314, -0.2764166011738695, 0.0, 1.0, 44796.907813930826], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.78, 0.0, 0.0, 0.5, 0.38923284336109515, 0.40786113294204346, 0.0, 1.0, 0.21331860863776583], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6961875], dtype=float32), 0.90586835]. 
=============================================
[2019-04-07 17:14:17,613] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:14:17,613] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:14:17,616] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run36
[2019-04-07 17:14:19,894] A3C_AGENT_WORKER-Thread-16 INFO:Local step 99500, global step 1576482: loss 0.5882
[2019-04-07 17:14:19,895] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 99500, global step 1576482: learning rate 0.0000
[2019-04-07 17:14:21,460] A3C_AGENT_WORKER-Thread-6 INFO:Local step 99500, global step 1576695: loss 0.5209
[2019-04-07 17:14:21,460] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 99500, global step 1576695: learning rate 0.0000
[2019-04-07 17:14:26,875] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9397368e-29 3.3303339e-28 2.8629029e-26 2.3092263e-18 2.3417834e-25
 1.0000000e+00 1.2154751e-19 6.2962318e-21], sum to 1.0000
[2019-04-07 17:14:26,875] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8960
[2019-04-07 17:14:27,015] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 81.5, 0.0, 0.0, 24.0, 23.063397854593, -0.007786851579268191, 0.0, 1.0, 129046.59958549129], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2925000.0000, 
sim time next is 2926800.0000, 
raw observation next is [-1.0, 85.0, 0.0, 0.0, 24.0, 23.56843634614177, 0.05490156264127995, 0.0, 1.0, 38685.17785069586], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.85, 0.0, 0.0, 0.5, 0.4640363621784808, 0.5183005208804267, 0.0, 1.0, 0.18421513262236122], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06693034], dtype=float32), 1.2062728]. 
=============================================
[2019-04-07 17:14:27,431] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3031948e-30 2.0670853e-29 1.4831879e-26 8.6745593e-19 2.6188826e-26
 1.0000000e+00 6.8958577e-20 4.1610608e-22], sum to 1.0000
[2019-04-07 17:14:27,431] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6677
[2019-04-07 17:14:27,705] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 21.0, 0.0, 24.0, 21.9191293035185, -0.3492143463773412, 0.0, 1.0, 90934.28898703305], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 30600.0000, 
sim time next is 32400.0000, 
raw observation next is [7.7, 93.0, 29.5, 0.0, 24.0, 22.58013643996006, -0.2901820277045345, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.09833333333333333, 0.0, 0.5, 0.3816780366633384, 0.4032726574318219, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.033503], dtype=float32), -2.1890006]. 
=============================================
[2019-04-07 17:14:28,418] A3C_AGENT_WORKER-Thread-19 INFO:Local step 99500, global step 1577768: loss 0.5391
[2019-04-07 17:14:28,419] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 99500, global step 1577768: learning rate 0.0000
[2019-04-07 17:14:28,669] A3C_AGENT_WORKER-Thread-12 INFO:Local step 100000, global step 1577813: loss 0.2944
[2019-04-07 17:14:28,670] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 100000, global step 1577813: learning rate 0.0000
[2019-04-07 17:14:32,333] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0111173e-28 1.0038886e-27 9.4553499e-25 1.9680043e-17 2.2309947e-24
 1.0000000e+00 2.2842479e-18 6.3214086e-20], sum to 1.0000
[2019-04-07 17:14:32,333] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5801
[2019-04-07 17:14:32,587] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 77.0, 156.0, 0.0, 24.0, 23.54664131067668, -0.1565584338319133, 1.0, 1.0, 51591.84316888807], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2032200.0000, 
sim time next is 2034000.0000, 
raw observation next is [-4.5, 79.0, 152.0, 0.0, 24.0, 22.94532650544621, -0.1054836295775713, 1.0, 1.0, 124821.07017393876], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.79, 0.5066666666666667, 0.0, 0.5, 0.41211054212051756, 0.46483879014080953, 1.0, 1.0, 0.5943860484473275], 
reward next is 0.6913, 
noisyNet noise sample is [array([0.57481575], dtype=float32), -1.0388197]. 
=============================================
[2019-04-07 17:14:32,591] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[97.12269 ]
 [96.99636 ]
 [97.20438 ]
 [97.305595]
 [96.79723 ]], R is [[98.01425934]
 [98.03411865]
 [98.0537796 ]
 [98.07324219]
 [98.09251404]].
[2019-04-07 17:14:33,295] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5907603e-27 6.6625682e-27 4.9233704e-24 1.1520466e-16 5.9308172e-24
 1.0000000e+00 4.9235924e-18 1.7588646e-19], sum to 1.0000
[2019-04-07 17:14:33,304] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8108
[2019-04-07 17:14:33,365] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 79.5, 0.0, 0.0, 24.0, 23.35378677605096, -0.1240116557086316, 0.0, 1.0, 44755.63137844112], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 869400.0000, 
sim time next is 871200.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 24.0, 23.31013220785421, -0.1318254842210017, 0.0, 1.0, 41582.5266378877], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.79, 0.0, 0.0, 0.5, 0.44251101732118414, 0.4560581719263328, 0.0, 1.0, 0.19801203160898903], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23707445], dtype=float32), -1.2634985]. 
=============================================
[2019-04-07 17:14:41,495] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7435942e-33 4.8080722e-32 3.5331839e-30 8.9979005e-22 9.7565920e-29
 1.0000000e+00 7.2093088e-21 2.1915793e-22], sum to 1.0000
[2019-04-07 17:14:41,495] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1413
[2019-04-07 17:14:41,543] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.1, 77.5, 0.0, 0.0, 24.0, 23.77886444113878, 0.1436778260711853, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1053000.0000, 
sim time next is 1054800.0000, 
raw observation next is [13.8, 78.0, 0.0, 0.0, 24.0, 23.81427729562689, 0.1286878364546634, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.844875346260388, 0.78, 0.0, 0.0, 0.5, 0.4845231079689076, 0.5428959454848877, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17028114], dtype=float32), 0.089084595]. 
=============================================
[2019-04-07 17:14:41,618] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-07 17:14:41,619] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:14:41,619] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:14:41,620] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:14:41,621] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:14:41,629] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run80
[2019-04-07 17:14:41,647] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 17:14:41,650] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:14:41,656] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run80
[2019-04-07 17:14:41,684] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run80
[2019-04-07 17:17:02,889] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 17:17:20,169] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 17:17:23,836] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 17:17:24,860] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1580000, evaluation results [1580000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 17:17:30,291] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.9743715e-31 8.3125935e-29 4.1033468e-25 1.3599543e-19 1.8225012e-27
 1.0000000e+00 3.6698863e-20 7.8760898e-22], sum to 1.0000
[2019-04-07 17:17:30,291] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6006
[2019-04-07 17:17:30,363] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 1.0, 82.0, 24.0, 23.73777745171573, 0.07302103074397426, 1.0, 1.0, 15178.217435802313], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3223800.0000, 
sim time next is 3225600.0000, 
raw observation next is [-3.0, 92.0, 43.0, 226.0, 24.0, 23.69729072941846, 0.0743548139826619, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.14333333333333334, 0.24972375690607734, 0.5, 0.47477422745153824, 0.5247849379942207, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0931921], dtype=float32), 3.678004]. 
=============================================
[2019-04-07 17:17:42,049] A3C_AGENT_WORKER-Thread-16 INFO:Local step 100000, global step 1583428: loss 0.3069
[2019-04-07 17:17:42,074] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 100000, global step 1583428: learning rate 0.0000
[2019-04-07 17:17:43,774] A3C_AGENT_WORKER-Thread-15 INFO:Local step 100500, global step 1583768: loss 0.6908
[2019-04-07 17:17:43,774] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 100500, global step 1583768: learning rate 0.0000
[2019-04-07 17:17:44,004] A3C_AGENT_WORKER-Thread-6 INFO:Local step 100000, global step 1583821: loss 0.3082
[2019-04-07 17:17:44,010] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 100000, global step 1583821: learning rate 0.0000
[2019-04-07 17:17:50,065] A3C_AGENT_WORKER-Thread-19 INFO:Local step 100000, global step 1585014: loss 0.2869
[2019-04-07 17:17:50,067] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 100000, global step 1585014: learning rate 0.0000
[2019-04-07 17:17:52,356] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6296945e-26 1.3347457e-25 9.6465540e-24 2.3481464e-15 1.0623568e-22
 1.0000000e+00 5.3125753e-18 1.0193566e-17], sum to 1.0000
[2019-04-07 17:17:52,357] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7829
[2019-04-07 17:17:52,483] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 60.0, 0.0, 0.0, 24.0, 23.3085470329043, -0.06328030754359826, 1.0, 1.0, 33203.004962455285], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2660400.0000, 
sim time next is 2662200.0000, 
raw observation next is [-1.2, 61.5, 0.0, 0.0, 24.0, 23.1563035183478, -0.09960618226151961, 0.0, 1.0, 40115.538338743536], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.615, 0.0, 0.0, 0.5, 0.4296919598623168, 0.46679793924616014, 0.0, 1.0, 0.1910263730416359], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1871786], dtype=float32), 0.88589704]. 
=============================================
[2019-04-07 17:17:59,284] A3C_AGENT_WORKER-Thread-14 INFO:Local step 99500, global step 1586628: loss 0.5399
[2019-04-07 17:17:59,285] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 99500, global step 1586628: learning rate 0.0000
[2019-04-07 17:18:05,637] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.7998241e-31 3.8324140e-29 5.7784570e-27 1.8351498e-19 4.3966198e-28
 1.0000000e+00 3.5624330e-21 1.6132160e-22], sum to 1.0000
[2019-04-07 17:18:05,638] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6756
[2019-04-07 17:18:05,851] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 93.0, 52.5, 91.5, 24.0, 23.5160716166138, -0.0986416915560006, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2880000.0000, 
sim time next is 2881800.0000, 
raw observation next is [1.5, 93.0, 105.0, 156.0, 24.0, 23.58495674114477, -0.05624238790897068, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5041551246537397, 0.93, 0.35, 0.1723756906077348, 0.5, 0.46541306176206404, 0.48125253736367646, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4992963], dtype=float32), -0.97651714]. 
=============================================
[2019-04-07 17:18:05,948] A3C_AGENT_WORKER-Thread-12 INFO:Local step 100500, global step 1587780: loss 0.7560
[2019-04-07 17:18:05,949] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 100500, global step 1587780: learning rate 0.0000
[2019-04-07 17:18:09,317] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:18:09,317] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:18:09,320] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run36
[2019-04-07 17:18:19,422] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.4315333e-27 4.8445615e-25 1.1147253e-23 1.4584638e-16 1.1533974e-22
 1.0000000e+00 6.1360940e-18 2.2676565e-18], sum to 1.0000
[2019-04-07 17:18:19,423] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0702
[2019-04-07 17:18:19,577] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 54.0, 0.0, 0.0, 24.0, 23.72825611869406, 0.08613407806725781, 1.0, 1.0, 98800.78685577621], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2743200.0000, 
sim time next is 2745000.0000, 
raw observation next is [-4.5, 56.5, 0.0, 0.0, 24.0, 23.7302790764661, -0.04654417544028309, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3379501385041552, 0.565, 0.0, 0.0, 0.5, 0.477523256372175, 0.48448527485323895, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.82015896], dtype=float32), -0.4226258]. 
=============================================
[2019-04-07 17:18:19,581] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[94.16681 ]
 [93.596306]
 [94.12047 ]
 [94.91239 ]
 [95.35822 ]], R is [[93.62393188]
 [93.50292206]
 [93.5421524 ]
 [93.60673523]
 [93.67066956]].
[2019-04-07 17:18:23,165] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.6135244e-25 3.2404611e-24 2.8074187e-21 1.0837975e-15 5.9864672e-22
 1.0000000e+00 8.0928117e-16 7.1896044e-17], sum to 1.0000
[2019-04-07 17:18:23,165] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3467
[2019-04-07 17:18:23,172] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 24.0, 22.69587834404956, -0.06510113590565834, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1222200.0000, 
sim time next is 1224000.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 24.0, 22.64287186233774, -0.07356598864994043, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8919667590027703, 0.93, 0.0, 0.0, 0.5, 0.3869059885281449, 0.4754780037833532, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0420115], dtype=float32), 2.7717435]. 
=============================================
[2019-04-07 17:18:23,177] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[90.98398]
 [90.66521]
 [90.41824]
 [90.33393]
 [90.1322 ]], R is [[91.33184814]
 [91.41853333]
 [91.50434875]
 [91.58930969]
 [91.67341614]].
[2019-04-07 17:18:30,336] A3C_AGENT_WORKER-Thread-15 INFO:Local step 101000, global step 1592336: loss 0.0372
[2019-04-07 17:18:30,336] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 101000, global step 1592336: learning rate 0.0000
[2019-04-07 17:18:32,045] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:18:32,045] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:18:32,049] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run36
[2019-04-07 17:18:35,074] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7374133e-26 7.0499784e-25 1.0920314e-22 1.4894160e-15 5.7220537e-22
 1.0000000e+00 9.2463314e-17 5.2252859e-18], sum to 1.0000
[2019-04-07 17:18:35,074] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7400
[2019-04-07 17:18:35,135] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 75.0, 0.0, 0.0, 24.0, 23.06872532880417, -0.1803640923363517, 0.0, 1.0, 45822.50607586724], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 252000.0000, 
sim time next is 253800.0000, 
raw observation next is [-3.9, 78.5, 0.0, 0.0, 24.0, 23.01831547096532, -0.1930354737767235, 0.0, 1.0, 45617.72343514537], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.785, 0.0, 0.0, 0.5, 0.41819295591377664, 0.4356548420744255, 0.0, 1.0, 0.2172272544530732], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23430008], dtype=float32), -1.4879223]. 
=============================================
[2019-04-07 17:18:37,137] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:18:37,137] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:18:37,140] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run36
[2019-04-07 17:18:37,382] A3C_AGENT_WORKER-Thread-16 INFO:Local step 100500, global step 1593513: loss 0.8083
[2019-04-07 17:18:37,382] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 100500, global step 1593513: learning rate 0.0000
[2019-04-07 17:18:38,471] A3C_AGENT_WORKER-Thread-14 INFO:Local step 100000, global step 1593654: loss 0.2641
[2019-04-07 17:18:38,472] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 100000, global step 1593654: learning rate 0.0000
[2019-04-07 17:18:39,516] A3C_AGENT_WORKER-Thread-6 INFO:Local step 100500, global step 1593821: loss 0.8157
[2019-04-07 17:18:39,516] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 100500, global step 1593821: learning rate 0.0000
[2019-04-07 17:18:41,411] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.0634612e-26 2.7749082e-24 2.1763820e-23 1.8628223e-16 3.3116454e-22
 1.0000000e+00 8.7849898e-18 1.0007044e-18], sum to 1.0000
[2019-04-07 17:18:41,412] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3835
[2019-04-07 17:18:41,501] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.3, 29.0, 136.5, 313.0, 24.0, 24.3168776536059, 0.01614130218823658, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2559600.0000, 
sim time next is 2561400.0000, 
raw observation next is [3.3, 29.0, 114.0, 351.0, 24.0, 24.33705010386684, 0.03140175954380906, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.554016620498615, 0.29, 0.38, 0.3878453038674033, 0.5, 0.52808750865557, 0.5104672531812696, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5029492], dtype=float32), 0.12746623]. 
=============================================
[2019-04-07 17:18:44,109] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:18:44,109] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:18:44,114] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run36
[2019-04-07 17:18:45,559] A3C_AGENT_WORKER-Thread-19 INFO:Local step 100500, global step 1594849: loss 0.8795
[2019-04-07 17:18:45,582] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 100500, global step 1594852: learning rate 0.0000
[2019-04-07 17:18:48,345] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:18:48,345] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:18:48,349] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run36
[2019-04-07 17:18:49,511] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:18:49,511] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:18:49,514] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run36
[2019-04-07 17:18:50,893] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:18:50,893] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:18:50,898] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run36
[2019-04-07 17:18:51,736] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:18:51,737] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:18:51,740] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run36
[2019-04-07 17:18:52,620] A3C_AGENT_WORKER-Thread-12 INFO:Local step 101000, global step 1595821: loss 0.0277
[2019-04-07 17:18:52,621] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 101000, global step 1595821: learning rate 0.0000
[2019-04-07 17:18:52,981] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:18:52,981] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:18:52,996] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run36
[2019-04-07 17:18:57,662] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.9919827e-28 8.1676301e-28 4.6391416e-25 2.5662916e-17 3.9819750e-24
 1.0000000e+00 2.2327722e-18 6.0703864e-20], sum to 1.0000
[2019-04-07 17:18:57,662] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2470
[2019-04-07 17:18:57,725] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 24.0, 20.58943728148884, -0.7294043441150452, 0.0, 1.0, 42405.57484272158], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 10800.0000, 
sim time next is 12600.0000, 
raw observation next is [7.45, 94.5, 0.0, 0.0, 24.0, 20.75195243092939, -0.6874634758983964, 0.0, 1.0, 41825.57035195166], 
processed observation next is [0.0, 0.13043478260869565, 0.6689750692520776, 0.945, 0.0, 0.0, 0.5, 0.22932936924411576, 0.27084550803386787, 0.0, 1.0, 0.19916938262834125], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02695316], dtype=float32), 0.09678479]. 
=============================================
[2019-04-07 17:19:01,149] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:19:01,149] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:19:01,152] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run36
[2019-04-07 17:19:06,968] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.0054913e-29 2.8786794e-29 2.3607972e-26 1.0254482e-18 2.8703296e-25
 1.0000000e+00 1.8654719e-19 2.8236705e-20], sum to 1.0000
[2019-04-07 17:19:06,969] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1436
[2019-04-07 17:19:07,074] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 24.0, 23.32833845994326, -0.02782938399350159, 0.0, 1.0, 64154.33764673411], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2934000.0000, 
sim time next is 2935800.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 24.0, 23.35838508606079, -0.02729399716902068, 0.0, 1.0, 46135.11909908726], 
processed observation next is [1.0, 1.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.5, 0.44653209050506576, 0.49090200094365977, 0.0, 1.0, 0.21969104332898692], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13928443], dtype=float32), 0.4087525]. 
=============================================
[2019-04-07 17:19:07,101] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5766825e-30 7.4282588e-29 5.7678225e-26 7.4958445e-19 7.1108267e-26
 1.0000000e+00 4.1011059e-19 6.7816739e-20], sum to 1.0000
[2019-04-07 17:19:07,101] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8135
[2019-04-07 17:19:07,219] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3, 93.0, 0.0, 0.0, 24.0, 23.23991433731215, -0.08717335844315806, 0.0, 1.0, 41076.176410226384], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 88200.0000, 
sim time next is 90000.0000, 
raw observation next is [-0.6, 91.0, 0.0, 0.0, 24.0, 23.35761307847208, -0.09027248197484637, 0.0, 1.0, 41389.25089519037], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.91, 0.0, 0.0, 0.5, 0.44646775653934007, 0.4699091726750512, 0.0, 1.0, 0.19709167092947796], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4841278], dtype=float32), 1.925253]. 
=============================================
[2019-04-07 17:19:07,253] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[106.796555]
 [105.59997 ]
 [104.34319 ]
 [104.51819 ]
 [104.83859 ]], R is [[106.1706543 ]
 [106.10894775]
 [106.04785919]
 [105.98738098]
 [105.92750549]].
[2019-04-07 17:19:08,305] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5926094e-28 4.8551227e-26 3.7122160e-25 2.0407188e-16 8.2006146e-24
 1.0000000e+00 1.2069188e-18 3.9923583e-19], sum to 1.0000
[2019-04-07 17:19:08,306] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4160
[2019-04-07 17:19:08,344] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.0, 71.5, 0.0, 0.0, 24.0, 22.46616368487309, -0.2748444367141281, 0.0, 1.0, 45991.329820044244], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 109800.0000, 
sim time next is 111600.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 24.0, 22.41064020263096, -0.2958585613224826, 0.0, 1.0, 46307.29609582811], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.5, 0.3675533502192468, 0.40138047955917244, 0.0, 1.0, 0.22051093378965766], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.444976], dtype=float32), 0.09227778]. 
=============================================
[2019-04-07 17:19:08,598] A3C_AGENT_WORKER-Thread-18 INFO:Local step 99500, global step 1597562: loss 0.5705
[2019-04-07 17:19:08,598] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 99500, global step 1597562: learning rate 0.0000
[2019-04-07 17:19:12,360] A3C_AGENT_WORKER-Thread-15 INFO:Local step 101500, global step 1598012: loss 2.4077
[2019-04-07 17:19:12,360] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 101500, global step 1598012: learning rate 0.0000
[2019-04-07 17:19:26,095] A3C_AGENT_WORKER-Thread-16 INFO:Local step 101000, global step 1599936: loss 0.0335
[2019-04-07 17:19:26,109] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 101000, global step 1599936: learning rate 0.0000
[2019-04-07 17:19:26,612] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-07 17:19:26,613] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:19:26,614] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:19:26,617] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run81
[2019-04-07 17:19:26,643] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:19:26,643] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:19:26,649] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run81
[2019-04-07 17:19:26,671] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 17:19:26,673] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:19:26,683] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run81
[2019-04-07 17:19:35,978] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.1143121], dtype=float32), 0.15596771]
[2019-04-07 17:19:35,978] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [3.0, 84.5, 0.0, 0.0, 24.0, 23.56816439029648, 0.04399699082012638, 1.0, 1.0, 20098.672982288695]
[2019-04-07 17:19:35,978] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 17:19:35,978] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [1.2004281e-30 2.3422788e-29 2.1873977e-27 2.4286757e-19 9.3972316e-27
 1.0000000e+00 5.2054650e-20 1.6962247e-21], sampled 0.6920670645179426
[2019-04-07 17:21:52,232] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 17:21:52,695] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.1143121], dtype=float32), 0.15596771]
[2019-04-07 17:21:52,695] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [8.686842454, 24.607108065, 0.0, 0.0, 24.0, 23.67082958522411, -0.07283625281346394, 0.0, 1.0, 22618.933814418135]
[2019-04-07 17:21:52,695] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 17:21:52,696] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [2.0081267e-27 3.4195127e-26 2.0620501e-24 2.1613475e-17 4.5166637e-24
 1.0000000e+00 7.5300683e-18 3.3945967e-19], sampled 0.7190024547643696
[2019-04-07 17:22:09,005] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 17:22:13,043] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 17:22:14,067] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1600000, evaluation results [1600000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 17:22:18,364] A3C_AGENT_WORKER-Thread-6 INFO:Local step 101000, global step 1600638: loss 0.0267
[2019-04-07 17:22:18,365] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 101000, global step 1600638: learning rate 0.0000
[2019-04-07 17:22:21,478] A3C_AGENT_WORKER-Thread-12 INFO:Local step 101500, global step 1601117: loss 2.2814
[2019-04-07 17:22:21,480] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 101500, global step 1601117: learning rate 0.0000
[2019-04-07 17:22:21,566] A3C_AGENT_WORKER-Thread-2 INFO:Local step 99500, global step 1601132: loss 0.5348
[2019-04-07 17:22:21,567] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 99500, global step 1601132: learning rate 0.0000
[2019-04-07 17:22:22,005] A3C_AGENT_WORKER-Thread-19 INFO:Local step 101000, global step 1601196: loss 0.0615
[2019-04-07 17:22:22,006] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 101000, global step 1601196: learning rate 0.0000
[2019-04-07 17:22:25,547] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5422423e-26 4.1728948e-26 6.2838648e-24 2.5824378e-16 2.3075837e-22
 1.0000000e+00 2.1466466e-18 2.5010728e-18], sum to 1.0000
[2019-04-07 17:22:25,547] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0480
[2019-04-07 17:22:25,691] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 56.0, 0.0, 0.0, 24.0, 23.32825714758948, -0.1056948526282776, 1.0, 1.0, 29152.695962678685], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 756000.0000, 
sim time next is 757800.0000, 
raw observation next is [-3.9, 54.5, 0.0, 0.0, 24.0, 23.04343304249935, -0.1558095048682821, 1.0, 1.0, 68025.3973584783], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.545, 0.0, 0.0, 0.5, 0.4202860868749457, 0.4480634983772393, 1.0, 1.0, 0.3239304636118014], 
reward next is 0.9618, 
noisyNet noise sample is [array([0.2611711], dtype=float32), 0.28098166]. 
=============================================
[2019-04-07 17:22:26,201] A3C_AGENT_WORKER-Thread-4 INFO:Local step 99500, global step 1601825: loss 0.5240
[2019-04-07 17:22:26,201] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 99500, global step 1601825: learning rate 0.0000
[2019-04-07 17:22:26,637] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0853437e-27 1.3481939e-25 2.2798894e-23 7.8145159e-18 3.2450027e-23
 1.0000000e+00 3.9307517e-17 1.5125685e-18], sum to 1.0000
[2019-04-07 17:22:26,637] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4610
[2019-04-07 17:22:26,693] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 50.0, 0.0, 0.0, 24.0, 23.4634673320809, -0.1261527452305043, 0.0, 1.0, 42423.68746884785], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4937400.0000, 
sim time next is 4939200.0000, 
raw observation next is [-2.0, 50.0, 0.0, 0.0, 24.0, 23.44765651870108, -0.1410312966247059, 0.0, 1.0, 30840.738113392727], 
processed observation next is [1.0, 0.17391304347826086, 0.40720221606648205, 0.5, 0.0, 0.0, 0.5, 0.4539713765584234, 0.45298956779176475, 0.0, 1.0, 0.1468606576828225], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13578473], dtype=float32), -0.3403367]. 
=============================================
[2019-04-07 17:22:27,707] A3C_AGENT_WORKER-Thread-14 INFO:Local step 100500, global step 1602074: loss 0.9879
[2019-04-07 17:22:27,707] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 100500, global step 1602074: learning rate 0.0000
[2019-04-07 17:22:33,258] A3C_AGENT_WORKER-Thread-3 INFO:Local step 99500, global step 1602929: loss 0.4352
[2019-04-07 17:22:33,258] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 99500, global step 1602929: learning rate 0.0000
[2019-04-07 17:22:34,103] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1283843e-26 7.2696318e-26 2.3808138e-24 3.5253984e-17 5.9897701e-25
 1.0000000e+00 3.6582749e-18 9.3536374e-19], sum to 1.0000
[2019-04-07 17:22:34,107] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7878
[2019-04-07 17:22:34,189] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:22:34,189] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:22:34,193] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run37
[2019-04-07 17:22:34,310] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 83.0, 100.0, 73.0, 24.0, 23.07966411690581, -0.1145209109019648, 0.0, 1.0, 41268.961855148664], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 574200.0000, 
sim time next is 576000.0000, 
raw observation next is [-1.2, 83.0, 70.5, 59.0, 24.0, 23.08559074199562, -0.1246402451061642, 0.0, 1.0, 27453.06912209262], 
processed observation next is [0.0, 0.6956521739130435, 0.42936288088642666, 0.83, 0.235, 0.06519337016574586, 0.5, 0.42379922849963503, 0.45845325163127865, 0.0, 1.0, 0.13072890058139344], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35233238], dtype=float32), -1.8673034]. 
=============================================
[2019-04-07 17:22:34,340] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[101.00088 ]
 [101.63316 ]
 [102.401924]
 [102.59522 ]
 [102.97233 ]], R is [[100.67063904]
 [100.6639328 ]
 [100.65729523]
 [100.65072632]
 [100.64421844]].
[2019-04-07 17:22:35,721] A3C_AGENT_WORKER-Thread-18 INFO:Local step 100000, global step 1603308: loss 0.3009
[2019-04-07 17:22:35,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 100000, global step 1603308: learning rate 0.0000
[2019-04-07 17:22:35,784] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.88338307e-27 1.02829615e-25 4.68856757e-24 5.55563219e-17
 7.81130636e-23 1.00000000e+00 1.97809656e-18 2.15385540e-19], sum to 1.0000
[2019-04-07 17:22:35,784] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7779
[2019-04-07 17:22:36,008] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 60.0, 112.0, 100.0, 24.0, 23.01944662035378, -0.196205983220197, 0.0, 1.0, 32368.995669173437], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 649800.0000, 
sim time next is 651600.0000, 
raw observation next is [-2.3, 59.0, 147.0, 96.5, 24.0, 23.0295992025819, -0.1869028324791652, 0.0, 1.0, 33897.445473511965], 
processed observation next is [0.0, 0.5652173913043478, 0.3988919667590028, 0.59, 0.49, 0.10662983425414364, 0.5, 0.4191332668818249, 0.43769905584027824, 0.0, 1.0, 0.16141640701672363], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11854531], dtype=float32), -0.22008978]. 
=============================================
[2019-04-07 17:22:38,693] A3C_AGENT_WORKER-Thread-20 INFO:Local step 99500, global step 1603771: loss 0.5533
[2019-04-07 17:22:38,694] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 99500, global step 1603771: learning rate 0.0000
[2019-04-07 17:22:40,500] A3C_AGENT_WORKER-Thread-5 INFO:Local step 99500, global step 1604033: loss 0.4765
[2019-04-07 17:22:40,501] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 99500, global step 1604033: learning rate 0.0000
[2019-04-07 17:22:42,431] A3C_AGENT_WORKER-Thread-17 INFO:Local step 99500, global step 1604322: loss 0.5333
[2019-04-07 17:22:42,435] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 99500, global step 1604322: learning rate 0.0000
[2019-04-07 17:22:42,477] A3C_AGENT_WORKER-Thread-10 INFO:Local step 99500, global step 1604330: loss 0.4766
[2019-04-07 17:22:42,478] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 99500, global step 1604330: learning rate 0.0000
[2019-04-07 17:22:42,634] A3C_AGENT_WORKER-Thread-13 INFO:Local step 99500, global step 1604356: loss 0.5573
[2019-04-07 17:22:42,635] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 99500, global step 1604356: learning rate 0.0000
[2019-04-07 17:22:45,112] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7379940e-28 6.9992416e-28 1.4743660e-25 5.0098127e-19 2.5676827e-24
 1.0000000e+00 1.0114466e-18 1.0741721e-19], sum to 1.0000
[2019-04-07 17:22:45,112] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2847
[2019-04-07 17:22:45,168] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.15, 67.0, 139.0, 68.0, 24.0, 23.89336529886913, -0.1194065237878235, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 729000.0000, 
sim time next is 730800.0000, 
raw observation next is [-0.6, 66.0, 111.5, 423.5, 24.0, 23.9355021345288, -0.09091840296460053, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.66, 0.37166666666666665, 0.46795580110497237, 0.5, 0.4946251778774, 0.46969386567846644, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6032006], dtype=float32), 1.0252842]. 
=============================================
[2019-04-07 17:22:50,082] A3C_AGENT_WORKER-Thread-11 INFO:Local step 99500, global step 1605612: loss 0.5105
[2019-04-07 17:22:50,089] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 99500, global step 1605612: learning rate 0.0000
[2019-04-07 17:22:53,930] A3C_AGENT_WORKER-Thread-16 INFO:Local step 101500, global step 1606235: loss 2.4095
[2019-04-07 17:22:53,931] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 101500, global step 1606235: learning rate 0.0000
[2019-04-07 17:22:56,992] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8410363e-30 4.4716136e-30 1.2143162e-27 1.2072089e-18 2.2285086e-26
 1.0000000e+00 2.8584718e-19 4.4981500e-22], sum to 1.0000
[2019-04-07 17:22:56,993] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9198
[2019-04-07 17:22:57,102] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.7, 94.0, 0.0, 0.0, 24.0, 23.61627057422769, -0.1297289899590786, 1.0, 1.0, 4447.7170643406], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 927000.0000, 
sim time next is 928800.0000, 
raw observation next is [4.4, 96.0, 0.0, 0.0, 24.0, 22.95705748922729, -0.1341621502275182, 1.0, 1.0, 93294.98051800636], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.96, 0.0, 0.0, 0.5, 0.4130881241022741, 0.4552792832574939, 1.0, 1.0, 0.44426181199050646], 
reward next is 0.8415, 
noisyNet noise sample is [array([3.0076196], dtype=float32), 0.76847756]. 
=============================================
[2019-04-07 17:22:57,785] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:22:57,785] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:22:57,788] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run37
[2019-04-07 17:23:00,441] A3C_AGENT_WORKER-Thread-6 INFO:Local step 101500, global step 1607511: loss 2.3542
[2019-04-07 17:23:00,442] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 101500, global step 1607511: learning rate 0.0000
[2019-04-07 17:23:01,594] A3C_AGENT_WORKER-Thread-2 INFO:Local step 100000, global step 1607774: loss 0.3039
[2019-04-07 17:23:01,595] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 100000, global step 1607774: learning rate 0.0000
[2019-04-07 17:23:03,316] A3C_AGENT_WORKER-Thread-19 INFO:Local step 101500, global step 1608188: loss 2.4672
[2019-04-07 17:23:03,326] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 101500, global step 1608188: learning rate 0.0000
[2019-04-07 17:23:04,134] A3C_AGENT_WORKER-Thread-4 INFO:Local step 100000, global step 1608399: loss 0.3014
[2019-04-07 17:23:04,135] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 100000, global step 1608399: learning rate 0.0000
[2019-04-07 17:23:12,814] A3C_AGENT_WORKER-Thread-14 INFO:Local step 101000, global step 1610179: loss 0.0456
[2019-04-07 17:23:12,815] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 101000, global step 1610179: learning rate 0.0000
[2019-04-07 17:23:13,076] A3C_AGENT_WORKER-Thread-3 INFO:Local step 100000, global step 1610235: loss 0.2949
[2019-04-07 17:23:13,077] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 100000, global step 1610235: learning rate 0.0000
[2019-04-07 17:23:15,638] A3C_AGENT_WORKER-Thread-20 INFO:Local step 100000, global step 1610713: loss 0.2928
[2019-04-07 17:23:15,639] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 100000, global step 1610713: learning rate 0.0000
[2019-04-07 17:23:20,179] A3C_AGENT_WORKER-Thread-5 INFO:Local step 100000, global step 1611510: loss 0.3018
[2019-04-07 17:23:20,180] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 100000, global step 1611510: learning rate 0.0000
[2019-04-07 17:23:21,798] A3C_AGENT_WORKER-Thread-17 INFO:Local step 100000, global step 1611814: loss 0.2824
[2019-04-07 17:23:21,798] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 100000, global step 1611814: learning rate 0.0000
[2019-04-07 17:23:21,821] A3C_AGENT_WORKER-Thread-13 INFO:Local step 100000, global step 1611817: loss 0.2966
[2019-04-07 17:23:21,869] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 100000, global step 1611819: learning rate 0.0000
[2019-04-07 17:23:22,138] A3C_AGENT_WORKER-Thread-10 INFO:Local step 100000, global step 1611859: loss 0.2886
[2019-04-07 17:23:22,139] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 100000, global step 1611859: learning rate 0.0000
[2019-04-07 17:23:28,689] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:23:28,689] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:23:28,702] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run37
[2019-04-07 17:23:29,387] A3C_AGENT_WORKER-Thread-11 INFO:Local step 100000, global step 1613053: loss 0.2956
[2019-04-07 17:23:29,391] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 100000, global step 1613053: learning rate 0.0000
[2019-04-07 17:23:29,825] A3C_AGENT_WORKER-Thread-18 INFO:Local step 100500, global step 1613110: loss 0.8086
[2019-04-07 17:23:29,826] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 100500, global step 1613110: learning rate 0.0000
[2019-04-07 17:23:34,498] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:23:34,499] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:23:34,502] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run37
[2019-04-07 17:23:36,693] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:23:36,693] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:23:36,711] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run37
[2019-04-07 17:23:38,588] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1994235e-26 1.4351176e-25 1.0665346e-23 6.1557908e-17 6.2101343e-24
 1.0000000e+00 1.1571016e-18 1.0535610e-18], sum to 1.0000
[2019-04-07 17:23:38,588] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6585
[2019-04-07 17:23:38,705] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.3, 68.0, 125.0, 0.0, 24.0, 24.18435337837495, 0.01014853597624707, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2125800.0000, 
sim time next is 2127600.0000, 
raw observation next is [-5.0, 68.0, 105.5, 0.0, 24.0, 24.1471951436504, -0.08957231435626657, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.32409972299168976, 0.68, 0.3516666666666667, 0.0, 0.5, 0.5122662619708667, 0.4701425618812445, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8250209], dtype=float32), -2.1016073]. 
=============================================
[2019-04-07 17:23:54,953] A3C_AGENT_WORKER-Thread-14 INFO:Local step 101500, global step 1616163: loss 2.2300
[2019-04-07 17:23:54,954] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 101500, global step 1616163: learning rate 0.0000
[2019-04-07 17:23:59,466] A3C_AGENT_WORKER-Thread-2 INFO:Local step 100500, global step 1616795: loss 0.8684
[2019-04-07 17:23:59,467] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 100500, global step 1616795: learning rate 0.0000
[2019-04-07 17:24:00,122] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7160431e-30 3.7247479e-29 1.3329533e-27 1.4595921e-18 2.3514127e-27
 1.0000000e+00 3.6992360e-19 6.0108606e-22], sum to 1.0000
[2019-04-07 17:24:00,122] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1284
[2019-04-07 17:24:00,171] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.9, 92.0, 0.0, 0.0, 24.0, 23.58047851845539, 0.123432919502212, 0.0, 1.0, 31107.671156484837], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1312200.0000, 
sim time next is 1314000.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 24.0, 23.59543107418499, 0.1349201538127369, 0.0, 1.0, 26593.224101621327], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.5, 0.4662859228487492, 0.5449733846042456, 0.0, 1.0, 0.12663440048391109], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.163622], dtype=float32), 1.0826825]. 
=============================================
[2019-04-07 17:24:00,209] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[107.888306]
 [108.39124 ]
 [108.446846]
 [108.954384]
 [110.39749 ]], R is [[107.54050446]
 [107.46510315]
 [107.39044952]
 [107.31654358]
 [107.24337769]].
[2019-04-07 17:24:01,312] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.8688321e-27 8.2422868e-26 5.9491003e-25 1.0548329e-16 2.5534122e-24
 1.0000000e+00 4.2937291e-18 1.8642777e-18], sum to 1.0000
[2019-04-07 17:24:01,312] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4287
[2019-04-07 17:24:01,487] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 87.0, 73.0, 27.5, 24.0, 24.00259332914349, -0.07529822568867035, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2278800.0000, 
sim time next is 2280600.0000, 
raw observation next is [-7.550000000000001, 82.5, 101.0, 39.0, 24.0, 23.98086364396066, -0.08045110422273201, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.25346260387811637, 0.825, 0.33666666666666667, 0.0430939226519337, 0.5, 0.49840530366338837, 0.4731829652590893, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7971617], dtype=float32), -1.8564377]. 
=============================================
[2019-04-07 17:24:04,322] A3C_AGENT_WORKER-Thread-4 INFO:Local step 100500, global step 1617444: loss 0.9148
[2019-04-07 17:24:04,322] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 100500, global step 1617444: learning rate 0.0000
[2019-04-07 17:24:15,122] A3C_AGENT_WORKER-Thread-3 INFO:Local step 100500, global step 1618996: loss 0.8255
[2019-04-07 17:24:15,123] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 100500, global step 1618996: learning rate 0.0000
[2019-04-07 17:24:15,448] A3C_AGENT_WORKER-Thread-20 INFO:Local step 100500, global step 1619035: loss 0.8893
[2019-04-07 17:24:15,456] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 100500, global step 1619035: learning rate 0.0000
[2019-04-07 17:24:17,597] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6046938e-26 3.2509356e-24 5.5267620e-23 1.5664650e-16 5.6354235e-23
 1.0000000e+00 4.7518753e-17 4.8508313e-17], sum to 1.0000
[2019-04-07 17:24:17,597] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7050
[2019-04-07 17:24:17,727] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.5, 70.0, 117.0, 674.0, 24.0, 24.4295354471131, 0.07622527949898629, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2716200.0000, 
sim time next is 2718000.0000, 
raw observation next is [-9.0, 64.0, 114.5, 727.5, 24.0, 24.48118614871373, 0.09160561117466494, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.21329639889196678, 0.64, 0.38166666666666665, 0.8038674033149171, 0.5, 0.5400988457261441, 0.5305352037248884, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02464847], dtype=float32), 0.60012233]. 
=============================================
[2019-04-07 17:24:17,747] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[91.92041 ]
 [91.60764 ]
 [91.20154 ]
 [90.26894 ]
 [88.873764]], R is [[92.4162674 ]
 [92.49210358]
 [92.56718445]
 [92.64151001]
 [92.71509552]].
[2019-04-07 17:24:18,629] A3C_AGENT_WORKER-Thread-18 INFO:Local step 101000, global step 1619538: loss 0.0157
[2019-04-07 17:24:18,632] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 101000, global step 1619539: learning rate 0.0000
[2019-04-07 17:24:19,744] A3C_AGENT_WORKER-Thread-5 INFO:Local step 100500, global step 1619742: loss 0.9611
[2019-04-07 17:24:19,760] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 100500, global step 1619744: learning rate 0.0000
[2019-04-07 17:24:21,150] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-07 17:24:21,178] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:24:21,178] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:24:21,182] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run82
[2019-04-07 17:24:21,207] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:24:21,210] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:24:21,210] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 17:24:21,210] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:24:21,214] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run82
[2019-04-07 17:24:21,231] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run82
[2019-04-07 17:24:50,714] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.1141552], dtype=float32), 0.15623552]
[2019-04-07 17:24:50,715] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-3.4, 69.0, 0.0, 0.0, 24.0, 23.31591540758114, -0.1751568515601486, 0.0, 1.0, 45795.56980370992]
[2019-04-07 17:24:50,715] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 17:24:50,716] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [9.2054598e-27 1.5358493e-25 8.5209166e-24 7.3965782e-17 2.1385413e-23
 1.0000000e+00 1.6642389e-17 6.8326509e-19], sampled 0.6690101739622164
[2019-04-07 17:26:46,218] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 17:27:02,951] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 17:27:07,055] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 17:27:08,079] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 1620000, evaluation results [1620000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 17:27:08,829] A3C_AGENT_WORKER-Thread-10 INFO:Local step 100500, global step 1620127: loss 0.8856
[2019-04-07 17:27:08,830] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 100500, global step 1620127: learning rate 0.0000
[2019-04-07 17:27:09,007] A3C_AGENT_WORKER-Thread-17 INFO:Local step 100500, global step 1620150: loss 0.9431
[2019-04-07 17:27:09,008] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 100500, global step 1620150: learning rate 0.0000
[2019-04-07 17:27:10,436] A3C_AGENT_WORKER-Thread-13 INFO:Local step 100500, global step 1620372: loss 0.8932
[2019-04-07 17:27:10,437] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 100500, global step 1620372: learning rate 0.0000
[2019-04-07 17:27:12,395] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.6920884e-28 6.2732692e-27 1.6848070e-25 1.2052395e-18 5.8586810e-24
 1.0000000e+00 1.5306835e-19 1.5403308e-19], sum to 1.0000
[2019-04-07 17:27:12,399] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4925
[2019-04-07 17:27:12,439] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 24.0, 22.73358350115716, -0.2368717458799012, 0.0, 1.0, 47610.89691876027], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1825200.0000, 
sim time next is 1827000.0000, 
raw observation next is [-6.2, 85.0, 0.0, 0.0, 24.0, 22.73073060434142, -0.2395512658257105, 0.0, 1.0, 47723.53268337075], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.85, 0.0, 0.0, 0.5, 0.3942275503617851, 0.42014957805809655, 0.0, 1.0, 0.2272549175398607], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3150817], dtype=float32), -0.9693651]. 
=============================================
[2019-04-07 17:27:12,443] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[95.16079 ]
 [95.37094 ]
 [95.681816]
 [95.865295]
 [96.164246]], R is [[95.07034302]
 [95.11964417]
 [95.1684494 ]
 [95.21676636]
 [95.26460266]].
[2019-04-07 17:27:13,413] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.2838138e-26 1.3638733e-24 8.1405539e-24 4.1036788e-16 3.5256163e-23
 1.0000000e+00 8.1742835e-17 1.9714673e-18], sum to 1.0000
[2019-04-07 17:27:13,413] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6985
[2019-04-07 17:27:13,480] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.5, 70.0, 3.0, 121.0, 24.0, 23.03411801029851, -0.1216103853148929, 0.0, 1.0, 42655.62828472668], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3569400.0000, 
sim time next is 3571200.0000, 
raw observation next is [-7.0, 70.0, 45.5, 273.0, 24.0, 22.9866805940757, -0.1067377949563188, 0.0, 1.0, 42619.90208597912], 
processed observation next is [0.0, 0.34782608695652173, 0.2686980609418283, 0.7, 0.15166666666666667, 0.30165745856353593, 0.5, 0.415556716172975, 0.4644207350145604, 0.0, 1.0, 0.20295191469513868], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2733693], dtype=float32), -1.2745696]. 
=============================================
[2019-04-07 17:27:17,278] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:27:17,279] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:27:17,300] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run37
[2019-04-07 17:27:18,299] A3C_AGENT_WORKER-Thread-11 INFO:Local step 100500, global step 1621596: loss 0.9661
[2019-04-07 17:27:18,300] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 100500, global step 1621596: learning rate 0.0000
[2019-04-07 17:27:29,418] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3785013e-27 2.3719467e-25 6.7983930e-24 2.3365115e-16 1.0555370e-24
 1.0000000e+00 9.1405906e-18 6.5011593e-19], sum to 1.0000
[2019-04-07 17:27:29,419] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8428
[2019-04-07 17:27:29,493] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 24.0, 23.70870805968569, -0.005102462866109168, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2842200.0000, 
sim time next is 2844000.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 24.0, 23.56242865620562, -0.02502179021395137, 0.0, 1.0, 37667.73951378742], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.44, 0.0, 0.0, 0.5, 0.4635357213504682, 0.49165940326201624, 0.0, 1.0, 0.17937018816089248], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7395627], dtype=float32), -0.3105187]. 
=============================================
[2019-04-07 17:27:29,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[95.29458 ]
 [95.893326]
 [96.064125]
 [95.258125]
 [95.60227 ]], R is [[95.2824173 ]
 [95.32959747]
 [95.37630463]
 [95.04077148]
 [95.05562592]].
[2019-04-07 17:27:34,347] A3C_AGENT_WORKER-Thread-2 INFO:Local step 101000, global step 1623954: loss 0.0442
[2019-04-07 17:27:34,347] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 101000, global step 1623954: learning rate 0.0000
[2019-04-07 17:27:35,542] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5213583e-29 1.1714858e-28 6.6012725e-26 6.8643930e-19 6.6452637e-25
 1.0000000e+00 5.1119536e-20 2.3087659e-20], sum to 1.0000
[2019-04-07 17:27:35,543] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9779
[2019-04-07 17:27:35,642] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 85.0, 0.0, 0.0, 24.0, 23.56843634614177, 0.05490156264127995, 0.0, 1.0, 38685.17785069586], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2926800.0000, 
sim time next is 2928600.0000, 
raw observation next is [-1.0, 81.5, 0.0, 0.0, 24.0, 23.58481101586782, 0.03211344344923542, 0.0, 1.0, 6246.520663731687], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.815, 0.0, 0.0, 0.5, 0.4654009179889851, 0.5107044811497451, 0.0, 1.0, 0.029745336493960415], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5018313], dtype=float32), 0.6671963]. 
=============================================
[2019-04-07 17:27:39,883] A3C_AGENT_WORKER-Thread-4 INFO:Local step 101000, global step 1624807: loss 0.0326
[2019-04-07 17:27:39,885] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 101000, global step 1624807: learning rate 0.0000
[2019-04-07 17:27:45,941] A3C_AGENT_WORKER-Thread-18 INFO:Local step 101500, global step 1626081: loss 2.3648
[2019-04-07 17:27:45,942] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 101500, global step 1626081: learning rate 0.0000
[2019-04-07 17:27:50,655] A3C_AGENT_WORKER-Thread-20 INFO:Local step 101000, global step 1627079: loss 0.0597
[2019-04-07 17:27:50,655] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 101000, global step 1627079: learning rate 0.0000
[2019-04-07 17:27:50,855] A3C_AGENT_WORKER-Thread-3 INFO:Local step 101000, global step 1627120: loss 0.0487
[2019-04-07 17:27:50,855] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 101000, global step 1627120: learning rate 0.0000
[2019-04-07 17:27:54,215] A3C_AGENT_WORKER-Thread-5 INFO:Local step 101000, global step 1627720: loss 0.0254
[2019-04-07 17:27:54,216] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 101000, global step 1627720: learning rate 0.0000
[2019-04-07 17:27:55,914] A3C_AGENT_WORKER-Thread-17 INFO:Local step 101000, global step 1628030: loss 0.0296
[2019-04-07 17:27:55,918] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 101000, global step 1628030: learning rate 0.0000
[2019-04-07 17:27:56,239] A3C_AGENT_WORKER-Thread-10 INFO:Local step 101000, global step 1628098: loss 0.0324
[2019-04-07 17:27:56,241] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 101000, global step 1628098: learning rate 0.0000
[2019-04-07 17:27:57,052] A3C_AGENT_WORKER-Thread-13 INFO:Local step 101000, global step 1628263: loss 0.0343
[2019-04-07 17:27:57,052] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 101000, global step 1628263: learning rate 0.0000
[2019-04-07 17:28:03,589] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1260915e-27 3.5209221e-25 6.4958410e-23 5.0652988e-16 1.2725464e-23
 1.0000000e+00 7.0409766e-17 1.3155567e-18], sum to 1.0000
[2019-04-07 17:28:03,589] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2427
[2019-04-07 17:28:03,646] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 24.0, 23.45472528677302, 0.05896623466221153, 0.0, 1.0, 110148.38549514099], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3528000.0000, 
sim time next is 3529800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 24.0, 23.33898183761177, 0.08800326827704698, 0.0, 1.0, 143412.80115189592], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.5, 0.44491515313431407, 0.5293344227590157, 0.0, 1.0, 0.6829181007233139], 
reward next is 0.6028, 
noisyNet noise sample is [array([-1.4768947], dtype=float32), -0.22048141]. 
=============================================
[2019-04-07 17:28:05,097] A3C_AGENT_WORKER-Thread-11 INFO:Local step 101000, global step 1629804: loss 0.0310
[2019-04-07 17:28:05,110] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 101000, global step 1629804: learning rate 0.0000
[2019-04-07 17:28:11,272] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.8748101e-30 2.3416099e-28 2.3032774e-27 3.6447478e-19 1.4078914e-25
 1.0000000e+00 9.5036138e-20 4.4032819e-21], sum to 1.0000
[2019-04-07 17:28:11,272] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1140
[2019-04-07 17:28:11,326] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 117.0, 825.5, 24.0, 25.0060093582718, 0.2454806368376509, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3758400.0000, 
sim time next is 3760200.0000, 
raw observation next is [-1.5, 62.5, 119.0, 829.0, 24.0, 25.06906974315358, 0.2516198115780938, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4210526315789474, 0.625, 0.39666666666666667, 0.9160220994475138, 0.5, 0.5890891452627983, 0.5838732705260313, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5210744], dtype=float32), -1.7854321]. 
=============================================
[2019-04-07 17:28:15,132] A3C_AGENT_WORKER-Thread-2 INFO:Local step 101500, global step 1631583: loss 2.1870
[2019-04-07 17:28:15,137] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 101500, global step 1631583: learning rate 0.0000
[2019-04-07 17:28:17,728] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3046976e-27 8.2373521e-26 7.4037223e-24 1.8031071e-16 1.9432969e-24
 1.0000000e+00 7.8798668e-17 1.3524741e-18], sum to 1.0000
[2019-04-07 17:28:17,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8340
[2019-04-07 17:28:17,803] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.0, 26.0, 95.0, 533.0, 24.0, 23.80082042781959, 0.02099349534565122, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3661200.0000, 
sim time next is 3663000.0000, 
raw observation next is [11.0, 27.0, 101.0, 663.0, 24.0, 23.92217369051003, 0.04054986319247422, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.27, 0.33666666666666667, 0.7325966850828729, 0.5, 0.49351447420916905, 0.5135166210641581, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3570035], dtype=float32), 0.47125936]. 
=============================================
[2019-04-07 17:28:17,808] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[98.77368 ]
 [98.194916]
 [97.50231 ]
 [96.80524 ]
 [96.19238 ]], R is [[99.23011017]
 [99.23780823]
 [99.24542999]
 [99.25297546]
 [99.26044464]].
[2019-04-07 17:28:19,490] A3C_AGENT_WORKER-Thread-4 INFO:Local step 101500, global step 1632371: loss 2.1619
[2019-04-07 17:28:19,491] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 101500, global step 1632371: learning rate 0.0000
[2019-04-07 17:28:20,276] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:28:20,276] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:28:20,290] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run37
[2019-04-07 17:28:31,811] A3C_AGENT_WORKER-Thread-20 INFO:Local step 101500, global step 1634375: loss 2.1220
[2019-04-07 17:28:31,811] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 101500, global step 1634375: learning rate 0.0000
[2019-04-07 17:28:32,166] A3C_AGENT_WORKER-Thread-3 INFO:Local step 101500, global step 1634439: loss 2.1856
[2019-04-07 17:28:32,166] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 101500, global step 1634439: learning rate 0.0000
[2019-04-07 17:28:34,400] A3C_AGENT_WORKER-Thread-5 INFO:Local step 101500, global step 1634881: loss 2.1382
[2019-04-07 17:28:34,401] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 101500, global step 1634881: learning rate 0.0000
[2019-04-07 17:28:35,918] A3C_AGENT_WORKER-Thread-17 INFO:Local step 101500, global step 1635222: loss 2.2913
[2019-04-07 17:28:35,920] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 101500, global step 1635223: learning rate 0.0000
[2019-04-07 17:28:36,766] A3C_AGENT_WORKER-Thread-10 INFO:Local step 101500, global step 1635391: loss 2.2533
[2019-04-07 17:28:36,768] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 101500, global step 1635391: learning rate 0.0000
[2019-04-07 17:28:38,297] A3C_AGENT_WORKER-Thread-13 INFO:Local step 101500, global step 1635702: loss 2.1076
[2019-04-07 17:28:38,299] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 101500, global step 1635702: learning rate 0.0000
[2019-04-07 17:28:38,389] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2767015e-29 3.0040499e-28 5.2063230e-26 8.6229223e-20 1.3169128e-25
 1.0000000e+00 1.5595582e-19 2.8159257e-20], sum to 1.0000
[2019-04-07 17:28:38,389] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8147
[2019-04-07 17:28:38,433] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.9, 70.0, 0.0, 0.0, 24.0, 23.62430783908896, -0.03436933952065236, 0.0, 1.0, 22520.66090404839], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4334400.0000, 
sim time next is 4336200.0000, 
raw observation next is [3.75, 69.5, 0.0, 0.0, 24.0, 23.74859224320679, -0.05047896283832857, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5664819944598338, 0.695, 0.0, 0.0, 0.5, 0.47904935360056583, 0.48317367905389047, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.775202], dtype=float32), -1.3930898]. 
=============================================
[2019-04-07 17:28:38,905] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9619016e-27 1.4746867e-25 3.6168992e-24 1.2380696e-17 2.6919058e-23
 1.0000000e+00 1.4838341e-17 1.9754712e-19], sum to 1.0000
[2019-04-07 17:28:38,908] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2348
[2019-04-07 17:28:38,985] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 60.0, 0.0, 0.0, 24.0, 23.40408789393516, -0.09439776176247454, 0.0, 1.0, 46372.28443412539], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4847400.0000, 
sim time next is 4849200.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 24.0, 23.40165534541459, -0.1003601659733187, 0.0, 1.0, 40850.409110996225], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.6, 0.0, 0.0, 0.5, 0.45013794545121577, 0.46654661134222714, 0.0, 1.0, 0.1945257576714106], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6680446], dtype=float32), -1.7360946]. 
=============================================
[2019-04-07 17:28:39,613] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4994210e-27 1.4311962e-27 3.9849687e-24 1.3152770e-16 2.3247676e-23
 1.0000000e+00 1.3187926e-18 1.9000602e-19], sum to 1.0000
[2019-04-07 17:28:39,613] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0764
[2019-04-07 17:28:39,734] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.0, 77.0, 95.0, 505.5, 24.0, 24.39042097126757, 0.1284478171720626, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3315600.0000, 
sim time next is 3317400.0000, 
raw observation next is [-8.5, 73.5, 104.0, 615.0, 24.0, 24.57752834123074, 0.1563649328808045, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.22714681440443216, 0.735, 0.3466666666666667, 0.6795580110497238, 0.5, 0.5481273617692283, 0.5521216442936016, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28197625], dtype=float32), 0.80991775]. 
=============================================
[2019-04-07 17:28:43,346] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8678686e-29 1.2164343e-28 2.8903142e-26 4.3334512e-18 3.5009664e-25
 1.0000000e+00 3.4649841e-19 1.1864849e-20], sum to 1.0000
[2019-04-07 17:28:43,346] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8327
[2019-04-07 17:28:43,397] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 81.5, 0.0, 0.0, 24.0, 23.58481101586782, 0.03211344344923542, 0.0, 1.0, 6246.520663731687], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2928600.0000, 
sim time next is 2930400.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 24.0, 23.43775719653787, 0.02968138622308402, 0.0, 1.0, 66257.44478138116], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.78, 0.0, 0.0, 0.5, 0.4531464330448225, 0.5098937954076946, 0.0, 1.0, 0.31551164181610075], 
reward next is 0.9702, 
noisyNet noise sample is [array([-1.0066847], dtype=float32), 0.48121065]. 
=============================================
[2019-04-07 17:28:45,737] A3C_AGENT_WORKER-Thread-11 INFO:Local step 101500, global step 1637142: loss 2.1877
[2019-04-07 17:28:45,737] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 101500, global step 1637142: learning rate 0.0000
[2019-04-07 17:28:50,238] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:28:50,238] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:28:50,275] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run37
[2019-04-07 17:28:54,013] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:28:54,013] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:28:54,016] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run37
[2019-04-07 17:29:01,894] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-07 17:29:01,904] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:29:01,904] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:29:01,909] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:29:01,909] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:29:01,913] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 17:29:01,913] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:29:01,914] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run83
[2019-04-07 17:29:01,942] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run83
[2019-04-07 17:29:01,969] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run83
[2019-04-07 17:31:25,382] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 17:31:41,739] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 17:31:44,602] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 17:31:45,626] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1640000, evaluation results [1640000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 17:31:50,106] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:31:50,106] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:31:50,110] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run37
[2019-04-07 17:31:50,773] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:31:50,773] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:31:50,777] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run37
[2019-04-07 17:31:51,085] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:31:51,085] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:31:51,089] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run37
[2019-04-07 17:31:52,643] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.5280365e-29 1.1168117e-29 5.0704540e-26 8.2299373e-19 5.9977008e-26
 1.0000000e+00 3.1919575e-20 9.2218966e-22], sum to 1.0000
[2019-04-07 17:31:52,643] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0644
[2019-04-07 17:31:52,700] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 24.0, 23.69387094667335, 0.1555247082687069, 0.0, 1.0, 92552.72687123304], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3445200.0000, 
sim time next is 3447000.0000, 
raw observation next is [1.0, 82.5, 0.0, 0.0, 24.0, 24.06286686593406, 0.167943275060719, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.825, 0.0, 0.0, 0.5, 0.5052389054945049, 0.5559810916869063, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49714354], dtype=float32), -0.7056657]. 
=============================================
[2019-04-07 17:31:52,704] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[103.87327]
 [102.77943]
 [101.43757]
 [102.1809 ]
 [101.11565]], R is [[103.74655914]
 [103.55407715]
 [103.09698486]
 [103.06601715]
 [102.85891724]].
[2019-04-07 17:31:53,569] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:31:53,569] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:31:53,572] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run37
[2019-04-07 17:31:55,437] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:31:55,437] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:31:55,441] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run37
[2019-04-07 17:31:55,637] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:31:55,637] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:31:55,640] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run37
[2019-04-07 17:32:04,151] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:32:04,152] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:32:04,156] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run37
[2019-04-07 17:32:04,569] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1174074e-23 3.7946569e-25 1.3889402e-22 4.3292259e-16 2.0865114e-21
 1.0000000e+00 2.2521856e-15 1.4286187e-16], sum to 1.0000
[2019-04-07 17:32:04,572] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7831
[2019-04-07 17:32:04,660] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 70.0, 0.0, 0.0, 24.0, 22.33101054595598, -0.3665955100983316, 0.0, 1.0, 47165.2611695505], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 273600.0000, 
sim time next is 275400.0000, 
raw observation next is [-10.05, 68.5, 0.0, 0.0, 24.0, 22.14663982007652, -0.3969847483640376, 0.0, 1.0, 47683.8079518307], 
processed observation next is [1.0, 0.17391304347826086, 0.18421052631578946, 0.685, 0.0, 0.0, 0.5, 0.3455533183397099, 0.3676717505453208, 0.0, 1.0, 0.22706575215157476], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.53006214], dtype=float32), -0.41300187]. 
=============================================
[2019-04-07 17:32:29,300] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.6186561e-26 8.6473047e-26 6.3548609e-23 2.6938567e-16 8.7030467e-23
 1.0000000e+00 5.5351763e-17 8.1891315e-19], sum to 1.0000
[2019-04-07 17:32:29,300] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9896
[2019-04-07 17:32:29,420] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 65.0, 139.0, 0.0, 24.0, 23.72032921916288, -0.09886490835403106, 1.0, 1.0, 7425.66659559921], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 219600.0000, 
sim time next is 221400.0000, 
raw observation next is [-3.95, 63.5, 149.0, 0.0, 24.0, 23.86359756594529, -0.08193775969088936, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3531855955678671, 0.635, 0.49666666666666665, 0.0, 0.5, 0.48863313049544094, 0.47268741343637016, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15853126], dtype=float32), -0.82708144]. 
=============================================
[2019-04-07 17:32:31,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.8968406e-27 7.1490951e-26 4.9502609e-25 2.1961869e-18 1.2962456e-22
 1.0000000e+00 5.3827083e-18 6.7657346e-19], sum to 1.0000
[2019-04-07 17:32:31,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6065
[2019-04-07 17:32:31,975] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 48.0, 87.0, 674.0, 24.0, 24.59522857675741, 0.2161679467134738, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3339000.0000, 
sim time next is 3340800.0000, 
raw observation next is [-2.0, 46.0, 73.5, 587.5, 24.0, 24.86313340655321, 0.1584910080073035, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.46, 0.245, 0.649171270718232, 0.5, 0.5719277838794342, 0.5528303360024345, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.77060086], dtype=float32), -0.7381276]. 
=============================================
[2019-04-07 17:32:33,743] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.8997571e-26 3.9188050e-24 1.2924395e-21 4.1345391e-16 4.1029930e-22
 1.0000000e+00 1.4912174e-16 3.1865954e-18], sum to 1.0000
[2019-04-07 17:32:33,743] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5817
[2019-04-07 17:32:33,796] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.05, 68.5, 0.0, 0.0, 24.0, 22.14663982007652, -0.3969847483640376, 0.0, 1.0, 47683.8079518307], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 275400.0000, 
sim time next is 277200.0000, 
raw observation next is [-10.6, 67.0, 0.0, 0.0, 24.0, 21.98089984191056, -0.4313760809245187, 0.0, 1.0, 48165.01522681318], 
processed observation next is [1.0, 0.21739130434782608, 0.1689750692520776, 0.67, 0.0, 0.0, 0.5, 0.33174165349254664, 0.35620797302516044, 0.0, 1.0, 0.22935721536577705], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4486484], dtype=float32), 1.3704282]. 
=============================================
[2019-04-07 17:32:37,136] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0186424e-30 3.2814157e-29 3.4266666e-27 5.2551453e-20 4.7479912e-27
 1.0000000e+00 1.2221037e-21 4.8054927e-22], sum to 1.0000
[2019-04-07 17:32:37,136] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9569
[2019-04-07 17:32:37,199] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.5, 74.0, 0.0, 0.0, 24.0, 23.75604281505299, 0.09950483280539431, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4429800.0000, 
sim time next is 4431600.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 24.0, 23.84651606009406, 0.08152096451433599, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.5, 0.4872096716745051, 0.527173654838112, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5671845], dtype=float32), -0.6990091]. 
=============================================
[2019-04-07 17:32:42,473] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:32:42,473] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:32:42,476] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run38
[2019-04-07 17:32:43,422] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.8913818e-29 3.0243458e-27 1.8340804e-25 2.7137721e-17 1.4441687e-24
 1.0000000e+00 4.5996859e-18 3.2253850e-20], sum to 1.0000
[2019-04-07 17:32:43,426] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9952
[2019-04-07 17:32:43,498] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 24.0, 23.65005913473699, 0.02469880273102871, 0.0, 1.0, 6248.5942686292365], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4579200.0000, 
sim time next is 4581000.0000, 
raw observation next is [0.7, 62.0, 0.0, 0.0, 24.0, 23.57799797983006, 0.007263054014500087, 0.0, 1.0, 25212.94722258326], 
processed observation next is [1.0, 0.0, 0.4819944598337951, 0.62, 0.0, 0.0, 0.5, 0.4648331649858382, 0.5024210180048333, 0.0, 1.0, 0.12006165344087268], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4742844], dtype=float32), 0.83284503]. 
=============================================
[2019-04-07 17:32:43,569] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[98.27612]
 [98.52806]
 [98.45848]
 [97.79797]
 [98.26657]], R is [[98.37698364]
 [98.39321136]
 [98.40927887]
 [98.22795868]
 [98.24568176]].
[2019-04-07 17:33:07,819] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:33:07,819] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:33:07,822] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run38
[2019-04-07 17:33:19,034] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2554090e-25 5.8230285e-25 2.8957499e-22 1.3684135e-15 3.9351073e-22
 1.0000000e+00 2.0401963e-17 2.7975662e-18], sum to 1.0000
[2019-04-07 17:33:19,040] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5817
[2019-04-07 17:33:19,052] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 24.0, 23.28771891640734, 0.06388176650322813, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1195200.0000, 
sim time next is 1197000.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 24.0, 23.28013573228503, 0.05830213140542146, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.9529085872576178, 0.67, 0.0, 0.0, 0.5, 0.4400113110237524, 0.5194340438018071, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39950985], dtype=float32), -1.1482949]. 
=============================================
[2019-04-07 17:33:19,106] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[89.306   ]
 [89.445496]
 [89.583206]
 [89.60417 ]
 [89.643585]], R is [[89.28883362]
 [89.39594269]
 [89.50198364]
 [89.60696411]
 [89.71089172]].
[2019-04-07 17:33:25,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9574695e-29 2.4062648e-28 3.9346471e-27 4.5131766e-20 1.9142265e-25
 1.0000000e+00 4.6437061e-19 1.0471971e-20], sum to 1.0000
[2019-04-07 17:33:25,192] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9658
[2019-04-07 17:33:25,315] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8, 94.5, 18.0, 0.0, 24.0, 23.97230966933513, 0.05091167520611905, 1.0, 1.0, 6226.803890076847], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1355400.0000, 
sim time next is 1357200.0000, 
raw observation next is [0.5, 96.0, 9.0, 0.0, 24.0, 23.34962499459697, -0.009087880387072533, 1.0, 1.0, 60937.645936818655], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.03, 0.0, 0.5, 0.4458020828830807, 0.49697070653764247, 1.0, 1.0, 0.2901792663658031], 
reward next is 0.9955, 
noisyNet noise sample is [array([-1.823516], dtype=float32), 0.7530967]. 
=============================================
[2019-04-07 17:33:39,997] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:33:39,997] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:33:40,001] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run38
[2019-04-07 17:33:49,009] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:33:49,009] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:33:49,033] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run38
[2019-04-07 17:33:49,101] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:33:49,101] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:33:49,104] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run38
[2019-04-07 17:33:56,496] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9440394e-26 2.6145901e-26 1.2486665e-23 3.7285378e-17 2.0294859e-23
 1.0000000e+00 2.1822425e-16 8.2967642e-19], sum to 1.0000
[2019-04-07 17:33:56,496] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3675
[2019-04-07 17:33:56,599] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 24.0, 23.31291463161064, -0.1624552121360958, 0.0, 1.0, 46278.72306769151], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1890000.0000, 
sim time next is 1891800.0000, 
raw observation next is [-5.9, 79.0, 0.0, 0.0, 24.0, 23.24600573192475, -0.1765061581482829, 0.0, 1.0, 46197.105656792184], 
processed observation next is [0.0, 0.9130434782608695, 0.2991689750692521, 0.79, 0.0, 0.0, 0.5, 0.43716714432706255, 0.4411646139505723, 0.0, 1.0, 0.2199862174132961], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.394838], dtype=float32), -0.80778867]. 
=============================================
[2019-04-07 17:33:57,616] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-07 17:33:57,624] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:33:57,624] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:33:57,628] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run84
[2019-04-07 17:33:57,655] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:33:57,656] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:33:57,661] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run84
[2019-04-07 17:33:57,656] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 17:33:57,688] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:33:57,693] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run84
[2019-04-07 17:36:22,522] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 17:36:41,680] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 17:36:45,327] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 17:36:46,350] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1660000, evaluation results [1660000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 17:36:55,470] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9927872e-28 2.7841724e-26 4.8471494e-25 6.0364018e-18 1.3722022e-24
 1.0000000e+00 3.1794436e-18 3.4890562e-20], sum to 1.0000
[2019-04-07 17:36:55,470] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2131
[2019-04-07 17:36:55,554] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 86.0, 0.0, 0.0, 24.0, 23.13533139492483, -0.1455271141028645, 0.0, 1.0, 43950.79951711506], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2080800.0000, 
sim time next is 2082600.0000, 
raw observation next is [-4.75, 86.0, 0.0, 0.0, 24.0, 23.20956209426955, -0.1476207986671536, 0.0, 1.0, 43920.77737012369], 
processed observation next is [1.0, 0.08695652173913043, 0.3310249307479225, 0.86, 0.0, 0.0, 0.5, 0.43413017452246255, 0.4507930671109488, 0.0, 1.0, 0.2091465589053509], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15802984], dtype=float32), 0.7184724]. 
=============================================
[2019-04-07 17:36:58,608] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.7574878e-27 2.4055433e-25 6.7374898e-24 5.8152604e-17 1.1233782e-23
 1.0000000e+00 4.8111703e-18 7.5045997e-19], sum to 1.0000
[2019-04-07 17:36:58,608] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8291
[2019-04-07 17:36:58,858] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 79.0, 36.5, 18.5, 24.0, 23.49414900012665, -0.1046845016572593, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2102400.0000, 
sim time next is 2104200.0000, 
raw observation next is [-7.55, 80.5, 72.0, 37.0, 24.0, 23.57344261708887, -0.08889460230991397, 1.0, 1.0, 6245.9931522377565], 
processed observation next is [1.0, 0.34782608695652173, 0.25346260387811637, 0.805, 0.24, 0.04088397790055249, 0.5, 0.4644535514240724, 0.47036846589669534, 1.0, 1.0, 0.029742824534465508], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3969526], dtype=float32), -1.584213]. 
=============================================
[2019-04-07 17:37:20,969] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.5931194e-32 8.0416385e-30 2.6477800e-28 6.4476295e-22 2.6639090e-28
 1.0000000e+00 2.9869447e-20 1.9694638e-22], sum to 1.0000
[2019-04-07 17:37:20,969] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7296
[2019-04-07 17:37:21,031] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.7, 92.0, 0.0, 0.0, 24.0, 23.70711461246263, 0.1451568991538327, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1306800.0000, 
sim time next is 1308600.0000, 
raw observation next is [2.45, 92.0, 0.0, 0.0, 24.0, 23.61402892249805, 0.1221117285544829, 0.0, 1.0, 26173.539462664685], 
processed observation next is [1.0, 0.13043478260869565, 0.5304709141274239, 0.92, 0.0, 0.0, 0.5, 0.4678357435415042, 0.5407039095181609, 0.0, 1.0, 0.12463590220316517], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11115715], dtype=float32), 1.5121312]. 
=============================================
[2019-04-07 17:37:31,548] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:37:31,548] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:37:31,560] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run38
[2019-04-07 17:37:34,769] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.2492340e-27 2.9531376e-27 7.9509772e-24 7.1846969e-17 2.3903064e-23
 1.0000000e+00 1.3389533e-17 3.5134998e-19], sum to 1.0000
[2019-04-07 17:37:34,791] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2274
[2019-04-07 17:37:34,845] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 24.0, 23.38428002335728, -0.1053874008920966, 0.0, 1.0, 49394.21471584166], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3735000.0000, 
sim time next is 3736800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 24.0, 23.47193198632659, -0.1112596056627601, 0.0, 1.0, 21889.538816231343], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.65, 0.0, 0.0, 0.5, 0.45599433219388263, 0.46291346477907996, 0.0, 1.0, 0.10423589912491116], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9451771], dtype=float32), 0.5783938]. 
=============================================
[2019-04-07 17:37:54,568] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.5373946e-26 5.0320727e-25 4.4088552e-24 1.7618749e-17 5.9169225e-23
 1.0000000e+00 3.9033770e-17 5.5086713e-19], sum to 1.0000
[2019-04-07 17:37:54,568] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3675
[2019-04-07 17:37:54,704] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 62.5, 0.0, 0.0, 24.0, 23.11739718236412, -0.06867312502068515, 0.0, 1.0, 130383.42448703363], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3007800.0000, 
sim time next is 3009600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 24.0, 23.62817576806341, -0.03663619424266645, 0.0, 1.0, 28001.190747657238], 
processed observation next is [0.0, 0.8695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.5, 0.4690146473386176, 0.4877879352524445, 0.0, 1.0, 0.13333900356027256], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.45340857], dtype=float32), 0.76101047]. 
=============================================
[2019-04-07 17:38:04,513] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4119153e-29 7.1051253e-28 2.2379307e-26 7.4089678e-19 6.4209364e-26
 1.0000000e+00 3.6632446e-19 1.3059552e-20], sum to 1.0000
[2019-04-07 17:38:04,521] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0805
[2019-04-07 17:38:04,581] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 24.0, 23.44523132779227, -0.09859971771791347, 0.0, 1.0, 56362.06002467137], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3108600.0000, 
sim time next is 3110400.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 24.0, 23.45402389611071, -0.09457859656799106, 0.0, 1.0, 39839.38748424973], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.5, 0.45450199134255903, 0.468473801144003, 0.0, 1.0, 0.18971136897261778], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09018867], dtype=float32), 1.043617]. 
=============================================
[2019-04-07 17:38:18,422] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7143503e-27 4.9404271e-27 3.4497688e-24 1.1784711e-17 2.1160468e-23
 1.0000000e+00 8.2308838e-18 2.0402409e-19], sum to 1.0000
[2019-04-07 17:38:18,424] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5874
[2019-04-07 17:38:18,550] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.8, 90.0, 0.0, 0.0, 24.0, 23.10844893952315, -0.212792559397309, 1.0, 1.0, 61079.6895153583], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 498600.0000, 
sim time next is 500400.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 24.0, 23.12329228578083, -0.2412788551868757, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.5, 0.42694102381506926, 0.4195737149377081, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.77200866], dtype=float32), -0.50681525]. 
=============================================
[2019-04-07 17:38:25,906] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.2170425e-27 1.4235258e-25 2.9043760e-24 1.9408008e-16 3.4751566e-23
 1.0000000e+00 9.1477818e-17 4.1580818e-20], sum to 1.0000
[2019-04-07 17:38:25,906] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8354
[2019-04-07 17:38:26,051] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 52.5, 118.0, 823.0, 24.0, 23.43134786387991, 0.03665044513661635, 0.0, 1.0, 18702.24721808585], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3587400.0000, 
sim time next is 3589200.0000, 
raw observation next is [-2.0, 50.0, 116.0, 817.5, 24.0, 23.4777449995563, 0.0496296224750822, 0.0, 1.0, 18698.550480579757], 
processed observation next is [0.0, 0.5652173913043478, 0.40720221606648205, 0.5, 0.38666666666666666, 0.9033149171270718, 0.5, 0.456478749963025, 0.5165432074916941, 0.0, 1.0, 0.08904071657418933], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06473551], dtype=float32), -1.0947568]. 
=============================================
[2019-04-07 17:38:33,645] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1945006e-28 3.4020632e-27 8.7083797e-26 1.6302692e-18 4.4914880e-26
 1.0000000e+00 9.4610075e-19 3.9342707e-20], sum to 1.0000
[2019-04-07 17:38:33,646] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4509
[2019-04-07 17:38:33,715] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 24.0, 23.20392650600022, -0.08636485750658608, 0.0, 1.0, 43746.82139864576], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2950200.0000, 
sim time next is 2952000.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 24.0, 23.12094015663507, -0.1048122286624172, 0.0, 1.0, 43771.430366826105], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.84, 0.0, 0.0, 0.5, 0.4267450130529224, 0.46506259044586096, 0.0, 1.0, 0.20843538269917192], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14252256], dtype=float32), -1.1128179]. 
=============================================
[2019-04-07 17:38:33,718] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[100.2183  ]
 [100.50624 ]
 [100.055626]
 [ 99.7316  ]
 [100.14389 ]], R is [[99.88514709]
 [99.88629913]
 [99.88743591]
 [99.88856506]
 [99.88967896]].
[2019-04-07 17:38:35,105] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:38:35,105] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:38:35,108] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run38
[2019-04-07 17:38:50,320] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-07 17:38:50,326] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:38:50,326] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:38:50,327] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 17:38:50,327] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:38:50,327] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:38:50,327] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:38:50,336] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run85
[2019-04-07 17:38:50,366] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run85
[2019-04-07 17:38:50,392] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run85
[2019-04-07 17:41:11,746] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 17:41:34,696] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 17:41:37,815] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 17:41:38,853] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1680000, evaluation results [1680000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 17:41:43,786] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8520630e-27 5.6822117e-27 1.0178375e-24 3.5727949e-18 1.4545630e-24
 1.0000000e+00 2.3467739e-17 2.8672943e-20], sum to 1.0000
[2019-04-07 17:41:43,786] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0502
[2019-04-07 17:41:43,858] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 24.0, 23.48517111057748, -0.1064522803236564, 0.0, 1.0, 40685.921341413225], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4255200.0000, 
sim time next is 4257000.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 24.0, 23.46805355860836, -0.1053477941919925, 0.0, 1.0, 42389.05156264201], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 0.5, 0.4556711298840301, 0.4648840686026692, 0.0, 1.0, 0.20185262648877147], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7945492], dtype=float32), -1.040074]. 
=============================================
[2019-04-07 17:41:43,862] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[95.913376]
 [95.65831 ]
 [95.75513 ]
 [95.544586]
 [95.46745 ]], R is [[96.16716003]
 [96.20549011]
 [96.24343872]
 [96.28100586]
 [96.31819916]].
[2019-04-07 17:41:50,262] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6651130e-27 5.5701216e-26 1.7131546e-25 1.0197650e-17 4.1376149e-24
 1.0000000e+00 1.3317294e-18 4.2431671e-20], sum to 1.0000
[2019-04-07 17:41:50,262] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3031
[2019-04-07 17:41:50,342] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 44.0, 16.5, 93.5, 24.0, 23.18478954664035, -0.1402185983305223, 0.0, 1.0, 23240.48265095132], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4903200.0000, 
sim time next is 4905000.0000, 
raw observation next is [1.5, 45.5, 0.0, 0.0, 24.0, 23.05668482269783, -0.1487251107548145, 0.0, 1.0, 59035.419031385136], 
processed observation next is [0.0, 0.782608695652174, 0.5041551246537397, 0.455, 0.0, 0.0, 0.5, 0.42139040189148574, 0.4504249630817285, 0.0, 1.0, 0.2811210430065959], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45253652], dtype=float32), 0.32071933]. 
=============================================
[2019-04-07 17:41:50,383] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[94.00916 ]
 [94.308556]
 [94.462296]
 [94.5143  ]
 [94.36996 ]], R is [[94.06433105]
 [94.12368774]
 [94.18244934]
 [94.24062347]
 [94.29821777]].
[2019-04-07 17:41:58,965] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:41:58,965] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:41:58,972] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run38
[2019-04-07 17:41:59,379] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:41:59,379] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:41:59,412] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run38
[2019-04-07 17:42:03,319] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1449627e-23 1.4968951e-22 1.5328514e-19 4.6633848e-15 3.3206954e-19
 1.0000000e+00 3.6203686e-14 2.5663613e-16], sum to 1.0000
[2019-04-07 17:42:03,320] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8835
[2019-04-07 17:42:03,366] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 44.0, 0.0, 0.0, 24.0, 21.48994696116194, -0.5781126200982478, 0.0, 1.0, 47420.511777546955], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 453600.0000, 
sim time next is 455400.0000, 
raw observation next is [-8.95, 43.5, 0.0, 0.0, 24.0, 21.47562300701095, -0.5710627994204466, 0.0, 1.0, 47334.83473620135], 
processed observation next is [1.0, 0.2608695652173913, 0.21468144044321333, 0.435, 0.0, 0.0, 0.5, 0.2896352505842457, 0.3096457335265178, 0.0, 1.0, 0.22540397493429215], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00056815], dtype=float32), -1.1999242]. 
=============================================
[2019-04-07 17:42:13,437] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:42:13,437] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:42:13,441] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run38
[2019-04-07 17:42:15,007] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:42:15,008] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:42:15,012] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run38
[2019-04-07 17:42:15,485] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:42:15,485] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:42:15,489] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run38
[2019-04-07 17:42:15,821] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:42:15,821] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:42:15,828] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run38
[2019-04-07 17:42:18,881] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:42:18,881] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:42:18,892] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run38
[2019-04-07 17:42:18,947] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:42:18,948] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:42:18,952] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run38
[2019-04-07 17:42:23,737] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:42:23,737] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:42:23,742] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run38
[2019-04-07 17:42:25,542] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.4441806e-27 8.9003497e-26 2.3740844e-24 7.2505243e-18 5.9687690e-23
 1.0000000e+00 1.6660865e-18 3.4274782e-19], sum to 1.0000
[2019-04-07 17:42:25,543] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4282
[2019-04-07 17:42:25,571] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.5, 27.0, 118.0, 0.0, 24.0, 23.65796786003804, -0.1023142372479352, 1.0, 1.0, 12453.607780153689], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2817000.0000, 
sim time next is 2818800.0000, 
raw observation next is [7.0, 24.0, 106.5, 0.0, 24.0, 23.72240975479113, -0.07384127141280113, 1.0, 1.0, 9340.205835115268], 
processed observation next is [1.0, 0.6521739130434783, 0.6565096952908588, 0.24, 0.355, 0.0, 0.5, 0.4768674795659275, 0.4753862428623996, 1.0, 1.0, 0.04447717064340604], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32788256], dtype=float32), -0.35672244]. 
=============================================
[2019-04-07 17:42:38,678] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2604347e-26 1.1024314e-24 1.7842104e-22 4.2795072e-16 9.7885332e-23
 1.0000000e+00 4.6451459e-17 2.6443629e-17], sum to 1.0000
[2019-04-07 17:42:38,678] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9057
[2019-04-07 17:42:38,914] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 61.0, 104.5, 75.5, 24.0, 23.51630057880474, -0.04772401474458243, 1.0, 1.0, 65465.562942386074], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 140400.0000, 
sim time next is 142200.0000, 
raw observation next is [-6.7, 62.5, 61.0, 45.0, 24.0, 23.98636164911574, 0.0461763761610753, 1.0, 1.0, 85131.26064295774], 
processed observation next is [1.0, 0.6521739130434783, 0.2770083102493075, 0.625, 0.20333333333333334, 0.049723756906077346, 0.5, 0.49886347075964493, 0.5153921253870252, 1.0, 1.0, 0.4053869554426559], 
reward next is 0.8803, 
noisyNet noise sample is [array([-0.09445018], dtype=float32), -1.297535]. 
=============================================
[2019-04-07 17:42:42,348] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5297939e-22 3.4774922e-22 1.3684622e-20 2.9875747e-14 1.5628747e-20
 1.0000000e+00 1.0376086e-15 8.6211920e-17], sum to 1.0000
[2019-04-07 17:42:42,348] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0337
[2019-04-07 17:42:42,387] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.9, 52.0, 0.0, 0.0, 24.0, 21.62944502485522, -0.5296514573794894, 0.0, 1.0, 47414.619374710914], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 448200.0000, 
sim time next is 450000.0000, 
raw observation next is [-10.6, 52.0, 0.0, 0.0, 24.0, 21.64773125509613, -0.5461201244790997, 0.0, 1.0, 47501.42571669415], 
processed observation next is [1.0, 0.21739130434782608, 0.1689750692520776, 0.52, 0.0, 0.0, 0.5, 0.3039776045913441, 0.31795995850696673, 0.0, 1.0, 0.2261972653175912], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20207757], dtype=float32), -0.18064274]. 
=============================================
[2019-04-07 17:42:42,395] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.11428 ]
 [80.513214]
 [80.87312 ]
 [81.19479 ]
 [81.45207 ]], R is [[79.99026489]
 [80.19036102]
 [80.38845825]
 [80.58457184]
 [80.77872467]].
[2019-04-07 17:42:43,585] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0378719e-26 9.4606856e-26 3.0370460e-23 1.8536062e-17 2.6970965e-23
 1.0000000e+00 4.9439589e-17 5.3640119e-19], sum to 1.0000
[2019-04-07 17:42:43,586] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4697
[2019-04-07 17:42:43,717] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 24.0, 23.01119133405594, -0.1964768535123685, 0.0, 1.0, 45508.31093924312], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 255600.0000, 
sim time next is 257400.0000, 
raw observation next is [-4.2, 80.5, 0.0, 0.0, 24.0, 22.93606488084302, -0.2141805308447834, 0.0, 1.0, 45494.09310444561], 
processed observation next is [1.0, 1.0, 0.34626038781163443, 0.805, 0.0, 0.0, 0.5, 0.4113387400702517, 0.4286064897184055, 0.0, 1.0, 0.21663853859259813], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1454403], dtype=float32), -0.67243856]. 
=============================================
[2019-04-07 17:42:58,316] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:42:58,316] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:42:58,320] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run39
[2019-04-07 17:43:19,729] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:43:19,729] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:43:19,733] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run39
[2019-04-07 17:43:24,894] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.5829713e-28 1.6810825e-26 6.7126052e-24 7.2456853e-18 1.4887225e-23
 1.0000000e+00 5.6724676e-19 1.9900539e-19], sum to 1.0000
[2019-04-07 17:43:24,894] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3774
[2019-04-07 17:43:24,933] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [18.55, 64.0, 160.0, 0.0, 24.0, 23.53578148466575, 0.1172867440358287, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1164600.0000, 
sim time next is 1166400.0000, 
raw observation next is [18.8, 63.0, 165.5, 0.0, 24.0, 23.53497864378522, 0.1221501822108193, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.9833795013850417, 0.63, 0.5516666666666666, 0.0, 0.5, 0.46124822031543494, 0.5407167274036064, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8376448], dtype=float32), -1.0171508]. 
=============================================
[2019-04-07 17:43:32,615] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.05539889e-31 1.08498106e-29 1.59190063e-28 1.12449957e-19
 1.28651311e-27 1.00000000e+00 6.29536710e-21 8.79224733e-22], sum to 1.0000
[2019-04-07 17:43:32,615] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4032
[2019-04-07 17:43:32,650] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.7, 97.0, 100.5, 0.0, 24.0, 24.02267391883382, -0.04627113910242343, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 907200.0000, 
sim time next is 909000.0000, 
raw observation next is [3.25, 95.0, 104.0, 0.0, 24.0, 23.96089753212429, -0.0571941093680626, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5526315789473685, 0.95, 0.3466666666666667, 0.0, 0.5, 0.4967414610103574, 0.48093529687731246, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03489655], dtype=float32), -1.1964071]. 
=============================================
[2019-04-07 17:43:32,697] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[105.57387]
 [104.49091]
 [103.52772]
 [102.91373]
 [102.57119]], R is [[106.21550751]
 [106.15335083]
 [106.09181976]
 [106.03089905]
 [105.97058868]].
[2019-04-07 17:43:40,452] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6861309e-29 3.4931704e-28 1.2600006e-25 2.7819423e-19 1.2946475e-25
 1.0000000e+00 3.0787085e-18 1.3620668e-20], sum to 1.0000
[2019-04-07 17:43:40,453] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9187
[2019-04-07 17:43:40,507] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 24.0, 23.72789931426498, 0.08419855702884844, 0.0, 1.0, 42719.88616805462], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4669200.0000, 
sim time next is 4671000.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 24.0, 23.79211157205026, 0.05889144810400554, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.57, 0.0, 0.0, 0.5, 0.48267596433752163, 0.5196304827013352, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00660363], dtype=float32), -1.2446009]. 
=============================================
[2019-04-07 17:43:40,523] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[100.098656]
 [100.44167 ]
 [100.88546 ]
 [101.21428 ]
 [101.36239 ]], R is [[99.69963074]
 [99.70263672]
 [99.70561218]
 [99.70855713]
 [99.71147156]].
[2019-04-07 17:43:44,793] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-07 17:43:44,794] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:43:44,795] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:43:44,807] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:43:44,795] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:43:44,813] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run86
[2019-04-07 17:43:44,834] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 17:43:44,835] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:43:44,844] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run86
[2019-04-07 17:43:44,860] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run86
[2019-04-07 17:44:03,466] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.1146975], dtype=float32), 0.15838473]
[2019-04-07 17:44:03,466] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-0.6, 35.0, 106.5, 0.0, 24.0, 22.78547162477815, -0.2524478763175897, 1.0, 1.0, 78187.24384656434]
[2019-04-07 17:44:03,466] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 17:44:03,467] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [5.1638790e-25 4.8666983e-24 1.7684086e-22 4.1862610e-16 8.9081122e-22
 1.0000000e+00 7.7424754e-17 7.6864981e-18], sampled 0.8255180777723513
[2019-04-07 17:46:06,536] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.1146975], dtype=float32), 0.15838473]
[2019-04-07 17:46:06,537] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [0.647422801, 76.73938663, 0.0, 0.0, 24.0, 23.5173640951343, -0.02846421406076857, 0.0, 1.0, 28129.19340131058]
[2019-04-07 17:46:06,537] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 17:46:06,538] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [7.1611486e-29 1.2038555e-27 9.5078274e-26 1.4721402e-18 2.4990041e-25
 1.0000000e+00 3.8109449e-19 1.7853832e-20], sampled 0.3307605587859801
[2019-04-07 17:46:08,445] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 17:46:29,479] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 17:46:34,557] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 17:46:35,593] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1700000, evaluation results [1700000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 17:46:49,997] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:46:49,997] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:46:50,001] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run39
[2019-04-07 17:46:54,925] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2018116e-29 5.4815607e-29 2.0013749e-26 4.4031687e-19 3.3752696e-25
 1.0000000e+00 5.3265110e-20 1.7238379e-20], sum to 1.0000
[2019-04-07 17:46:54,925] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5054
[2019-04-07 17:46:54,987] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 60.0, 64.0, 539.0, 24.0, 25.28479282286366, 0.3368115836632102, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3774600.0000, 
sim time next is 3776400.0000, 
raw observation next is [0.0, 60.0, 40.5, 343.0, 24.0, 25.0602658637444, 0.3015458085874542, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.6, 0.135, 0.37900552486187844, 0.5, 0.5883554886453668, 0.6005152695291515, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43112165], dtype=float32), -0.86270255]. 
=============================================
[2019-04-07 17:47:00,133] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:47:00,133] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:47:00,136] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run39
[2019-04-07 17:47:04,932] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:47:04,932] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:47:04,936] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run39
[2019-04-07 17:47:06,403] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.0795098e-27 2.0223264e-26 2.0873929e-23 1.1558361e-17 2.4807323e-23
 1.0000000e+00 4.2175450e-18 1.6084501e-19], sum to 1.0000
[2019-04-07 17:47:06,404] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0531
[2019-04-07 17:47:06,619] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.05, 84.5, 0.0, 0.0, 24.0, 23.10545734434969, -0.166474366645841, 0.0, 1.0, 108330.14487676551], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1884600.0000, 
sim time next is 1886400.0000, 
raw observation next is [-5.6, 86.0, 0.0, 0.0, 24.0, 23.33262021354145, -0.1487387014740344, 0.0, 1.0, 54744.61810216985], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.86, 0.0, 0.0, 0.5, 0.4443850177951208, 0.4504204328419885, 0.0, 1.0, 0.26068865762938026], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3487736], dtype=float32), 1.0737581]. 
=============================================
[2019-04-07 17:47:17,311] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1056305e-27 6.3168056e-27 2.6475935e-24 3.0560234e-16 7.3515481e-23
 1.0000000e+00 2.3292740e-18 2.3122951e-17], sum to 1.0000
[2019-04-07 17:47:17,333] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8117
[2019-04-07 17:47:17,522] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 86.0, 87.5, 0.0, 24.0, 24.25257901839727, -0.1089643999968851, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2041200.0000, 
sim time next is 2043000.0000, 
raw observation next is [-4.2, 84.0, 71.0, 0.0, 24.0, 23.60599683626941, -0.05930944357024478, 1.0, 1.0, 70716.18533967304], 
processed observation next is [1.0, 0.6521739130434783, 0.34626038781163443, 0.84, 0.23666666666666666, 0.0, 0.5, 0.4671664030224507, 0.4802301854765851, 1.0, 1.0, 0.3367437397127288], 
reward next is 0.9490, 
noisyNet noise sample is [array([-0.17220624], dtype=float32), -0.3809226]. 
=============================================
[2019-04-07 17:47:17,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[97.52251 ]
 [97.47666 ]
 [97.64844 ]
 [97.511566]
 [97.52337 ]], R is [[98.06998444]
 [98.0892868 ]
 [98.10839844]
 [98.12731171]
 [98.14604187]].
[2019-04-07 17:47:21,567] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.9135666e-25 4.0812885e-25 1.5482561e-24 1.5350239e-17 8.2020582e-23
 1.0000000e+00 7.7785623e-17 6.8538743e-19], sum to 1.0000
[2019-04-07 17:47:21,567] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2580
[2019-04-07 17:47:21,714] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.55, 64.5, 37.0, 9.0, 24.0, 23.72499034569833, -0.05479051902223914, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 117000.0000, 
sim time next is 118800.0000, 
raw observation next is [-7.8, 61.0, 41.0, 4.5, 24.0, 23.89826829710873, -0.06782267939304877, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.61, 0.13666666666666666, 0.004972375690607734, 0.5, 0.49152235809239403, 0.4773924402023171, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3853174], dtype=float32), -0.7800756]. 
=============================================
[2019-04-07 17:47:27,602] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0490800e-28 1.2496033e-26 1.1822035e-25 9.7595825e-19 1.1707867e-24
 1.0000000e+00 1.0651875e-18 2.8898492e-20], sum to 1.0000
[2019-04-07 17:47:27,602] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2736
[2019-04-07 17:47:27,812] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.9, 72.0, 0.0, 0.0, 24.0, 23.49317660479851, -0.04282111465504116, 1.0, 1.0, 12904.204287834558], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4519800.0000, 
sim time next is 4521600.0000, 
raw observation next is [-0.8, 73.0, 55.5, 33.0, 24.0, 23.4932473773906, -0.03744910280446483, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4404432132963989, 0.73, 0.185, 0.036464088397790057, 0.5, 0.45777061478254993, 0.48751696573184505, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2701196], dtype=float32), -0.1388366]. 
=============================================
[2019-04-07 17:47:33,448] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.44755868e-30 2.04752161e-28 3.19990718e-27 1.86532950e-19
 1.01079496e-25 1.00000000e+00 8.68684012e-20 1.56015755e-21], sum to 1.0000
[2019-04-07 17:47:33,448] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1449
[2019-04-07 17:47:33,513] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 85.0, 0.0, 0.0, 24.0, 23.38185480697233, -0.01049315305444871, 0.0, 1.0, 46288.21397795829], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1744200.0000, 
sim time next is 1746000.0000, 
raw observation next is [-0.6, 83.0, 0.0, 0.0, 24.0, 23.37735778998062, -0.01372882197367658, 0.0, 1.0, 45892.553861669236], 
processed observation next is [0.0, 0.21739130434782608, 0.44598337950138506, 0.83, 0.0, 0.0, 0.5, 0.4481131491650518, 0.49542372600877443, 0.0, 1.0, 0.2185359707698535], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48590505], dtype=float32), 0.4567057]. 
=============================================
[2019-04-07 17:47:33,517] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[104.26886 ]
 [104.666306]
 [105.03068 ]
 [105.230194]
 [104.58308 ]], R is [[103.7515564 ]
 [103.71404266]
 [103.67690277]
 [103.64013672]
 [103.60373688]].
[2019-04-07 17:47:41,722] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.1401757e-30 6.6659199e-28 6.2526510e-27 8.6292822e-18 5.7979403e-24
 1.0000000e+00 3.8278021e-19 1.4687395e-20], sum to 1.0000
[2019-04-07 17:47:41,724] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8936
[2019-04-07 17:47:41,764] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 59.0, 0.0, 24.0, 23.98816780294477, 0.06897880631554446, 1.0, 1.0, 6226.803890076847], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1436400.0000, 
sim time next is 1438200.0000, 
raw observation next is [1.1, 92.0, 46.0, 0.0, 24.0, 23.68753125976665, 0.02749622213878739, 1.0, 1.0, 13270.205334521406], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.15333333333333332, 0.0, 0.5, 0.4739609383138876, 0.5091654073795958, 1.0, 1.0, 0.06319145397391146], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23378305], dtype=float32), -0.46510488]. 
=============================================
[2019-04-07 17:47:53,855] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:47:53,855] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:47:53,861] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run39
[2019-04-07 17:47:54,879] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2966673e-30 1.4925001e-28 3.6699855e-27 4.6051435e-19 9.6469446e-27
 1.0000000e+00 1.6695649e-20 2.4059142e-22], sum to 1.0000
[2019-04-07 17:47:54,879] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5036
[2019-04-07 17:47:54,936] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.55, 96.5, 0.0, 0.0, 24.0, 23.39797185475286, -0.1252496379769057, 0.0, 1.0, 38710.18052442749], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 516600.0000, 
sim time next is 518400.0000, 
raw observation next is [3.8, 97.0, 0.0, 0.0, 24.0, 23.44341742504854, -0.1249889661459803, 0.0, 1.0, 26995.509344832742], 
processed observation next is [0.0, 0.0, 0.5678670360110805, 0.97, 0.0, 0.0, 0.5, 0.45361811875404506, 0.45833701128467325, 0.0, 1.0, 0.12855004449920354], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3854145], dtype=float32), -0.7758972]. 
=============================================
[2019-04-07 17:47:57,643] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.7694655e-30 1.0626946e-27 2.4839577e-26 1.5102853e-18 4.0355850e-26
 1.0000000e+00 3.4532122e-20 1.0554868e-20], sum to 1.0000
[2019-04-07 17:47:57,643] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5433
[2019-04-07 17:47:57,707] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 24.0, 23.36046132038414, -0.1190000107550645, 0.0, 1.0, 43548.224161514874], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 601200.0000, 
sim time next is 603000.0000, 
raw observation next is [-3.4, 85.0, 0.0, 0.0, 24.0, 23.31257396553674, -0.1324147363537405, 0.0, 1.0, 44194.08865034335], 
processed observation next is [0.0, 1.0, 0.368421052631579, 0.85, 0.0, 0.0, 0.5, 0.4427144971280616, 0.45586175454875316, 0.0, 1.0, 0.2104480411921112], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42591965], dtype=float32), -0.7357126]. 
=============================================
[2019-04-07 17:47:57,730] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[ 99.69778]
 [100.11053]
 [100.27959]
 [100.52899]
 [100.80632]], R is [[99.49560547]
 [99.5006485 ]
 [99.50564575]
 [99.5105896 ]
 [99.51548767]].
[2019-04-07 17:48:31,194] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0497655e-27 8.1973495e-27 6.0781565e-25 5.5866389e-17 7.0267572e-24
 1.0000000e+00 2.5320226e-18 1.6781676e-18], sum to 1.0000
[2019-04-07 17:48:31,194] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5694
[2019-04-07 17:48:31,261] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.5, 65.5, 0.0, 0.0, 24.0, 23.44884269487106, 0.0130093272047655, 0.0, 1.0, 53880.91300912498], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3547800.0000, 
sim time next is 3549600.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 24.0, 23.46569802927998, 0.01166069828426732, 0.0, 1.0, 34356.480271356355], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.71, 0.0, 0.0, 0.5, 0.45547483577333175, 0.5038868994280891, 0.0, 1.0, 0.16360228700645885], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0334387], dtype=float32), -0.85819554]. 
=============================================
[2019-04-07 17:48:42,831] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-07 17:48:42,833] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:48:42,834] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:48:42,838] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run87
[2019-04-07 17:48:42,858] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:48:42,859] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:48:42,860] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 17:48:42,864] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run87
[2019-04-07 17:48:42,881] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:48:42,902] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run87
[2019-04-07 17:51:03,793] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 17:51:21,744] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 17:51:25,801] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 17:51:26,826] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1720000, evaluation results [1720000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 17:51:34,619] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8690543e-27 1.0203523e-25 2.1044387e-23 4.2625279e-17 2.0080082e-23
 1.0000000e+00 5.3389256e-18 1.1719834e-18], sum to 1.0000
[2019-04-07 17:51:34,619] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3312
[2019-04-07 17:51:34,764] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.75, 59.5, 182.0, 93.0, 24.0, 23.04429615403912, -0.1806663228822614, 0.0, 1.0, 24658.044012999744], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 653400.0000, 
sim time next is 655200.0000, 
raw observation next is [-1.2, 60.0, 131.5, 74.5, 24.0, 23.04516933755869, -0.1928349710296634, 0.0, 1.0, 22076.14599531762], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.6, 0.43833333333333335, 0.08232044198895028, 0.5, 0.4204307781298908, 0.43572167632344555, 0.0, 1.0, 0.10512450473960772], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.526341], dtype=float32), 0.6596971]. 
=============================================
[2019-04-07 17:51:38,321] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:51:38,321] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:51:38,326] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run39
[2019-04-07 17:51:57,355] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.7083054e-28 4.7773355e-26 4.6790954e-25 4.9135538e-18 6.3140206e-24
 1.0000000e+00 4.0811317e-18 1.6794403e-20], sum to 1.0000
[2019-04-07 17:51:57,356] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1925
[2019-04-07 17:51:57,463] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 83.0, 85.5, 0.0, 24.0, 23.85255347860515, -0.1185467940981139, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2023200.0000, 
sim time next is 2025000.0000, 
raw observation next is [-5.6, 83.0, 102.0, 0.0, 24.0, 23.75714627800347, -0.1331307084141234, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.30747922437673136, 0.83, 0.34, 0.0, 0.5, 0.4797621898336226, 0.4556230971952922, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7277428], dtype=float32), 0.045022916]. 
=============================================
[2019-04-07 17:51:57,472] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[98.1937  ]
 [98.31018 ]
 [98.31987 ]
 [98.344536]
 [98.33133 ]], R is [[97.82798004]
 [97.84970093]
 [97.87120819]
 [97.8924942 ]
 [97.91356659]].
[2019-04-07 17:52:10,853] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.8747363e-30 1.0595010e-27 1.3468111e-25 1.0455456e-18 2.2036982e-25
 1.0000000e+00 1.2596397e-18 7.7859684e-21], sum to 1.0000
[2019-04-07 17:52:10,854] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8501
[2019-04-07 17:52:10,904] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.55, 73.0, 0.0, 0.0, 24.0, 23.43989072903774, 0.04946517128123575, 0.0, 1.0, 67805.22609667761], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4494600.0000, 
sim time next is 4496400.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 24.0, 23.77433260471673, 0.05728754581223572, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.5, 0.4811943837263941, 0.5190958486040785, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.508236], dtype=float32), -0.31551]. 
=============================================
[2019-04-07 17:52:17,109] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:52:17,110] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:52:17,115] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run39
[2019-04-07 17:52:20,633] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:52:20,633] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:52:20,637] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run39
[2019-04-07 17:52:24,642] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7125107e-29 5.2567306e-27 7.6343308e-27 1.6073408e-18 4.3485218e-25
 1.0000000e+00 3.0109688e-20 3.0353930e-21], sum to 1.0000
[2019-04-07 17:52:24,643] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1169
[2019-04-07 17:52:24,772] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.55, 83.0, 126.0, 0.0, 24.0, 23.05572354281031, -0.08598751575729306, 0.0, 1.0, 40485.92297825631], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1773000.0000, 
sim time next is 1774800.0000, 
raw observation next is [-2.8, 83.0, 122.5, 0.0, 24.0, 23.10609610226797, -0.0855699012124904, 0.0, 1.0, 27330.447137605035], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.4083333333333333, 0.0, 0.5, 0.4255080085223308, 0.47147669959583655, 0.0, 1.0, 0.1301449863695478], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.81512123], dtype=float32), 0.3733515]. 
=============================================
[2019-04-07 17:52:31,018] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:52:31,018] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:52:31,057] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run39
[2019-04-07 17:52:34,007] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:52:34,008] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:52:34,012] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run39
[2019-04-07 17:52:37,305] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:52:37,305] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:52:37,312] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run39
[2019-04-07 17:52:37,945] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:52:37,945] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:52:37,949] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run39
[2019-04-07 17:52:39,329] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:52:39,329] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:52:39,333] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run39
[2019-04-07 17:52:39,405] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:52:39,405] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:52:39,409] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run39
[2019-04-07 17:52:42,357] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:52:42,357] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:52:42,361] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run39
[2019-04-07 17:53:03,103] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1990112e-27 4.9694031e-27 3.2424611e-26 3.2139168e-18 4.4635489e-24
 1.0000000e+00 1.7072174e-18 3.8945679e-19], sum to 1.0000
[2019-04-07 17:53:03,104] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3767
[2019-04-07 17:53:03,255] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3, 77.0, 7.0, 88.0, 24.0, 23.14607163318678, -0.1379812639229286, 0.0, 1.0, 40049.25965658308], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3087000.0000, 
sim time next is 3088800.0000, 
raw observation next is [-0.6, 82.0, 0.0, 0.0, 24.0, 23.1137590848462, -0.1467652878492912, 0.0, 1.0, 42249.508828650585], 
processed observation next is [0.0, 0.782608695652174, 0.44598337950138506, 0.82, 0.0, 0.0, 0.5, 0.4261465904038501, 0.4510782373835696, 0.0, 1.0, 0.2011881372792885], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60373145], dtype=float32), -0.6275658]. 
=============================================
[2019-04-07 17:53:05,025] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.6568804e-28 9.6388148e-27 6.8021997e-26 1.2718952e-17 1.6565364e-25
 1.0000000e+00 1.5652321e-19 1.7332142e-20], sum to 1.0000
[2019-04-07 17:53:05,025] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1434
[2019-04-07 17:53:05,093] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 24.0, 23.34049335268188, -0.03006133379726445, 1.0, 1.0, 18709.181378036814], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4563000.0000, 
sim time next is 4564800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 24.0, 23.10330466696607, -0.04623812686126279, 1.0, 1.0, 44135.34107525734], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.5, 0.42527538891383926, 0.4845872910462457, 1.0, 1.0, 0.21016829083455876], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23415849], dtype=float32), -0.23104773]. 
=============================================
[2019-04-07 17:53:07,766] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3295388e-27 3.2422880e-26 6.1789897e-24 9.6965751e-18 2.9059054e-24
 1.0000000e+00 2.5061060e-18 3.8730264e-19], sum to 1.0000
[2019-04-07 17:53:07,766] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1755
[2019-04-07 17:53:07,934] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 24.0, 23.60417076271389, 0.1290424859848598, 1.0, 1.0, 103413.13618899077], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3265200.0000, 
sim time next is 3267000.0000, 
raw observation next is [-4.0, 68.0, 0.0, 0.0, 24.0, 23.8646057817761, 0.1557860274977142, 1.0, 1.0, 20755.853907444412], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.68, 0.0, 0.0, 0.5, 0.48871714848134157, 0.5519286758325714, 1.0, 1.0, 0.0988373995592591], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6855393], dtype=float32), 0.83684415]. 
=============================================
[2019-04-07 17:53:07,941] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[ 99.88997 ]
 [ 99.44902 ]
 [100.40839 ]
 [101.3625  ]
 [103.112724]], R is [[99.24599457]
 [99.04680634]
 [99.05633545]
 [99.04003143]
 [99.04962921]].
[2019-04-07 17:53:10,105] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.1481004e-26 4.9414311e-25 9.5170224e-23 8.0694711e-17 1.9078808e-21
 1.0000000e+00 5.2542885e-18 1.1727974e-18], sum to 1.0000
[2019-04-07 17:53:10,106] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2538
[2019-04-07 17:53:10,241] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 65.0, 95.0, 383.0, 24.0, 23.904145956371, -0.08732575653347106, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 293400.0000, 
sim time next is 295200.0000, 
raw observation next is [-11.7, 63.0, 91.0, 447.5, 24.0, 23.91951035889454, -0.08764788882319831, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.13850415512465375, 0.63, 0.30333333333333334, 0.494475138121547, 0.5, 0.4932925299078785, 0.47078403705893385, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.47386217], dtype=float32), 0.08292405]. 
=============================================
[2019-04-07 17:53:10,947] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2379881e-25 2.5168668e-24 5.0607226e-23 4.2556251e-16 5.6595764e-22
 1.0000000e+00 1.8921285e-16 1.4126422e-17], sum to 1.0000
[2019-04-07 17:53:10,947] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7946
[2019-04-07 17:53:11,094] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.6, 49.0, 94.5, 708.0, 24.0, 23.81732224080217, -0.06059233582031864, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 302400.0000, 
sim time next is 304200.0000, 
raw observation next is [-10.05, 46.5, 83.0, 758.0, 24.0, 23.77542954781068, -0.08634762782050713, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.18421052631578946, 0.465, 0.27666666666666667, 0.8375690607734807, 0.5, 0.4812857956508901, 0.4712174573931643, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.83995813], dtype=float32), -2.6849926]. 
=============================================
[2019-04-07 17:53:14,251] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:53:14,252] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:53:14,255] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run40
[2019-04-07 17:53:30,348] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:53:30,348] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:53:30,366] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run40
[2019-04-07 17:53:33,548] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-07 17:53:33,555] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:53:33,556] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:53:33,559] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run88
[2019-04-07 17:53:33,601] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:53:33,602] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:53:33,606] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run88
[2019-04-07 17:53:33,638] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 17:53:33,639] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:53:33,645] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run88
[2019-04-07 17:56:01,573] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 17:56:18,668] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 17:56:23,010] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 17:56:24,034] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 1740000, evaluation results [1740000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 17:56:30,373] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.09554494e-29 1.15891954e-29 2.69592868e-27 1.14183689e-20
 1.57633504e-27 1.00000000e+00 1.45948971e-21 4.29011843e-22], sum to 1.0000
[2019-04-07 17:56:30,374] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9807
[2019-04-07 17:56:30,499] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.5, 0.0, 0.0, 24.0, 23.47982012999571, -0.03766153140677247, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2917800.0000, 
sim time next is 2919600.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 24.0, 23.21378210617351, -0.0681842574357777, 1.0, 1.0, 20250.245134644952], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.5, 0.4344818421811259, 0.47727191418807413, 1.0, 1.0, 0.09642973873640454], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.6345997], dtype=float32), -1.417642]. 
=============================================
[2019-04-07 17:56:47,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4121731e-30 4.3093922e-29 6.4675140e-28 1.6577553e-19 2.4564988e-26
 1.0000000e+00 5.5067748e-21 2.0335333e-21], sum to 1.0000
[2019-04-07 17:56:47,279] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3786
[2019-04-07 17:56:47,331] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 92.0, 9.0, 0.0, 24.0, 23.72246529571965, -0.06128954413916566, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 925200.0000, 
sim time next is 927000.0000, 
raw observation next is [4.7, 94.0, 0.0, 0.0, 24.0, 23.61627057422769, -0.1297289899590786, 1.0, 1.0, 4447.7170643406], 
processed observation next is [1.0, 0.7391304347826086, 0.592797783933518, 0.94, 0.0, 0.0, 0.5, 0.4680225478523076, 0.45675700334697383, 1.0, 1.0, 0.02117960506828857], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26351613], dtype=float32), 0.46406338]. 
=============================================
[2019-04-07 17:56:47,358] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[109.2355 ]
 [109.15478]
 [109.51201]
 [109.53505]
 [109.85443]], R is [[109.00008392]
 [108.91008759]
 [108.82099152]
 [108.73278046]
 [108.64545441]].
[2019-04-07 17:56:49,828] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.0665710e-29 4.2417722e-29 8.8775587e-27 1.0508672e-19 7.9025879e-26
 1.0000000e+00 2.4764340e-20 1.2144773e-21], sum to 1.0000
[2019-04-07 17:56:49,830] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8792
[2019-04-07 17:56:49,902] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.5, 78.0, 0.0, 0.0, 24.0, 23.46494643143525, 0.008360897094927896, 1.0, 1.0, 26053.25823173548], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4732200.0000, 
sim time next is 4734000.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 24.0, 23.31261902657566, -0.01518157566908458, 1.0, 1.0, 29998.76916795762], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 0.5, 0.4427182522146384, 0.4949394747769718, 1.0, 1.0, 0.14285128175217915], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.225246], dtype=float32), -1.1955938]. 
=============================================
[2019-04-07 17:56:49,912] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[101.90618 ]
 [101.841324]
 [101.747055]
 [102.229095]
 [102.27524 ]], R is [[101.7846756 ]
 [101.76683044]
 [101.74916077]
 [101.73166656]
 [101.71434784]].
[2019-04-07 17:57:05,457] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:57:05,458] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:57:05,461] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run40
[2019-04-07 17:57:18,813] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:57:18,813] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:57:18,816] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run40
[2019-04-07 17:57:22,549] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:57:22,549] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:57:22,565] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run40
[2019-04-07 17:57:37,911] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.2892456e-31 7.2321968e-29 9.5349132e-28 1.8707891e-18 2.1272204e-26
 1.0000000e+00 5.0023218e-21 1.6198954e-21], sum to 1.0000
[2019-04-07 17:57:37,911] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6443
[2019-04-07 17:57:37,969] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.9, 92.0, 0.0, 0.0, 24.0, 23.67871914153418, 0.03124834020339055, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1474200.0000, 
sim time next is 1476000.0000, 
raw observation next is [2.2, 92.0, 0.0, 0.0, 24.0, 23.48872468311032, 0.01528240531071919, 0.0, 1.0, 59440.67823421295], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.92, 0.0, 0.0, 0.5, 0.45739372359252667, 0.505094135103573, 0.0, 1.0, 0.2830508487343474], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0233144], dtype=float32), 0.82007194]. 
=============================================
[2019-04-07 17:57:37,983] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[107.94927]
 [108.1101 ]
 [108.2627 ]
 [107.70081]
 [107.64996]], R is [[108.32902527]
 [108.24573517]
 [108.16327667]
 [108.08164215]
 [108.00082397]].
[2019-04-07 17:57:47,245] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0799747e-28 2.0776740e-26 5.6203066e-24 3.0393964e-17 4.9714014e-24
 1.0000000e+00 2.6067631e-18 2.0262409e-19], sum to 1.0000
[2019-04-07 17:57:47,245] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6783
[2019-04-07 17:57:47,334] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 24.0, 22.88402461438137, -0.2272767669470873, 0.0, 1.0, 44423.84064297316], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2091600.0000, 
sim time next is 2093400.0000, 
raw observation next is [-6.45, 85.0, 0.0, 0.0, 24.0, 22.92357620525539, -0.2191471234325367, 0.0, 1.0, 44533.284706939885], 
processed observation next is [1.0, 0.21739130434782608, 0.28393351800554023, 0.85, 0.0, 0.0, 0.5, 0.41029801710461583, 0.4269509588558211, 0.0, 1.0, 0.21206326050923754], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1733106], dtype=float32), -1.9548925]. 
=============================================
[2019-04-07 17:58:04,873] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2191445e-25 7.2719691e-24 3.4273681e-22 1.6253599e-15 5.8189103e-21
 1.0000000e+00 5.2133199e-18 1.9058758e-18], sum to 1.0000
[2019-04-07 17:58:04,873] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8171
[2019-04-07 17:58:04,987] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.5, 46.0, 51.5, 859.5, 24.0, 24.91474009412324, 0.1496921529763808, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 396000.0000, 
sim time next is 397800.0000, 
raw observation next is [-10.0, 43.0, 48.0, 832.0, 24.0, 24.8931937996608, 0.1398622366035842, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.18559556786703602, 0.43, 0.16, 0.9193370165745857, 0.5, 0.5744328166384, 0.5466207455345281, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0943693], dtype=float32), 0.65896225]. 
=============================================
[2019-04-07 17:58:09,444] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 17:58:09,444] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:58:09,447] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run40
[2019-04-07 17:58:31,074] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-07 17:58:31,078] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:58:31,079] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 17:58:31,079] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 17:58:31,080] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:58:31,080] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:58:31,083] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 17:58:31,087] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run89
[2019-04-07 17:58:31,114] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run89
[2019-04-07 17:58:31,138] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run89
[2019-04-07 18:00:53,078] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 18:01:10,516] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 18:01:13,580] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 18:01:14,604] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1760000, evaluation results [1760000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 18:01:41,449] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.29650445e-30 8.34045437e-28 5.95152597e-26 5.56360357e-18
 1.24192375e-26 1.00000000e+00 1.74256349e-18 1.41216581e-20], sum to 1.0000
[2019-04-07 18:01:41,450] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9655
[2019-04-07 18:01:41,548] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 24.0, 23.45525534354412, 0.05284726244830604, 0.0, 1.0, 39519.30010747887], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1391400.0000, 
sim time next is 1393200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 24.0, 23.56255605534059, 0.05109492901550673, 0.0, 1.0, 12496.989338161153], 
processed observation next is [1.0, 0.13043478260869565, 0.46260387811634357, 0.95, 0.0, 0.0, 0.5, 0.4635463379450491, 0.5170316430051689, 0.0, 1.0, 0.059509473038862636], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26081005], dtype=float32), -0.3319055]. 
=============================================
[2019-04-07 18:01:41,598] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.2034757e-29 8.9319335e-29 8.8653082e-27 1.6212951e-18 1.9320975e-25
 1.0000000e+00 3.1885257e-20 3.2398851e-21], sum to 1.0000
[2019-04-07 18:01:41,598] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6211
[2019-04-07 18:01:41,675] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 74.0, 0.0, 0.0, 24.0, 23.4465723569768, 0.01624147187025874, 0.0, 1.0, 54137.24523209431], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3803400.0000, 
sim time next is 3805200.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 24.0, 23.58076945202781, -0.01546380583043417, 0.0, 1.0, 6246.899613866967], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.5, 0.4650641210023174, 0.49484539805652195, 0.0, 1.0, 0.02974714101841413], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3665831], dtype=float32), -0.08754636]. 
=============================================
[2019-04-07 18:01:53,751] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:01:53,751] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:01:53,758] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run40
[2019-04-07 18:01:54,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4085313e-26 1.4708494e-25 1.4188214e-23 2.1452231e-17 2.1319843e-23
 1.0000000e+00 3.9891331e-18 8.5356063e-20], sum to 1.0000
[2019-04-07 18:01:54,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5852
[2019-04-07 18:01:55,195] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 91.0, 497.0, 24.0, 23.76947646577556, -0.05952160558738397, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3056400.0000, 
sim time next is 3058200.0000, 
raw observation next is [-5.0, 56.5, 99.0, 635.0, 24.0, 23.6136885062966, -0.06617148965678225, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.32409972299168976, 0.565, 0.33, 0.7016574585635359, 0.5, 0.46780737552471674, 0.4779428367810726, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.6405404], dtype=float32), 2.0028853]. 
=============================================
[2019-04-07 18:02:08,115] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1968698e-28 9.3428294e-27 2.8121935e-25 3.8829397e-18 1.2180992e-24
 1.0000000e+00 4.9158201e-19 1.4972000e-19], sum to 1.0000
[2019-04-07 18:02:08,115] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7716
[2019-04-07 18:02:08,220] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.9, 79.0, 0.0, 0.0, 24.0, 23.24600573192475, -0.1765061581482829, 0.0, 1.0, 46197.105656792184], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1891800.0000, 
sim time next is 1893600.0000, 
raw observation next is [-6.2, 75.0, 0.0, 0.0, 24.0, 23.17097032223834, -0.1922917693660889, 0.0, 1.0, 46130.72564406949], 
processed observation next is [0.0, 0.9565217391304348, 0.2908587257617729, 0.75, 0.0, 0.0, 0.5, 0.43091419351986165, 0.43590274354463704, 0.0, 1.0, 0.21967012211461662], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38340133], dtype=float32), -0.85521185]. 
=============================================
[2019-04-07 18:02:11,305] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4057381e-31 2.5111817e-30 8.4207886e-29 1.7680798e-20 3.4909476e-27
 1.0000000e+00 8.8063787e-22 6.3907215e-22], sum to 1.0000
[2019-04-07 18:02:11,307] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4898
[2019-04-07 18:02:11,364] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 96.5, 0.0, 0.0, 24.0, 23.87459230859811, 0.1669159830566828, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3191400.0000, 
sim time next is 3193200.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 24.0, 23.57575605446022, 0.1441213869441676, 0.0, 1.0, 78782.8387028728], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.5, 0.46464633787168513, 0.5480404623147225, 0.0, 1.0, 0.3751563747755848], 
reward next is 0.9106, 
noisyNet noise sample is [array([0.8081603], dtype=float32), -0.03382158]. 
=============================================
[2019-04-07 18:02:13,589] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5729453e-26 3.9370832e-25 4.0354353e-24 5.7394039e-17 7.5438461e-24
 1.0000000e+00 1.1113993e-18 2.1886920e-19], sum to 1.0000
[2019-04-07 18:02:13,589] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4857
[2019-04-07 18:02:13,671] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 68.0, 169.5, 80.0, 24.0, 23.99638617637481, -0.06886728110526177, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2286000.0000, 
sim time next is 2287800.0000, 
raw observation next is [-4.1, 63.0, 161.0, 110.0, 24.0, 23.94316418499431, -0.07496627275703603, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3490304709141275, 0.63, 0.5366666666666666, 0.12154696132596685, 0.5, 0.4952636820828591, 0.47501124241432136, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4572419], dtype=float32), 0.67078847]. 
=============================================
[2019-04-07 18:02:16,998] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.14024467e-26 1.50550643e-24 1.06157446e-23 9.05501726e-16
 9.11097742e-23 1.00000000e+00 4.01138393e-17 2.16032424e-18], sum to 1.0000
[2019-04-07 18:02:16,999] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6835
[2019-04-07 18:02:17,069] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 24.0, 23.46089055550422, -0.0378861579948712, 0.0, 1.0, 34812.72850533315], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4159800.0000, 
sim time next is 4161600.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 24.0, 23.41819685180872, -0.04887107000722738, 0.0, 1.0, 45667.82561457094], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.5, 0.0, 0.0, 0.5, 0.4515164043173933, 0.4837096433309242, 0.0, 1.0, 0.2174658362598616], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9493936], dtype=float32), 1.0055555]. 
=============================================
[2019-04-07 18:02:30,695] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:02:30,696] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:02:30,712] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run40
[2019-04-07 18:02:35,822] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.4905175e-29 1.2093645e-28 4.7393419e-27 3.2755667e-19 1.7789269e-25
 1.0000000e+00 1.8458760e-21 5.8949144e-22], sum to 1.0000
[2019-04-07 18:02:35,822] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9482
[2019-04-07 18:02:35,886] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 80.5, 0.0, 0.0, 24.0, 23.42551933678875, -0.01040182430523715, 0.0, 1.0, 31207.515096559055], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4750200.0000, 
sim time next is 4752000.0000, 
raw observation next is [-4.0, 84.0, 0.0, 0.0, 24.0, 23.42624096603502, -0.01006765987708691, 0.0, 1.0, 47038.71642462248], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.84, 0.0, 0.0, 0.5, 0.45218674716958507, 0.4966441133743043, 0.0, 1.0, 0.22399388773629753], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07814828], dtype=float32), 0.1695643]. 
=============================================
[2019-04-07 18:02:35,890] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[100.184525]
 [100.68727 ]
 [100.82324 ]
 [100.667694]
 [100.74889 ]], R is [[98.66751099]
 [98.68083954]
 [98.69403076]
 [98.60249329]
 [98.61647034]].
[2019-04-07 18:02:37,221] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:02:37,221] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:02:37,225] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run40
[2019-04-07 18:02:45,539] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.4660283e-26 3.6758598e-25 1.4029505e-22 9.3072695e-17 7.5083932e-23
 1.0000000e+00 5.5427404e-17 1.2238924e-18], sum to 1.0000
[2019-04-07 18:02:45,539] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5435
[2019-04-07 18:02:45,771] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 71.0, 174.0, 421.0, 24.0, 23.74505199730778, 0.0271874793372605, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4786200.0000, 
sim time next is 4788000.0000, 
raw observation next is [-3.0, 65.0, 163.5, 575.5, 24.0, 23.69051226993184, 0.02744617446743249, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3795013850415513, 0.65, 0.545, 0.6359116022099448, 0.5, 0.4742093558276534, 0.5091487248224775, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00026501], dtype=float32), 0.53366]. 
=============================================
[2019-04-07 18:02:45,797] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[97.43452]
 [98.02468]
 [98.2325 ]
 [96.99911]
 [96.39121]], R is [[97.24452972]
 [97.2720871 ]
 [97.29936981]
 [97.25102234]
 [97.27851105]].
[2019-04-07 18:02:49,098] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:02:49,099] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:02:49,105] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run40
[2019-04-07 18:02:52,351] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:02:52,351] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:02:52,355] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run40
[2019-04-07 18:02:55,141] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:02:55,141] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:02:55,145] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run40
[2019-04-07 18:02:56,775] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:02:56,776] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:02:56,780] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run40
[2019-04-07 18:02:59,041] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:02:59,041] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:02:59,044] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run40
[2019-04-07 18:02:59,506] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:02:59,506] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:02:59,510] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run40
[2019-04-07 18:03:00,077] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:03:00,077] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:03:00,080] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run40
[2019-04-07 18:03:03,718] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.99411133e-27 4.64467935e-27 5.15370718e-24 4.81778196e-18
 1.21672044e-23 1.00000000e+00 6.37466491e-18 1.10618346e-19], sum to 1.0000
[2019-04-07 18:03:03,718] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9289
[2019-04-07 18:03:03,796] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 24.0, 20.58943728148884, -0.7294043441150452, 0.0, 1.0, 42405.57484272158], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 10800.0000, 
sim time next is 12600.0000, 
raw observation next is [7.45, 94.5, 0.0, 0.0, 24.0, 20.75195243092939, -0.6874634758983964, 0.0, 1.0, 41825.57035195166], 
processed observation next is [0.0, 0.13043478260869565, 0.6689750692520776, 0.945, 0.0, 0.0, 0.5, 0.22932936924411576, 0.27084550803386787, 0.0, 1.0, 0.19916938262834125], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0605282], dtype=float32), -1.0156118]. 
=============================================
[2019-04-07 18:03:10,338] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-07 18:03:10,344] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:03:10,344] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:03:10,348] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run90
[2019-04-07 18:03:10,367] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:03:10,370] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:03:10,377] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 18:03:10,378] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:03:10,393] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run90
[2019-04-07 18:03:10,418] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run90
[2019-04-07 18:05:11,229] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11467717], dtype=float32), 0.15986678]
[2019-04-07 18:05:11,230] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [4.0, 53.0, 0.0, 0.0, 24.0, 23.67302240957969, 0.05974862152281817, 0.0, 1.0, 23525.885575300053]
[2019-04-07 18:05:11,230] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 18:05:11,231] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [3.6344411e-29 1.0580873e-27 7.0743555e-26 8.7236060e-19 1.5545404e-25
 1.0000000e+00 2.0411594e-19 7.9402893e-21], sampled 0.4553560693574691
[2019-04-07 18:05:32,541] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 18:05:51,846] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 18:05:53,103] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 18:05:54,126] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1780000, evaluation results [1780000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 18:06:12,296] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:06:12,296] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:06:12,300] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run41
[2019-04-07 18:06:19,193] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:06:19,193] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:06:19,197] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run41
[2019-04-07 18:06:21,138] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.3252421e-26 2.2150085e-25 4.0136044e-24 2.6341740e-17 1.3147012e-22
 1.0000000e+00 3.8447504e-18 8.7003426e-19], sum to 1.0000
[2019-04-07 18:06:21,138] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4523
[2019-04-07 18:06:21,260] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 34.0, 77.0, 630.0, 24.0, 24.71413763289782, 0.2423919045134091, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3945600.0000, 
sim time next is 3947400.0000, 
raw observation next is [-4.5, 36.0, 66.0, 536.0, 24.0, 24.92635867823773, 0.2354296765215386, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.36, 0.22, 0.5922651933701657, 0.5, 0.5771965565198108, 0.5784765588405129, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1882501], dtype=float32), -0.33073184]. 
=============================================
[2019-04-07 18:06:28,468] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6379592e-29 4.7152088e-29 3.1466595e-26 2.4812484e-19 5.3044161e-25
 1.0000000e+00 7.1091669e-20 1.6188082e-21], sum to 1.0000
[2019-04-07 18:06:28,468] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3374
[2019-04-07 18:06:28,618] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 77.0, 100.0, 675.0, 24.0, 24.54428334643686, 0.1108185894016948, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3749400.0000, 
sim time next is 3751200.0000, 
raw observation next is [-3.0, 77.0, 105.5, 722.0, 24.0, 24.69038272221522, 0.1503818252244234, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.77, 0.3516666666666667, 0.7977900552486188, 0.5, 0.5575318935179349, 0.5501272750748077, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1800394], dtype=float32), -0.79623705]. 
=============================================
[2019-04-07 18:06:54,353] A3C_AGENT_WORKER-Thread-15 INFO:Local step 113500, global step 1788498: loss 1.2626
[2019-04-07 18:06:54,354] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 113500, global step 1788498: learning rate 0.0000
[2019-04-07 18:06:58,403] A3C_AGENT_WORKER-Thread-12 INFO:Local step 113500, global step 1789201: loss 1.2962
[2019-04-07 18:06:58,406] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 113500, global step 1789202: learning rate 0.0000
[2019-04-07 18:07:08,698] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2684658e-29 6.0698956e-29 4.5977175e-27 1.2975587e-19 4.5283016e-27
 1.0000000e+00 3.2071636e-21 4.2946542e-22], sum to 1.0000
[2019-04-07 18:07:08,698] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2658
[2019-04-07 18:07:08,740] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3, 89.0, 0.0, 0.0, 24.0, 23.39636559947053, -0.003930653205715516, 0.0, 1.0, 51897.519495697044], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1740600.0000, 
sim time next is 1742400.0000, 
raw observation next is [-0.6, 87.0, 0.0, 0.0, 24.0, 23.38949185761991, -0.007220415165415305, 0.0, 1.0, 46287.81586802896], 
processed observation next is [0.0, 0.17391304347826086, 0.44598337950138506, 0.87, 0.0, 0.0, 0.5, 0.44912432146832576, 0.49759319494486154, 0.0, 1.0, 0.22041817080013792], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43320346], dtype=float32), -0.982566]. 
=============================================
[2019-04-07 18:07:12,045] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:07:12,045] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:07:12,086] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run41
[2019-04-07 18:07:16,528] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1275625e-26 5.1251763e-26 1.0769348e-24 4.9229939e-17 8.7233886e-23
 1.0000000e+00 5.5095962e-19 2.5935005e-19], sum to 1.0000
[2019-04-07 18:07:16,529] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0487
[2019-04-07 18:07:16,728] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.45, 77.0, 0.0, 0.0, 24.0, 22.85339650613684, -0.2095783065523679, 1.0, 1.0, 111721.10898292153], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1963800.0000, 
sim time next is 1965600.0000, 
raw observation next is [-5.0, 79.0, 0.0, 0.0, 24.0, 23.89112217801281, 0.004234608990552113, 1.0, 1.0, 63693.164101985494], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.79, 0.0, 0.0, 0.5, 0.49092684816773424, 0.501411536330184, 1.0, 1.0, 0.3033007814380262], 
reward next is 0.9824, 
noisyNet noise sample is [array([-0.6658618], dtype=float32), -0.7722166]. 
=============================================
[2019-04-07 18:07:27,003] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2627371e-27 2.8469374e-26 2.2139876e-24 6.8135500e-18 4.4694978e-23
 1.0000000e+00 4.3456326e-19 4.7973290e-20], sum to 1.0000
[2019-04-07 18:07:27,003] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9424
[2019-04-07 18:07:27,145] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 24.0, 23.61060877725942, 0.005558280377699232, 1.0, 1.0, 43249.65520857563], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2748600.0000, 
sim time next is 2750400.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 24.0, 23.48619379018855, -0.004396852693777357, 0.0, 1.0, 48926.58154435627], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.59, 0.0, 0.0, 0.5, 0.4571828158490459, 0.49853438243540754, 0.0, 1.0, 0.23298372163979175], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3734813], dtype=float32), 1.0111092]. 
=============================================
[2019-04-07 18:07:27,453] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:07:27,453] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:07:27,457] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run41
[2019-04-07 18:07:34,321] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:07:34,321] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:07:34,326] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run41
[2019-04-07 18:07:39,648] A3C_AGENT_WORKER-Thread-15 INFO:Local step 114000, global step 1796604: loss 0.3755
[2019-04-07 18:07:39,651] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 114000, global step 1796604: learning rate 0.0000
[2019-04-07 18:07:40,298] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0532232e-26 1.9372415e-25 2.2319139e-24 2.6474010e-17 1.3744864e-24
 1.0000000e+00 1.5685802e-18 1.2115363e-18], sum to 1.0000
[2019-04-07 18:07:40,298] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7575
[2019-04-07 18:07:40,393] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 24.0, 23.5501422115201, -0.06869445947808246, 0.0, 1.0, 32609.939135147157], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4230000.0000, 
sim time next is 4231800.0000, 
raw observation next is [1.5, 47.5, 0.0, 0.0, 24.0, 23.5204042800608, -0.06962828340365944, 0.0, 1.0, 43153.47124664352], 
processed observation next is [0.0, 1.0, 0.5041551246537397, 0.475, 0.0, 0.0, 0.5, 0.46003369000506655, 0.4767905721987802, 0.0, 1.0, 0.205492720222112], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2768017], dtype=float32), -2.1872466]. 
=============================================
[2019-04-07 18:07:43,815] A3C_AGENT_WORKER-Thread-12 INFO:Local step 114000, global step 1797204: loss 0.3795
[2019-04-07 18:07:43,815] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 114000, global step 1797204: learning rate 0.0000
[2019-04-07 18:07:51,603] A3C_AGENT_WORKER-Thread-16 INFO:Local step 113500, global step 1798275: loss 1.3706
[2019-04-07 18:07:51,621] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 113500, global step 1798275: learning rate 0.0000
[2019-04-07 18:08:03,570] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-07 18:08:03,576] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:08:03,577] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:08:03,580] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:08:03,581] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:08:03,581] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 18:08:03,581] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:08:03,585] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run91
[2019-04-07 18:08:03,616] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run91
[2019-04-07 18:08:03,637] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run91
[2019-04-07 18:08:33,131] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11444454], dtype=float32), 0.16007741]
[2019-04-07 18:08:33,131] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [-3.95, 42.0, 0.0, 0.0, 24.0, 23.09621713152985, -0.2475151002059336, 0.0, 1.0, 43098.08377223319]
[2019-04-07 18:08:33,131] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 18:08:33,132] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [3.3906393e-25 4.5227155e-24 1.9828520e-22 2.3625570e-16 4.1922409e-22
 1.0000000e+00 5.6820022e-17 3.5222645e-18], sampled 0.7748455667115209
[2019-04-07 18:10:29,989] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 18:10:48,548] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 18:10:52,639] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 18:10:53,662] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1800000, evaluation results [1800000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 18:10:58,431] A3C_AGENT_WORKER-Thread-6 INFO:Local step 113500, global step 1800639: loss 1.3533
[2019-04-07 18:10:58,432] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 113500, global step 1800639: learning rate 0.0000
[2019-04-07 18:11:03,123] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.54546855e-27 3.28115754e-26 3.89071030e-25 4.92558086e-19
 2.61377504e-24 1.00000000e+00 5.53559564e-18 1.20221227e-20], sum to 1.0000
[2019-04-07 18:11:03,124] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6637
[2019-04-07 18:11:03,177] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.0, 25.0, 0.0, 0.0, 24.0, 23.64665903091439, -0.07325040768247056, 0.0, 1.0, 24834.580764789414], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3650400.0000, 
sim time next is 3652200.0000, 
raw observation next is [9.5, 26.0, 0.0, 0.0, 24.0, 23.67148066518728, -0.08095048558813832, 0.0, 1.0, 18723.6695793788], 
processed observation next is [0.0, 0.2608695652173913, 0.7257617728531857, 0.26, 0.0, 0.0, 0.5, 0.47262338876560656, 0.4730165048039539, 0.0, 1.0, 0.08916033133037524], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1300336], dtype=float32), 0.5174175]. 
=============================================
[2019-04-07 18:11:05,699] A3C_AGENT_WORKER-Thread-19 INFO:Local step 113500, global step 1801611: loss 1.3737
[2019-04-07 18:11:05,700] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 113500, global step 1801611: learning rate 0.0000
[2019-04-07 18:11:07,313] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:11:07,313] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:11:07,316] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run41
[2019-04-07 18:11:12,031] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2639281e-26 2.1485406e-24 1.9432015e-23 1.7645452e-17 2.8367839e-23
 1.0000000e+00 2.1867664e-17 1.1093365e-18], sum to 1.0000
[2019-04-07 18:11:12,031] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0819
[2019-04-07 18:11:12,272] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 45.0, 42.5, 37.0, 24.0, 23.0598651451444, -0.1610923869346757, 0.0, 1.0, 36419.7183435918], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2394000.0000, 
sim time next is 2395800.0000, 
raw observation next is [-1.15, 44.5, 0.0, 0.0, 24.0, 23.05677331799215, -0.1763931699318679, 0.0, 1.0, 37596.79445239987], 
processed observation next is [0.0, 0.7391304347826086, 0.4307479224376732, 0.445, 0.0, 0.0, 0.5, 0.4213977764993458, 0.44120227668937734, 0.0, 1.0, 0.17903235453523747], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7524831], dtype=float32), 1.9905623]. 
=============================================
[2019-04-07 18:11:16,053] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.9701404e-31 2.1661876e-29 2.5994538e-26 1.1512892e-19 4.7118693e-27
 1.0000000e+00 6.0662397e-21 1.0800008e-22], sum to 1.0000
[2019-04-07 18:11:16,053] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3480
[2019-04-07 18:11:16,110] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 24.0, 21.02428530240748, -0.6260026937834978, 0.0, 1.0, 40759.21811890019], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 18000.0000, 
sim time next is 19800.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 24.0, 21.15439888979542, -0.5988358517197008, 0.0, 1.0, 40521.57259797563], 
processed observation next is [0.0, 0.21739130434782608, 0.6759002770083103, 0.93, 0.0, 0.0, 0.5, 0.26286657414961834, 0.3003880494267664, 0.0, 1.0, 0.19295986951416969], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06677432], dtype=float32), -0.64291596]. 
=============================================
[2019-04-07 18:11:23,858] A3C_AGENT_WORKER-Thread-15 INFO:Local step 114500, global step 1804182: loss 1.1186
[2019-04-07 18:11:23,859] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 114500, global step 1804182: learning rate 0.0000
[2019-04-07 18:11:25,632] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6994190e-27 7.7529607e-26 6.2313018e-24 6.5225007e-18 5.0103159e-24
 1.0000000e+00 2.1124378e-19 4.4553101e-19], sum to 1.0000
[2019-04-07 18:11:25,635] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9895
[2019-04-07 18:11:25,827] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 54.0, 0.0, 0.0, 24.0, 23.72825611869406, 0.08613407806725781, 1.0, 1.0, 98800.78685577621], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2743200.0000, 
sim time next is 2745000.0000, 
raw observation next is [-4.5, 56.5, 0.0, 0.0, 24.0, 23.7302790764661, -0.04654417544028309, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3379501385041552, 0.565, 0.0, 0.0, 0.5, 0.477523256372175, 0.48448527485323895, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2826856], dtype=float32), -1.7033889]. 
=============================================
[2019-04-07 18:11:25,833] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[93.604034]
 [93.046906]
 [93.59866 ]
 [94.37781 ]
 [94.81435 ]], R is [[93.2634201 ]
 [93.14601898]
 [93.18881989]
 [93.25693512]
 [93.32436371]].
[2019-04-07 18:11:27,552] A3C_AGENT_WORKER-Thread-12 INFO:Local step 114500, global step 1804753: loss 1.1562
[2019-04-07 18:11:27,553] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 114500, global step 1804753: learning rate 0.0000
[2019-04-07 18:11:31,092] A3C_AGENT_WORKER-Thread-16 INFO:Local step 114000, global step 1805326: loss 0.3385
[2019-04-07 18:11:31,101] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 114000, global step 1805326: learning rate 0.0000
[2019-04-07 18:11:39,068] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.9482544e-27 4.2216486e-26 4.1515616e-23 1.4488255e-17 1.1973003e-22
 1.0000000e+00 5.8734425e-19 7.3182944e-19], sum to 1.0000
[2019-04-07 18:11:39,070] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4559
[2019-04-07 18:11:39,204] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 24.0, 23.73670582477465, 0.02921278266239188, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3353400.0000, 
sim time next is 3355200.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 24.0, 23.39063651727228, 0.0180948816609291, 1.0, 1.0, 78549.16571634008], 
processed observation next is [1.0, 0.8695652173913043, 0.3795013850415513, 0.55, 0.0, 0.0, 0.5, 0.44921970977269005, 0.5060316272203097, 1.0, 1.0, 0.3740436462682861], 
reward next is 0.9117, 
noisyNet noise sample is [array([0.04802431], dtype=float32), -0.61370975]. 
=============================================
[2019-04-07 18:11:47,834] A3C_AGENT_WORKER-Thread-6 INFO:Local step 114000, global step 1808277: loss 0.3535
[2019-04-07 18:11:47,835] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 114000, global step 1808277: learning rate 0.0000
[2019-04-07 18:11:47,856] A3C_AGENT_WORKER-Thread-14 INFO:Local step 113500, global step 1808284: loss 1.3335
[2019-04-07 18:11:47,884] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 113500, global step 1808285: learning rate 0.0000
[2019-04-07 18:11:54,360] A3C_AGENT_WORKER-Thread-19 INFO:Local step 114000, global step 1809531: loss 0.3838
[2019-04-07 18:11:54,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 114000, global step 1809531: learning rate 0.0000
[2019-04-07 18:12:01,828] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3965714e-27 1.1624071e-24 3.7960634e-24 1.3317606e-17 8.4162541e-23
 1.0000000e+00 2.5390749e-17 2.5254233e-19], sum to 1.0000
[2019-04-07 18:12:01,829] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1193
[2019-04-07 18:12:01,923] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 24.0, 23.28306184313328, -0.08449117033396698, 0.0, 1.0, 47538.80009082138], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2763000.0000, 
sim time next is 2764800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 24.0, 23.26968209874418, -0.09533554355438713, 0.0, 1.0, 45930.20528366213], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.5, 0.43914017489534835, 0.4682214854818709, 0.0, 1.0, 0.21871526325553395], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5067543], dtype=float32), 0.27007753]. 
=============================================
[2019-04-07 18:12:03,693] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.5518759e-26 5.3056507e-25 1.3407949e-22 2.6991131e-16 3.0752161e-22
 1.0000000e+00 1.8005012e-17 2.9356965e-18], sum to 1.0000
[2019-04-07 18:12:03,693] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3032
[2019-04-07 18:12:03,766] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 39.5, 0.0, 0.0, 24.0, 23.25507778704125, -0.152340572687935, 0.0, 1.0, 42353.5077720647], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4084200.0000, 
sim time next is 4086000.0000, 
raw observation next is [-5.0, 41.0, 0.0, 0.0, 24.0, 23.27183272009962, -0.1582673607237633, 0.0, 1.0, 41347.04772114514], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.41, 0.0, 0.0, 0.5, 0.43931939334163506, 0.4472442130920789, 0.0, 1.0, 0.19689070343402448], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4408234], dtype=float32), -1.3095616]. 
=============================================
[2019-04-07 18:12:03,774] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[87.41103]
 [87.42645]
 [87.21159]
 [87.35617]
 [87.45297]], R is [[87.519104  ]
 [87.64391327]
 [87.76145172]
 [87.88383484]
 [88.00499725]].
[2019-04-07 18:12:08,131] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:12:08,133] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:12:08,199] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run41
[2019-04-07 18:12:14,149] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1690515e-29 2.8203984e-27 4.2330011e-26 2.9961685e-18 1.8157588e-24
 1.0000000e+00 3.6490742e-19 2.0308906e-20], sum to 1.0000
[2019-04-07 18:12:14,149] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4755
[2019-04-07 18:12:14,211] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 88.5, 0.0, 0.0, 24.0, 23.15880714291628, -0.1376264302384556, 0.0, 1.0, 44000.38871572329], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2079000.0000, 
sim time next is 2080800.0000, 
raw observation next is [-4.5, 86.0, 0.0, 0.0, 24.0, 23.13533139492483, -0.1455271141028645, 0.0, 1.0, 43950.79951711506], 
processed observation next is [1.0, 0.08695652173913043, 0.3379501385041552, 0.86, 0.0, 0.0, 0.5, 0.4279442829104025, 0.45149096196571187, 0.0, 1.0, 0.2092895215100717], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.45112294], dtype=float32), -0.67736346]. 
=============================================
[2019-04-07 18:12:16,247] A3C_AGENT_WORKER-Thread-15 INFO:Local step 115000, global step 1813439: loss 34.0765
[2019-04-07 18:12:16,248] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 115000, global step 1813439: learning rate 0.0000
[2019-04-07 18:12:20,198] A3C_AGENT_WORKER-Thread-12 INFO:Local step 115000, global step 1814165: loss 34.0889
[2019-04-07 18:12:20,198] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 115000, global step 1814165: learning rate 0.0000
[2019-04-07 18:12:21,836] A3C_AGENT_WORKER-Thread-16 INFO:Local step 114500, global step 1814469: loss 1.2237
[2019-04-07 18:12:21,839] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 114500, global step 1814469: learning rate 0.0000
[2019-04-07 18:12:27,424] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0975830e-26 4.2482255e-25 1.3463861e-23 7.4977689e-17 2.3363605e-23
 1.0000000e+00 7.3451761e-18 1.8759192e-19], sum to 1.0000
[2019-04-07 18:12:27,424] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4393
[2019-04-07 18:12:27,473] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 24.0, 22.87543435258983, -0.1899784575844363, 0.0, 1.0, 46978.521345671536], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 165600.0000, 
sim time next is 167400.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 24.0, 22.82690701781876, -0.2047717574014153, 0.0, 1.0, 46654.26369172938], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.5, 0.4022422514848965, 0.4317427475328615, 0.0, 1.0, 0.22216316043680656], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6011608], dtype=float32), -0.55559367]. 
=============================================
[2019-04-07 18:12:33,966] A3C_AGENT_WORKER-Thread-14 INFO:Local step 114000, global step 1816598: loss 0.3488
[2019-04-07 18:12:33,967] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 114000, global step 1816598: learning rate 0.0000
[2019-04-07 18:12:34,927] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0179298e-30 3.0577321e-27 2.1406497e-28 1.6546153e-19 2.0147649e-26
 1.0000000e+00 1.0834811e-19 3.0897870e-21], sum to 1.0000
[2019-04-07 18:12:34,928] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8251
[2019-04-07 18:12:34,987] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 49.0, 75.0, 606.0, 24.0, 25.05426911725025, 0.325052301325305, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3513600.0000, 
sim time next is 3515400.0000, 
raw observation next is [3.0, 49.0, 62.0, 525.0, 24.0, 25.11520613449399, 0.3413112080404365, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.20666666666666667, 0.580110497237569, 0.5, 0.5929338445411659, 0.6137704026801455, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1233228], dtype=float32), 0.108059876]. 
=============================================
[2019-04-07 18:12:37,665] A3C_AGENT_WORKER-Thread-6 INFO:Local step 114500, global step 1817335: loss 1.1470
[2019-04-07 18:12:37,665] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 114500, global step 1817335: learning rate 0.0000
[2019-04-07 18:12:41,790] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2313261e-30 3.0696598e-30 8.5455409e-28 6.4718592e-20 1.8553220e-28
 1.0000000e+00 2.3441219e-20 1.6561535e-22], sum to 1.0000
[2019-04-07 18:12:41,790] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1861
[2019-04-07 18:12:41,810] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.1, 66.0, 0.0, 0.0, 24.0, 24.11214764264484, 0.1711854819282446, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4413600.0000, 
sim time next is 4415400.0000, 
raw observation next is [5.55, 66.5, 0.0, 0.0, 24.0, 23.98279417242411, 0.1404853977869814, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6163434903047093, 0.665, 0.0, 0.0, 0.5, 0.49856618103534256, 0.5468284659289938, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16339746], dtype=float32), -0.70971954]. 
=============================================
[2019-04-07 18:12:44,646] A3C_AGENT_WORKER-Thread-19 INFO:Local step 114500, global step 1818806: loss 1.0963
[2019-04-07 18:12:44,646] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 114500, global step 1818806: learning rate 0.0000
[2019-04-07 18:12:44,861] A3C_AGENT_WORKER-Thread-18 INFO:Local step 113500, global step 1818848: loss 1.3671
[2019-04-07 18:12:44,862] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 113500, global step 1818848: learning rate 0.0000
[2019-04-07 18:12:47,470] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:12:47,470] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:12:47,475] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run41
[2019-04-07 18:12:50,588] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-07 18:12:50,589] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:12:50,589] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:12:50,593] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:12:50,593] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:12:50,596] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run92
[2019-04-07 18:12:50,620] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run92
[2019-04-07 18:12:50,643] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 18:12:50,644] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:12:50,665] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run92
[2019-04-07 18:15:15,244] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 18:15:33,814] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 18:15:37,494] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 18:15:38,516] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1820000, evaluation results [1820000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 18:15:40,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.5035367e-30 9.1937642e-29 2.3713948e-25 6.8884732e-19 6.9745417e-26
 1.0000000e+00 5.5368362e-20 4.0843339e-21], sum to 1.0000
[2019-04-07 18:15:40,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3830
[2019-04-07 18:15:40,889] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.4, 63.0, 0.0, 0.0, 24.0, 23.54747983230766, -0.004891265456536838, 0.0, 1.0, 34632.017311252166], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4582800.0000, 
sim time next is 4584600.0000, 
raw observation next is [0.1, 64.0, 0.0, 0.0, 24.0, 23.55938374297265, -0.006342750981502175, 0.0, 1.0, 33824.51350030176], 
processed observation next is [1.0, 0.043478260869565216, 0.4653739612188367, 0.64, 0.0, 0.0, 0.5, 0.4632819785810541, 0.4978857496728326, 0.0, 1.0, 0.16106911190619885], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0901458], dtype=float32), -0.91706413]. 
=============================================
[2019-04-07 18:15:41,023] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8059026e-27 2.0105652e-26 3.1500513e-23 7.6153432e-18 2.2311308e-24
 1.0000000e+00 5.7579288e-19 2.4052834e-20], sum to 1.0000
[2019-04-07 18:15:41,023] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2587
[2019-04-07 18:15:41,066] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 65.0, 0.0, 0.0, 24.0, 23.40788930325032, -0.07645108170271317, 0.0, 1.0, 32838.41183860756], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2332800.0000, 
sim time next is 2334600.0000, 
raw observation next is [-2.3, 63.5, 0.0, 0.0, 24.0, 23.37369396525079, -0.1030718567552442, 0.0, 1.0, 37822.204799585794], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.635, 0.0, 0.0, 0.5, 0.4478078304375659, 0.4656427144149186, 0.0, 1.0, 0.18010573714088474], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9790453], dtype=float32), -0.4871064]. 
=============================================
[2019-04-07 18:15:41,801] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:15:41,801] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:15:41,804] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run41
[2019-04-07 18:15:43,816] A3C_AGENT_WORKER-Thread-15 INFO:Local step 115500, global step 1820913: loss 0.7280
[2019-04-07 18:15:43,816] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 115500, global step 1820913: learning rate 0.0000
[2019-04-07 18:15:46,872] A3C_AGENT_WORKER-Thread-12 INFO:Local step 115500, global step 1821380: loss 0.7231
[2019-04-07 18:15:46,873] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 115500, global step 1821380: learning rate 0.0000
[2019-04-07 18:15:47,803] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5383793e-29 1.2223231e-28 3.4329208e-27 5.5425838e-20 2.6686209e-26
 1.0000000e+00 1.6036147e-21 4.5763443e-23], sum to 1.0000
[2019-04-07 18:15:47,804] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3981
[2019-04-07 18:15:47,846] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 73.0, 165.5, 3.0, 24.0, 23.52184897871008, 0.03316491400379574, 1.0, 1.0, 73945.92767145095], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4716000.0000, 
sim time next is 4717800.0000, 
raw observation next is [1.5, 72.5, 196.0, 6.0, 24.0, 24.24506538152679, 0.09802529492880645, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5041551246537397, 0.725, 0.6533333333333333, 0.0066298342541436465, 0.5, 0.5204221151272325, 0.5326750983096021, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23663406], dtype=float32), -1.6020994]. 
=============================================
[2019-04-07 18:15:51,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.9426648e-26 8.0885506e-24 3.5172645e-21 5.3704901e-16 1.5615321e-22
 1.0000000e+00 7.9117898e-18 2.6765840e-18], sum to 1.0000
[2019-04-07 18:15:51,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0761
[2019-04-07 18:15:51,921] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 38.0, 0.0, 0.0, 24.0, 23.86500267031317, -0.01362158623272577, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4912200.0000, 
sim time next is 4914000.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 24.0, 23.77819985615671, -0.04739261017888016, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 0.5, 0.4815166546797259, 0.4842024632737066, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.71546257], dtype=float32), -1.4501686]. 
=============================================
[2019-04-07 18:15:51,965] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[93.5813  ]
 [94.1202  ]
 [93.850655]
 [92.67403 ]
 [92.71278 ]], R is [[93.02826691]
 [93.09798431]
 [93.16700745]
 [92.86091614]
 [92.93230438]].
[2019-04-07 18:15:53,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0559703e-27 3.7908643e-26 5.8667573e-23 5.0978064e-16 2.2647029e-23
 1.0000000e+00 1.3623948e-19 2.6427164e-19], sum to 1.0000
[2019-04-07 18:15:53,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2737
[2019-04-07 18:15:53,746] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.6, 75.0, 201.5, 123.0, 24.0, 23.93088299040279, -0.09463405833785486, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1940400.0000, 
sim time next is 1942200.0000, 
raw observation next is [-5.3, 70.0, 232.0, 10.0, 24.0, 23.93751747239969, -0.1080390293977777, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.31578947368421056, 0.7, 0.7733333333333333, 0.011049723756906077, 0.5, 0.4947931226999742, 0.46398699020074075, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4370348], dtype=float32), -0.3176664]. 
=============================================
[2019-04-07 18:16:00,129] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:16:00,129] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:16:00,137] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run41
[2019-04-07 18:16:00,257] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:16:00,257] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:16:00,267] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run41
[2019-04-07 18:16:00,352] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:16:00,352] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:16:00,356] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run41
[2019-04-07 18:16:00,416] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:16:00,416] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:16:00,430] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run41
[2019-04-07 18:16:02,595] A3C_AGENT_WORKER-Thread-16 INFO:Local step 115000, global step 1823970: loss 33.2043
[2019-04-07 18:16:02,603] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 115000, global step 1823970: learning rate 0.0000
[2019-04-07 18:16:03,677] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:16:03,677] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:16:03,680] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run41
[2019-04-07 18:16:04,802] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:16:04,802] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:16:04,806] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run41
[2019-04-07 18:16:07,733] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:16:07,733] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:16:07,737] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run41
[2019-04-07 18:16:10,689] A3C_AGENT_WORKER-Thread-2 INFO:Local step 113500, global step 1824865: loss 1.3488
[2019-04-07 18:16:10,690] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 113500, global step 1824865: learning rate 0.0000
[2019-04-07 18:16:11,961] A3C_AGENT_WORKER-Thread-14 INFO:Local step 114500, global step 1825003: loss 1.0744
[2019-04-07 18:16:11,961] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 114500, global step 1825003: learning rate 0.0000
[2019-04-07 18:16:18,186] A3C_AGENT_WORKER-Thread-6 INFO:Local step 115000, global step 1825887: loss 33.8038
[2019-04-07 18:16:18,188] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 115000, global step 1825887: learning rate 0.0000
[2019-04-07 18:16:18,782] A3C_AGENT_WORKER-Thread-18 INFO:Local step 114000, global step 1825975: loss 0.3304
[2019-04-07 18:16:18,783] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 114000, global step 1825975: learning rate 0.0000
[2019-04-07 18:16:20,659] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6325229e-27 4.3709689e-27 1.2275591e-24 4.0315721e-17 2.1314627e-24
 1.0000000e+00 4.0574724e-19 3.5739637e-20], sum to 1.0000
[2019-04-07 18:16:20,659] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0846
[2019-04-07 18:16:20,745] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.5, 65.0, 0.0, 0.0, 24.0, 23.40571844460761, -0.06965827597148914, 0.0, 1.0, 72248.38818350075], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3013200.0000, 
sim time next is 3015000.0000, 
raw observation next is [-3.75, 65.0, 0.0, 0.0, 24.0, 23.43690126162169, -0.07442679232062639, 0.0, 1.0, 33872.499877905524], 
processed observation next is [0.0, 0.9130434782608695, 0.3587257617728532, 0.65, 0.0, 0.0, 0.5, 0.4530751051351407, 0.4751910692264579, 0.0, 1.0, 0.1612976184662168], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.55315524], dtype=float32), 0.13188566]. 
=============================================
[2019-04-07 18:16:20,798] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[95.77848 ]
 [95.30241 ]
 [95.32983 ]
 [94.982254]
 [93.63113 ]], R is [[95.48940277]
 [95.47618103]
 [95.52142334]
 [95.56620789]
 [95.27539062]].
[2019-04-07 18:16:21,168] A3C_AGENT_WORKER-Thread-4 INFO:Local step 113500, global step 1826299: loss 1.3478
[2019-04-07 18:16:21,169] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 113500, global step 1826299: learning rate 0.0000
[2019-04-07 18:16:23,469] A3C_AGENT_WORKER-Thread-15 INFO:Local step 116000, global step 1826664: loss 0.3485
[2019-04-07 18:16:23,469] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 116000, global step 1826664: learning rate 0.0000
[2019-04-07 18:16:24,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5698413e-28 7.7341150e-26 1.6441665e-24 2.1847641e-18 2.3993872e-23
 1.0000000e+00 2.8878558e-19 9.8542576e-21], sum to 1.0000
[2019-04-07 18:16:24,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8509
[2019-04-07 18:16:24,525] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 24.0, 23.46362533256185, -0.02622657789941994, 1.0, 1.0, 103258.8203528324], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 147600.0000, 
sim time next is 149400.0000, 
raw observation next is [-7.3, 66.0, 0.0, 0.0, 24.0, 24.03172284014539, -0.03949193154156055, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.26038781163434904, 0.66, 0.0, 0.0, 0.5, 0.502643570012116, 0.48683602281947985, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33978483], dtype=float32), -1.5226127]. 
=============================================
[2019-04-07 18:16:24,871] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2193762e-27 1.5374437e-25 2.8853694e-25 3.1347052e-17 1.7190602e-24
 1.0000000e+00 2.6443515e-18 7.4396644e-20], sum to 1.0000
[2019-04-07 18:16:24,872] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6410
[2019-04-07 18:16:24,990] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 56.0, 57.0, 486.0, 24.0, 23.32948084969151, -0.07583863293931653, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3083400.0000, 
sim time next is 3085200.0000, 
raw observation next is [0.0, 72.0, 32.0, 287.0, 24.0, 23.25817891869686, -0.1095358024971488, 0.0, 1.0, 18681.17194629402], 
processed observation next is [0.0, 0.7391304347826086, 0.46260387811634357, 0.72, 0.10666666666666667, 0.31712707182320443, 0.5, 0.43818157655807166, 0.4634880658342837, 0.0, 1.0, 0.08895796164901915], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1566149], dtype=float32), -0.21763681]. 
=============================================
[2019-04-07 18:16:25,701] A3C_AGENT_WORKER-Thread-12 INFO:Local step 116000, global step 1826932: loss 0.4161
[2019-04-07 18:16:25,702] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 116000, global step 1826932: learning rate 0.0000
[2019-04-07 18:16:25,990] A3C_AGENT_WORKER-Thread-19 INFO:Local step 115000, global step 1826975: loss 33.8832
[2019-04-07 18:16:25,991] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 115000, global step 1826975: learning rate 0.0000
[2019-04-07 18:16:33,115] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:16:33,115] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:16:33,119] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run42
[2019-04-07 18:16:34,671] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:16:34,672] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:16:34,675] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run42
[2019-04-07 18:16:39,714] A3C_AGENT_WORKER-Thread-10 INFO:Local step 113500, global step 1828815: loss 1.3864
[2019-04-07 18:16:39,714] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 113500, global step 1828815: learning rate 0.0000
[2019-04-07 18:16:41,570] A3C_AGENT_WORKER-Thread-20 INFO:Local step 113500, global step 1828993: loss 1.3817
[2019-04-07 18:16:41,571] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 113500, global step 1828993: learning rate 0.0000
[2019-04-07 18:16:42,498] A3C_AGENT_WORKER-Thread-5 INFO:Local step 113500, global step 1829091: loss 1.3140
[2019-04-07 18:16:42,498] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 113500, global step 1829091: learning rate 0.0000
[2019-04-07 18:16:42,953] A3C_AGENT_WORKER-Thread-3 INFO:Local step 113500, global step 1829141: loss 1.3760
[2019-04-07 18:16:42,961] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 113500, global step 1829141: learning rate 0.0000
[2019-04-07 18:16:44,212] A3C_AGENT_WORKER-Thread-16 INFO:Local step 115500, global step 1829299: loss 0.6357
[2019-04-07 18:16:44,231] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 115500, global step 1829299: learning rate 0.0000
[2019-04-07 18:16:44,779] A3C_AGENT_WORKER-Thread-11 INFO:Local step 113500, global step 1829361: loss 1.3436
[2019-04-07 18:16:44,779] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 113500, global step 1829361: learning rate 0.0000
[2019-04-07 18:16:45,996] A3C_AGENT_WORKER-Thread-13 INFO:Local step 113500, global step 1829520: loss 1.2549
[2019-04-07 18:16:45,996] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 113500, global step 1829520: learning rate 0.0000
[2019-04-07 18:16:50,007] A3C_AGENT_WORKER-Thread-17 INFO:Local step 113500, global step 1830050: loss 1.4071
[2019-04-07 18:16:50,007] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 113500, global step 1830050: learning rate 0.0000
[2019-04-07 18:16:57,620] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0458685e-26 1.5592917e-25 2.2589480e-23 1.1274128e-17 1.6188499e-23
 1.0000000e+00 1.6861310e-18 6.2281096e-19], sum to 1.0000
[2019-04-07 18:16:57,620] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2151
[2019-04-07 18:16:57,809] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 68.0, 135.0, 51.0, 24.0, 23.33432192874583, -0.1719496261325922, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 639000.0000, 
sim time next is 640800.0000, 
raw observation next is [-3.9, 65.0, 117.5, 25.5, 24.0, 23.1182338409543, -0.2224823914875851, 0.0, 1.0, 8419.94860832634], 
processed observation next is [0.0, 0.43478260869565216, 0.3545706371191136, 0.65, 0.39166666666666666, 0.0281767955801105, 0.5, 0.42651948674619167, 0.4258392028374716, 0.0, 1.0, 0.04009499337298257], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2461777], dtype=float32), -0.6922207]. 
=============================================
[2019-04-07 18:16:58,107] A3C_AGENT_WORKER-Thread-6 INFO:Local step 115500, global step 1831227: loss 0.7951
[2019-04-07 18:16:58,107] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 115500, global step 1831227: learning rate 0.0000
[2019-04-07 18:16:59,813] A3C_AGENT_WORKER-Thread-2 INFO:Local step 114000, global step 1831493: loss 0.3180
[2019-04-07 18:16:59,813] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 114000, global step 1831493: learning rate 0.0000
[2019-04-07 18:17:05,782] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6517715e-28 4.5781358e-26 1.9031457e-24 6.8697893e-19 2.1554035e-24
 1.0000000e+00 1.7193738e-18 3.4633851e-19], sum to 1.0000
[2019-04-07 18:17:05,782] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8477
[2019-04-07 18:17:05,906] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.1, 75.0, 0.0, 0.0, 24.0, 23.18847606385962, -0.2273105053921235, 0.0, 1.0, 42419.57307319805], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 703800.0000, 
sim time next is 705600.0000, 
raw observation next is [-2.8, 75.0, 0.0, 0.0, 24.0, 23.09982403664581, -0.2442847007688024, 0.0, 1.0, 42490.56530090143], 
processed observation next is [1.0, 0.17391304347826086, 0.38504155124653744, 0.75, 0.0, 0.0, 0.5, 0.4249853363871508, 0.41857176641039917, 0.0, 1.0, 0.20233602524238775], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5034379], dtype=float32), -0.5498235]. 
=============================================
[2019-04-07 18:17:07,739] A3C_AGENT_WORKER-Thread-14 INFO:Local step 115000, global step 1832672: loss 33.5453
[2019-04-07 18:17:07,740] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 115000, global step 1832672: learning rate 0.0000
[2019-04-07 18:17:07,914] A3C_AGENT_WORKER-Thread-19 INFO:Local step 115500, global step 1832701: loss 0.7368
[2019-04-07 18:17:07,915] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 115500, global step 1832701: learning rate 0.0000
[2019-04-07 18:17:11,616] A3C_AGENT_WORKER-Thread-4 INFO:Local step 114000, global step 1833313: loss 0.3050
[2019-04-07 18:17:11,616] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 114000, global step 1833313: learning rate 0.0000
[2019-04-07 18:17:13,714] A3C_AGENT_WORKER-Thread-18 INFO:Local step 114500, global step 1833665: loss 1.1863
[2019-04-07 18:17:13,725] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 114500, global step 1833665: learning rate 0.0000
[2019-04-07 18:17:22,805] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7233209e-32 8.6770874e-31 1.1536316e-28 3.5155608e-21 1.8109415e-27
 1.0000000e+00 3.7980659e-22 2.7657050e-23], sum to 1.0000
[2019-04-07 18:17:22,805] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3500
[2019-04-07 18:17:22,856] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.15, 81.0, 0.0, 0.0, 24.0, 23.64693547121273, -0.006627228555247384, 0.0, 1.0, 6364.220790626364], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 959400.0000, 
sim time next is 961200.0000, 
raw observation next is [7.7, 80.0, 0.0, 0.0, 24.0, 23.50750071384315, -0.005550045009981209, 0.0, 1.0, 59436.42077483015], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.8, 0.0, 0.0, 0.5, 0.4589583928202625, 0.4981499849966729, 0.0, 1.0, 0.2830305751182388], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0681353], dtype=float32), 0.6896281]. 
=============================================
[2019-04-07 18:17:24,322] A3C_AGENT_WORKER-Thread-16 INFO:Local step 116000, global step 1835469: loss 0.3155
[2019-04-07 18:17:24,324] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 116000, global step 1835469: learning rate 0.0000
[2019-04-07 18:17:27,861] A3C_AGENT_WORKER-Thread-10 INFO:Local step 114000, global step 1836387: loss 0.3150
[2019-04-07 18:17:27,862] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 114000, global step 1836387: learning rate 0.0000
[2019-04-07 18:17:30,621] A3C_AGENT_WORKER-Thread-20 INFO:Local step 114000, global step 1837088: loss 0.3199
[2019-04-07 18:17:30,621] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 114000, global step 1837088: learning rate 0.0000
[2019-04-07 18:17:31,147] A3C_AGENT_WORKER-Thread-5 INFO:Local step 114000, global step 1837208: loss 0.3184
[2019-04-07 18:17:31,147] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 114000, global step 1837208: learning rate 0.0000
[2019-04-07 18:17:32,100] A3C_AGENT_WORKER-Thread-3 INFO:Local step 114000, global step 1837417: loss 0.3180
[2019-04-07 18:17:32,110] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 114000, global step 1837417: learning rate 0.0000
[2019-04-07 18:17:32,289] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:17:32,289] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:17:32,328] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run42
[2019-04-07 18:17:32,428] A3C_AGENT_WORKER-Thread-11 INFO:Local step 114000, global step 1837494: loss 0.3095
[2019-04-07 18:17:32,429] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 114000, global step 1837494: learning rate 0.0000
[2019-04-07 18:17:34,175] A3C_AGENT_WORKER-Thread-13 INFO:Local step 114000, global step 1837804: loss 0.3055
[2019-04-07 18:17:34,176] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 114000, global step 1837804: learning rate 0.0000
[2019-04-07 18:17:35,317] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9573351e-32 5.5149879e-31 1.7646429e-28 6.0645243e-22 1.9970020e-29
 1.0000000e+00 1.0440335e-21 7.7938269e-25], sum to 1.0000
[2019-04-07 18:17:35,318] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6058
[2019-04-07 18:17:35,369] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 24.0, 23.63928295598618, 0.1746749887814976, 0.0, 1.0, 24878.957741019538], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1290600.0000, 
sim time next is 1292400.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 24.0, 23.648031668378, 0.1620284637468383, 0.0, 1.0, 18831.845292679955], 
processed observation next is [0.0, 1.0, 0.6149584487534627, 1.0, 0.0, 0.0, 0.5, 0.4706693056981666, 0.5540094879156128, 0.0, 1.0, 0.08967545377466644], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1595612], dtype=float32), 0.25065064]. 
=============================================
[2019-04-07 18:17:35,571] A3C_AGENT_WORKER-Thread-17 INFO:Local step 114000, global step 1838051: loss 0.3226
[2019-04-07 18:17:35,572] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 114000, global step 1838051: learning rate 0.0000
[2019-04-07 18:17:36,538] A3C_AGENT_WORKER-Thread-6 INFO:Local step 116000, global step 1838229: loss 0.4097
[2019-04-07 18:17:36,541] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 116000, global step 1838229: learning rate 0.0000
[2019-04-07 18:17:38,620] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2380725e-29 2.6867589e-28 2.3248970e-26 3.9136011e-19 4.3342437e-26
 1.0000000e+00 1.1813054e-19 9.3880839e-22], sum to 1.0000
[2019-04-07 18:17:38,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4373
[2019-04-07 18:17:38,692] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 93.0, 0.0, 24.0, 23.47587307060972, -0.002456704381510558, 1.0, 1.0, 26843.434755986993], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1429200.0000, 
sim time next is 1431000.0000, 
raw observation next is [0.8, 92.0, 90.0, 0.0, 24.0, 23.47699962898454, 0.03663942977607146, 1.0, 1.0, 36109.032347083725], 
processed observation next is [1.0, 0.5652173913043478, 0.4847645429362882, 0.92, 0.3, 0.0, 0.5, 0.45641663574871166, 0.5122131432586905, 1.0, 1.0, 0.17194777308135106], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3405261], dtype=float32), -0.6887306]. 
=============================================
[2019-04-07 18:17:38,721] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[107.344696]
 [107.53534 ]
 [107.80846 ]
 [107.89604 ]
 [107.82355 ]], R is [[107.24594879]
 [107.17349243]
 [107.10176086]
 [107.03074646]
 [106.96044159]].
[2019-04-07 18:17:45,121] A3C_AGENT_WORKER-Thread-19 INFO:Local step 116000, global step 1839684: loss 0.3653
[2019-04-07 18:17:45,124] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 116000, global step 1839684: learning rate 0.0000
[2019-04-07 18:17:45,429] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:17:45,429] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:17:45,433] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run42
[2019-04-07 18:17:46,509] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-07 18:17:46,510] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:17:46,510] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:17:46,511] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:17:46,511] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:17:46,514] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 18:17:46,516] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run93
[2019-04-07 18:17:46,541] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:17:46,545] A3C_AGENT_WORKER-Thread-14 INFO:Local step 115500, global step 1840000: loss 0.6527
[2019-04-07 18:17:46,552] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 115500, global step 1840000: learning rate 0.0000
[2019-04-07 18:17:46,554] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run93
[2019-04-07 18:17:46,557] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run93
[2019-04-07 18:19:03,341] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.1144093], dtype=float32), 0.16078338]
[2019-04-07 18:19:03,341] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-4.5, 66.5, 26.0, 0.0, 24.0, 23.82705005061689, -0.02513515490520116, 1.0, 1.0, 34073.995806230065]
[2019-04-07 18:19:03,342] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 18:19:03,343] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [7.55705482e-27 8.05252202e-26 4.13545302e-24 1.19750424e-17
 1.82636745e-23 1.00000000e+00 1.88564175e-18 1.86540072e-19], sampled 0.8371973239756743
[2019-04-07 18:20:11,893] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 18:20:30,072] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 18:20:34,156] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 18:20:35,179] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1840000, evaluation results [1840000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 18:20:38,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9611192e-30 4.6979164e-30 2.0225028e-27 7.5104058e-21 6.6883670e-26
 1.0000000e+00 8.3584153e-21 7.1607605e-23], sum to 1.0000
[2019-04-07 18:20:38,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3086
[2019-04-07 18:20:38,229] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.9, 84.0, 0.0, 0.0, 24.0, 23.83460569551566, 0.1439315023433659, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1636200.0000, 
sim time next is 1638000.0000, 
raw observation next is [7.2, 82.0, 0.0, 0.0, 24.0, 23.62782477802187, 0.1359999550092512, 0.0, 1.0, 78157.84771100686], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.82, 0.0, 0.0, 0.5, 0.4689853981684893, 0.5453333183364171, 0.0, 1.0, 0.3721802271952708], 
reward next is 0.9135, 
noisyNet noise sample is [array([1.1363398], dtype=float32), -0.39669314]. 
=============================================
[2019-04-07 18:20:38,261] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[109.84011 ]
 [110.11265 ]
 [109.902435]
 [110.00769 ]
 [109.17939 ]], R is [[109.84753418]
 [109.74906158]
 [109.65157318]
 [109.55506134]
 [109.04464722]].
[2019-04-07 18:20:39,226] A3C_AGENT_WORKER-Thread-2 INFO:Local step 114500, global step 1840686: loss 1.1522
[2019-04-07 18:20:39,236] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 114500, global step 1840686: learning rate 0.0000
[2019-04-07 18:20:43,061] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:20:43,061] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:20:43,074] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run42
[2019-04-07 18:20:51,761] A3C_AGENT_WORKER-Thread-4 INFO:Local step 114500, global step 1842474: loss 1.0801
[2019-04-07 18:20:51,762] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 114500, global step 1842474: learning rate 0.0000
[2019-04-07 18:20:54,044] A3C_AGENT_WORKER-Thread-18 INFO:Local step 115000, global step 1842772: loss 33.2334
[2019-04-07 18:20:54,045] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 115000, global step 1842772: learning rate 0.0000
[2019-04-07 18:21:02,515] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.21535016e-26 6.65424339e-25 3.42911903e-23 2.02155768e-16
 2.44819662e-22 1.00000000e+00 4.75502552e-19 1.19014426e-18], sum to 1.0000
[2019-04-07 18:21:02,515] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8175
[2019-04-07 18:21:02,734] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.45, 64.0, 151.0, 134.0, 24.0, 23.78761165378732, -0.09208072608618083, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2118600.0000, 
sim time next is 2120400.0000, 
raw observation next is [-6.2, 64.0, 150.0, 67.0, 24.0, 23.69146714798284, -0.1326228894063182, 1.0, 1.0, 150325.00649394185], 
processed observation next is [1.0, 0.5652173913043478, 0.2908587257617729, 0.64, 0.5, 0.07403314917127071, 0.5, 0.4742889289985701, 0.4557923701978939, 1.0, 1.0, 0.715833364256866], 
reward next is 0.5699, 
noisyNet noise sample is [array([1.7882911], dtype=float32), 0.8680057]. 
=============================================
[2019-04-07 18:21:08,592] A3C_AGENT_WORKER-Thread-10 INFO:Local step 114500, global step 1844826: loss 1.0588
[2019-04-07 18:21:08,593] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 114500, global step 1844826: learning rate 0.0000
[2019-04-07 18:21:12,458] A3C_AGENT_WORKER-Thread-20 INFO:Local step 114500, global step 1845312: loss 1.1366
[2019-04-07 18:21:12,458] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 114500, global step 1845312: learning rate 0.0000
[2019-04-07 18:21:13,654] A3C_AGENT_WORKER-Thread-5 INFO:Local step 114500, global step 1845458: loss 1.1627
[2019-04-07 18:21:13,654] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 114500, global step 1845458: learning rate 0.0000
[2019-04-07 18:21:13,790] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.1383762e-25 6.0051733e-23 3.8255795e-21 2.1496561e-16 3.6120788e-21
 1.0000000e+00 2.6668128e-16 1.7320998e-17], sum to 1.0000
[2019-04-07 18:21:13,790] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2897
[2019-04-07 18:21:13,938] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 25.0, 125.5, 0.0, 24.0, 23.64480898887171, -0.2011077320009066, 1.0, 1.0, 20656.058867009648], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 475200.0000, 
sim time next is 477000.0000, 
raw observation next is [-1.45, 26.5, 129.0, 0.0, 24.0, 23.65163247464021, -0.2091387937993445, 1.0, 1.0, 15289.121743835303], 
processed observation next is [1.0, 0.5217391304347826, 0.422437673130194, 0.265, 0.43, 0.0, 0.5, 0.4709693728866841, 0.4302870687335518, 1.0, 1.0, 0.07280534163731096], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32322374], dtype=float32), 1.2729738]. 
=============================================
[2019-04-07 18:21:13,942] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[84.85981 ]
 [84.381096]
 [83.78436 ]
 [83.41951 ]
 [83.240005]], R is [[85.44172668]
 [85.58731079]
 [85.73143768]
 [85.87412262]
 [86.01538086]].
[2019-04-07 18:21:14,311] A3C_AGENT_WORKER-Thread-11 INFO:Local step 114500, global step 1845550: loss 1.1391
[2019-04-07 18:21:14,311] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 114500, global step 1845550: learning rate 0.0000
[2019-04-07 18:21:14,908] A3C_AGENT_WORKER-Thread-14 INFO:Local step 116000, global step 1845630: loss 0.3842
[2019-04-07 18:21:14,908] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 116000, global step 1845630: learning rate 0.0000
[2019-04-07 18:21:16,099] A3C_AGENT_WORKER-Thread-17 INFO:Local step 114500, global step 1845798: loss 1.1504
[2019-04-07 18:21:16,099] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 114500, global step 1845798: learning rate 0.0000
[2019-04-07 18:21:16,189] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.03132054e-26 2.25754143e-25 4.66021789e-23 5.97174771e-17
 1.93858251e-22 1.00000000e+00 2.66540788e-17 5.10954097e-18], sum to 1.0000
[2019-04-07 18:21:16,189] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8648
[2019-04-07 18:21:16,199] A3C_AGENT_WORKER-Thread-3 INFO:Local step 114500, global step 1845811: loss 1.1318
[2019-04-07 18:21:16,201] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 114500, global step 1845811: learning rate 0.0000
[2019-04-07 18:21:16,276] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 83.0, 0.0, 0.0, 24.0, 22.6537212914146, -0.2551894553480581, 0.0, 1.0, 47770.868016756045], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1828800.0000, 
sim time next is 1830600.0000, 
raw observation next is [-6.2, 81.0, 0.0, 0.0, 24.0, 22.6043586126985, -0.2728422504956386, 0.0, 1.0, 47803.7263514625], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.81, 0.0, 0.0, 0.5, 0.38369655105820843, 0.40905258316812043, 0.0, 1.0, 0.22763679214982144], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01167696], dtype=float32), -0.11194223]. 
=============================================
[2019-04-07 18:21:17,404] A3C_AGENT_WORKER-Thread-13 INFO:Local step 114500, global step 1845963: loss 1.1609
[2019-04-07 18:21:17,405] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 114500, global step 1845963: learning rate 0.0000
[2019-04-07 18:21:24,217] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:21:24,218] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:21:24,221] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run42
[2019-04-07 18:21:30,748] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4542916e-24 4.7910280e-24 3.0639867e-22 1.6405331e-16 2.5206456e-22
 1.0000000e+00 5.0863460e-16 6.5146646e-19], sum to 1.0000
[2019-04-07 18:21:30,748] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9209
[2019-04-07 18:21:31,064] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 58.0, 21.5, 228.0, 24.0, 21.79018212733161, -0.4800834142026335, 0.0, 1.0, 44937.415832632425], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2448000.0000, 
sim time next is 2449800.0000, 
raw observation next is [-8.4, 54.0, 40.0, 416.0, 24.0, 22.23952866603489, -0.266062323356679, 0.0, 1.0, 150161.7113691674], 
processed observation next is [0.0, 0.34782608695652173, 0.2299168975069252, 0.54, 0.13333333333333333, 0.45966850828729283, 0.5, 0.35329405550290743, 0.411312558881107, 0.0, 1.0, 0.7150557684246067], 
reward next is 0.5707, 
noisyNet noise sample is [array([0.86127293], dtype=float32), 0.30324286]. 
=============================================
[2019-04-07 18:21:35,518] A3C_AGENT_WORKER-Thread-18 INFO:Local step 115500, global step 1848356: loss 0.6596
[2019-04-07 18:21:35,518] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 115500, global step 1848356: learning rate 0.0000
[2019-04-07 18:21:36,928] A3C_AGENT_WORKER-Thread-2 INFO:Local step 115000, global step 1848537: loss 33.1311
[2019-04-07 18:21:36,931] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 115000, global step 1848537: learning rate 0.0000
[2019-04-07 18:21:39,689] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0915311e-32 2.0984982e-31 1.2854914e-29 2.6308234e-22 6.7592475e-30
 1.0000000e+00 2.1166371e-22 1.7274352e-24], sum to 1.0000
[2019-04-07 18:21:39,689] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1148
[2019-04-07 18:21:39,757] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.05, 85.5, 0.0, 0.0, 24.0, 23.5365777817376, -0.00246668871975444, 0.0, 1.0, 25885.543707546367], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 955800.0000, 
sim time next is 957600.0000, 
raw observation next is [6.6, 82.0, 0.0, 0.0, 24.0, 23.64802685825381, 0.01197874215119812, 0.0, 1.0, 8403.477287162346], 
processed observation next is [1.0, 0.08695652173913043, 0.6454293628808865, 0.82, 0.0, 0.0, 0.5, 0.47066890485448426, 0.5039929140503994, 0.0, 1.0, 0.04001655851029689], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06798184], dtype=float32), -0.166083]. 
=============================================
[2019-04-07 18:21:48,135] A3C_AGENT_WORKER-Thread-4 INFO:Local step 115000, global step 1850350: loss 33.4152
[2019-04-07 18:21:48,142] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 115000, global step 1850350: learning rate 0.0000
[2019-04-07 18:21:53,435] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.4375918e-31 4.2331357e-31 1.1736647e-27 4.0938513e-20 5.0702552e-28
 1.0000000e+00 1.2064475e-21 5.5898648e-22], sum to 1.0000
[2019-04-07 18:21:53,436] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8090
[2019-04-07 18:21:53,498] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 92.0, 101.0, 653.0, 24.0, 24.3019114941413, 0.2144291179993056, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3231000.0000, 
sim time next is 3232800.0000, 
raw observation next is [-3.0, 92.0, 105.0, 702.5, 24.0, 24.5409891689949, 0.2505646736630376, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.92, 0.35, 0.7762430939226519, 0.5, 0.5450824307495751, 0.5835215578876792, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17200683], dtype=float32), -0.16356207]. 
=============================================
[2019-04-07 18:22:00,536] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.6228845e-30 7.9940212e-29 4.2824878e-27 6.2281572e-19 4.2215337e-27
 1.0000000e+00 2.5334389e-21 2.5902537e-22], sum to 1.0000
[2019-04-07 18:22:00,536] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9705
[2019-04-07 18:22:00,624] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 96.5, 0.0, 0.0, 24.0, 23.51953402226139, -0.09561055514954314, 0.0, 1.0, 38895.00179503055], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2871000.0000, 
sim time next is 2872800.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 24.0, 23.34518444985364, -0.1260219358154087, 0.0, 1.0, 71932.29234540061], 
processed observation next is [1.0, 0.2608695652173913, 0.4903047091412743, 1.0, 0.0, 0.0, 0.5, 0.44543203748780336, 0.45799268806153043, 0.0, 1.0, 0.3425347254542886], 
reward next is 0.9432, 
noisyNet noise sample is [array([0.69554156], dtype=float32), 0.83445185]. 
=============================================
[2019-04-07 18:22:03,496] A3C_AGENT_WORKER-Thread-10 INFO:Local step 115000, global step 1852936: loss 32.8758
[2019-04-07 18:22:03,505] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 115000, global step 1852936: learning rate 0.0000
[2019-04-07 18:22:07,924] A3C_AGENT_WORKER-Thread-20 INFO:Local step 115000, global step 1853768: loss 33.3482
[2019-04-07 18:22:07,926] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 115000, global step 1853768: learning rate 0.0000
[2019-04-07 18:22:08,803] A3C_AGENT_WORKER-Thread-11 INFO:Local step 115000, global step 1853953: loss 33.4436
[2019-04-07 18:22:08,805] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 115000, global step 1853953: learning rate 0.0000
[2019-04-07 18:22:09,185] A3C_AGENT_WORKER-Thread-5 INFO:Local step 115000, global step 1854034: loss 33.0446
[2019-04-07 18:22:09,197] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 115000, global step 1854034: learning rate 0.0000
[2019-04-07 18:22:09,487] A3C_AGENT_WORKER-Thread-17 INFO:Local step 115000, global step 1854090: loss 33.7011
[2019-04-07 18:22:09,487] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 115000, global step 1854090: learning rate 0.0000
[2019-04-07 18:22:09,804] A3C_AGENT_WORKER-Thread-3 INFO:Local step 115000, global step 1854157: loss 32.7968
[2019-04-07 18:22:09,806] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 115000, global step 1854157: learning rate 0.0000
[2019-04-07 18:22:12,774] A3C_AGENT_WORKER-Thread-13 INFO:Local step 115000, global step 1854758: loss 33.1289
[2019-04-07 18:22:12,774] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 115000, global step 1854758: learning rate 0.0000
[2019-04-07 18:22:14,098] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5526233e-24 9.2046707e-23 4.0424825e-21 2.7845704e-15 1.4135887e-20
 1.0000000e+00 5.2898912e-16 5.5814839e-17], sum to 1.0000
[2019-04-07 18:22:14,108] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4092
[2019-04-07 18:22:14,408] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 24.0, 21.65922433945151, -0.482345261367944, 0.0, 1.0, 44052.323161073844], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2703600.0000, 
sim time next is 2705400.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 24.0, 22.06387657814225, -0.2884920519118328, 1.0, 1.0, 149974.58821484703], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.5, 0.3386563815118541, 0.40383598269605575, 1.0, 1.0, 0.7141647057849858], 
reward next is 0.5715, 
noisyNet noise sample is [array([-0.6883701], dtype=float32), 0.46748468]. 
=============================================
[2019-04-07 18:22:15,061] A3C_AGENT_WORKER-Thread-18 INFO:Local step 116000, global step 1855203: loss 0.4038
[2019-04-07 18:22:15,063] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 116000, global step 1855203: learning rate 0.0000
[2019-04-07 18:22:16,293] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6601618e-28 5.5536572e-26 6.1839151e-25 1.6416458e-18 1.6519938e-24
 1.0000000e+00 9.2898037e-20 4.6860202e-20], sum to 1.0000
[2019-04-07 18:22:16,293] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8918
[2019-04-07 18:22:16,395] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.5, 88.0, 0.0, 0.0, 24.0, 23.4324468317476, 0.02759547169777271, 0.0, 1.0, 36469.89874637019], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3281400.0000, 
sim time next is 3283200.0000, 
raw observation next is [-7.0, 84.0, 0.0, 0.0, 24.0, 23.36955834370434, 0.02089944729269884, 0.0, 1.0, 61457.32870780273], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.84, 0.0, 0.0, 0.5, 0.4474631953086951, 0.5069664824308996, 0.0, 1.0, 0.292653946227632], 
reward next is 0.9931, 
noisyNet noise sample is [array([0.40843996], dtype=float32), -0.21587679]. 
=============================================
[2019-04-07 18:22:18,262] A3C_AGENT_WORKER-Thread-2 INFO:Local step 115500, global step 1855747: loss 0.6315
[2019-04-07 18:22:18,263] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 115500, global step 1855747: learning rate 0.0000
[2019-04-07 18:22:24,233] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:22:24,233] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:22:24,237] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run42
[2019-04-07 18:22:27,077] A3C_AGENT_WORKER-Thread-4 INFO:Local step 115500, global step 1857267: loss 0.6994
[2019-04-07 18:22:27,087] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 115500, global step 1857267: learning rate 0.0000
[2019-04-07 18:22:27,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3057113e-28 9.9365222e-26 5.7970497e-26 3.9440441e-17 2.1340510e-23
 1.0000000e+00 2.0326743e-19 5.0725397e-20], sum to 1.0000
[2019-04-07 18:22:27,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2952
[2019-04-07 18:22:27,505] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.49929507260835, 0.03213421207423264, 0.0, 1.0, 62260.600905477106], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3544200.0000, 
sim time next is 3546000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.49151510538731, 0.01703500952706579, 0.0, 1.0, 34862.09011888018], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.5, 0.4576262587822759, 0.5056783365090219, 0.0, 1.0, 0.16600995294704848], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2702112], dtype=float32), -1.2073953]. 
=============================================
[2019-04-07 18:22:27,510] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[ 96.817764]
 [ 98.12727 ]
 [100.21335 ]
 [101.61601 ]
 [101.20294 ]], R is [[96.52883911]
 [96.55278778]
 [96.58725739]
 [96.62138367]
 [96.11790466]].
[2019-04-07 18:22:28,975] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7094335e-25 2.2988556e-25 5.6931587e-23 1.2208155e-17 6.9176402e-22
 1.0000000e+00 1.0512847e-18 1.4123234e-18], sum to 1.0000
[2019-04-07 18:22:28,976] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1201
[2019-04-07 18:22:29,058] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 72.5, 0.0, 0.0, 24.0, 22.71749998735309, -0.2570510591450363, 0.0, 1.0, 42472.34601495386], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 786600.0000, 
sim time next is 788400.0000, 
raw observation next is [-7.8, 74.0, 0.0, 0.0, 24.0, 22.65825511230607, -0.2596295367045433, 0.0, 1.0, 42327.98482409425], 
processed observation next is [1.0, 0.13043478260869565, 0.24653739612188366, 0.74, 0.0, 0.0, 0.5, 0.3881879260255057, 0.4134568210984855, 0.0, 1.0, 0.2015618324956869], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.99307454], dtype=float32), -2.2572007]. 
=============================================
[2019-04-07 18:22:33,784] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.3833475e-27 2.5929571e-26 2.8517414e-26 5.8852428e-18 1.8182195e-24
 1.0000000e+00 2.6256941e-19 1.5195591e-20], sum to 1.0000
[2019-04-07 18:22:33,784] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3018
[2019-04-07 18:22:33,830] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 24.0, 23.00605636558726, -0.2153922511566723, 0.0, 1.0, 42022.405673624315], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2005200.0000, 
sim time next is 2007000.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 24.0, 23.04278585536452, -0.2205445335672655, 0.0, 1.0, 41964.5878340914], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.5, 0.42023215461371005, 0.42648515547757815, 0.0, 1.0, 0.19983137063853046], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3651006], dtype=float32), 0.43991446]. 
=============================================
[2019-04-07 18:22:33,840] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[95.40391 ]
 [95.376335]
 [95.39028 ]
 [95.50131 ]
 [95.5881  ]], R is [[95.46401215]
 [95.50937653]
 [95.55428314]
 [95.59873962]
 [95.6427536 ]].
[2019-04-07 18:22:36,753] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4463024e-29 7.0193259e-27 6.1154149e-25 1.5508736e-18 3.3444069e-25
 1.0000000e+00 3.5072998e-19 7.3219766e-20], sum to 1.0000
[2019-04-07 18:22:36,753] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1514
[2019-04-07 18:22:36,777] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 24.0, 23.93017951157919, 0.02843076064540667, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3704400.0000, 
sim time next is 3706200.0000, 
raw observation next is [1.0, 67.0, 0.0, 0.0, 24.0, 23.78286296606154, -0.02220461872321834, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.67, 0.0, 0.0, 0.5, 0.481905247171795, 0.4925984604255939, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.61988246], dtype=float32), -1.1519516]. 
=============================================
[2019-04-07 18:22:41,705] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-07 18:22:41,706] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:22:41,706] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:22:41,711] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run94
[2019-04-07 18:22:41,740] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:22:41,740] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:22:41,741] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 18:22:41,741] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:22:41,747] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run94
[2019-04-07 18:22:41,777] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run94
[2019-04-07 18:25:03,031] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 18:25:24,247] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 18:25:25,850] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 18:25:26,874] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1860000, evaluation results [1860000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 18:25:26,997] A3C_AGENT_WORKER-Thread-10 INFO:Local step 115500, global step 1860028: loss 0.6042
[2019-04-07 18:25:27,007] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 115500, global step 1860028: learning rate 0.0000
[2019-04-07 18:25:31,822] A3C_AGENT_WORKER-Thread-20 INFO:Local step 115500, global step 1860890: loss 0.7190
[2019-04-07 18:25:31,823] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 115500, global step 1860890: learning rate 0.0000
[2019-04-07 18:25:33,565] A3C_AGENT_WORKER-Thread-11 INFO:Local step 115500, global step 1861200: loss 0.6092
[2019-04-07 18:25:33,565] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 115500, global step 1861200: learning rate 0.0000
[2019-04-07 18:25:34,432] A3C_AGENT_WORKER-Thread-5 INFO:Local step 115500, global step 1861343: loss 0.7629
[2019-04-07 18:25:34,433] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 115500, global step 1861343: learning rate 0.0000
[2019-04-07 18:25:34,799] A3C_AGENT_WORKER-Thread-17 INFO:Local step 115500, global step 1861415: loss 0.7564
[2019-04-07 18:25:34,799] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 115500, global step 1861415: learning rate 0.0000
[2019-04-07 18:25:35,189] A3C_AGENT_WORKER-Thread-3 INFO:Local step 115500, global step 1861477: loss 0.6386
[2019-04-07 18:25:35,190] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 115500, global step 1861477: learning rate 0.0000
[2019-04-07 18:25:37,711] A3C_AGENT_WORKER-Thread-13 INFO:Local step 115500, global step 1861895: loss 0.6542
[2019-04-07 18:25:37,711] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 115500, global step 1861895: learning rate 0.0000
[2019-04-07 18:25:38,899] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5196451e-30 1.4797560e-28 4.2087347e-27 6.8760487e-21 5.5631143e-26
 1.0000000e+00 9.4242206e-21 3.2937301e-22], sum to 1.0000
[2019-04-07 18:25:38,903] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8775
[2019-04-07 18:25:38,964] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 67.0, 0.0, 0.0, 24.0, 23.61441545673437, -0.02890300156977195, 0.0, 1.0, 8656.495453146566], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3470400.0000, 
sim time next is 3472200.0000, 
raw observation next is [0.5, 69.5, 0.0, 0.0, 24.0, 23.40486427583352, -0.03067784026270099, 0.0, 1.0, 87615.17901090796], 
processed observation next is [1.0, 0.17391304347826086, 0.4764542936288089, 0.695, 0.0, 0.0, 0.5, 0.4504053563194601, 0.48977405324576634, 0.0, 1.0, 0.41721513814718075], 
reward next is 0.8685, 
noisyNet noise sample is [array([0.8188037], dtype=float32), -0.5212161]. 
=============================================
[2019-04-07 18:25:41,992] A3C_AGENT_WORKER-Thread-2 INFO:Local step 116000, global step 1862677: loss 0.3215
[2019-04-07 18:25:41,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 116000, global step 1862677: learning rate 0.0000
[2019-04-07 18:25:50,857] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:25:50,857] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:25:50,861] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run42
[2019-04-07 18:25:51,040] A3C_AGENT_WORKER-Thread-4 INFO:Local step 116000, global step 1864577: loss 0.3196
[2019-04-07 18:25:51,040] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 116000, global step 1864577: learning rate 0.0000
[2019-04-07 18:26:00,262] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:26:00,262] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:26:00,266] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run42
[2019-04-07 18:26:05,759] A3C_AGENT_WORKER-Thread-10 INFO:Local step 116000, global step 1867119: loss 0.3994
[2019-04-07 18:26:05,761] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 116000, global step 1867119: learning rate 0.0000
[2019-04-07 18:26:11,554] A3C_AGENT_WORKER-Thread-20 INFO:Local step 116000, global step 1868110: loss 0.4184
[2019-04-07 18:26:11,573] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 116000, global step 1868110: learning rate 0.0000
[2019-04-07 18:26:12,391] A3C_AGENT_WORKER-Thread-11 INFO:Local step 116000, global step 1868252: loss 0.4012
[2019-04-07 18:26:12,391] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 116000, global step 1868252: learning rate 0.0000
[2019-04-07 18:26:12,721] A3C_AGENT_WORKER-Thread-5 INFO:Local step 116000, global step 1868309: loss 0.3654
[2019-04-07 18:26:12,722] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 116000, global step 1868309: learning rate 0.0000
[2019-04-07 18:26:13,081] A3C_AGENT_WORKER-Thread-17 INFO:Local step 116000, global step 1868379: loss 0.4337
[2019-04-07 18:26:13,081] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 116000, global step 1868379: learning rate 0.0000
[2019-04-07 18:26:14,280] A3C_AGENT_WORKER-Thread-3 INFO:Local step 116000, global step 1868616: loss 0.4395
[2019-04-07 18:26:14,285] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 116000, global step 1868616: learning rate 0.0000
[2019-04-07 18:26:15,049] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:26:15,049] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:26:15,052] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run42
[2019-04-07 18:26:15,732] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.6104661e-27 1.0285747e-25 3.1933393e-24 1.7513745e-17 4.4147121e-24
 1.0000000e+00 5.3362174e-18 7.9735610e-20], sum to 1.0000
[2019-04-07 18:26:15,732] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8490
[2019-04-07 18:26:15,836] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 39.0, 100.5, 638.5, 24.0, 24.33119154573241, 0.07382673247291034, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4957200.0000, 
sim time next is 4959000.0000, 
raw observation next is [0.0, 34.5, 108.0, 717.0, 24.0, 24.72279369871124, 0.1379489901465012, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.345, 0.36, 0.7922651933701658, 0.5, 0.5602328082259366, 0.5459829967155004, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36870024], dtype=float32), -0.05945449]. 
=============================================
[2019-04-07 18:26:15,841] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[94.41159 ]
 [94.02499 ]
 [93.40069 ]
 [92.64121 ]
 [92.099785]], R is [[94.62683868]
 [94.68057251]
 [94.73376465]
 [94.78643036]
 [94.83856964]].
[2019-04-07 18:26:16,048] A3C_AGENT_WORKER-Thread-13 INFO:Local step 116000, global step 1868851: loss 0.4497
[2019-04-07 18:26:16,048] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 116000, global step 1868851: learning rate 0.0000
[2019-04-07 18:26:20,808] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:26:20,809] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:26:20,814] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run42
[2019-04-07 18:26:21,542] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:26:21,542] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:26:21,546] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run42
[2019-04-07 18:26:21,693] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:26:21,693] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:26:21,697] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run42
[2019-04-07 18:26:21,909] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:26:21,909] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:26:21,955] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run42
[2019-04-07 18:26:23,219] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:26:23,219] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:26:23,222] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run42
[2019-04-07 18:26:24,717] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:26:24,717] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:26:24,721] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run42
[2019-04-07 18:26:35,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9282704e-33 3.5512246e-29 3.0982841e-29 1.5323068e-20 1.0727544e-28
 1.0000000e+00 3.9720876e-22 1.2084644e-22], sum to 1.0000
[2019-04-07 18:26:35,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4919
[2019-04-07 18:26:36,192] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 82.0, 34.0, 0.0, 24.0, 22.72953989368218, -0.2087257255689516, 0.0, 1.0, 19094.411111014455], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 57600.0000, 
sim time next is 59400.0000, 
raw observation next is [6.05, 84.0, 18.0, 0.0, 24.0, 22.68334363562841, -0.2106612744813418, 0.0, 1.0, 36462.84582435909], 
processed observation next is [0.0, 0.6956521739130435, 0.6301939058171746, 0.84, 0.06, 0.0, 0.5, 0.3902786363023676, 0.42977957517288606, 0.0, 1.0, 0.1736325991636147], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0367749], dtype=float32), 0.028594667]. 
=============================================
[2019-04-07 18:26:40,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.2145379e-28 1.3182440e-26 1.1956786e-24 1.1848849e-18 1.0368842e-23
 1.0000000e+00 1.4032531e-18 2.9716018e-20], sum to 1.0000
[2019-04-07 18:26:40,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8151
[2019-04-07 18:26:41,099] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 66.0, 0.0, 0.0, 24.0, 24.03172284014539, -0.03949193154156055, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 149400.0000, 
sim time next is 151200.0000, 
raw observation next is [-7.3, 61.0, 0.0, 0.0, 24.0, 23.26352793688251, -0.09990905714762223, 1.0, 1.0, 77809.5946172059], 
processed observation next is [1.0, 0.782608695652174, 0.26038781163434904, 0.61, 0.0, 0.0, 0.5, 0.4386273280735426, 0.4666969809507926, 1.0, 1.0, 0.3705218791295519], 
reward next is 0.9152, 
noisyNet noise sample is [array([1.8297429], dtype=float32), -1.3601216]. 
=============================================
[2019-04-07 18:26:45,614] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:26:45,614] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:26:45,623] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run43
[2019-04-07 18:26:48,302] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4737349e-25 8.8958109e-24 2.4108373e-23 1.4189780e-16 7.8532548e-23
 1.0000000e+00 1.5036759e-16 2.2857112e-19], sum to 1.0000
[2019-04-07 18:26:48,302] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7088
[2019-04-07 18:26:48,392] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.9, 74.0, 0.0, 0.0, 24.0, 22.39507998022345, -0.3280842889735918, 0.0, 1.0, 45247.19476169648], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 180000.0000, 
sim time next is 181800.0000, 
raw observation next is [-8.9, 76.0, 0.0, 0.0, 24.0, 22.37539435312499, -0.3254222952481883, 0.0, 1.0, 45058.939703420845], 
processed observation next is [1.0, 0.08695652173913043, 0.21606648199445982, 0.76, 0.0, 0.0, 0.5, 0.36461619609374907, 0.39152590158393724, 0.0, 1.0, 0.21456637954009927], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.038841], dtype=float32), 0.73929447]. 
=============================================
[2019-04-07 18:26:51,025] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:26:51,025] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:26:51,030] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run43
[2019-04-07 18:26:59,281] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1689341e-25 4.0887060e-25 5.4151944e-23 1.7605455e-16 2.7060431e-22
 1.0000000e+00 5.4177313e-17 1.6636342e-19], sum to 1.0000
[2019-04-07 18:26:59,281] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6940
[2019-04-07 18:26:59,365] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.8, 77.0, 0.0, 0.0, 24.0, 23.13026174487518, -0.1686495683762299, 0.0, 1.0, 68433.7632489085], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 334800.0000, 
sim time next is 336600.0000, 
raw observation next is [-13.1, 79.5, 0.0, 0.0, 24.0, 22.95745490288165, -0.2053041714892045, 0.0, 1.0, 50664.84830057161], 
processed observation next is [1.0, 0.9130434782608695, 0.0997229916897507, 0.795, 0.0, 0.0, 0.5, 0.4131212419068042, 0.43156527617026513, 0.0, 1.0, 0.24126118238367433], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2150664], dtype=float32), -0.76033217]. 
=============================================
[2019-04-07 18:27:00,042] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.8181859e-31 1.5441882e-29 1.0813873e-27 4.9789107e-19 3.3185291e-27
 1.0000000e+00 2.7434954e-20 1.3340612e-21], sum to 1.0000
[2019-04-07 18:27:00,042] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7461
[2019-04-07 18:27:00,129] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.4, 95.5, 0.0, 0.0, 24.0, 23.28933781553005, -0.08920553165886728, 0.0, 1.0, 41424.42002047473], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 81000.0000, 
sim time next is 82800.0000, 
raw observation next is [0.3, 95.0, 0.0, 0.0, 24.0, 23.25844786066373, -0.09468647211922217, 0.0, 1.0, 41211.496349879504], 
processed observation next is [0.0, 1.0, 0.47091412742382277, 0.95, 0.0, 0.0, 0.5, 0.4382039883886441, 0.46843784262692595, 0.0, 1.0, 0.19624522071371192], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8540028], dtype=float32), 1.8363142]. 
=============================================
[2019-04-07 18:27:04,289] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.8366122e-27 1.8991114e-26 8.1179122e-25 5.0702234e-18 8.7730412e-24
 1.0000000e+00 9.2624552e-19 1.9864740e-19], sum to 1.0000
[2019-04-07 18:27:04,309] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6335
[2019-04-07 18:27:04,510] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.25, 61.0, 139.0, 484.0, 24.0, 24.2090802662114, 0.07772661427687669, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 135000.0000, 
sim time next is 136800.0000, 
raw observation next is [-6.7, 61.0, 143.5, 295.0, 24.0, 24.25678749909929, 0.06260807862181582, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.47833333333333333, 0.3259668508287293, 0.5, 0.5213989582582741, 0.5208693595406052, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9816804], dtype=float32), 0.41762874]. 
=============================================
[2019-04-07 18:27:09,517] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.8411172e-32 2.9885479e-30 8.8970049e-30 3.2401695e-21 1.5327507e-28
 1.0000000e+00 9.8645953e-22 8.2079425e-23], sum to 1.0000
[2019-04-07 18:27:09,517] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0083
[2019-04-07 18:27:09,604] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 81.5, 0.0, 0.0, 24.0, 23.60096428468083, 0.0064833891022379, 0.0, 1.0, 24709.2401620852], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 963000.0000, 
sim time next is 964800.0000, 
raw observation next is [7.7, 83.0, 0.0, 0.0, 24.0, 23.68115079018386, 0.01001046679103798, 0.0, 1.0, 12491.174141163525], 
processed observation next is [1.0, 0.17391304347826086, 0.6759002770083103, 0.83, 0.0, 0.0, 0.5, 0.4734292325153217, 0.5033368222636793, 0.0, 1.0, 0.05948178162458821], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0138478], dtype=float32), 0.052700028]. 
=============================================
[2019-04-07 18:27:27,044] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.9933849e-28 8.9200932e-27 1.8219469e-25 7.1411071e-19 5.9105748e-26
 1.0000000e+00 9.2229617e-20 3.4778011e-20], sum to 1.0000
[2019-04-07 18:27:27,045] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1267
[2019-04-07 18:27:27,100] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 24.0, 23.09231036548333, -0.2292887218223207, 0.0, 1.0, 42102.47930822032], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 700200.0000, 
sim time next is 702000.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 24.0, 23.08831718648648, -0.2358395200468829, 0.0, 1.0, 42247.65454758098], 
processed observation next is [1.0, 0.13043478260869565, 0.368421052631579, 0.75, 0.0, 0.0, 0.5, 0.42402643220720676, 0.42138682665103905, 0.0, 1.0, 0.20117930736943326], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6799094], dtype=float32), 1.4365264]. 
=============================================
[2019-04-07 18:27:27,106] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[96.85709]
 [97.24758]
 [96.55574]
 [96.66027]
 [96.93995]], R is [[96.57893372]
 [96.61314392]
 [96.6470108 ]
 [96.68054199]
 [96.71373749]].
[2019-04-07 18:27:31,979] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8979065e-29 1.0454571e-27 2.3127255e-26 7.6165597e-19 3.7118599e-26
 1.0000000e+00 9.6913094e-21 8.1103452e-21], sum to 1.0000
[2019-04-07 18:27:31,980] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0909
[2019-04-07 18:27:32,057] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.45, 77.5, 0.0, 0.0, 24.0, 23.31312212718883, -0.1374010805637478, 0.0, 1.0, 40542.53344684385], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 876600.0000, 
sim time next is 878400.0000, 
raw observation next is [-1.2, 76.0, 0.0, 0.0, 24.0, 23.39308033714373, -0.1192996182123463, 0.0, 1.0, 36399.404803622725], 
processed observation next is [1.0, 0.17391304347826086, 0.42936288088642666, 0.76, 0.0, 0.0, 0.5, 0.4494233614286441, 0.4602334605958846, 0.0, 1.0, 0.1733304990648701], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0946461], dtype=float32), -0.002672852]. 
=============================================
[2019-04-07 18:27:33,981] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7157173e-27 2.6535564e-26 2.2618193e-23 3.3381860e-18 3.0062470e-23
 1.0000000e+00 4.3286413e-19 3.2959722e-19], sum to 1.0000
[2019-04-07 18:27:33,981] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4913
[2019-04-07 18:27:34,199] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 24.0, 22.3880082516669, -0.3556685707903278, 0.0, 1.0, 42836.29444465398], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 802800.0000, 
sim time next is 804600.0000, 
raw observation next is [-6.7, 71.0, 0.0, 0.0, 24.0, 22.66973001710783, -0.1908933017965385, 1.0, 1.0, 141419.12781709235], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.71, 0.0, 0.0, 0.5, 0.3891441680923193, 0.43636889940115386, 1.0, 1.0, 0.6734244181766302], 
reward next is 0.6123, 
noisyNet noise sample is [array([-0.97949195], dtype=float32), -1.0455415]. 
=============================================
[2019-04-07 18:27:36,298] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-07 18:27:36,310] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:27:36,310] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:27:36,316] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run95
[2019-04-07 18:27:36,339] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:27:36,345] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 18:27:36,358] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:27:36,371] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run95
[2019-04-07 18:27:36,347] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:27:36,402] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run95
[2019-04-07 18:30:00,364] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 18:30:20,949] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 18:30:24,571] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 18:30:25,596] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1880000, evaluation results [1880000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 18:30:32,127] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5870689e-27 1.8952035e-26 7.0269950e-25 9.8272386e-19 3.2878265e-24
 1.0000000e+00 2.1415019e-18 1.5517359e-20], sum to 1.0000
[2019-04-07 18:30:32,127] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2770
[2019-04-07 18:30:32,199] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.41874303814862, -0.07077523793925554, 0.0, 1.0, 57677.822319924504], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4842000.0000, 
sim time next is 4843800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.45174695487147, -0.0822457935546174, 0.0, 1.0, 23611.12396683109], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.5, 0.45431224623928923, 0.4725847354817942, 0.0, 1.0, 0.11243392365157662], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6270154], dtype=float32), 0.23683791]. 
=============================================
[2019-04-07 18:30:35,408] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.53492253e-27 4.66392779e-26 6.79878193e-24 1.26213675e-17
 3.05505515e-24 1.00000000e+00 1.09812848e-18 1.52628717e-19], sum to 1.0000
[2019-04-07 18:30:35,408] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2945
[2019-04-07 18:30:35,475] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.55, 55.5, 0.0, 0.0, 24.0, 23.31847645605481, -0.2093982937844294, 0.0, 1.0, 45154.633708544134], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2529000.0000, 
sim time next is 2530800.0000, 
raw observation next is [-2.8, 54.0, 0.0, 0.0, 24.0, 23.50186297427817, -0.1934232367130971, 0.0, 1.0, 13577.579051821262], 
processed observation next is [1.0, 0.30434782608695654, 0.38504155124653744, 0.54, 0.0, 0.0, 0.5, 0.45848858118984737, 0.435525587762301, 0.0, 1.0, 0.06465513834200601], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34315422], dtype=float32), -0.15097064]. 
=============================================
[2019-04-07 18:30:37,734] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7353056e-33 1.5771988e-30 8.0021980e-29 1.3271372e-22 2.4406362e-28
 1.0000000e+00 3.3679049e-22 1.6131284e-24], sum to 1.0000
[2019-04-07 18:30:37,735] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2985
[2019-04-07 18:30:37,752] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 100.0, 64.0, 0.0, 24.0, 23.2272142811849, 0.08751351177457116, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1263600.0000, 
sim time next is 1265400.0000, 
raw observation next is [13.8, 100.0, 51.0, 0.0, 24.0, 23.19398003691138, 0.0813935374178306, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.17, 0.0, 0.5, 0.43283166974261505, 0.5271311791392769, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34157702], dtype=float32), -1.0121226]. 
=============================================
[2019-04-07 18:30:38,315] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:30:38,315] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:30:38,335] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run43
[2019-04-07 18:30:40,147] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.7095830e-29 4.2200664e-28 1.9134297e-26 4.7975533e-18 3.2543710e-25
 1.0000000e+00 5.6197397e-20 1.4523751e-21], sum to 1.0000
[2019-04-07 18:30:40,147] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8746
[2019-04-07 18:30:40,198] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 95.0, 59.0, 0.0, 24.0, 24.44495732604407, 0.1381114950870087, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1418400.0000, 
sim time next is 1420200.0000, 
raw observation next is [0.0, 95.0, 72.0, 0.0, 24.0, 24.39951159761215, 0.1238775163071178, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.24, 0.0, 0.5, 0.5332926331343458, 0.541292505435706, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.52262187], dtype=float32), 0.6345774]. 
=============================================
[2019-04-07 18:30:42,271] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8888617e-31 7.9999312e-31 6.4917619e-28 3.5634812e-20 3.0639102e-26
 1.0000000e+00 7.0086448e-21 8.6010300e-23], sum to 1.0000
[2019-04-07 18:30:42,271] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9068
[2019-04-07 18:30:42,301] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 127.0, 0.0, 24.0, 24.43966252938922, 0.1878565277811722, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1337400.0000, 
sim time next is 1339200.0000, 
raw observation next is [1.1, 92.0, 120.0, 0.0, 24.0, 24.39733510228491, 0.1775684207343826, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.4, 0.0, 0.5, 0.5331112585237424, 0.5591894735781275, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6110212], dtype=float32), -0.053941045]. 
=============================================
[2019-04-07 18:30:42,443] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:30:42,443] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:30:42,447] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run43
[2019-04-07 18:30:54,837] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:30:54,837] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:30:54,840] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run43
[2019-04-07 18:31:06,148] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8522616e-30 6.6879229e-29 6.2634173e-27 2.0141035e-19 1.4231721e-26
 1.0000000e+00 2.2663299e-21 9.4728650e-24], sum to 1.0000
[2019-04-07 18:31:06,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2870
[2019-04-07 18:31:06,207] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.45, 75.5, 0.0, 0.0, 24.0, 23.74225676756854, -0.02083316298983393, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4318200.0000, 
sim time next is 4320000.0000, 
raw observation next is [4.5, 76.0, 0.0, 0.0, 24.0, 23.60969566951075, -0.02366084998050372, 0.0, 1.0, 54505.76090204335], 
processed observation next is [1.0, 0.0, 0.5872576177285319, 0.76, 0.0, 0.0, 0.5, 0.46747463912589576, 0.49211305000649874, 0.0, 1.0, 0.2595512423906826], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09758211], dtype=float32), -0.7807249]. 
=============================================
[2019-04-07 18:31:06,211] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[103.332146]
 [103.96533 ]
 [104.17944 ]
 [104.133514]
 [104.69842 ]], R is [[104.74927521]
 [104.70178223]
 [104.65476227]
 [104.60821533]
 [104.56213379]].
[2019-04-07 18:31:17,031] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.3988830e-29 9.8288063e-28 7.7984028e-26 9.3968762e-19 2.1011638e-25
 1.0000000e+00 9.4208779e-21 1.0625525e-20], sum to 1.0000
[2019-04-07 18:31:17,031] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6988
[2019-04-07 18:31:17,205] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 24.0, 23.38085491732046, -0.07822555273060094, 1.0, 1.0, 23287.976488177086], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2055600.0000, 
sim time next is 2057400.0000, 
raw observation next is [-3.9, 84.0, 0.0, 0.0, 24.0, 23.28508138339079, -0.1022402428673094, 0.0, 1.0, 31241.06408099682], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.84, 0.0, 0.0, 0.5, 0.4404234486158991, 0.4659199190442302, 0.0, 1.0, 0.14876697181427057], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.91868305], dtype=float32), 0.30848315]. 
=============================================
[2019-04-07 18:31:25,553] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.6919930e-28 1.7157819e-27 3.6630098e-25 4.6467362e-19 3.5131541e-25
 1.0000000e+00 6.0616862e-20 2.2326413e-21], sum to 1.0000
[2019-04-07 18:31:25,553] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4869
[2019-04-07 18:31:25,698] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 65.0, 163.5, 575.5, 24.0, 23.69051226993184, 0.02744617446743249, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4788000.0000, 
sim time next is 4789800.0000, 
raw observation next is [-2.5, 55.5, 153.0, 730.0, 24.0, 23.51336058134284, 0.01349845908906531, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.39335180055401664, 0.555, 0.51, 0.8066298342541437, 0.5, 0.45944671511190344, 0.5044994863630218, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3779837], dtype=float32), 0.08119618]. 
=============================================
[2019-04-07 18:31:39,333] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:31:39,333] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:31:39,336] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run43
[2019-04-07 18:31:42,842] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1689279e-26 2.5852717e-27 6.5772210e-25 3.4144152e-19 5.4304347e-25
 1.0000000e+00 4.4213300e-19 1.9051036e-20], sum to 1.0000
[2019-04-07 18:31:42,842] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3524
[2019-04-07 18:31:42,879] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.55, 63.5, 0.0, 0.0, 24.0, 23.20001832898526, -0.1462741341905161, 0.0, 1.0, 40932.53816271843], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2345400.0000, 
sim time next is 2347200.0000, 
raw observation next is [-2.8, 65.0, 0.0, 0.0, 24.0, 23.12756792851273, -0.1552827050099788, 0.0, 1.0, 41207.18579655719], 
processed observation next is [0.0, 0.17391304347826086, 0.38504155124653744, 0.65, 0.0, 0.0, 0.5, 0.42729732737606074, 0.4482390983300071, 0.0, 1.0, 0.19622469426931996], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.47252592], dtype=float32), 0.3271794]. 
=============================================
[2019-04-07 18:32:30,877] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-07 18:32:30,879] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:32:30,879] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:32:30,887] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run96
[2019-04-07 18:32:30,925] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:32:30,926] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:32:30,930] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run96
[2019-04-07 18:32:30,958] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 18:32:30,958] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:32:30,963] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run96
[2019-04-07 18:34:13,050] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11455914], dtype=float32), 0.16201493]
[2019-04-07 18:34:13,050] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [1.650547224, 48.667677465, 0.0, 72.98734222, 24.0, 23.63665204224917, -0.1416647871728979, 1.0, 1.0, 13901.925514463117]
[2019-04-07 18:34:13,050] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 18:34:13,051] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [6.1032901e-27 6.2967925e-26 3.1513236e-24 6.9757671e-18 1.5549155e-23
 1.0000000e+00 1.2598416e-18 1.3654750e-19], sampled 0.37592872443636505
[2019-04-07 18:34:52,987] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 18:35:08,833] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 18:35:11,840] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 18:35:12,864] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1900000, evaluation results [1900000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 18:35:17,644] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.53557507e-31 1.21382275e-31 1.60581884e-28 1.36431184e-22
 1.16157514e-29 1.00000000e+00 5.48823330e-22 1.02758698e-24], sum to 1.0000
[2019-04-07 18:35:17,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7620
[2019-04-07 18:35:17,696] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 24.0, 23.78834255415501, 0.1512107311747071, 0.0, 1.0, 6243.07794240826], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3200400.0000, 
sim time next is 3202200.0000, 
raw observation next is [0.5, 100.0, 0.0, 0.0, 24.0, 23.82759337517385, 0.1187314885780193, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4764542936288089, 1.0, 0.0, 0.0, 0.5, 0.4856327812644876, 0.5395771628593398, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5028245], dtype=float32), -0.48755044]. 
=============================================
[2019-04-07 18:35:19,885] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:35:19,885] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:35:19,888] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run43
[2019-04-07 18:35:31,491] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5466217e-29 2.6533311e-28 3.9716835e-27 8.0175121e-20 5.1887935e-27
 1.0000000e+00 2.2710218e-20 7.9390436e-22], sum to 1.0000
[2019-04-07 18:35:31,491] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3911
[2019-04-07 18:35:31,551] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 60.5, 109.0, 770.0, 24.0, 24.64404561123834, 0.2019221301018516, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3493800.0000, 
sim time next is 3495600.0000, 
raw observation next is [1.0, 61.0, 112.0, 790.0, 24.0, 24.631755028021, 0.2103680862419249, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.61, 0.37333333333333335, 0.8729281767955801, 0.5, 0.5526462523350834, 0.570122695413975, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22072703], dtype=float32), -0.43567815]. 
=============================================
[2019-04-07 18:35:34,401] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6762755e-27 3.7056211e-26 2.5354751e-24 3.3410669e-17 7.2654176e-26
 1.0000000e+00 1.0763342e-18 2.4150291e-20], sum to 1.0000
[2019-04-07 18:35:34,401] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7024
[2019-04-07 18:35:34,475] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 24.0, 23.01755094446171, -0.2005295473985785, 0.0, 1.0, 40312.220762523786], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3034800.0000, 
sim time next is 3036600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 24.0, 22.96612308457762, -0.2134273689470805, 0.0, 1.0, 40692.56967958141], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.5, 0.4138435903814684, 0.4288575436843065, 0.0, 1.0, 0.19377414133134002], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15073441], dtype=float32), -0.39274466]. 
=============================================
[2019-04-07 18:36:03,040] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6911154e-27 8.8906020e-28 1.1665776e-25 1.7565859e-18 9.0664123e-26
 1.0000000e+00 5.2122832e-19 3.2721181e-20], sum to 1.0000
[2019-04-07 18:36:03,040] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4333
[2019-04-07 18:36:03,101] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.0, 37.0, 114.0, 544.0, 24.0, 23.42003770954984, -0.002038109278561758, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4204800.0000, 
sim time next is 4206600.0000, 
raw observation next is [2.5, 38.5, 68.0, 550.0, 24.0, 23.55540761249222, 0.01343252940691284, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5318559556786704, 0.385, 0.22666666666666666, 0.6077348066298343, 0.5, 0.46295063437435174, 0.5044775098023043, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40687555], dtype=float32), 2.031181]. 
=============================================
[2019-04-07 18:36:05,163] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:36:05,163] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:36:05,166] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run43
[2019-04-07 18:36:09,041] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4829348e-31 4.6110792e-29 2.1307033e-27 1.6582730e-20 8.6241371e-28
 1.0000000e+00 2.2459768e-20 7.6316716e-22], sum to 1.0000
[2019-04-07 18:36:09,050] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7476
[2019-04-07 18:36:09,089] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 92.0, 0.0, 0.0, 24.0, 23.55173065126201, 0.03366932484890922, 0.0, 1.0, 73447.13027309027], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4680000.0000, 
sim time next is 4681800.0000, 
raw observation next is [-0.5, 96.0, 0.0, 0.0, 24.0, 23.7142472493689, 0.02849003529536765, 0.0, 1.0, 6242.29175953121], 
processed observation next is [1.0, 0.17391304347826086, 0.44875346260387816, 0.96, 0.0, 0.0, 0.5, 0.4761872707807416, 0.5094966784317893, 0.0, 1.0, 0.029725198854910526], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.237807], dtype=float32), -1.2237984]. 
=============================================
[2019-04-07 18:36:18,269] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:36:18,269] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:36:18,272] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run43
[2019-04-07 18:36:19,928] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4339912e-29 1.7134011e-27 1.3820094e-25 3.1361012e-19 3.0590419e-25
 1.0000000e+00 2.4887088e-19 4.3423141e-21], sum to 1.0000
[2019-04-07 18:36:19,928] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8155
[2019-04-07 18:36:20,000] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.4, 63.0, 0.0, 0.0, 24.0, 23.54747983230766, -0.004891265456536838, 0.0, 1.0, 34632.017311252166], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4582800.0000, 
sim time next is 4584600.0000, 
raw observation next is [0.1, 64.0, 0.0, 0.0, 24.0, 23.55938374297265, -0.006342750981502175, 0.0, 1.0, 33824.51350030176], 
processed observation next is [1.0, 0.043478260869565216, 0.4653739612188367, 0.64, 0.0, 0.0, 0.5, 0.4632819785810541, 0.4978857496728326, 0.0, 1.0, 0.16106911190619885], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7507329], dtype=float32), -1.8752503]. 
=============================================
[2019-04-07 18:36:28,149] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:36:28,149] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:36:28,152] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run43
[2019-04-07 18:36:31,175] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5463693e-29 7.3817106e-28 1.6965837e-25 2.6867318e-18 2.9750311e-25
 1.0000000e+00 1.1101016e-20 7.9570231e-22], sum to 1.0000
[2019-04-07 18:36:31,176] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6809
[2019-04-07 18:36:31,215] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 24.0, 24.70659634722935, 0.347115090892961, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4993200.0000, 
sim time next is 4995000.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 24.0, 24.91532728261967, 0.3369576284633819, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.6288088642659281, 0.23, 0.0, 0.0, 0.5, 0.5762772735516393, 0.612319209487794, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5699462], dtype=float32), 1.8309344]. 
=============================================
[2019-04-07 18:36:31,219] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[101.6577  ]
 [102.816086]
 [104.18027 ]
 [105.30421 ]
 [106.09183 ]], R is [[100.08140564]
 [100.08058929]
 [100.07978058]
 [100.07898712]
 [100.07820129]].
[2019-04-07 18:36:36,220] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:36:36,220] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:36:36,244] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run43
[2019-04-07 18:36:38,353] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:36:38,353] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:36:38,357] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run43
[2019-04-07 18:36:39,609] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:36:39,610] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:36:39,613] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run43
[2019-04-07 18:36:41,113] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:36:41,113] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:36:41,116] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run43
[2019-04-07 18:36:41,885] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:36:41,885] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:36:41,891] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run43
[2019-04-07 18:36:42,257] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:36:42,257] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:36:42,260] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run43
[2019-04-07 18:36:53,036] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.64220638e-31 1.25322466e-29 2.94503626e-28 2.70069219e-21
 4.10882478e-27 1.00000000e+00 2.45379962e-20 2.16017152e-22], sum to 1.0000
[2019-04-07 18:36:53,036] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5156
[2019-04-07 18:36:53,204] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 21.0, 0.0, 24.0, 21.9191293035185, -0.3492143463773412, 0.0, 1.0, 90934.28898703305], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 30600.0000, 
sim time next is 32400.0000, 
raw observation next is [7.7, 93.0, 29.5, 0.0, 24.0, 22.58013643996006, -0.2901820277045345, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.09833333333333333, 0.0, 0.5, 0.3816780366633384, 0.4032726574318219, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0904378], dtype=float32), -2.1928604]. 
=============================================
[2019-04-07 18:37:00,393] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:37:00,393] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:37:00,396] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run44
[2019-04-07 18:37:02,341] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.9144567e-27 2.9364127e-26 3.0969685e-24 9.5501410e-18 3.9708110e-24
 1.0000000e+00 1.9758115e-18 2.7183372e-20], sum to 1.0000
[2019-04-07 18:37:02,341] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8521
[2019-04-07 18:37:02,643] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.8, 64.0, 0.0, 0.0, 24.0, 23.18331715698164, -0.1215808071626905, 1.0, 1.0, 59074.84025017405], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 154800.0000, 
sim time next is 156600.0000, 
raw observation next is [-8.1, 66.0, 0.0, 0.0, 24.0, 23.15969124386004, -0.1223449497299035, 1.0, 1.0, 65428.304772413256], 
processed observation next is [1.0, 0.8260869565217391, 0.23822714681440446, 0.66, 0.0, 0.0, 0.5, 0.42997427032166985, 0.4592183500900322, 1.0, 1.0, 0.31156335605911073], 
reward next is 0.9742, 
noisyNet noise sample is [array([-0.46267214], dtype=float32), 0.6084556]. 
=============================================
[2019-04-07 18:37:02,933] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.1902950e-28 2.1121563e-26 4.8919329e-26 4.6307752e-19 5.2208639e-25
 1.0000000e+00 8.4018958e-20 1.6750392e-19], sum to 1.0000
[2019-04-07 18:37:02,933] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1224
[2019-04-07 18:37:02,989] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 42.0, 115.0, 823.5, 24.0, 23.63000440459617, 0.05187720811838953, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3675600.0000, 
sim time next is 3677400.0000, 
raw observation next is [5.5, 42.5, 113.0, 818.0, 24.0, 23.6160515191607, 0.05370392505310487, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6149584487534627, 0.425, 0.37666666666666665, 0.9038674033149171, 0.5, 0.4680042932633916, 0.517901308351035, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31102568], dtype=float32), -0.71336263]. 
=============================================
[2019-04-07 18:37:05,117] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:37:05,117] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:37:05,120] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run44
[2019-04-07 18:37:15,365] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-07 18:37:15,367] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:37:15,368] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:37:15,372] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run97
[2019-04-07 18:37:15,398] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:37:15,399] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:37:15,414] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run97
[2019-04-07 18:37:15,444] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 18:37:15,445] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:37:15,462] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run97
[2019-04-07 18:39:36,375] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 18:39:58,018] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 18:40:01,991] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 18:40:03,016] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1920000, evaluation results [1920000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 18:40:06,375] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1323001e-25 5.1573458e-24 1.6344619e-23 5.9111200e-17 7.9020273e-23
 1.0000000e+00 5.6397174e-18 2.5049686e-18], sum to 1.0000
[2019-04-07 18:40:06,375] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1282
[2019-04-07 18:40:06,670] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.8, 51.0, 58.0, 834.5, 24.0, 24.20523908511404, 0.001453147175822409, 1.0, 1.0, 66409.44910805566], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 388800.0000, 
sim time next is 390600.0000, 
raw observation next is [-12.25, 51.0, 58.0, 905.0, 24.0, 23.61827215982507, -0.0703915108026634, 1.0, 1.0, 66230.61601262067], 
processed observation next is [1.0, 0.5217391304347826, 0.12326869806094183, 0.51, 0.19333333333333333, 1.0, 0.5, 0.4681893466520893, 0.4765361630657789, 1.0, 1.0, 0.31538388577438414], 
reward next is 0.9703, 
noisyNet noise sample is [array([0.8408038], dtype=float32), -0.84506625]. 
=============================================
[2019-04-07 18:40:15,221] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5177753e-27 8.8020286e-26 1.3118200e-23 8.9358684e-18 1.4200769e-24
 1.0000000e+00 6.5515739e-19 1.4620928e-20], sum to 1.0000
[2019-04-07 18:40:15,221] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0272
[2019-04-07 18:40:15,281] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 38.0, 105.0, 788.0, 24.0, 24.77663000937336, 0.2424736306923444, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3940200.0000, 
sim time next is 3942000.0000, 
raw observation next is [-4.0, 38.0, 96.5, 756.0, 24.0, 24.8774396406435, 0.1713826781306603, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.38, 0.32166666666666666, 0.8353591160220994, 0.5, 0.5731199700536251, 0.5571275593768867, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5860629], dtype=float32), -0.30013913]. 
=============================================
[2019-04-07 18:40:15,297] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[91.863785]
 [92.2184  ]
 [92.65014 ]
 [93.11005 ]
 [93.14684 ]], R is [[91.65250397]
 [91.73597717]
 [91.81861877]
 [91.9004364 ]
 [91.98143005]].
[2019-04-07 18:40:46,088] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.0209436e-32 1.1112277e-29 3.4402997e-28 1.5263970e-20 2.3285260e-27
 1.0000000e+00 1.4339487e-22 1.6308878e-22], sum to 1.0000
[2019-04-07 18:40:46,094] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9011
[2019-04-07 18:40:46,174] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 100.0, 60.0, 0.0, 24.0, 24.53268522716215, 0.1311050862700485, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1504800.0000, 
sim time next is 1506600.0000, 
raw observation next is [2.75, 98.0, 73.0, 0.0, 24.0, 24.5196534440209, 0.1400520705355159, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5387811634349031, 0.98, 0.24333333333333335, 0.0, 0.5, 0.5433044536684083, 0.5466840235118386, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1536961], dtype=float32), 1.2723751]. 
=============================================
[2019-04-07 18:40:52,882] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:40:52,882] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:40:52,886] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run44
[2019-04-07 18:40:52,924] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:40:52,925] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:40:52,935] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run44
[2019-04-07 18:41:02,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.7286966e-27 1.6255624e-25 2.1889119e-24 6.0131890e-18 7.0838915e-25
 1.0000000e+00 6.0525390e-19 1.4506988e-19], sum to 1.0000
[2019-04-07 18:41:02,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7384
[2019-04-07 18:41:02,828] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.0, 38.0, 110.5, 806.0, 24.0, 24.57089741720664, 0.2280620399922047, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3938400.0000, 
sim time next is 3940200.0000, 
raw observation next is [-4.5, 38.0, 105.0, 788.0, 24.0, 24.77663000937336, 0.2424736306923444, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3379501385041552, 0.38, 0.35, 0.8707182320441988, 0.5, 0.56471916744778, 0.5808245435641148, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8613806], dtype=float32), 0.13625103]. 
=============================================
[2019-04-07 18:41:07,317] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:41:07,317] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:41:07,321] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run44
[2019-04-07 18:41:13,897] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.6850594e-31 2.6712635e-30 1.1201323e-27 8.5458000e-21 2.3089350e-26
 1.0000000e+00 3.0460428e-20 7.9726360e-23], sum to 1.0000
[2019-04-07 18:41:13,897] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9110
[2019-04-07 18:41:13,958] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 92.0, 41.5, 0.0, 24.0, 23.67109975058693, 0.05380515078448889, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1674000.0000, 
sim time next is 1675800.0000, 
raw observation next is [1.85, 92.0, 53.0, 0.0, 24.0, 23.8565617614731, 0.07859616894683243, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5138504155124655, 0.92, 0.17666666666666667, 0.0, 0.5, 0.4880468134560916, 0.5261987229822774, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3199589], dtype=float32), 2.1283777]. 
=============================================
[2019-04-07 18:41:54,408] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:41:54,408] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:41:54,413] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run44
[2019-04-07 18:42:06,295] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.8387449e-29 5.3389797e-28 4.1943118e-26 9.8214624e-20 1.6828982e-26
 1.0000000e+00 6.5919820e-20 1.7294677e-22], sum to 1.0000
[2019-04-07 18:42:06,296] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4742
[2019-04-07 18:42:06,394] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 24.0, 23.31013220785421, -0.1318254842210017, 0.0, 1.0, 41582.5266378877], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 871200.0000, 
sim time next is 873000.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 24.0, 23.38572384236971, -0.1380504995941051, 0.0, 1.0, 34400.70121050938], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.79, 0.0, 0.0, 0.5, 0.44881032019747585, 0.453983166801965, 0.0, 1.0, 0.16381286290718752], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5413997], dtype=float32), -0.96979356]. 
=============================================
[2019-04-07 18:42:06,405] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[98.26451]
 [98.08773]
 [97.61994]
 [97.45191]
 [97.09417]], R is [[98.42860413]
 [98.44432068]
 [98.45987701]
 [98.47528076]
 [98.49053192]].
[2019-04-07 18:42:11,072] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4809840e-26 2.6912458e-25 2.1600314e-22 1.7089254e-17 2.6740564e-23
 1.0000000e+00 1.1802211e-17 2.1409863e-19], sum to 1.0000
[2019-04-07 18:42:11,073] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8122
[2019-04-07 18:42:11,138] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 29.5, 90.0, 845.0, 24.0, 23.13787215946996, -0.164004134051151, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2464200.0000, 
sim time next is 2466000.0000, 
raw observation next is [1.6, 28.0, 88.5, 838.5, 24.0, 23.06238612273448, -0.1610482919335301, 0.0, 1.0, 20204.577572801703], 
processed observation next is [0.0, 0.5652173913043478, 0.5069252077562327, 0.28, 0.295, 0.9265193370165746, 0.5, 0.4218655102278734, 0.4463172360221566, 0.0, 1.0, 0.09621227415619858], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3375572], dtype=float32), -1.0511699]. 
=============================================
[2019-04-07 18:42:11,141] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[88.4255  ]
 [88.16198 ]
 [87.99001 ]
 [87.300026]
 [87.048645]], R is [[89.07143402]
 [89.18071747]
 [89.28890991]
 [89.39601898]
 [89.50205994]].
[2019-04-07 18:42:13,357] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-07 18:42:13,358] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:42:13,358] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:42:13,372] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run98
[2019-04-07 18:42:13,398] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:42:13,400] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:42:13,402] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 18:42:13,406] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:42:13,409] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run98
[2019-04-07 18:42:13,441] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run98
[2019-04-07 18:43:37,252] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11471864], dtype=float32), 0.16296966]
[2019-04-07 18:43:37,252] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [-4.65, 78.5, 0.0, 0.0, 24.0, 23.10427342904492, -0.1884860828390361, 0.0, 1.0, 42667.9725569926]
[2019-04-07 18:43:37,252] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 18:43:37,254] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [2.6797694e-27 3.3856745e-26 1.7797221e-24 3.2832370e-18 4.1420996e-24
 1.0000000e+00 9.8634061e-19 7.6050010e-20], sampled 0.1213796665052892
[2019-04-07 18:44:38,548] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 18:44:55,506] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 18:45:00,476] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 18:45:01,515] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1940000, evaluation results [1940000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 18:45:05,395] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.06331050e-28 5.43868257e-26 1.06031985e-23 2.79356719e-18
 7.33398556e-23 1.00000000e+00 1.50091209e-18 1.49780407e-21], sum to 1.0000
[2019-04-07 18:45:05,395] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3278
[2019-04-07 18:45:05,518] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.85, 76.5, 79.0, 0.0, 24.0, 23.82285367325645, -0.1360638053982433, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 207000.0000, 
sim time next is 208800.0000, 
raw observation next is [-7.3, 75.0, 101.5, 0.0, 24.0, 23.77793702921971, -0.146105556346494, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.26038781163434904, 0.75, 0.3383333333333333, 0.0, 0.5, 0.4814947524349759, 0.451298147884502, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2806259], dtype=float32), -1.3672526]. 
=============================================
[2019-04-07 18:45:08,433] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.4192137e-32 8.1974312e-30 2.1721382e-28 1.0053173e-21 1.1876081e-27
 1.0000000e+00 4.7364804e-22 5.5132851e-23], sum to 1.0000
[2019-04-07 18:45:08,433] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3315
[2019-04-07 18:45:08,517] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 100.0, 78.0, 27.0, 24.0, 23.75648365712848, -0.1254735339879913, 1.0, 1.0, 12453.607780153694], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2908800.0000, 
sim time next is 2910600.0000, 
raw observation next is [2.0, 96.5, 71.0, 54.0, 24.0, 23.40632111405294, -0.07158438766332897, 1.0, 1.0, 20023.872736789694], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.965, 0.23666666666666666, 0.05966850828729282, 0.5, 0.4505267595044116, 0.47613853744555695, 1.0, 1.0, 0.09535177493709378], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49997458], dtype=float32), 0.3032498]. 
=============================================
[2019-04-07 18:45:11,646] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.23582773e-29 1.00913674e-28 1.79918835e-26 4.44153306e-18
 1.45576687e-25 1.00000000e+00 8.07990087e-20 2.97077292e-21], sum to 1.0000
[2019-04-07 18:45:11,646] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4312
[2019-04-07 18:45:11,683] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.25, 95.0, 104.0, 0.0, 24.0, 23.96089753212429, -0.0571941093680626, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 909000.0000, 
sim time next is 910800.0000, 
raw observation next is [3.8, 93.0, 100.0, 0.0, 24.0, 23.89539790976526, -0.06645917285412732, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.3333333333333333, 0.0, 0.5, 0.49128315914710513, 0.4778469423819576, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15277705], dtype=float32), 0.38032362]. 
=============================================
[2019-04-07 18:45:23,160] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1822437e-33 2.2694923e-32 4.0867000e-30 4.6503442e-23 4.4515499e-29
 1.0000000e+00 1.1909911e-22 5.0057887e-24], sum to 1.0000
[2019-04-07 18:45:23,160] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9410
[2019-04-07 18:45:23,282] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 24.0, 22.84552583137755, 0.02932354320414946, 0.0, 1.0, 23166.319049388407], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1278000.0000, 
sim time next is 1279800.0000, 
raw observation next is [6.65, 96.0, 0.0, 0.0, 24.0, 22.89647259770059, 0.0756216996015368, 0.0, 1.0, 138309.1490886767], 
processed observation next is [0.0, 0.8260869565217391, 0.6468144044321331, 0.96, 0.0, 0.0, 0.5, 0.4080393831417159, 0.5252072332005123, 0.0, 1.0, 0.6586149956603652], 
reward next is 0.6271, 
noisyNet noise sample is [array([-0.10167803], dtype=float32), -0.95770603]. 
=============================================
[2019-04-07 18:45:28,670] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7795903e-27 2.4097762e-26 5.1975178e-24 4.9770332e-18 2.4195056e-24
 1.0000000e+00 6.8438194e-18 4.5876426e-20], sum to 1.0000
[2019-04-07 18:45:28,671] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8238
[2019-04-07 18:45:28,722] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 24.0, 23.41509205587747, -0.08083138024958585, 0.0, 1.0, 43953.5374030922], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3016800.0000, 
sim time next is 3018600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 24.0, 23.43503271167145, -0.08983894386800872, 0.0, 1.0, 31757.20743415983], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.5, 0.45291939263928754, 0.47005368537733044, 0.0, 1.0, 0.15122479730552302], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9801313], dtype=float32), 1.3338451]. 
=============================================
[2019-04-07 18:45:36,489] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:45:36,489] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:45:36,493] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run44
[2019-04-07 18:45:40,461] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.6991818e-29 4.3418866e-27 9.6209667e-26 5.6400586e-19 3.5665730e-25
 1.0000000e+00 7.5870066e-20 1.2808563e-21], sum to 1.0000
[2019-04-07 18:45:40,461] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8134
[2019-04-07 18:45:40,525] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 24.0, 23.5345357736723, 0.1107485447733379, 0.0, 1.0, 69775.0091327239], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3276000.0000, 
sim time next is 3277800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 24.0, 23.45516311299975, 0.04225273833632007, 0.0, 1.0, 55854.12784932493], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.5, 0.4545969260833124, 0.5140842461121067, 0.0, 1.0, 0.2659720373777377], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8167837], dtype=float32), -0.065607615]. 
=============================================
[2019-04-07 18:45:43,329] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0995844e-29 1.3340521e-27 2.6526976e-25 1.3264096e-20 2.3118269e-25
 1.0000000e+00 4.5160010e-20 8.3287157e-22], sum to 1.0000
[2019-04-07 18:45:43,329] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7994
[2019-04-07 18:45:43,376] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 24.0, 23.63017698370287, -0.0686938011343653, 0.0, 1.0, 50495.68318261334], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3634200.0000, 
sim time next is 3636000.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 24.0, 23.63777518917008, -0.06484368289878166, 0.0, 1.0, 30849.74921855249], 
processed observation next is [0.0, 0.08695652173913043, 0.7119113573407203, 0.25, 0.0, 0.0, 0.5, 0.4698145990975065, 0.4783854390337394, 0.0, 1.0, 0.1469035677073928], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.53566664], dtype=float32), 1.0090342]. 
=============================================
[2019-04-07 18:45:43,410] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[96.23098 ]
 [95.45539 ]
 [94.77302 ]
 [94.37387 ]
 [94.116554]], R is [[96.87871552]
 [96.90992737]
 [96.94082642]
 [96.97142029]
 [97.00170898]].
[2019-04-07 18:45:44,663] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6766418e-28 4.1548483e-27 1.9365036e-24 3.7641780e-18 1.6746204e-23
 1.0000000e+00 4.2578734e-20 1.8968873e-20], sum to 1.0000
[2019-04-07 18:45:44,663] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6272
[2019-04-07 18:45:44,851] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.9, 65.0, 182.0, 2.0, 24.0, 23.76178940529542, -0.08488883627074228, 1.0, 1.0, 53619.01153801213], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1947600.0000, 
sim time next is 1949400.0000, 
raw observation next is [-3.65, 63.5, 137.0, 0.0, 24.0, 23.91190009671133, -0.06498204355255972, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3614958448753463, 0.635, 0.45666666666666667, 0.0, 0.5, 0.49265834139261094, 0.47833931881581343, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0950851], dtype=float32), -1.4867359]. 
=============================================
[2019-04-07 18:45:52,169] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7923792e-26 4.2978949e-24 2.6894013e-23 9.8570594e-17 1.0124216e-21
 1.0000000e+00 2.7095001e-17 6.0467267e-18], sum to 1.0000
[2019-04-07 18:45:52,170] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3585
[2019-04-07 18:45:52,368] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 54.0, 108.5, 782.0, 24.0, 23.28584404532073, -0.08064074955850266, 0.0, 1.0, 18710.86170742491], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3063600.0000, 
sim time next is 3065400.0000, 
raw observation next is [-3.5, 54.5, 111.0, 805.0, 24.0, 23.2715564805002, -0.08151509329253044, 0.0, 1.0, 18709.332203299055], 
processed observation next is [0.0, 0.4782608695652174, 0.36565096952908593, 0.545, 0.37, 0.8895027624309392, 0.5, 0.4392963733750168, 0.47282830223582323, 0.0, 1.0, 0.08909205811094788], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5115321], dtype=float32), 1.0246977]. 
=============================================
[2019-04-07 18:46:21,317] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:46:21,317] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:46:21,322] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run44
[2019-04-07 18:46:22,196] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2122032e-31 2.1873860e-30 2.6923110e-27 1.8559090e-21 3.0836191e-27
 1.0000000e+00 2.2221731e-21 2.3958842e-23], sum to 1.0000
[2019-04-07 18:46:22,196] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4014
[2019-04-07 18:46:22,246] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 84.0, 95.0, 0.0, 24.0, 23.98867372067635, 0.06827724507915041, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1684800.0000, 
sim time next is 1686600.0000, 
raw observation next is [1.1, 86.0, 107.0, 0.0, 24.0, 23.89504078958651, 0.03861936001006961, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.86, 0.3566666666666667, 0.0, 0.5, 0.49125339913220917, 0.5128731200033566, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4662222], dtype=float32), 0.85316545]. 
=============================================
[2019-04-07 18:46:28,260] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0050007e-28 1.4201723e-27 2.0872883e-24 1.0118209e-18 1.6796619e-23
 1.0000000e+00 8.6083627e-20 6.5382385e-20], sum to 1.0000
[2019-04-07 18:46:28,260] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2680
[2019-04-07 18:46:28,341] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 62.5, 0.0, 0.0, 24.0, 23.60139604172217, 0.03185945281496511, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3886200.0000, 
sim time next is 3888000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 24.0, 23.49966202665273, 0.007851543517962176, 0.0, 1.0, 43977.21320731608], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.5, 0.4583051688877274, 0.5026171811726541, 0.0, 1.0, 0.20941530098721944], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41042173], dtype=float32), 0.6458449]. 
=============================================
[2019-04-07 18:46:28,372] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[97.96757]
 [98.50903]
 [98.16197]
 [98.4916 ]
 [99.01714]], R is [[98.13163757]
 [98.15032196]
 [97.91675568]
 [97.93759155]
 [97.95821381]].
[2019-04-07 18:46:37,366] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9119732e-27 8.5755844e-28 3.4989903e-25 4.5448643e-19 3.5532800e-24
 1.0000000e+00 2.8093499e-19 2.8108386e-20], sum to 1.0000
[2019-04-07 18:46:37,366] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2309
[2019-04-07 18:46:37,444] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.8, 24.5, 95.0, 0.0, 24.0, 23.55873548478687, -0.1141704917981029, 1.0, 1.0, 17296.677472435684], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2820600.0000, 
sim time next is 2822400.0000, 
raw observation next is [6.6, 25.0, 83.0, 38.0, 24.0, 23.78933661984439, -0.05860386782745974, 1.0, 1.0, 12453.607780153694], 
processed observation next is [1.0, 0.6956521739130435, 0.6454293628808865, 0.25, 0.27666666666666667, 0.041988950276243095, 0.5, 0.48244471832036595, 0.48046537739084677, 1.0, 1.0, 0.059302894191208065], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1818899], dtype=float32), 1.0315593]. 
=============================================
[2019-04-07 18:46:38,753] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1214017e-27 6.0606501e-26 2.6712921e-24 9.7316674e-18 1.4861175e-24
 1.0000000e+00 4.3342571e-20 1.4036486e-19], sum to 1.0000
[2019-04-07 18:46:38,753] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8442
[2019-04-07 18:46:38,826] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.5, 33.5, 114.0, 769.0, 24.0, 24.89686373556102, 0.1952630057724516, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4098600.0000, 
sim time next is 4100400.0000, 
raw observation next is [-1.0, 32.0, 117.5, 792.5, 24.0, 24.99992744671047, 0.2161286093604059, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.32, 0.39166666666666666, 0.8756906077348067, 0.5, 0.5833272872258725, 0.572042869786802, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.1014497], dtype=float32), 0.7081363]. 
=============================================
[2019-04-07 18:46:39,117] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:46:39,117] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:46:39,126] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res4/Eplus-env-sub_run44
[2019-04-07 18:46:46,025] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:46:46,025] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:46:46,028] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res7/Eplus-env-sub_run44
[2019-04-07 18:46:53,790] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5809145e-31 1.0044808e-30 1.4546758e-29 1.8637406e-22 1.3368832e-29
 1.0000000e+00 3.3516791e-22 4.1386881e-24], sum to 1.0000
[2019-04-07 18:46:53,791] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1522
[2019-04-07 18:46:53,858] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.5, 96.5, 0.0, 0.0, 24.0, 23.26286400344668, -0.1096928121765715, 0.0, 1.0, 63666.98256688778], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2874600.0000, 
sim time next is 2876400.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 24.0, 23.45480626102453, -0.1070505083442991, 0.0, 1.0, 29429.97451867085], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.0, 0.0, 0.5, 0.4545671884187108, 0.46431649721856694, 0.0, 1.0, 0.14014273580319453], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.69887394], dtype=float32), -0.039243653]. 
=============================================
[2019-04-07 18:46:54,235] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-07 18:46:54,243] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:46:54,243] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:46:54,251] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run99
[2019-04-07 18:46:54,278] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:46:54,279] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:46:54,283] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run99
[2019-04-07 18:46:54,306] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 18:46:54,307] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:46:54,313] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run99
[2019-04-07 18:47:32,218] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11473577], dtype=float32), 0.16336037]
[2019-04-07 18:47:32,218] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [17.74044373, 68.86014188, 0.0, 0.0, 24.0, 23.90213265548967, 0.201457571102045, 0.0, 0.0, 0.0]
[2019-04-07 18:47:32,218] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 18:47:32,220] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [1.3463878e-28 2.9519325e-27 1.3956035e-25 4.4380422e-19 4.6277233e-25
 1.0000000e+00 1.1566465e-19 1.1065881e-20], sampled 0.7795053796120369
[2019-04-07 18:48:05,097] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11473577], dtype=float32), 0.16336037]
[2019-04-07 18:48:05,097] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation this: [0.0, 45.5, 221.0, 245.0, 24.0, 23.15571788918431, -0.1121492950223962, 0.0, 1.0, 12463.724003163006]
[2019-04-07 18:48:05,097] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 18:48:05,099] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Softmax [4.0748595e-27 8.1734667e-26 2.4468777e-24 6.7204657e-18 6.1405008e-24
 1.0000000e+00 1.3975744e-18 1.0367242e-19], sampled 0.7378692443016689
[2019-04-07 18:48:31,984] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11473577], dtype=float32), 0.16336037]
[2019-04-07 18:48:31,985] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [-2.8, 55.0, 51.0, 18.0, 24.0, 23.54064874203367, -0.1597236013727229, 1.0, 1.0, 0.0]
[2019-04-07 18:48:31,985] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 18:48:31,986] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [7.7018139e-27 7.8948441e-26 3.7821581e-24 5.5211287e-18 1.2205324e-23
 1.0000000e+00 1.4921321e-18 1.4665119e-19], sampled 0.6078824230347092
[2019-04-07 18:48:46,549] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:NoisyNet noise sample: [array([0.11473577], dtype=float32), 0.16336037]
[2019-04-07 18:48:46,549] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation this: [1.0, 93.0, 0.0, 0.0, 24.0, 23.33158078168603, -0.1177424042005908, 0.0, 1.0, 59462.73110687543]
[2019-04-07 18:48:46,549] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 DEBUG:Observation forecast: []
[2019-04-07 18:48:46,550] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Softmax [2.9915133e-30 4.8578757e-29 3.6309388e-27 3.2228269e-20 1.0947069e-26
 1.0000000e+00 8.6546857e-21 5.4737233e-22], sampled 0.44965697756342826
[2019-04-07 18:49:14,838] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 18:49:36,558] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 18:49:38,602] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 18:49:39,627] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 1960000, evaluation results [1960000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 18:49:41,113] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:49:41,113] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:49:41,116] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res17/Eplus-env-sub_run44
[2019-04-07 18:49:41,885] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2690145e-28 5.0269067e-27 1.5236530e-25 8.0663241e-19 3.1332340e-25
 1.0000000e+00 1.9240863e-19 1.3144124e-19], sum to 1.0000
[2019-04-07 18:49:41,885] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2663
[2019-04-07 18:49:41,925] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.0, 25.0, 122.5, 855.0, 24.0, 25.71328253203415, 0.3839023311675804, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4968000.0000, 
sim time next is 4969800.0000, 
raw observation next is [6.5, 24.5, 123.0, 865.0, 24.0, 25.80747807178243, 0.4047296920940502, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6426592797783934, 0.245, 0.41, 0.9558011049723757, 0.5, 0.6506231726485359, 0.6349098973646834, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5386646], dtype=float32), -0.5392526]. 
=============================================
[2019-04-07 18:49:44,366] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:49:44,366] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:49:44,371] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res5/Eplus-env-sub_run44
[2019-04-07 18:49:46,597] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:49:46,597] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:49:46,602] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res14/Eplus-env-sub_run44
[2019-04-07 18:49:47,316] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:49:47,317] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:49:47,320] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res3/Eplus-env-sub_run44
[2019-04-07 18:49:49,269] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:49:49,270] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:49:49,273] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res8/Eplus-env-sub_run44
[2019-04-07 18:49:49,353] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:49:49,353] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:49:49,357] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res10/Eplus-env-sub_run44
[2019-04-07 18:49:58,109] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2948325e-29 1.4650187e-27 8.3355641e-26 1.9295475e-19 5.0422761e-25
 1.0000000e+00 7.9882654e-20 1.4151372e-20], sum to 1.0000
[2019-04-07 18:49:58,109] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0768
[2019-04-07 18:49:58,182] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 61.0, 112.0, 790.0, 24.0, 24.631755028021, 0.2103680862419249, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3495600.0000, 
sim time next is 3497400.0000, 
raw observation next is [1.5, 59.0, 115.0, 810.0, 24.0, 24.72900800506283, 0.2228163665224329, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5041551246537397, 0.59, 0.38333333333333336, 0.8950276243093923, 0.5, 0.5607506670885692, 0.5742721221741444, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14459306], dtype=float32), 0.5526924]. 
=============================================
[2019-04-07 18:50:07,633] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:50:07,633] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:50:07,647] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res9/Eplus-env-sub_run45
[2019-04-07 18:50:08,853] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0319683e-29 5.5979783e-28 2.5949077e-25 7.0526629e-19 3.9004140e-25
 1.0000000e+00 1.1701468e-19 3.4901593e-21], sum to 1.0000
[2019-04-07 18:50:08,853] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8033
[2019-04-07 18:50:08,884] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.0, 71.0, 113.0, 795.5, 24.0, 24.91525335379378, 0.2079695473430271, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3754800.0000, 
sim time next is 3756600.0000, 
raw observation next is [-2.5, 68.0, 115.0, 822.0, 24.0, 24.93857010007013, 0.2297346724210362, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.39335180055401664, 0.68, 0.38333333333333336, 0.9082872928176795, 0.5, 0.5782141750058442, 0.5765782241403454, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32636815], dtype=float32), 0.17963944]. 
=============================================
[2019-04-07 18:50:12,277] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:50:12,277] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:50:12,298] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res12/Eplus-env-sub_run45
[2019-04-07 18:50:19,176] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6329405e-24 3.0976318e-23 1.7370118e-22 9.5282450e-16 5.5133511e-22
 1.0000000e+00 8.7215475e-18 4.7283480e-18], sum to 1.0000
[2019-04-07 18:50:19,176] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3178
[2019-04-07 18:50:19,348] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.8, 70.0, 15.0, 205.5, 24.0, 23.4312345265451, -0.1539749453207195, 1.0, 1.0, 80089.52734205734], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 288000.0000, 
sim time next is 289800.0000, 
raw observation next is [-12.55, 68.5, 30.0, 386.0, 24.0, 23.84345829796423, -0.08247579914918828, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.11495844875346259, 0.685, 0.1, 0.4265193370165746, 0.5, 0.4869548581636858, 0.4725080669502706, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0160543], dtype=float32), 1.0578748]. 
=============================================
[2019-04-07 18:50:32,033] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.9132967e-26 3.9626093e-24 1.3815950e-22 8.0422748e-17 1.1541155e-21
 1.0000000e+00 6.9415773e-18 4.1535817e-19], sum to 1.0000
[2019-04-07 18:50:32,034] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5993
[2019-04-07 18:50:32,232] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 33.0, 42.5, 0.0, 24.0, 23.82611555782544, -0.1827635304340433, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 464400.0000, 
sim time next is 466200.0000, 
raw observation next is [-5.35, 32.5, 62.0, 0.0, 24.0, 23.87247507259396, -0.185466395875035, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.31440443213296404, 0.325, 0.20666666666666667, 0.0, 0.5, 0.4893729227161634, 0.43817786804165504, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6177416], dtype=float32), 0.83155626]. 
=============================================
[2019-04-07 18:50:34,736] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3164902e-26 2.2811747e-25 9.1455855e-24 1.8788430e-17 8.0642764e-23
 1.0000000e+00 2.0006273e-18 1.3688336e-19], sum to 1.0000
[2019-04-07 18:50:34,736] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6697
[2019-04-07 18:50:35,007] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-12.0, 58.0, 93.0, 444.0, 24.0, 24.28998441257087, 0.07592843829721262, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4005000.0000, 
sim time next is 4006800.0000, 
raw observation next is [-11.0, 53.0, 97.0, 571.0, 24.0, 24.7355146728252, 0.1344055840244933, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.15789473684210528, 0.53, 0.3233333333333333, 0.630939226519337, 0.5, 0.5612928894021, 0.5448018613414978, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40551537], dtype=float32), -0.43003878]. 
=============================================
[2019-04-07 18:50:47,974] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0862044e-26 1.3587109e-25 2.5792275e-24 1.8708685e-17 8.5313857e-25
 1.0000000e+00 3.8711672e-18 6.4985558e-19], sum to 1.0000
[2019-04-07 18:50:47,974] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4161
[2019-04-07 18:50:48,166] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 24.0, 23.02249597120933, -0.2293410157365606, 0.0, 1.0, 38669.55462629627], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 673200.0000, 
sim time next is 675000.0000, 
raw observation next is [-2.55, 63.5, 0.0, 0.0, 24.0, 23.01496994988763, -0.1979613767170623, 0.0, 1.0, 110937.76364185494], 
processed observation next is [0.0, 0.8260869565217391, 0.3919667590027701, 0.635, 0.0, 0.0, 0.5, 0.41791416249063573, 0.4340128744276459, 0.0, 1.0, 0.528275064961214], 
reward next is 0.7574, 
noisyNet noise sample is [array([0.29113296], dtype=float32), 0.892778]. 
=============================================
[2019-04-07 18:50:48,170] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[92.71668 ]
 [92.553474]
 [92.38573 ]
 [92.26731 ]
 [92.3217  ]], R is [[93.47697449]
 [93.54220581]
 [93.60678101]
 [93.67071533]
 [93.73400879]].
[2019-04-07 18:50:50,370] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9036573e-28 3.5892514e-28 1.0040620e-25 8.7896450e-19 1.9787913e-25
 1.0000000e+00 3.8179129e-20 1.8080764e-20], sum to 1.0000
[2019-04-07 18:50:50,376] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8181
[2019-04-07 18:50:50,462] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 24.0, 23.54298610295409, 0.0623293956486722, 0.0, 1.0, 130811.93961286407], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4660200.0000, 
sim time next is 4662000.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 24.0, 23.56411343146258, 0.1271608517985463, 0.0, 1.0, 123360.91835472878], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.57, 0.0, 0.0, 0.5, 0.46367611928854835, 0.5423869505995155, 0.0, 1.0, 0.5874329445463276], 
reward next is 0.6983, 
noisyNet noise sample is [array([1.4401191], dtype=float32), -0.56474143]. 
=============================================
[2019-04-07 18:50:50,477] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[100.3398 ]
 [ 99.34294]
 [ 99.9898 ]
 [100.88316]
 [101.4308 ]], R is [[101.14832306]
 [100.79964447]
 [100.79164886]
 [100.78372955]
 [100.77589417]].
[2019-04-07 18:51:07,947] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:51:07,947] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:51:07,954] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res13/Eplus-env-sub_run45
[2019-04-07 18:51:10,024] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:51:10,025] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:51:10,028] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res6/Eplus-env-sub_run45
[2019-04-07 18:51:14,159] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.80063859e-29 1.27911962e-26 1.29866334e-26 8.92077584e-19
 1.42203459e-23 1.00000000e+00 8.80437215e-20 1.04832024e-20], sum to 1.0000
[2019-04-07 18:51:14,160] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2302
[2019-04-07 18:51:14,210] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 39.5, 0.0, 0.0, 24.0, 23.49487263920319, -0.08785416010481796, 0.0, 1.0, 49186.78561259238], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4923000.0000, 
sim time next is 4924800.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 24.0, 23.52422927054057, -0.08770076215416066, 0.0, 1.0, 30219.886602611296], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.4, 0.0, 0.0, 0.5, 0.46035243921171415, 0.4707664126152798, 0.0, 1.0, 0.14390422191719665], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9666772], dtype=float32), -0.37731358]. 
=============================================
[2019-04-07 18:51:20,702] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7433546e-31 3.2944202e-30 1.0513240e-27 1.3572328e-20 1.3113127e-26
 1.0000000e+00 3.0131488e-22 5.0041706e-22], sum to 1.0000
[2019-04-07 18:51:20,702] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0834
[2019-04-07 18:51:20,734] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 92.0, 127.0, 0.0, 24.0, 24.43966252938922, 0.1878565277811722, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1337400.0000, 
sim time next is 1339200.0000, 
raw observation next is [1.1, 92.0, 120.0, 0.0, 24.0, 24.39733510228491, 0.1775684207343826, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.4, 0.0, 0.5, 0.5331112585237424, 0.5591894735781275, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8205738], dtype=float32), 0.9535417]. 
=============================================
[2019-04-07 18:51:22,330] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:51:22,330] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:51:22,334] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res16/Eplus-env-sub_run45
[2019-04-07 18:51:44,777] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4333106e-31 2.5268944e-29 4.0694052e-28 7.6193706e-21 1.1210342e-27
 1.0000000e+00 6.3835826e-22 9.4415196e-23], sum to 1.0000
[2019-04-07 18:51:44,777] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2254
[2019-04-07 18:51:44,870] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 24.0, 23.45194528219655, 0.007704701142957622, 0.0, 1.0, 35211.45625448536], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1400400.0000, 
sim time next is 1402200.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 24.0, 23.50878257824345, 0.04005600993777392, 0.0, 1.0, 54047.39843745023], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.5, 0.45906521485362095, 0.5133520033125913, 0.0, 1.0, 0.25736856398785823], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6427507], dtype=float32), 0.25947747]. 
=============================================
[2019-04-07 18:51:50,460] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1566600e-26 4.7747846e-26 3.9613400e-24 2.7572109e-18 5.5002613e-23
 1.0000000e+00 2.2964097e-18 2.2939396e-19], sum to 1.0000
[2019-04-07 18:51:50,460] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2572
[2019-04-07 18:51:50,585] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-5.05, 84.5, 0.0, 0.0, 24.0, 23.10545734434969, -0.166474366645841, 0.0, 1.0, 108330.14487676551], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1884600.0000, 
sim time next is 1886400.0000, 
raw observation next is [-5.6, 86.0, 0.0, 0.0, 24.0, 23.33262021354145, -0.1487387014740344, 0.0, 1.0, 54744.61810216985], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.86, 0.0, 0.0, 0.5, 0.4443850177951208, 0.4504204328419885, 0.0, 1.0, 0.26068865762938026], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6055093], dtype=float32), -0.6118499]. 
=============================================
[2019-04-07 18:51:50,625] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-07 18:51:50,629] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:51:50,629] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:51:50,634] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run100
[2019-04-07 18:51:50,661] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:51:50,665] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 18:51:50,665] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:51:50,671] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run100
[2019-04-07 18:51:50,697] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:51:50,705] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run100
[2019-04-07 18:54:13,923] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 18:54:34,354] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 18:54:37,371] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 18:54:38,396] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 1980000, evaluation results [1980000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
[2019-04-07 18:54:42,370] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0666567e-27 2.4979068e-26 5.6985254e-23 1.1562021e-17 1.4571510e-24
 1.0000000e+00 7.8101343e-18 4.8356953e-19], sum to 1.0000
[2019-04-07 18:54:42,370] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0512
[2019-04-07 18:54:42,510] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.1, 67.0, 0.0, 0.0, 24.0, 22.98667259301833, -0.2046797227680671, 0.0, 1.0, 42110.62820513222], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2356200.0000, 
sim time next is 2358000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 24.0, 22.91106768706481, -0.2189703673053253, 0.0, 1.0, 42221.24580297598], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.0, 0.0, 0.5, 0.40925564058873426, 0.42700987756489156, 0.0, 1.0, 0.20105355144274276], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5937802], dtype=float32), 2.1009114]. 
=============================================
[2019-04-07 18:54:42,529] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[92.67237 ]
 [92.788956]
 [92.961044]
 [93.09085 ]
 [93.13362 ]], R is [[92.69684601]
 [92.7698822 ]
 [92.84218597]
 [92.91376495]
 [92.98462677]].
[2019-04-07 18:54:53,097] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:54:53,097] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:54:53,100] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res11/Eplus-env-sub_run45
[2019-04-07 18:55:09,883] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3399634e-29 1.5387799e-27 1.1034611e-26 1.2513298e-19 8.2505646e-25
 1.0000000e+00 8.9514171e-20 4.7621365e-21], sum to 1.0000
[2019-04-07 18:55:09,896] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2103
[2019-04-07 18:55:10,072] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 24.0, 23.74796928189979, -0.08599892729912133, 1.0, 1.0, 8496.30788556198], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 891000.0000, 
sim time next is 892800.0000, 
raw observation next is [0.0, 72.0, 14.5, 0.0, 24.0, 23.51077336776335, -0.126977970120183, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.72, 0.04833333333333333, 0.0, 0.5, 0.4592311139802791, 0.457674009959939, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0110887], dtype=float32), 1.3337165]. 
=============================================
[2019-04-07 18:55:14,351] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.4732889e-26 1.4178414e-24 7.1188985e-23 1.0530995e-17 4.1129438e-22
 1.0000000e+00 8.3613302e-18 7.8104329e-18], sum to 1.0000
[2019-04-07 18:55:14,352] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9998
[2019-04-07 18:55:14,435] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.55, 54.0, 0.0, 0.0, 24.0, 22.73304338557922, -0.2914966061826448, 0.0, 1.0, 44589.63538672067], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2428200.0000, 
sim time next is 2430000.0000, 
raw observation next is [-7.8, 55.0, 0.0, 0.0, 24.0, 22.60970541427487, -0.3144716183560541, 0.0, 1.0, 44737.89042437626], 
processed observation next is [0.0, 0.13043478260869565, 0.24653739612188366, 0.55, 0.0, 0.0, 0.5, 0.3841421178562392, 0.3951761272146486, 0.0, 1.0, 0.21303757344941077], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1273453], dtype=float32), 0.39392287]. 
=============================================
[2019-04-07 18:55:14,486] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.50725]
 [86.89884]
 [87.00124]
 [87.05053]
 [87.10439]], R is [[86.1971283 ]
 [86.3351593 ]
 [86.47180939]
 [86.60709381]
 [86.7410202 ]].
[2019-04-07 18:55:15,211] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.4670904e-23 5.7203049e-23 2.1952788e-21 6.2570078e-15 7.7526558e-21
 1.0000000e+00 2.0035328e-15 6.5045149e-17], sum to 1.0000
[2019-04-07 18:55:15,211] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9383
[2019-04-07 18:55:15,271] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-9.5, 59.5, 0.0, 0.0, 24.0, 21.82266396609261, -0.4964186357717622, 0.0, 1.0, 44961.20221348965], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2446200.0000, 
sim time next is 2448000.0000, 
raw observation next is [-9.5, 58.0, 21.5, 228.0, 24.0, 21.79018212733161, -0.4800834142026335, 0.0, 1.0, 44937.415832632425], 
processed observation next is [0.0, 0.34782608695652173, 0.1994459833795014, 0.58, 0.07166666666666667, 0.25193370165745854, 0.5, 0.31584851061096764, 0.3399721952657888, 0.0, 1.0, 0.2139876944411068], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.75633997], dtype=float32), -0.013780419]. 
=============================================
[2019-04-07 18:55:15,298] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.36992]
 [84.87584]
 [85.29683]
 [85.68743]
 [85.98656]], R is [[84.73910522]
 [84.891716  ]
 [85.0428009 ]
 [85.19237518]
 [85.3404541 ]].
[2019-04-07 18:55:28,561] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8973762e-28 2.8183886e-26 1.1703938e-24 1.2028006e-18 2.4400279e-23
 1.0000000e+00 4.0247780e-18 2.0885282e-19], sum to 1.0000
[2019-04-07 18:55:28,562] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0976
[2019-04-07 18:55:28,625] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [6.6, 25.0, 83.0, 38.0, 24.0, 23.78933661984439, -0.05860386782745974, 1.0, 1.0, 12453.607780153694], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2822400.0000, 
sim time next is 2824200.0000, 
raw observation next is [6.3, 26.5, 71.0, 76.0, 24.0, 24.03661149700474, -0.1321428880618087, 1.0, 1.0, 8302.405186769129], 
processed observation next is [1.0, 0.6956521739130435, 0.6371191135734073, 0.265, 0.23666666666666666, 0.08397790055248619, 0.5, 0.5030509580837282, 0.45595237064606375, 1.0, 1.0, 0.03953526279413871], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00586695], dtype=float32), -0.47296932]. 
=============================================
[2019-04-07 18:55:30,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4457036e-30 1.6911928e-27 1.3551652e-25 3.4765160e-19 5.4928724e-26
 1.0000000e+00 1.3377061e-19 9.4345038e-22], sum to 1.0000
[2019-04-07 18:55:30,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4019
[2019-04-07 18:55:30,653] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.0, 28.0, 106.0, 713.0, 24.0, 23.90044161560645, 0.05045746977104254, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3664800.0000, 
sim time next is 3666600.0000, 
raw observation next is [11.5, 26.0, 111.0, 763.0, 24.0, 23.89561474646973, 0.0692391600053541, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.7811634349030472, 0.26, 0.37, 0.8430939226519337, 0.5, 0.4913012288724774, 0.5230797200017847, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.99071676], dtype=float32), 0.6671539]. 
=============================================
[2019-04-07 18:55:44,450] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6583765e-28 6.2409948e-28 1.6442598e-25 9.7817639e-18 5.6491498e-25
 1.0000000e+00 3.5071813e-18 5.5009381e-21], sum to 1.0000
[2019-04-07 18:55:44,464] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1255
[2019-04-07 18:55:44,504] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 26.0, 0.0, 0.0, 24.0, 24.70165224356602, 0.276339413030836, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 4998600.0000, 
sim time next is 5000400.0000, 
raw observation next is [4.0, 29.0, 0.0, 0.0, 24.0, 24.51579408555131, 0.2336742872371512, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5734072022160666, 0.29, 0.0, 0.0, 0.5, 0.5429828404626091, 0.5778914290790503, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4211812], dtype=float32), 0.8849667]. 
=============================================
[2019-04-07 18:55:48,928] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:55:48,928] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:55:48,960] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res15/Eplus-env-sub_run45
[2019-04-07 18:56:03,288] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2423093e-28 9.1035550e-28 1.7010941e-25 1.8390532e-19 1.5944306e-26
 1.0000000e+00 1.5462938e-20 7.5741885e-22], sum to 1.0000
[2019-04-07 18:56:03,288] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3307
[2019-04-07 18:56:03,333] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.49929507260835, 0.03213421207423264, 0.0, 1.0, 62260.600905477106], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3544200.0000, 
sim time next is 3546000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 24.0, 23.49151510538731, 0.01703500952706579, 0.0, 1.0, 34862.09011888018], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.5, 0.4576262587822759, 0.5056783365090219, 0.0, 1.0, 0.16600995294704848], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4241242], dtype=float32), 1.3154548]. 
=============================================
[2019-04-07 18:56:03,378] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[ 96.86015 ]
 [ 98.121284]
 [100.05042 ]
 [101.410034]
 [101.055504]], R is [[96.51473236]
 [96.53881836]
 [96.57343292]
 [96.60769653]
 [96.10434723]].
[2019-04-07 18:56:17,331] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1990797e-27 4.9799378e-26 4.2849457e-24 2.1955586e-18 1.0674336e-24
 1.0000000e+00 3.0977933e-18 5.5214810e-20], sum to 1.0000
[2019-04-07 18:56:17,331] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8989
[2019-04-07 18:56:17,367] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 41.0, 63.0, 515.0, 24.0, 23.81070732997166, 0.08344298100351306, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 3601800.0000, 
sim time next is 3603600.0000, 
raw observation next is [0.0, 39.0, 38.5, 328.5, 24.0, 23.72002764580093, 0.03980870342654767, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.46260387811634357, 0.39, 0.12833333333333333, 0.3629834254143646, 0.5, 0.4766689704834108, 0.5132695678088492, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08380209], dtype=float32), 0.13024108]. 
=============================================
[2019-04-07 18:56:33,129] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-07 18:56:33,129] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:56:33,132] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res2/Eplus-env-sub_run45
[2019-04-07 18:56:34,809] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0129952e-25 1.2883322e-24 3.8672461e-23 4.8834878e-18 1.1686149e-22
 1.0000000e+00 7.3400818e-17 4.8401266e-18], sum to 1.0000
[2019-04-07 18:56:34,810] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4940
[2019-04-07 18:56:34,923] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 30.0, 0.0, 0.0, 24.0, 23.01398393113227, -0.2240623837301149, 0.0, 1.0, 41728.295498734486], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2487600.0000, 
sim time next is 2489400.0000, 
raw observation next is [-0.35, 29.5, 0.0, 0.0, 24.0, 23.04770072513502, -0.1730278788771231, 0.0, 1.0, 131026.23995899833], 
processed observation next is [0.0, 0.8260869565217391, 0.45290858725761773, 0.295, 0.0, 0.0, 0.5, 0.42064172709458497, 0.4423240403742923, 0.0, 1.0, 0.6239344759952301], 
reward next is 0.6618, 
noisyNet noise sample is [array([1.7005991], dtype=float32), -1.3707049]. 
=============================================
[2019-04-07 18:56:41,187] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-07 18:56:41,189] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:56:41,189] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:56:41,213] EPLUS_ENV_Part4-Light-Pit-Train-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Train-Repeat-v1-res1/Eplus-env-sub_run101
[2019-04-07 18:56:41,235] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation job starts!
[2019-04-07 18:56:41,237] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:56:41,238] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation job starts!
[2019-04-07 18:56:41,239] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-07 18:56:41,903] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v1-res1/Eplus-env-sub_run101
[2019-04-07 18:56:42,057] EPLUS_ENV_Part4-Light-Pit-Test-Repeat-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/46/Eplus-env-Part4-Light-Pit-Test-Repeat-v2-res1/Eplus-env-sub_run101
[2019-04-07 18:59:03,144] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v1 INFO:Evaluation: average rewards by now are 2789.0480 70927233.8163 166.2180
[2019-04-07 18:59:03,472] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:NoisyNet noise sample: [array([0.11478129], dtype=float32), 0.16417977]
[2019-04-07 18:59:03,472] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation this: [2.323849157, 42.82818325, 88.732095325, 918.26569265, 24.0, 23.72282442729885, 0.09380515328295447, 0.0, 1.0, 0.0]
[2019-04-07 18:59:03,472] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 DEBUG:Observation forecast: []
[2019-04-07 18:59:03,473] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Softmax [1.2395707e-27 2.2274038e-26 5.8027829e-25 2.2712407e-18 1.6230537e-24
 1.0000000e+00 4.7391080e-19 3.8944322e-20], sampled 0.27160049997843305
[2019-04-07 18:59:23,059] A3C_EVAL-Part4-Light-Pit-Train-Repeat-v1 INFO:Evaluation: average rewards by now are 2782.4603 79463814.5229 95.0531
[2019-04-07 18:59:27,903] A3C_EVAL-Part4-Light-Pit-Test-Repeat-v2 INFO:Evaluation: average rewards by now are 2785.0735 83805026.4189 32.8860
[2019-04-07 18:59:28,926] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 2000000, evaluation results [2000000.0, 2782.4602722413592, 79463814.52292724, 95.05306144070829, 2789.048033151059, 70927233.8162715, 166.21801628696863, 2785.073544749966, 83805026.41892925, 32.88598103848735]
