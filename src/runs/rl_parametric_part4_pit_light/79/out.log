Using TensorFlow backend.
[2019-04-09 14:52:00,878] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v3', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.01, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-Repeat-v2', eval_act_func='part4_v4', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=10000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=5e-06, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=500000, metric_func='part4_v2', model_dir='None', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-Repeat-v2-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_v2', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=4.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=11, test_env=['Part4-Light-Pit-Test-Repeat-v3', 'Part4-Light-Pit-Test-Repeat-v4'], test_mode='Multiple', train_act_func='part4_v4', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=2.0, weight_initer='glorot_uniform', window_len=10)
[2019-04-09 14:52:00,878] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-09 14:52:00.919116: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-09 14:52:19,817] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-09 14:52:19,817] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-Repeat-v2', 'Part4-Light-Pit-Test-Repeat-v3', 'Part4-Light-Pit-Test-Repeat-v4'] ...
[2019-04-09 14:52:19,832] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation worker starts!
[2019-04-09 14:52:19,841] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation worker starts!
[2019-04-09 14:52:19,849] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation worker starts!
[2019-04-09 14:52:19,849] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:19,850] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-09 14:52:19,913] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:19,914] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res2/Eplus-env-sub_run1
[2019-04-09 14:52:20,851] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:20,853] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-09 14:52:20,924] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:20,925] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res3/Eplus-env-sub_run1
[2019-04-09 14:52:21,853] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:21,854] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-09 14:52:21,925] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:21,927] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res4/Eplus-env-sub_run1
[2019-04-09 14:52:22,855] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:22,856] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-09 14:52:22,949] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:22,951] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res5/Eplus-env-sub_run1
[2019-04-09 14:52:23,857] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:23,858] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-09 14:52:23,951] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:23,952] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res6/Eplus-env-sub_run1
[2019-04-09 14:52:24,859] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:24,860] A3C_AGENT_WORKER-Thread-7 INFO:Local worker starts!
[2019-04-09 14:52:24,942] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:24,944] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res7/Eplus-env-sub_run1
[2019-04-09 14:52:25,860] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:25,861] A3C_AGENT_WORKER-Thread-8 INFO:Local worker starts!
[2019-04-09 14:52:25,882] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-09 14:52:25,883] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:52:25,883] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:52:25,883] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:25,883] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:52:25,883] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:25,883] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:25,885] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run1
[2019-04-09 14:52:25,893] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run1
[2019-04-09 14:52:25,919] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run1
[2019-04-09 14:52:26,016] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:26,018] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res8/Eplus-env-sub_run1
[2019-04-09 14:52:26,862] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:26,863] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-09 14:52:27,007] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:27,009] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res9/Eplus-env-sub_run1
[2019-04-09 14:52:27,864] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:27,865] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-09 14:52:28,028] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:28,046] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res10/Eplus-env-sub_run1
[2019-04-09 14:52:28,865] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:28,866] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-09 14:52:29,107] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:29,109] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res11/Eplus-env-sub_run1
[2019-04-09 14:52:29,866] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:29,867] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-09 14:52:29,988] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:29,989] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res12/Eplus-env-sub_run1
[2019-04-09 14:52:30,868] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:30,884] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-09 14:52:30,995] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:30,996] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res13/Eplus-env-sub_run1
[2019-04-09 14:52:31,885] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:31,901] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-09 14:52:32,180] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:32,182] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res14/Eplus-env-sub_run1
[2019-04-09 14:52:32,902] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:32,903] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-09 14:52:33,091] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:33,093] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res15/Eplus-env-sub_run1
[2019-04-09 14:52:33,904] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:33,905] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-09 14:52:34,167] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:34,169] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res16/Eplus-env-sub_run1
[2019-04-09 14:52:34,905] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-09 14:52:34,907] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-09 14:52:35,096] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:52:35,098] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res17/Eplus-env-sub_run1
[2019-04-09 14:52:38,325] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.08250558 0.10491818 0.10621877 0.07429561 0.07882047 0.11056063
 0.10256822 0.09791551 0.08804722 0.07817259 0.07597726], sum to 1.0000
[2019-04-09 14:52:38,325] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9151
[2019-04-09 14:52:38,504] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 20.91375239504256, -0.75, 0.0, 1.0, 40.0, 27.91182655106121], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 0.0000, 
sim time next is 1200.0000, 
raw observation next is [2.4, 95.33333333333334, 0.0, 0.0, 19.0, 20.93221237583023, -0.4892434798891973, 0.0, 1.0, 60.0, 56.84179872318478], 
processed observation next is [0.0, 0.0, 0.5290858725761773, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.24435103131918576, 0.33691884003693423, 0.0, 1.0, 0.9, 0.5684179872318478], 
reward next is 0.4316, 
noisyNet noise sample is [array([-0.3298084], dtype=float32), -0.108471796]. 
=============================================
[2019-04-09 14:53:07,678] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-09 14:53:07,679] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [5.366666666666667, 70.0, 102.1666666666667, 239.1666666666667, 22.5, 27.88570066547169, 1.114187945205607, 1.0, 1.0, 40.0, 28.07620188175735]
[2019-04-09 14:53:07,680] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 14:53:07,680] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.10526456 0.11413698 0.08334234 0.09041151 0.07490205 0.11643185
 0.09418521 0.08781733 0.08388249 0.08048269 0.06914297], sampled 0.89163674944665
[2019-04-09 14:53:58,953] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2918.0888 130868.6692 1116.7481
[2019-04-09 14:53:58,976] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:53:59,092] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:54:10,872] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2842.6643 139952.9705 845.6720
[2019-04-09 14:54:10,892] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:54:11,036] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:54:14,136] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2821.4517 141360.0742 654.4805
[2019-04-09 14:54:14,156] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:54:14,264] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:54:15,158] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 2842.66429022711, 139952.97050059176, 845.6720465563882, 2918.0887743363655, 130868.66919466009, 1116.7481213237093, 2821.451680175422, 141360.07422529976, 654.4805016047421]
[2019-04-09 14:54:17,543] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.10122164 0.11299396 0.09668402 0.08182094 0.0755844  0.10874445
 0.0848011  0.10469065 0.07151803 0.07494931 0.08699149], sum to 1.0000
[2019-04-09 14:54:17,549] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0827
[2019-04-09 14:54:17,674] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 21.29795926296705, -0.5552696044896539, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 6000.0000, 
sim time next is 7200.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 21.00885040153976, -0.6259962230357498, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.2507375334616467, 0.29133459232141673, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6878754], dtype=float32), 0.61129194]. 
=============================================
[2019-04-09 14:54:18,033] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.11453567 0.12484867 0.07840852 0.08135612 0.08634702 0.11368848
 0.08943675 0.10345188 0.07201523 0.07723074 0.0586809 ], sum to 1.0000
[2019-04-09 14:54:18,033] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6827
[2019-04-09 14:54:18,182] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 22.12775256567219, -0.3547242425723285, 0.0, 1.0, 20.0, 23.267754130689468], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 14400.0000, 
sim time next is 15600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 22.07218005533774, -0.3487212666783235, 0.0, 1.0, 45.0, 31.045652554689248], 
processed observation next is [0.0, 0.17391304347826086, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.3393483379448116, 0.3837595777738922, 0.0, 1.0, 0.6, 0.31045652554689246], 
reward next is 0.6895, 
noisyNet noise sample is [array([0.66663384], dtype=float32), 0.36089185]. 
=============================================
[2019-04-09 14:54:18,268] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.12542498 0.11698765 0.08827637 0.08504029 0.08365653 0.10502359
 0.08545465 0.09865607 0.06197554 0.0824073  0.06709706], sum to 1.0000
[2019-04-09 14:54:18,268] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0588
[2019-04-09 14:54:18,372] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [7.699999999999999, 93.0, 0.0, 0.0, 19.0, 22.62486138911714, -0.2371854932729045, 0.0, 1.0, 45.0, 29.1354240375273], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 20400.0000, 
sim time next is 21600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 22.41474082999701, -0.344387066800443, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.3678950691664176, 0.38520431106651903, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2540223], dtype=float32), 0.9599775]. 
=============================================
[2019-04-09 14:54:18,548] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.1099958  0.12651987 0.09119567 0.07301738 0.09894096 0.10984821
 0.08778502 0.09654585 0.05888417 0.0812621  0.06600486], sum to 1.0000
[2019-04-09 14:54:18,548] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5419
[2019-04-09 14:54:18,691] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 22.49483749252224, -0.1642490518143725, 0.0, 1.0, 65.0, 61.06832866090527], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 22800.0000, 
sim time next is 24000.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 19.0, 22.9843705765653, -0.1102183751068384, 0.0, 1.0, 40.0, 40.194205924686926], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.41536421471377505, 0.46326054163105385, 0.0, 1.0, 0.5, 0.40194205924686927], 
reward next is 0.5981, 
noisyNet noise sample is [array([0.6114521], dtype=float32), 0.09069373]. 
=============================================
[2019-04-09 14:54:18,728] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[-0.11258131]
 [-0.25295964]
 [-0.3242612 ]
 [-0.13715354]
 [-0.18425553]], R is [[0.47897521]
 [0.86350214]
 [1.41395092]
 [2.39981127]
 [3.03853703]].
[2019-04-09 14:54:19,263] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.10914431 0.1256897  0.08150613 0.08250136 0.08813401 0.10408811
 0.09149009 0.09618971 0.07516007 0.08725234 0.05884414], sum to 1.0000
[2019-04-09 14:54:19,272] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2647
[2019-04-09 14:54:19,287] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.11607327 0.11608934 0.07976847 0.09311509 0.08552427 0.10324721
 0.09807724 0.09950399 0.06907832 0.08235244 0.05717035], sum to 1.0000
[2019-04-09 14:54:19,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8610
[2019-04-09 14:54:19,382] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [8.3, 86.0, 91.5, 0.0, 19.0, 23.35408521855809, -0.01265126863339789, 0.0, 1.0, 35.0, 28.196415893202193], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 46800.0000, 
sim time next is 48000.0000, 
raw observation next is [8.100000000000001, 86.0, 88.5, 0.0, 19.0, 23.36573415150827, -0.02268548587485457, 0.0, 1.0, 25.0, 25.364858432527264], 
processed observation next is [0.0, 0.5652173913043478, 0.6869806094182827, 0.86, 0.295, 0.0, 0.08333333333333333, 0.4471445126256892, 0.4924381713750485, 0.0, 1.0, 0.2, 0.25364858432527265], 
reward next is 0.7464, 
noisyNet noise sample is [array([-0.51715386], dtype=float32), -1.8927101]. 
=============================================
[2019-04-09 14:54:19,384] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [8.3, 86.0, 91.5, 0.0, 19.0, 22.50900304643557, -0.1510361722393367, 0.0, 1.0, 50.0, 52.27479863909425], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 46800.0000, 
sim time next is 48000.0000, 
raw observation next is [8.100000000000001, 86.0, 88.5, 0.0, 19.0, 22.8734102235385, -0.06598408425798984, 0.0, 1.0, 60.0, 58.42050214680191], 
processed observation next is [0.0, 0.5652173913043478, 0.6869806094182827, 0.86, 0.295, 0.0, 0.08333333333333333, 0.4061175186282083, 0.47800530524733675, 0.0, 1.0, 0.9, 0.5842050214680191], 
reward next is 0.4158, 
noisyNet noise sample is [array([-1.2585397], dtype=float32), -0.5690002]. 
=============================================
[2019-04-09 14:54:19,406] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[-0.14116451]
 [-0.23311007]
 [-0.20061052]
 [-0.18297538]
 [-0.17522025]], R is [[0.28051209]
 [0.75495899]
 [1.74740934]
 [2.40158772]
 [2.8291595 ]].
[2019-04-09 14:54:19,407] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[-0.13460636]
 [-0.09246926]
 [-0.10737122]
 [-0.059148  ]
 [-0.04778995]], R is [[0.59304684]
 [1.30515218]
 [1.95341063]
 [2.35622835]
 [3.11667395]].
[2019-04-09 14:54:19,638] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.12386718 0.11997999 0.08221578 0.09237992 0.08131152 0.10213213
 0.09084862 0.10869542 0.05630948 0.07565383 0.06660613], sum to 1.0000
[2019-04-09 14:54:19,638] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8394
[2019-04-09 14:54:19,666] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [7.366666666666667, 86.0, 74.16666666666667, 0.0, 19.0, 23.84911093798567, 0.07454735477435655, 0.0, 1.0, 35.0, 36.846001270489445], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 52800.0000, 
sim time next is 54000.0000, 
raw observation next is [7.2, 86.0, 64.5, 0.0, 19.0, 23.8810180167094, 0.07694667238692741, 0.0, 1.0, 55.0, 42.44311737544896], 
processed observation next is [0.0, 0.6521739130434783, 0.662049861495845, 0.86, 0.215, 0.0, 0.08333333333333333, 0.4900848347257834, 0.5256488907956425, 0.0, 1.0, 0.8, 0.42443117375448963], 
reward next is 0.5756, 
noisyNet noise sample is [array([-0.89868927], dtype=float32), 1.845157]. 
=============================================
[2019-04-09 14:54:19,670] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[-0.16512878]
 [-0.21601199]
 [-0.17669767]
 [-0.09419855]
 [-0.12533684]], R is [[0.41877013]
 [1.04612243]
 [1.37981129]
 [2.36601305]
 [2.70347857]].
[2019-04-09 14:54:19,863] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.10850275 0.13919982 0.07672735 0.09297795 0.07764573 0.10575595
 0.10115405 0.099107   0.0682811  0.07460961 0.05603874], sum to 1.0000
[2019-04-09 14:54:19,864] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6080
[2019-04-09 14:54:19,888] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.5, 86.0, 0.0, 0.0, 19.0, 24.1628253965897, 0.1398505274331222, 0.0, 1.0, 25.0, 34.40438924237861], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 61200.0000, 
sim time next is 62400.0000, 
raw observation next is [5.133333333333334, 87.0, 0.0, 0.0, 19.0, 24.13001629537938, 0.1137192781886851, 0.0, 1.0, 45.0, 30.625887201330322], 
processed observation next is [0.0, 0.7391304347826086, 0.6048014773776548, 0.87, 0.0, 0.0, 0.08333333333333333, 0.5108346912816151, 0.5379064260628951, 0.0, 1.0, 0.6, 0.3062588720133032], 
reward next is 0.6937, 
noisyNet noise sample is [array([0.8075905], dtype=float32), 0.18491037]. 
=============================================
[2019-04-09 14:54:20,139] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.11725183 0.12047993 0.0768526  0.08896354 0.08740015 0.09279384
 0.11104798 0.09043019 0.07015979 0.08321282 0.06140738], sum to 1.0000
[2019-04-09 14:54:20,141] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9515
[2019-04-09 14:54:20,315] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.4, 89.0, 0.0, 0.0, 19.0, 23.77383928378752, 0.1014576723569896, 0.0, 1.0, 65.0, 63.57141183551745], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 64800.0000, 
sim time next is 66000.0000, 
raw observation next is [4.200000000000001, 88.0, 0.0, 0.0, 19.0, 24.00396281299886, 0.1991226547443969, 0.0, 1.0, 65.0, 65.88873971485202], 
processed observation next is [0.0, 0.782608695652174, 0.5789473684210527, 0.88, 0.0, 0.0, 0.08333333333333333, 0.5003302344165718, 0.5663742182481323, 0.0, 1.0, 1.0, 0.6588873971485202], 
reward next is 0.3411, 
noisyNet noise sample is [array([-0.22558396], dtype=float32), -0.61663073]. 
=============================================
[2019-04-09 14:54:20,320] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[-0.10752031]
 [-0.16970843]
 [-0.11205827]
 [-0.22669855]
 [-0.17479911]], R is [[0.26360953]
 [0.62525928]
 [1.34151363]
 [2.02171469]
 [2.65687633]].
[2019-04-09 14:54:20,361] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.11893192 0.13412029 0.08263057 0.09487365 0.07312126 0.10091381
 0.09475989 0.09418064 0.06589434 0.07970017 0.06087348], sum to 1.0000
[2019-04-09 14:54:20,362] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6493
[2019-04-09 14:54:20,502] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.133333333333334, 87.0, 0.0, 0.0, 19.0, 24.87543611858066, 0.3186099025963593, 0.0, 1.0, 40.0, 45.725196678120525], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 62400.0000, 
sim time next is 63600.0000, 
raw observation next is [4.766666666666667, 88.0, 0.0, 0.0, 19.0, 24.94711572208021, 0.309092960747388, 0.0, 1.0, 45.0, 38.80749389191597], 
processed observation next is [0.0, 0.7391304347826086, 0.5946445060018468, 0.88, 0.0, 0.0, 0.08333333333333333, 0.5789263101733507, 0.603030986915796, 0.0, 1.0, 0.6, 0.3880749389191597], 
reward next is 0.6119, 
noisyNet noise sample is [array([0.86246884], dtype=float32), -0.23516904]. 
=============================================
[2019-04-09 14:54:20,563] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.12309023 0.12181123 0.07926372 0.09092152 0.08234355 0.09705482
 0.10005603 0.08898795 0.07118353 0.08772852 0.05755893], sum to 1.0000
[2019-04-09 14:54:20,564] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7710
[2019-04-09 14:54:20,701] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [2.333333333333333, 87.66666666666667, 0.0, 0.0, 19.0, 23.2923038031307, -0.02940303227889261, 0.0, 1.0, 45.0, 31.41941373910654], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 73200.0000, 
sim time next is 74400.0000, 
raw observation next is [1.966666666666667, 86.33333333333334, 0.0, 0.0, 19.0, 23.2037464474932, -0.01990045789832237, 0.0, 1.0, 50.0, 39.87868180479539], 
processed observation next is [0.0, 0.8695652173913043, 0.5170821791320407, 0.8633333333333334, 0.0, 0.0, 0.08333333333333333, 0.4336455372910999, 0.49336651403389253, 0.0, 1.0, 0.7, 0.3987868180479539], 
reward next is 0.6012, 
noisyNet noise sample is [array([-1.8479078], dtype=float32), 0.37207714]. 
=============================================
[2019-04-09 14:54:22,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.10979347 0.13696736 0.07901717 0.08647434 0.09297101 0.10069756
 0.08576149 0.09731676 0.06129034 0.09257901 0.05713142], sum to 1.0000
[2019-04-09 14:54:22,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3921
[2019-04-09 14:54:22,151] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 75.0, 0.0, 0.0, 19.0, 22.62862697931395, -0.1267751330946719, 0.0, 1.0, 45.0, 70.31569793957831], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 108000.0000, 
sim time next is 109200.0000, 
raw observation next is [-6.9, 72.66666666666667, 0.0, 0.0, 19.0, 22.96961038669549, -0.1230562697663882, 0.0, 1.0, 30.0, 39.803839937013024], 
processed observation next is [1.0, 0.2608695652173913, 0.27146814404432135, 0.7266666666666667, 0.0, 0.0, 0.08333333333333333, 0.4141341988912908, 0.45898124341120394, 0.0, 1.0, 0.3, 0.39803839937013025], 
reward next is 0.6020, 
noisyNet noise sample is [array([1.0701475], dtype=float32), -0.39677122]. 
=============================================
[2019-04-09 14:54:22,702] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.09945504 0.11111203 0.0792406  0.08172229 0.0683254  0.121374
 0.09628978 0.0993828  0.0762619  0.10933876 0.05749733], sum to 1.0000
[2019-04-09 14:54:22,702] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9359
[2019-04-09 14:54:22,777] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.09699902 0.1112453  0.08545911 0.11333465 0.06730558 0.11203446
 0.08945277 0.08594389 0.08602284 0.08575615 0.06644629], sum to 1.0000
[2019-04-09 14:54:22,777] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3484
[2019-04-09 14:54:22,953] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.799999999999999, 82.0, 189.0, 32.16666666666666, 22.5, 23.03524501351909, -0.2821573956294259, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 124800.0000, 
sim time next is 126000.0000, 
raw observation next is [-7.8, 86.0, 187.0, 24.5, 22.5, 22.7770077809119, -0.2333819656227453, 1.0, 1.0, 30.0, 36.77287295784669], 
processed observation next is [1.0, 0.4782608695652174, 0.24653739612188366, 0.86, 0.6233333333333333, 0.02707182320441989, 0.375, 0.39808398174265847, 0.4222060114590849, 1.0, 1.0, 0.3, 0.36772872957846686], 
reward next is 0.6323, 
noisyNet noise sample is [array([-0.13033801], dtype=float32), 0.06168088]. 
=============================================
[2019-04-09 14:54:22,956] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-7.8, 61.0, 41.0, 4.5, 22.5, 23.09570753420201, -0.1616954406561966, 1.0, 1.0, 30.0, 41.676712236551396], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 118800.0000, 
sim time next is 120000.0000, 
raw observation next is [-7.8, 65.33333333333334, 43.66666666666666, 1.5, 22.5, 23.46318212728166, -0.01820454874813154, 1.0, 1.0, 60.0, 77.54606198096356], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.6533333333333334, 0.14555555555555552, 0.0016574585635359116, 0.375, 0.45526517727347154, 0.49393181708395617, 1.0, 1.0, 0.9, 0.7754606198096355], 
reward next is 0.2245, 
noisyNet noise sample is [array([0.6883123], dtype=float32), 0.0017817488]. 
=============================================
[2019-04-09 14:54:22,976] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[-0.42155597]
 [-0.17626128]
 [-0.29093468]
 [-0.20447066]
 [-0.30784175]], R is [[0.34274188]
 [1.33931446]
 [2.0120852 ]
 [2.54120708]
 [3.51579499]].
[2019-04-09 14:54:22,977] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[-0.20796184]
 [-0.2033269 ]
 [-0.31586444]
 [-0.3152771 ]
 [-0.29563373]], R is [[-0.05320354]
 [ 0.53056139]
 [ 0.85242283]
 [ 1.31760359]
 [ 2.18386936]].
[2019-04-09 14:54:23,523] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.12020282 0.1307628  0.08485985 0.09741516 0.05863246 0.10579263
 0.08659207 0.07352956 0.0720768  0.09311406 0.07702181], sum to 1.0000
[2019-04-09 14:54:23,523] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5441
[2019-04-09 14:54:23,602] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.066666666666666, 61.0, 140.5, 421.0000000000001, 22.5, 24.9355099833129, 0.3425292001466331, 1.0, 1.0, 55.0, 68.66074351376133], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 135600.0000, 
sim time next is 136800.0000, 
raw observation next is [-6.7, 61.0, 143.5, 295.0, 22.5, 25.31186855016259, 0.2656769252575673, 1.0, 1.0, 40.0, 40.140320845478904], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.47833333333333333, 0.3259668508287293, 0.375, 0.6093223791802158, 0.5885589750858558, 1.0, 1.0, 0.5, 0.40140320845478905], 
reward next is 0.5986, 
noisyNet noise sample is [array([-1.0839003], dtype=float32), 1.0313772]. 
=============================================
[2019-04-09 14:54:23,655] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.09830012 0.12186778 0.08401514 0.08931668 0.07414564 0.10738153
 0.08463382 0.08224022 0.07881366 0.10343876 0.0758466 ], sum to 1.0000
[2019-04-09 14:54:23,655] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8951
[2019-04-09 14:54:23,852] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.7, 61.0, 104.5, 75.5, 22.5, 24.78336316800449, 0.3204169021395065, 1.0, 1.0, 65.0, 65.76427929499164], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 140400.0000, 
sim time next is 141600.0000, 
raw observation next is [-6.700000000000001, 62.0, 75.5, 55.16666666666666, 22.5, 25.17279782205946, 0.3178709851416863, 1.0, 1.0, 60.0, 52.95849963062328], 
processed observation next is [1.0, 0.6521739130434783, 0.2770083102493075, 0.62, 0.25166666666666665, 0.06095764272559852, 0.375, 0.5977331518382885, 0.6059569950472288, 1.0, 1.0, 0.9, 0.5295849963062328], 
reward next is 0.4704, 
noisyNet noise sample is [array([1.0388594], dtype=float32), 1.5791318]. 
=============================================
[2019-04-09 14:54:24,494] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.11126818 0.1314705  0.09851245 0.10145088 0.06738722 0.09863412
 0.08451062 0.07718685 0.0748439  0.08349943 0.07123577], sum to 1.0000
[2019-04-09 14:54:24,494] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3067
[2019-04-09 14:54:24,511] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.10604702 0.12802184 0.08861956 0.0927437  0.05424035 0.10893872
 0.07865107 0.08500129 0.07977471 0.10001397 0.07794775], sum to 1.0000
[2019-04-09 14:54:24,511] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4382
[2019-04-09 14:54:24,543] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.7, 61.0, 143.5, 295.0, 22.5, 25.67900491477938, 0.4159313806720807, 1.0, 1.0, 20.0, 39.639295050659705], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 136800.0000, 
sim time next is 138000.0000, 
raw observation next is [-6.700000000000001, 61.0, 146.5, 169.0, 22.5, 25.61952307174375, 0.390980070445288, 1.0, 1.0, 25.0, 36.90938439715353], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.48833333333333334, 0.1867403314917127, 0.375, 0.6349602559786458, 0.6303266901484293, 1.0, 1.0, 0.2, 0.36909384397153533], 
reward next is 0.6309, 
noisyNet noise sample is [array([-2.5344117], dtype=float32), -0.28281206]. 
=============================================
[2019-04-09 14:54:24,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[-0.29073215]
 [-0.3254158 ]
 [-0.40868306]
 [-0.36603037]
 [-0.4848652 ]], R is [[0.40257972]
 [1.00216103]
 [1.53935111]
 [1.84501839]
 [2.40877962]].
[2019-04-09 14:54:24,573] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.10088524 0.11748388 0.10074969 0.08832077 0.05726745 0.11317767
 0.08480193 0.07307602 0.09334783 0.10220672 0.06868281], sum to 1.0000
[2019-04-09 14:54:24,579] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5081
[2019-04-09 14:54:24,620] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 64.33333333333333, 0.0, 0.0, 22.5, 24.39162791172786, 0.1287610696146913, 1.0, 1.0, 55.0, 49.81831972137128], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 150000.0000, 
sim time next is 151200.0000, 
raw observation next is [-7.3, 61.0, 0.0, 0.0, 22.5, 24.16475732572638, 0.0982550843685307, 1.0, 1.0, 35.0, 38.67417694201559], 
processed observation next is [1.0, 0.782608695652174, 0.26038781163434904, 0.61, 0.0, 0.0, 0.375, 0.513729777143865, 0.5327516947895102, 1.0, 1.0, 0.4, 0.3867417694201559], 
reward next is 0.6133, 
noisyNet noise sample is [array([-1.685135], dtype=float32), -0.5530426]. 
=============================================
[2019-04-09 14:54:24,627] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.3, 64.33333333333333, 0.0, 0.0, 22.5, 23.50762260417061, -0.06229400308624889, 1.0, 1.0, 55.0, 59.055680085902], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 150000.0000, 
sim time next is 151200.0000, 
raw observation next is [-7.3, 61.0, 0.0, 0.0, 22.5, 23.39448047255917, -0.06868317710387088, 1.0, 1.0, 40.0, 38.560300884088285], 
processed observation next is [1.0, 0.782608695652174, 0.26038781163434904, 0.61, 0.0, 0.0, 0.375, 0.44954003937993087, 0.477105607632043, 1.0, 1.0, 0.5, 0.38560300884088283], 
reward next is 0.6144, 
noisyNet noise sample is [array([0.04873373], dtype=float32), -0.7549419]. 
=============================================
[2019-04-09 14:54:25,224] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.10673007 0.11888939 0.0858587  0.08914497 0.06406403 0.10327636
 0.09153269 0.08417561 0.08928639 0.09668177 0.07035996], sum to 1.0000
[2019-04-09 14:54:25,224] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5315
[2019-04-09 14:54:25,272] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.300000000000001, 67.66666666666667, 0.0, 0.0, 22.5, 25.84110069823843, 0.4099918728712961, 1.0, 1.0, 55.0, 52.5972139971078], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 148800.0000, 
sim time next is 150000.0000, 
raw observation next is [-7.3, 64.33333333333333, 0.0, 0.0, 22.5, 25.55045992218665, 0.3317181318823912, 1.0, 1.0, 40.0, 46.20474742351139], 
processed observation next is [1.0, 0.7391304347826086, 0.26038781163434904, 0.6433333333333333, 0.0, 0.0, 0.375, 0.6292049935155543, 0.6105727106274638, 1.0, 1.0, 0.5, 0.46204747423511394], 
reward next is 0.5380, 
noisyNet noise sample is [array([-1.0322436], dtype=float32), 1.7911172]. 
=============================================
[2019-04-09 14:54:25,276] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[-0.20859396]
 [-0.22006296]
 [-0.19734831]
 [-0.20773502]
 [-0.27422616]], R is [[0.38477379]
 [0.85495389]
 [1.20316923]
 [1.38794792]
 [2.3740685 ]].
[2019-04-09 14:54:25,734] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.11059386 0.13346215 0.07099099 0.07584197 0.09049189 0.11755107
 0.07911436 0.09132857 0.06533173 0.09433573 0.07095777], sum to 1.0000
[2019-04-09 14:54:25,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2860
[2019-04-09 14:54:25,790] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 19.0, 21.22773828652176, -0.5314572087844084, 0.0, 1.0, 65.0, 72.87069598886315], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 186000.0000, 
sim time next is 187200.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 21.42308816045965, -0.4997427929357474, 0.0, 1.0, 25.0, 47.584236356435945], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.28525734670497077, 0.33341906902141755, 0.0, 1.0, 0.2, 0.47584236356435944], 
reward next is 0.5242, 
noisyNet noise sample is [array([-0.14825758], dtype=float32), -0.67911553]. 
=============================================
[2019-04-09 14:54:26,226] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.10590879 0.13348338 0.10186934 0.09486096 0.06498049 0.10131415
 0.09149992 0.08555827 0.08647163 0.08482848 0.04922456], sum to 1.0000
[2019-04-09 14:54:26,227] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8727
[2019-04-09 14:54:26,280] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-8.9, 74.0, 0.0, 0.0, 19.0, 22.0964398648618, -0.3531206069855453, 0.0, 1.0, 50.0, 43.17795914632044], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 176400.0000, 
sim time next is 177600.0000, 
raw observation next is [-8.900000000000002, 74.0, 0.0, 0.0, 19.0, 22.13118855541545, -0.3181591609810957, 0.0, 1.0, 60.0, 61.38962029649608], 
processed observation next is [1.0, 0.043478260869565216, 0.2160664819944598, 0.74, 0.0, 0.0, 0.08333333333333333, 0.3442657129512874, 0.3939469463396348, 0.0, 1.0, 0.9, 0.6138962029649608], 
reward next is 0.3861, 
noisyNet noise sample is [array([0.9259986], dtype=float32), 1.4693557]. 
=============================================
[2019-04-09 14:54:26,511] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.1004913  0.13649385 0.08020769 0.08617801 0.07471397 0.12040872
 0.08523977 0.09757838 0.06459795 0.09336764 0.06072269], sum to 1.0000
[2019-04-09 14:54:26,511] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8315
[2019-04-09 14:54:26,681] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 19.0, 23.02470442852919, -0.1563187813091662, 0.0, 1.0, 65.0, 67.05023516821521], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 184800.0000, 
sim time next is 186000.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 23.07502220716127, -0.1638079742111734, 0.0, 1.0, 55.0, 47.352138849434134], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.42291851726343904, 0.4453973419296089, 0.0, 1.0, 0.8, 0.47352138849434133], 
reward next is 0.5265, 
noisyNet noise sample is [array([0.36970213], dtype=float32), -0.048257053]. 
=============================================
[2019-04-09 14:54:26,703] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[-0.07395109]
 [-0.16065428]
 [-0.0545277 ]
 [-0.01412258]
 [-0.09398771]], R is [[0.4218854 ]
 [0.74716419]
 [1.33244348]
 [1.81679249]
 [2.08408594]].
[2019-04-09 14:54:27,287] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.10699148 0.10291554 0.0963224  0.08347569 0.07813471 0.1237248
 0.08428082 0.08051265 0.07937413 0.09606172 0.06820605], sum to 1.0000
[2019-04-09 14:54:27,287] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6163
[2019-04-09 14:54:27,450] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.666666666666666, 76.0, 86.5, 0.0, 22.5, 23.44606961595424, -0.2223247289487039, 1.0, 1.0, 25.0, 38.11762122717977], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 207600.0000, 
sim time next is 208800.0000, 
raw observation next is [-7.3, 75.0, 101.5, 0.0, 22.5, 23.45408538877277, -0.2152030026639724, 1.0, 1.0, 45.0, 40.20708846956669], 
processed observation next is [1.0, 0.43478260869565216, 0.26038781163434904, 0.75, 0.3383333333333333, 0.0, 0.375, 0.4545071157310643, 0.42826566577867586, 1.0, 1.0, 0.6, 0.40207088469566693], 
reward next is 0.5979, 
noisyNet noise sample is [array([-0.90634817], dtype=float32), 0.040034007]. 
=============================================
[2019-04-09 14:54:29,329] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.10895292 0.13243306 0.09355202 0.10098721 0.0663012  0.11221021
 0.07056855 0.08033694 0.07710096 0.0879081  0.06964874], sum to 1.0000
[2019-04-09 14:54:29,329] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3779
[2019-04-09 14:54:29,387] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.4, 67.33333333333333, 149.0, 0.0, 22.5, 24.12231775654986, -0.03319085667340094, 1.0, 1.0, 25.0, 28.150762210734364], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 214800.0000, 
sim time next is 216000.0000, 
raw observation next is [-5.0, 65.0, 141.0, 0.0, 22.5, 23.84189251815828, -0.08059149033644493, 1.0, 1.0, 30.0, 24.334412310604485], 
processed observation next is [1.0, 0.5217391304347826, 0.32409972299168976, 0.65, 0.47, 0.0, 0.375, 0.4868243765131899, 0.47313616988785173, 1.0, 1.0, 0.3, 0.24334412310604484], 
reward next is 0.7567, 
noisyNet noise sample is [array([-0.2983432], dtype=float32), 0.31003636]. 
=============================================
[2019-04-09 14:54:29,393] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[-0.31647032]
 [-0.31303653]
 [-0.26625207]
 [-0.19596434]
 [-0.09786775]], R is [[0.45739853]
 [1.17131698]
 [1.69812894]
 [2.68114758]
 [3.21059275]].
[2019-04-09 14:54:30,239] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.10729372 0.1253376  0.0888923  0.08693337 0.07439541 0.10264634
 0.08310485 0.07472823 0.08962799 0.0967025  0.07033778], sum to 1.0000
[2019-04-09 14:54:30,242] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3947
[2019-04-09 14:54:30,274] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 22.19161292409817, -0.2227898958985357, 1.0, 1.0, 65.0, 66.49963342200375], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 237600.0000, 
sim time next is 238800.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 23.24720460763632, -0.1434760544269709, 1.0, 1.0, 30.0, 38.72146007089135], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.43726705063636, 0.452174648524343, 1.0, 1.0, 0.3, 0.38721460070891345], 
reward next is 0.6128, 
noisyNet noise sample is [array([1.087498], dtype=float32), 0.43292117]. 
=============================================
[2019-04-09 14:54:31,847] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.10181355 0.10886838 0.07988807 0.08211306 0.08563378 0.12416404
 0.09898838 0.10020604 0.07165691 0.0930056  0.05366215], sum to 1.0000
[2019-04-09 14:54:31,848] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7615
[2019-04-09 14:54:31,903] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-9.866666666666667, 69.0, 0.0, 0.0, 19.0, 19.66895947318122, -0.8381380248548315, 0.0, 1.0, 65.0, 59.33346243775418], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 274800.0000, 
sim time next is 276000.0000, 
raw observation next is [-10.23333333333333, 68.0, 0.0, 0.0, 19.0, 19.85590117962951, -0.7855524413780901, 0.0, 1.0, 50.0, 53.5958196856562], 
processed observation next is [1.0, 0.17391304347826086, 0.1791320406278856, 0.68, 0.0, 0.0, 0.08333333333333333, 0.1546584316357924, 0.2381491862073033, 0.0, 1.0, 0.7, 0.535958196856562], 
reward next is 0.4640, 
noisyNet noise sample is [array([-0.41943607], dtype=float32), 1.1185501]. 
=============================================
[2019-04-09 14:54:31,916] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[0.07694011]
 [0.07893763]
 [0.02507606]
 [0.09109613]
 [0.09012689]], R is [[0.48095948]
 [0.88281524]
 [1.527143  ]
 [2.51187158]
 [3.23279905]].
[2019-04-09 14:54:32,064] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.12230017 0.1474589  0.09675149 0.07888422 0.06816695 0.09002049
 0.08888427 0.08311725 0.08481623 0.08367478 0.05592521], sum to 1.0000
[2019-04-09 14:54:32,067] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1228
[2019-04-09 14:54:32,217] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.566666666666667, 68.33333333333334, 0.0, 0.0, 19.0, 23.42585033877637, -0.07321924035043494, 0.0, 1.0, 30.0, 24.587022072433662], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 249600.0000, 
sim time next is 250800.0000, 
raw observation next is [-3.733333333333333, 71.66666666666667, 0.0, 0.0, 19.0, 23.21628257720625, -0.1215409937357821, 0.0, 1.0, 20.0, 22.53850015671938], 
processed observation next is [1.0, 0.9130434782608695, 0.35918744228993543, 0.7166666666666667, 0.0, 0.0, 0.08333333333333333, 0.43469021476718755, 0.459486335421406, 0.0, 1.0, 0.1, 0.2253850015671938], 
reward next is 0.7746, 
noisyNet noise sample is [array([-0.41958106], dtype=float32), -1.995923]. 
=============================================
[2019-04-09 14:54:32,329] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.11618491 0.10622137 0.07538075 0.08526641 0.07195501 0.12130735
 0.07703901 0.10349743 0.07280912 0.10082062 0.06951807], sum to 1.0000
[2019-04-09 14:54:32,329] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0427
[2019-04-09 14:54:32,535] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-12.3, 67.0, 62.5, 384.5, 22.5, 21.58735156410157, -0.546464748870301, 1.0, 1.0, 35.0, 49.49048682763775], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 291600.0000, 
sim time next is 292800.0000, 
raw observation next is [-12.1, 65.66666666666667, 84.16666666666667, 383.5, 22.5, 21.63962115293888, -0.5994133752108871, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.12742382271468145, 0.6566666666666667, 0.28055555555555556, 0.42375690607734806, 0.375, 0.3033017627449066, 0.30019554159637096, 1.0, 1.0, 0.0, 0.0], 
reward next is 0.8419, 
noisyNet noise sample is [array([-0.50839347], dtype=float32), -1.4242399]. 
=============================================
[2019-04-09 14:54:33,700] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.10661045 0.13344505 0.08097625 0.07525634 0.08966328 0.0993963
 0.08495219 0.10080548 0.0778188  0.09100909 0.06006678], sum to 1.0000
[2019-04-09 14:54:33,703] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9683
[2019-04-09 14:54:33,757] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.6, 67.0, 0.0, 0.0, 19.0, 23.06011859947355, -0.1435382724809017, 0.0, 1.0, 45.0, 45.16067703523841], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 277200.0000, 
sim time next is 278400.0000, 
raw observation next is [-10.96666666666667, 68.0, 0.0, 0.0, 19.0, 23.15521463886588, -0.106074969453419, 0.0, 1.0, 65.0, 70.53263434184777], 
processed observation next is [1.0, 0.21739130434782608, 0.15881809787626952, 0.68, 0.0, 0.0, 0.08333333333333333, 0.4296012199054899, 0.4646416768488603, 0.0, 1.0, 1.0, 0.7053263434184777], 
reward next is 0.2947, 
noisyNet noise sample is [array([1.4150901], dtype=float32), -0.15227453]. 
=============================================
[2019-04-09 14:54:33,873] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.08544414 0.11824664 0.08780484 0.09732431 0.07381972 0.12501478
 0.09001384 0.07889146 0.07263212 0.09274553 0.07806259], sum to 1.0000
[2019-04-09 14:54:33,877] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4169
[2019-04-09 14:54:33,907] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 42.0, 72.0, 501.3333333333333, 22.5, 24.0766598378011, 0.01374595247275789, 1.0, 1.0, 40.0, 41.90401760756652], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 314400.0000, 
sim time next is 315600.0000, 
raw observation next is [-9.5, 42.0, 62.33333333333334, 437.5, 22.5, 24.32578652162646, 0.02959504248578413, 1.0, 1.0, 35.0, 36.007350864719214], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.2077777777777778, 0.48342541436464087, 0.375, 0.5271488768022049, 0.509865014161928, 1.0, 1.0, 0.4, 0.3600735086471921], 
reward next is 0.6399, 
noisyNet noise sample is [array([1.9136193], dtype=float32), -0.8590124]. 
=============================================
[2019-04-09 14:54:34,145] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.11042932 0.1143636  0.08121261 0.07580538 0.07121007 0.11923195
 0.09252007 0.07521553 0.07729788 0.09978206 0.08293154], sum to 1.0000
[2019-04-09 14:54:34,145] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3288
[2019-04-09 14:54:34,224] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 44.0, 92.83333333333334, 629.6666666666667, 22.5, 24.24823428951131, 0.06294795203952548, 1.0, 1.0, 45.0, 33.0670341717872], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 308400.0000, 
sim time next is 309600.0000, 
raw observation next is [-9.5, 44.0, 88.5, 627.0, 22.5, 24.38556690890493, -0.01881598522557512, 1.0, 1.0, 30.0, 30.879012745620766], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.44, 0.295, 0.6928176795580111, 0.375, 0.5321305757420776, 0.4937280049248083, 1.0, 1.0, 0.3, 0.30879012745620765], 
reward next is 0.6912, 
noisyNet noise sample is [array([0.6147525], dtype=float32), 0.80485713]. 
=============================================
[2019-04-09 14:54:34,395] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.09701861 0.11778648 0.08021556 0.08771004 0.07742671 0.11458708
 0.10010768 0.07769278 0.07067743 0.09703575 0.07974193], sum to 1.0000
[2019-04-09 14:54:34,396] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3994
[2019-04-09 14:54:34,539] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.23333333333333, 47.33333333333334, 86.83333333333334, 741.3333333333334, 22.5, 25.97675596484494, 0.2941632198265809, 1.0, 1.0, 65.0, 81.40072894923514], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 303600.0000, 
sim time next is 304800.0000, 
raw observation next is [-9.866666666666667, 45.66666666666667, 85.0, 736.8333333333334, 22.5, 25.97103777328694, 0.4202338693151497, 1.0, 1.0, 35.0, 56.63772763008965], 
processed observation next is [1.0, 0.5217391304347826, 0.18928901200369344, 0.4566666666666667, 0.2833333333333333, 0.8141804788213628, 0.375, 0.6642531477739118, 0.6400779564383833, 1.0, 1.0, 0.4, 0.5663772763008965], 
reward next is 0.4336, 
noisyNet noise sample is [array([0.17838156], dtype=float32), -2.3926113]. 
=============================================
[2019-04-09 14:54:34,771] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.09314039 0.13610347 0.11177754 0.103814   0.06219231 0.11097729
 0.07442145 0.07175598 0.08696814 0.08517555 0.06367388], sum to 1.0000
[2019-04-09 14:54:34,772] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9585
[2019-04-09 14:54:35,049] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.8, 70.0, 0.0, 0.0, 22.5, 20.64057302361445, -0.5774966414939701, 0.0, 1.0, 25.0, 68.41784344475931], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 331200.0000, 
sim time next is 332400.0000, 
raw observation next is [-12.8, 72.33333333333334, 0.0, 0.0, 19.0, 21.01934819995681, -0.4959066576036762, 0.0, 1.0, 65.0, 66.3549951480042], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7233333333333334, 0.0, 0.0, 0.08333333333333333, 0.25161234999640075, 0.3346977807987746, 0.0, 1.0, 1.0, 0.663549951480042], 
reward next is 0.3365, 
noisyNet noise sample is [array([-0.1878032], dtype=float32), 2.5719254]. 
=============================================
[2019-04-09 14:54:35,629] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.09179189 0.12195528 0.07858728 0.08116987 0.08382254 0.12090342
 0.08874258 0.09042101 0.07489454 0.09020477 0.0775068 ], sum to 1.0000
[2019-04-09 14:54:35,630] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7437
[2019-04-09 14:54:35,631] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.07068743 0.12002927 0.08324087 0.09297433 0.08154659 0.1309603
 0.09282848 0.0775293  0.08616373 0.09642885 0.06761084], sum to 1.0000
[2019-04-09 14:54:35,638] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1838
[2019-04-09 14:54:35,701] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.23333333333333, 46.66666666666666, 20.0, 201.0, 22.5, 23.88487624367185, -0.1708255339700941, 1.0, 1.0, 45.0, 37.75721866362839], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 319200.0000, 
sim time next is 320400.0000, 
raw observation next is [-10.6, 49.0, 12.0, 123.0, 22.5, 23.57882073847195, -0.1031589585061987, 1.0, 1.0, 20.0, 30.360479646070324], 
processed observation next is [1.0, 0.7391304347826086, 0.1689750692520776, 0.49, 0.04, 0.13591160220994475, 0.375, 0.4649017282059959, 0.4656136804979338, 1.0, 1.0, 0.1, 0.30360479646070326], 
reward next is 0.6964, 
noisyNet noise sample is [array([-0.678834], dtype=float32), 1.0434728]. 
=============================================
[2019-04-09 14:54:35,703] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-9.5, 44.0, 88.5, 627.0, 22.5, 24.62484540236721, 0.1667682681913725, 1.0, 1.0, 45.0, 32.64968671695059], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 309600.0000, 
sim time next is 310800.0000, 
raw observation next is [-9.5, 43.33333333333334, 84.16666666666667, 624.3333333333334, 22.5, 24.66042206180298, 0.1933670416906222, 1.0, 1.0, 50.0, 42.232026361199146], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4333333333333334, 0.28055555555555556, 0.6898710865561695, 0.375, 0.555035171816915, 0.5644556805635407, 1.0, 1.0, 0.7, 0.4223202636119915], 
reward next is 0.5777, 
noisyNet noise sample is [array([1.6553729], dtype=float32), -0.38987172]. 
=============================================
[2019-04-09 14:54:36,053] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.09420229 0.11793423 0.07743292 0.08504635 0.08253749 0.11964612
 0.0923617  0.0697178  0.07751846 0.10040944 0.08319329], sum to 1.0000
[2019-04-09 14:54:36,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6570
[2019-04-09 14:54:36,092] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 42.0, 47.0, 358.5, 22.5, 25.4553221140006, 0.3440713161642363, 1.0, 1.0, 20.0, 42.603941366988096], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 316800.0000, 
sim time next is 318000.0000, 
raw observation next is [-9.866666666666667, 44.33333333333334, 31.66666666666666, 279.5, 22.5, 25.52784537944335, 0.3215798665506024, 1.0, 1.0, 45.0, 37.9337164777317], 
processed observation next is [1.0, 0.6956521739130435, 0.18928901200369344, 0.4433333333333334, 0.10555555555555554, 0.30883977900552484, 0.375, 0.6273204482869458, 0.6071932888502009, 1.0, 1.0, 0.6, 0.379337164777317], 
reward next is 0.6207, 
noisyNet noise sample is [array([-0.655359], dtype=float32), -0.13474259]. 
=============================================
[2019-04-09 14:54:36,205] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[-0.08267021]
 [-0.06436936]
 [-0.09950989]
 [-0.08822557]
 [-0.07627881]], R is [[0.65265495]
 [1.22008896]
 [1.60117018]
 [1.99035501]
 [2.66203117]].
[2019-04-09 14:54:36,697] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.10783841 0.12374374 0.09445225 0.0806561  0.07560653 0.10010283
 0.09717974 0.08377226 0.09034272 0.08707255 0.05923285], sum to 1.0000
[2019-04-09 14:54:36,698] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2332
[2019-04-09 14:54:36,730] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-14.5, 69.0, 0.0, 0.0, 19.0, 20.46054959211319, -0.736485339521531, 0.0, 1.0, 20.0, 27.97243548472649], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 349200.0000, 
sim time next is 350400.0000, 
raw observation next is [-14.66666666666667, 69.0, 0.0, 0.0, 19.0, 20.34109207406276, -0.7695312020995274, 0.0, 1.0, 25.0, 25.621256839408215], 
processed observation next is [1.0, 0.043478260869565216, 0.05632502308402576, 0.69, 0.0, 0.0, 0.08333333333333333, 0.19509100617189676, 0.24348959930015754, 0.0, 1.0, 0.2, 0.25621256839408213], 
reward next is 0.7438, 
noisyNet noise sample is [array([-0.36212224], dtype=float32), -0.6040714]. 
=============================================
[2019-04-09 14:54:36,904] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.10455891 0.11922487 0.07825683 0.07021525 0.08490817 0.12645973
 0.09194911 0.08786019 0.06942098 0.10506423 0.06208177], sum to 1.0000
[2019-04-09 14:54:36,906] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.08361817 0.12870495 0.08661288 0.07758202 0.0852806  0.1190498
 0.08279657 0.07717402 0.07000095 0.10265309 0.08652695], sum to 1.0000
[2019-04-09 14:54:36,906] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7706
[2019-04-09 14:54:36,907] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3837
[2019-04-09 14:54:36,953] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.4, 71.66666666666667, 0.0, 0.0, 19.0, 21.0791953683142, -0.670058547153117, 0.0, 1.0, 55.0, 54.47733749321972], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 358800.0000, 
sim time next is 360000.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 19.0, 20.8708489955082, -0.7183924258360778, 0.0, 1.0, 35.0, 43.06992888497119], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.08333333333333333, 0.23923741629235007, 0.26053585805464075, 0.0, 1.0, 0.4, 0.4306992888497119], 
reward next is 0.5693, 
noisyNet noise sample is [array([0.46185532], dtype=float32), -0.41348624]. 
=============================================
[2019-04-09 14:54:36,964] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[0.07576246]
 [0.02393968]
 [0.1748709 ]
 [0.09070859]
 [0.07795098]], R is [[0.70869493]
 [1.1568346 ]
 [1.60306036]
 [1.7912904 ]
 [2.07585096]].
[2019-04-09 14:54:37,004] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-9.5, 42.66666666666667, 80.0, 598.6666666666667, 22.5, 25.20521129165081, 0.1593613606457542, 1.0, 1.0, 25.0, 25.468096850212486], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 312000.0000, 
sim time next is 313200.0000, 
raw observation next is [-9.5, 42.0, 76.0, 550.0, 22.5, 23.92690123488907, 0.2062939981151788, 1.0, 1.0, 55.0, 55.696654903932455], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.25333333333333335, 0.6077348066298343, 0.375, 0.49390843624075575, 0.5687646660383929, 1.0, 1.0, 0.8, 0.5569665490393245], 
reward next is 0.4430, 
noisyNet noise sample is [array([0.36686888], dtype=float32), -0.5852053]. 
=============================================
[2019-04-09 14:54:37,317] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.1043629  0.12118221 0.08594789 0.07325046 0.08938243 0.12019792
 0.07601974 0.09746601 0.07460157 0.09445859 0.06313027], sum to 1.0000
[2019-04-09 14:54:37,317] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4483
[2019-04-09 14:54:37,358] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 19.0, 19.07371270561051, -1.080011211281943, 0.0, 1.0, 35.0, 26.8297430592578], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 367200.0000, 
sim time next is 368400.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 19.0, 18.83310323099118, -1.110669559861688, 0.0, 1.0, 35.0, 29.678612457939685], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.08333333333333333, 0.0694252692492651, 0.12977681337943733, 0.0, 1.0, 0.4, 0.29678612457939685], 
reward next is 0.6909, 
noisyNet noise sample is [array([0.69502276], dtype=float32), 1.4360371]. 
=============================================
[2019-04-09 14:54:37,842] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.10555229 0.10159463 0.08465294 0.07592725 0.0732486  0.12745865
 0.07904603 0.08869571 0.0946136  0.09874282 0.07046748], sum to 1.0000
[2019-04-09 14:54:37,843] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1156
[2019-04-09 14:54:37,942] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-15.6, 90.0, 32.0, 607.5, 22.5, 21.47435370398327, -0.6514479158927124, 1.0, 1.0, 20.0, 34.322919246110246], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 378000.0000, 
sim time next is 379200.0000, 
raw observation next is [-15.23333333333333, 82.0, 36.66666666666666, 694.5, 22.5, 21.46299657073591, -0.6558731308455948, 1.0, 1.0, 20.0, 30.170591935654123], 
processed observation next is [1.0, 0.391304347826087, 0.04062788550323182, 0.82, 0.12222222222222219, 0.7674033149171271, 0.375, 0.28858304756132586, 0.2813756230514684, 1.0, 1.0, 0.1, 0.30170591935654123], 
reward next is 0.4977, 
noisyNet noise sample is [array([-0.99204755], dtype=float32), -1.3177557]. 
=============================================
[2019-04-09 14:54:38,847] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.11283827 0.13562483 0.08532681 0.08783151 0.07599422 0.09090687
 0.10374103 0.08583296 0.07256472 0.09302282 0.05631592], sum to 1.0000
[2019-04-09 14:54:38,847] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0949
[2019-04-09 14:54:38,898] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-14.83333333333333, 69.0, 0.0, 0.0, 19.0, 21.97634738656718, -0.4182172696514315, 0.0, 1.0, 40.0, 40.300617065898926], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 351600.0000, 
sim time next is 352800.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 19.0, 21.75954615836642, -0.6118841276232319, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.08333333333333333, 0.31329551319720156, 0.2960386241255894, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04847749], dtype=float32), 0.9366467]. 
=============================================
[2019-04-09 14:54:40,588] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.07303196 0.10166427 0.08738312 0.08904427 0.08953853 0.12871967
 0.10485773 0.07210048 0.08586605 0.10630509 0.06148884], sum to 1.0000
[2019-04-09 14:54:40,588] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2902
[2019-04-09 14:54:40,640] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.3, 38.66666666666666, 0.0, 0.0, 22.5, 23.99756575227734, -0.05591902558870077, 1.0, 1.0, 50.0, 40.47667302482344], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 409200.0000, 
sim time next is 410400.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 22.5, 23.54857436293397, -0.1254707321569863, 1.0, 1.0, 30.0, 34.02312004905436], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.375, 0.46238119691116414, 0.4581764226143379, 1.0, 1.0, 0.3, 0.3402312004905436], 
reward next is 0.6598, 
noisyNet noise sample is [array([-0.5246572], dtype=float32), 0.2782834]. 
=============================================
[2019-04-09 14:54:40,646] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.09392801 0.11391518 0.08288298 0.08639637 0.08299075 0.12736182
 0.08671927 0.07601708 0.0699928  0.0944455  0.0853502 ], sum to 1.0000
[2019-04-09 14:54:40,646] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3583
[2019-04-09 14:54:40,720] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0979159  0.1180393  0.08775342 0.07688443 0.07841216 0.12667441
 0.09045358 0.07287047 0.07822678 0.0983289  0.07444058], sum to 1.0000
[2019-04-09 14:54:40,720] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3201
[2019-04-09 14:54:40,767] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.2, 57.00000000000001, 60.16666666666666, 758.1666666666667, 22.5, 22.9364318765403, -0.2314965940356077, 1.0, 1.0, 65.0, 83.44644819316582], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 386400.0000, 
sim time next is 387600.0000, 
raw observation next is [-13.0, 54.0, 58.0, 787.5, 22.5, 23.84764796750771, -0.1229393914533388, 1.0, 1.0, 30.0, 52.493614355633795], 
processed observation next is [1.0, 0.4782608695652174, 0.10249307479224376, 0.54, 0.19333333333333333, 0.8701657458563536, 0.375, 0.4873039972923093, 0.45902020284888706, 1.0, 1.0, 0.3, 0.524936143556338], 
reward next is 0.4751, 
noisyNet noise sample is [array([0.2209476], dtype=float32), -0.040301148]. 
=============================================
[2019-04-09 14:54:41,018] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.2, 57.00000000000001, 60.16666666666666, 758.1666666666667, 22.5, 23.40503900157547, -0.2107237982965899, 1.0, 1.0, 50.0, 51.745404488350175], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 386400.0000, 
sim time next is 387600.0000, 
raw observation next is [-13.0, 54.0, 58.0, 787.5, 22.5, 23.49763237588243, -0.1923718499532706, 1.0, 1.0, 30.0, 40.34234960077089], 
processed observation next is [1.0, 0.4782608695652174, 0.10249307479224376, 0.54, 0.19333333333333333, 0.8701657458563536, 0.375, 0.4581360313235357, 0.4358760500155765, 1.0, 1.0, 0.3, 0.4034234960077089], 
reward next is 0.5966, 
noisyNet noise sample is [array([0.35860375], dtype=float32), -0.15573005]. 
=============================================
[2019-04-09 14:54:41,395] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.08485471 0.12319139 0.08453683 0.08197498 0.09098517 0.1254822
 0.08056108 0.07741885 0.07403874 0.09749161 0.07946443], sum to 1.0000
[2019-04-09 14:54:41,396] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0534
[2019-04-09 14:54:41,422] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-10.16666666666667, 44.0, 49.16666666666666, 841.1666666666667, 22.5, 23.44527565879471, -0.1452522783906547, 1.0, 1.0, 30.0, 18.03092320303808], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 397200.0000, 
sim time next is 398400.0000, 
raw observation next is [-9.833333333333334, 42.0, 46.16666666666666, 811.1666666666666, 22.5, 23.4277991312275, -0.1927487575532552, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1902123730378578, 0.42, 0.15388888888888885, 0.8963167587476979, 0.375, 0.4523165942689582, 0.43575041414891497, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5929437], dtype=float32), -0.54989684]. 
=============================================
[2019-04-09 14:54:42,387] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.09260881 0.12203864 0.07827797 0.08275114 0.08692401 0.1226844
 0.08644666 0.07974355 0.07399452 0.08905706 0.08547325], sum to 1.0000
[2019-04-09 14:54:42,387] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7869
[2019-04-09 14:54:42,403] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.10088012 0.11417531 0.07995367 0.07864747 0.06934603 0.11669575
 0.10198115 0.10635322 0.08395098 0.09195423 0.05606196], sum to 1.0000
[2019-04-09 14:54:42,407] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3483
[2019-04-09 14:54:42,435] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.23333333333333, 49.33333333333334, 0.0, 0.0, 19.0, 20.51837610584081, -0.9561536790951629, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 451200.0000, 
sim time next is 452400.0000, 
raw observation next is [-9.866666666666667, 46.66666666666667, 0.0, 0.0, 19.0, 19.98674393409565, -0.8808293875441865, 0.0, 1.0, 30.0, 68.64946908660689], 
processed observation next is [1.0, 0.21739130434782608, 0.18928901200369344, 0.46666666666666673, 0.0, 0.0, 0.08333333333333333, 0.1655619945079708, 0.20639020415193784, 0.0, 1.0, 0.3, 0.6864946908660688], 
reward next is 0.3135, 
noisyNet noise sample is [array([1.387663], dtype=float32), -0.25614917]. 
=============================================
[2019-04-09 14:54:42,540] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-12.8, 51.0, 58.0, 834.5, 22.5, 23.73377992564398, -0.1038904807380804, 1.0, 1.0, 20.0, 31.94768544624884], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 388800.0000, 
sim time next is 390000.0000, 
raw observation next is [-12.43333333333333, 51.00000000000001, 58.0, 881.5, 22.5, 23.92669892576701, -0.1460908685864305, 1.0, 1.0, 55.0, 67.34514904130647], 
processed observation next is [1.0, 0.5217391304347826, 0.11819021237303795, 0.5100000000000001, 0.19333333333333333, 0.9740331491712707, 0.375, 0.4938915771472508, 0.4513030438045232, 1.0, 1.0, 0.8, 0.6734514904130647], 
reward next is 0.3265, 
noisyNet noise sample is [array([0.8831977], dtype=float32), -0.55635774]. 
=============================================
[2019-04-09 14:54:42,544] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[0.07899088]
 [0.04654192]
 [0.15485337]
 [0.14760461]
 [0.11360624]], R is [[0.53064948]
 [1.2058661 ]
 [1.81479692]
 [2.25749826]
 [2.92404127]].
[2019-04-09 14:54:42,843] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.1087671  0.1311895  0.08043049 0.08291906 0.06471317 0.09771737
 0.09451108 0.09296019 0.09012966 0.1043993  0.05226313], sum to 1.0000
[2019-04-09 14:54:42,843] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9190
[2019-04-09 14:54:42,907] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-10.6, 49.0, 0.0, 0.0, 19.0, 22.51623653444448, -0.2847382071375641, 0.0, 1.0, 65.0, 69.36123008336526], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 424800.0000, 
sim time next is 426000.0000, 
raw observation next is [-10.96666666666667, 50.66666666666667, 0.0, 0.0, 19.0, 22.69470317492166, -0.2688567403880684, 0.0, 1.0, 60.0, 57.33441670909441], 
processed observation next is [1.0, 0.9565217391304348, 0.15881809787626952, 0.5066666666666667, 0.0, 0.0, 0.08333333333333333, 0.391225264576805, 0.41038108653731054, 0.0, 1.0, 0.9, 0.5733441670909442], 
reward next is 0.4267, 
noisyNet noise sample is [array([-2.262258], dtype=float32), 0.29705086]. 
=============================================
[2019-04-09 14:54:42,917] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[-0.03246747]
 [-0.02187509]
 [ 0.15653247]
 [ 0.0445356 ]
 [ 0.01382442]], R is [[0.4720296 ]
 [0.77369702]
 [1.32233596]
 [1.66767108]
 [2.25754404]].
[2019-04-09 14:54:43,019] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.08976048 0.13296793 0.08969185 0.07083836 0.07477725 0.10360751
 0.11553776 0.08442996 0.08761023 0.10273914 0.04803948], sum to 1.0000
[2019-04-09 14:54:43,019] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7671
[2019-04-09 14:54:43,033] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.07534013 0.1193279  0.0973189  0.08377307 0.07960759 0.12326788
 0.10096878 0.06983858 0.0846932  0.10177707 0.06408686], sum to 1.0000
[2019-04-09 14:54:43,033] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3135
[2019-04-09 14:54:43,089] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 19.0, 21.94867120714793, -0.4148977890394354, 0.0, 1.0, 65.0, 66.72761346722098], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 430800.0000, 
sim time next is 432000.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 19.0, 22.19649690728467, -0.4038156394647159, 0.0, 1.0, 55.0, 50.85793332799912], 
processed observation next is [1.0, 0.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.08333333333333333, 0.3497080756070557, 0.3653947868450947, 0.0, 1.0, 0.8, 0.5085793332799912], 
reward next is 0.4914, 
noisyNet noise sample is [array([-0.75294894], dtype=float32), 0.5949143]. 
=============================================
[2019-04-09 14:54:43,098] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.666666666666668, 40.66666666666667, 0.0, 0.0, 22.5, 23.26049760368579, -0.1606657131996118, 1.0, 1.0, 60.0, 57.94071891391927], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 415200.0000, 
sim time next is 416400.0000, 
raw observation next is [-9.833333333333332, 41.33333333333334, 0.0, 0.0, 22.5, 23.17166040598351, -0.1869805650501373, 1.0, 1.0, 30.0, 44.09582463537272], 
processed observation next is [1.0, 0.8260869565217391, 0.19021237303785785, 0.41333333333333344, 0.0, 0.0, 0.375, 0.43097170049862576, 0.4376731449832876, 1.0, 1.0, 0.3, 0.4409582463537272], 
reward next is 0.5590, 
noisyNet noise sample is [array([-1.6663045], dtype=float32), 0.41013607]. 
=============================================
[2019-04-09 14:54:43,113] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[ 0.23688589]
 [ 0.17278108]
 [ 0.13322687]
 [ 0.11048397]
 [-0.15209164]], R is [[0.61014497]
 [0.9367674 ]
 [1.2314477 ]
 [1.55897009]
 [1.92148352]].
[2019-04-09 14:54:43,518] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.10239622 0.11865675 0.07518835 0.06984834 0.08031911 0.12430853
 0.09618723 0.10514244 0.06882003 0.09652872 0.06260425], sum to 1.0000
[2019-04-09 14:54:43,519] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6159
[2019-04-09 14:54:43,543] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.07625852 0.10032025 0.08048113 0.07889585 0.08115544 0.15215363
 0.10497054 0.07736037 0.07146503 0.09862032 0.07831886], sum to 1.0000
[2019-04-09 14:54:43,543] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3148
[2019-04-09 14:54:43,581] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.2, 52.0, 0.0, 0.0, 19.0, 20.78690215251249, -0.7909500443121745, 0.0, 1.0, 25.0, 35.88334118355016], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 446400.0000, 
sim time next is 447600.0000, 
raw observation next is [-11.0, 52.0, 0.0, 0.0, 19.0, 20.51226138583791, -0.8379053365320157, 0.0, 1.0, 45.0, 38.73637960078], 
processed observation next is [1.0, 0.17391304347826086, 0.15789473684210528, 0.52, 0.0, 0.0, 0.08333333333333333, 0.20935511548649247, 0.22069822115599477, 0.0, 1.0, 0.6, 0.38736379600779997], 
reward next is 0.6126, 
noisyNet noise sample is [array([-1.0842885], dtype=float32), -0.7892449]. 
=============================================
[2019-04-09 14:54:43,639] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.833333333333334, 42.0, 46.16666666666666, 811.1666666666666, 22.5, 25.73901031643586, 0.2246872665907868, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 398400.0000, 
sim time next is 399600.0000, 
raw observation next is [-9.5, 40.0, 42.5, 769.5, 22.5, 25.23906283630619, 0.3371184556622156, 1.0, 1.0, 30.0, 63.503737352607324], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.4, 0.14166666666666666, 0.8502762430939227, 0.375, 0.6032552363588491, 0.6123728185540719, 1.0, 1.0, 0.3, 0.6350373735260733], 
reward next is 0.3650, 
noisyNet noise sample is [array([-1.5393234], dtype=float32), -1.0323677]. 
=============================================
[2019-04-09 14:54:43,794] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.07069796 0.1118369  0.07752198 0.07967711 0.07820136 0.15934777
 0.09244478 0.08113731 0.0585857  0.10787062 0.08267844], sum to 1.0000
[2019-04-09 14:54:43,794] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0198
[2019-04-09 14:54:43,813] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.09969482 0.12212823 0.07499176 0.08261951 0.07688379 0.11302089
 0.09305535 0.10411786 0.0640209  0.09983689 0.06962994], sum to 1.0000
[2019-04-09 14:54:43,814] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0308
[2019-04-09 14:54:43,821] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.09701549 0.11604697 0.07691709 0.06869017 0.08840164 0.11295538
 0.09411696 0.10210493 0.07776669 0.10230511 0.06367949], sum to 1.0000
[2019-04-09 14:54:43,822] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2778
[2019-04-09 14:54:43,853] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-11.2, 52.0, 0.0, 0.0, 19.0, 21.11641564817207, -0.6743114571925237, 0.0, 1.0, 30.0, 31.68678065039746], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 446400.0000, 
sim time next is 447600.0000, 
raw observation next is [-11.0, 52.0, 0.0, 0.0, 19.0, 20.63662280633256, -0.8423030206809979, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.15789473684210528, 0.52, 0.0, 0.0, 0.08333333333333333, 0.21971856719438007, 0.21923232643966736, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11070561], dtype=float32), -1.3083382]. 
=============================================
[2019-04-09 14:54:43,879] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-9.3, 39.33333333333334, 38.83333333333334, 727.8333333333334, 22.5, 26.05392016259097, 0.4523844893020972, 1.0, 1.0, 60.0, 56.76124786365347], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 400800.0000, 
sim time next is 402000.0000, 
raw observation next is [-9.100000000000001, 38.66666666666667, 34.33333333333334, 656.3333333333334, 22.5, 26.16439597015797, 0.2032950709766052, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.21052631578947364, 0.3866666666666667, 0.11444444444444447, 0.7252302025782689, 0.375, 0.6803663308464974, 0.5677650236588684, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5393234], dtype=float32), -1.0323677]. 
=============================================
[2019-04-09 14:54:43,886] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-9.5, 44.0, 0.0, 0.0, 19.0, 19.49477716112732, -1.021232925490512, 0.0, 1.0, 40.0, 29.309084765009153], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 453600.0000, 
sim time next is 454800.0000, 
raw observation next is [-9.133333333333333, 43.66666666666667, 0.0, 0.0, 19.0, 19.40027920580133, -1.056162494040531, 0.0, 1.0, 25.0, 26.307411751525684], 
processed observation next is [1.0, 0.2608695652173913, 0.20960295475530935, 0.4366666666666667, 0.0, 0.0, 0.08333333333333333, 0.11668993381677743, 0.147945835319823, 0.0, 1.0, 0.2, 0.26307411751525683], 
reward next is 0.7369, 
noisyNet noise sample is [array([-0.7028125], dtype=float32), -0.65117687]. 
=============================================
[2019-04-09 14:54:43,895] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[0.2487052 ]
 [0.13262331]
 [0.21711914]
 [0.19743107]
 [0.18972798]], R is [[1.10267019]
 [1.52403092]
 [1.87375331]
 [2.85501575]
 [3.19910407]].
[2019-04-09 14:54:44,114] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.09267564 0.11667284 0.07918952 0.07172641 0.09652455 0.12137246
 0.08639754 0.0984235  0.07406767 0.1004829  0.06246706], sum to 1.0000
[2019-04-09 14:54:44,115] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4901
[2019-04-09 14:54:44,167] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.8, 50.0, 0.0, 0.0, 19.0, 20.50915732717878, -0.7699616586642789, 0.0, 1.0, 50.0, 46.248846981205574], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 444000.0000, 
sim time next is 445200.0000, 
raw observation next is [-11.0, 51.0, 0.0, 0.0, 19.0, 20.47384999130114, -0.7898172695018735, 0.0, 1.0, 40.0, 35.14205427599023], 
processed observation next is [1.0, 0.13043478260869565, 0.15789473684210528, 0.51, 0.0, 0.0, 0.08333333333333333, 0.2061541659417617, 0.23672757683270884, 0.0, 1.0, 0.5, 0.3514205427599023], 
reward next is 0.6486, 
noisyNet noise sample is [array([0.94114405], dtype=float32), 0.10265419]. 
=============================================
[2019-04-09 14:54:44,205] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.09859502 0.10605841 0.08230638 0.07672264 0.08518014 0.11814006
 0.0917387  0.10130716 0.07856181 0.09990532 0.06148432], sum to 1.0000
[2019-04-09 14:54:44,206] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9206
[2019-04-09 14:54:44,293] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.09012085 0.11505912 0.07844492 0.07602111 0.08376357 0.12598996
 0.09948145 0.0871643  0.07342255 0.10733804 0.06319413], sum to 1.0000
[2019-04-09 14:54:44,293] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8375
[2019-04-09 14:54:44,309] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-11.2, 52.0, 0.0, 0.0, 19.0, 20.2051280485613, -0.9589416289053573, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 446400.0000, 
sim time next is 447600.0000, 
raw observation next is [-11.0, 52.0, 0.0, 0.0, 19.0, 19.72335876070074, -0.8420438375041847, 0.0, 1.0, 60.0, 80.09415417070062], 
processed observation next is [1.0, 0.17391304347826086, 0.15789473684210528, 0.52, 0.0, 0.0, 0.08333333333333333, 0.14361323005839507, 0.21931872083193846, 0.0, 1.0, 0.9, 0.8009415417070062], 
reward next is 0.1991, 
noisyNet noise sample is [array([0.94114405], dtype=float32), 0.10265419]. 
=============================================
[2019-04-09 14:54:44,501] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-8.2, 42.0, 0.0, 0.0, 22.5, 21.07968303978105, -0.7014576697218856, 1.0, 1.0, 30.0, 44.01597023620882], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 458400.0000, 
sim time next is 459600.0000, 
raw observation next is [-8.0, 41.0, 0.0, 0.0, 22.5, 21.21600134895638, -0.5765602928212742, 1.0, 1.0, 60.0, 78.20621970852918], 
processed observation next is [1.0, 0.30434782608695654, 0.24099722991689754, 0.41, 0.0, 0.0, 0.375, 0.26800011241303157, 0.3078132357262419, 1.0, 1.0, 0.9, 0.7820621970852918], 
reward next is 0.3637, 
noisyNet noise sample is [array([0.09124856], dtype=float32), 0.6835554]. 
=============================================
[2019-04-09 14:54:45,245] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.09221022 0.1173793  0.07126041 0.07677845 0.08172187 0.12791525
 0.09602635 0.10142645 0.06477666 0.10466571 0.06583938], sum to 1.0000
[2019-04-09 14:54:45,245] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3852
[2019-04-09 14:54:45,451] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.0, 41.0, 0.0, 0.0, 22.5, 21.82933526887602, -0.6504907306647852, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 459600.0000, 
sim time next is 460800.0000, 
raw observation next is [-7.8, 40.0, 11.5, 0.0, 22.5, 21.98835959207916, -0.4719169380045725, 1.0, 1.0, 35.0, 79.51094275506719], 
processed observation next is [1.0, 0.34782608695652173, 0.24653739612188366, 0.4, 0.03833333333333333, 0.0, 0.375, 0.33236329933993, 0.34269435399847586, 1.0, 1.0, 0.4, 0.7951094275506719], 
reward next is 0.2049, 
noisyNet noise sample is [array([-0.7320723], dtype=float32), 0.16345453]. 
=============================================
[2019-04-09 14:54:45,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.09532578 0.11579798 0.07624885 0.08781587 0.08696617 0.11090381
 0.09451886 0.09473051 0.06752376 0.10010483 0.07006349], sum to 1.0000
[2019-04-09 14:54:45,553] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7015
[2019-04-09 14:54:45,590] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.09835894 0.11099412 0.07983083 0.08685499 0.06733447 0.11741849
 0.09285225 0.08864805 0.07988126 0.10195158 0.07587498], sum to 1.0000
[2019-04-09 14:54:45,590] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6005
[2019-04-09 14:54:45,622] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-9.5, 44.0, 0.0, 0.0, 19.0, 20.95250827448054, -0.592955925911896, 0.0, 1.0, 60.0, 61.617118747167815], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 453600.0000, 
sim time next is 454800.0000, 
raw observation next is [-9.133333333333333, 43.66666666666667, 0.0, 0.0, 19.0, 21.56043378524226, -0.5594286484678068, 0.0, 1.0, 50.0, 45.83046323112332], 
processed observation next is [1.0, 0.2608695652173913, 0.20960295475530935, 0.4366666666666667, 0.0, 0.0, 0.08333333333333333, 0.296702815436855, 0.3135237838440644, 0.0, 1.0, 0.7, 0.4583046323112332], 
reward next is 0.5417, 
noisyNet noise sample is [array([1.7990735], dtype=float32), -1.5132263]. 
=============================================
[2019-04-09 14:54:45,627] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.1, 27.0, 118.0, 0.0, 22.5, 24.13807485266354, -0.1349908789217526, 1.0, 1.0, 35.0, 38.591366267802584], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 472800.0000, 
sim time next is 474000.0000, 
raw observation next is [-1.9, 26.0, 123.1666666666667, 0.0, 22.5, 24.06915529496287, -0.1357574871976053, 1.0, 1.0, 45.0, 34.48862153095482], 
processed observation next is [1.0, 0.4782608695652174, 0.4099722991689751, 0.26, 0.4105555555555557, 0.0, 0.375, 0.5057629412469057, 0.4547475042674649, 1.0, 1.0, 0.6, 0.3448862153095482], 
reward next is 0.6551, 
noisyNet noise sample is [array([0.501861], dtype=float32), 0.7054907]. 
=============================================
[2019-04-09 14:54:45,635] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[0.03258217]
 [0.09782426]
 [0.11688128]
 [0.12564299]
 [0.17071846]], R is [[0.84747255]
 [1.45308423]
 [1.9745872 ]
 [2.28294897]
 [2.88164043]].
[2019-04-09 14:54:47,326] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.09503006 0.11867778 0.09115367 0.07065252 0.09151608 0.0976593
 0.11512019 0.09601508 0.0832995  0.08894841 0.0519274 ], sum to 1.0000
[2019-04-09 14:54:47,326] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0989
[2019-04-09 14:54:47,352] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.466666666666667, 96.33333333333333, 0.0, 0.0, 19.0, 21.32155571059378, -0.5286008093428329, 0.0, 1.0, 25.0, 26.00095255764154], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 516000.0000, 
sim time next is 517200.0000, 
raw observation next is [3.633333333333333, 96.66666666666666, 0.0, 0.0, 19.0, 21.36110713963105, -0.532419738619203, 0.0, 1.0, 20.0, 22.821496697252087], 
processed observation next is [1.0, 1.0, 0.5632502308402586, 0.9666666666666666, 0.0, 0.0, 0.08333333333333333, 0.2800922616359207, 0.322526753793599, 0.0, 1.0, 0.1, 0.22821496697252086], 
reward next is 0.7718, 
noisyNet noise sample is [array([-0.23037952], dtype=float32), 0.37552503]. 
=============================================
[2019-04-09 14:54:47,561] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.08939836 0.12603468 0.0874076  0.07535141 0.09628095 0.10094003
 0.09862185 0.07276616 0.10037947 0.08069985 0.07211968], sum to 1.0000
[2019-04-09 14:54:47,565] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1107
[2019-04-09 14:54:47,631] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.7000000000000001, 88.0, 0.0, 0.0, 22.5, 23.83500104235424, -0.03987307996261449, 1.0, 1.0, 30.0, 42.81094344762608], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 498000.0000, 
sim time next is 499200.0000, 
raw observation next is [0.9000000000000001, 92.0, 0.0, 0.0, 22.5, 23.80658311914979, -0.05422269063879043, 1.0, 1.0, 20.0, 36.25274939323124], 
processed observation next is [1.0, 0.782608695652174, 0.48753462603878117, 0.92, 0.0, 0.0, 0.375, 0.4838819265958157, 0.48192576978706986, 1.0, 1.0, 0.1, 0.36252749393231243], 
reward next is 0.6375, 
noisyNet noise sample is [array([-0.1571287], dtype=float32), 0.69632125]. 
=============================================
[2019-04-09 14:54:47,694] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.08194444 0.1259328  0.08929532 0.08240157 0.08685881 0.10316004
 0.11516871 0.07744507 0.09621657 0.07760233 0.06397431], sum to 1.0000
[2019-04-09 14:54:47,709] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6876
[2019-04-09 14:54:47,756] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.9000000000000001, 56.66666666666667, 0.0, 0.0, 22.5, 25.21868912988474, -0.01034137945350368, 1.0, 1.0, 20.0, 26.71242390250222], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 494400.0000, 
sim time next is 495600.0000, 
raw observation next is [0.7000000000000001, 70.33333333333334, 0.0, 0.0, 22.5, 24.19206041898016, 0.0551226191602483, 1.0, 1.0, 50.0, 44.157936011803244], 
processed observation next is [1.0, 0.7391304347826086, 0.4819944598337951, 0.7033333333333335, 0.0, 0.0, 0.375, 0.5160050349150133, 0.5183742063867495, 1.0, 1.0, 0.7, 0.44157936011803245], 
reward next is 0.5584, 
noisyNet noise sample is [array([0.08254305], dtype=float32), -0.98014885]. 
=============================================
[2019-04-09 14:54:48,394] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.11011592 0.11855002 0.08684032 0.07248402 0.09725528 0.1045576
 0.08949267 0.10788783 0.06467015 0.07700235 0.07114381], sum to 1.0000
[2019-04-09 14:54:48,397] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8282
[2019-04-09 14:54:48,431] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.1066521  0.10842232 0.08607203 0.06738292 0.11066256 0.11136246
 0.08728092 0.09341887 0.07063735 0.09478586 0.06332262], sum to 1.0000
[2019-04-09 14:54:48,431] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3765
[2019-04-09 14:54:48,449] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [2.7, 82.0, 0.0, 0.0, 19.0, 22.35295913555174, -0.4085714286720281, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 532800.0000, 
sim time next is 534000.0000, 
raw observation next is [2.333333333333333, 83.0, 0.0, 0.0, 19.0, 21.97335827397, -0.2753247108659325, 0.0, 1.0, 55.0, 70.07116866735007], 
processed observation next is [0.0, 0.17391304347826086, 0.5272391505078486, 0.83, 0.0, 0.0, 0.08333333333333333, 0.33111318949749996, 0.4082250963780225, 0.0, 1.0, 0.8, 0.7007116866735007], 
reward next is 0.2993, 
noisyNet noise sample is [array([1.0029672], dtype=float32), -0.4636318]. 
=============================================
[2019-04-09 14:54:48,466] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[0.04640551]
 [0.15925391]
 [0.18649364]
 [0.16489752]
 [0.36269495]], R is [[0.58422077]
 [1.57837856]
 [2.56259489]
 [3.1859901 ]
 [3.75402689]].
[2019-04-09 14:54:48,507] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 85.66666666666667, 88.16666666666666, 134.6666666666667, 19.0, 23.13079984027941, -0.08412504326619091, 0.0, 1.0, 60.0, 57.929094999727745], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 555600.0000, 
sim time next is 556800.0000, 
raw observation next is [-0.6, 84.33333333333333, 79.0, 140.0, 19.0, 23.31571210297385, -0.05914817856366628, 0.0, 1.0, 35.0, 40.3630496971997], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.8433333333333333, 0.2633333333333333, 0.15469613259668508, 0.08333333333333333, 0.44297600858115427, 0.4802839404787779, 0.0, 1.0, 0.4, 0.403630496971997], 
reward next is 0.5964, 
noisyNet noise sample is [array([-1.1140833], dtype=float32), -1.1356758]. 
=============================================
[2019-04-09 14:54:49,075] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.10702734 0.11924893 0.08434493 0.06508837 0.11146066 0.09896222
 0.09711179 0.09850934 0.0744227  0.08132092 0.06250285], sum to 1.0000
[2019-04-09 14:54:49,075] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7745
[2019-04-09 14:54:49,108] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.2, 89.66666666666667, 125.6666666666667, 103.1666666666667, 19.0, 22.76212841444771, -0.1314294456494543, 0.0, 1.0, 60.0, 55.17705402617828], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 552000.0000, 
sim time next is 553200.0000, 
raw observation next is [-0.4, 88.33333333333334, 132.8333333333333, 109.3333333333333, 19.0, 23.04219328272978, -0.1023064481285989, 0.0, 1.0, 50.0, 41.32846816981362], 
processed observation next is [0.0, 0.391304347826087, 0.45152354570637127, 0.8833333333333334, 0.4427777777777776, 0.12081031307550641, 0.08333333333333333, 0.420182773560815, 0.46589785062380035, 0.0, 1.0, 0.7, 0.4132846816981362], 
reward next is 0.5867, 
noisyNet noise sample is [array([-0.1988625], dtype=float32), 0.16432564]. 
=============================================
[2019-04-09 14:54:49,291] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.10679851 0.1029724  0.08800564 0.06814408 0.11361519 0.09937248
 0.08977812 0.09600451 0.07148147 0.09217899 0.07164864], sum to 1.0000
[2019-04-09 14:54:49,291] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6620
[2019-04-09 14:54:49,318] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 84.33333333333333, 79.0, 140.0, 19.0, 23.02705904343506, -0.136278129184541, 0.0, 1.0, 35.0, 32.9215289910456], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 556800.0000, 
sim time next is 558000.0000, 
raw observation next is [-0.6, 83.0, 83.0, 138.0, 19.0, 22.99888123332304, -0.1545635341623736, 0.0, 1.0, 45.0, 29.704540319380595], 
processed observation next is [0.0, 0.4782608695652174, 0.44598337950138506, 0.83, 0.27666666666666667, 0.15248618784530388, 0.08333333333333333, 0.4165734361102533, 0.44847882194587546, 0.0, 1.0, 0.6, 0.29704540319380596], 
reward next is 0.7030, 
noisyNet noise sample is [array([1.1332355], dtype=float32), -1.1717902]. 
=============================================
[2019-04-09 14:54:49,330] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[0.17196515]
 [0.0935507 ]
 [0.16918755]
 [0.10621294]
 [0.06154625]], R is [[0.97123545]
 [1.63230777]
 [2.22684097]
 [2.57333279]
 [3.27460074]].
[2019-04-09 14:54:49,550] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.12017336 0.11239766 0.09067363 0.08377635 0.09648974 0.09283487
 0.08032513 0.10476027 0.06393854 0.08578936 0.06884112], sum to 1.0000
[2019-04-09 14:54:49,550] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5137
[2019-04-09 14:54:49,595] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666667, 82.33333333333334, 87.0, 136.0, 19.0, 22.7469049333314, -0.2073383634493836, 0.0, 1.0, 30.0, 33.56636085768247], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 559200.0000, 
sim time next is 560400.0000, 
raw observation next is [-0.7333333333333334, 81.66666666666667, 95.83333333333333, 178.5, 19.0, 22.71610975628793, -0.2269031665414534, 0.0, 1.0, 40.0, 30.10040901645579], 
processed observation next is [0.0, 0.4782608695652174, 0.44228993536472766, 0.8166666666666668, 0.3194444444444444, 0.19723756906077347, 0.08333333333333333, 0.3930091463573276, 0.4243656111528489, 0.0, 1.0, 0.5, 0.30100409016455787], 
reward next is 0.6990, 
noisyNet noise sample is [array([0.71959686], dtype=float32), 0.99945134]. 
=============================================
[2019-04-09 14:54:49,709] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.10144587 0.11898689 0.08223086 0.06457949 0.11089635 0.11066549
 0.09551211 0.09755421 0.07315499 0.08192878 0.063045  ], sum to 1.0000
[2019-04-09 14:54:49,710] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3126
[2019-04-09 14:54:49,751] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.4, 88.33333333333334, 132.8333333333333, 109.3333333333333, 19.0, 22.52814120562316, -0.265979777622347, 0.0, 1.0, 20.0, 32.05795491526926], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 553200.0000, 
sim time next is 554400.0000, 
raw observation next is [-0.6, 87.0, 110.5, 122.0, 19.0, 22.47925735354125, -0.2864543195616965, 0.0, 1.0, 30.0, 28.85310087139724], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.87, 0.36833333333333335, 0.13480662983425415, 0.08333333333333333, 0.3732714461284375, 0.40451522681276786, 0.0, 1.0, 0.3, 0.28853100871397236], 
reward next is 0.7115, 
noisyNet noise sample is [array([-0.96112245], dtype=float32), 1.3528371]. 
=============================================
[2019-04-09 14:54:49,814] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 7410: loss 24.0281
[2019-04-09 14:54:49,894] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 7410: learning rate 0.0000
[2019-04-09 14:54:49,909] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.11645381 0.11125489 0.08397697 0.07478953 0.10442472 0.10876928
 0.0959916  0.09496724 0.06951802 0.07814927 0.06170459], sum to 1.0000
[2019-04-09 14:54:49,909] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9755
[2019-04-09 14:54:49,943] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 23.5310880398521, -0.09994885538045262, 0.0, 1.0, 40.0, 30.910175095099156], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 544800.0000, 
sim time next is 546000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 23.31153790126115, -0.08530142630506192, 0.0, 1.0, 65.0, 66.37650506278825], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.44262815843842923, 0.471566191231646, 0.0, 1.0, 1.0, 0.6637650506278825], 
reward next is 0.3362, 
noisyNet noise sample is [array([-0.89747196], dtype=float32), -0.6040708]. 
=============================================
[2019-04-09 14:54:49,960] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[0.17293626]
 [0.28455713]
 [0.11006683]
 [0.19058497]
 [0.21331622]], R is [[0.61648321]
 [1.3012166 ]
 [1.94687247]
 [2.54654431]
 [3.04260159]].
[2019-04-09 14:54:50,258] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.08878996 0.09925176 0.08535926 0.0762051  0.09928282 0.10966428
 0.11768276 0.10120331 0.08189245 0.07840137 0.06226694], sum to 1.0000
[2019-04-09 14:54:50,258] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3794
[2019-04-09 14:54:50,298] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 83.0, 104.5, 138.6666666666667, 19.0, 22.94708625943926, -0.2422956894096956, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 573600.0000, 
sim time next is 574800.0000, 
raw observation next is [-1.2, 83.0, 90.16666666666667, 68.33333333333333, 19.0, 22.62982987337761, -0.1859880026107141, 0.0, 1.0, 35.0, 47.77543045661885], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.3005555555555556, 0.07550644567219153, 0.08333333333333333, 0.3858191561148008, 0.43800399912976196, 0.0, 1.0, 0.4, 0.4777543045661885], 
reward next is 0.5222, 
noisyNet noise sample is [array([2.598833], dtype=float32), 1.3957691]. 
=============================================
[2019-04-09 14:54:50,496] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7603: loss 25.1394
[2019-04-09 14:54:50,496] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7603: learning rate 0.0000
[2019-04-09 14:54:50,508] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 7606: loss 20.6969
[2019-04-09 14:54:50,511] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 7606: learning rate 0.0000
[2019-04-09 14:54:50,859] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7711: loss 23.5510
[2019-04-09 14:54:50,859] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7711: learning rate 0.0000
[2019-04-09 14:54:50,992] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.07745264 0.13202411 0.09972785 0.0742451  0.0974577  0.10280214
 0.10520366 0.07696618 0.09860142 0.07371868 0.06180048], sum to 1.0000
[2019-04-09 14:54:50,995] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6636
[2019-04-09 14:54:51,004] A3C_AGENT_WORKER-Thread-8 INFO:Local step 500, global step 7766: loss 21.7276
[2019-04-09 14:54:51,007] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 500, global step 7766: learning rate 0.0000
[2019-04-09 14:54:51,028] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7772: loss 33.3939
[2019-04-09 14:54:51,028] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7772: learning rate 0.0000
[2019-04-09 14:54:51,053] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 22.5, 24.82773981517425, 0.2422211866029443, 0.0, 1.0, 60.0, 60.32981535280757], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 500400.0000, 
sim time next is 501600.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 22.5, 24.93422984251976, 0.2575723808581048, 0.0, 1.0, 45.0, 39.05313033764905], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.375, 0.5778524868766466, 0.5858574602860349, 0.0, 1.0, 0.6, 0.3905313033764905], 
reward next is 0.6095, 
noisyNet noise sample is [array([-0.52727914], dtype=float32), 1.5417819]. 
=============================================
[2019-04-09 14:54:51,288] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7856: loss 24.9242
[2019-04-09 14:54:51,288] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7856: learning rate 0.0000
[2019-04-09 14:54:51,421] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7904: loss 24.7470
[2019-04-09 14:54:51,433] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7904: learning rate 0.0000
[2019-04-09 14:54:51,530] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 7942: loss 30.4062
[2019-04-09 14:54:51,533] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 7946: learning rate 0.0000
[2019-04-09 14:54:51,738] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8018: loss 22.4767
[2019-04-09 14:54:51,749] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8019: learning rate 0.0000
[2019-04-09 14:54:51,776] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 8030: loss 23.8880
[2019-04-09 14:54:51,779] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 8030: learning rate 0.0000
[2019-04-09 14:54:51,780] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 8030: loss 27.7570
[2019-04-09 14:54:51,781] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 8030: learning rate 0.0000
[2019-04-09 14:54:51,961] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.10336687 0.1089421  0.08990904 0.07340349 0.1047083  0.10832282
 0.09519416 0.09995954 0.06885259 0.08318958 0.0641515 ], sum to 1.0000
[2019-04-09 14:54:51,961] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6309
[2019-04-09 14:54:51,985] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 8107: loss 24.9322
[2019-04-09 14:54:51,985] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 8107: learning rate 0.0000
[2019-04-09 14:54:52,005] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 75.0, 0.0, 0.0, 19.0, 22.22673325648841, -0.3546563258983873, 0.0, 1.0, 20.0, 37.61967234372004], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 615600.0000, 
sim time next is 616800.0000, 
raw observation next is [-4.1, 75.0, 0.0, 0.0, 19.0, 22.27322371063854, -0.3694327606685312, 0.0, 1.0, 45.0, 33.33528751177288], 
processed observation next is [0.0, 0.13043478260869565, 0.3490304709141275, 0.75, 0.0, 0.0, 0.08333333333333333, 0.35610197588654496, 0.3768557464438229, 0.0, 1.0, 0.6, 0.3333528751177288], 
reward next is 0.6666, 
noisyNet noise sample is [array([-0.9258218], dtype=float32), 0.1770477]. 
=============================================
[2019-04-09 14:54:52,042] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8124: loss 31.8235
[2019-04-09 14:54:52,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8125: learning rate 0.0000
[2019-04-09 14:54:52,803] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.09640621 0.10715025 0.08214124 0.07424089 0.10937113 0.09391994
 0.11379974 0.08568604 0.08896601 0.09831858 0.05000003], sum to 1.0000
[2019-04-09 14:54:52,808] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6734
[2019-04-09 14:54:52,853] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.466666666666667, 63.0, 0.0, 0.0, 19.0, 21.41728655040553, -0.5423898438616849, 0.0, 1.0, 60.0, 64.29848951923961], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 674400.0000, 
sim time next is 675600.0000, 
raw observation next is [-2.633333333333333, 64.0, 0.0, 0.0, 19.0, 21.57767506323767, -0.5206046986833103, 0.0, 1.0, 45.0, 41.77254947125027], 
processed observation next is [0.0, 0.8260869565217391, 0.38965835641735924, 0.64, 0.0, 0.0, 0.08333333333333333, 0.2981395886031392, 0.3264651004388966, 0.0, 1.0, 0.6, 0.4177254947125027], 
reward next is 0.5823, 
noisyNet noise sample is [array([0.5542414], dtype=float32), 0.045540407]. 
=============================================
[2019-04-09 14:54:53,087] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.09848324 0.09594617 0.08478226 0.05540812 0.12027086 0.10810498
 0.10652401 0.07748709 0.07929578 0.10837886 0.06531862], sum to 1.0000
[2019-04-09 14:54:53,087] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6993
[2019-04-09 14:54:53,147] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-0.6, 87.0, 110.5, 122.0, 19.0, 24.31394997338259, 0.2214694624174945, 0.0, 1.0, 65.0, 65.84787377792158], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 554400.0000, 
sim time next is 555600.0000, 
raw observation next is [-0.6, 85.66666666666667, 88.16666666666666, 134.6666666666667, 19.0, 24.58767099779535, 0.2529638569212146, 0.0, 1.0, 50.0, 44.349216757302905], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.8566666666666667, 0.29388888888888887, 0.1488029465930019, 0.08333333333333333, 0.5489725831496125, 0.5843212856404049, 0.0, 1.0, 0.7, 0.44349216757302906], 
reward next is 0.5565, 
noisyNet noise sample is [array([0.84353954], dtype=float32), -1.7192367]. 
=============================================
[2019-04-09 14:54:53,249] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.10525451 0.11706641 0.08446798 0.07436839 0.10267594 0.10592838
 0.09498081 0.09094986 0.0718125  0.08307268 0.06942245], sum to 1.0000
[2019-04-09 14:54:53,250] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2554
[2019-04-09 14:54:53,292] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.5, 67.0, 0.0, 0.0, 19.0, 22.59101583981967, -0.3138413515158733, 0.0, 1.0, 35.0, 42.615265336373554], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 628800.0000, 
sim time next is 630000.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 19.0, 22.49758911436196, -0.3483745771038256, 0.0, 1.0, 25.0, 38.30502889817647], 
processed observation next is [0.0, 0.30434782608695654, 0.3379501385041552, 0.68, 0.0, 0.0, 0.08333333333333333, 0.3747990928634968, 0.3838751409653915, 0.0, 1.0, 0.2, 0.3830502889817647], 
reward next is 0.6169, 
noisyNet noise sample is [array([1.098681], dtype=float32), -0.4363258]. 
=============================================
[2019-04-09 14:54:53,298] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[0.16909136]
 [0.29231542]
 [0.3689282 ]
 [0.27379042]
 [0.3428052 ]], R is [[0.99037182]
 [1.55431545]
 [2.03331876]
 [2.11666346]
 [3.09549689]].
[2019-04-09 14:54:53,602] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.08934152 0.10328355 0.09059675 0.07468035 0.10065717 0.10151956
 0.111798   0.09452827 0.09216288 0.08554641 0.05588548], sum to 1.0000
[2019-04-09 14:54:53,608] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4640
[2019-04-09 14:54:53,639] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 57.0, 0.0, 0.0, 19.0, 20.71481196633169, -0.6781067813452925, 0.0, 1.0, 30.0, 37.16323214531107], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 669600.0000, 
sim time next is 670800.0000, 
raw observation next is [-1.566666666666667, 58.66666666666667, 0.0, 0.0, 19.0, 20.66036161977039, -0.6884362223488792, 0.0, 1.0, 40.0, 29.911896487040842], 
processed observation next is [0.0, 0.782608695652174, 0.4192059095106187, 0.5866666666666667, 0.0, 0.0, 0.08333333333333333, 0.2216968016475326, 0.27052125921704023, 0.0, 1.0, 0.5, 0.2991189648704084], 
reward next is 0.7009, 
noisyNet noise sample is [array([-0.42555678], dtype=float32), 0.26181313]. 
=============================================
[2019-04-09 14:54:53,723] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.09729577 0.12368009 0.08540609 0.08492775 0.08180159 0.10356845
 0.11084713 0.08825868 0.08450554 0.08099299 0.05871595], sum to 1.0000
[2019-04-09 14:54:53,723] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7216
[2019-04-09 14:54:53,765] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.733333333333333, 71.33333333333334, 0.0, 0.0, 19.0, 19.63535483943579, -0.9696252248715375, 0.0, 1.0, 25.0, 18.547317861323386], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 692400.0000, 
sim time next is 693600.0000, 
raw observation next is [-3.566666666666666, 71.66666666666667, 0.0, 0.0, 19.0, 19.49131041396585, -0.9231755142609067, 0.0, 1.0, 50.0, 49.43136223426319], 
processed observation next is [1.0, 0.0, 0.3638042474607572, 0.7166666666666667, 0.0, 0.0, 0.08333333333333333, 0.12427586783048739, 0.19227482857969777, 0.0, 1.0, 0.7, 0.4943136223426319], 
reward next is 0.5057, 
noisyNet noise sample is [array([-1.1856383], dtype=float32), 0.6333536]. 
=============================================
[2019-04-09 14:54:53,772] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.10342684 0.10102509 0.09169985 0.06126965 0.1142384  0.11019986
 0.08943023 0.08703992 0.07700104 0.09988175 0.06478736], sum to 1.0000
[2019-04-09 14:54:53,783] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2034
[2019-04-09 14:54:53,821] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.566666666666666, 65.0, 98.16666666666667, 6.333333333333332, 19.0, 20.93660266389148, -0.5330596766399621, 0.0, 1.0, 65.0, 75.88077684655082], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 643200.0000, 
sim time next is 644400.0000, 
raw observation next is [-3.4, 65.0, 94.5, 19.0, 19.0, 21.40905273160253, -0.4777923146468475, 0.0, 1.0, 20.0, 41.53359062931801], 
processed observation next is [0.0, 0.4782608695652174, 0.368421052631579, 0.65, 0.315, 0.020994475138121547, 0.08333333333333333, 0.28408772763354406, 0.3407358951177175, 0.0, 1.0, 0.1, 0.4153359062931801], 
reward next is 0.5847, 
noisyNet noise sample is [array([0.05690852], dtype=float32), 1.5774859]. 
=============================================
[2019-04-09 14:54:53,959] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.10035451 0.08545338 0.08562332 0.06998605 0.11110094 0.10126293
 0.10437118 0.09939796 0.07305463 0.09824041 0.07115473], sum to 1.0000
[2019-04-09 14:54:53,959] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5589
[2019-04-09 14:54:53,992] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 83.0, 70.5, 59.0, 19.0, 24.27274905253978, 0.1457182344778094, 0.0, 1.0, 20.0, 24.422142073966107], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 576000.0000, 
sim time next is 577200.0000, 
raw observation next is [-1.366666666666667, 84.33333333333334, 50.83333333333334, 49.66666666666666, 19.0, 24.10459049505754, 0.1176483437512246, 0.0, 1.0, 45.0, 30.960003409206053], 
processed observation next is [0.0, 0.6956521739130435, 0.42474607571560485, 0.8433333333333334, 0.16944444444444448, 0.05488029465930017, 0.08333333333333333, 0.5087158745881283, 0.5392161145837415, 0.0, 1.0, 0.6, 0.30960003409206055], 
reward next is 0.6904, 
noisyNet noise sample is [array([-0.80913436], dtype=float32), -2.561038]. 
=============================================
[2019-04-09 14:54:54,065] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.08466236 0.11274265 0.09814228 0.06159561 0.12126334 0.09231519
 0.1202537  0.07785578 0.0790586  0.09745128 0.05465927], sum to 1.0000
[2019-04-09 14:54:54,066] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7343
[2019-04-09 14:54:54,108] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.2, 67.66666666666667, 0.0, 0.0, 19.0, 21.8521435466261, -0.4547907711604686, 0.0, 1.0, 65.0, 68.14126253834564], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 679200.0000, 
sim time next is 680400.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 22.00654123239225, -0.4287222160901241, 0.0, 1.0, 50.0, 43.289209793850475], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.33387843603268763, 0.3570925946366253, 0.0, 1.0, 0.7, 0.43289209793850475], 
reward next is 0.5671, 
noisyNet noise sample is [array([-0.74709296], dtype=float32), -0.87286663]. 
=============================================
[2019-04-09 14:54:54,188] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.09116935 0.09954396 0.09534515 0.06519306 0.10609278 0.09843849
 0.12917572 0.08193283 0.08155665 0.09943461 0.05211742], sum to 1.0000
[2019-04-09 14:54:54,188] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2645
[2019-04-09 14:54:54,213] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 19.0, 21.82372458784145, -0.5157605206513408, 0.0, 1.0, 55.0, 43.365472292946436], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 686400.0000, 
sim time next is 687600.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 19.0, 21.7750270271484, -0.5405183225116436, 0.0, 1.0, 25.0, 38.400333818230486], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.08333333333333333, 0.31458558559570005, 0.31982722582945217, 0.0, 1.0, 0.2, 0.38400333818230487], 
reward next is 0.6160, 
noisyNet noise sample is [array([2.1796007], dtype=float32), 1.4291598]. 
=============================================
[2019-04-09 14:54:54,456] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.08756949 0.11650004 0.08638874 0.07206149 0.10329393 0.0999726
 0.10863075 0.09938436 0.08465488 0.08771848 0.05382518], sum to 1.0000
[2019-04-09 14:54:54,457] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1307
[2019-04-09 14:54:54,500] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.2, 57.0, 0.0, 0.0, 19.0, 20.78548505020891, -0.6527706064532507, 0.0, 1.0, 45.0, 37.07232768397455], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 669600.0000, 
sim time next is 670800.0000, 
raw observation next is [-1.566666666666667, 58.66666666666667, 0.0, 0.0, 19.0, 20.83180601696281, -0.6594778459907521, 0.0, 1.0, 20.0, 28.374013120312263], 
processed observation next is [0.0, 0.782608695652174, 0.4192059095106187, 0.5866666666666667, 0.0, 0.0, 0.08333333333333333, 0.23598383474690068, 0.280174051336416, 0.0, 1.0, 0.1, 0.28374013120312264], 
reward next is 0.7163, 
noisyNet noise sample is [array([-0.92326623], dtype=float32), 1.2937943]. 
=============================================
[2019-04-09 14:54:54,818] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 9072: loss 23.0273
[2019-04-09 14:54:54,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 9073: learning rate 0.0000
[2019-04-09 14:54:54,952] A3C_AGENT_WORKER-Thread-7 INFO:Local step 500, global step 9115: loss 24.0133
[2019-04-09 14:54:54,952] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 500, global step 9115: learning rate 0.0000
[2019-04-09 14:54:55,024] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.09069192 0.1029793  0.07807883 0.06288752 0.10178602 0.12331119
 0.09918207 0.10343688 0.07996631 0.09094529 0.06673459], sum to 1.0000
[2019-04-09 14:54:55,024] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4146
[2019-04-09 14:54:55,140] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 76.0, 14.5, 0.0, 22.5, 23.03022675943874, -0.3278496608407102, 1.0, 1.0, 30.0, 45.219430532538105], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 720000.0000, 
sim time next is 721200.0000, 
raw observation next is [-2.3, 76.0, 24.16666666666667, 0.0, 22.5, 22.96983902854961, -0.3322866740533774, 1.0, 1.0, 35.0, 37.833798024843986], 
processed observation next is [1.0, 0.34782608695652173, 0.3988919667590028, 0.76, 0.08055555555555557, 0.0, 0.375, 0.41415325237913425, 0.3892377753155409, 1.0, 1.0, 0.4, 0.37833798024843984], 
reward next is 0.6217, 
noisyNet noise sample is [array([-2.0202587], dtype=float32), -0.25005734]. 
=============================================
[2019-04-09 14:54:55,804] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.08474269 0.09554081 0.08531135 0.05327808 0.1161871  0.11552087
 0.10813703 0.09653477 0.07883082 0.09774711 0.06816942], sum to 1.0000
[2019-04-09 14:54:55,804] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0159
[2019-04-09 14:54:55,835] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-4.5, 72.66666666666667, 0.0, 0.0, 19.0, 23.02702692572075, -0.188104763699977, 0.0, 1.0, 45.0, 34.171976770591584], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 620400.0000, 
sim time next is 621600.0000, 
raw observation next is [-4.5, 70.33333333333333, 0.0, 0.0, 19.0, 22.69572796653937, -0.3568600125874184, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.7033333333333333, 0.0, 0.0, 0.08333333333333333, 0.3913106638782808, 0.38104666247086055, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8859786], dtype=float32), 1.2132688]. 
=============================================
[2019-04-09 14:54:56,494] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.07760375 0.0995068  0.07844265 0.06138797 0.11167733 0.1436312
 0.10098392 0.07892438 0.07765897 0.10096548 0.06921745], sum to 1.0000
[2019-04-09 14:54:56,494] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2363
[2019-04-09 14:54:56,553] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.3333333333333334, 45.66666666666667, 81.5, 723.8333333333333, 22.5, 24.6653983265802, 0.2641148229815889, 1.0, 1.0, 50.0, 50.44000916898629], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 742800.0000, 
sim time next is 744000.0000, 
raw observation next is [0.1666666666666667, 46.33333333333334, 80.83333333333334, 600.1666666666666, 22.5, 25.15570233061599, 0.3044896214599193, 1.0, 1.0, 25.0, 30.100458108753642], 
processed observation next is [1.0, 0.6086956521739131, 0.4672206832871654, 0.46333333333333343, 0.2694444444444445, 0.6631675874769797, 0.375, 0.5963085275513326, 0.6014965404866398, 1.0, 1.0, 0.2, 0.3010045810875364], 
reward next is 0.6990, 
noisyNet noise sample is [array([2.0602205], dtype=float32), 0.27300033]. 
=============================================
[2019-04-09 14:54:56,563] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[0.5482162 ]
 [0.54642254]
 [0.5971177 ]
 [0.5810668 ]
 [0.6170228 ]], R is [[1.33504009]
 [1.81728959]
 [2.79911661]
 [3.27092743]
 [4.23821831]].
[2019-04-09 14:54:56,761] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.07352275 0.10489173 0.09393941 0.06927399 0.09505717 0.1250369
 0.1061077  0.07493393 0.08813239 0.09191435 0.07718965], sum to 1.0000
[2019-04-09 14:54:56,761] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8486
[2019-04-09 14:54:56,817] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.9, 55.0, 0.0, 0.0, 22.5, 24.79604418353231, 0.1523099662417995, 1.0, 1.0, 35.0, 33.8344729692074], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 757200.0000, 
sim time next is 758400.0000, 
raw observation next is [-3.899999999999999, 54.0, 0.0, 0.0, 22.5, 24.28019135391431, 0.1627450223806232, 1.0, 1.0, 60.0, 59.98219418233225], 
processed observation next is [1.0, 0.782608695652174, 0.35457063711911363, 0.54, 0.0, 0.0, 0.375, 0.5233492794928593, 0.5542483407935411, 1.0, 1.0, 0.9, 0.5998219418233225], 
reward next is 0.4002, 
noisyNet noise sample is [array([-1.2868288], dtype=float32), 0.04578804]. 
=============================================
[2019-04-09 14:54:56,954] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.08271996 0.0840547  0.0840126  0.05916806 0.10921256 0.12421811
 0.10595521 0.08431835 0.089366   0.1023472  0.07462725], sum to 1.0000
[2019-04-09 14:54:56,955] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0063
[2019-04-09 14:54:57,047] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-2.3, 76.0, 65.0, 24.5, 22.5, 24.37601250329199, 0.0184507106531499, 1.0, 1.0, 60.0, 68.37648430727029], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 723600.0000, 
sim time next is 724800.0000, 
raw observation next is [-2.1, 73.33333333333334, 89.0, 40.83333333333334, 22.5, 24.66423462214451, -0.0704098341338215, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.404432132963989, 0.7333333333333334, 0.2966666666666667, 0.045119705340699826, 0.375, 0.5553528851787091, 0.47653005528872616, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04130216], dtype=float32), -1.1257538]. 
=============================================
[2019-04-09 14:54:57,328] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.09474849 0.09635752 0.08674997 0.0648647  0.1237391  0.10438371
 0.09468794 0.08792126 0.07157477 0.10694752 0.06802505], sum to 1.0000
[2019-04-09 14:54:57,328] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4460
[2019-04-09 14:54:57,356] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.07151204 0.11621188 0.09251409 0.05580424 0.11891236 0.11753152
 0.10354968 0.08480828 0.08893042 0.08771057 0.06251489], sum to 1.0000
[2019-04-09 14:54:57,357] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8388
[2019-04-09 14:54:57,386] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.566666666666667, 59.66666666666667, 165.1666666666667, 86.83333333333333, 19.0, 22.59832488360446, -0.2437567144727551, 0.0, 1.0, 55.0, 49.10182128805687], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 654000.0000, 
sim time next is 655200.0000, 
raw observation next is [-1.2, 60.0, 131.5, 74.5, 19.0, 22.7088642664041, -0.2378280423505984, 0.0, 1.0, 35.0, 37.104642919479026], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.6, 0.43833333333333335, 0.08232044198895028, 0.08333333333333333, 0.3924053555336749, 0.4207239858831338, 0.0, 1.0, 0.4, 0.37104642919479025], 
reward next is 0.6290, 
noisyNet noise sample is [array([-0.02432369], dtype=float32), -0.40951496]. 
=============================================
[2019-04-09 14:54:57,423] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-0.6, 45.0, 76.5, 17.0, 22.5, 25.21406985533802, 0.1591452646162944, 1.0, 1.0, 40.0, 23.640079435170257], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 748800.0000, 
sim time next is 750000.0000, 
raw observation next is [-1.333333333333333, 48.0, 70.83333333333334, 7.666666666666665, 22.5, 24.22232266846773, 0.2331819475487141, 1.0, 1.0, 55.0, 51.60370279043188], 
processed observation next is [1.0, 0.6956521739130435, 0.42566943674976926, 0.48, 0.23611111111111113, 0.008471454880294658, 0.375, 0.5185268890389775, 0.5777273158495714, 1.0, 1.0, 0.8, 0.5160370279043188], 
reward next is 0.4840, 
noisyNet noise sample is [array([0.06393407], dtype=float32), 0.44159335]. 
=============================================
[2019-04-09 14:54:57,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[0.6892003 ]
 [0.70771724]
 [0.68340194]
 [0.62859225]
 [0.63530725]], R is [[1.20239997]
 [1.9539752 ]
 [2.66616297]
 [3.34104276]
 [3.96566129]].
[2019-04-09 14:54:58,527] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.07574164 0.13051799 0.08624151 0.07882449 0.09611785 0.10814398
 0.10684953 0.08383987 0.08558377 0.10057718 0.04756222], sum to 1.0000
[2019-04-09 14:54:58,527] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7533
[2019-04-09 14:54:58,577] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-7.300000000000001, 71.0, 0.0, 0.0, 19.0, 22.28670715487097, -0.2728738668990882, 0.0, 1.0, 25.0, 57.09580624329759], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 778800.0000, 
sim time next is 780000.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 19.0, 22.38577986785153, -0.2669156825502294, 0.0, 1.0, 50.0, 37.87891235796177], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.71, 0.0, 0.0, 0.08333333333333333, 0.3654816556542941, 0.41102810581659016, 0.0, 1.0, 0.7, 0.3787891235796177], 
reward next is 0.6212, 
noisyNet noise sample is [array([0.09745689], dtype=float32), 0.6567785]. 
=============================================
[2019-04-09 14:54:58,616] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[0.46941063]
 [0.606642  ]
 [0.72312135]
 [0.64879376]
 [0.70783764]], R is [[1.27524912]
 [1.69153857]
 [2.67462325]
 [3.22129822]
 [3.63322067]].
[2019-04-09 14:54:58,635] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-09 14:54:58,637] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:54:58,637] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:54:58,638] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run2
[2019-04-09 14:54:58,656] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:54:58,657] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:54:58,659] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run2
[2019-04-09 14:54:58,675] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:54:58,676] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:54:58,679] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run2
[2019-04-09 14:56:24,309] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2910.1189 131576.3378 1166.2415
[2019-04-09 14:56:24,329] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:56:24,329] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:56:24,461] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:56:24,461] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:56:32,393] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2830.9617 141200.3587 867.0955
[2019-04-09 14:56:32,416] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:56:32,416] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:56:32,529] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:56:32,529] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:56:35,649] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2807.4267 143370.6392 647.9857
[2019-04-09 14:56:35,669] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:56:35,669] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:56:35,770] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:56:35,770] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:56:36,672] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 10000, evaluation results [10000.0, 2830.9616864941418, 141200.3587383286, 867.0954791370314, 2910.1188743487096, 131576.33778847032, 1166.2415159958862, 2807.4266510551965, 143370.63915615284, 647.9856577821115]
[2019-04-09 14:56:36,855] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0986647  0.12118542 0.07275485 0.06532408 0.10590457 0.11177131
 0.09330395 0.09811705 0.07657887 0.09609096 0.0603043 ], sum to 1.0000
[2019-04-09 14:56:36,856] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5752
[2019-04-09 14:56:36,870] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.07231952 0.10686642 0.09843524 0.06147703 0.09660742 0.12449671
 0.11669741 0.06619214 0.10432606 0.08462375 0.0679583 ], sum to 1.0000
[2019-04-09 14:56:36,870] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6797
[2019-04-09 14:56:36,894] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 19.0, 20.76576099757276, -0.6301501780981261, 0.0, 1.0, 20.0, 47.229641391201184], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 699600.0000, 
sim time next is 700800.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 19.0, 20.9792331502383, -0.6274423157667602, 0.0, 1.0, 40.0, 29.779051030926105], 
processed observation next is [1.0, 0.08695652173913043, 0.368421052631579, 0.75, 0.0, 0.0, 0.08333333333333333, 0.24826942918652506, 0.2908525614110799, 0.0, 1.0, 0.5, 0.29779051030926107], 
reward next is 0.7022, 
noisyNet noise sample is [array([0.31898212], dtype=float32), -0.023087783]. 
=============================================
[2019-04-09 14:56:36,944] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.9, 53.0, 0.0, 0.0, 22.5, 24.23408961649409, 0.1558573963399896, 1.0, 1.0, 60.0, 55.120762062497974], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 759600.0000, 
sim time next is 760800.0000, 
raw observation next is [-4.266666666666667, 54.66666666666667, 0.0, 0.0, 22.5, 24.39603096412656, 0.1673626507848892, 1.0, 1.0, 50.0, 41.344971262518165], 
processed observation next is [1.0, 0.8260869565217391, 0.3444136657433057, 0.5466666666666667, 0.0, 0.0, 0.375, 0.5330025803438799, 0.5557875502616297, 1.0, 1.0, 0.7, 0.41344971262518165], 
reward next is 0.5866, 
noisyNet noise sample is [array([-0.38005134], dtype=float32), -0.0021782927]. 
=============================================
[2019-04-09 14:56:37,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.07136197 0.0991923  0.08761083 0.05756527 0.10475272 0.13098477
 0.11628688 0.07442378 0.10192862 0.09231649 0.06357642], sum to 1.0000
[2019-04-09 14:56:37,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9159
[2019-04-09 14:56:37,036] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.266666666666667, 54.66666666666667, 0.0, 0.0, 22.5, 24.3578987444756, 0.19108247860404, 1.0, 1.0, 45.0, 35.939389049837516], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 760800.0000, 
sim time next is 762000.0000, 
raw observation next is [-4.633333333333333, 56.33333333333333, 0.0, 0.0, 22.5, 24.40746668240361, 0.2112634588066592, 1.0, 1.0, 60.0, 58.46007631225895], 
processed observation next is [1.0, 0.8260869565217391, 0.3342566943674977, 0.5633333333333332, 0.0, 0.0, 0.375, 0.5339555568669674, 0.5704211529355531, 1.0, 1.0, 0.9, 0.5846007631225895], 
reward next is 0.4154, 
noisyNet noise sample is [array([0.55628896], dtype=float32), -0.6003164]. 
=============================================
[2019-04-09 14:56:37,065] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[0.83111995]
 [0.89195234]
 [0.87608314]
 [0.7872785 ]
 [0.84972876]], R is [[1.22813272]
 [1.85645747]
 [2.31063914]
 [2.9790144 ]
 [3.67347932]].
[2019-04-09 14:56:37,808] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.05883342 0.10180786 0.0945731  0.05790353 0.09638746 0.11202832
 0.13597855 0.07605719 0.10727789 0.09064163 0.06851096], sum to 1.0000
[2019-04-09 14:56:37,809] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7255
[2019-04-09 14:56:37,889] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.9, 56.0, 0.0, 0.0, 22.5, 26.51745093196826, 0.4618023497441726, 1.0, 1.0, 40.0, 36.88097301126896], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 756000.0000, 
sim time next is 757200.0000, 
raw observation next is [-3.9, 55.0, 0.0, 0.0, 22.5, 24.79740333915658, 0.3064911282061316, 1.0, 1.0, 50.0, 36.1855273229077], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.55, 0.0, 0.0, 0.375, 0.5664502782630484, 0.6021637094020439, 1.0, 1.0, 0.7, 0.361855273229077], 
reward next is 0.6381, 
noisyNet noise sample is [array([-0.39616552], dtype=float32), -0.14740312]. 
=============================================
[2019-04-09 14:56:38,274] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.085302   0.08399004 0.0850089  0.06726877 0.10991126 0.11551828
 0.11564016 0.08227408 0.09013128 0.09577308 0.06918208], sum to 1.0000
[2019-04-09 14:56:38,274] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7298
[2019-04-09 14:56:38,325] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.1, 73.33333333333334, 89.0, 40.83333333333334, 22.5, 23.18892323785091, -0.3431230121311228, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 724800.0000, 
sim time next is 726000.0000, 
raw observation next is [-1.9, 70.66666666666666, 107.3333333333333, 52.16666666666666, 22.5, 23.25387942986611, -0.2092869483610677, 1.0, 1.0, 50.0, 57.81548960847062], 
processed observation next is [1.0, 0.391304347826087, 0.4099722991689751, 0.7066666666666666, 0.3577777777777777, 0.05764272559852669, 0.375, 0.43782328582217583, 0.43023768387964406, 1.0, 1.0, 0.7, 0.5781548960847062], 
reward next is 0.4218, 
noisyNet noise sample is [array([-0.45821893], dtype=float32), -0.89414734]. 
=============================================
[2019-04-09 14:56:38,352] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[0.73995364]
 [0.8398202 ]
 [0.60315585]
 [0.74296796]
 [0.76784915]], R is [[1.23658991]
 [2.22422409]
 [2.6657536 ]
 [3.1205771 ]
 [4.0688324 ]].
[2019-04-09 14:56:39,283] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.07829564 0.09065653 0.09365703 0.05405621 0.10859901 0.10929829
 0.12227958 0.07261036 0.11023073 0.09213465 0.06818189], sum to 1.0000
[2019-04-09 14:56:39,283] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5904
[2019-04-09 14:56:39,343] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 75.0, 74.0, 0.0, 22.5, 23.84567743814207, -0.03526162186861018, 1.0, 1.0, 55.0, 67.38538267014962], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 813600.0000, 
sim time next is 814800.0000, 
raw observation next is [-5.633333333333334, 73.66666666666667, 82.66666666666666, 0.0, 22.5, 24.23931256190595, -0.005001788886105733, 1.0, 1.0, 45.0, 37.631904774588], 
processed observation next is [1.0, 0.43478260869565216, 0.30655586334256696, 0.7366666666666667, 0.2755555555555555, 0.0, 0.375, 0.5199427134921626, 0.4983327370379647, 1.0, 1.0, 0.6, 0.37631904774588], 
reward next is 0.6237, 
noisyNet noise sample is [array([-0.23812512], dtype=float32), -0.13365038]. 
=============================================
[2019-04-09 14:56:39,409] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0750009  0.10376287 0.09335189 0.04988868 0.10796657 0.1154614
 0.10821653 0.0806215  0.0897414  0.09400437 0.08198396], sum to 1.0000
[2019-04-09 14:56:39,409] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6285
[2019-04-09 14:56:39,519] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 73.66666666666667, 100.8333333333333, 0.0, 22.5, 25.42721944415864, 0.2852286601867744, 1.0, 1.0, 35.0, 29.199528037131323], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 822000.0000, 
sim time next is 823200.0000, 
raw observation next is [-4.5, 76.33333333333333, 97.66666666666666, 0.0, 22.5, 25.56131969874282, 0.3004423528787514, 1.0, 1.0, 45.0, 34.87240658498271], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7633333333333333, 0.32555555555555554, 0.0, 0.375, 0.6301099748952351, 0.6001474509595838, 1.0, 1.0, 0.6, 0.3487240658498271], 
reward next is 0.6513, 
noisyNet noise sample is [array([-1.2283175], dtype=float32), 1.3648136]. 
=============================================
[2019-04-09 14:56:39,521] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.07428075 0.11179683 0.09454703 0.04766769 0.11518369 0.1106979
 0.09891672 0.08122473 0.09386306 0.09202566 0.07979595], sum to 1.0000
[2019-04-09 14:56:39,521] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0214
[2019-04-09 14:56:39,602] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-4.5, 76.33333333333333, 97.66666666666666, 0.0, 22.5, 25.56131969874282, 0.3004423528787514, 1.0, 1.0, 45.0, 34.87240658498271], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 823200.0000, 
sim time next is 824400.0000, 
raw observation next is [-4.5, 79.0, 95.0, 0.0, 22.5, 25.45812806948511, 0.2112208231650051, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.79, 0.31666666666666665, 0.0, 0.375, 0.6215106724570925, 0.5704069410550017, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2283175], dtype=float32), 1.3648136]. 
=============================================
[2019-04-09 14:56:39,775] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.09585446 0.11164995 0.08678772 0.05570108 0.13062496 0.10182402
 0.09036535 0.08755653 0.08375198 0.09168023 0.06420364], sum to 1.0000
[2019-04-09 14:56:39,776] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1036
[2019-04-09 14:56:39,824] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.100000000000001, 69.66666666666667, 0.0, 0.0, 19.0, 22.48198555280967, -0.2428712181534177, 0.0, 1.0, 20.0, 35.43305632791335], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 800400.0000, 
sim time next is 801600.0000, 
raw observation next is [-6.9, 68.33333333333333, 0.0, 0.0, 19.0, 22.63825927527352, -0.2464140453178969, 0.0, 1.0, 20.0, 31.08755015639165], 
processed observation next is [1.0, 0.2608695652173913, 0.27146814404432135, 0.6833333333333332, 0.0, 0.0, 0.08333333333333333, 0.38652160627279325, 0.41786198489403437, 0.0, 1.0, 0.1, 0.3108755015639165], 
reward next is 0.6891, 
noisyNet noise sample is [array([-0.42910892], dtype=float32), -0.15668267]. 
=============================================
[2019-04-09 14:56:40,053] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.05527685 0.11601108 0.08828776 0.05487928 0.10637815 0.14028981
 0.11499823 0.0598795  0.10085566 0.09416204 0.06898163], sum to 1.0000
[2019-04-09 14:56:40,053] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3407
[2019-04-09 14:56:40,103] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.633333333333333, 56.33333333333333, 0.0, 0.0, 22.5, 24.67375143700782, 0.2102823437773283, 1.0, 1.0, 35.0, 36.183999101277124], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 762000.0000, 
sim time next is 763200.0000, 
raw observation next is [-5.0, 58.0, 0.0, 0.0, 22.5, 24.47861353049319, 0.1613972155480764, 1.0, 1.0, 35.0, 32.568071639695525], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.58, 0.0, 0.0, 0.375, 0.5398844608744326, 0.5537990718493587, 1.0, 1.0, 0.4, 0.3256807163969552], 
reward next is 0.6743, 
noisyNet noise sample is [array([-1.993551], dtype=float32), -2.2945933]. 
=============================================
[2019-04-09 14:56:40,210] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.07397534 0.0980723  0.09868149 0.05497439 0.1058866  0.12258241
 0.11185531 0.06950289 0.10231773 0.09351154 0.06864004], sum to 1.0000
[2019-04-09 14:56:40,211] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7328
[2019-04-09 14:56:40,235] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-3.9, 84.66666666666667, 50.66666666666666, 0.0, 22.5, 24.46483559879167, 0.06320235575098554, 1.0, 1.0, 45.0, 27.384097417463458], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 832800.0000, 
sim time next is 834000.0000, 
raw observation next is [-3.899999999999999, 83.33333333333334, 45.66666666666667, 0.0, 22.5, 24.37431695210161, 0.06734799523996372, 1.0, 1.0, 50.0, 37.920673048381985], 
processed observation next is [1.0, 0.6521739130434783, 0.35457063711911363, 0.8333333333333335, 0.15222222222222223, 0.0, 0.375, 0.5311930793418007, 0.5224493317466546, 1.0, 1.0, 0.7, 0.37920673048381986], 
reward next is 0.6208, 
noisyNet noise sample is [array([0.31382287], dtype=float32), -0.68182904]. 
=============================================
[2019-04-09 14:56:40,274] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[0.9085316 ]
 [0.95138925]
 [0.85870343]
 [0.76749283]
 [0.93989176]], R is [[1.46772742]
 [2.17920923]
 [2.85534716]
 [3.47180176]
 [3.9398942 ]].
[2019-04-09 14:56:40,295] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.07247356 0.11857285 0.10396078 0.04781066 0.12226817 0.11083244
 0.09747479 0.07343944 0.09189408 0.08405972 0.07721345], sum to 1.0000
[2019-04-09 14:56:40,296] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5810
[2019-04-09 14:56:40,389] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 83.33333333333334, 32.33333333333333, 0.0, 22.5, 26.12540908220161, 0.3661988681130484, 1.0, 1.0, 55.0, 45.53418006036462], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 836400.0000, 
sim time next is 837600.0000, 
raw observation next is [-3.899999999999999, 84.66666666666666, 24.16666666666667, 0.0, 22.5, 25.38553354616144, 0.3724520182999226, 1.0, 1.0, 45.0, 35.935941443510515], 
processed observation next is [1.0, 0.6956521739130435, 0.35457063711911363, 0.8466666666666666, 0.08055555555555557, 0.0, 0.375, 0.6154611288467867, 0.6241506727666408, 1.0, 1.0, 0.6, 0.35935941443510516], 
reward next is 0.6406, 
noisyNet noise sample is [array([1.621749], dtype=float32), 0.8501735]. 
=============================================
[2019-04-09 14:56:40,658] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.09807081 0.11981702 0.0763031  0.0556494  0.11878841 0.11131408
 0.08901718 0.08751453 0.07702458 0.09884523 0.06765565], sum to 1.0000
[2019-04-09 14:56:40,667] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3005
[2019-04-09 14:56:40,699] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 19.0, 21.94246409151212, -0.444945713243746, 0.0, 1.0, 40.0, 23.774371876004587], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 873600.0000, 
sim time next is 874800.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 19.0, 21.73799624599263, -0.4791042725527847, 0.0, 1.0, 30.0, 21.54606612138928], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.79, 0.0, 0.0, 0.08333333333333333, 0.3114996871660525, 0.3402985758157384, 0.0, 1.0, 0.3, 0.21546066121389282], 
reward next is 0.7845, 
noisyNet noise sample is [array([-1.3976688], dtype=float32), 0.6363407]. 
=============================================
[2019-04-09 14:56:41,342] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.08312637 0.11490801 0.09497488 0.04530896 0.11124373 0.12243786
 0.09329708 0.09217346 0.07281226 0.1058836  0.06383376], sum to 1.0000
[2019-04-09 14:56:41,343] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0669
[2019-04-09 14:56:41,382] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-1.533333333333333, 78.0, 0.0, 0.0, 19.0, 22.18530634614526, -0.3092704004282255, 0.0, 1.0, 60.0, 56.56200228028385], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 876000.0000, 
sim time next is 877200.0000, 
raw observation next is [-1.366666666666667, 77.0, 0.0, 0.0, 19.0, 22.35595520196872, -0.4448630737944292, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.42474607571560485, 0.77, 0.0, 0.0, 0.08333333333333333, 0.36299626683072655, 0.35171230873519027, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.94248563], dtype=float32), 0.11022168]. 
=============================================
[2019-04-09 14:56:41,412] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.07712609 0.10579105 0.08959075 0.05302287 0.12829591 0.1035914
 0.12366799 0.06376757 0.1070075  0.0870342  0.06110466], sum to 1.0000
[2019-04-09 14:56:41,412] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0871
[2019-04-09 14:56:41,450] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.566666666666666, 84.0, 0.0, 0.0, 22.5, 23.17166296400671, -0.05329625937421434, 1.0, 1.0, 25.0, 44.005346788287056], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 848400.0000, 
sim time next is 849600.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 22.5, 23.15665392019579, -0.0774748592737191, 0.0, 1.0, 20.0, 25.71118509492797], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.375, 0.42972116001631583, 0.4741750469087603, 0.0, 1.0, 0.1, 0.2571118509492797], 
reward next is 0.7429, 
noisyNet noise sample is [array([-1.684371], dtype=float32), -1.3111836]. 
=============================================
[2019-04-09 14:56:41,482] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.06876106 0.10075119 0.09705611 0.05091437 0.11580297 0.1200844
 0.1118374  0.06940435 0.08793233 0.10607083 0.07138499], sum to 1.0000
[2019-04-09 14:56:41,482] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3155
[2019-04-09 14:56:41,591] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.300000000000001, 79.0, 92.33333333333334, 0.0, 22.5, 26.62748936057401, 0.530624018421073, 1.0, 1.0, 30.0, 36.24819484327264], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 825600.0000, 
sim time next is 826800.0000, 
raw observation next is [-4.1, 79.0, 85.66666666666667, 0.0, 22.5, 26.72781640943093, 0.5348173826957751, 1.0, 1.0, 30.0, 31.798293424084896], 
processed observation next is [1.0, 0.5652173913043478, 0.3490304709141275, 0.79, 0.28555555555555556, 0.0, 0.375, 0.7273180341192441, 0.6782724608985916, 1.0, 1.0, 0.3, 0.31798293424084895], 
reward next is 0.6820, 
noisyNet noise sample is [array([0.7163571], dtype=float32), 1.3457137]. 
=============================================
[2019-04-09 14:56:41,669] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.07958075 0.10519653 0.08283753 0.04893127 0.11269949 0.11296409
 0.12035003 0.0816173  0.10706503 0.09243698 0.05632098], sum to 1.0000
[2019-04-09 14:56:41,670] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5670
[2019-04-09 14:56:41,696] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 19.0, 22.70936708538695, -0.2029790041640664, 0.0, 1.0, 35.0, 21.536042016530285], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 855600.0000, 
sim time next is 856800.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 19.0, 22.56911845848683, -0.2032351413588349, 0.0, 1.0, 45.0, 34.656439938986175], 
processed observation next is [1.0, 0.9565217391304348, 0.368421052631579, 0.83, 0.0, 0.0, 0.08333333333333333, 0.3807598715405692, 0.43225495288038834, 0.0, 1.0, 0.6, 0.3465643993898617], 
reward next is 0.6534, 
noisyNet noise sample is [array([1.9955578], dtype=float32), -0.3853029]. 
=============================================
[2019-04-09 14:56:42,007] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.06758355 0.10706188 0.10661972 0.05543447 0.14034753 0.09532548
 0.1044507  0.06496727 0.11090369 0.08358557 0.06372017], sum to 1.0000
[2019-04-09 14:56:42,008] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7941
[2019-04-09 14:56:42,019] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.07498771 0.11965083 0.10411113 0.0428124  0.1294341  0.10535074
 0.10263692 0.06272701 0.09261266 0.10666965 0.05900678], sum to 1.0000
[2019-04-09 14:56:42,019] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9739
[2019-04-09 14:56:42,029] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06208902 0.0946452  0.09637669 0.04786471 0.1169556  0.14006107
 0.11498477 0.07596933 0.09604163 0.08883408 0.06617794], sum to 1.0000
[2019-04-09 14:56:42,030] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2400
[2019-04-09 14:56:42,033] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 19.0, 23.34253721620694, -0.08046326602085016, 0.0, 1.0, 45.0, 27.80547456655605], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 850800.0000, 
sim time next is 852000.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 19.0, 23.19688624113667, -0.08004678624452378, 0.0, 1.0, 55.0, 47.151233712089706], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.08333333333333333, 0.4330738534280559, 0.4733177379184921, 0.0, 1.0, 0.8, 0.4715123371208971], 
reward next is 0.5285, 
noisyNet noise sample is [array([1.1913606], dtype=float32), 1.2437418]. 
=============================================
[2019-04-09 14:56:42,051] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[1.1070648]
 [1.1397098]
 [1.0935535]
 [1.1243913]
 [1.1342047]], R is [[1.56183493]
 [2.26816177]
 [2.93914175]
 [3.57032585]
 [4.13897038]].
[2019-04-09 14:56:42,059] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 19.0, 25.5255256273599, 0.4410870866824084, 0.0, 1.0, 65.0, 60.427932652412196], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 853200.0000, 
sim time next is 854400.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 19.0, 25.66682321150498, 0.4571799673935464, 0.0, 1.0, 65.0, 57.9669152280901], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6389019342920816, 0.6523933224645154, 0.0, 1.0, 1.0, 0.579669152280901], 
reward next is 0.4203, 
noisyNet noise sample is [array([0.8988544], dtype=float32), -0.07232441]. 
=============================================
[2019-04-09 14:56:42,076] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.9, 81.33333333333334, 64.33333333333333, 0.0, 22.5, 25.64623331054012, 0.207495805920789, 1.0, 1.0, 45.0, 31.81012433373084], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 829200.0000, 
sim time next is 830400.0000, 
raw observation next is [-3.899999999999999, 83.66666666666666, 57.33333333333334, 0.0, 22.5, 24.52245105017879, 0.1760878798992577, 1.0, 1.0, 25.0, 29.47685846786979], 
processed observation next is [1.0, 0.6086956521739131, 0.35457063711911363, 0.8366666666666666, 0.19111111111111115, 0.0, 0.375, 0.5435375875148992, 0.5586959599664193, 1.0, 1.0, 0.2, 0.2947685846786979], 
reward next is 0.7052, 
noisyNet noise sample is [array([1.0210813], dtype=float32), 0.7023318]. 
=============================================
[2019-04-09 14:56:42,114] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.06848352 0.08337599 0.08502818 0.05648188 0.11425401 0.14059193
 0.10849404 0.09250062 0.08594914 0.08996548 0.07487515], sum to 1.0000
[2019-04-09 14:56:42,114] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5191
[2019-04-09 14:56:42,154] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.2, 75.0, 56.16666666666667, 0.0, 22.5, 23.11926366133281, -0.2163471214965164, 1.0, 1.0, 50.0, 39.38057709182994], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 811200.0000, 
sim time next is 812400.0000, 
raw observation next is [-6.199999999999999, 75.0, 65.33333333333333, 0.0, 22.5, 23.28610766277107, -0.2006621503628897, 1.0, 1.0, 40.0, 31.04837817143767], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.75, 0.21777777777777776, 0.0, 0.375, 0.44050897189758914, 0.4331126165457035, 1.0, 1.0, 0.5, 0.3104837817143767], 
reward next is 0.6895, 
noisyNet noise sample is [array([0.23829676], dtype=float32), 1.0277817]. 
=============================================
[2019-04-09 14:56:42,200] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.08761655 0.12961379 0.10932086 0.05817046 0.10950464 0.10655516
 0.09160566 0.07681175 0.08815094 0.0849466  0.05770361], sum to 1.0000
[2019-04-09 14:56:42,202] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2331
[2019-04-09 14:56:42,230] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 19.0, 23.66219007768482, -0.02239519954137999, 0.0, 1.0, 20.0, 31.66063103747749], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 854400.0000, 
sim time next is 855600.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 19.0, 23.43275238858825, -0.06752113652994872, 0.0, 1.0, 25.0, 28.70658843606825], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 0.08333333333333333, 0.4527293657156874, 0.4774929544900171, 0.0, 1.0, 0.2, 0.28706588436068253], 
reward next is 0.7129, 
noisyNet noise sample is [array([-0.8277068], dtype=float32), 0.06834649]. 
=============================================
[2019-04-09 14:56:42,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.09917779 0.09853022 0.0884255  0.04923274 0.11188899 0.1172542
 0.10995104 0.09335432 0.07229387 0.0984675  0.06142386], sum to 1.0000
[2019-04-09 14:56:42,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8096
[2019-04-09 14:56:42,265] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 23.06422510039177, -0.1857497126117521, 0.0, 1.0, 20.0, 39.174537375113374], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 885600.0000, 
sim time next is 886800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 23.03799241164493, -0.2036301225250463, 0.0, 1.0, 55.0, 42.241287793803735], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.41983270097041075, 0.4321232924916512, 0.0, 1.0, 0.8, 0.42241287793803733], 
reward next is 0.5776, 
noisyNet noise sample is [array([0.63439983], dtype=float32), 0.120739274]. 
=============================================
[2019-04-09 14:56:42,502] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.07241227 0.0977554  0.09427807 0.05631231 0.11239649 0.11400765
 0.10998818 0.08143602 0.09768693 0.08181595 0.08191072], sum to 1.0000
[2019-04-09 14:56:42,504] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7025
[2019-04-09 14:56:42,568] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.5, 71.0, 104.5, 0.0, 22.5, 23.02163233303615, -0.2307971395830717, 1.0, 1.0, 20.0, 26.910065018289576], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 820800.0000, 
sim time next is 822000.0000, 
raw observation next is [-4.5, 73.66666666666667, 100.8333333333333, 0.0, 22.5, 22.97236335524945, -0.1997984745015021, 1.0, 1.0, 50.0, 43.95057378785364], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7366666666666667, 0.336111111111111, 0.0, 0.375, 0.4143636129374541, 0.43340050849949935, 1.0, 1.0, 0.7, 0.4395057378785364], 
reward next is 0.5605, 
noisyNet noise sample is [array([0.45196998], dtype=float32), -1.8326437]. 
=============================================
[2019-04-09 14:56:42,580] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[0.7854997 ]
 [0.84100246]
 [0.8750371 ]
 [0.8544415 ]
 [0.7446784 ]], R is [[1.42647195]
 [2.14310646]
 [2.79280472]
 [3.44048238]
 [4.20029259]].
[2019-04-09 14:56:42,710] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.0643286  0.08726416 0.08930995 0.04578975 0.13088048 0.12321499
 0.10555647 0.08617455 0.08854304 0.1009628  0.07797521], sum to 1.0000
[2019-04-09 14:56:42,710] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9815
[2019-04-09 14:56:42,774] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 80.0, 38.5, 0.0, 22.5, 23.96037467415351, 0.007016555414733334, 1.0, 1.0, 55.0, 42.80116299659302], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 896400.0000, 
sim time next is 897600.0000, 
raw observation next is [1.1, 81.33333333333334, 44.83333333333333, 0.0, 22.5, 24.29229541447878, 0.11265032839942, 1.0, 1.0, 65.0, 63.48636161676073], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8133333333333335, 0.14944444444444444, 0.0, 0.375, 0.5243579512065649, 0.5375501094664733, 1.0, 1.0, 1.0, 0.6348636161676073], 
reward next is 0.3651, 
noisyNet noise sample is [array([0.30558714], dtype=float32), 1.079188]. 
=============================================
[2019-04-09 14:56:43,093] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0775239  0.10421561 0.09617095 0.05478973 0.12122457 0.11430234
 0.10614944 0.07924651 0.09890318 0.08532237 0.06215135], sum to 1.0000
[2019-04-09 14:56:43,098] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9953
[2019-04-09 14:56:43,177] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.7, 97.0, 100.5, 0.0, 22.5, 24.56380340620829, 0.1101795960723883, 1.0, 1.0, 25.0, 18.08231397387254], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 907200.0000, 
sim time next is 908400.0000, 
raw observation next is [3.066666666666667, 95.66666666666667, 102.8333333333333, 0.0, 22.5, 24.15495582860324, 0.122595145624295, 1.0, 1.0, 65.0, 51.0308925937389], 
processed observation next is [1.0, 0.5217391304347826, 0.5475530932594646, 0.9566666666666667, 0.3427777777777777, 0.0, 0.375, 0.5129129857169366, 0.5408650485414317, 1.0, 1.0, 1.0, 0.510308925937389], 
reward next is 0.4897, 
noisyNet noise sample is [array([1.4587374], dtype=float32), 0.35439077]. 
=============================================
[2019-04-09 14:56:43,769] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.06447501 0.10170009 0.10846449 0.04970481 0.12361191 0.10773116
 0.12241573 0.08255244 0.10064369 0.07462733 0.06407335], sum to 1.0000
[2019-04-09 14:56:43,769] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1447
[2019-04-09 14:56:43,790] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.07478815 0.10544198 0.10412586 0.05057545 0.11666804 0.12963231
 0.09328278 0.07310201 0.09242275 0.09355942 0.06640121], sum to 1.0000
[2019-04-09 14:56:43,790] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1268
[2019-04-09 14:56:43,822] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.600000000000001, 92.66666666666667, 24.0, 0.0, 22.5, 25.53257325038169, 0.373064890726724, 1.0, 1.0, 20.0, 19.684165784674164], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 922800.0000, 
sim time next is 924000.0000, 
raw observation next is [4.8, 92.33333333333333, 15.0, 0.0, 22.5, 25.46608554437699, 0.256510744491703, 1.0, 1.0, 20.0, 18.81651666030433], 
processed observation next is [1.0, 0.6956521739130435, 0.5955678670360112, 0.9233333333333333, 0.05, 0.0, 0.375, 0.6221737953647493, 0.5855035814972344, 1.0, 1.0, 0.1, 0.1881651666030433], 
reward next is 0.8118, 
noisyNet noise sample is [array([-0.21005654], dtype=float32), 0.006387038]. 
=============================================
[2019-04-09 14:56:43,838] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[1.0816923]
 [1.1900324]
 [1.035225 ]
 [1.1347659]
 [1.029669 ]], R is [[1.92992067]
 [2.71377993]
 [3.4708221 ]
 [4.19852161]
 [4.87694836]].
[2019-04-09 14:56:43,860] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.06914245 0.0970263  0.0910703  0.04329108 0.12745926 0.13488093
 0.1075443  0.07703327 0.09715626 0.09205227 0.06334361], sum to 1.0000
[2019-04-09 14:56:43,860] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4958
[2019-04-09 14:56:43,863] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 73.66666666666667, 100.8333333333333, 0.0, 22.5, 24.44253679362607, 0.08030893467860989, 1.0, 1.0, 30.0, 28.134311907457267], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 822000.0000, 
sim time next is 823200.0000, 
raw observation next is [-4.5, 76.33333333333333, 97.66666666666666, 0.0, 22.5, 23.81552931216811, -0.02494175400764203, 1.0, 1.0, 20.0, 27.530954878581028], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7633333333333333, 0.32555555555555554, 0.0, 0.375, 0.4846274426806758, 0.4916860819974527, 1.0, 1.0, 0.1, 0.27530954878581027], 
reward next is 0.7247, 
noisyNet noise sample is [array([-0.5345918], dtype=float32), 1.9685937]. 
=============================================
[2019-04-09 14:56:43,883] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.07915632 0.10588326 0.10338175 0.04559632 0.11798627 0.12126721
 0.09388438 0.07180281 0.09477874 0.09419313 0.07206979], sum to 1.0000
[2019-04-09 14:56:43,883] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6231
[2019-04-09 14:56:43,903] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 80.0, 38.5, 0.0, 22.5, 25.62112743702682, 0.3583162425397917, 1.0, 1.0, 50.0, 32.87751559650854], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 896400.0000, 
sim time next is 897600.0000, 
raw observation next is [1.1, 81.33333333333334, 44.83333333333333, 0.0, 22.5, 25.66285700293879, 0.3560735511732647, 1.0, 1.0, 40.0, 27.179261400388466], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8133333333333335, 0.14944444444444444, 0.0, 0.375, 0.6385714169115658, 0.6186911837244216, 1.0, 1.0, 0.5, 0.27179261400388466], 
reward next is 0.7282, 
noisyNet noise sample is [array([0.13527066], dtype=float32), 0.02330191]. 
=============================================
[2019-04-09 14:56:43,927] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 76.33333333333333, 97.66666666666666, 0.0, 22.5, 23.81552931216811, -0.02494175400764203, 1.0, 1.0, 20.0, 27.530954878581028], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 823200.0000, 
sim time next is 824400.0000, 
raw observation next is [-4.5, 79.0, 95.0, 0.0, 22.5, 23.46562213653446, -0.04019716560377408, 1.0, 1.0, 45.0, 32.56542039527677], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.79, 0.31666666666666665, 0.0, 0.375, 0.45546851137787164, 0.486600944798742, 1.0, 1.0, 0.6, 0.3256542039527677], 
reward next is 0.6743, 
noisyNet noise sample is [array([-0.5345918], dtype=float32), 1.9685937]. 
=============================================
[2019-04-09 14:56:44,058] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.08761539 0.10497745 0.08735994 0.04396198 0.12397204 0.12103102
 0.08948667 0.09013286 0.08333404 0.09996927 0.06815933], sum to 1.0000
[2019-04-09 14:56:44,059] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7325
[2019-04-09 14:56:44,098] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.0, 74.66666666666667, 0.0, 0.0, 19.0, 23.04450487672536, -0.1796961899991651, 0.0, 1.0, 45.0, 33.40999383385315], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 879600.0000, 
sim time next is 880800.0000, 
raw observation next is [-0.8, 73.33333333333334, 0.0, 0.0, 19.0, 22.9846577620545, -0.1984699717868348, 0.0, 1.0, 50.0, 37.21899985217578], 
processed observation next is [1.0, 0.17391304347826086, 0.4404432132963989, 0.7333333333333334, 0.0, 0.0, 0.08333333333333333, 0.415388146837875, 0.43384334273772174, 0.0, 1.0, 0.7, 0.37218999852175777], 
reward next is 0.6278, 
noisyNet noise sample is [array([0.16540518], dtype=float32), -0.8772227]. 
=============================================
[2019-04-09 14:56:44,327] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.08494455 0.10183013 0.08487997 0.04440377 0.12813988 0.12932357
 0.10635137 0.08429077 0.07327741 0.09847735 0.06408115], sum to 1.0000
[2019-04-09 14:56:44,329] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3313
[2019-04-09 14:56:44,389] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 80.0, 0.0, 0.0, 19.0, 23.84628036044307, 0.1119235653903859, 0.0, 1.0, 35.0, 34.77890882123578], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 961200.0000, 
sim time next is 962400.0000, 
raw observation next is [7.7, 81.0, 0.0, 0.0, 19.0, 23.93926282019997, 0.1115345494961776, 0.0, 1.0, 35.0, 29.080831879300966], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.81, 0.0, 0.0, 0.08333333333333333, 0.4949385683499976, 0.5371781831653926, 0.0, 1.0, 0.4, 0.29080831879300967], 
reward next is 0.7092, 
noisyNet noise sample is [array([0.5773712], dtype=float32), -0.10642497]. 
=============================================
[2019-04-09 14:56:44,640] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.06784831 0.07590348 0.09010008 0.04913392 0.12303248 0.12703277
 0.13075751 0.07353392 0.09183127 0.09456705 0.0762592 ], sum to 1.0000
[2019-04-09 14:56:44,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1234
[2019-04-09 14:56:44,796] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 81.33333333333334, 44.83333333333333, 0.0, 22.5, 25.81747292565618, 0.3317712314487609, 1.0, 1.0, 25.0, 51.24911261397344], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 897600.0000, 
sim time next is 898800.0000, 
raw observation next is [1.1, 82.66666666666667, 52.83333333333333, 0.0, 22.5, 26.03939907276632, 0.3547894271803091, 1.0, 1.0, 20.0, 41.053833110370505], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8266666666666667, 0.1761111111111111, 0.0, 0.375, 0.6699499227305266, 0.6182631423934364, 1.0, 1.0, 0.1, 0.410538331103705], 
reward next is 0.5895, 
noisyNet noise sample is [array([1.3572521], dtype=float32), 0.48493645]. 
=============================================
[2019-04-09 14:56:44,908] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.06879917 0.1043663  0.10591123 0.04304089 0.13863048 0.11332715
 0.09674531 0.07949498 0.09890748 0.07813034 0.07264668], sum to 1.0000
[2019-04-09 14:56:44,909] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2699
[2019-04-09 14:56:44,967] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [3.8, 93.0, 95.0, 0.0, 22.5, 25.73111577147473, 0.4844608230429659, 1.0, 1.0, 35.0, 37.96465422741889], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 913200.0000, 
sim time next is 914400.0000, 
raw observation next is [3.8, 93.0, 93.0, 0.0, 22.5, 26.24361561572182, 0.5345541251213056, 1.0, 1.0, 25.0, 30.840908793572417], 
processed observation next is [1.0, 0.6086956521739131, 0.5678670360110805, 0.93, 0.31, 0.0, 0.375, 0.6869679679768183, 0.6781847083737685, 1.0, 1.0, 0.2, 0.3084090879357242], 
reward next is 0.6916, 
noisyNet noise sample is [array([-0.09189151], dtype=float32), 1.2169814]. 
=============================================
[2019-04-09 14:56:45,194] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.07299389 0.09001917 0.10726597 0.05709117 0.10356539 0.13715222
 0.11526668 0.07087743 0.0995312  0.07743687 0.06879999], sum to 1.0000
[2019-04-09 14:56:45,199] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2414
[2019-04-09 14:56:45,241] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [3.066666666666667, 95.66666666666667, 102.8333333333333, 0.0, 22.5, 24.68835571353944, 0.08426479522682236, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 908400.0000, 
sim time next is 909600.0000, 
raw observation next is [3.433333333333334, 94.33333333333334, 102.6666666666667, 0.0, 22.5, 24.54906314351424, 0.2262161724390055, 1.0, 1.0, 25.0, 61.486181552083096], 
processed observation next is [1.0, 0.5217391304347826, 0.5577100646352725, 0.9433333333333335, 0.3422222222222223, 0.0, 0.375, 0.54575526195952, 0.5754053908130018, 1.0, 1.0, 0.2, 0.614861815520831], 
reward next is 0.3851, 
noisyNet noise sample is [array([0.33613482], dtype=float32), -0.47104016]. 
=============================================
[2019-04-09 14:56:45,305] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.08264155 0.11687667 0.10165209 0.040385   0.14518602 0.10821325
 0.09800592 0.06646752 0.08798305 0.09903318 0.05355572], sum to 1.0000
[2019-04-09 14:56:45,313] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8928
[2019-04-09 14:56:45,346] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 19.0, 24.53143135376417, 0.3238134082091911, 0.0, 1.0, 65.0, 65.3589420289702], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 949200.0000, 
sim time next is 950400.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 19.0, 24.91239604902394, 0.3556847689324631, 0.0, 1.0, 60.0, 48.57160268580576], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.08333333333333333, 0.5760330040853283, 0.6185615896441544, 0.0, 1.0, 0.9, 0.4857160268580576], 
reward next is 0.5143, 
noisyNet noise sample is [array([1.4814491], dtype=float32), -0.073807955]. 
=============================================
[2019-04-09 14:56:45,558] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.07543522 0.0935597  0.08984878 0.03861014 0.16726954 0.0918409
 0.11529817 0.06134227 0.10546717 0.10029031 0.06103777], sum to 1.0000
[2019-04-09 14:56:45,559] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3373
[2019-04-09 14:56:45,597] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 22.5, 25.93981017316828, 0.6477316406811182, 1.0, 1.0, 65.0, 61.35756148284996], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 936000.0000, 
sim time next is 937200.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 19.0, 26.10765012880535, 0.6605703539939178, 0.0, 1.0, 35.0, 39.54445071151166], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6756375107337792, 0.7201901179979725, 0.0, 1.0, 0.4, 0.3954445071151166], 
reward next is 0.6046, 
noisyNet noise sample is [array([-0.32024425], dtype=float32), -0.7850733]. 
=============================================
[2019-04-09 14:56:45,880] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.08714265 0.10227405 0.10295478 0.04477144 0.1152388  0.12118315
 0.0947059  0.08019064 0.09356993 0.09109047 0.06687819], sum to 1.0000
[2019-04-09 14:56:45,895] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2387
[2019-04-09 14:56:45,929] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 19.0, 22.85578713110737, -0.1922916439106788, 0.0, 1.0, 35.0, 29.42437917266671], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 873600.0000, 
sim time next is 874800.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 19.0, 22.76130557194108, -0.2174142243545612, 0.0, 1.0, 25.0, 26.740878266248405], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.79, 0.0, 0.0, 0.08333333333333333, 0.3967754643284233, 0.42752859188181297, 0.0, 1.0, 0.2, 0.267408782662484], 
reward next is 0.7326, 
noisyNet noise sample is [array([0.89722306], dtype=float32), -0.77484214]. 
=============================================
[2019-04-09 14:56:46,759] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.08686303 0.10163464 0.08599344 0.03987993 0.14658174 0.11378751
 0.11258932 0.06576061 0.09589851 0.09982999 0.05118129], sum to 1.0000
[2019-04-09 14:56:46,760] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3068
[2019-04-09 14:56:46,800] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 19.0, 24.19124836179256, 0.2408139212241307, 0.0, 1.0, 55.0, 48.85825929972354], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 948000.0000, 
sim time next is 949200.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 19.0, 24.29167538270086, 0.2600959447121616, 0.0, 1.0, 30.0, 31.785566252257738], 
processed observation next is [1.0, 1.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.08333333333333333, 0.5243062818917382, 0.5866986482373872, 0.0, 1.0, 0.3, 0.31785566252257735], 
reward next is 0.6821, 
noisyNet noise sample is [array([-1.8927761], dtype=float32), -0.4752884]. 
=============================================
[2019-04-09 14:56:46,880] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.07676108 0.08939715 0.08850005 0.04983415 0.10853042 0.12197984
 0.12820363 0.09309756 0.08911886 0.09891058 0.05566658], sum to 1.0000
[2019-04-09 14:56:46,880] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9723
[2019-04-09 14:56:46,942] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.7333333333333334, 77.33333333333333, 32.16666666666666, 0.0, 22.5, 23.98342736888072, 0.1001025251988341, 1.0, 1.0, 65.0, 62.358215645525036], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 895200.0000, 
sim time next is 896400.0000, 
raw observation next is [1.1, 80.0, 38.5, 0.0, 22.5, 24.7841641830545, 0.2092949923745956, 1.0, 1.0, 65.0, 58.87950971731431], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8, 0.12833333333333333, 0.0, 0.375, 0.5653470152545417, 0.5697649974581985, 1.0, 1.0, 1.0, 0.5887950971731432], 
reward next is 0.4112, 
noisyNet noise sample is [array([2.0634935], dtype=float32), -1.2747302]. 
=============================================
[2019-04-09 14:56:47,084] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.05789278 0.09667493 0.10125092 0.0301402  0.15342577 0.12147277
 0.11313557 0.06481694 0.10888975 0.08906177 0.06323861], sum to 1.0000
[2019-04-09 14:56:47,088] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1546
[2019-04-09 14:56:47,112] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.07325313 0.11379703 0.09157149 0.0380382  0.15184183 0.12846467
 0.09967458 0.06860788 0.09468564 0.09134398 0.04872157], sum to 1.0000
[2019-04-09 14:56:47,113] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7005
[2019-04-09 14:56:47,136] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.53333333333333, 86.0, 126.5, 0.0, 22.5, 26.23383312494183, 0.7818460160444781, 1.0, 1.0, 55.0, 44.66670929037929], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 996000.0000, 
sim time next is 997200.0000, 
raw observation next is [12.7, 86.0, 123.5, 0.0, 22.5, 26.74825790182226, 0.7846145421159626, 1.0, 1.0, 20.0, 28.255524291075822], 
processed observation next is [1.0, 0.5652173913043478, 0.8144044321329641, 0.86, 0.4116666666666667, 0.0, 0.375, 0.7290214918185217, 0.7615381807053209, 1.0, 1.0, 0.1, 0.28255524291075823], 
reward next is 0.7174, 
noisyNet noise sample is [array([-0.04331154], dtype=float32), -0.8046197]. 
=============================================
[2019-04-09 14:56:47,177] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 26.00497589637745, 0.706573328358453, 0.0, 1.0, 60.0, 51.45719197624889], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1035600.0000, 
sim time next is 1036800.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 26.15100624092777, 0.7437179102057586, 0.0, 1.0, 50.0, 33.24614970399377], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.6792505200773142, 0.7479059700685862, 0.0, 1.0, 0.7, 0.3324614970399377], 
reward next is 0.6675, 
noisyNet noise sample is [array([-0.07606424], dtype=float32), -0.034519814]. 
=============================================
[2019-04-09 14:56:48,208] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0747178  0.10020026 0.09516931 0.03977521 0.13263954 0.11733633
 0.13835062 0.06663772 0.08663271 0.09540639 0.05313402], sum to 1.0000
[2019-04-09 14:56:48,212] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3153
[2019-04-09 14:56:48,253] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 19.0, 27.059367867995, 0.866389210522152, 0.0, 1.0, 40.0, 24.951690446667214], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1023600.0000, 
sim time next is 1024800.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 19.0, 26.96832773104116, 0.8447259083628226, 0.0, 1.0, 35.0, 22.130788552335986], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7473606442534301, 0.7815753027876076, 0.0, 1.0, 0.4, 0.22130788552335987], 
reward next is 0.7787, 
noisyNet noise sample is [array([-0.02372304], dtype=float32), 1.0647414]. 
=============================================
[2019-04-09 14:56:48,275] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0683442  0.13014758 0.08968461 0.04333249 0.12982872 0.12811753
 0.1163007  0.08011143 0.08995614 0.07920661 0.04496994], sum to 1.0000
[2019-04-09 14:56:48,276] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4244
[2019-04-09 14:56:48,327] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.27962184864036, 0.9674058100867401, 0.0, 1.0, 45.0, 25.83938411517781], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1038000.0000, 
sim time next is 1039200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.42769023170224, 0.959453798098045, 0.0, 1.0, 35.0, 23.928349025706638], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.7856408526418533, 0.8198179326993483, 0.0, 1.0, 0.4, 0.23928349025706638], 
reward next is 0.7607, 
noisyNet noise sample is [array([-0.05819877], dtype=float32), 0.28585967]. 
=============================================
[2019-04-09 14:56:49,021] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06044384 0.1097656  0.10765707 0.02836963 0.15052035 0.11495353
 0.09691928 0.07396149 0.12076391 0.08785467 0.04879063], sum to 1.0000
[2019-04-09 14:56:49,032] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5360
[2019-04-09 14:56:49,072] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [14.76666666666667, 79.0, 63.16666666666666, 0.0, 22.5, 27.03884101362207, 0.9943753454524221, 1.0, 1.0, 60.0, 45.107099826095585], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1005600.0000, 
sim time next is 1006800.0000, 
raw observation next is [15.13333333333333, 77.0, 51.66666666666666, 0.0, 22.5, 27.77704722971215, 0.9730351074410987, 1.0, 1.0, 40.0, 12.881971033369888], 
processed observation next is [1.0, 0.6521739130434783, 0.8818097876269622, 0.77, 0.1722222222222222, 0.0, 0.375, 0.8147539358093457, 0.8243450358136996, 1.0, 1.0, 0.5, 0.12881971033369888], 
reward next is 0.8712, 
noisyNet noise sample is [array([-1.1309527], dtype=float32), -0.45152435]. 
=============================================
[2019-04-09 14:56:49,162] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.07384955 0.1168009  0.09996688 0.03766423 0.13460389 0.11936089
 0.12322901 0.06685252 0.09515314 0.08463122 0.04788774], sum to 1.0000
[2019-04-09 14:56:49,162] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4624
[2019-04-09 14:56:49,200] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 26.98766364423852, 0.868313701168563, 0.0, 1.0, 45.0, 20.276227015055333], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1029600.0000, 
sim time next is 1030800.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 26.97825309319671, 0.8525325485461238, 0.0, 1.0, 35.0, 18.893564694897677], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.7481877577663925, 0.7841775161820412, 0.0, 1.0, 0.4, 0.18893564694897677], 
reward next is 0.8111, 
noisyNet noise sample is [array([-0.55055094], dtype=float32), 1.0269216]. 
=============================================
[2019-04-09 14:56:49,885] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.05195428 0.09883814 0.11964458 0.03096161 0.14248948 0.13820735
 0.10685343 0.07263875 0.12025516 0.07182104 0.04633621], sum to 1.0000
[2019-04-09 14:56:49,893] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3041
[2019-04-09 14:56:49,918] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.8, 54.0, 155.5, 0.0, 22.5, 29.2827778571401, 1.375026358821129, 1.0, 0.0, 20.0, 0.9870705563343112], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1087200.0000, 
sim time next is 1088400.0000, 
raw observation next is [19.0, 52.33333333333334, 145.1666666666667, 0.0, 22.5, 29.35304299145218, 1.389733151900714, 1.0, 0.0, 35.0, 2.330327651099428], 
processed observation next is [1.0, 0.6086956521739131, 0.9889196675900279, 0.5233333333333334, 0.48388888888888903, 0.0, 0.375, 0.9460869159543485, 0.9632443839669046, 1.0, 0.0, 0.4, 0.023303276510994283], 
reward next is 0.9767, 
noisyNet noise sample is [array([-0.851061], dtype=float32), -0.3888023]. 
=============================================
[2019-04-09 14:56:50,351] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.04591134 0.09330948 0.101423   0.03266884 0.14041258 0.13753963
 0.11543666 0.07166987 0.12491187 0.08080299 0.05591384], sum to 1.0000
[2019-04-09 14:56:50,351] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2010
[2019-04-09 14:56:50,364] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [14.03333333333333, 76.66666666666667, 111.6666666666667, 38.99999999999999, 22.5, 28.55923832176573, 1.199368052027888, 1.0, 1.0, 20.0, 10.40364940043052], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1074000.0000, 
sim time next is 1075200.0000, 
raw observation next is [14.76666666666667, 73.33333333333334, 137.3333333333333, 35.83333333333333, 22.5, 28.61137393237869, 1.222408402018212, 1.0, 1.0, 25.0, 10.619793344122726], 
processed observation next is [1.0, 0.43478260869565216, 0.8716528162511544, 0.7333333333333334, 0.4577777777777776, 0.03959484346224677, 0.375, 0.8842811610315575, 0.907469467339404, 1.0, 1.0, 0.2, 0.10619793344122726], 
reward next is 0.8938, 
noisyNet noise sample is [array([0.0205563], dtype=float32), -0.554458]. 
=============================================
[2019-04-09 14:56:50,461] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0755806  0.10122123 0.09892463 0.02751256 0.15828922 0.13561913
 0.09778562 0.07041925 0.09463121 0.09115092 0.04886569], sum to 1.0000
[2019-04-09 14:56:50,462] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5258
[2019-04-09 14:56:50,496] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 78.0, 0.0, 0.0, 19.0, 27.49680887692395, 1.009763267087203, 0.0, 1.0, 60.0, 32.688381893662296], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1054800.0000, 
sim time next is 1056000.0000, 
raw observation next is [13.63333333333333, 78.66666666666667, 0.0, 0.0, 19.0, 27.47048863389013, 1.016804392199479, 0.0, 1.0, 40.0, 25.109147270306625], 
processed observation next is [1.0, 0.21739130434782608, 0.840258541089566, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.789207386157511, 0.8389347973998262, 0.0, 1.0, 0.5, 0.25109147270306625], 
reward next is 0.7489, 
noisyNet noise sample is [array([-0.05668603], dtype=float32), -1.2029676]. 
=============================================
[2019-04-09 14:56:50,504] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[1.5444578]
 [1.4666241]
 [1.3954374]
 [1.4690368]
 [1.5518026]], R is [[2.14247441]
 [2.79416585]
 [3.52177358]
 [4.23621225]
 [4.91622686]].
[2019-04-09 14:56:50,583] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.05901087 0.09727581 0.0848266  0.02542941 0.16675417 0.13157032
 0.1184     0.06852131 0.11119082 0.08586828 0.05115237], sum to 1.0000
[2019-04-09 14:56:50,584] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8870
[2019-04-09 14:56:50,608] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.08276927 0.09219292 0.08825798 0.04562951 0.14825962 0.13074844
 0.1131911  0.08062888 0.07826284 0.08290721 0.05715227], sum to 1.0000
[2019-04-09 14:56:50,609] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1204
[2019-04-09 14:56:50,612] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [14.76666666666667, 73.33333333333334, 137.3333333333333, 35.83333333333333, 22.5, 28.6686796422057, 1.223193492334181, 1.0, 1.0, 55.0, 11.633854063408691], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1075200.0000, 
sim time next is 1076400.0000, 
raw observation next is [15.5, 70.0, 184.0, 107.5, 22.5, 28.70398523526008, 1.249968907493331, 1.0, 1.0, 60.0, 10.88302697361329], 
processed observation next is [1.0, 0.4782608695652174, 0.8919667590027703, 0.7, 0.6133333333333333, 0.11878453038674033, 0.375, 0.8919987696050068, 0.9166563024977771, 1.0, 1.0, 0.9, 0.10883026973613291], 
reward next is 0.8912, 
noisyNet noise sample is [array([0.3667249], dtype=float32), 1.0958687]. 
=============================================
[2019-04-09 14:56:50,642] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 19.0, 27.51186597458641, 1.05958469398044, 0.0, 1.0, 55.0, 27.778136212256044], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1137600.0000, 
sim time next is 1138800.0000, 
raw observation next is [11.26666666666667, 77.0, 0.0, 0.0, 19.0, 27.52317280722534, 1.055786552910007, 0.0, 1.0, 20.0, 26.484241543265043], 
processed observation next is [0.0, 0.17391304347826086, 0.7746999076638967, 0.77, 0.0, 0.0, 0.08333333333333333, 0.793597733935445, 0.8519288509700024, 0.0, 1.0, 0.1, 0.2648424154326504], 
reward next is 0.7352, 
noisyNet noise sample is [array([-0.37769708], dtype=float32), -0.13592985]. 
=============================================
[2019-04-09 14:56:50,689] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.05642811 0.0843532  0.09649678 0.03187664 0.16262543 0.14092943
 0.11235145 0.06941484 0.10862812 0.07860342 0.05829266], sum to 1.0000
[2019-04-09 14:56:50,691] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0070
[2019-04-09 14:56:50,707] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [12.2, 83.0, 61.0, 151.5, 22.5, 27.96640327309577, 1.085679026028923, 1.0, 1.0, 35.0, 18.084960091820886], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1069200.0000, 
sim time next is 1070400.0000, 
raw observation next is [12.56666666666667, 82.0, 87.00000000000001, 206.5, 22.5, 27.98064458596283, 1.071655097454455, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.8107109879963068, 0.82, 0.29000000000000004, 0.2281767955801105, 0.375, 0.831720382163569, 0.8572183658181517, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00295584], dtype=float32), -1.2537704]. 
=============================================
[2019-04-09 14:56:50,986] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06932785 0.11370897 0.08562268 0.03638894 0.1277621  0.1458029
 0.10811829 0.06820614 0.10718916 0.09543482 0.0424382 ], sum to 1.0000
[2019-04-09 14:56:50,988] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.05591649 0.11560554 0.11697176 0.04373082 0.13032848 0.13364837
 0.10775556 0.07109856 0.10564401 0.06905548 0.05024497], sum to 1.0000
[2019-04-09 14:56:50,989] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7327
[2019-04-09 14:56:50,992] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0440
[2019-04-09 14:56:51,025] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [12.2, 66.0, 0.0, 0.0, 19.0, 27.80553747246803, 1.147044457823475, 0.0, 1.0, 25.0, 22.94003783942317], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1119600.0000, 
sim time next is 1120800.0000, 
raw observation next is [12.0, 67.66666666666667, 0.0, 0.0, 19.0, 27.78785670906421, 1.141401246502052, 0.0, 1.0, 50.0, 23.13493652199501], 
processed observation next is [1.0, 1.0, 0.7950138504155125, 0.6766666666666667, 0.0, 0.0, 0.08333333333333333, 0.8156547257553509, 0.8804670821673506, 0.0, 1.0, 0.7, 0.2313493652199501], 
reward next is 0.7687, 
noisyNet noise sample is [array([-0.33090976], dtype=float32), -0.044435862]. 
=============================================
[2019-04-09 14:56:51,034] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [11.6, 71.0, 0.0, 0.0, 19.0, 27.70550116764134, 1.051157898672465, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1123200.0000, 
sim time next is 1124400.0000, 
raw observation next is [11.23333333333333, 73.0, 0.0, 0.0, 19.0, 27.26498552614103, 0.968981546470116, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.0, 0.7737765466297323, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7720821271784191, 0.822993848823372, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6556139], dtype=float32), 0.8249061]. 
=============================================
[2019-04-09 14:56:51,307] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06087455 0.11975985 0.09659927 0.03381167 0.17489494 0.1147237
 0.110568   0.05958911 0.10487501 0.08059534 0.04370854], sum to 1.0000
[2019-04-09 14:56:51,307] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2160
[2019-04-09 14:56:51,336] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [10.16666666666667, 78.33333333333333, 0.0, 0.0, 19.0, 27.6595425640513, 1.102939994433025, 0.0, 1.0, 40.0, 24.9105353734348], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1129200.0000, 
sim time next is 1130400.0000, 
raw observation next is [10.0, 79.0, 0.0, 0.0, 19.0, 27.63433063067308, 1.097238060679806, 0.0, 1.0, 25.0, 25.418102377615867], 
processed observation next is [0.0, 0.08695652173913043, 0.739612188365651, 0.79, 0.0, 0.0, 0.08333333333333333, 0.8028608858894234, 0.865746020226602, 0.0, 1.0, 0.2, 0.25418102377615864], 
reward next is 0.7458, 
noisyNet noise sample is [array([-1.4745501], dtype=float32), 1.152622]. 
=============================================
[2019-04-09 14:56:51,458] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.05483564 0.09727081 0.09852815 0.03497965 0.14172994 0.15718807
 0.11514404 0.0750386  0.09764103 0.08384807 0.04379592], sum to 1.0000
[2019-04-09 14:56:51,463] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6886
[2019-04-09 14:56:51,506] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 67.66666666666667, 0.0, 0.0, 19.0, 27.71680585391772, 1.060525085530663, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1120800.0000, 
sim time next is 1122000.0000, 
raw observation next is [11.8, 69.33333333333333, 0.0, 0.0, 19.0, 27.5230172754252, 1.121265981237797, 0.0, 1.0, 45.0, 39.5554400259488], 
processed observation next is [1.0, 1.0, 0.7894736842105264, 0.6933333333333332, 0.0, 0.0, 0.08333333333333333, 0.7935847729521001, 0.8737553270792656, 0.0, 1.0, 0.6, 0.39555440025948796], 
reward next is 0.6044, 
noisyNet noise sample is [array([2.6058922], dtype=float32), -1.4294322]. 
=============================================
[2019-04-09 14:56:51,515] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[1.528112 ]
 [1.5621163]
 [1.6808659]
 [1.4749055]
 [1.5585858]], R is [[2.21647763]
 [3.19431281]
 [3.93536305]
 [4.63482094]
 [5.36832476]].
[2019-04-09 14:56:51,565] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.06407805 0.11426925 0.09211898 0.04347759 0.15837136 0.12111651
 0.10953374 0.07582843 0.10768037 0.07346262 0.04006303], sum to 1.0000
[2019-04-09 14:56:51,582] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7239
[2019-04-09 14:56:51,603] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [10.5, 77.0, 0.0, 0.0, 19.0, 27.6815321770331, 1.129447896735987, 0.0, 1.0, 35.0, 24.850140560033793], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1126800.0000, 
sim time next is 1128000.0000, 
raw observation next is [10.33333333333333, 77.66666666666667, 0.0, 0.0, 19.0, 27.64813806095351, 1.123608390014431, 0.0, 1.0, 50.0, 25.16375308591097], 
processed observation next is [0.0, 0.043478260869565216, 0.7488457987072946, 0.7766666666666667, 0.0, 0.0, 0.08333333333333333, 0.8040115050794592, 0.8745361300048103, 0.0, 1.0, 0.7, 0.25163753085910967], 
reward next is 0.7484, 
noisyNet noise sample is [array([0.77766955], dtype=float32), 0.4790958]. 
=============================================
[2019-04-09 14:56:51,609] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.07394108 0.09907348 0.09172563 0.03222293 0.15664239 0.12237442
 0.11859044 0.06029315 0.09155874 0.09892865 0.05464909], sum to 1.0000
[2019-04-09 14:56:51,609] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7330
[2019-04-09 14:56:51,614] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[1.3923951]
 [1.6372575]
 [1.7433856]
 [1.5611038]
 [1.8370172]], R is [[2.32446337]
 [3.05271721]
 [3.7588923 ]
 [4.43888378]
 [5.16477966]].
[2019-04-09 14:56:51,637] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [12.93333333333334, 81.0, 0.0, 0.0, 22.5, 27.37680472629363, 0.9884151123100309, 1.0, 1.0, 65.0, 29.59459921381694], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1063200.0000, 
sim time next is 1064400.0000, 
raw observation next is [12.56666666666667, 82.0, 0.0, 0.0, 22.5, 27.50082170955472, 0.9809816891689925, 1.0, 1.0, 50.0, 23.291604644468997], 
processed observation next is [1.0, 0.30434782608695654, 0.8107109879963068, 0.82, 0.0, 0.0, 0.375, 0.7917351424628934, 0.8269938963896641, 1.0, 1.0, 0.7, 0.23291604644468997], 
reward next is 0.7671, 
noisyNet noise sample is [array([-2.4762416], dtype=float32), 1.453613]. 
=============================================
[2019-04-09 14:56:51,876] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04230998 0.09616837 0.11194612 0.02743185 0.1294133  0.16939741
 0.10579687 0.06449436 0.12678984 0.06971021 0.0565417 ], sum to 1.0000
[2019-04-09 14:56:51,877] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7266
[2019-04-09 14:56:51,918] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [15.0, 57.0, 0.0, 0.0, 22.5, 28.4092508439459, 1.273552994579505, 1.0, 0.0, 30.0, 15.35823518773307], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1105200.0000, 
sim time next is 1106400.0000, 
raw observation next is [14.6, 58.00000000000001, 0.0, 0.0, 22.5, 28.34226811841717, 1.242834412466962, 0.0, 1.0, 50.0, 21.010428699139574], 
processed observation next is [1.0, 0.8260869565217391, 0.8670360110803325, 0.5800000000000001, 0.0, 0.0, 0.375, 0.8618556765347641, 0.9142781374889873, 0.0, 1.0, 0.7, 0.21010428699139574], 
reward next is 0.7899, 
noisyNet noise sample is [array([-0.7007732], dtype=float32), 0.36610144]. 
=============================================
[2019-04-09 14:56:52,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.07077967 0.11201061 0.11193645 0.0398246  0.1195167  0.12623338
 0.09423913 0.08620101 0.10191014 0.07893375 0.05841453], sum to 1.0000
[2019-04-09 14:56:52,411] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7535
[2019-04-09 14:56:52,443] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [18.63333333333333, 63.66666666666667, 161.8333333333333, 0.0, 19.0, 27.97106577117799, 1.166533446105529, 0.0, 0.0, 20.0, 19.117565982759608], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1165200.0000, 
sim time next is 1166400.0000, 
raw observation next is [18.8, 63.0, 165.5, 0.0, 19.0, 28.01604499498182, 1.174386049418765, 0.0, 0.0, 50.0, 18.24867559439549], 
processed observation next is [0.0, 0.5217391304347826, 0.9833795013850417, 0.63, 0.5516666666666666, 0.0, 0.08333333333333333, 0.8346704162484849, 0.8914620164729218, 0.0, 0.0, 0.7, 0.1824867559439549], 
reward next is 0.8175, 
noisyNet noise sample is [array([0.52533734], dtype=float32), -1.6046367]. 
=============================================
[2019-04-09 14:56:52,660] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 15138: loss 39.2545
[2019-04-09 14:56:52,662] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 15139: learning rate 0.0000
[2019-04-09 14:56:53,194] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 15366: loss 34.5933
[2019-04-09 14:56:53,195] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 15366: learning rate 0.0000
[2019-04-09 14:56:53,417] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1000, global step 15461: loss 39.9491
[2019-04-09 14:56:53,417] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1000, global step 15461: learning rate 0.0000
[2019-04-09 14:56:53,446] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.06935824 0.11180091 0.11194908 0.03910878 0.12341615 0.13938153
 0.08858743 0.07301488 0.10463206 0.07821734 0.06053369], sum to 1.0000
[2019-04-09 14:56:53,446] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9326
[2019-04-09 14:56:53,461] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15480: loss 37.6058
[2019-04-09 14:56:53,462] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15481: learning rate 0.0000
[2019-04-09 14:56:53,491] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [18.46666666666667, 64.33333333333333, 169.0, 0.0, 19.0, 28.1088740893798, 1.212768428874079, 0.0, 0.0, 40.0, 17.181715506184133], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1168800.0000, 
sim time next is 1170000.0000, 
raw observation next is [18.3, 65.0, 165.0, 0.0, 19.0, 28.12076903059243, 1.21752932142417, 0.0, 0.0, 60.0, 20.32475331532624], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.55, 0.0, 0.08333333333333333, 0.843397419216036, 0.9058431071413899, 0.0, 0.0, 0.9, 0.2032475331532624], 
reward next is 0.7968, 
noisyNet noise sample is [array([-1.1040857], dtype=float32), -0.13286485]. 
=============================================
[2019-04-09 14:56:53,501] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[0.95947194]
 [0.9136268 ]
 [1.0611991 ]
 [1.2664162 ]
 [1.1964216 ]], R is [[1.73136115]
 [2.54223037]
 [3.33703661]
 [4.09516716]
 [4.83702803]].
[2019-04-09 14:56:53,780] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.06554382 0.10300662 0.10952786 0.03948304 0.12143393 0.1273702
 0.09543117 0.0787293  0.11353446 0.08884274 0.05709682], sum to 1.0000
[2019-04-09 14:56:53,789] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4105
[2019-04-09 14:56:53,820] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 19.0, 28.05978213133682, 1.154591936429858, 0.0, 0.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1196400.0000, 
sim time next is 1197600.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 19.0, 27.98963281968166, 1.206445954743655, 0.0, 0.0, 35.0, 29.36192665300598], 
processed observation next is [0.0, 0.8695652173913043, 0.9529085872576178, 0.67, 0.0, 0.0, 0.08333333333333333, 0.8324694016401383, 0.9021486515812184, 0.0, 0.0, 0.4, 0.2936192665300598], 
reward next is 0.7064, 
noisyNet noise sample is [array([-0.7874848], dtype=float32), 0.75016505]. 
=============================================
[2019-04-09 14:56:53,916] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15684: loss 37.7905
[2019-04-09 14:56:53,918] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15685: learning rate 0.0000
[2019-04-09 14:56:53,937] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0708219  0.09568324 0.0900756  0.04077409 0.16460213 0.12288351
 0.10777923 0.07929052 0.09207799 0.07924345 0.05676826], sum to 1.0000
[2019-04-09 14:56:53,941] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1977
[2019-04-09 14:56:53,965] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15701: loss 35.6967
[2019-04-09 14:56:53,966] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15701: learning rate 0.0000
[2019-04-09 14:56:53,974] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [13.63333333333333, 81.0, 26.0, 0.1666666666666666, 19.0, 27.47370419535948, 1.064894644604107, 0.0, 1.0, 30.0, 22.66123792536964], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1153200.0000, 
sim time next is 1154400.0000, 
raw observation next is [14.56666666666667, 78.0, 39.66666666666666, 0.0, 19.0, 27.58984825078147, 1.077129331988792, 0.0, 1.0, 25.0, 20.375604504502263], 
processed observation next is [0.0, 0.34782608695652173, 0.8661126500461682, 0.78, 0.13222222222222219, 0.0, 0.08333333333333333, 0.7991540208984557, 0.8590431106629307, 0.0, 1.0, 0.2, 0.20375604504502262], 
reward next is 0.7962, 
noisyNet noise sample is [array([-0.5182938], dtype=float32), 0.10954843]. 
=============================================
[2019-04-09 14:56:54,183] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15797: loss 37.1861
[2019-04-09 14:56:54,185] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15798: learning rate 0.0000
[2019-04-09 14:56:54,302] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15852: loss 38.3260
[2019-04-09 14:56:54,304] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15852: learning rate 0.0000
[2019-04-09 14:56:54,309] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15854: loss 43.8597
[2019-04-09 14:56:54,310] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15854: learning rate 0.0000
[2019-04-09 14:56:54,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.07353965 0.11175082 0.101466   0.04269447 0.11542794 0.14155303
 0.09587474 0.08064479 0.1011519  0.07418646 0.06171018], sum to 1.0000
[2019-04-09 14:56:54,512] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6125
[2019-04-09 14:56:54,529] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.3, 65.0, 165.0, 0.0, 19.0, 28.07877903145963, 1.207190879059326, 0.0, 0.0, 45.0, 17.862291297696643], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1170000.0000, 
sim time next is 1171200.0000, 
raw observation next is [18.3, 65.0, 161.0, 0.0, 19.0, 28.10114946310667, 1.2148845884904, 0.0, 0.0, 45.0, 17.96562674153281], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.5366666666666666, 0.0, 0.08333333333333333, 0.841762455258889, 0.9049615294968, 0.0, 0.0, 0.6, 0.17965626741532809], 
reward next is 0.8203, 
noisyNet noise sample is [array([1.6296322], dtype=float32), 0.9829162]. 
=============================================
[2019-04-09 14:56:54,612] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 15988: loss 36.1564
[2019-04-09 14:56:54,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 15988: learning rate 0.0000
[2019-04-09 14:56:54,745] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 16044: loss 35.9880
[2019-04-09 14:56:54,748] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 16045: learning rate 0.0000
[2019-04-09 14:56:54,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.06476372 0.10542835 0.1131556  0.04348423 0.12415057 0.1540569
 0.09186938 0.08232984 0.08918863 0.07645665 0.05511612], sum to 1.0000
[2019-04-09 14:56:54,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4002
[2019-04-09 14:56:54,882] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 65.0, 165.0, 0.0, 19.0, 28.0809663086761, 1.21004135378097, 0.0, 0.0, 60.0, 20.931071912241528], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1170000.0000, 
sim time next is 1171200.0000, 
raw observation next is [18.3, 65.0, 161.0, 0.0, 19.0, 28.09070271540639, 1.216465262789326, 0.0, 0.0, 35.0, 17.782206512808358], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.5366666666666666, 0.0, 0.08333333333333333, 0.8408918929505326, 0.9054884209297752, 0.0, 0.0, 0.4, 0.17782206512808357], 
reward next is 0.8222, 
noisyNet noise sample is [array([0.8949472], dtype=float32), 1.9508424]. 
=============================================
[2019-04-09 14:56:55,023] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.06000869 0.08328542 0.12077995 0.03756861 0.14410064 0.12403382
 0.12158123 0.06445231 0.12169301 0.07516763 0.04732869], sum to 1.0000
[2019-04-09 14:56:55,023] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4951
[2019-04-09 14:56:55,049] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.06488928 0.10281438 0.11094119 0.03583811 0.13615662 0.16147995
 0.09190948 0.075363   0.09835497 0.07027072 0.05198233], sum to 1.0000
[2019-04-09 14:56:55,050] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8248
[2019-04-09 14:56:55,054] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 19.0, 28.21603119906644, 1.223726653363616, 0.0, 0.0, 60.0, 20.11573602369241], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1198800.0000, 
sim time next is 1200000.0000, 
raw observation next is [17.33333333333334, 69.66666666666667, 0.0, 0.0, 19.0, 28.19062351044151, 1.220673047875653, 0.0, 0.0, 40.0, 18.60600466942489], 
processed observation next is [0.0, 0.9130434782608695, 0.9427516158818101, 0.6966666666666668, 0.0, 0.0, 0.08333333333333333, 0.849218625870126, 0.9068910159585509, 0.0, 0.0, 0.5, 0.18606004669424892], 
reward next is 0.8139, 
noisyNet noise sample is [array([0.35869163], dtype=float32), 1.020917]. 
=============================================
[2019-04-09 14:56:55,058] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[1.5082426]
 [1.42133  ]
 [1.405053 ]
 [1.4175951]
 [1.46571  ]], R is [[2.18876553]
 [2.96572065]
 [3.75607014]
 [4.50985765]
 [5.14032841]].
[2019-04-09 14:56:55,058] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 16179: loss 35.7632
[2019-04-09 14:56:55,059] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 16180: learning rate 0.0000
[2019-04-09 14:56:55,085] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [15.16666666666667, 95.0, 0.0, 0.0, 19.0, 28.0564391624896, 1.210615470407985, 0.0, 0.0, 45.0, 20.15584797215087], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1226400.0000, 
sim time next is 1227600.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 28.04180357740543, 1.209728680822201, 0.0, 0.0, 55.0, 23.70128471395615], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.836816964783786, 0.9032428936074003, 0.0, 0.0, 0.8, 0.2370128471395615], 
reward next is 0.7630, 
noisyNet noise sample is [array([-1.5059488], dtype=float32), 0.80407006]. 
=============================================
[2019-04-09 14:56:55,274] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.05750214 0.11083631 0.12194411 0.03913501 0.12893581 0.13672644
 0.0928954  0.07789721 0.10740006 0.06848875 0.05823882], sum to 1.0000
[2019-04-09 14:56:55,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2981
[2019-04-09 14:56:55,316] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.46666666666667, 64.33333333333333, 24.16666666666667, 0.0, 19.0, 28.23481210096677, 1.241171568861926, 0.0, 0.0, 20.0, 16.8884082654792], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1183200.0000, 
sim time next is 1184400.0000, 
raw observation next is [18.3, 65.0, 14.5, 0.0, 19.0, 28.22408479845497, 1.239124212483985, 0.0, 0.0, 30.0, 17.125259332981255], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.65, 0.04833333333333333, 0.0, 0.08333333333333333, 0.8520070665379142, 0.9130414041613283, 0.0, 0.0, 0.3, 0.17125259332981255], 
reward next is 0.8287, 
noisyNet noise sample is [array([-0.15851136], dtype=float32), 0.10631191]. 
=============================================
[2019-04-09 14:56:55,325] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0609316  0.09372047 0.10812086 0.03755889 0.12770377 0.15504815
 0.11037765 0.07542716 0.0937137  0.07745506 0.05994261], sum to 1.0000
[2019-04-09 14:56:55,325] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2475
[2019-04-09 14:56:55,355] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [15.0, 97.33333333333333, 23.33333333333334, 0.0, 19.0, 27.99458972508809, 1.208274640388819, 0.0, 0.0, 60.0, 23.406300779237384], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1239600.0000, 
sim time next is 1240800.0000, 
raw observation next is [15.0, 98.66666666666666, 35.66666666666666, 0.0, 19.0, 27.99094309182001, 1.211529479012078, 0.0, 0.0, 25.0, 19.73469187618906], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.9866666666666666, 0.11888888888888886, 0.0, 0.08333333333333333, 0.8325785909850009, 0.9038431596706927, 0.0, 0.0, 0.2, 0.19734691876189062], 
reward next is 0.8027, 
noisyNet noise sample is [array([-0.18374057], dtype=float32), -0.88541996]. 
=============================================
[2019-04-09 14:56:55,360] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16314: loss 38.3969
[2019-04-09 14:56:55,361] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16314: learning rate 0.0000
[2019-04-09 14:56:55,595] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.06185858 0.10521146 0.09667264 0.04503162 0.12002194 0.14936231
 0.09927125 0.07851568 0.11627328 0.07554716 0.05223409], sum to 1.0000
[2019-04-09 14:56:55,596] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0593
[2019-04-09 14:56:55,614] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [15.0, 100.0, 76.0, 0.0, 19.0, 28.05740147711308, 1.234710082711457, 0.0, 0.0, 25.0, 18.139989466480255], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1245600.0000, 
sim time next is 1246800.0000, 
raw observation next is [14.8, 100.0, 77.33333333333333, 0.0, 19.0, 28.01174951677073, 1.179772893171807, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.8725761772853187, 1.0, 0.2577777777777778, 0.0, 0.08333333333333333, 0.8343124597308943, 0.893257631057269, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32663292], dtype=float32), -0.23232728]. 
=============================================
[2019-04-09 14:56:55,801] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16509: loss 37.3688
[2019-04-09 14:56:55,811] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16510: learning rate 0.0000
[2019-04-09 14:56:56,257] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.05530638 0.1067801  0.12159132 0.03607054 0.13814825 0.13118455
 0.11213633 0.06712928 0.1193535  0.07082928 0.04147045], sum to 1.0000
[2019-04-09 14:56:56,257] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5638
[2019-04-09 14:56:56,273] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.06439618 0.10079861 0.11171733 0.03589383 0.16949423 0.11993709
 0.10640308 0.07271001 0.10052457 0.07267531 0.04544979], sum to 1.0000
[2019-04-09 14:56:56,276] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9668
[2019-04-09 14:56:56,285] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [16.1, 80.0, 0.0, 0.0, 19.0, 28.13564553865866, 1.216026432056227, 0.0, 0.0, 35.0, 18.891584935759724], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1213200.0000, 
sim time next is 1214400.0000, 
raw observation next is [16.1, 81.0, 0.0, 0.0, 19.0, 28.1012654487394, 1.214613572949848, 0.0, 0.0, 40.0, 19.800672452034476], 
processed observation next is [0.0, 0.043478260869565216, 0.9085872576177286, 0.81, 0.0, 0.0, 0.08333333333333333, 0.8417721207282834, 0.9048711909832826, 0.0, 0.0, 0.5, 0.19800672452034476], 
reward next is 0.8020, 
noisyNet noise sample is [array([0.5061206], dtype=float32), 0.5233847]. 
=============================================
[2019-04-09 14:56:56,309] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06753157 0.09906883 0.0924279  0.02933941 0.16428654 0.1143796
 0.09688067 0.07720704 0.09716246 0.1041856  0.05753035], sum to 1.0000
[2019-04-09 14:56:56,311] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5598
[2019-04-09 14:56:56,327] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.9, 98.66666666666667, 0.0, 0.0, 19.0, 27.9051476792645, 1.158799940642916, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1272000.0000, 
sim time next is 1273200.0000, 
raw observation next is [9.600000000000001, 97.33333333333334, 0.0, 0.0, 19.0, 27.69014620653737, 1.217629982384543, 0.0, 1.0, 65.0, 40.76133123455894], 
processed observation next is [0.0, 0.7391304347826086, 0.7285318559556788, 0.9733333333333334, 0.0, 0.0, 0.08333333333333333, 0.8075121838781142, 0.9058766607948477, 0.0, 1.0, 1.0, 0.4076133123455894], 
reward next is 0.5924, 
noisyNet noise sample is [array([-0.5536074], dtype=float32), 0.45381397]. 
=============================================
[2019-04-09 14:56:56,341] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.9, 92.0, 0.0, 0.0, 19.0, 27.26599127768595, 1.027681611685582, 0.0, 1.0, 50.0, 32.66848246435916], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1305600.0000, 
sim time next is 1306800.0000, 
raw observation next is [2.7, 92.0, 0.0, 0.0, 19.0, 27.32859911027819, 1.024671493232201, 0.0, 1.0, 40.0, 30.158913683437376], 
processed observation next is [1.0, 0.13043478260869565, 0.5373961218836566, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7773832591898492, 0.8415571644107337, 0.0, 1.0, 0.5, 0.30158913683437377], 
reward next is 0.6984, 
noisyNet noise sample is [array([-0.3032335], dtype=float32), -1.7881244]. 
=============================================
[2019-04-09 14:56:56,385] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.07083424 0.09270919 0.10184404 0.03597572 0.1486413  0.13834886
 0.10486627 0.07940604 0.09510043 0.07881662 0.05345714], sum to 1.0000
[2019-04-09 14:56:56,390] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5887
[2019-04-09 14:56:56,423] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 100.0, 74.66666666666667, 0.0, 19.0, 27.99287190064819, 1.229074859688791, 0.0, 0.0, 55.0, 30.894647779152717], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1244400.0000, 
sim time next is 1245600.0000, 
raw observation next is [15.0, 100.0, 76.0, 0.0, 19.0, 28.02854962220692, 1.235092727964176, 0.0, 0.0, 45.0, 18.01246639640856], 
processed observation next is [0.0, 0.43478260869565216, 0.8781163434903049, 1.0, 0.25333333333333335, 0.0, 0.08333333333333333, 0.8357124685172433, 0.9116975759880587, 0.0, 0.0, 0.6, 0.18012466396408558], 
reward next is 0.8199, 
noisyNet noise sample is [array([-0.7276078], dtype=float32), 0.5690489]. 
=============================================
[2019-04-09 14:56:56,473] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.06384432 0.09462933 0.10488616 0.03984824 0.15347888 0.14108868
 0.09614548 0.07922895 0.08991852 0.08771649 0.04921496], sum to 1.0000
[2019-04-09 14:56:56,473] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4227
[2019-04-09 14:56:56,501] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 28.02216016287228, 1.216867448974078, 0.0, 0.0, 60.0, 31.5453828766291], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1237200.0000, 
sim time next is 1238400.0000, 
raw observation next is [15.0, 96.0, 14.0, 0.0, 19.0, 28.05272175218704, 1.214570970156855, 0.0, 0.0, 35.0, 18.627972230505918], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.96, 0.04666666666666667, 0.0, 0.08333333333333333, 0.8377268126822534, 0.904856990052285, 0.0, 0.0, 0.4, 0.18627972230505918], 
reward next is 0.8137, 
noisyNet noise sample is [array([-0.47499186], dtype=float32), 0.840371]. 
=============================================
[2019-04-09 14:56:56,727] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.06055643 0.10114203 0.11182155 0.03904905 0.1629777  0.12660334
 0.10282969 0.07669366 0.10371424 0.06227641 0.05233591], sum to 1.0000
[2019-04-09 14:56:56,727] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5187
[2019-04-09 14:56:56,762] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [13.8, 100.0, 98.0, 0.0, 19.0, 28.08090518908219, 1.260147297558717, 0.0, 1.0, 50.0, 18.096193736849337], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1256400.0000, 
sim time next is 1257600.0000, 
raw observation next is [13.8, 100.0, 96.0, 0.0, 19.0, 28.08279471225503, 1.262288818155696, 0.0, 1.0, 40.0, 18.093881696351666], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.32, 0.0, 0.08333333333333333, 0.8402328926879191, 0.920762939385232, 0.0, 1.0, 0.5, 0.18093881696351666], 
reward next is 0.8191, 
noisyNet noise sample is [array([-0.2382714], dtype=float32), -1.4834478]. 
=============================================
[2019-04-09 14:56:57,311] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1000, global step 17141: loss 33.1974
[2019-04-09 14:56:57,318] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1000, global step 17145: learning rate 0.0000
[2019-04-09 14:56:57,598] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.04454711 0.06764504 0.10410719 0.02503632 0.17234232 0.1457726
 0.11403825 0.06838837 0.09431344 0.10295952 0.06084985], sum to 1.0000
[2019-04-09 14:56:57,599] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4869
[2019-04-09 14:56:57,638] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 92.0, 54.5, 0.0, 22.5, 27.68452796595345, 0.9883234257808317, 1.0, 1.0, 35.0, 25.6195281869279], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1330800.0000, 
sim time next is 1332000.0000, 
raw observation next is [0.5, 92.0, 73.5, 0.0, 22.5, 27.70654384198861, 1.002846575312407, 1.0, 1.0, 40.0, 27.916148018902696], 
processed observation next is [1.0, 0.43478260869565216, 0.4764542936288089, 0.92, 0.245, 0.0, 0.375, 0.8088786534990507, 0.8342821917708023, 1.0, 1.0, 0.5, 0.279161480189027], 
reward next is 0.7208, 
noisyNet noise sample is [array([0.25697783], dtype=float32), 0.168453]. 
=============================================
[2019-04-09 14:56:57,642] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[2.3507981]
 [2.037376 ]
 [2.1649847]
 [2.1277342]
 [2.004939 ]], R is [[2.81404257]
 [3.52970695]
 [4.18290901]
 [4.80525255]
 [5.1387825 ]].
[2019-04-09 14:56:57,659] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.05420101 0.10353401 0.12061677 0.04032379 0.13521415 0.11212992
 0.11405849 0.07956634 0.11566357 0.07550213 0.04918988], sum to 1.0000
[2019-04-09 14:56:57,662] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2418
[2019-04-09 14:56:57,668] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.04311901 0.0700315  0.08827972 0.02688969 0.1829935  0.14771968
 0.12479705 0.0609632  0.12003256 0.08540668 0.04976735], sum to 1.0000
[2019-04-09 14:56:57,668] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0208
[2019-04-09 14:56:57,678] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [0.5, 92.0, 73.5, 0.0, 22.5, 27.70654384198861, 1.002846575312407, 1.0, 1.0, 40.0, 27.916148018902696], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1332000.0000, 
sim time next is 1333200.0000, 
raw observation next is [0.7000000000000001, 92.0, 92.5, 0.0, 22.5, 27.70455591466253, 0.9409099602655004, 1.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4819944598337951, 0.92, 0.30833333333333335, 0.0, 0.375, 0.8087129928885443, 0.8136366534218334, 1.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00314888], dtype=float32), -1.4835627]. 
=============================================
[2019-04-09 14:56:57,707] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [16.1, 78.66666666666667, 0.0, 0.0, 19.0, 28.12003477238637, 1.219623390916629, 0.0, 0.0, 40.0, 19.4010957696947], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1210800.0000, 
sim time next is 1212000.0000, 
raw observation next is [16.1, 79.33333333333333, 0.0, 0.0, 19.0, 28.14400313484815, 1.227589932380953, 0.0, 0.0, 25.0, 18.66995711569745], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7933333333333333, 0.0, 0.0, 0.08333333333333333, 0.845333594570679, 0.9091966441269843, 0.0, 0.0, 0.2, 0.1866995711569745], 
reward next is 0.8133, 
noisyNet noise sample is [array([-0.20416403], dtype=float32), -0.24863684]. 
=============================================
[2019-04-09 14:56:57,719] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[1.3440479]
 [1.4989041]
 [1.5141175]
 [1.5032724]
 [1.5358288]], R is [[2.07410097]
 [2.85934901]
 [3.63715076]
 [4.41348219]
 [5.17877197]].
[2019-04-09 14:56:57,756] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.06666598 0.10677242 0.09788025 0.03055418 0.16780384 0.11167444
 0.11320962 0.06343155 0.09890074 0.10209908 0.04100787], sum to 1.0000
[2019-04-09 14:56:57,758] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7986
[2019-04-09 14:56:57,795] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [3.633333333333334, 92.66666666666667, 0.0, 0.0, 19.0, 26.53965203672424, 0.9194896239448336, 0.0, 1.0, 65.0, 52.83550786880808], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1300800.0000, 
sim time next is 1302000.0000, 
raw observation next is [3.466666666666667, 92.33333333333333, 0.0, 0.0, 19.0, 26.62153619132154, 0.9676520220040635, 0.0, 1.0, 55.0, 48.60787451171768], 
processed observation next is [1.0, 0.043478260869565216, 0.5586334256694367, 0.9233333333333333, 0.0, 0.0, 0.08333333333333333, 0.718461349276795, 0.8225506740013545, 0.0, 1.0, 0.8, 0.4860787451171768], 
reward next is 0.5139, 
noisyNet noise sample is [array([0.2397846], dtype=float32), -0.09063086]. 
=============================================
[2019-04-09 14:56:57,812] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[1.844209 ]
 [1.6687865]
 [1.533459 ]
 [1.6342165]
 [1.7390361]], R is [[2.24725819]
 [2.69643044]
 [3.50754786]
 [4.29306412]
 [5.02532816]].
[2019-04-09 14:56:57,837] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 17355: loss 41.0267
[2019-04-09 14:56:57,840] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 17358: learning rate 0.0000
[2019-04-09 14:56:57,860] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.06058818 0.09693877 0.10119361 0.03614763 0.17783706 0.11647512
 0.11124203 0.06355025 0.10617519 0.08639836 0.04345378], sum to 1.0000
[2019-04-09 14:56:57,861] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3650
[2019-04-09 14:56:57,904] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.466666666666667, 92.33333333333333, 0.0, 0.0, 19.0, 26.62153619132154, 0.9676520220040635, 0.0, 1.0, 55.0, 48.60787451171768], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1302000.0000, 
sim time next is 1303200.0000, 
raw observation next is [3.3, 92.0, 0.0, 0.0, 19.0, 26.82728162443357, 0.9776716395967077, 0.0, 1.0, 35.0, 33.436930284987476], 
processed observation next is [1.0, 0.08695652173913043, 0.554016620498615, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7356068020361309, 0.8258905465322358, 0.0, 1.0, 0.4, 0.33436930284987476], 
reward next is 0.6656, 
noisyNet noise sample is [array([1.3591834], dtype=float32), 0.56985676]. 
=============================================
[2019-04-09 14:56:58,188] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.05893741 0.08842194 0.09903342 0.03126242 0.17375867 0.1510338
 0.10423387 0.07187408 0.08258782 0.08779866 0.05105792], sum to 1.0000
[2019-04-09 14:56:58,189] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7742
[2019-04-09 14:56:58,236] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 19.0, 27.15126369039876, 0.9312078552418873, 0.0, 1.0, 35.0, 29.96077165423085], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1321200.0000, 
sim time next is 1322400.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 26.9829291461235, 0.9188361799752126, 1.0, 1.0, 50.0, 36.28051625128579], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.748577428843625, 0.8062787266584043, 1.0, 1.0, 0.7, 0.3628051625128579], 
reward next is 0.6372, 
noisyNet noise sample is [array([-1.5123354], dtype=float32), 1.5362772]. 
=============================================
[2019-04-09 14:56:58,559] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.05491108 0.10153135 0.10546213 0.02295845 0.1696494  0.12617752
 0.12523022 0.05928739 0.10662428 0.08172862 0.04643963], sum to 1.0000
[2019-04-09 14:56:58,560] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7603
[2019-04-09 14:56:58,594] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 19.0, 26.54064909956855, 0.8648058831303865, 0.0, 1.0, 40.0, 25.30937779110853], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1371600.0000, 
sim time next is 1372800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 19.0, 26.45591448547024, 0.8696066715481336, 0.0, 1.0, 50.0, 37.777835591715856], 
processed observation next is [1.0, 0.9130434782608695, 0.4764542936288089, 0.96, 0.0, 0.0, 0.08333333333333333, 0.7046595404558534, 0.7898688905160446, 0.0, 1.0, 0.7, 0.3777783559171586], 
reward next is 0.6222, 
noisyNet noise sample is [array([1.225425], dtype=float32), -0.5174607]. 
=============================================
[2019-04-09 14:56:58,765] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.04124704 0.0720924  0.10672569 0.02287542 0.19846037 0.1268093
 0.13418685 0.05211969 0.10276271 0.08414247 0.05857807], sum to 1.0000
[2019-04-09 14:56:58,772] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4523
[2019-04-09 14:56:58,806] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 27.25127560693253, 0.9755268623274889, 1.0, 1.0, 55.0, 31.033026421894775], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1364400.0000, 
sim time next is 1365600.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 27.26505520040207, 0.9695386806623842, 0.0, 1.0, 40.0, 31.206298552867015], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.7720879333668392, 0.8231795602207947, 0.0, 1.0, 0.5, 0.3120629855286701], 
reward next is 0.6879, 
noisyNet noise sample is [array([-0.44435975], dtype=float32), 0.7455236]. 
=============================================
[2019-04-09 14:56:59,857] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.06628201 0.09201351 0.09629696 0.03197809 0.17880873 0.13819377
 0.10945269 0.06379548 0.08048365 0.0939163  0.04877881], sum to 1.0000
[2019-04-09 14:56:59,858] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8320
[2019-04-09 14:56:59,888] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 26.44866320768116, 0.7755572115544386, 0.0, 1.0, 35.0, 30.26809349523299], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1389600.0000, 
sim time next is 1390800.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 26.34520154081707, 0.7776401854294699, 0.0, 1.0, 55.0, 42.72132296361519], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.6954334617347557, 0.7592133951431567, 0.0, 1.0, 0.8, 0.4272132296361519], 
reward next is 0.5728, 
noisyNet noise sample is [array([-0.33877945], dtype=float32), 0.7053184]. 
=============================================
[2019-04-09 14:57:00,029] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.05685746 0.10464709 0.11059917 0.02429637 0.1611842  0.11295684
 0.12856777 0.06518723 0.10961614 0.07843772 0.04764996], sum to 1.0000
[2019-04-09 14:57:00,030] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1138
[2019-04-09 14:57:00,072] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 19.0, 26.4241419553029, 0.8414030099105342, 0.0, 1.0, 60.0, 46.135086943211036], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1372800.0000, 
sim time next is 1374000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 19.0, 26.66243946012284, 0.8588758972533919, 0.0, 1.0, 20.0, 34.7507194172954], 
processed observation next is [1.0, 0.9130434782608695, 0.4764542936288089, 0.96, 0.0, 0.0, 0.08333333333333333, 0.7218699550102367, 0.7862919657511306, 0.0, 1.0, 0.1, 0.347507194172954], 
reward next is 0.6525, 
noisyNet noise sample is [array([0.6195007], dtype=float32), 1.5902474]. 
=============================================
[2019-04-09 14:57:00,081] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[2.3709455]
 [2.5800512]
 [2.3233728]
 [2.4254937]
 [2.4591699]], R is [[2.81234813]
 [3.32287383]
 [3.70212626]
 [4.42509031]
 [5.08152866]].
[2019-04-09 14:57:00,645] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03866852 0.08228703 0.10968006 0.02232178 0.18883683 0.1323831
 0.10908538 0.06796847 0.10739614 0.08297049 0.05840218], sum to 1.0000
[2019-04-09 14:57:00,646] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5842
[2019-04-09 14:57:00,682] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 27.07109388040315, 0.9762906038370294, 1.0, 1.0, 30.0, 31.31289842062288], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1360800.0000, 
sim time next is 1362000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 27.41727210231763, 0.9995521331364522, 1.0, 1.0, 45.0, 27.48647380674955], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.7847726751931358, 0.8331840443788173, 1.0, 1.0, 0.6, 0.2748647380674955], 
reward next is 0.7251, 
noisyNet noise sample is [array([-1.1164589], dtype=float32), 0.22476932]. 
=============================================
[2019-04-09 14:57:00,709] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[2.419711 ]
 [2.4811215]
 [2.289268 ]
 [2.4737554]
 [2.3908036]], R is [[3.07650185]
 [3.73260784]
 [4.13239479]
 [5.09107065]
 [5.78670883]].
[2019-04-09 14:57:01,041] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.06114892 0.08620408 0.09932304 0.02840697 0.16824439 0.1623307
 0.10922839 0.06625405 0.06236573 0.09926965 0.05722411], sum to 1.0000
[2019-04-09 14:57:01,059] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8088
[2019-04-09 14:57:01,094] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 26.76330327484976, 0.796255866005286, 0.0, 1.0, 60.0, 41.835095333680414], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1398000.0000, 
sim time next is 1399200.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 26.78401510091164, 0.7977588187717232, 0.0, 1.0, 55.0, 35.946991254920704], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7320012584093032, 0.7659196062572411, 0.0, 1.0, 0.8, 0.359469912549207], 
reward next is 0.6405, 
noisyNet noise sample is [array([-0.11198409], dtype=float32), -2.8657122]. 
=============================================
[2019-04-09 14:57:01,188] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.05109014 0.07112047 0.08589765 0.02564289 0.17069131 0.1608116
 0.11667883 0.07412851 0.0949926  0.09158322 0.05736275], sum to 1.0000
[2019-04-09 14:57:01,191] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2023
[2019-04-09 14:57:01,237] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.6, 100.0, 22.66666666666666, 0.0, 22.5, 26.06380729778468, 0.6382907457741491, 1.0, 1.0, 50.0, 35.871548343001876], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1413600.0000, 
sim time next is 1414800.0000, 
raw observation next is [-0.6, 100.0, 32.0, 0.0, 22.5, 26.41038760889819, 0.683184903500091, 1.0, 1.0, 25.0, 27.43318440321064], 
processed observation next is [1.0, 0.391304347826087, 0.44598337950138506, 1.0, 0.10666666666666667, 0.0, 0.375, 0.7008656340748493, 0.727728301166697, 1.0, 1.0, 0.2, 0.27433184403210636], 
reward next is 0.7257, 
noisyNet noise sample is [array([0.7540746], dtype=float32), -0.8170619]. 
=============================================
[2019-04-09 14:57:01,791] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04795857 0.08105423 0.10303771 0.02320903 0.18377213 0.13140167
 0.11493899 0.06296703 0.11581898 0.08222285 0.05361882], sum to 1.0000
[2019-04-09 14:57:01,792] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8856
[2019-04-09 14:57:01,840] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 26.89606338795179, 0.8351657408273181, 0.0, 1.0, 35.0, 29.08713694710389], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1453200.0000, 
sim time next is 1454400.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 26.75888481336693, 0.8323987035276531, 1.0, 1.0, 60.0, 41.129204694582384], 
processed observation next is [1.0, 0.8695652173913043, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.7299070677805775, 0.7774662345092177, 1.0, 1.0, 0.9, 0.41129204694582383], 
reward next is 0.5887, 
noisyNet noise sample is [array([0.08444454], dtype=float32), 0.2995204]. 
=============================================
[2019-04-09 14:57:02,325] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.04657205 0.09324419 0.11024523 0.02060689 0.17417942 0.10953545
 0.10746557 0.06702809 0.1228389  0.09498157 0.05330258], sum to 1.0000
[2019-04-09 14:57:02,332] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7300
[2019-04-09 14:57:02,370] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.266666666666667, 91.0, 0.0, 0.0, 19.0, 26.09665930917959, 0.7445721331611668, 0.0, 1.0, 60.0, 52.024991232058], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1460400.0000, 
sim time next is 1461600.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 19.0, 26.34493871183299, 0.7689581736579675, 0.0, 1.0, 55.0, 38.36114515793419], 
processed observation next is [1.0, 0.9565217391304348, 0.49307479224376743, 0.92, 0.0, 0.0, 0.08333333333333333, 0.695411559319416, 0.7563193912193226, 0.0, 1.0, 0.8, 0.3836114515793419], 
reward next is 0.6164, 
noisyNet noise sample is [array([0.39844266], dtype=float32), -0.35773876]. 
=============================================
[2019-04-09 14:57:02,737] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.0520771  0.09351356 0.10408352 0.02765895 0.16526662 0.12945426
 0.11859515 0.06773144 0.09299709 0.10295379 0.04566853], sum to 1.0000
[2019-04-09 14:57:02,740] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3088
[2019-04-09 14:57:02,799] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 19.0, 26.45463771781744, 0.7300265831926454, 0.0, 1.0, 40.0, 32.491627521635415], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1470000.0000, 
sim time next is 1471200.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 19.0, 26.54311736063357, 0.7220728151433882, 0.0, 1.0, 35.0, 30.71196958615772], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7119264467194641, 0.7406909383811294, 0.0, 1.0, 0.4, 0.3071196958615772], 
reward next is 0.6929, 
noisyNet noise sample is [array([-1.0566838], dtype=float32), 1.5188531]. 
=============================================
[2019-04-09 14:57:02,803] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.04643184 0.09710898 0.10478587 0.02387768 0.17717604 0.12222324
 0.12549701 0.06528226 0.10255818 0.09522437 0.03983457], sum to 1.0000
[2019-04-09 14:57:02,803] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9370
[2019-04-09 14:57:02,865] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 19.0, 26.54311736063357, 0.7220728151433882, 0.0, 1.0, 35.0, 30.71196958615772], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1471200.0000, 
sim time next is 1472400.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 19.0, 26.53194524408275, 0.739879633387951, 0.0, 1.0, 60.0, 44.25040185546466], 
processed observation next is [1.0, 0.043478260869565216, 0.5069252077562327, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7109954370068957, 0.7466265444626504, 0.0, 1.0, 0.9, 0.4425040185546466], 
reward next is 0.5575, 
noisyNet noise sample is [array([-1.0566838], dtype=float32), 1.5188531]. 
=============================================
[2019-04-09 14:57:03,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04214113 0.08404379 0.09330542 0.02081666 0.18022057 0.14484571
 0.13570571 0.06053605 0.10880629 0.08292892 0.04664975], sum to 1.0000
[2019-04-09 14:57:03,230] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8868
[2019-04-09 14:57:03,265] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 19.0, 26.80501547833472, 0.838472379903035, 0.0, 1.0, 60.0, 35.87571816226911], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1461600.0000, 
sim time next is 1462800.0000, 
raw observation next is [1.266666666666667, 92.0, 0.0, 0.0, 19.0, 26.78171826016522, 0.8001935624591061, 0.0, 1.0, 60.0, 44.63379404621935], 
processed observation next is [1.0, 0.9565217391304348, 0.4976915974145891, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7318098550137684, 0.7667311874863687, 0.0, 1.0, 0.9, 0.4463379404621935], 
reward next is 0.5537, 
noisyNet noise sample is [array([-1.4232025], dtype=float32), -1.0674922]. 
=============================================
[2019-04-09 14:57:03,431] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.03821323 0.09442666 0.10620128 0.025072   0.18341883 0.13782671
 0.10608725 0.06036919 0.10914849 0.09627347 0.04296294], sum to 1.0000
[2019-04-09 14:57:03,431] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3263
[2019-04-09 14:57:03,448] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03529784 0.08964957 0.09827434 0.01779363 0.23167615 0.10591751
 0.13435891 0.05345776 0.10123265 0.08747055 0.04487107], sum to 1.0000
[2019-04-09 14:57:03,448] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9153
[2019-04-09 14:57:03,473] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.06040735 0.09330337 0.09700024 0.02846118 0.1976908  0.1271795
 0.0951207  0.07348908 0.08234386 0.08867346 0.05633054], sum to 1.0000
[2019-04-09 14:57:03,474] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4139
[2019-04-09 14:57:03,490] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 19.0, 26.51814217220792, 0.7534262843145916, 0.0, 1.0, 20.0, 30.198384826575833], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1470000.0000, 
sim time next is 1471200.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 19.0, 26.57607571685761, 0.7405617184979557, 0.0, 1.0, 35.0, 28.790959807763755], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7146729764048008, 0.7468539061659852, 0.0, 1.0, 0.4, 0.28790959807763755], 
reward next is 0.7121, 
noisyNet noise sample is [array([0.39185524], dtype=float32), 0.58696616]. 
=============================================
[2019-04-09 14:57:03,497] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 26.39635286803476, 0.7779093443367647, 1.0, 1.0, 60.0, 49.09190381504585], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1454400.0000, 
sim time next is 1455600.0000, 
raw observation next is [1.266666666666667, 91.0, 0.0, 0.0, 19.0, 26.51694529622399, 0.7950196208513419, 0.0, 1.0, 60.0, 42.800432210907005], 
processed observation next is [1.0, 0.8695652173913043, 0.4976915974145891, 0.91, 0.0, 0.0, 0.08333333333333333, 0.7097454413519992, 0.7650065402837806, 0.0, 1.0, 0.9, 0.42800432210907], 
reward next is 0.5720, 
noisyNet noise sample is [array([1.870216], dtype=float32), 0.02035767]. 
=============================================
[2019-04-09 14:57:03,501] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.2, 94.66666666666667, 0.0, 0.0, 19.0, 26.31792363992579, 0.693991318864703, 0.0, 1.0, 45.0, 26.151763124267795], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1480800.0000, 
sim time next is 1482000.0000, 
raw observation next is [2.2, 95.33333333333334, 0.0, 0.0, 19.0, 26.27207524744277, 0.6620362153710825, 0.0, 1.0, 35.0, 23.829161599846998], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.6893396039535643, 0.7206787384570275, 0.0, 1.0, 0.4, 0.23829161599846999], 
reward next is 0.7617, 
noisyNet noise sample is [array([1.1474411], dtype=float32), -1.0054562]. 
=============================================
[2019-04-09 14:57:03,521] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[2.1366673]
 [2.1706243]
 [2.2429094]
 [2.2529058]
 [2.3579028]], R is [[2.91000366]
 [3.6193862 ]
 [4.28656721]
 [4.85583162]
 [5.55228376]].
[2019-04-09 14:57:03,852] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.03495574 0.07672167 0.11861292 0.02076613 0.18115516 0.11954595
 0.1342974  0.05941669 0.10757364 0.08541831 0.06153636], sum to 1.0000
[2019-04-09 14:57:03,852] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0951
[2019-04-09 14:57:03,896] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 26.68997594896668, 0.8023961296732042, 1.0, 1.0, 35.0, 29.45350493592095], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1452000.0000, 
sim time next is 1453200.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 26.53535623571498, 0.7782325615763549, 0.0, 1.0, 20.0, 28.788135297905974], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.7112796863095815, 0.7594108538587849, 0.0, 1.0, 0.1, 0.28788135297905976], 
reward next is 0.7121, 
noisyNet noise sample is [array([1.5668881], dtype=float32), 0.09441059]. 
=============================================
[2019-04-09 14:57:04,045] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.06704645 0.09000906 0.09949234 0.02398792 0.1592588  0.12757036
 0.11431904 0.0695608  0.0893185  0.10666774 0.05276906], sum to 1.0000
[2019-04-09 14:57:04,045] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0096
[2019-04-09 14:57:04,055] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [1.833333333333334, 97.33333333333333, 0.0, 0.0, 19.0, 25.84967492635268, 0.4724058638067793, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1491600.0000, 
sim time next is 1492800.0000, 
raw observation next is [1.466666666666667, 98.66666666666666, 0.0, 0.0, 19.0, 25.34998267719226, 0.3829959399055088, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.5032317636195753, 0.9866666666666666, 0.0, 0.0, 0.08333333333333333, 0.6124985564326882, 0.6276653133018363, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3951948], dtype=float32), 0.027701145]. 
=============================================
[2019-04-09 14:57:04,150] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.04070586 0.08586197 0.09466681 0.0237016  0.18295088 0.15224312
 0.11574879 0.06559113 0.0997207  0.08655582 0.0522534 ], sum to 1.0000
[2019-04-09 14:57:04,150] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3442
[2019-04-09 14:57:04,194] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.433333333333333, 100.0, 22.83333333333333, 0.0, 22.5, 25.34477495756086, 0.4088745635038754, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1500000.0000, 
sim time next is 1501200.0000, 
raw observation next is [1.6, 100.0, 32.5, 0.0, 22.5, 25.39054260326871, 0.4534971074869235, 1.0, 1.0, 35.0, 22.292224811328936], 
processed observation next is [1.0, 0.391304347826087, 0.5069252077562327, 1.0, 0.10833333333333334, 0.0, 0.375, 0.6158785502723925, 0.6511657024956411, 1.0, 1.0, 0.4, 0.22292224811328937], 
reward next is 0.7771, 
noisyNet noise sample is [array([0.8900376], dtype=float32), 0.13345392]. 
=============================================
[2019-04-09 14:57:04,483] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.03335521 0.08051609 0.08031466 0.02171567 0.18834126 0.17370613
 0.11123978 0.06217694 0.11518328 0.08334074 0.05011022], sum to 1.0000
[2019-04-09 14:57:04,487] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9054
[2019-04-09 14:57:04,493] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.04806913 0.09948477 0.1190093  0.02048274 0.18014145 0.10601342
 0.12018637 0.04881538 0.11144245 0.0981427  0.0482123 ], sum to 1.0000
[2019-04-09 14:57:04,495] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8416
[2019-04-09 14:57:04,509] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04759801 0.07974836 0.11185505 0.02629763 0.19722152 0.13689499
 0.08511377 0.07590991 0.09558198 0.08973239 0.05404636], sum to 1.0000
[2019-04-09 14:57:04,512] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5996
[2019-04-09 14:57:04,517] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [11.6, 52.0, 76.0, 570.5, 22.5, 28.57596809930942, 1.187540917916038, 1.0, 1.0, 40.0, 6.975270462862702], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1522800.0000, 
sim time next is 1524000.0000, 
raw observation next is [11.8, 51.33333333333334, 76.66666666666667, 508.8333333333334, 22.5, 28.62809147248799, 1.189576312547507, 1.0, 1.0, 60.0, 7.220128073899384], 
processed observation next is [1.0, 0.6521739130434783, 0.7894736842105264, 0.5133333333333334, 0.2555555555555556, 0.5622467771639044, 0.375, 0.8856742893739993, 0.8965254375158356, 1.0, 1.0, 0.9, 0.07220128073899384], 
reward next is 0.9278, 
noisyNet noise sample is [array([0.1448576], dtype=float32), -2.3513713]. 
=============================================
[2019-04-09 14:57:04,527] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[2.5850275]
 [2.5034456]
 [2.5583484]
 [2.6059043]
 [2.5354772]], R is [[3.53796196]
 [4.43282986]
 [5.30967522]
 [6.21796274]
 [7.0796175 ]].
[2019-04-09 14:57:04,534] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.433333333333333, 90.0, 0.0, 0.0, 19.0, 26.65058520137532, 0.8065901931119036, 0.0, 1.0, 60.0, 35.018928636764564], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1456800.0000, 
sim time next is 1458000.0000, 
raw observation next is [1.6, 89.0, 0.0, 0.0, 19.0, 26.69221228502704, 0.8122607865138901, 0.0, 1.0, 55.0, 34.245052219166524], 
processed observation next is [1.0, 0.9130434782608695, 0.5069252077562327, 0.89, 0.0, 0.0, 0.08333333333333333, 0.7243510237522534, 0.7707535955046301, 0.0, 1.0, 0.8, 0.3424505221916652], 
reward next is 0.6575, 
noisyNet noise sample is [array([-0.83179814], dtype=float32), 0.48364675]. 
=============================================
[2019-04-09 14:57:04,537] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[2.5746744]
 [2.5603402]
 [2.809622 ]
 [2.5873759]
 [2.5712018]], R is [[3.34985113]
 [3.9661634 ]
 [4.51228189]
 [5.03197098]
 [5.38415575]].
[2019-04-09 14:57:04,557] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 22.5, 25.90156856599492, 0.5931683772820069, 1.0, 1.0, 30.0, 30.78920856897039], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1496400.0000, 
sim time next is 1497600.0000, 
raw observation next is [1.1, 100.0, 9.0, 0.0, 22.5, 25.87079358218211, 0.5762848407480005, 1.0, 1.0, 40.0, 27.41668447055256], 
processed observation next is [1.0, 0.34782608695652173, 0.49307479224376743, 1.0, 0.03, 0.0, 0.375, 0.6558994651818425, 0.6920949469160002, 1.0, 1.0, 0.5, 0.2741668447055256], 
reward next is 0.7258, 
noisyNet noise sample is [array([-0.07097368], dtype=float32), -0.2927188]. 
=============================================
[2019-04-09 14:57:04,634] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0406488  0.07634228 0.09741054 0.02218436 0.19284292 0.13334003
 0.11330329 0.07242784 0.10331094 0.0939398  0.05424934], sum to 1.0000
[2019-04-09 14:57:04,634] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6647
[2019-04-09 14:57:04,670] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 9.0, 0.0, 22.5, 25.87412595160993, 0.5805727593883881, 1.0, 1.0, 25.0, 22.581217102348162], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1411200.0000, 
sim time next is 1412400.0000, 
raw observation next is [-0.6, 100.0, 15.0, 0.0, 22.5, 25.71193539814884, 0.6201048085642338, 1.0, 1.0, 45.0, 28.44254728011134], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.05, 0.0, 0.375, 0.6426612831790699, 0.7067016028547446, 1.0, 1.0, 0.6, 0.2844254728011134], 
reward next is 0.7156, 
noisyNet noise sample is [array([-0.9522317], dtype=float32), 0.0391262]. 
=============================================
[2019-04-09 14:57:04,994] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.05278293 0.10336328 0.10303374 0.02349495 0.18133692 0.12358592
 0.11488973 0.08207361 0.08259846 0.08587839 0.04696198], sum to 1.0000
[2019-04-09 14:57:04,995] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0630
[2019-04-09 14:57:05,016] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.2, 95.33333333333334, 0.0, 0.0, 19.0, 26.08112389158071, 0.6295343943978627, 0.0, 1.0, 50.0, 32.37687850504597], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1482000.0000, 
sim time next is 1483200.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 19.0, 26.03199538106612, 0.6008563039345228, 0.0, 1.0, 20.0, 29.28886263539792], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.08333333333333333, 0.6693329484221767, 0.7002854346448409, 0.0, 1.0, 0.1, 0.29288862635397916], 
reward next is 0.7071, 
noisyNet noise sample is [array([-1.2464029], dtype=float32), -0.449699]. 
=============================================
[2019-04-09 14:57:05,182] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-09 14:57:05,183] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:57:05,183] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:57:05,183] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:57:05,183] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:57:05,184] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:57:05,186] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run3
[2019-04-09 14:57:05,188] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:57:05,205] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run3
[2019-04-09 14:57:05,222] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run3
[2019-04-09 14:57:47,978] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.00530392], dtype=float32), 0.015837248]
[2019-04-09 14:57:47,978] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [1.733333333333333, 92.0, 55.16666666666667, 0.0, 22.5, 27.42153343120484, 0.9348998647756313, 1.0, 1.0, 25.0, 19.612864145121982]
[2019-04-09 14:57:47,978] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:57:47,979] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.04122695 0.07739364 0.10257838 0.0237565  0.2026087  0.13118735
 0.11833971 0.06992274 0.10162153 0.0774483  0.05391622], sampled 0.5241569744313288
[2019-04-09 14:57:56,721] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.00530392], dtype=float32), 0.015837248]
[2019-04-09 14:57:56,721] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-6.9, 78.33333333333334, 0.0, 0.0, 22.5, 23.32731086464655, -0.05488560945518822, 1.0, 1.0, 65.0, 65.31493419947074]
[2019-04-09 14:57:56,721] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:57:56,722] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.05759732 0.07565392 0.08341007 0.03362291 0.19219963 0.12521489
 0.12867075 0.07093704 0.07699726 0.09984801 0.05584823], sampled 0.0046148310744269505
[2019-04-09 14:58:32,564] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2918.9383 131561.8295 1164.3589
[2019-04-09 14:58:32,584] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:58:32,584] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:58:32,584] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:58:32,742] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:58:32,742] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:58:32,742] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:58:44,256] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2829.2511 141884.0963 941.8936
[2019-04-09 14:58:44,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:58:44,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:58:44,276] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:58:44,385] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:58:44,385] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:58:44,385] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:58:45,836] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2791.5203 145597.9893 741.5371
[2019-04-09 14:58:45,855] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:58:45,855] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:58:45,855] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 14:58:45,964] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:58:45,964] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:58:45,964] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 14:58:46,857] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 20000, evaluation results [20000.0, 2829.251064139739, 141884.09628506526, 941.8935540947938, 2918.938283404883, 131561.82951412883, 1164.3589435243289, 2791.5202501149533, 145597.9892535166, 741.5371111295359]
[2019-04-09 14:58:47,372] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.04520423 0.1075009  0.09068749 0.02636282 0.19216093 0.13719358
 0.10458574 0.06712484 0.09101876 0.08918206 0.04897871], sum to 1.0000
[2019-04-09 14:58:47,373] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5446
[2019-04-09 14:58:47,423] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [4.4, 86.0, 0.0, 0.0, 19.0, 26.79078437379125, 0.8846930547896709, 0.0, 1.0, 20.0, 42.03572833350607], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1566000.0000, 
sim time next is 1567200.0000, 
raw observation next is [4.466666666666667, 85.66666666666667, 0.0, 0.0, 19.0, 26.91569570303501, 0.8775233608232901, 0.0, 1.0, 40.0, 23.478885844911787], 
processed observation next is [1.0, 0.13043478260869565, 0.5863342566943676, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.7429746419195841, 0.7925077869410967, 0.0, 1.0, 0.5, 0.23478885844911787], 
reward next is 0.7652, 
noisyNet noise sample is [array([0.85406786], dtype=float32), -0.9085749]. 
=============================================
[2019-04-09 14:58:47,444] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.04637545 0.10547887 0.08811631 0.02688615 0.18737723 0.1460599
 0.09755279 0.07075164 0.08571827 0.09229439 0.05338898], sum to 1.0000
[2019-04-09 14:58:47,445] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7834
[2019-04-09 14:58:47,467] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [4.466666666666667, 85.66666666666667, 0.0, 0.0, 19.0, 26.91569570303501, 0.8775233608232901, 0.0, 1.0, 40.0, 23.478885844911787], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1567200.0000, 
sim time next is 1568400.0000, 
raw observation next is [4.533333333333333, 85.33333333333334, 0.0, 0.0, 19.0, 26.88272253244835, 0.9102825729523416, 0.0, 1.0, 55.0, 43.62308921061067], 
processed observation next is [1.0, 0.13043478260869565, 0.5881809787626964, 0.8533333333333334, 0.0, 0.0, 0.08333333333333333, 0.7402268777040293, 0.8034275243174472, 0.0, 1.0, 0.8, 0.4362308921061067], 
reward next is 0.5638, 
noisyNet noise sample is [array([0.85406786], dtype=float32), -0.9085749]. 
=============================================
[2019-04-09 14:58:47,736] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.03814346 0.09162543 0.10047346 0.01842116 0.18362905 0.14473116
 0.10963685 0.07471904 0.11061118 0.07546747 0.0525417 ], sum to 1.0000
[2019-04-09 14:58:47,736] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5353
[2019-04-09 14:58:47,790] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.333333333333334, 86.33333333333334, 98.0, 701.3333333333334, 22.5, 27.27533421651484, 0.7253129732788959, 1.0, 1.0, 50.0, 33.918093551991134], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1513200.0000, 
sim time next is 1514400.0000, 
raw observation next is [6.266666666666667, 79.66666666666667, 97.5, 700.1666666666667, 22.5, 27.00076810087772, 0.9388408091726493, 1.0, 1.0, 40.0, 22.413973264580015], 
processed observation next is [1.0, 0.5217391304347826, 0.6361957525392429, 0.7966666666666667, 0.325, 0.7736648250460406, 0.375, 0.7500640084064768, 0.8129469363908831, 1.0, 1.0, 0.5, 0.22413973264580014], 
reward next is 0.7759, 
noisyNet noise sample is [array([-0.01308108], dtype=float32), 0.13697287]. 
=============================================
[2019-04-09 14:58:48,081] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.03771534 0.09015222 0.09959701 0.02374923 0.19262724 0.14082113
 0.1280176  0.04727452 0.11569846 0.07638964 0.04795755], sum to 1.0000
[2019-04-09 14:58:48,082] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2421
[2019-04-09 14:58:48,119] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.2, 73.0, 0.0, 0.0, 22.5, 27.59412368363374, 1.084037871012678, 0.0, 1.0, 35.0, 33.5432972612568], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1540800.0000, 
sim time next is 1542000.0000, 
raw observation next is [7.366666666666667, 73.33333333333334, 0.0, 0.0, 19.0, 27.61860448444074, 1.075520603766564, 0.0, 1.0, 30.0, 21.989541172653297], 
processed observation next is [1.0, 0.8695652173913043, 0.6666666666666667, 0.7333333333333334, 0.0, 0.0, 0.08333333333333333, 0.8015503737033951, 0.858506867922188, 0.0, 1.0, 0.3, 0.21989541172653296], 
reward next is 0.7801, 
noisyNet noise sample is [array([0.05124817], dtype=float32), -0.25341976]. 
=============================================
[2019-04-09 14:58:48,127] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[2.8967617]
 [2.9006588]
 [2.9777324]
 [2.8933349]
 [3.0242026]], R is [[3.56295538]
 [4.1918931 ]
 [5.14997435]
 [5.91183233]
 [6.68761396]].
[2019-04-09 14:58:48,269] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04941639 0.08641539 0.0980031  0.02288182 0.18283565 0.15516095
 0.09245855 0.07562211 0.07087871 0.09232422 0.07400303], sum to 1.0000
[2019-04-09 14:58:48,269] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7106
[2019-04-09 14:58:48,297] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [4.9, 82.66666666666667, 0.0, 0.0, 19.0, 27.08778783538266, 0.8920951210016055, 0.0, 1.0, 45.0, 29.43112988923523], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1575600.0000, 
sim time next is 1576800.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 19.0, 27.21691753454878, 0.9029475649672195, 0.0, 1.0, 50.0, 29.557694421508415], 
processed observation next is [1.0, 0.2608695652173913, 0.6011080332409973, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7680764612123984, 0.8009825216557398, 0.0, 1.0, 0.7, 0.2955769442150842], 
reward next is 0.7044, 
noisyNet noise sample is [array([0.06586627], dtype=float32), 0.9215298]. 
=============================================
[2019-04-09 14:58:48,435] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03981876 0.09683201 0.09736639 0.02810058 0.1636616  0.14563142
 0.1523442  0.06161438 0.09332643 0.08192407 0.03938016], sum to 1.0000
[2019-04-09 14:58:48,435] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4783
[2019-04-09 14:58:48,479] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 19.0, 26.9962232558225, 0.9174655653279581, 0.0, 1.0, 50.0, 33.63703694398279], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1558800.0000, 
sim time next is 1560000.0000, 
raw observation next is [5.0, 81.0, 0.0, 0.0, 19.0, 27.05713398791828, 0.9238331032075607, 0.0, 1.0, 40.0, 26.601062492479052], 
processed observation next is [1.0, 0.043478260869565216, 0.6011080332409973, 0.81, 0.0, 0.0, 0.08333333333333333, 0.7547611656598567, 0.8079443677358537, 0.0, 1.0, 0.5, 0.2660106249247905], 
reward next is 0.7340, 
noisyNet noise sample is [array([0.37753683], dtype=float32), 2.1376278]. 
=============================================
[2019-04-09 14:58:48,485] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[2.6076314]
 [2.508774 ]
 [2.7200487]
 [2.6597812]
 [2.7746341]], R is [[3.38599586]
 [4.01576567]
 [4.71095276]
 [5.31989813]
 [6.01576757]].
[2019-04-09 14:58:48,846] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04917055 0.09133959 0.09002886 0.02178056 0.19712923 0.13941018
 0.12003694 0.05515123 0.08841992 0.0974975  0.05003547], sum to 1.0000
[2019-04-09 14:58:48,847] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0552
[2019-04-09 14:58:48,895] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.0, 80.0, 0.0, 0.0, 19.0, 27.40495454486186, 0.9544782608535906, 0.0, 1.0, 45.0, 26.68275666842105], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1561200.0000, 
sim time next is 1562400.0000, 
raw observation next is [5.0, 79.0, 0.0, 0.0, 19.0, 27.24505090131472, 0.9302553172437508, 0.0, 1.0, 20.0, 26.365455478784813], 
processed observation next is [1.0, 0.08695652173913043, 0.6011080332409973, 0.79, 0.0, 0.0, 0.08333333333333333, 0.7704209084428933, 0.8100851057479169, 0.0, 1.0, 0.1, 0.26365455478784816], 
reward next is 0.7363, 
noisyNet noise sample is [array([0.58009595], dtype=float32), 0.87021035]. 
=============================================
[2019-04-09 14:58:48,965] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03371298 0.10854147 0.10688601 0.0211138  0.16613135 0.16667235
 0.10436164 0.06769882 0.10771406 0.07449365 0.04267393], sum to 1.0000
[2019-04-09 14:58:48,965] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1574
[2019-04-09 14:58:48,995] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [7.333333333333334, 74.66666666666667, 0.0, 0.0, 19.0, 27.76154667573116, 1.091214560047315, 0.0, 1.0, 20.0, 20.382861810091036], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1545600.0000, 
sim time next is 1546800.0000, 
raw observation next is [6.966666666666667, 75.33333333333333, 0.0, 0.0, 19.0, 27.69054764158803, 1.084556130411141, 0.0, 1.0, 25.0, 22.826811124702164], 
processed observation next is [1.0, 0.9130434782608695, 0.6555863342566944, 0.7533333333333333, 0.0, 0.0, 0.08333333333333333, 0.8075456367990025, 0.8615187101370471, 0.0, 1.0, 0.2, 0.22826811124702165], 
reward next is 0.7717, 
noisyNet noise sample is [array([0.40636587], dtype=float32), 0.35476235]. 
=============================================
[2019-04-09 14:58:49,092] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.04435513 0.09886762 0.10515746 0.02225236 0.15881759 0.16264305
 0.11173192 0.07175559 0.08238494 0.09025183 0.05178239], sum to 1.0000
[2019-04-09 14:58:49,092] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6090
[2019-04-09 14:58:49,143] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 94.66666666666667, 0.0, 0.0, 19.0, 26.20390397708714, 0.7034451331586952, 0.0, 1.0, 40.0, 32.33685917283604], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1480800.0000, 
sim time next is 1482000.0000, 
raw observation next is [2.2, 95.33333333333334, 0.0, 0.0, 19.0, 26.31277882293781, 0.6832054311420014, 0.0, 1.0, 45.0, 26.93536421110645], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.6927315685781507, 0.7277351437140004, 0.0, 1.0, 0.6, 0.2693536421110645], 
reward next is 0.7306, 
noisyNet noise sample is [array([0.96763515], dtype=float32), -0.0055846963]. 
=============================================
[2019-04-09 14:58:49,160] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[2.4597058]
 [2.416275 ]
 [2.4817333]
 [2.4621453]
 [2.4926858]], R is [[3.11177826]
 [3.75729203]
 [4.32102299]
 [4.99333143]
 [5.46134567]].
[2019-04-09 14:58:49,858] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03297587 0.0964628  0.09719065 0.01865643 0.18628584 0.14283629
 0.12593731 0.06324623 0.12592591 0.06772636 0.04275635], sum to 1.0000
[2019-04-09 14:58:49,861] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3639
[2019-04-09 14:58:49,903] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 49.0, 111.5, 0.0, 22.5, 28.85464908131188, 1.171533924450262, 1.0, 1.0, 40.0, 33.531056672709454], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1609200.0000, 
sim time next is 1610400.0000, 
raw observation next is [13.63333333333333, 49.66666666666667, 89.16666666666666, 0.0, 22.5, 28.03918271286277, 1.200879999917633, 1.0, 1.0, 35.0, 7.077612227192019], 
processed observation next is [1.0, 0.6521739130434783, 0.840258541089566, 0.4966666666666667, 0.29722222222222217, 0.0, 0.375, 0.836598559405231, 0.9002933333058776, 1.0, 1.0, 0.4, 0.07077612227192019], 
reward next is 0.9292, 
noisyNet noise sample is [array([-0.5005836], dtype=float32), -1.5664771]. 
=============================================
[2019-04-09 14:58:50,112] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.02646602 0.08546265 0.09463612 0.02176799 0.17297065 0.16477348
 0.13097051 0.0767033  0.11504266 0.06154505 0.04966164], sum to 1.0000
[2019-04-09 14:58:50,116] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1763
[2019-04-09 14:58:50,185] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [12.2, 54.0, 25.5, 18.5, 22.5, 28.98144220671899, 1.266588551545626, 1.0, 1.0, 35.0, 10.90852813152461], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1616400.0000, 
sim time next is 1617600.0000, 
raw observation next is [11.63333333333333, 56.33333333333334, 0.0, 0.0, 22.5, 28.37001537745633, 1.224354495330017, 1.0, 1.0, 25.0, 36.925667283729155], 
processed observation next is [1.0, 0.7391304347826086, 0.7848568790397045, 0.5633333333333335, 0.0, 0.0, 0.375, 0.8641679481213608, 0.9081181651100056, 1.0, 1.0, 0.2, 0.3692566728372915], 
reward next is 0.6307, 
noisyNet noise sample is [array([-0.09701348], dtype=float32), 0.6565858]. 
=============================================
[2019-04-09 14:58:50,306] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.051674   0.09976346 0.08310485 0.02326742 0.17970738 0.15715908
 0.09896453 0.07058944 0.09218565 0.09563392 0.04795032], sum to 1.0000
[2019-04-09 14:58:50,307] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0027
[2019-04-09 14:58:50,333] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [4.800000000000001, 83.33333333333334, 0.0, 0.0, 19.0, 26.23547763932316, 0.7246012624347801, 0.0, 1.0, 40.0, 26.4453018436205], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1574400.0000, 
sim time next is 1575600.0000, 
raw observation next is [4.9, 82.66666666666667, 0.0, 0.0, 19.0, 26.20182917711462, 0.6329870059478783, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5983379501385043, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.6834857647595518, 0.7109956686492928, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20156823], dtype=float32), -0.7656946]. 
=============================================
[2019-04-09 14:58:51,182] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.0313321  0.0615351  0.08846746 0.01899011 0.19852781 0.17163782
 0.12172879 0.06127781 0.10400153 0.086877   0.05562441], sum to 1.0000
[2019-04-09 14:58:51,186] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8136
[2019-04-09 14:58:51,225] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [2.566666666666667, 92.0, 33.83333333333333, 0.0, 22.5, 27.57393156366432, 0.9953743481907279, 1.0, 1.0, 55.0, 31.53467720549378], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1672800.0000, 
sim time next is 1674000.0000, 
raw observation next is [2.2, 92.0, 41.5, 0.0, 22.5, 27.64442860561491, 1.004125018846246, 1.0, 1.0, 55.0, 30.752705105805383], 
processed observation next is [1.0, 0.391304347826087, 0.5235457063711911, 0.92, 0.13833333333333334, 0.0, 0.375, 0.8037023838012424, 0.8347083396154154, 1.0, 1.0, 0.8, 0.30752705105805384], 
reward next is 0.6925, 
noisyNet noise sample is [array([-0.66380167], dtype=float32), 0.47531107]. 
=============================================
[2019-04-09 14:58:51,237] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[3.183706 ]
 [3.2598314]
 [2.9108572]
 [3.116269 ]
 [3.2151382]], R is [[3.80680895]
 [4.45339394]
 [5.10519648]
 [5.73803568]
 [6.3859458 ]].
[2019-04-09 14:58:51,320] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.03705578 0.098209   0.09180715 0.0203663  0.21041833 0.12773295
 0.1243733  0.05748251 0.09673463 0.08927279 0.0465473 ], sum to 1.0000
[2019-04-09 14:58:51,320] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2458
[2019-04-09 14:58:51,340] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.02884653 0.08166008 0.09348029 0.01456263 0.21281156 0.15260185
 0.12058145 0.06025586 0.10573266 0.07845066 0.05101641], sum to 1.0000
[2019-04-09 14:58:51,340] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3521
[2019-04-09 14:58:51,351] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 88.33333333333334, 0.0, 0.0, 19.0, 27.23655358721425, 1.022329623537328, 0.0, 1.0, 60.0, 44.78667792646237], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 1642800.0000, 
sim time next is 1644000.0000, 
raw observation next is [6.8, 90.66666666666667, 0.0, 0.0, 19.0, 27.47660755977434, 1.021053963880117, 0.0, 1.0, 30.0, 25.326685870103635], 
processed observation next is [1.0, 0.0, 0.6509695290858727, 0.9066666666666667, 0.0, 0.0, 0.08333333333333333, 0.7897172966478617, 0.8403513212933724, 0.0, 1.0, 0.3, 0.2532668587010363], 
reward next is 0.7467, 
noisyNet noise sample is [array([0.47191405], dtype=float32), -1.2253073]. 
=============================================
[2019-04-09 14:58:51,357] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[3.0255399]
 [2.9840522]
 [3.312022 ]
 [3.105148 ]
 [3.1925797]], R is [[3.66600513]
 [4.1814785 ]
 [4.91133928]
 [5.60097885]
 [6.26665163]].
[2019-04-09 14:58:51,362] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02829822 0.06359094 0.10279917 0.01464933 0.19798622 0.14207318
 0.12793897 0.05034659 0.13332872 0.09017808 0.04881056], sum to 1.0000
[2019-04-09 14:58:51,364] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8023
[2019-04-09 14:58:51,375] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.06666666666667, 58.66666666666667, 0.0, 0.0, 22.5, 28.23192396599426, 1.195900193564237, 1.0, 1.0, 40.0, 11.955095078287254], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1618800.0000, 
sim time next is 1620000.0000, 
raw observation next is [10.5, 61.0, 0.0, 0.0, 22.5, 28.34079091335024, 1.205352547546118, 1.0, 1.0, 35.0, 13.557019563204786], 
processed observation next is [1.0, 0.782608695652174, 0.7534626038781165, 0.61, 0.0, 0.0, 0.375, 0.86173257611252, 0.9017841825153727, 1.0, 1.0, 0.4, 0.13557019563204786], 
reward next is 0.8644, 
noisyNet noise sample is [array([0.9272829], dtype=float32), 1.0031302]. 
=============================================
[2019-04-09 14:58:51,390] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[3.279748 ]
 [3.2223332]
 [3.0176392]
 [3.1910748]
 [3.0699978]], R is [[3.99739265]
 [4.83786774]
 [5.4157896 ]
 [6.24780464]
 [7.09538746]].
[2019-04-09 14:58:51,396] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [13.8, 49.0, 150.1666666666667, 0.0, 22.5, 28.79826764379607, 1.239141636690844, 1.0, 1.0, 60.0, 12.28218947561156], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1606800.0000, 
sim time next is 1608000.0000, 
raw observation next is [13.8, 49.0, 133.8333333333333, 0.0, 22.5, 28.86430251868214, 1.244820144324382, 1.0, 1.0, 55.0, 9.230200111197114], 
processed observation next is [1.0, 0.6086956521739131, 0.844875346260388, 0.49, 0.44611111111111096, 0.0, 0.375, 0.9053585432235117, 0.9149400481081273, 1.0, 1.0, 0.8, 0.09230200111197114], 
reward next is 0.9077, 
noisyNet noise sample is [array([-0.65077364], dtype=float32), 1.5503105]. 
=============================================
[2019-04-09 14:58:51,409] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[3.2481039]
 [3.2622883]
 [3.2466006]
 [3.1780472]
 [3.2155824]], R is [[4.15265846]
 [4.98831034]
 [5.83941746]
 [6.69167709]
 [7.508564  ]].
[2019-04-09 14:58:51,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.03387676 0.08673051 0.10680358 0.01749999 0.16813223 0.13523985
 0.12998521 0.05761147 0.14508547 0.07844939 0.04058554], sum to 1.0000
[2019-04-09 14:58:51,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1763
[2019-04-09 14:58:51,506] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [13.3, 51.0, 64.0, 18.5, 22.5, 28.37768642191983, 1.225314289079752, 1.0, 1.0, 40.0, 26.708969165112958], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1612800.0000, 
sim time next is 1614000.0000, 
raw observation next is [12.93333333333334, 52.00000000000001, 54.66666666666667, 30.83333333333334, 22.5, 28.46287877436406, 1.20518109028495, 1.0, 1.0, 25.0, 7.625966413158613], 
processed observation next is [1.0, 0.6956521739130435, 0.8208679593721148, 0.52, 0.18222222222222223, 0.03406998158379374, 0.375, 0.8719065645303384, 0.9017270300949832, 1.0, 1.0, 0.2, 0.07625966413158614], 
reward next is 0.9237, 
noisyNet noise sample is [array([-0.16543396], dtype=float32), 1.9465871]. 
=============================================
[2019-04-09 14:58:51,511] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[3.200221 ]
 [3.2166579]
 [3.104886 ]
 [3.163076 ]
 [3.0614676]], R is [[4.08261347]
 [4.7746973 ]
 [5.62437725]
 [6.45375729]
 [7.27529383]].
[2019-04-09 14:58:51,531] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.03700522 0.10192914 0.08844165 0.02048664 0.15804414 0.13129556
 0.1275205  0.07535338 0.10223345 0.10618039 0.05150999], sum to 1.0000
[2019-04-09 14:58:51,532] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7377
[2019-04-09 14:58:51,575] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [6.6, 76.0, 0.0, 0.0, 19.0, 27.17297804129037, 1.032662841696436, 0.0, 1.0, 60.0, 51.262210860696825], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1548000.0000, 
sim time next is 1549200.0000, 
raw observation next is [6.233333333333333, 78.0, 0.0, 0.0, 19.0, 27.34158804156805, 1.004186584227889, 0.0, 1.0, 50.0, 33.975086143163345], 
processed observation next is [1.0, 0.9565217391304348, 0.6352723915050786, 0.78, 0.0, 0.0, 0.08333333333333333, 0.7784656701306707, 0.8347288614092964, 0.0, 1.0, 0.7, 0.33975086143163347], 
reward next is 0.6602, 
noisyNet noise sample is [array([-0.4954462], dtype=float32), -1.0725989]. 
=============================================
[2019-04-09 14:58:52,356] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04325065 0.06631011 0.10319078 0.01828475 0.18703702 0.15552495
 0.10816946 0.07054967 0.09017344 0.10151354 0.05599567], sum to 1.0000
[2019-04-09 14:58:52,359] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9413
[2019-04-09 14:58:52,366] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04720841 0.08120931 0.09668935 0.01694537 0.20025058 0.15723625
 0.10226139 0.06850806 0.08326585 0.08618708 0.06023835], sum to 1.0000
[2019-04-09 14:58:52,366] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1093
[2019-04-09 14:58:52,398] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.333333333333334, 95.33333333333334, 0.0, 0.0, 19.0, 27.2427640893324, 0.9586264391445938, 0.0, 1.0, 55.0, 34.4786589906531], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1664400.0000, 
sim time next is 1665600.0000, 
raw observation next is [5.166666666666666, 93.66666666666666, 0.0, 0.0, 19.0, 27.27520085028146, 0.9586715379887106, 0.0, 1.0, 20.0, 30.8801112182053], 
processed observation next is [1.0, 0.2608695652173913, 0.6057248384118191, 0.9366666666666665, 0.0, 0.0, 0.08333333333333333, 0.7729334041901218, 0.8195571793295703, 0.0, 1.0, 0.1, 0.30880111218205303], 
reward next is 0.6912, 
noisyNet noise sample is [array([-0.94567883], dtype=float32), -0.2714262]. 
=============================================
[2019-04-09 14:58:52,407] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [3.3, 92.0, 15.5, 0.0, 22.5, 27.31119183519597, 0.9420235273889842, 1.0, 1.0, 35.0, 25.007835062998375], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1670400.0000, 
sim time next is 1671600.0000, 
raw observation next is [2.933333333333334, 92.0, 25.16666666666667, 0.0, 22.5, 27.23656225220395, 0.9755810235886839, 1.0, 1.0, 60.0, 43.59564023626149], 
processed observation next is [1.0, 0.34782608695652173, 0.543859649122807, 0.92, 0.0838888888888889, 0.0, 0.375, 0.7697135210169957, 0.8251936745295613, 1.0, 1.0, 0.9, 0.4359564023626149], 
reward next is 0.5640, 
noisyNet noise sample is [array([-0.23634171], dtype=float32), -0.40674436]. 
=============================================
[2019-04-09 14:58:52,502] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.0427887  0.07337673 0.10358287 0.01741515 0.17716134 0.1558298
 0.12386517 0.0690848  0.09697568 0.08722465 0.05269509], sum to 1.0000
[2019-04-09 14:58:52,504] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9501
[2019-04-09 14:58:52,516] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.037356   0.07462902 0.08673849 0.01631316 0.21134603 0.13961522
 0.15152828 0.05066782 0.11209015 0.06943519 0.05028065], sum to 1.0000
[2019-04-09 14:58:52,521] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6684
[2019-04-09 14:58:52,534] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.533333333333333, 80.0, 31.0, 30.0, 22.5, 26.74628152277974, 0.8566616162669067, 1.0, 1.0, 40.0, 30.777138538438628], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1585200.0000, 
sim time next is 1586400.0000, 
raw observation next is [6.066666666666666, 78.0, 50.0, 51.83333333333333, 22.5, 27.36351360605774, 0.9147259450950457, 1.0, 1.0, 65.0, 33.96953812420868], 
processed observation next is [1.0, 0.34782608695652173, 0.6306555863342568, 0.78, 0.16666666666666666, 0.057274401473296495, 0.375, 0.7802928005048116, 0.8049086483650152, 1.0, 1.0, 1.0, 0.3396953812420868], 
reward next is 0.6603, 
noisyNet noise sample is [array([0.07809953], dtype=float32), -1.1410465]. 
=============================================
[2019-04-09 14:58:52,554] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 86.66666666666667, 105.8333333333333, 0.0, 22.5, 27.88669609377536, 1.05012588736569, 1.0, 1.0, 20.0, 28.491482795584574], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1687200.0000, 
sim time next is 1688400.0000, 
raw observation next is [1.1, 88.0, 103.5, 0.0, 22.5, 27.82967664731973, 1.059050759161538, 1.0, 1.0, 45.0, 28.10482039903969], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.345, 0.0, 0.375, 0.8191397206099774, 0.8530169197205127, 1.0, 1.0, 0.6, 0.28104820399039687], 
reward next is 0.7190, 
noisyNet noise sample is [array([1.8353655], dtype=float32), -0.39256254]. 
=============================================
[2019-04-09 14:58:52,573] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.04403974 0.08270127 0.13784137 0.01838887 0.21309157 0.16189906
 0.08485281 0.0522726  0.07928336 0.08389285 0.04173646], sum to 1.0000
[2019-04-09 14:58:52,574] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2373
[2019-04-09 14:58:52,613] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [7.2, 86.0, 0.0, 0.0, 19.0, 27.16802115827798, 0.9845223761452543, 0.0, 1.0, 55.0, 47.40727359592516], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1641600.0000, 
sim time next is 1642800.0000, 
raw observation next is [7.0, 88.33333333333334, 0.0, 0.0, 19.0, 27.23643558657902, 1.000946108227329, 0.0, 1.0, 25.0, 30.18236556853511], 
processed observation next is [1.0, 0.0, 0.6565096952908588, 0.8833333333333334, 0.0, 0.0, 0.08333333333333333, 0.7697029655482517, 0.8336487027424431, 0.0, 1.0, 0.2, 0.30182365568535113], 
reward next is 0.6982, 
noisyNet noise sample is [array([-1.6110094], dtype=float32), -0.63919383]. 
=============================================
[2019-04-09 14:58:52,632] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.0223234  0.08794979 0.08764717 0.01443714 0.24507846 0.15320797
 0.11262446 0.04809482 0.09553877 0.08759919 0.04549894], sum to 1.0000
[2019-04-09 14:58:52,632] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2941
[2019-04-09 14:58:52,642] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03305727 0.07782579 0.09749714 0.02234575 0.19641279 0.13190253
 0.16222669 0.04812623 0.10207448 0.0902035  0.03832776], sum to 1.0000
[2019-04-09 14:58:52,642] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8227
[2019-04-09 14:58:52,667] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [7.0, 88.33333333333334, 0.0, 0.0, 19.0, 27.44389006481509, 1.036476155061518, 0.0, 1.0, 60.0, 34.74552363511853], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1642800.0000, 
sim time next is 1644000.0000, 
raw observation next is [6.8, 90.66666666666667, 0.0, 0.0, 19.0, 27.54009979533806, 1.027299544148658, 0.0, 1.0, 55.0, 26.04141343129539], 
processed observation next is [1.0, 0.0, 0.6509695290858727, 0.9066666666666667, 0.0, 0.0, 0.08333333333333333, 0.7950083162781718, 0.8424331813828859, 0.0, 1.0, 0.8, 0.2604141343129539], 
reward next is 0.7396, 
noisyNet noise sample is [array([0.02373852], dtype=float32), -0.012329354]. 
=============================================
[2019-04-09 14:58:52,672] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[3.1138434]
 [3.1683848]
 [3.104546 ]
 [3.26486  ]
 [3.259708 ]], R is [[3.64292908]
 [4.25904465]
 [4.93297052]
 [5.57949781]
 [6.18709421]].
[2019-04-09 14:58:52,680] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 81.0, 41.5, 0.0, 22.5, 27.6550690741697, 0.9791232915055558, 1.0, 1.0, 40.0, 22.75126511878342], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1699200.0000, 
sim time next is 1700400.0000, 
raw observation next is [1.433333333333334, 83.33333333333334, 33.83333333333333, 0.0, 22.5, 27.94959303478758, 0.9972772186134001, 1.0, 1.0, 35.0, 24.521522179349045], 
processed observation next is [1.0, 0.6956521739130435, 0.502308402585411, 0.8333333333333335, 0.11277777777777777, 0.0, 0.375, 0.8291327528989649, 0.8324257395378001, 1.0, 1.0, 0.4, 0.24521522179349045], 
reward next is 0.7548, 
noisyNet noise sample is [array([-1.4553292], dtype=float32), -0.29459876]. 
=============================================
[2019-04-09 14:58:53,102] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.03441162 0.07585002 0.10842725 0.02310512 0.1663502  0.16568945
 0.11968906 0.07012945 0.09373154 0.09547757 0.0471388 ], sum to 1.0000
[2019-04-09 14:58:53,104] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8089
[2019-04-09 14:58:53,157] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 26.58472340876241, 0.7473105707824441, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1719600.0000, 
sim time next is 1720800.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 26.22911500820426, 0.825746178486372, 0.0, 1.0, 55.0, 61.28179920206148], 
processed observation next is [1.0, 0.9565217391304348, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.6857595840170218, 0.775248726162124, 0.0, 1.0, 0.8, 0.6128179920206148], 
reward next is 0.3872, 
noisyNet noise sample is [array([0.39231512], dtype=float32), 1.3680478]. 
=============================================
[2019-04-09 14:58:53,271] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.02754505 0.0706581  0.08849626 0.01850507 0.23278517 0.13230576
 0.15357283 0.05044882 0.09483159 0.08793256 0.04291876], sum to 1.0000
[2019-04-09 14:58:53,281] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9253
[2019-04-09 14:58:53,321] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [1.6, 81.0, 41.5, 0.0, 22.5, 27.15195497129583, 0.9418885017571959, 1.0, 1.0, 25.0, 32.08253861589296], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1699200.0000, 
sim time next is 1700400.0000, 
raw observation next is [1.433333333333334, 83.33333333333334, 33.83333333333333, 0.0, 22.5, 27.26129621075025, 0.9535615526554668, 1.0, 1.0, 60.0, 31.646603722852966], 
processed observation next is [1.0, 0.6956521739130435, 0.502308402585411, 0.8333333333333335, 0.11277777777777777, 0.0, 0.375, 0.7717746842291874, 0.8178538508851556, 1.0, 1.0, 0.9, 0.3164660372285297], 
reward next is 0.6835, 
noisyNet noise sample is [array([-0.9095682], dtype=float32), 0.17554075]. 
=============================================
[2019-04-09 14:58:53,612] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.03190405 0.07331848 0.09273899 0.01678948 0.22131617 0.13714552
 0.13866065 0.05024722 0.0966094  0.09288199 0.04838805], sum to 1.0000
[2019-04-09 14:58:53,614] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1209
[2019-04-09 14:58:53,640] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.7000000000000001, 90.66666666666666, 0.0, 0.0, 19.0, 26.83946471746824, 0.890326509758714, 0.0, 1.0, 45.0, 35.12667140250184], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1716000.0000, 
sim time next is 1717200.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 26.87124387091903, 0.8869409271121406, 0.0, 1.0, 25.0, 33.78488909794608], 
processed observation next is [1.0, 0.9130434782608695, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7392703225765859, 0.7956469757040469, 0.0, 1.0, 0.2, 0.3378488909794608], 
reward next is 0.6622, 
noisyNet noise sample is [array([1.2873846], dtype=float32), -1.3200884]. 
=============================================
[2019-04-09 14:58:53,646] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04051825 0.07216188 0.09826349 0.01647181 0.21036445 0.1399003
 0.12909713 0.05826028 0.09404174 0.09333843 0.04758227], sum to 1.0000
[2019-04-09 14:58:53,647] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2895
[2019-04-09 14:58:53,680] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.266666666666667, 85.66666666666667, 25.16666666666667, 0.0, 22.5, 26.0222293851492, 0.867949471750788, 1.0, 1.0, 35.0, 29.763468334415162], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1701600.0000, 
sim time next is 1702800.0000, 
raw observation next is [1.1, 88.0, 15.5, 0.0, 22.5, 27.58019677938078, 0.9628680023477517, 1.0, 1.0, 35.0, 25.10542582729301], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.88, 0.051666666666666666, 0.0, 0.375, 0.7983497316150651, 0.8209560007825839, 1.0, 1.0, 0.4, 0.2510542582729301], 
reward next is 0.7489, 
noisyNet noise sample is [array([-0.37910166], dtype=float32), -0.9172844]. 
=============================================
[2019-04-09 14:58:53,702] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.0388414  0.0995314  0.10517693 0.02530173 0.23927854 0.10025784
 0.11451146 0.05773969 0.10574903 0.07789161 0.03572039], sum to 1.0000
[2019-04-09 14:58:53,707] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3132
[2019-04-09 14:58:53,723] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 25.20525271367465, 0.5225004753522576, 0.0, 1.0, 45.0, 34.15620038730134], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1731600.0000, 
sim time next is 1732800.0000, 
raw observation next is [0.4, 91.66666666666667, 0.0, 0.0, 19.0, 25.11997720053626, 0.4970965855207978, 0.0, 1.0, 35.0, 26.028772361142074], 
processed observation next is [0.0, 0.043478260869565216, 0.4736842105263158, 0.9166666666666667, 0.0, 0.0, 0.08333333333333333, 0.5933314333780215, 0.6656988618402659, 0.0, 1.0, 0.4, 0.2602877236114207], 
reward next is 0.7397, 
noisyNet noise sample is [array([-1.5075804], dtype=float32), -0.9347325]. 
=============================================
[2019-04-09 14:58:54,152] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.03818722 0.07713401 0.09826228 0.01502609 0.21032122 0.13936722
 0.11740188 0.05469612 0.09360684 0.11721736 0.03877977], sum to 1.0000
[2019-04-09 14:58:54,156] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0810
[2019-04-09 14:58:54,194] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.3333333333333334, 93.0, 0.0, 0.0, 19.0, 26.09969881200997, 0.6506284539096631, 0.0, 1.0, 40.0, 25.647259793226155], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1722000.0000, 
sim time next is 1723200.0000, 
raw observation next is [0.1666666666666667, 94.0, 0.0, 0.0, 19.0, 25.69018938718103, 0.5892324309985552, 0.0, 1.0, 20.0, 21.66289047887055], 
processed observation next is [1.0, 0.9565217391304348, 0.4672206832871654, 0.94, 0.0, 0.0, 0.08333333333333333, 0.6408491155984191, 0.6964108103328517, 0.0, 1.0, 0.1, 0.2166289047887055], 
reward next is 0.7834, 
noisyNet noise sample is [array([0.3081408], dtype=float32), 0.38530618]. 
=============================================
[2019-04-09 14:58:54,512] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.03552456 0.08871129 0.12424994 0.01377362 0.17613268 0.1377936
 0.14074853 0.05988941 0.10371814 0.08122245 0.03823565], sum to 1.0000
[2019-04-09 14:58:54,512] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6245
[2019-04-09 14:58:54,554] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 83.33333333333334, 0.0, 0.0, 19.0, 27.50447930308948, 1.056899811573212, 0.0, 1.0, 25.0, 29.479298382010256], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1636800.0000, 
sim time next is 1638000.0000, 
raw observation next is [7.2, 82.0, 0.0, 0.0, 19.0, 27.50308639919839, 1.036523810651478, 0.0, 1.0, 45.0, 27.239480496334735], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7919238665998659, 0.8455079368838261, 0.0, 1.0, 0.6, 0.27239480496334734], 
reward next is 0.7276, 
noisyNet noise sample is [array([0.3021087], dtype=float32), 2.5671844]. 
=============================================
[2019-04-09 14:58:54,572] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[3.3351636]
 [3.2401347]
 [3.4877653]
 [3.5881507]
 [3.2552705]], R is [[4.15699911]
 [4.8206358 ]
 [5.44057131]
 [6.09250116]
 [6.74119949]].
[2019-04-09 14:58:55,584] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03628781 0.07464863 0.10993607 0.0168875  0.20616631 0.15075701
 0.11087881 0.0481632  0.10787042 0.09981162 0.03859263], sum to 1.0000
[2019-04-09 14:58:55,593] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3095
[2019-04-09 14:58:55,621] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333334, 93.0, 0.0, 0.0, 19.0, 25.84788687555278, 0.6104984762960365, 0.0, 1.0, 35.0, 22.34261573020363], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1722000.0000, 
sim time next is 1723200.0000, 
raw observation next is [0.1666666666666667, 94.0, 0.0, 0.0, 19.0, 25.44914296615472, 0.5527566938385935, 0.0, 1.0, 35.0, 18.9023317042315], 
processed observation next is [1.0, 0.9565217391304348, 0.4672206832871654, 0.94, 0.0, 0.0, 0.08333333333333333, 0.6207619138462267, 0.684252231279531, 0.0, 1.0, 0.4, 0.189023317042315], 
reward next is 0.8110, 
noisyNet noise sample is [array([-1.5656139], dtype=float32), -0.43954378]. 
=============================================
[2019-04-09 14:58:55,777] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04432186 0.0742157  0.09647612 0.02708672 0.2192808  0.12192704
 0.14383768 0.06929127 0.07462654 0.07783003 0.05110626], sum to 1.0000
[2019-04-09 14:58:55,778] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5556
[2019-04-09 14:58:55,830] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.533333333333333, 87.0, 0.0, 0.0, 19.0, 25.68324647742617, 0.5395010666680687, 0.0, 1.0, 35.0, 33.05202565400644], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1752000.0000, 
sim time next is 1753200.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 19.0, 25.50675182974471, 0.497388456722367, 0.0, 1.0, 40.0, 29.929974795777383], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 0.08333333333333333, 0.625562652478726, 0.665796152240789, 0.0, 1.0, 0.5, 0.2992997479577738], 
reward next is 0.7007, 
noisyNet noise sample is [array([-1.3166642], dtype=float32), 0.35352892]. 
=============================================
[2019-04-09 14:58:56,022] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04689946 0.08720154 0.10499937 0.02574094 0.19158995 0.11749173
 0.12856963 0.07047715 0.08609321 0.08878195 0.05215506], sum to 1.0000
[2019-04-09 14:58:56,022] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3176
[2019-04-09 14:58:56,048] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 84.33333333333333, 32.5, 0.0, 19.0, 25.15805307035263, 0.4406810511302012, 0.0, 1.0, 40.0, 32.24336150960908], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1759200.0000, 
sim time next is 1760400.0000, 
raw observation next is [-1.7, 83.0, 45.5, 0.0, 19.0, 25.04307940121354, 0.4025931997699836, 0.0, 1.0, 35.0, 28.724219689081874], 
processed observation next is [0.0, 0.391304347826087, 0.4155124653739613, 0.83, 0.15166666666666667, 0.0, 0.08333333333333333, 0.5869232834344617, 0.6341977332566612, 0.0, 1.0, 0.4, 0.28724219689081876], 
reward next is 0.7128, 
noisyNet noise sample is [array([0.28562596], dtype=float32), -0.5427126]. 
=============================================
[2019-04-09 14:58:56,214] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.05309394 0.08378679 0.10295582 0.02600569 0.19687732 0.1183481
 0.11916718 0.0649512  0.08771803 0.09962381 0.04747211], sum to 1.0000
[2019-04-09 14:58:56,216] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4432
[2019-04-09 14:58:56,280] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 83.0, 120.1666666666667, 0.0, 19.0, 24.07832503855491, 0.2893399428554035, 0.0, 1.0, 55.0, 69.79730652680169], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1776000.0000, 
sim time next is 1777200.0000, 
raw observation next is [-2.8, 83.0, 115.6666666666667, 0.0, 19.0, 24.4897442294555, 0.3259524600287801, 0.0, 1.0, 35.0, 37.692067947393014], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.38555555555555565, 0.0, 0.08333333333333333, 0.5408120191212916, 0.6086508200095934, 0.0, 1.0, 0.4, 0.37692067947393015], 
reward next is 0.6231, 
noisyNet noise sample is [array([0.74761], dtype=float32), 0.11907315]. 
=============================================
[2019-04-09 14:58:56,351] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 23437: loss 24.2883
[2019-04-09 14:58:56,354] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 23438: learning rate 0.0000
[2019-04-09 14:58:56,456] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.04594093 0.07205813 0.11028358 0.02978223 0.18434128 0.10406525
 0.15156952 0.06558809 0.09618191 0.09273089 0.04745825], sum to 1.0000
[2019-04-09 14:58:56,457] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4630
[2019-04-09 14:58:56,492] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.833333333333333, 85.0, 0.0, 0.0, 19.0, 23.26856183984139, -0.03938134862187119, 0.0, 1.0, 50.0, 44.04164439766388], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1802400.0000, 
sim time next is 1803600.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 19.0, 23.27878403696089, -0.04339117657265302, 0.0, 1.0, 40.0, 34.52430646748053], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.08333333333333333, 0.4398986697467408, 0.4855362744757823, 0.0, 1.0, 0.5, 0.34524306467480526], 
reward next is 0.6548, 
noisyNet noise sample is [array([0.7960201], dtype=float32), -0.009382807]. 
=============================================
[2019-04-09 14:58:56,502] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.04858259 0.07504108 0.10720179 0.02978724 0.1754857  0.10186619
 0.15655364 0.06998808 0.08727736 0.09893221 0.04928413], sum to 1.0000
[2019-04-09 14:58:56,502] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1100
[2019-04-09 14:58:56,545] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 19.0, 23.27878403696089, -0.04339117657265302, 0.0, 1.0, 40.0, 34.52430646748053], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1803600.0000, 
sim time next is 1804800.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 19.0, 23.22255016645676, -0.0832483930471019, 0.0, 1.0, 20.0, 30.57182511307785], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.08333333333333333, 0.43521251387139664, 0.47225053565096603, 0.0, 1.0, 0.1, 0.3057182511307785], 
reward next is 0.6943, 
noisyNet noise sample is [array([0.7960201], dtype=float32), -0.009382807]. 
=============================================
[2019-04-09 14:58:56,568] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02724034 0.06774517 0.09063666 0.01097454 0.2799084  0.1359764
 0.11988019 0.05001149 0.10747842 0.07235903 0.03778939], sum to 1.0000
[2019-04-09 14:58:56,568] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1055
[2019-04-09 14:58:56,606] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.1, 88.0, 96.66666666666667, 0.0, 22.5, 26.84250800415597, 0.9028842573996121, 1.0, 1.0, 30.0, 24.61089197409823], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1690800.0000, 
sim time next is 1692000.0000, 
raw observation next is [1.1, 88.0, 90.0, 0.0, 22.5, 27.38405029421006, 0.9367164745716904, 1.0, 1.0, 25.0, 20.685474452123444], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.88, 0.3, 0.0, 0.375, 0.7820041911841716, 0.8122388248572302, 1.0, 1.0, 0.2, 0.20685474452123442], 
reward next is 0.7931, 
noisyNet noise sample is [array([-0.99520147], dtype=float32), 0.13881634]. 
=============================================
[2019-04-09 14:58:56,612] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[3.5478106]
 [3.5078712]
 [3.6230822]
 [3.483273 ]
 [3.5213537]], R is [[4.26413822]
 [4.97538805]
 [5.63004827]
 [6.28763294]
 [6.92139864]].
[2019-04-09 14:58:56,643] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.04928877 0.07910496 0.10428321 0.0293094  0.18954092 0.13479173
 0.11774428 0.06677239 0.08639014 0.08685896 0.05591524], sum to 1.0000
[2019-04-09 14:58:56,643] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7802
[2019-04-09 14:58:56,699] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1500, global step 23552: loss 16.9605
[2019-04-09 14:58:56,700] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1500, global step 23552: learning rate 0.0000
[2019-04-09 14:58:56,702] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.0, 85.66666666666667, 0.0, 0.0, 19.0, 25.2596762690879, 0.460526330646188, 0.0, 1.0, 20.0, 37.17202055147972], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1748400.0000, 
sim time next is 1749600.0000, 
raw observation next is [-1.2, 87.0, 0.0, 0.0, 19.0, 25.22316194763395, 0.4600863347358191, 0.0, 1.0, 55.0, 47.69528241902505], 
processed observation next is [0.0, 0.2608695652173913, 0.42936288088642666, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6019301623028293, 0.6533621115786064, 0.0, 1.0, 0.8, 0.4769528241902505], 
reward next is 0.5230, 
noisyNet noise sample is [array([-0.6391833], dtype=float32), 0.10600625]. 
=============================================
[2019-04-09 14:58:56,847] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23600: loss 23.9164
[2019-04-09 14:58:56,847] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23600: learning rate 0.0000
[2019-04-09 14:58:57,101] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23688: loss 19.0610
[2019-04-09 14:58:57,101] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23688: learning rate 0.0000
[2019-04-09 14:58:57,181] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23715: loss 21.2342
[2019-04-09 14:58:57,183] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23715: learning rate 0.0000
[2019-04-09 14:58:57,244] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23737: loss 24.4341
[2019-04-09 14:58:57,244] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23737: learning rate 0.0000
[2019-04-09 14:58:57,437] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23801: loss 23.8178
[2019-04-09 14:58:57,439] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23803: learning rate 0.0000
[2019-04-09 14:58:57,470] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23818: loss 24.2381
[2019-04-09 14:58:57,470] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23818: learning rate 0.0000
[2019-04-09 14:58:57,617] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04166406 0.0946085  0.10793336 0.02879329 0.17703979 0.0934351
 0.16658711 0.06209977 0.10525518 0.08106788 0.04151599], sum to 1.0000
[2019-04-09 14:58:57,622] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7946
[2019-04-09 14:58:57,651] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-5.733333333333333, 79.66666666666667, 0.0, 0.0, 19.0, 21.7243931599636, -0.4432689195264646, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1819200.0000, 
sim time next is 1820400.0000, 
raw observation next is [-5.866666666666667, 81.33333333333333, 0.0, 0.0, 19.0, 21.43829586306934, -0.3574144127504089, 0.0, 1.0, 55.0, 71.00548934215931], 
processed observation next is [0.0, 0.043478260869565216, 0.30009233610341646, 0.8133333333333332, 0.0, 0.0, 0.08333333333333333, 0.28652465525577825, 0.38086186241653036, 0.0, 1.0, 0.8, 0.7100548934215931], 
reward next is 0.2899, 
noisyNet noise sample is [array([-1.1400508], dtype=float32), -1.105353]. 
=============================================
[2019-04-09 14:58:57,754] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04709511 0.08771247 0.09988806 0.02371513 0.22936176 0.11925463
 0.13169149 0.06104197 0.07776292 0.07541382 0.04706261], sum to 1.0000
[2019-04-09 14:58:57,756] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1592
[2019-04-09 14:58:57,777] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 19.0, 24.37917607139443, 0.227044348826197, 0.0, 1.0, 40.0, 32.29881663793094], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1792800.0000, 
sim time next is 1794000.0000, 
raw observation next is [-4.1, 82.33333333333334, 0.0, 0.0, 19.0, 24.23931011006898, 0.1861098585998033, 0.0, 1.0, 25.0, 28.677862776628594], 
processed observation next is [0.0, 0.782608695652174, 0.3490304709141275, 0.8233333333333335, 0.0, 0.0, 0.08333333333333333, 0.5199425091724151, 0.5620366195332678, 0.0, 1.0, 0.2, 0.28677862776628593], 
reward next is 0.7132, 
noisyNet noise sample is [array([0.38728723], dtype=float32), 0.48308468]. 
=============================================
[2019-04-09 14:58:57,796] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[2.5121477]
 [2.6807396]
 [2.5598292]
 [2.5433269]
 [2.720179 ]], R is [[3.15719748]
 [3.80263758]
 [4.34420347]
 [5.01645565]
 [5.65462399]].
[2019-04-09 14:58:57,846] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23954: loss 28.0712
[2019-04-09 14:58:57,849] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23955: learning rate 0.0000
[2019-04-09 14:58:57,946] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.05128051 0.06863094 0.09791379 0.01308485 0.24183449 0.1224477
 0.11204801 0.04849698 0.08381322 0.10951446 0.05093496], sum to 1.0000
[2019-04-09 14:58:57,946] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6862
[2019-04-09 14:58:57,974] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666667, 94.0, 0.0, 0.0, 19.0, 26.08858696437947, 0.6820476439525834, 0.0, 1.0, 55.0, 46.47104185125569], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1723200.0000, 
sim time next is 1724400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 26.13091813479154, 0.66349317090758, 0.0, 1.0, 45.0, 36.205069173942746], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.6775765112326283, 0.7211643903025267, 0.0, 1.0, 0.6, 0.36205069173942744], 
reward next is 0.6379, 
noisyNet noise sample is [array([-0.30012318], dtype=float32), -1.1976199]. 
=============================================
[2019-04-09 14:58:58,001] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.05172669 0.06825431 0.09813742 0.02350732 0.21607974 0.11619189
 0.13443965 0.06219274 0.07466989 0.09998892 0.05481151], sum to 1.0000
[2019-04-09 14:58:58,001] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9234
[2019-04-09 14:58:58,031] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 24007: loss 20.0879
[2019-04-09 14:58:58,032] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 24007: learning rate 0.0000
[2019-04-09 14:58:58,056] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.533333333333333, 78.33333333333334, 0.0, 0.0, 19.0, 23.44269791297687, -0.0496446676486286, 0.0, 1.0, 30.0, 50.52338328635054], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1838400.0000, 
sim time next is 1839600.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 19.0, 23.4796631248794, -0.05703671705656582, 0.0, 1.0, 60.0, 56.65155971615907], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.45663859373995014, 0.48098776098114476, 0.0, 1.0, 0.9, 0.5665155971615907], 
reward next is 0.4335, 
noisyNet noise sample is [array([-0.20690309], dtype=float32), 0.7523175]. 
=============================================
[2019-04-09 14:58:58,116] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24039: loss 25.3850
[2019-04-09 14:58:58,117] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24039: learning rate 0.0000
[2019-04-09 14:58:58,184] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04989417 0.08815139 0.10305329 0.02997245 0.20459612 0.08671749
 0.15257356 0.05158024 0.10223707 0.09158234 0.03964181], sum to 1.0000
[2019-04-09 14:58:58,185] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0046
[2019-04-09 14:58:58,211] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-5.733333333333333, 79.66666666666667, 0.0, 0.0, 19.0, 23.18196761528986, -0.09574844217634117, 0.0, 1.0, 45.0, 34.49036552627956], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 1819200.0000, 
sim time next is 1820400.0000, 
raw observation next is [-5.866666666666667, 81.33333333333333, 0.0, 0.0, 19.0, 22.88644209146839, -0.2594411721692236, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.30009233610341646, 0.8133333333333332, 0.0, 0.0, 0.08333333333333333, 0.40720350762236573, 0.4135196092769255, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3909876], dtype=float32), -0.30829564]. 
=============================================
[2019-04-09 14:58:58,271] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 24083: loss 23.5889
[2019-04-09 14:58:58,275] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 24086: learning rate 0.0000
[2019-04-09 14:58:58,391] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.05073709 0.07071485 0.1096979  0.02728682 0.20373876 0.12334131
 0.13676941 0.05884337 0.08002174 0.08731262 0.05153615], sum to 1.0000
[2019-04-09 14:58:58,393] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2868
[2019-04-09 14:58:58,423] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 82.0, 0.0, 0.0, 19.0, 23.38417915241556, -0.00891000863490809, 0.0, 1.0, 45.0, 33.16100301706567], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1810800.0000, 
sim time next is 1812000.0000, 
raw observation next is [-5.0, 81.0, 0.0, 0.0, 19.0, 23.2802060855384, -0.04305226090512263, 0.0, 1.0, 35.0, 29.712027413613768], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.81, 0.0, 0.0, 0.08333333333333333, 0.4400171737948666, 0.4856492463649591, 0.0, 1.0, 0.4, 0.2971202741361377], 
reward next is 0.7029, 
noisyNet noise sample is [array([0.24743445], dtype=float32), -0.53086346]. 
=============================================
[2019-04-09 14:58:58,427] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[2.6771977]
 [2.6603131]
 [2.7882512]
 [2.6001616]
 [2.6554246]], R is [[3.4529438 ]
 [4.08680439]
 [4.67154741]
 [5.00819063]
 [5.70127201]].
[2019-04-09 14:58:58,442] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.05186126 0.08129345 0.09137037 0.03302893 0.2099317  0.1108364
 0.11804815 0.07888748 0.08531232 0.08770662 0.05172335], sum to 1.0000
[2019-04-09 14:58:58,442] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9140
[2019-04-09 14:58:58,448] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04947304 0.09249478 0.09967574 0.02827839 0.17141706 0.1429434
 0.10980077 0.08307866 0.07020632 0.08540348 0.06722831], sum to 1.0000
[2019-04-09 14:58:58,448] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0777
[2019-04-09 14:58:58,469] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.2, 79.0, 0.0, 0.0, 19.0, 21.2638272476588, -0.5081519159573985, 0.0, 1.0, 40.0, 33.514181377777994], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1836000.0000, 
sim time next is 1837200.0000, 
raw observation next is [-6.366666666666667, 78.66666666666667, 0.0, 0.0, 19.0, 21.16742961728912, -0.480389496881215, 0.0, 1.0, 60.0, 63.60945464413368], 
processed observation next is [0.0, 0.2608695652173913, 0.28624192059095105, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.2639524681074266, 0.33987016770626166, 0.0, 1.0, 0.9, 0.6360945464413368], 
reward next is 0.3639, 
noisyNet noise sample is [array([1.7304615], dtype=float32), -0.9835981]. 
=============================================
[2019-04-09 14:58:58,482] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 19.0, 23.4883772654267, -0.01852529408700066, 0.0, 1.0, 55.0, 48.76158905201617], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1825200.0000, 
sim time next is 1826400.0000, 
raw observation next is [-6.2, 85.66666666666667, 0.0, 0.0, 19.0, 23.49349038401776, -0.03040361076818614, 0.0, 1.0, 20.0, 40.41719002343944], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.45779086533481345, 0.48986546307727125, 0.0, 1.0, 0.1, 0.40417190023439437], 
reward next is 0.5958, 
noisyNet noise sample is [array([-0.8739082], dtype=float32), 1.300981]. 
=============================================
[2019-04-09 14:58:58,658] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24221: loss 23.2602
[2019-04-09 14:58:58,658] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24221: learning rate 0.0000
[2019-04-09 14:58:58,731] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24248: loss 26.7177
[2019-04-09 14:58:58,731] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24248: learning rate 0.0000
[2019-04-09 14:58:58,874] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.05218575 0.07834153 0.09369087 0.03455969 0.19536483 0.1296568
 0.11852007 0.08273502 0.07507022 0.08938678 0.05048844], sum to 1.0000
[2019-04-09 14:58:58,875] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1220
[2019-04-09 14:58:58,911] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.133333333333333, 85.66666666666667, 0.0, 0.0, 19.0, 21.77114881260249, -0.3593884961603974, 0.0, 1.0, 55.0, 53.73208148625538], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 1824000.0000, 
sim time next is 1825200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 21.94373523161287, -0.3498427119319005, 0.0, 1.0, 20.0, 41.176628511983395], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.3286446026344058, 0.38338576268936647, 0.0, 1.0, 0.1, 0.41176628511983393], 
reward next is 0.5882, 
noisyNet noise sample is [array([-0.08533049], dtype=float32), -1.111146]. 
=============================================
[2019-04-09 14:58:59,028] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.04822126 0.095685   0.09605785 0.02982018 0.18443601 0.12414034
 0.13351619 0.07651967 0.07148907 0.08238479 0.05772961], sum to 1.0000
[2019-04-09 14:58:59,029] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9833
[2019-04-09 14:58:59,035] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.04435588 0.09562074 0.09498621 0.03314937 0.20205052 0.11954661
 0.12565862 0.08032574 0.08525532 0.06804525 0.0510058 ], sum to 1.0000
[2019-04-09 14:58:59,038] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9164
[2019-04-09 14:58:59,054] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.700000000000001, 78.0, 47.16666666666666, 15.66666666666666, 19.0, 22.33066790511987, -0.2814894500405459, 0.0, 1.0, 35.0, 39.466707272241166], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 1845600.0000, 
sim time next is 1846800.0000, 
raw observation next is [-6.7, 78.0, 87.5, 47.0, 19.0, 22.36829250079781, -0.2319002844727737, 0.0, 1.0, 65.0, 72.96092240727674], 
processed observation next is [0.0, 0.391304347826087, 0.2770083102493075, 0.78, 0.2916666666666667, 0.051933701657458566, 0.08333333333333333, 0.3640243750664842, 0.4226999051757421, 0.0, 1.0, 1.0, 0.7296092240727674], 
reward next is 0.2704, 
noisyNet noise sample is [array([-1.0768529], dtype=float32), 2.7741814]. 
=============================================
[2019-04-09 14:58:59,084] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.133333333333333, 85.66666666666667, 0.0, 0.0, 19.0, 22.18555218730455, -0.2524812434350837, 0.0, 1.0, 25.0, 40.83484778213906], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1824000.0000, 
sim time next is 1825200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 22.31201998401583, -0.2070139124550419, 0.0, 1.0, 60.0, 65.06090464279723], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.35933499866798585, 0.43099536251498605, 0.0, 1.0, 0.9, 0.6506090464279722], 
reward next is 0.3494, 
noisyNet noise sample is [array([1.6243309], dtype=float32), -0.4550082]. 
=============================================
[2019-04-09 14:58:59,205] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.05360444 0.07637624 0.08112767 0.02780122 0.2138993  0.13538727
 0.12890795 0.0672174  0.07829161 0.08908571 0.04830112], sum to 1.0000
[2019-04-09 14:58:59,205] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3231
[2019-04-09 14:58:59,257] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.7, 78.0, 87.5, 47.0, 19.0, 22.71856651202151, -0.2403284016146804, 0.0, 1.0, 25.0, 35.97471438341729], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1846800.0000, 
sim time next is 1848000.0000, 
raw observation next is [-6.333333333333334, 78.0, 127.8333333333333, 78.33333333333334, 19.0, 22.5514778328379, -0.2673411400771304, 0.0, 1.0, 35.0, 32.42442978943458], 
processed observation next is [0.0, 0.391304347826087, 0.28716528162511545, 0.78, 0.426111111111111, 0.08655616942909762, 0.08333333333333333, 0.37928981940315837, 0.41088628664095656, 0.0, 1.0, 0.4, 0.32424429789434583], 
reward next is 0.6758, 
noisyNet noise sample is [array([-0.6980402], dtype=float32), 0.59726715]. 
=============================================
[2019-04-09 14:58:59,270] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[2.2931638]
 [2.5031352]
 [2.5074337]
 [2.4007113]
 [2.5236316]], R is [[3.02702951]
 [3.637012  ]
 [4.20047379]
 [4.71158791]
 [5.1525383 ]].
[2019-04-09 14:58:59,407] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.04753966 0.0835584  0.10102971 0.03163557 0.1775085  0.14488125
 0.12446488 0.06654488 0.07570044 0.09294568 0.05419106], sum to 1.0000
[2019-04-09 14:58:59,407] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7229
[2019-04-09 14:58:59,448] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.533333333333333, 78.33333333333334, 0.0, 0.0, 19.0, 21.67554087969215, -0.4153886544964582, 0.0, 1.0, 35.0, 45.55467056117764], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1838400.0000, 
sim time next is 1839600.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 19.0, 21.79489777330823, -0.4254671125409614, 0.0, 1.0, 50.0, 40.79131503293108], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.31624148110901906, 0.3581776291530128, 0.0, 1.0, 0.7, 0.4079131503293108], 
reward next is 0.5921, 
noisyNet noise sample is [array([-0.04203998], dtype=float32), 0.36220518]. 
=============================================
[2019-04-09 14:58:59,552] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.05498943 0.09474377 0.10574598 0.04026759 0.17710915 0.10419524
 0.11666594 0.07269942 0.08525677 0.09392785 0.05439887], sum to 1.0000
[2019-04-09 14:58:59,552] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2066
[2019-04-09 14:58:59,575] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.05828723 0.07968328 0.07276955 0.02765097 0.18389821 0.13608506
 0.1271945  0.07651558 0.08897019 0.09280965 0.05613577], sum to 1.0000
[2019-04-09 14:58:59,576] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8694
[2019-04-09 14:58:59,582] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.199999999999999, 72.33333333333333, 173.3333333333333, 67.5, 19.0, 22.24738867170172, -0.3133780136553666, 0.0, 1.0, 20.0, 27.82167219581633], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1856400.0000, 
sim time next is 1857600.0000, 
raw observation next is [-5.0, 71.0, 152.0, 40.5, 19.0, 22.0837099143353, -0.3502620198360878, 0.0, 1.0, 25.0, 25.211689344074735], 
processed observation next is [0.0, 0.5217391304347826, 0.32409972299168976, 0.71, 0.5066666666666667, 0.044751381215469614, 0.08333333333333333, 0.3403091595279418, 0.3832459933879708, 0.0, 1.0, 0.2, 0.25211689344074734], 
reward next is 0.7479, 
noisyNet noise sample is [array([-0.63081044], dtype=float32), 0.37596142]. 
=============================================
[2019-04-09 14:58:59,613] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.133333333333333, 85.66666666666667, 0.0, 0.0, 19.0, 21.82727335067146, -0.3379213524483581, 0.0, 1.0, 60.0, 64.04089931857379], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1824000.0000, 
sim time next is 1825200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 22.03008058959722, -0.2457513175000132, 0.0, 1.0, 60.0, 67.04656796454292], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.33584004913310156, 0.4180828941666623, 0.0, 1.0, 0.9, 0.6704656796454291], 
reward next is 0.3295, 
noisyNet noise sample is [array([-1.1568238], dtype=float32), -0.17083824]. 
=============================================
[2019-04-09 14:58:59,941] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.04760443 0.08613918 0.09081686 0.03199049 0.20167351 0.10640712
 0.1528174  0.06478854 0.08398434 0.08205958 0.05171859], sum to 1.0000
[2019-04-09 14:58:59,942] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8083
[2019-04-09 14:58:59,989] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.5, 75.0, 183.3333333333333, 76.66666666666667, 19.0, 22.06702886699429, -0.3268404393893162, 0.0, 1.0, 45.0, 36.184097302495665], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1866000.0000, 
sim time next is 1867200.0000, 
raw observation next is [-4.5, 79.0, 167.0, 70.0, 19.0, 22.0736665179233, -0.2800019432622847, 0.0, 1.0, 55.0, 57.52244300885722], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.79, 0.5566666666666666, 0.07734806629834254, 0.08333333333333333, 0.33947220982694165, 0.40666601891257176, 0.0, 1.0, 0.8, 0.5752244300885723], 
reward next is 0.4248, 
noisyNet noise sample is [array([0.3594235], dtype=float32), 1.3791641]. 
=============================================
[2019-04-09 14:59:00,042] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.04880684 0.06857407 0.09249807 0.02717054 0.20741493 0.11817633
 0.1623536  0.06094243 0.07569396 0.09110261 0.04726663], sum to 1.0000
[2019-04-09 14:59:00,046] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4280
[2019-04-09 14:59:00,101] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 19.0, 22.61876139180779, -0.2723460073034281, 0.0, 1.0, 55.0, 50.56072142083312], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1893600.0000, 
sim time next is 1894800.0000, 
raw observation next is [-6.566666666666666, 76.33333333333334, 0.0, 0.0, 19.0, 22.59868013952659, -0.2916717176256904, 0.0, 1.0, 35.0, 38.975479917418326], 
processed observation next is [0.0, 0.9565217391304348, 0.28070175438596495, 0.7633333333333334, 0.0, 0.0, 0.08333333333333333, 0.3832233449605491, 0.40277609412476983, 0.0, 1.0, 0.4, 0.38975479917418326], 
reward next is 0.6102, 
noisyNet noise sample is [array([0.6873207], dtype=float32), 0.14353745]. 
=============================================
[2019-04-09 14:59:00,159] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.04778674 0.08812084 0.09421404 0.03102857 0.2205304  0.11040624
 0.11482038 0.08003336 0.07876743 0.08091548 0.05337651], sum to 1.0000
[2019-04-09 14:59:00,159] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7524
[2019-04-09 14:59:00,197] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-5.199999999999999, 72.33333333333333, 173.3333333333333, 67.5, 19.0, 22.07555653511034, -0.3713010135047521, 0.0, 1.0, 25.0, 31.45895995137477], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 1856400.0000, 
sim time next is 1857600.0000, 
raw observation next is [-5.0, 71.0, 152.0, 40.5, 19.0, 21.97736201030714, -0.371495888282589, 0.0, 1.0, 50.0, 43.06718991892537], 
processed observation next is [0.0, 0.5217391304347826, 0.32409972299168976, 0.71, 0.5066666666666667, 0.044751381215469614, 0.08333333333333333, 0.3314468341922616, 0.376168037239137, 0.0, 1.0, 0.7, 0.43067189918925375], 
reward next is 0.5693, 
noisyNet noise sample is [array([0.3435678], dtype=float32), -0.5344781]. 
=============================================
[2019-04-09 14:59:00,203] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.05065021 0.08249442 0.08766111 0.02790051 0.24415435 0.11125775
 0.10355108 0.07617538 0.06701318 0.09105023 0.05809183], sum to 1.0000
[2019-04-09 14:59:00,203] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2850
[2019-04-09 14:59:00,241] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 71.0, 152.0, 40.5, 19.0, 21.97736201030714, -0.371495888282589, 0.0, 1.0, 50.0, 43.06718991892537], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1857600.0000, 
sim time next is 1858800.0000, 
raw observation next is [-4.833333333333334, 71.0, 130.6666666666667, 13.5, 19.0, 21.98638157865914, -0.3841287802103737, 0.0, 1.0, 35.0, 33.27174626008683], 
processed observation next is [0.0, 0.5217391304347826, 0.32871652816251157, 0.71, 0.4355555555555557, 0.014917127071823204, 0.08333333333333333, 0.3321984648882618, 0.37195707326320876, 0.0, 1.0, 0.4, 0.33271746260086826], 
reward next is 0.6673, 
noisyNet noise sample is [array([0.3435678], dtype=float32), -0.5344781]. 
=============================================
[2019-04-09 14:59:00,292] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.05588186 0.08397328 0.10768793 0.02264348 0.20967339 0.10416999
 0.13115168 0.06129452 0.08725694 0.09407149 0.04219554], sum to 1.0000
[2019-04-09 14:59:00,295] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8662
[2019-04-09 14:59:00,347] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-7.3, 81.0, 0.0, 0.0, 19.0, 22.19532504572696, -0.3971588433068758, 0.0, 1.0, 35.0, 30.92089097359161], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1899600.0000, 
sim time next is 1900800.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 19.0, 21.97271500714307, -0.3889775950589482, 0.0, 1.0, 60.0, 66.48945277727185], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.08333333333333333, 0.3310595839285891, 0.37034080164701727, 0.0, 1.0, 0.9, 0.6648945277727185], 
reward next is 0.3351, 
noisyNet noise sample is [array([-0.5423107], dtype=float32), -0.7680315]. 
=============================================
[2019-04-09 14:59:00,740] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1500, global step 24940: loss 21.6533
[2019-04-09 14:59:00,740] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1500, global step 24940: learning rate 0.0000
[2019-04-09 14:59:00,926] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 25008: loss 20.4076
[2019-04-09 14:59:00,927] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 25008: learning rate 0.0000
[2019-04-09 14:59:01,402] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.05436579 0.08658887 0.09037252 0.02435937 0.17905588 0.14701977
 0.09721866 0.07614239 0.08635519 0.09200157 0.06651995], sum to 1.0000
[2019-04-09 14:59:01,402] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2690
[2019-04-09 14:59:01,429] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.05435983 0.08573213 0.10787125 0.02556353 0.20867531 0.11169926
 0.12051638 0.0671957  0.08686467 0.08383393 0.04768801], sum to 1.0000
[2019-04-09 14:59:01,429] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2134
[2019-04-09 14:59:01,465] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.6, 86.0, 0.0, 0.0, 19.0, 21.76693652762724, -0.4004390852617479, 0.0, 1.0, 55.0, 56.74427874511924], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1886400.0000, 
sim time next is 1887600.0000, 
raw observation next is [-5.6, 85.0, 0.0, 0.0, 19.0, 21.88755215358572, -0.3822641301481675, 0.0, 1.0, 25.0, 37.59644652515961], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.85, 0.0, 0.0, 0.08333333333333333, 0.32396267946547663, 0.37257862328394414, 0.0, 1.0, 0.2, 0.37596446525159605], 
reward next is 0.6240, 
noisyNet noise sample is [array([1.2073462], dtype=float32), -1.1972797]. 
=============================================
[2019-04-09 14:59:01,470] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 19.0, 20.68381723329722, -0.6705423071495736, 0.0, 1.0, 55.0, 59.81264231111206], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1915200.0000, 
sim time next is 1916400.0000, 
raw observation next is [-8.566666666666666, 79.33333333333334, 0.0, 0.0, 19.0, 20.88972333285848, -0.6459688069508539, 0.0, 1.0, 35.0, 39.90937864897231], 
processed observation next is [1.0, 0.17391304347826086, 0.22530009233610343, 0.7933333333333334, 0.0, 0.0, 0.08333333333333333, 0.2408102777382067, 0.28467706434971535, 0.0, 1.0, 0.4, 0.39909378648972305], 
reward next is 0.6009, 
noisyNet noise sample is [array([-0.64249396], dtype=float32), -0.18655144]. 
=============================================
[2019-04-09 14:59:01,709] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.04889198 0.08390073 0.10215645 0.02817523 0.16158542 0.14317472
 0.10980089 0.084879   0.082185   0.09307702 0.06217358], sum to 1.0000
[2019-04-09 14:59:01,730] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5981
[2019-04-09 14:59:01,769] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.199999999999999, 84.33333333333333, 0.0, 0.0, 19.0, 22.17516330949132, -0.3230955397342559, 0.0, 1.0, 25.0, 26.915870074471968], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1827600.0000, 
sim time next is 1828800.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 19.0, 21.96250715941635, -0.3356652737783785, 0.0, 1.0, 45.0, 39.08125357962571], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.83, 0.0, 0.0, 0.08333333333333333, 0.3302089299513626, 0.38811157540720714, 0.0, 1.0, 0.6, 0.3908125357962571], 
reward next is 0.6092, 
noisyNet noise sample is [array([1.0703874], dtype=float32), 0.31059015]. 
=============================================
[2019-04-09 14:59:02,079] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.04190582 0.07900049 0.09098939 0.02354274 0.23448908 0.09621661
 0.13532381 0.06019742 0.09880398 0.10083008 0.03870056], sum to 1.0000
[2019-04-09 14:59:02,080] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1302
[2019-04-09 14:59:02,101] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.04232759 0.07418702 0.09003729 0.01723524 0.1783191  0.15909572
 0.11545969 0.07315728 0.07570093 0.10247124 0.07200896], sum to 1.0000
[2019-04-09 14:59:02,101] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7882
[2019-04-09 14:59:02,137] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-8.566666666666666, 79.33333333333334, 0.0, 0.0, 19.0, 21.75811770369961, -0.5055006680386825, 0.0, 1.0, 45.0, 35.996502235697946], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1916400.0000, 
sim time next is 1917600.0000, 
raw observation next is [-8.733333333333334, 80.66666666666667, 0.0, 0.0, 19.0, 21.62013752715003, -0.4842397907083891, 0.0, 1.0, 55.0, 57.959999434293636], 
processed observation next is [1.0, 0.17391304347826086, 0.22068328716528163, 0.8066666666666668, 0.0, 0.0, 0.08333333333333333, 0.30167812726250237, 0.338586736430537, 0.0, 1.0, 0.8, 0.5795999943429364], 
reward next is 0.4204, 
noisyNet noise sample is [array([0.88580835], dtype=float32), 0.8610202]. 
=============================================
[2019-04-09 14:59:02,147] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.3, 81.0, 0.0, 0.0, 19.0, 21.26339054020747, -0.5839069811180484, 0.0, 1.0, 40.0, 32.26232168234721], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1899600.0000, 
sim time next is 1900800.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 19.0, 21.12301540575668, -0.6228633963605159, 0.0, 1.0, 25.0, 29.180870746466937], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.08333333333333333, 0.26025128381305657, 0.29237886787982803, 0.0, 1.0, 0.2, 0.29180870746466936], 
reward next is 0.7082, 
noisyNet noise sample is [array([-0.40837088], dtype=float32), -1.8323619]. 
=============================================
[2019-04-09 14:59:02,261] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.04297734 0.09429614 0.10022797 0.02745588 0.17788433 0.11675455
 0.14729668 0.0588649  0.09071622 0.09986175 0.04366427], sum to 1.0000
[2019-04-09 14:59:02,261] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4209
[2019-04-09 14:59:02,306] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 77.33333333333334, 0.0, 0.0, 19.0, 20.78811838755638, -0.6977471588388049, 0.0, 1.0, 35.0, 24.29416467305399], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1903200.0000, 
sim time next is 1904400.0000, 
raw observation next is [-7.3, 75.0, 0.0, 0.0, 19.0, 20.63521180198497, -0.7323862067537141, 0.0, 1.0, 35.0, 22.313761015162058], 
processed observation next is [1.0, 0.043478260869565216, 0.26038781163434904, 0.75, 0.0, 0.0, 0.08333333333333333, 0.21960098349874743, 0.2558712644154286, 0.0, 1.0, 0.4, 0.2231376101516206], 
reward next is 0.7769, 
noisyNet noise sample is [array([-1.2412218], dtype=float32), 1.9407604]. 
=============================================
[2019-04-09 14:59:02,456] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03282181 0.05780082 0.08663035 0.0171712  0.20065156 0.16558093
 0.16467096 0.05559687 0.08250559 0.07655163 0.06001829], sum to 1.0000
[2019-04-09 14:59:02,457] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6007
[2019-04-09 14:59:02,615] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.3, 89.33333333333334, 27.83333333333333, 17.66666666666667, 22.5, 23.00224470936676, -0.2225270286560572, 1.0, 1.0, 60.0, 70.4280587586099], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1930800.0000, 
sim time next is 1932000.0000, 
raw observation next is [-9.100000000000001, 87.66666666666666, 41.66666666666666, 109.0, 22.5, 23.59382616711882, -0.1756158681086042, 1.0, 1.0, 45.0, 52.530772626968556], 
processed observation next is [1.0, 0.34782608695652173, 0.21052631578947364, 0.8766666666666666, 0.13888888888888887, 0.12044198895027625, 0.375, 0.466152180593235, 0.4414613772971319, 1.0, 1.0, 0.6, 0.5253077262696856], 
reward next is 0.4747, 
noisyNet noise sample is [array([0.8212862], dtype=float32), -0.68711185]. 
=============================================
[2019-04-09 14:59:02,630] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[3.2635112]
 [3.113824 ]
 [2.9127817]
 [2.9057984]
 [2.9108536]], R is [[3.51456094]
 [3.7751348 ]
 [3.87248349]
 [4.27253819]
 [4.52033997]].
[2019-04-09 14:59:02,970] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.04132923 0.07282715 0.08064725 0.02202713 0.20178995 0.12761001
 0.14178659 0.06097571 0.07771159 0.11116026 0.0621352 ], sum to 1.0000
[2019-04-09 14:59:02,971] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8805
[2019-04-09 14:59:03,035] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-8.900000000000002, 82.0, 0.0, 0.0, 19.0, 20.00230697370976, -0.8553034668251875, 0.0, 1.0, 60.0, 63.13422948259887], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1920000.0000, 
sim time next is 1921200.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 19.0, 20.23106582066901, -0.7606545595977199, 0.0, 1.0, 60.0, 69.11600227259603], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.82, 0.0, 0.0, 0.08333333333333333, 0.18592215172241744, 0.24644848013409337, 0.0, 1.0, 0.9, 0.6911600227259603], 
reward next is 0.3088, 
noisyNet noise sample is [array([-0.01393651], dtype=float32), -0.55643886]. 
=============================================
[2019-04-09 14:59:04,749] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.02718276 0.05520108 0.09479444 0.01830604 0.23960137 0.14993258
 0.12262614 0.0666635  0.06891044 0.08840525 0.06837641], sum to 1.0000
[2019-04-09 14:59:04,749] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6894
[2019-04-09 14:59:04,843] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.199999999999999, 68.33333333333333, 231.1666666666667, 9.0, 22.5, 25.15447314822512, 0.1653152876511512, 1.0, 1.0, 35.0, 28.52925657638835], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1942800.0000, 
sim time next is 1944000.0000, 
raw observation next is [-5.0, 65.0, 229.5, 7.0, 22.5, 25.17834886071345, 0.1913374517073685, 1.0, 1.0, 45.0, 38.09194654096511], 
processed observation next is [1.0, 0.5217391304347826, 0.32409972299168976, 0.65, 0.765, 0.0077348066298342545, 0.375, 0.5981957383927874, 0.5637791505691229, 1.0, 1.0, 0.6, 0.3809194654096511], 
reward next is 0.6191, 
noisyNet noise sample is [array([-0.9002437], dtype=float32), -1.2768492]. 
=============================================
[2019-04-09 14:59:04,903] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[3.759461 ]
 [3.6042762]
 [3.723042 ]
 [3.7391193]
 [3.6815152]], R is [[4.23070574]
 [4.90310621]
 [5.53492165]
 [6.08512688]
 [6.36591768]].
[2019-04-09 14:59:07,043] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.02876539 0.05754758 0.0774844  0.01492767 0.25948918 0.14482392
 0.14316131 0.05174926 0.08600639 0.07827585 0.0577691 ], sum to 1.0000
[2019-04-09 14:59:07,044] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6563
[2019-04-09 14:59:07,085] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.866666666666667, 77.66666666666667, 148.5, 0.0, 22.5, 23.70763210881605, -0.06091778921723739, 1.0, 1.0, 50.0, 41.196513836088315], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2029200.0000, 
sim time next is 2030400.0000, 
raw observation next is [-4.5, 75.0, 151.5, 0.0, 22.5, 23.87083287173945, -0.03408953446689035, 1.0, 1.0, 45.0, 30.67747412902331], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.75, 0.505, 0.0, 0.375, 0.48923607264495406, 0.4886368218443699, 1.0, 1.0, 0.6, 0.3067747412902331], 
reward next is 0.6932, 
noisyNet noise sample is [array([-0.13009937], dtype=float32), 1.388617]. 
=============================================
[2019-04-09 14:59:07,362] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02887847 0.07059277 0.07931617 0.02029935 0.21791434 0.13872087
 0.14726189 0.0611189  0.08650165 0.08998957 0.05940607], sum to 1.0000
[2019-04-09 14:59:07,362] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8537
[2019-04-09 14:59:07,520] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.166666666666666, 76.33333333333334, 181.1666666666667, 198.3333333333333, 22.5, 22.96850049034698, -0.3225896438632868, 1.0, 1.0, 25.0, 22.29279514179678], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1939200.0000, 
sim time next is 1940400.0000, 
raw observation next is [-5.6, 75.0, 201.5, 123.0, 22.5, 22.96334032148192, -0.2217579997169807, 1.0, 1.0, 60.0, 62.61250426784406], 
processed observation next is [1.0, 0.4782608695652174, 0.30747922437673136, 0.75, 0.6716666666666666, 0.13591160220994475, 0.375, 0.4136116934568266, 0.4260806667610064, 1.0, 1.0, 0.9, 0.6261250426784406], 
reward next is 0.3739, 
noisyNet noise sample is [array([0.2318254], dtype=float32), -0.45467848]. 
=============================================
[2019-04-09 14:59:08,068] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.02803837 0.07094332 0.09261346 0.01472299 0.22173727 0.1616426
 0.1299313  0.06190707 0.09479888 0.08728524 0.03637957], sum to 1.0000
[2019-04-09 14:59:08,069] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8136
[2019-04-09 14:59:08,096] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02038657 0.05670866 0.0898735  0.01427398 0.21021557 0.13956955
 0.21464187 0.05426889 0.08450364 0.05998527 0.05557239], sum to 1.0000
[2019-04-09 14:59:08,096] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5429
[2019-04-09 14:59:08,097] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.0, 81.33333333333333, 0.0, 0.0, 19.0, 23.35425337814569, -0.02495951557185482, 0.0, 1.0, 35.0, 21.85134234234613], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 1978800.0000, 
sim time next is 1980000.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 19.0, 23.21463703542737, -0.007154104725391686, 0.0, 1.0, 55.0, 58.01439459772824], 
processed observation next is [1.0, 0.9565217391304348, 0.2908587257617729, 0.83, 0.0, 0.0, 0.08333333333333333, 0.4345530862856141, 0.4976152984248694, 0.0, 1.0, 0.8, 0.5801439459772824], 
reward next is 0.4199, 
noisyNet noise sample is [array([0.504559], dtype=float32), -0.4783451]. 
=============================================
[2019-04-09 14:59:08,102] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[3.8045163]
 [3.8452637]
 [3.7765481]
 [3.9935718]
 [3.888999 ]], R is [[4.06416798]
 [4.8050127 ]
 [5.517241  ]
 [6.19233465]
 [6.77504444]].
[2019-04-09 14:59:08,106] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03780946 0.08238735 0.09423285 0.01463833 0.23673356 0.13025428
 0.13371946 0.05129343 0.10405467 0.08020522 0.03467136], sum to 1.0000
[2019-04-09 14:59:08,106] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8903
[2019-04-09 14:59:08,151] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.2, 83.0, 0.0, 0.0, 19.0, 23.21463703542737, -0.007154104725391686, 0.0, 1.0, 55.0, 58.01439459772824], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 1980000.0000, 
sim time next is 1981200.0000, 
raw observation next is [-6.0, 83.0, 0.0, 0.0, 19.0, 23.39290998848203, 0.02284320940401252, 0.0, 1.0, 60.0, 63.263312145260485], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.83, 0.0, 0.0, 0.08333333333333333, 0.449409165706836, 0.5076144031346709, 0.0, 1.0, 0.9, 0.6326331214526049], 
reward next is 0.3674, 
noisyNet noise sample is [array([-0.6120395], dtype=float32), 0.53120226]. 
=============================================
[2019-04-09 14:59:08,180] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.1, 83.33333333333334, 64.5, 0.0, 22.5, 24.92825438869873, 0.2333039384269946, 1.0, 1.0, 55.0, 45.582485321581075], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2043600.0000, 
sim time next is 2044800.0000, 
raw observation next is [-3.9, 82.0, 51.5, 0.0, 22.5, 25.02887706685318, 0.1354428868977398, 1.0, 1.0, 45.0, 37.3481929146858], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.17166666666666666, 0.0, 0.375, 0.5857397555710984, 0.5451476289659133, 1.0, 1.0, 0.6, 0.37348192914685796], 
reward next is 0.6265, 
noisyNet noise sample is [array([1.0654564], dtype=float32), -0.8499001]. 
=============================================
[2019-04-09 14:59:08,202] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.03618991 0.05856447 0.08357046 0.01580495 0.2152993  0.14763653
 0.15843822 0.04827859 0.08913023 0.10278894 0.04429841], sum to 1.0000
[2019-04-09 14:59:08,202] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6231
[2019-04-09 14:59:08,225] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.8, 84.33333333333333, 0.0, 0.0, 19.0, 23.57742490986028, -0.07253433601687537, 0.0, 1.0, 35.0, 33.439037833516], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1993200.0000, 
sim time next is 1994400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 23.41735869267762, -0.1139822400415041, 0.0, 1.0, 45.0, 30.13063326759229], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.451446557723135, 0.4620059199861653, 0.0, 1.0, 0.6, 0.3013063326759229], 
reward next is 0.6987, 
noisyNet noise sample is [array([-2.8452787], dtype=float32), -0.07570082]. 
=============================================
[2019-04-09 14:59:08,971] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.02048609 0.07874466 0.10118303 0.01244936 0.24701987 0.15031609
 0.13630171 0.05822561 0.0775308  0.07282516 0.04491765], sum to 1.0000
[2019-04-09 14:59:08,971] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7734
[2019-04-09 14:59:09,007] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.1, 81.33333333333334, 111.3333333333333, 0.0, 22.5, 23.57779197840108, -0.07803607263678829, 1.0, 1.0, 20.0, 19.647537176503363], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2038800.0000, 
sim time next is 2040000.0000, 
raw observation next is [-4.3, 83.66666666666666, 98.5, 0.0, 22.5, 23.71757252090752, -0.02008704425916386, 1.0, 1.0, 50.0, 47.51791133658888], 
processed observation next is [1.0, 0.6086956521739131, 0.34349030470914127, 0.8366666666666666, 0.3283333333333333, 0.0, 0.375, 0.47646437674229336, 0.49330431858027873, 1.0, 1.0, 0.7, 0.4751791133658888], 
reward next is 0.5248, 
noisyNet noise sample is [array([0.87903666], dtype=float32), 1.5618036]. 
=============================================
[2019-04-09 14:59:09,016] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[3.7462254]
 [3.829848 ]
 [3.7609327]
 [3.767848 ]
 [3.9263647]], R is [[4.30700064]
 [5.06745529]
 [5.79379034]
 [6.5003314 ]
 [7.17451048]].
[2019-04-09 14:59:09,065] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0359939  0.08271382 0.09508341 0.01625699 0.22830455 0.14633168
 0.10435869 0.06845807 0.07533544 0.08582004 0.06134335], sum to 1.0000
[2019-04-09 14:59:09,068] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3274
[2019-04-09 14:59:09,099] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.8, 84.33333333333334, 0.0, 0.0, 19.0, 23.57213761645466, -0.01231161073150178, 0.0, 1.0, 45.0, 32.63799847328649], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2002800.0000, 
sim time next is 2004000.0000, 
raw observation next is [-6.0, 85.66666666666667, 0.0, 0.0, 19.0, 23.66634044514278, -0.03554623849881088, 0.0, 1.0, 35.0, 27.198886300958534], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.47219503709523164, 0.48815125383372976, 0.0, 1.0, 0.4, 0.27198886300958536], 
reward next is 0.7280, 
noisyNet noise sample is [array([-0.49595284], dtype=float32), -0.55591995]. 
=============================================
[2019-04-09 14:59:09,103] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[3.310466 ]
 [3.4437394]
 [3.316351 ]
 [3.3085918]
 [3.4296021]], R is [[4.00989342]
 [4.6434145 ]
 [5.1814518 ]
 [5.79794264]
 [6.48390532]].
[2019-04-09 14:59:09,693] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03735094 0.08737014 0.08507343 0.01936911 0.19902925 0.16372436
 0.11068463 0.07353736 0.0867309  0.08428466 0.05284521], sum to 1.0000
[2019-04-09 14:59:09,693] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7154
[2019-04-09 14:59:09,745] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 19.0, 22.30202063391804, -0.4280153667803052, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2008800.0000, 
sim time next is 2010000.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 21.98029889728182, -0.3735658969428658, 0.0, 1.0, 50.0, 56.630070741971466], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.331691574773485, 0.37547803435237803, 0.0, 1.0, 0.7, 0.5663007074197146], 
reward next is 0.4337, 
noisyNet noise sample is [array([1.1317006], dtype=float32), 1.3246436]. 
=============================================
[2019-04-09 14:59:09,755] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[3.254522 ]
 [3.267994 ]
 [3.204485 ]
 [3.2990317]
 [3.222938 ]], R is [[3.64617944]
 [4.60971737]
 [5.28618097]
 [5.92510653]
 [6.34838247]].
[2019-04-09 14:59:10,008] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02427768 0.07695261 0.10032809 0.01343142 0.23355655 0.15649721
 0.12341734 0.04558463 0.09441511 0.077024   0.0545154 ], sum to 1.0000
[2019-04-09 14:59:10,009] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5700
[2019-04-09 14:59:10,058] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.866666666666667, 77.66666666666667, 148.5, 0.0, 22.5, 25.30299340476266, 0.2887025453722622, 1.0, 1.0, 35.0, 39.19341824156342], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2029200.0000, 
sim time next is 2030400.0000, 
raw observation next is [-4.5, 75.0, 151.5, 0.0, 22.5, 25.43665351826113, 0.2974089233463225, 1.0, 1.0, 40.0, 34.76197854092895], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.75, 0.505, 0.0, 0.375, 0.6197211265217609, 0.5991363077821075, 1.0, 1.0, 0.5, 0.3476197854092895], 
reward next is 0.6524, 
noisyNet noise sample is [array([-1.7992089], dtype=float32), -0.0636505]. 
=============================================
[2019-04-09 14:59:11,473] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02725693 0.09153028 0.10462087 0.01570661 0.2452624  0.12654333
 0.1218885  0.03998712 0.09705223 0.08014756 0.05000426], sum to 1.0000
[2019-04-09 14:59:11,486] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1597
[2019-04-09 14:59:11,541] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.6, 78.0, 0.0, 0.0, 19.0, 25.12920366071656, 0.320176019757274, 0.0, 1.0, 20.0, 33.666844840190606], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 1976400.0000, 
sim time next is 1977600.0000, 
raw observation next is [-5.8, 79.66666666666667, 0.0, 0.0, 19.0, 24.99768004552494, 0.2761588885629265, 0.0, 1.0, 25.0, 29.9542603239707], 
processed observation next is [1.0, 0.9130434782608695, 0.30193905817174516, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.5831400037937451, 0.5920529628543089, 0.0, 1.0, 0.2, 0.299542603239707], 
reward next is 0.7005, 
noisyNet noise sample is [array([-2.3557343], dtype=float32), 1.1219336]. 
=============================================
[2019-04-09 14:59:12,202] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03124004 0.06954353 0.12952186 0.01365882 0.20485045 0.13759446
 0.13774218 0.05570158 0.10067112 0.07611191 0.04336412], sum to 1.0000
[2019-04-09 14:59:12,202] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7545
[2019-04-09 14:59:12,265] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.5, 89.33333333333334, 0.0, 0.0, 19.0, 22.31349688693511, -0.335918919842846, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2078400.0000, 
sim time next is 2079600.0000, 
raw observation next is [-4.5, 87.66666666666666, 0.0, 0.0, 19.0, 22.02697569447136, -0.2575140517189274, 0.0, 1.0, 50.0, 59.84359409261599], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.8766666666666666, 0.0, 0.0, 0.08333333333333333, 0.33558130787261337, 0.41416198276035754, 0.0, 1.0, 0.7, 0.5984359409261599], 
reward next is 0.4016, 
noisyNet noise sample is [array([0.96848595], dtype=float32), 0.59281325]. 
=============================================
[2019-04-09 14:59:13,142] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04841297 0.07041585 0.08808949 0.01242039 0.18979172 0.15337507
 0.11760803 0.07692692 0.07912551 0.1076519  0.05618221], sum to 1.0000
[2019-04-09 14:59:13,158] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9435
[2019-04-09 14:59:13,217] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.700000000000001, 81.33333333333334, 0.0, 0.0, 19.0, 21.70896418001225, -0.4016474419824681, 0.0, 1.0, 40.0, 27.90725228889509], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2096400.0000, 
sim time next is 2097600.0000, 
raw observation next is [-6.700000000000001, 79.66666666666667, 0.0, 0.0, 19.0, 21.64000944947483, -0.3756784530567849, 0.0, 1.0, 60.0, 62.89791696179518], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.3033341207895693, 0.3747738489810717, 0.0, 1.0, 0.9, 0.6289791696179519], 
reward next is 0.3710, 
noisyNet noise sample is [array([0.5154161], dtype=float32), 0.057897545]. 
=============================================
[2019-04-09 14:59:13,219] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.05054008 0.06829881 0.08416526 0.01147543 0.19503558 0.14985782
 0.11946847 0.07163173 0.07964103 0.11649557 0.05339021], sum to 1.0000
[2019-04-09 14:59:13,241] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3704
[2019-04-09 14:59:13,288] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 79.66666666666667, 0.0, 0.0, 19.0, 21.64000944947483, -0.3756784530567849, 0.0, 1.0, 60.0, 62.89791696179518], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2097600.0000, 
sim time next is 2098800.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 19.0, 21.830242742907, -0.3495891010196164, 0.0, 1.0, 35.0, 38.35296100533347], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.31918689524225, 0.38347029966012786, 0.0, 1.0, 0.4, 0.3835296100533347], 
reward next is 0.6165, 
noisyNet noise sample is [array([0.5154161], dtype=float32), 0.057897545]. 
=============================================
[2019-04-09 14:59:13,644] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01908121 0.06829011 0.1027713  0.0129949  0.21959035 0.17806949
 0.14724746 0.03903634 0.10388928 0.07483875 0.03419089], sum to 1.0000
[2019-04-09 14:59:13,645] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8807
[2019-04-09 14:59:13,781] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.899999999999999, 86.0, 0.0, 0.0, 19.0, 25.17887749575308, 0.3421561328132032, 0.0, 1.0, 35.0, 27.159983512191573], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2061600.0000, 
sim time next is 2062800.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 19.0, 24.92834665969625, 0.3652221297326653, 0.0, 1.0, 55.0, 53.501595913718646], 
processed observation next is [1.0, 0.9130434782608695, 0.3545706371191136, 0.86, 0.0, 0.0, 0.08333333333333333, 0.5773622216413541, 0.6217407099108884, 0.0, 1.0, 0.8, 0.5350159591371865], 
reward next is 0.4650, 
noisyNet noise sample is [array([-0.4000661], dtype=float32), 0.31663993]. 
=============================================
[2019-04-09 14:59:14,106] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.02111746 0.07915514 0.09909936 0.01205068 0.25967717 0.14394385
 0.14203024 0.05135978 0.08557033 0.05871542 0.04728057], sum to 1.0000
[2019-04-09 14:59:14,107] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2570
[2019-04-09 14:59:14,162] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 65.33333333333334, 149.3333333333333, 22.33333333333333, 22.5, 25.06047723617782, 0.2781220605923734, 1.0, 1.0, 35.0, 27.254523149952433], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2121600.0000, 
sim time next is 2122800.0000, 
raw observation next is [-5.8, 66.66666666666666, 145.0, 0.0, 22.5, 25.19294368903013, 0.2771220550928864, 1.0, 1.0, 35.0, 24.499362650793273], 
processed observation next is [1.0, 0.5652173913043478, 0.30193905817174516, 0.6666666666666665, 0.48333333333333334, 0.0, 0.375, 0.5994119740858442, 0.5923740183642955, 1.0, 1.0, 0.4, 0.24499362650793274], 
reward next is 0.7550, 
noisyNet noise sample is [array([1.7760662], dtype=float32), 0.5296015]. 
=============================================
[2019-04-09 14:59:14,737] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02160151 0.07029346 0.0919927  0.01417936 0.26025152 0.16478498
 0.11479118 0.06302489 0.07928055 0.07538381 0.04441614], sum to 1.0000
[2019-04-09 14:59:14,738] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9931
[2019-04-09 14:59:14,788] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.03674066 0.06328853 0.07798756 0.01521659 0.24765565 0.15222695
 0.1280408  0.07337344 0.08329189 0.0736605  0.04851746], sum to 1.0000
[2019-04-09 14:59:14,788] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0438
[2019-04-09 14:59:14,807] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 75.0, 151.5, 0.0, 22.5, 24.54643896886842, 0.1196436119214369, 1.0, 1.0, 35.0, 30.95383639139652], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2030400.0000, 
sim time next is 2031600.0000, 
raw observation next is [-4.5, 76.33333333333334, 154.5, 0.0, 22.5, 24.65987467542956, 0.07006517647516937, 1.0, 1.0, 65.0, 62.96155723714353], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7633333333333334, 0.515, 0.0, 0.375, 0.5549895562857966, 0.5233550588250565, 1.0, 1.0, 1.0, 0.6296155723714353], 
reward next is 0.3704, 
noisyNet noise sample is [array([0.27528626], dtype=float32), 0.07784796]. 
=============================================
[2019-04-09 14:59:14,830] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.833333333333333, 86.0, 0.0, 0.0, 19.0, 24.21068875646561, 0.07839986109653264, 0.0, 1.0, 35.0, 32.094166732627286], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2083200.0000, 
sim time next is 2084400.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 19.0, 23.97806293605935, 0.03451343725250849, 0.0, 1.0, 20.0, 29.204510193857985], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.86, 0.0, 0.0, 0.08333333333333333, 0.49817191133827904, 0.5115044790841695, 0.0, 1.0, 0.1, 0.29204510193857985], 
reward next is 0.7080, 
noisyNet noise sample is [array([-0.61159354], dtype=float32), 1.6728538]. 
=============================================
[2019-04-09 14:59:15,180] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.02021756 0.04860952 0.07191533 0.00927843 0.25938398 0.17431273
 0.14153609 0.04416624 0.0766708  0.08586271 0.06804667], sum to 1.0000
[2019-04-09 14:59:15,181] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5629
[2019-04-09 14:59:15,325] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.03768018 0.08405522 0.0960265  0.01953156 0.17367691 0.1743506
 0.10487437 0.06386169 0.09621171 0.08263168 0.06709962], sum to 1.0000
[2019-04-09 14:59:15,325] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7579
[2019-04-09 14:59:15,336] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02442663 0.05692701 0.08841518 0.00853546 0.23825711 0.15076548
 0.13432473 0.05018278 0.11891934 0.07106765 0.05817855], sum to 1.0000
[2019-04-09 14:59:15,337] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8949
[2019-04-09 14:59:15,356] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.366666666666667, 64.0, 150.6666666666667, 111.6666666666667, 22.5, 25.54336700576982, 0.2577833886835431, 1.0, 1.0, 55.0, 64.69975903488852], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2119200.0000, 
sim time next is 2120400.0000, 
raw observation next is [-6.2, 64.0, 150.0, 67.0, 22.5, 25.61306595394348, 0.3892449458543975, 1.0, 1.0, 40.0, 38.61433579145698], 
processed observation next is [1.0, 0.5652173913043478, 0.2908587257617729, 0.64, 0.5, 0.07403314917127071, 0.375, 0.6344221628286233, 0.6297483152847992, 1.0, 1.0, 0.5, 0.38614335791456983], 
reward next is 0.6139, 
noisyNet noise sample is [array([0.20116355], dtype=float32), 1.5871451]. 
=============================================
[2019-04-09 14:59:15,376] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.700000000000001, 81.33333333333334, 0.0, 0.0, 19.0, 22.1751087668986, -0.4732133764016939, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2096400.0000, 
sim time next is 2097600.0000, 
raw observation next is [-6.700000000000001, 79.66666666666667, 0.0, 0.0, 19.0, 21.52932761974237, -0.3954553742248519, 0.0, 1.0, 55.0, 68.59817383171995], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.29411063497853096, 0.36818154192504937, 0.0, 1.0, 0.8, 0.6859817383171994], 
reward next is 0.3140, 
noisyNet noise sample is [array([1.653164], dtype=float32), 1.0091065]. 
=============================================
[2019-04-09 14:59:15,416] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.833333333333334, 67.0, 92.5, 0.0, 22.5, 24.20234157059651, 0.05241417306536374, 1.0, 1.0, 35.0, 30.61017881982577], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2128800.0000, 
sim time next is 2130000.0000, 
raw observation next is [-4.666666666666667, 66.0, 76.0, 0.0, 22.5, 23.90295184810946, 0.08932134584866797, 1.0, 1.0, 60.0, 60.549253849309245], 
processed observation next is [1.0, 0.6521739130434783, 0.3333333333333333, 0.66, 0.25333333333333335, 0.0, 0.375, 0.49191265400912165, 0.5297737819495559, 1.0, 1.0, 0.9, 0.6054925384930925], 
reward next is 0.3945, 
noisyNet noise sample is [array([0.77907604], dtype=float32), -0.8566817]. 
=============================================
[2019-04-09 14:59:15,458] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[4.232975 ]
 [4.057681 ]
 [4.1729403]
 [4.2033815]
 [3.9794168]], R is [[4.76459551]
 [5.41084766]
 [5.99288797]
 [6.39410448]
 [7.09779406]].
[2019-04-09 14:59:16,107] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02237548 0.06431633 0.07854643 0.01186708 0.2590621  0.14082074
 0.15506814 0.04861124 0.07622555 0.08837363 0.05473327], sum to 1.0000
[2019-04-09 14:59:16,107] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8999
[2019-04-09 14:59:16,157] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-7.3, 75.0, 250.5, 80.5, 22.5, 24.43857706893999, 0.1043230743641027, 1.0, 1.0, 35.0, 29.840508892596084], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2113200.0000, 
sim time next is 2114400.0000, 
raw observation next is [-7.100000000000001, 71.33333333333334, 278.8333333333334, 94.16666666666667, 22.5, 24.48900003818985, 0.1913369033603796, 1.0, 1.0, 60.0, 59.8310724112511], 
processed observation next is [1.0, 0.4782608695652174, 0.26592797783933514, 0.7133333333333334, 0.9294444444444447, 0.10405156537753224, 0.375, 0.5407500031824876, 0.5637789677867932, 1.0, 1.0, 0.9, 0.598310724112511], 
reward next is 0.4017, 
noisyNet noise sample is [array([0.04449049], dtype=float32), -0.78841275]. 
=============================================
[2019-04-09 14:59:16,859] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02701124 0.06471307 0.07743884 0.00907207 0.22484788 0.17686684
 0.13082509 0.04685695 0.09318142 0.09713628 0.05205034], sum to 1.0000
[2019-04-09 14:59:16,860] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6109
[2019-04-09 14:59:16,964] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 68.0, 129.0, 0.0, 22.5, 24.9283543042085, 0.2046389051289195, 1.0, 1.0, 25.0, 28.14611826885596], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2125200.0000, 
sim time next is 2126400.0000, 
raw observation next is [-5.199999999999999, 68.0, 118.5, 0.0, 22.5, 24.26478723717214, 0.1063730803021155, 1.0, 1.0, 45.0, 30.456588714201324], 
processed observation next is [1.0, 0.6086956521739131, 0.31855955678670367, 0.68, 0.395, 0.0, 0.375, 0.5220656030976784, 0.5354576934340385, 1.0, 1.0, 0.6, 0.3045658871420132], 
reward next is 0.6954, 
noisyNet noise sample is [array([0.85194385], dtype=float32), -0.19536261]. 
=============================================
[2019-04-09 14:59:17,800] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01554902 0.0400937  0.08322656 0.00867205 0.31891933 0.17785694
 0.1179467  0.04451509 0.07787707 0.06506968 0.05027384], sum to 1.0000
[2019-04-09 14:59:17,801] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7202
[2019-04-09 14:59:17,968] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 68.0, 137.0, 0.0, 22.5, 27.1987899052437, 0.6244189424390745, 1.0, 1.0, 60.0, 70.13128634226528], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2124000.0000, 
sim time next is 2125200.0000, 
raw observation next is [-5.4, 68.0, 129.0, 0.0, 22.5, 26.5855201563581, 0.6816353404110663, 1.0, 1.0, 45.0, 38.83475909196917], 
processed observation next is [1.0, 0.6086956521739131, 0.31301939058171746, 0.68, 0.43, 0.0, 0.375, 0.7154600130298417, 0.7272117801370221, 1.0, 1.0, 0.6, 0.3883475909196917], 
reward next is 0.6117, 
noisyNet noise sample is [array([0.38495234], dtype=float32), 0.36184362]. 
=============================================
[2019-04-09 14:59:17,981] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02253869 0.04498637 0.07523279 0.01144443 0.24424124 0.18398397
 0.13780254 0.05673083 0.08603776 0.09690289 0.04009847], sum to 1.0000
[2019-04-09 14:59:17,982] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9061
[2019-04-09 14:59:18,036] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-7.466666666666667, 80.0, 60.16666666666666, 30.83333333333334, 22.5, 22.14558164452072, -0.2683235391260234, 1.0, 1.0, 55.0, 52.30250807563803], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2103600.0000, 
sim time next is 2104800.0000, 
raw observation next is [-7.633333333333333, 81.0, 89.0, 50.5, 22.5, 22.87994663096699, -0.1677840522021189, 1.0, 1.0, 60.0, 57.924676803883195], 
processed observation next is [1.0, 0.34782608695652173, 0.2511542012927055, 0.81, 0.2966666666666667, 0.05580110497237569, 0.375, 0.40666221924724927, 0.44407198259929376, 1.0, 1.0, 0.9, 0.5792467680388319], 
reward next is 0.4208, 
noisyNet noise sample is [array([1.4462235], dtype=float32), -1.6113633]. 
=============================================
[2019-04-09 14:59:18,233] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02736661 0.07028403 0.12049462 0.0129893  0.20648721 0.16334549
 0.10528339 0.07029243 0.10138636 0.0867466  0.03532399], sum to 1.0000
[2019-04-09 14:59:18,233] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9370
[2019-04-09 14:59:18,301] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-7.1, 82.33333333333334, 0.0, 0.0, 19.0, 22.96548834162871, -0.06894219400598849, 0.0, 1.0, 55.0, 64.10105588185942], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2155200.0000, 
sim time next is 2156400.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 19.0, 23.11887792505159, -0.040421471504943, 0.0, 1.0, 60.0, 54.52043624139249], 
processed observation next is [1.0, 1.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.08333333333333333, 0.4265731604209657, 0.48652617616501903, 0.0, 1.0, 0.9, 0.5452043624139249], 
reward next is 0.4548, 
noisyNet noise sample is [array([-0.6041452], dtype=float32), -0.4281372]. 
=============================================
[2019-04-09 14:59:19,625] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03043917 0.07428271 0.09507819 0.01575477 0.23247363 0.16573346
 0.11864357 0.05164838 0.07413124 0.08354443 0.05827036], sum to 1.0000
[2019-04-09 14:59:19,625] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9627
[2019-04-09 14:59:19,684] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.533333333333334, 77.0, 0.0, 0.0, 19.0, 23.49084127545001, 0.01240056709327033, 0.0, 1.0, 60.0, 61.97980699658169], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2175600.0000, 
sim time next is 2176800.0000, 
raw observation next is [-6.366666666666667, 76.0, 0.0, 0.0, 19.0, 23.73000983018486, 0.1118297512323444, 0.0, 1.0, 65.0, 69.49112037802766], 
processed observation next is [1.0, 0.17391304347826086, 0.28624192059095105, 0.76, 0.0, 0.0, 0.08333333333333333, 0.47750081918207155, 0.5372765837441148, 0.0, 1.0, 1.0, 0.6949112037802766], 
reward next is 0.3051, 
noisyNet noise sample is [array([-0.2933904], dtype=float32), 0.106219016]. 
=============================================
[2019-04-09 14:59:20,887] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.02816838 0.08270584 0.08732942 0.0137939  0.22123219 0.1559406
 0.12580797 0.05656447 0.07800793 0.09522259 0.05522675], sum to 1.0000
[2019-04-09 14:59:20,891] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1243
[2019-04-09 14:59:20,919] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 22.39554409278455, -0.2727899934108038, 0.0, 1.0, 50.0, 40.48572147010483], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2173200.0000, 
sim time next is 2174400.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 19.0, 22.36139761487057, -0.2869203740067566, 0.0, 1.0, 25.0, 31.403648717117676], 
processed observation next is [1.0, 0.17391304347826086, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.36344980123921405, 0.40435987533108114, 0.0, 1.0, 0.2, 0.31403648717117677], 
reward next is 0.6860, 
noisyNet noise sample is [array([1.3323485], dtype=float32), -0.5126323]. 
=============================================
[2019-04-09 14:59:20,927] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.02720573 0.08185997 0.07499411 0.01297502 0.21220493 0.18689194
 0.11847936 0.06138135 0.0737887  0.09552681 0.05469211], sum to 1.0000
[2019-04-09 14:59:20,929] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1388
[2019-04-09 14:59:20,974] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 19.0, 22.86186184567076, -0.1581283329845381, 0.0, 1.0, 50.0, 49.14882988613668], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2170800.0000, 
sim time next is 2172000.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 23.0050076858771, -0.1758034257763225, 0.0, 1.0, 25.0, 34.10775224487675], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.41708397382309165, 0.44139885807455914, 0.0, 1.0, 0.2, 0.3410775224487675], 
reward next is 0.6589, 
noisyNet noise sample is [array([1.0617092], dtype=float32), -1.7461101]. 
=============================================
[2019-04-09 14:59:20,997] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[3.7527623]
 [3.6757877]
 [3.8949797]
 [3.993378 ]
 [3.9032137]], R is [[4.27130175]
 [4.73710012]
 [5.39793205]
 [6.05520821]
 [6.73512936]].
[2019-04-09 14:59:21,091] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.02296347 0.06230848 0.11310481 0.01055036 0.21741502 0.15833312
 0.14413612 0.056347   0.08880298 0.07649475 0.04954396], sum to 1.0000
[2019-04-09 14:59:21,091] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1830
[2019-04-09 14:59:21,122] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03604641 0.08333966 0.0817601  0.01352709 0.23806265 0.16431282
 0.10776841 0.06534561 0.07027394 0.08540491 0.05415835], sum to 1.0000
[2019-04-09 14:59:21,122] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5851
[2019-04-09 14:59:21,173] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.566666666666666, 88.33333333333334, 0.0, 0.0, 19.0, 21.00237445757373, -0.494480815297834, 0.0, 1.0, 55.0, 57.38612839189907], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2262000.0000, 
sim time next is 2263200.0000, 
raw observation next is [-8.733333333333334, 89.66666666666667, 0.0, 0.0, 19.0, 21.41857410694209, -0.4662002479072181, 0.0, 1.0, 40.0, 37.44066234283921], 
processed observation next is [1.0, 0.17391304347826086, 0.22068328716528163, 0.8966666666666667, 0.0, 0.0, 0.08333333333333333, 0.2848811755785074, 0.34459991736426066, 0.0, 1.0, 0.5, 0.3744066234283921], 
reward next is 0.6256, 
noisyNet noise sample is [array([-0.5742287], dtype=float32), 2.2331479]. 
=============================================
[2019-04-09 14:59:21,217] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.03004549 0.06516453 0.09226093 0.01175306 0.22065192 0.19710103
 0.09654135 0.04762324 0.0902219  0.08984082 0.05879569], sum to 1.0000
[2019-04-09 14:59:21,228] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8906
[2019-04-09 14:59:21,250] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-9.133333333333333, 89.66666666666667, 38.0, 16.66666666666667, 22.5, 22.10392382802141, -0.2753180297007664, 1.0, 1.0, 60.0, 77.80806195390237], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2276400.0000, 
sim time next is 2277600.0000, 
raw observation next is [-8.766666666666666, 88.33333333333334, 54.33333333333333, 19.83333333333333, 22.5, 23.48556534987582, -0.1371433628312504, 1.0, 1.0, 25.0, 50.61392472369522], 
processed observation next is [1.0, 0.34782608695652173, 0.2197599261311173, 0.8833333333333334, 0.18111111111111108, 0.021915285451197048, 0.375, 0.457130445822985, 0.4542855457229165, 1.0, 1.0, 0.2, 0.5061392472369522], 
reward next is 0.4939, 
noisyNet noise sample is [array([-0.7578051], dtype=float32), -0.2514603]. 
=============================================
[2019-04-09 14:59:21,274] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 19.0, 23.23969073137082, -0.1429889264182825, 0.0, 1.0, 50.0, 40.34338189416406], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2178000.0000, 
sim time next is 2179200.0000, 
raw observation next is [-6.2, 76.33333333333334, 0.0, 0.0, 19.0, 23.16329985130031, -0.1184138655078592, 0.0, 1.0, 60.0, 61.71977925673387], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7633333333333334, 0.0, 0.0, 0.08333333333333333, 0.4302749876083591, 0.4605287114973802, 0.0, 1.0, 0.9, 0.6171977925673388], 
reward next is 0.3828, 
noisyNet noise sample is [array([-0.3867379], dtype=float32), -1.5529238]. 
=============================================
[2019-04-09 14:59:21,437] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02760256 0.0598398  0.08139773 0.01268993 0.25792778 0.16872554
 0.12900294 0.04751674 0.07444401 0.09820774 0.04264524], sum to 1.0000
[2019-04-09 14:59:21,438] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1785
[2019-04-09 14:59:21,564] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 19.0, 21.90898535114412, -0.4441731565623339, 0.0, 1.0, 50.0, 40.96775653500264], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 2271600.0000, 
sim time next is 2272800.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 22.5, 21.75926283393274, -0.4492005005524009, 1.0, 1.0, 30.0, 47.086618908773765], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.375, 0.31327190282772843, 0.3502664998158664, 1.0, 1.0, 0.3, 0.47086618908773764], 
reward next is 0.5291, 
noisyNet noise sample is [array([2.318426], dtype=float32), -0.8715891]. 
=============================================
[2019-04-09 14:59:22,691] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-09 14:59:22,705] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 14:59:22,705] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:59:22,707] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run4
[2019-04-09 14:59:22,729] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 14:59:22,730] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:59:22,731] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run4
[2019-04-09 14:59:22,744] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 14:59:22,746] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 14:59:22,749] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run4
[2019-04-09 14:59:59,218] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.01250249], dtype=float32), 0.025590815]
[2019-04-09 14:59:59,218] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [0.1666666666666667, 94.0, 95.0, 0.0, 22.5, 27.52107897295961, 0.9158885970812124, 1.0, 1.0, 45.0, 25.009534923939395]
[2019-04-09 14:59:59,219] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:59:59,219] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.01783271 0.05611405 0.08801896 0.00910057 0.32777023 0.13898174
 0.13087289 0.04169856 0.08431163 0.06758596 0.03771265], sampled 0.2654799865761014
[2019-04-09 14:59:59,807] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.01250249], dtype=float32), 0.025590815]
[2019-04-09 14:59:59,807] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [1.1, 92.0, 0.0, 0.0, 19.0, 26.32534262831662, 0.6702342575432206, 0.0, 1.0, 15.0, 0.0]
[2019-04-09 14:59:59,807] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 14:59:59,808] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [0.02069765 0.06474081 0.09453277 0.01248218 0.2901957  0.12883571
 0.13082625 0.0503453  0.10437788 0.06796288 0.03500295], sampled 0.29529302655387357
[2019-04-09 15:00:32,956] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.01250249], dtype=float32), 0.025590815]
[2019-04-09 15:00:32,956] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-1.268621776, 65.51561897333333, 211.3465463, 292.1513632166667, 19.0, 21.30870153380895, -0.4353410730940673, 0.0, 1.0, 25.0, 22.590859081684698]
[2019-04-09 15:00:32,956] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:00:32,957] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [0.04007576 0.07957399 0.08585949 0.02322971 0.2001867  0.1337549
 0.14146583 0.07179554 0.08990353 0.08482645 0.04932815], sampled 0.15296186249554866
[2019-04-09 15:00:49,460] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2910.0102 130478.2382 1112.7291
[2019-04-09 15:00:49,494] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:00:49,494] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:00:49,494] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:00:49,494] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:00:49,605] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:00:49,605] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:00:49,605] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:00:49,605] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:01,835] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2859.9954 138410.4608 801.8355
[2019-04-09 15:01:01,856] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:01,856] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:01,856] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:01,856] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:01,967] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:01,967] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:01,967] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:01,967] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:03,261] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2821.8264 141914.0498 634.4875
[2019-04-09 15:01:03,280] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:03,280] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:03,280] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:03,280] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:01:03,391] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:03,391] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:03,391] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:03,391] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:01:04,283] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 30000, evaluation results [30000.0, 2859.995372366122, 138410.46077334232, 801.8354762636618, 2910.0101960001844, 130478.23824111494, 1112.729138797856, 2821.826350544848, 141914.04982341483, 634.4875112842769]
[2019-04-09 15:01:04,708] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01628454 0.06156904 0.07149377 0.00900191 0.3368341  0.14380321
 0.12576476 0.04200644 0.07961431 0.07101306 0.0426149 ], sum to 1.0000
[2019-04-09 15:01:04,708] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4049
[2019-04-09 15:01:04,802] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.566666666666667, 67.0, 141.3333333333333, 0.0, 22.5, 25.34475675073514, 0.3731765387767609, 1.0, 1.0, 60.0, 56.61450757182647], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2208000.0000, 
sim time next is 2209200.0000, 
raw observation next is [-3.733333333333333, 69.0, 140.0, 0.0, 22.5, 25.68197763651287, 0.4222995184732911, 1.0, 1.0, 35.0, 43.753021832547915], 
processed observation next is [1.0, 0.5652173913043478, 0.35918744228993543, 0.69, 0.4666666666666667, 0.0, 0.375, 0.640164803042739, 0.6407665061577638, 1.0, 1.0, 0.4, 0.43753021832547917], 
reward next is 0.5625, 
noisyNet noise sample is [array([-0.04732548], dtype=float32), -1.7331339]. 
=============================================
[2019-04-09 15:01:06,303] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.024139   0.05264008 0.08281922 0.00945512 0.25090966 0.17832173
 0.126349   0.05308129 0.123557   0.05970041 0.03902747], sum to 1.0000
[2019-04-09 15:01:06,303] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4091
[2019-04-09 15:01:06,485] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.899999999999999, 68.0, 136.6666666666667, 237.5, 22.5, 25.81305756278682, 0.4071009294810199, 1.0, 1.0, 20.0, 36.55612716738913], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2216400.0000, 
sim time next is 2217600.0000, 
raw observation next is [-3.9, 68.0, 96.0, 142.5, 22.5, 26.010865977237, 0.4017494809926377, 1.0, 1.0, 35.0, 31.43070667336104], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.68, 0.32, 0.1574585635359116, 0.375, 0.66757216476975, 0.6339164936642125, 1.0, 1.0, 0.4, 0.3143070667336104], 
reward next is 0.6857, 
noisyNet noise sample is [array([1.0113031], dtype=float32), -0.10738933]. 
=============================================
[2019-04-09 15:01:06,570] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.02324471 0.04208019 0.0983861  0.00845053 0.25474703 0.15658979
 0.15612854 0.04381173 0.0944313  0.07766245 0.04446768], sum to 1.0000
[2019-04-09 15:01:06,570] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7908
[2019-04-09 15:01:06,630] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01715693 0.04909942 0.09736352 0.00675822 0.26202482 0.11629873
 0.21769054 0.04008056 0.10115124 0.05514916 0.03722681], sum to 1.0000
[2019-04-09 15:01:06,630] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8913
[2019-04-09 15:01:06,669] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.2, 54.0, 0.0, 0.0, 22.5, 25.66475624955845, 0.4528911800082246, 1.0, 1.0, 35.0, 19.872438478838582], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2314800.0000, 
sim time next is 2316000.0000, 
raw observation next is [-1.366666666666667, 54.66666666666667, 0.0, 0.0, 22.5, 25.58448450898535, 0.4712422272098282, 0.0, 1.0, 55.0, 55.35066650872457], 
processed observation next is [1.0, 0.8260869565217391, 0.42474607571560485, 0.5466666666666667, 0.0, 0.0, 0.375, 0.6320403757487792, 0.6570807424032761, 0.0, 1.0, 0.8, 0.5535066650872458], 
reward next is 0.4465, 
noisyNet noise sample is [array([0.0599466], dtype=float32), -1.4716158]. 
=============================================
[2019-04-09 15:01:06,673] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[5.070534 ]
 [5.13775  ]
 [5.233665 ]
 [5.1175294]
 [5.1420474]], R is [[5.65371037]
 [6.39844894]
 [7.10990906]
 [7.6302042 ]
 [8.55390167]].
[2019-04-09 15:01:06,692] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.133333333333335, 74.66666666666667, 165.1666666666667, 48.16666666666667, 22.5, 25.34598784478104, 0.2334370997214847, 1.0, 1.0, 35.0, 38.343413803734805], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2283600.0000, 
sim time next is 2284800.0000, 
raw observation next is [-5.566666666666666, 71.33333333333333, 175.1666666666667, 60.0, 22.5, 25.4163307306019, 0.2942080553294419, 1.0, 1.0, 55.0, 56.700792941966824], 
processed observation next is [1.0, 0.43478260869565216, 0.3084025854108957, 0.7133333333333333, 0.583888888888889, 0.06629834254143646, 0.375, 0.6180275608834916, 0.5980693517764807, 1.0, 1.0, 0.8, 0.5670079294196683], 
reward next is 0.4330, 
noisyNet noise sample is [array([-1.4281063], dtype=float32), -0.5218263]. 
=============================================
[2019-04-09 15:01:06,872] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01976051 0.06694242 0.08812124 0.01018049 0.25840545 0.1291958
 0.12506308 0.04690218 0.10963732 0.09974806 0.04604349], sum to 1.0000
[2019-04-09 15:01:06,872] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5745
[2019-04-09 15:01:06,931] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 77.0, 0.0, 0.0, 19.0, 22.25623340748288, -0.2135585842637177, 0.0, 1.0, 60.0, 61.357246936862], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2247600.0000, 
sim time next is 2248800.0000, 
raw observation next is [-6.700000000000001, 76.0, 0.0, 0.0, 19.0, 22.51667185255614, -0.1850145335229164, 0.0, 1.0, 45.0, 37.62908820821113], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.76, 0.0, 0.0, 0.08333333333333333, 0.376389321046345, 0.4383284888256946, 0.0, 1.0, 0.6, 0.3762908820821113], 
reward next is 0.6237, 
noisyNet noise sample is [array([-1.2479228], dtype=float32), -1.5596926]. 
=============================================
[2019-04-09 15:01:07,476] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01526039 0.06760561 0.08949502 0.01044852 0.32789803 0.13743606
 0.1297084  0.04778511 0.07520153 0.07036791 0.02879343], sum to 1.0000
[2019-04-09 15:01:07,479] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4785
[2019-04-09 15:01:07,509] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 64.0, 0.0, 0.0, 19.0, 24.97088287247756, 0.3318591325388517, 0.0, 1.0, 65.0, 63.44128282155968], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2334000.0000, 
sim time next is 2335200.0000, 
raw observation next is [-2.3, 63.0, 0.0, 0.0, 19.0, 25.0909376237728, 0.3468973163869759, 0.0, 1.0, 35.0, 39.272657399505064], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.63, 0.0, 0.0, 0.08333333333333333, 0.5909114686477333, 0.6156324387956587, 0.0, 1.0, 0.4, 0.3927265739950506], 
reward next is 0.6073, 
noisyNet noise sample is [array([-0.49577624], dtype=float32), -0.0030597232]. 
=============================================
[2019-04-09 15:01:07,605] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.04240798 0.0798538  0.08632316 0.02184252 0.23627813 0.12324451
 0.14239034 0.06430551 0.07553054 0.08029436 0.0475291 ], sum to 1.0000
[2019-04-09 15:01:07,608] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0280
[2019-04-09 15:01:07,645] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-3.2, 67.66666666666667, 0.0, 0.0, 19.0, 21.53486831478221, -0.399017022975338, 0.0, 1.0, 50.0, 44.514967895923874], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 2352000.0000, 
sim time next is 2353200.0000, 
raw observation next is [-3.0, 66.33333333333333, 0.0, 0.0, 19.0, 21.55549455371462, -0.5130864067005906, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.3795013850415513, 0.6633333333333333, 0.0, 0.0, 0.08333333333333333, 0.2962912128095517, 0.3289711977664698, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.85839325], dtype=float32), 0.7042242]. 
=============================================
[2019-04-09 15:01:07,752] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01425234 0.0616933  0.08184293 0.00655585 0.2864678  0.14695062
 0.15692183 0.04137948 0.08600102 0.07498558 0.04294929], sum to 1.0000
[2019-04-09 15:01:07,756] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1006
[2019-04-09 15:01:07,913] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.7, 55.66666666666667, 245.1666666666667, 80.0, 22.5, 25.13919519635578, 0.3138748785075057, 1.0, 1.0, 65.0, 60.84636254399325], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2290800.0000, 
sim time next is 2292000.0000, 
raw observation next is [-2.2, 53.33333333333334, 255.1666666666667, 73.16666666666667, 22.5, 25.31983158532798, 0.2876924997199879, 1.0, 1.0, 25.0, 35.590569890257036], 
processed observation next is [1.0, 0.5217391304347826, 0.4016620498614959, 0.5333333333333334, 0.8505555555555557, 0.08084714548802947, 0.375, 0.6099859654439982, 0.5958974999066626, 1.0, 1.0, 0.2, 0.35590569890257034], 
reward next is 0.6441, 
noisyNet noise sample is [array([0.9555855], dtype=float32), -1.1566684]. 
=============================================
[2019-04-09 15:01:07,915] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02155058 0.06227031 0.09225694 0.0105563  0.26059845 0.1502384
 0.14969553 0.04161964 0.09212352 0.079469   0.03962139], sum to 1.0000
[2019-04-09 15:01:07,915] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9172
[2019-04-09 15:01:07,928] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[4.9709606]
 [4.9303384]
 [4.874482 ]
 [4.8742013]
 [4.8905444]], R is [[5.4605732 ]
 [5.79750395]
 [6.53487015]
 [7.23911238]
 [7.88750648]].
[2019-04-09 15:01:07,967] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-1.7, 56.0, 0.0, 0.0, 19.0, 23.41679936139785, 0.003157742094628523, 0.0, 1.0, 40.0, 22.877162783836546], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2325600.0000, 
sim time next is 2326800.0000, 
raw observation next is [-1.9, 57.0, 0.0, 0.0, 19.0, 23.33568244229693, -0.01403706357729939, 0.0, 1.0, 60.0, 53.41251033969699], 
processed observation next is [1.0, 0.9565217391304348, 0.4099722991689751, 0.57, 0.0, 0.0, 0.08333333333333333, 0.44464020352474404, 0.4953209788075669, 0.0, 1.0, 0.9, 0.53412510339697], 
reward next is 0.4659, 
noisyNet noise sample is [array([-0.8839095], dtype=float32), 1.1711502]. 
=============================================
[2019-04-09 15:01:08,316] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02575583 0.076078   0.09087231 0.01165557 0.22964294 0.17075023
 0.12525406 0.05631994 0.08445688 0.07870156 0.05051267], sum to 1.0000
[2019-04-09 15:01:08,318] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9120
[2019-04-09 15:01:08,373] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-8.733333333333334, 89.66666666666667, 0.0, 0.0, 19.0, 22.76082250697345, -0.1828758221646416, 0.0, 1.0, 50.0, 43.29028488096938], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2263200.0000, 
sim time next is 2264400.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 19.0, 22.79960301060547, -0.1428197167114861, 0.0, 1.0, 60.0, 62.686733716735105], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.08333333333333333, 0.3999669175504558, 0.4523934277628379, 0.0, 1.0, 0.9, 0.6268673371673511], 
reward next is 0.3731, 
noisyNet noise sample is [array([0.6658395], dtype=float32), 0.2535344]. 
=============================================
[2019-04-09 15:01:08,637] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01193779 0.05575828 0.08470509 0.00913259 0.3183449  0.14801775
 0.1392728  0.03608015 0.07789472 0.07979739 0.03905851], sum to 1.0000
[2019-04-09 15:01:08,637] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6101
[2019-04-09 15:01:08,675] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 53.33333333333334, 0.0, 0.0, 22.5, 24.99555267688316, 0.3675230296484016, 1.0, 1.0, 55.0, 48.44348741104492], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2313600.0000, 
sim time next is 2314800.0000, 
raw observation next is [-1.2, 54.0, 0.0, 0.0, 22.5, 25.20575306008243, 0.3896830566942414, 1.0, 1.0, 40.0, 28.026484004926857], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.54, 0.0, 0.0, 0.375, 0.6004794216735357, 0.6298943522314139, 1.0, 1.0, 0.5, 0.28026484004926855], 
reward next is 0.7197, 
noisyNet noise sample is [array([2.0751214], dtype=float32), -1.7409948]. 
=============================================
[2019-04-09 15:01:09,201] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.03017177 0.07383141 0.09455654 0.01879113 0.23702715 0.1401922
 0.13848695 0.05605106 0.08018657 0.08277346 0.04793187], sum to 1.0000
[2019-04-09 15:01:09,201] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5540
[2019-04-09 15:01:09,247] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 47.0, 209.5, 365.0, 19.0, 23.73128130515882, 0.1156672597863454, 0.0, 1.0, 35.0, 36.99160512758941], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2376000.0000, 
sim time next is 2377200.0000, 
raw observation next is [-1.0, 49.33333333333334, 237.8333333333333, 404.3333333333334, 19.0, 24.01561045623831, 0.1394525069129854, 0.0, 1.0, 40.0, 30.145578021342754], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.4933333333333334, 0.7927777777777776, 0.44677716390423583, 0.08333333333333333, 0.5013008713531925, 0.5464841689709952, 0.0, 1.0, 0.5, 0.30145578021342756], 
reward next is 0.6985, 
noisyNet noise sample is [array([0.541008], dtype=float32), -0.9382463]. 
=============================================
[2019-04-09 15:01:09,327] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0101638  0.06843997 0.07851177 0.00521433 0.281169   0.1817729
 0.12783532 0.03246535 0.11465075 0.06393597 0.03584081], sum to 1.0000
[2019-04-09 15:01:09,327] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3528
[2019-04-09 15:01:09,424] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 44.0, 89.5, 21.0, 22.5, 27.4672615511117, 0.7367464335939263, 1.0, 1.0, 45.0, 32.59362641636514], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2304000.0000, 
sim time next is 2305200.0000, 
raw observation next is [-0.2, 45.66666666666667, 57.16666666666667, 6.999999999999998, 22.5, 27.53796027639876, 0.7416506745182435, 1.0, 1.0, 35.0, 32.61816537579619], 
processed observation next is [1.0, 0.6956521739130435, 0.4570637119113574, 0.4566666666666667, 0.19055555555555556, 0.007734806629834252, 0.375, 0.79483002303323, 0.7472168915060812, 1.0, 1.0, 0.4, 0.3261816537579619], 
reward next is 0.6738, 
noisyNet noise sample is [array([-0.30880517], dtype=float32), 0.41119736]. 
=============================================
[2019-04-09 15:01:09,640] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 31231: loss 19.0443
[2019-04-09 15:01:09,641] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 31231: learning rate 0.0000
[2019-04-09 15:01:09,710] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 31254: loss 25.6833
[2019-04-09 15:01:09,711] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 31254: learning rate 0.0000
[2019-04-09 15:01:10,021] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01627754 0.06851036 0.07773253 0.01214398 0.23904704 0.19895652
 0.11169229 0.05161045 0.11201403 0.06659908 0.04541619], sum to 1.0000
[2019-04-09 15:01:10,036] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3139
[2019-04-09 15:01:10,076] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 54.0, 0.0, 0.0, 19.0, 24.04687489693517, 0.1105343481410285, 0.0, 1.0, 20.0, 21.6725090112636], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2322000.0000, 
sim time next is 2323200.0000, 
raw observation next is [-1.7, 54.66666666666667, 0.0, 0.0, 19.0, 23.87334356518107, 0.07456802593756821, 0.0, 1.0, 35.0, 19.79537853500409], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5466666666666667, 0.0, 0.0, 0.08333333333333333, 0.4894452970984225, 0.5248560086458561, 0.0, 1.0, 0.4, 0.1979537853500409], 
reward next is 0.8020, 
noisyNet noise sample is [array([0.2622323], dtype=float32), 0.6010521]. 
=============================================
[2019-04-09 15:01:10,176] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31396: loss 20.2984
[2019-04-09 15:01:10,177] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31396: learning rate 0.0000
[2019-04-09 15:01:10,426] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2000, global step 31482: loss 25.2005
[2019-04-09 15:01:10,428] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2000, global step 31483: learning rate 0.0000
[2019-04-09 15:01:10,452] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 31489: loss 21.2552
[2019-04-09 15:01:10,454] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 31490: learning rate 0.0000
[2019-04-09 15:01:10,474] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02493305 0.06058574 0.09567676 0.01573657 0.2541974  0.14095965
 0.14567046 0.04725972 0.096879   0.07808103 0.04002067], sum to 1.0000
[2019-04-09 15:01:10,496] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5165
[2019-04-09 15:01:10,538] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.4, 45.66666666666667, 66.83333333333334, 51.0, 19.0, 22.78909592165383, -0.1794590592875066, 0.0, 1.0, 65.0, 58.16136769751264], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2392800.0000, 
sim time next is 2394000.0000, 
raw observation next is [-0.6, 45.0, 42.5, 37.0, 19.0, 22.88479344211668, -0.1639365013026119, 0.0, 1.0, 40.0, 34.48408232888879], 
processed observation next is [0.0, 0.7391304347826086, 0.44598337950138506, 0.45, 0.14166666666666666, 0.04088397790055249, 0.08333333333333333, 0.4070661201763901, 0.44535449956579604, 0.0, 1.0, 0.5, 0.3448408232888879], 
reward next is 0.6552, 
noisyNet noise sample is [array([-2.3124323], dtype=float32), 0.43700224]. 
=============================================
[2019-04-09 15:01:10,572] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[3.8491366]
 [3.5356426]
 [3.7828388]
 [3.645134 ]
 [3.687467 ]], R is [[4.35438061]
 [4.72922325]
 [5.46343708]
 [6.15988636]
 [6.83466434]].
[2019-04-09 15:01:10,764] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 31590: loss 24.3281
[2019-04-09 15:01:10,765] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 31590: learning rate 0.0000
[2019-04-09 15:01:10,797] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31601: loss 21.9836
[2019-04-09 15:01:10,799] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 31601: learning rate 0.0000
[2019-04-09 15:01:11,128] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.02081089 0.07340977 0.09342264 0.01243159 0.27391157 0.12118889
 0.15543458 0.05318175 0.09843066 0.06758673 0.03019094], sum to 1.0000
[2019-04-09 15:01:11,128] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4345
[2019-04-09 15:01:11,162] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 23.16792302977831, -0.1193002451385102, 0.0, 1.0, 45.0, 28.60296182287783], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2337600.0000, 
sim time next is 2338800.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 19.0, 23.04087185520495, -0.1569891913123975, 0.0, 1.0, 35.0, 26.08660890618052], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.08333333333333333, 0.4200726546004126, 0.44767026956253414, 0.0, 1.0, 0.4, 0.2608660890618052], 
reward next is 0.7391, 
noisyNet noise sample is [array([0.98745644], dtype=float32), 0.2827463]. 
=============================================
[2019-04-09 15:01:11,445] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02803461 0.07083835 0.07607694 0.02078537 0.29438922 0.11216033
 0.12945618 0.0575442  0.07634448 0.09041062 0.04395982], sum to 1.0000
[2019-04-09 15:01:11,446] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1013
[2019-04-09 15:01:11,474] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 22.56445674669556, -0.2510347025036699, 0.0, 1.0, 40.0, 24.218010934208422], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2342400.0000, 
sim time next is 2343600.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 19.0, 22.34577474303888, -0.292734593320378, 0.0, 1.0, 25.0, 20.219865744190272], 
processed observation next is [0.0, 0.13043478260869565, 0.3988919667590028, 0.62, 0.0, 0.0, 0.08333333333333333, 0.36214789525324004, 0.40242180222654067, 0.0, 1.0, 0.2, 0.2021986574419027], 
reward next is 0.7978, 
noisyNet noise sample is [array([-0.834616], dtype=float32), 0.9653027]. 
=============================================
[2019-04-09 15:01:11,573] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31863: loss 18.3920
[2019-04-09 15:01:11,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31863: learning rate 0.0000
[2019-04-09 15:01:11,615] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31872: loss 33.8755
[2019-04-09 15:01:11,616] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31872: learning rate 0.0000
[2019-04-09 15:01:11,970] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.03674916 0.08454297 0.09435263 0.02132689 0.21204965 0.13670194
 0.11708241 0.05900647 0.08222093 0.09904939 0.05691753], sum to 1.0000
[2019-04-09 15:01:11,971] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0597
[2019-04-09 15:01:11,998] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.566666666666666, 60.66666666666667, 0.0, 0.0, 19.0, 21.44020041378107, -0.5611400534425557, 0.0, 1.0, 35.0, 32.2838018230571], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2438400.0000, 
sim time next is 2439600.0000, 
raw observation next is [-8.733333333333334, 60.33333333333334, 0.0, 0.0, 19.0, 21.30518258671344, -0.6033216391758826, 0.0, 1.0, 20.0, 29.038005793979607], 
processed observation next is [0.0, 0.21739130434782608, 0.22068328716528163, 0.6033333333333334, 0.0, 0.0, 0.08333333333333333, 0.2754318822261199, 0.2988927869413725, 0.0, 1.0, 0.1, 0.2903800579397961], 
reward next is 0.7096, 
noisyNet noise sample is [array([-0.12313439], dtype=float32), -0.98998547]. 
=============================================
[2019-04-09 15:01:12,002] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03483772 0.07092748 0.07655124 0.02040031 0.22955811 0.14317645
 0.13923797 0.06091738 0.07036029 0.09645433 0.05757876], sum to 1.0000
[2019-04-09 15:01:12,002] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0876
[2019-04-09 15:01:12,054] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02650518 0.06685594 0.08458275 0.0227159  0.23373206 0.136645
 0.15711033 0.05899034 0.07347903 0.09113631 0.04824714], sum to 1.0000
[2019-04-09 15:01:12,056] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.8, 55.0, 0.0, 0.0, 19.0, 22.02582092752577, -0.3618323041131289, 0.0, 1.0, 65.0, 70.40738177533865], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2430000.0000, 
sim time next is 2431200.0000, 
raw observation next is [-8.0, 57.0, 0.0, 0.0, 19.0, 22.44354860748663, -0.3418629499476996, 0.0, 1.0, 20.0, 48.580447244537694], 
processed observation next is [0.0, 0.13043478260869565, 0.24099722991689754, 0.57, 0.0, 0.0, 0.08333333333333333, 0.3702957172905525, 0.38604568335076683, 0.0, 1.0, 0.1, 0.48580447244537694], 
reward next is 0.5142, 
noisyNet noise sample is [array([1.3370705], dtype=float32), 0.96306735]. 
=============================================
[2019-04-09 15:01:12,069] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2204
[2019-04-09 15:01:12,093] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 32033: loss 25.8039
[2019-04-09 15:01:12,097] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 32033: learning rate 0.0000
[2019-04-09 15:01:12,097] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.733333333333333, 34.33333333333334, 84.33333333333334, 820.3333333333334, 19.0, 21.45776212446052, -0.5145517582791607, 0.0, 1.0, 35.0, 23.326620568702623], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2460000.0000, 
sim time next is 2461200.0000, 
raw observation next is [-1.166666666666667, 32.66666666666666, 86.66666666666667, 831.6666666666667, 19.0, 21.41978546398363, -0.5055115086197586, 0.0, 1.0, 35.0, 20.617123948081343], 
processed observation next is [0.0, 0.4782608695652174, 0.43028624192059095, 0.32666666666666655, 0.2888888888888889, 0.9189686924493555, 0.08333333333333333, 0.28498212199863576, 0.3314961637934138, 0.0, 1.0, 0.4, 0.20617123948081342], 
reward next is 0.7938, 
noisyNet noise sample is [array([0.34793663], dtype=float32), -0.90705085]. 
=============================================
[2019-04-09 15:01:12,766] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0404559  0.08898035 0.07807221 0.02086316 0.22944152 0.15224937
 0.12410127 0.07245467 0.07473496 0.06813672 0.05050985], sum to 1.0000
[2019-04-09 15:01:12,766] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3955
[2019-04-09 15:01:12,805] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.633333333333333, 54.33333333333333, 0.0, 0.0, 19.0, 21.83518862600933, -0.4636827675426504, 0.0, 1.0, 65.0, 68.54547165078358], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2428800.0000, 
sim time next is 2430000.0000, 
raw observation next is [-7.8, 55.0, 0.0, 0.0, 19.0, 21.90455479204751, -0.4511376564758393, 0.0, 1.0, 35.0, 43.94902657819004], 
processed observation next is [0.0, 0.13043478260869565, 0.24653739612188366, 0.55, 0.0, 0.0, 0.08333333333333333, 0.3253795660039591, 0.34962078117472023, 0.0, 1.0, 0.4, 0.4394902657819004], 
reward next is 0.5605, 
noisyNet noise sample is [array([-0.24948977], dtype=float32), -0.19965747]. 
=============================================
[2019-04-09 15:01:12,825] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[3.0931745]
 [3.3605742]
 [3.3852968]
 [3.3041885]
 [3.4796414]], R is [[3.55370474]
 [3.83271313]
 [4.46131849]
 [5.04760695]
 [5.58419895]].
[2019-04-09 15:01:12,945] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 32306: loss 22.6101
[2019-04-09 15:01:12,947] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 32306: learning rate 0.0000
[2019-04-09 15:01:13,414] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02065961 0.0550054  0.07205872 0.01584382 0.35772973 0.13114515
 0.11566122 0.04547914 0.08217346 0.07403871 0.03020496], sum to 1.0000
[2019-04-09 15:01:13,415] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3558
[2019-04-09 15:01:13,442] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 45.0, 42.5, 37.0, 19.0, 25.11085255603824, 0.2925518694520972, 0.0, 1.0, 40.0, 30.91935518050768], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2394000.0000, 
sim time next is 2395200.0000, 
raw observation next is [-0.9666666666666667, 44.66666666666667, 18.16666666666666, 23.0, 19.0, 24.94081107334986, 0.2458659190063749, 0.0, 1.0, 35.0, 28.090486610664477], 
processed observation next is [0.0, 0.7391304347826086, 0.43582640812557716, 0.4466666666666667, 0.060555555555555536, 0.02541436464088398, 0.08333333333333333, 0.578400922779155, 0.5819553063354582, 0.0, 1.0, 0.4, 0.2809048661066448], 
reward next is 0.7191, 
noisyNet noise sample is [array([-2.5694826], dtype=float32), 0.12413845]. 
=============================================
[2019-04-09 15:01:13,488] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32493: loss 20.9732
[2019-04-09 15:01:13,489] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32494: learning rate 0.0000
[2019-04-09 15:01:13,506] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02503551 0.06852049 0.07534158 0.01607018 0.25279963 0.12588587
 0.16998532 0.05119033 0.10834481 0.06716605 0.03966011], sum to 1.0000
[2019-04-09 15:01:13,507] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3129
[2019-04-09 15:01:13,527] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.9666666666666667, 44.66666666666667, 18.16666666666666, 23.0, 19.0, 22.77370848655957, -0.1600718127862953, 0.0, 1.0, 50.0, 39.9786924276313], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2395200.0000, 
sim time next is 2396400.0000, 
raw observation next is [-1.333333333333333, 44.33333333333334, 0.0, 0.0, 19.0, 22.88256417190178, -0.165823824492589, 0.0, 1.0, 35.0, 30.7198361467225], 
processed observation next is [0.0, 0.7391304347826086, 0.42566943674976926, 0.4433333333333334, 0.0, 0.0, 0.08333333333333333, 0.4068803476584817, 0.4447253918358037, 0.0, 1.0, 0.4, 0.307198361467225], 
reward next is 0.6928, 
noisyNet noise sample is [array([-0.7849106], dtype=float32), 1.2353686]. 
=============================================
[2019-04-09 15:01:13,622] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32538: loss 16.9368
[2019-04-09 15:01:13,623] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32538: learning rate 0.0000
[2019-04-09 15:01:13,677] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2000, global step 32558: loss 21.4039
[2019-04-09 15:01:13,678] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2000, global step 32558: learning rate 0.0000
[2019-04-09 15:01:14,128] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02608245 0.07896543 0.09444775 0.01956036 0.24434005 0.14652346
 0.1411933  0.05730557 0.06470778 0.07781905 0.04905479], sum to 1.0000
[2019-04-09 15:01:14,137] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9132
[2019-04-09 15:01:14,166] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-2.3, 36.0, 81.0, 803.0, 19.0, 20.94846225631622, -0.598761121621553, 0.0, 1.0, 40.0, 26.327786350646193], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2458800.0000, 
sim time next is 2460000.0000, 
raw observation next is [-1.733333333333333, 34.33333333333334, 84.33333333333334, 820.3333333333334, 19.0, 21.05359809645034, -0.5082766711068709, 0.0, 1.0, 60.0, 53.20523465779848], 
processed observation next is [0.0, 0.4782608695652174, 0.41458910433979695, 0.34333333333333343, 0.28111111111111114, 0.9064456721915286, 0.08333333333333333, 0.25446650803752835, 0.3305744429643764, 0.0, 1.0, 0.9, 0.5320523465779847], 
reward next is 0.4679, 
noisyNet noise sample is [array([-1.2167253], dtype=float32), 0.5489119]. 
=============================================
[2019-04-09 15:01:14,169] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[3.2924414]
 [3.144775 ]
 [3.2263846]
 [3.3504903]
 [3.28842  ]], R is [[3.82843781]
 [4.5268755 ]
 [5.27018547]
 [5.97895908]
 [6.64963818]].
[2019-04-09 15:01:14,445] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01955977 0.08017219 0.07776111 0.01667823 0.24261938 0.15004742
 0.1262536  0.05299251 0.10989315 0.08812138 0.03590114], sum to 1.0000
[2019-04-09 15:01:14,445] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7243
[2019-04-09 15:01:14,461] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.03094685 0.05864113 0.07545955 0.0132955  0.24937725 0.15283385
 0.14952365 0.04987827 0.07663642 0.08653443 0.05687317], sum to 1.0000
[2019-04-09 15:01:14,461] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0127
[2019-04-09 15:01:14,474] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-1.9, 51.66666666666667, 0.0, 0.0, 19.0, 23.45240575209581, -0.1611208421637251, 0.0, 1.0, 40.0, 33.31988810493789], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 2521200.0000, 
sim time next is 2522400.0000, 
raw observation next is [-2.1, 54.33333333333334, 0.0, 0.0, 19.0, 23.24887770613532, -0.3038555033015729, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.404432132963989, 0.5433333333333334, 0.0, 0.0, 0.08333333333333333, 0.43740647551127676, 0.39871483223280907, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7715137], dtype=float32), -1.4114956]. 
=============================================
[2019-04-09 15:01:14,496] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.0, 47.0, 82.5, 199.5, 19.0, 23.95333915729168, 0.0343354051828663, 0.0, 1.0, 45.0, 30.531923024566197], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2390400.0000, 
sim time next is 2391600.0000, 
raw observation next is [-0.2, 46.33333333333334, 80.16666666666667, 105.1666666666667, 19.0, 23.81808429944789, 0.01896006314241307, 0.0, 1.0, 50.0, 37.182332956902975], 
processed observation next is [0.0, 0.6956521739130435, 0.4570637119113574, 0.46333333333333343, 0.26722222222222225, 0.11620626151012894, 0.08333333333333333, 0.48484035828732414, 0.5063200210474711, 0.0, 1.0, 0.7, 0.37182332956902975], 
reward next is 0.6282, 
noisyNet noise sample is [array([-0.6655663], dtype=float32), 0.12457479]. 
=============================================
[2019-04-09 15:01:14,538] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0320518  0.05773867 0.06930882 0.01228891 0.25019833 0.15158418
 0.15282217 0.04947939 0.07898132 0.09215847 0.05338789], sum to 1.0000
[2019-04-09 15:01:14,538] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7745
[2019-04-09 15:01:14,556] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01606436 0.04842956 0.07159599 0.01558374 0.34693876 0.13358971
 0.14491382 0.04121616 0.07303487 0.07085641 0.03777663], sum to 1.0000
[2019-04-09 15:01:14,557] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7364
[2019-04-09 15:01:14,576] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.3, 57.0, 0.0, 0.0, 19.0, 22.89234035943247, -0.1802190008508493, 0.0, 1.0, 60.0, 70.44729499734717], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2523600.0000, 
sim time next is 2524800.0000, 
raw observation next is [-2.3, 57.00000000000001, 0.0, 0.0, 19.0, 23.40514724361872, -0.1404245111845189, 0.0, 1.0, 50.0, 39.596433429869876], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.5700000000000001, 0.0, 0.0, 0.08333333333333333, 0.4504289369682268, 0.4531918296051604, 0.0, 1.0, 0.7, 0.39596433429869876], 
reward next is 0.6040, 
noisyNet noise sample is [array([1.7715137], dtype=float32), -1.4114956]. 
=============================================
[2019-04-09 15:01:14,595] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 27.0, 82.5, 808.0, 19.0, 24.49478737825509, 0.160335094425367, 0.0, 1.0, 35.0, 32.70287854797869], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2469600.0000, 
sim time next is 2470800.0000, 
raw observation next is [2.566666666666667, 27.0, 79.5, 792.0, 19.0, 24.51464370070721, 0.1599381722912589, 0.0, 1.0, 45.0, 29.24085667069167], 
processed observation next is [0.0, 0.6086956521739131, 0.5337026777469991, 0.27, 0.265, 0.8751381215469614, 0.08333333333333333, 0.5428869750589342, 0.5533127240970863, 0.0, 1.0, 0.6, 0.29240856670691673], 
reward next is 0.7076, 
noisyNet noise sample is [array([-0.08198783], dtype=float32), 0.06753285]. 
=============================================
[2019-04-09 15:01:14,631] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02280984 0.06757555 0.07681499 0.01509882 0.30108535 0.12781642
 0.14297156 0.04235501 0.08191256 0.07710066 0.04445927], sum to 1.0000
[2019-04-09 15:01:14,631] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4749
[2019-04-09 15:01:14,684] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 69.0, 31.16666666666667, 0.0, 19.0, 24.90449608716664, 0.2372863164908258, 0.0, 1.0, 35.0, 47.65492370724671], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2362800.0000, 
sim time next is 2364000.0000, 
raw observation next is [-3.4, 69.0, 51.0, 59.99999999999999, 19.0, 24.91094632755279, 0.2231035989209831, 0.0, 1.0, 35.0, 40.365545075199975], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.17, 0.06629834254143646, 0.08333333333333333, 0.5759121939627324, 0.5743678663069943, 0.0, 1.0, 0.4, 0.40365545075199977], 
reward next is 0.5963, 
noisyNet noise sample is [array([0.6475975], dtype=float32), 1.506144]. 
=============================================
[2019-04-09 15:01:14,694] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[4.0204844]
 [3.9508445]
 [4.236946 ]
 [3.915872 ]
 [3.7750144]], R is [[4.78843355]
 [5.26399994]
 [5.54896641]
 [5.90407658]
 [6.5234108 ]].
[2019-04-09 15:01:14,818] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32961: loss 20.4418
[2019-04-09 15:01:14,820] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32961: learning rate 0.0000
[2019-04-09 15:01:15,353] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.02487363 0.07483885 0.08235545 0.01076104 0.2344331  0.19016762
 0.14274962 0.05830752 0.06519193 0.082659   0.03366223], sum to 1.0000
[2019-04-09 15:01:15,353] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4000
[2019-04-09 15:01:15,378] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 57.0, 0.0, 0.0, 19.0, 23.00735577532416, -0.2572567842160734, 0.0, 1.0, 35.0, 33.18488674235952], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2527200.0000, 
sim time next is 2528400.0000, 
raw observation next is [-2.466666666666667, 56.00000000000001, 0.0, 0.0, 19.0, 22.97864889350645, -0.2769548945904871, 0.0, 1.0, 35.0, 29.259529805219238], 
processed observation next is [1.0, 0.2608695652173913, 0.39427516158818104, 0.56, 0.0, 0.0, 0.08333333333333333, 0.41488740779220407, 0.4076817018031709, 0.0, 1.0, 0.4, 0.29259529805219237], 
reward next is 0.7074, 
noisyNet noise sample is [array([-0.28259912], dtype=float32), -0.15192221]. 
=============================================
[2019-04-09 15:01:15,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01894552 0.04426802 0.07758528 0.0074283  0.2665717  0.17481135
 0.17582741 0.04356208 0.08024044 0.07232693 0.03843307], sum to 1.0000
[2019-04-09 15:01:15,382] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2124
[2019-04-09 15:01:15,413] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 55.33333333333333, 65.16666666666666, 20.5, 22.5, 23.91671233361606, -0.08902488574850913, 1.0, 1.0, 25.0, 25.051271569169565], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2536800.0000, 
sim time next is 2538000.0000, 
raw observation next is [-2.8, 56.0, 93.5, 25.5, 22.5, 23.97920151571224, -0.07695477011228077, 1.0, 1.0, 35.0, 21.695103778554866], 
processed observation next is [1.0, 0.391304347826087, 0.38504155124653744, 0.56, 0.31166666666666665, 0.0281767955801105, 0.375, 0.49826679297602006, 0.47434840996257305, 1.0, 1.0, 0.4, 0.21695103778554867], 
reward next is 0.7830, 
noisyNet noise sample is [array([1.3682691], dtype=float32), -0.034297917]. 
=============================================
[2019-04-09 15:01:15,420] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[4.897412 ]
 [5.1930056]
 [4.97432  ]
 [4.8575554]
 [4.9865146]], R is [[5.932024  ]
 [6.62219143]
 [7.27481556]
 [7.90202999]
 [8.48492622]].
[2019-04-09 15:01:15,554] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.02079102 0.05076972 0.07441808 0.00859179 0.28327382 0.15255424
 0.1491426  0.04232576 0.10142074 0.07752501 0.03918727], sum to 1.0000
[2019-04-09 15:01:15,571] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7469
[2019-04-09 15:01:15,656] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.8, 55.33333333333333, 65.16666666666666, 20.5, 22.5, 23.94440981494943, -0.08375874469464288, 1.0, 1.0, 35.0, 29.638458476301658], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2536800.0000, 
sim time next is 2538000.0000, 
raw observation next is [-2.8, 56.0, 93.5, 25.5, 22.5, 24.05460116104068, -0.04215273772408224, 1.0, 1.0, 50.0, 36.11621819432319], 
processed observation next is [1.0, 0.391304347826087, 0.38504155124653744, 0.56, 0.31166666666666665, 0.0281767955801105, 0.375, 0.5045500967533899, 0.4859490874253059, 1.0, 1.0, 0.7, 0.3611621819432319], 
reward next is 0.6388, 
noisyNet noise sample is [array([-0.42240843], dtype=float32), -0.1986782]. 
=============================================
[2019-04-09 15:01:15,663] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[5.170502]
 [4.97434 ]
 [4.883611]
 [4.910354]
 [4.679952]], R is [[5.83226061]
 [6.47755337]
 [7.07723713]
 [7.62745047]
 [7.96153545]].
[2019-04-09 15:01:15,671] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0270876  0.09063575 0.08149317 0.01136694 0.24118783 0.13161246
 0.13064918 0.06038723 0.09486343 0.07849067 0.05222576], sum to 1.0000
[2019-04-09 15:01:15,671] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6929
[2019-04-09 15:01:15,719] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 57.0, 0.0, 0.0, 19.0, 21.92619626584304, -0.4848462184471872, 0.0, 1.0, 50.0, 40.71415240611469], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2526000.0000, 
sim time next is 2527200.0000, 
raw observation next is [-2.3, 57.0, 0.0, 0.0, 19.0, 21.98113404626408, -0.4893218015474446, 0.0, 1.0, 45.0, 29.694495912233027], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.57, 0.0, 0.0, 0.08333333333333333, 0.33176117052200677, 0.33689273281751847, 0.0, 1.0, 0.6, 0.29694495912233027], 
reward next is 0.7031, 
noisyNet noise sample is [array([0.9027206], dtype=float32), 0.93869025]. 
=============================================
[2019-04-09 15:01:16,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00892262 0.04935034 0.0672973  0.00757467 0.2935445  0.20048082
 0.1507825  0.02816606 0.08708148 0.06179898 0.04500075], sum to 1.0000
[2019-04-09 15:01:16,004] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6145
[2019-04-09 15:01:16,025] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.266666666666667, 27.33333333333334, 169.0, 447.5, 22.5, 25.17086266504021, 0.2906565039866858, 1.0, 1.0, 45.0, 28.935002759044792], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2554800.0000, 
sim time next is 2556000.0000, 
raw observation next is [3.8, 26.0, 165.0, 378.5, 22.5, 25.37300758872492, 0.3078898772252514, 1.0, 1.0, 40.0, 21.752099319067923], 
processed observation next is [1.0, 0.6086956521739131, 0.5678670360110805, 0.26, 0.55, 0.41823204419889504, 0.375, 0.6144172990604101, 0.6026299590750838, 1.0, 1.0, 0.5, 0.21752099319067925], 
reward next is 0.7825, 
noisyNet noise sample is [array([-0.16818726], dtype=float32), 0.5568954]. 
=============================================
[2019-04-09 15:01:16,028] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[5.5079474]
 [5.609869 ]
 [5.2457576]
 [5.475413 ]
 [5.5101533]], R is [[6.19814205]
 [6.84681082]
 [7.46342945]
 [8.17355633]
 [8.86641693]].
[2019-04-09 15:01:16,177] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 33434: loss 28.3699
[2019-04-09 15:01:16,178] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 33434: learning rate 0.0000
[2019-04-09 15:01:16,323] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01165868 0.04678309 0.08648874 0.00581536 0.24760726 0.17255753
 0.15598139 0.03458726 0.11598528 0.07855149 0.04398398], sum to 1.0000
[2019-04-09 15:01:16,323] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2091
[2019-04-09 15:01:16,328] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01856922 0.05914062 0.07552993 0.01843456 0.2819992  0.14890827
 0.17284532 0.04809817 0.08539587 0.06071443 0.03036433], sum to 1.0000
[2019-04-09 15:01:16,329] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3906
[2019-04-09 15:01:16,341] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.8, 26.0, 165.0, 378.5, 22.5, 25.16843218548833, 0.2659094885001846, 1.0, 1.0, 40.0, 20.95109867099312], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2556000.0000, 
sim time next is 2557200.0000, 
raw observation next is [3.633333333333334, 27.0, 161.0, 309.5, 22.5, 25.28174332993195, 0.2914474626874361, 1.0, 1.0, 35.0, 15.874878284014791], 
processed observation next is [1.0, 0.6086956521739131, 0.5632502308402586, 0.27, 0.5366666666666666, 0.3419889502762431, 0.375, 0.6068119441609957, 0.5971491542291454, 1.0, 1.0, 0.4, 0.1587487828401479], 
reward next is 0.8413, 
noisyNet noise sample is [array([0.64668673], dtype=float32), 0.58023334]. 
=============================================
[2019-04-09 15:01:16,353] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.566666666666667, 27.0, 79.5, 792.0, 19.0, 24.40023007777704, 0.1554914269158514, 0.0, 1.0, 35.0, 32.11865689440448], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2470800.0000, 
sim time next is 2472000.0000, 
raw observation next is [2.933333333333333, 27.0, 75.33333333333333, 766.6666666666667, 19.0, 24.47222838522245, 0.1598036573776657, 0.0, 1.0, 35.0, 28.480602536428876], 
processed observation next is [0.0, 0.6086956521739131, 0.543859649122807, 0.27, 0.2511111111111111, 0.847145488029466, 0.08333333333333333, 0.5393523654352043, 0.5532678857925553, 0.0, 1.0, 0.4, 0.2848060253642888], 
reward next is 0.7152, 
noisyNet noise sample is [array([1.3177953], dtype=float32), -0.7231859]. 
=============================================
[2019-04-09 15:01:16,359] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[4.0055666]
 [4.1272483]
 [4.081291 ]
 [3.9298203]
 [4.067028 ]], R is [[4.92119503]
 [5.55079651]
 [6.08695221]
 [6.47078896]
 [7.14606857]].
[2019-04-09 15:01:16,384] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02061165 0.05980782 0.07764879 0.01586219 0.282268   0.11347447
 0.15644924 0.04477459 0.11663622 0.07107666 0.04139044], sum to 1.0000
[2019-04-09 15:01:16,385] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0730
[2019-04-09 15:01:16,460] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 19.0, 23.72658279091539, -0.01653962815588602, 0.0, 1.0, 20.0, 22.840622776914387], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2404800.0000, 
sim time next is 2406000.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 19.0, 23.52052574907134, -0.05994802502215379, 0.0, 1.0, 20.0, 21.121190226183792], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.08333333333333333, 0.4600438124226116, 0.4800173249926154, 0.0, 1.0, 0.1, 0.21121190226183792], 
reward next is 0.7888, 
noisyNet noise sample is [array([0.350075], dtype=float32), 1.3526461]. 
=============================================
[2019-04-09 15:01:16,490] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[4.1511426]
 [4.4362073]
 [4.4389505]
 [4.3518305]
 [4.61428  ]], R is [[4.97965622]
 [5.70145321]
 [6.39667511]
 [7.06259251]
 [7.69017887]].
[2019-04-09 15:01:16,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01365598 0.04086803 0.1080994  0.0063705  0.27517295 0.15559502
 0.1947633  0.03860308 0.07277691 0.05601586 0.03807898], sum to 1.0000
[2019-04-09 15:01:16,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3650
[2019-04-09 15:01:16,591] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 48.33333333333334, 133.5, 43.0, 22.5, 25.38881967026993, 0.2232530402506878, 1.0, 1.0, 35.0, 26.899352541105255], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2542800.0000, 
sim time next is 2544000.0000, 
raw observation next is [-0.8, 47.66666666666667, 149.5, 49.33333333333333, 22.5, 25.36074513542398, 0.2243671183412951, 1.0, 1.0, 35.0, 24.28446418486627], 
processed observation next is [1.0, 0.43478260869565216, 0.4404432132963989, 0.47666666666666674, 0.49833333333333335, 0.054511970534069976, 0.375, 0.6133954279519983, 0.5747890394470984, 1.0, 1.0, 0.4, 0.2428446418486627], 
reward next is 0.7572, 
noisyNet noise sample is [array([0.7357462], dtype=float32), 0.60211843]. 
=============================================
[2019-04-09 15:01:16,603] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[5.641033 ]
 [5.6579437]
 [5.6895547]
 [5.5179434]
 [5.385703 ]], R is [[6.40910244]
 [7.07601833]
 [7.70505619]
 [8.28831005]
 [8.74972153]].
[2019-04-09 15:01:16,723] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02140621 0.05822029 0.09105461 0.01203132 0.21423349 0.17226516
 0.13178292 0.07697304 0.07634441 0.09717825 0.04851035], sum to 1.0000
[2019-04-09 15:01:16,723] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8117
[2019-04-09 15:01:16,775] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.1, 54.33333333333334, 0.0, 0.0, 19.0, 23.51914024303049, -0.1535687303454743, 0.0, 1.0, 50.0, 35.12724692521417], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2522400.0000, 
sim time next is 2523600.0000, 
raw observation next is [-2.3, 57.0, 0.0, 0.0, 19.0, 23.4052117430198, -0.1415356565425432, 0.0, 1.0, 55.0, 43.870162985797684], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.57, 0.0, 0.0, 0.08333333333333333, 0.4504343119183168, 0.4528214478191523, 0.0, 1.0, 0.8, 0.43870162985797684], 
reward next is 0.5613, 
noisyNet noise sample is [array([0.13716972], dtype=float32), 0.43588006]. 
=============================================
[2019-04-09 15:01:17,267] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01654882 0.03944356 0.08141988 0.00704891 0.2913656  0.11504699
 0.21285187 0.02814336 0.11900734 0.05871897 0.03040469], sum to 1.0000
[2019-04-09 15:01:17,268] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1951
[2019-04-09 15:01:17,305] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.3, 61.0, 0.0, 0.0, 19.0, 24.04728396453174, 0.1162580109336083, 0.0, 1.0, 40.0, 30.457598467417668], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2590800.0000, 
sim time next is 2592000.0000, 
raw observation next is [-4.5, 62.0, 0.0, 0.0, 19.0, 23.90874944787291, 0.07269828568707898, 0.0, 1.0, 35.0, 27.65872145778173], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.62, 0.0, 0.0, 0.08333333333333333, 0.49239578732274253, 0.524232761895693, 0.0, 1.0, 0.4, 0.2765872145778173], 
reward next is 0.7234, 
noisyNet noise sample is [array([-0.31766036], dtype=float32), 1.3815212]. 
=============================================
[2019-04-09 15:01:17,313] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[5.612426 ]
 [5.5121737]
 [5.5761356]
 [5.536954 ]
 [5.5702972]], R is [[6.14725733]
 [6.78120899]
 [7.36651087]
 [7.75710917]
 [8.36359024]].
[2019-04-09 15:01:17,455] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.02489712 0.06235068 0.10640054 0.01314427 0.27193326 0.12215599
 0.15190513 0.04596376 0.08934581 0.07193132 0.03997209], sum to 1.0000
[2019-04-09 15:01:17,455] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1262
[2019-04-09 15:01:17,465] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.01343928 0.05563959 0.08257878 0.0062739  0.26764435 0.17218982
 0.16640446 0.03837099 0.1075976  0.06667276 0.02318839], sum to 1.0000
[2019-04-09 15:01:17,466] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5044
[2019-04-09 15:01:17,486] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.0, 33.66666666666667, 0.0, 0.0, 19.0, 21.75545403648145, -0.4483793377708672, 0.0, 1.0, 45.0, 43.087537639274174], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2499600.0000, 
sim time next is 2500800.0000, 
raw observation next is [-0.8, 34.33333333333334, 0.0, 0.0, 19.0, 21.80372807833013, -0.4467388570350923, 0.0, 1.0, 25.0, 25.32651218125079], 
processed observation next is [0.0, 0.9565217391304348, 0.4404432132963989, 0.34333333333333343, 0.0, 0.0, 0.08333333333333333, 0.31697733986084425, 0.35108704765496923, 0.0, 1.0, 0.2, 0.25326512181250793], 
reward next is 0.7467, 
noisyNet noise sample is [array([-1.7707281], dtype=float32), 1.3170981]. 
=============================================
[2019-04-09 15:01:17,497] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 19.0, 25.33692726027186, 0.3963954787082453, 0.0, 1.0, 50.0, 32.131344179261234], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2582400.0000, 
sim time next is 2583600.0000, 
raw observation next is [-2.8, 56.0, 0.0, 0.0, 19.0, 25.18355908257103, 0.3618698678789871, 0.0, 1.0, 40.0, 28.965259164958887], 
processed observation next is [1.0, 0.9130434782608695, 0.38504155124653744, 0.56, 0.0, 0.0, 0.08333333333333333, 0.5986299235475858, 0.6206232892929957, 0.0, 1.0, 0.5, 0.2896525916495889], 
reward next is 0.7103, 
noisyNet noise sample is [array([-0.78587383], dtype=float32), 0.6981974]. 
=============================================
[2019-04-09 15:01:17,565] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01890926 0.06019017 0.07837292 0.02039067 0.2689203  0.14967941
 0.16123416 0.04602863 0.07366877 0.07895103 0.04365463], sum to 1.0000
[2019-04-09 15:01:17,568] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0587
[2019-04-09 15:01:17,605] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01035168 0.04833466 0.0710228  0.00641655 0.27002802 0.23391497
 0.13611008 0.03075722 0.07857638 0.07913543 0.03535223], sum to 1.0000
[2019-04-09 15:01:17,605] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4286
[2019-04-09 15:01:17,626] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.266666666666667, 27.33333333333334, 169.0, 447.5, 22.5, 25.91668074476997, 0.4568049119636424, 1.0, 1.0, 40.0, 30.632906170585866], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2554800.0000, 
sim time next is 2556000.0000, 
raw observation next is [3.8, 26.0, 165.0, 378.5, 22.5, 26.11847206274106, 0.4709051804818277, 1.0, 1.0, 40.0, 25.92681891814435], 
processed observation next is [1.0, 0.6086956521739131, 0.5678670360110805, 0.26, 0.55, 0.41823204419889504, 0.375, 0.676539338561755, 0.6569683934939426, 1.0, 1.0, 0.5, 0.2592681891814435], 
reward next is 0.7407, 
noisyNet noise sample is [array([-0.4873525], dtype=float32), -0.1741057]. 
=============================================
[2019-04-09 15:01:17,634] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[6.0204196]
 [5.931813 ]
 [5.951739 ]
 [5.9029818]
 [5.757404 ]], R is [[6.78710079]
 [7.41290045]
 [7.86501408]
 [8.55335712]
 [9.22896957]].
[2019-04-09 15:01:17,642] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.3, 27.0, 70.0, 732.0, 19.0, 23.76953183885749, 0.02051938009931488, 0.0, 1.0, 45.0, 42.986489344009485], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2473200.0000, 
sim time next is 2474400.0000, 
raw observation next is [3.3, 26.66666666666667, 64.66666666666667, 697.3333333333334, 19.0, 24.04170949057518, 0.04210775635864263, 0.0, 1.0, 20.0, 35.698782606578675], 
processed observation next is [0.0, 0.6521739130434783, 0.554016620498615, 0.2666666666666667, 0.21555555555555558, 0.7705340699815838, 0.08333333333333333, 0.5034757908812649, 0.5140359187862142, 0.0, 1.0, 0.1, 0.3569878260657868], 
reward next is 0.6430, 
noisyNet noise sample is [array([1.0800029], dtype=float32), -1.1781555]. 
=============================================
[2019-04-09 15:01:17,907] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0111273  0.05186489 0.0655202  0.00473682 0.2947747  0.18282527
 0.18168446 0.03232688 0.06737521 0.07570874 0.03205559], sum to 1.0000
[2019-04-09 15:01:17,909] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5092
[2019-04-09 15:01:17,964] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.266666666666667, 27.33333333333334, 169.0, 447.5, 22.5, 25.6732347269334, 0.4100235503480158, 1.0, 1.0, 45.0, 35.07974551779033], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2554800.0000, 
sim time next is 2556000.0000, 
raw observation next is [3.8, 26.0, 165.0, 378.5, 22.5, 25.98738582505133, 0.4323382707535564, 1.0, 1.0, 40.0, 29.207707913975906], 
processed observation next is [1.0, 0.6086956521739131, 0.5678670360110805, 0.26, 0.55, 0.41823204419889504, 0.375, 0.6656154854209442, 0.6441127569178521, 1.0, 1.0, 0.5, 0.29207707913975905], 
reward next is 0.7079, 
noisyNet noise sample is [array([2.4261074], dtype=float32), 1.5020466]. 
=============================================
[2019-04-09 15:01:18,003] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[6.1200247]
 [6.2499237]
 [5.95154  ]
 [5.878098 ]
 [6.117233 ]], R is [[6.79700375]
 [7.37823629]
 [7.77982855]
 [8.46935844]
 [9.14538574]].
[2019-04-09 15:01:18,377] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01193022 0.03440665 0.05657084 0.00621273 0.36316887 0.17602514
 0.14468911 0.02686382 0.06825924 0.06770269 0.0441707 ], sum to 1.0000
[2019-04-09 15:01:18,378] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3884
[2019-04-09 15:01:18,409] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.566666666666666, 68.33333333333333, 102.8333333333333, 121.6666666666667, 22.5, 24.72312692217833, 0.1276232122729758, 1.0, 1.0, 40.0, 36.98368728632829], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2626800.0000, 
sim time next is 2628000.0000, 
raw observation next is [-5.0, 65.0, 122.5, 183.0, 22.5, 24.73018628869638, 0.1301021105722579, 1.0, 1.0, 35.0, 33.09516563359759], 
processed observation next is [1.0, 0.43478260869565216, 0.32409972299168976, 0.65, 0.4083333333333333, 0.2022099447513812, 0.375, 0.560848857391365, 0.5433673701907527, 1.0, 1.0, 0.4, 0.3309516563359759], 
reward next is 0.6690, 
noisyNet noise sample is [array([-0.6685479], dtype=float32), -1.1902161]. 
=============================================
[2019-04-09 15:01:18,412] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[5.866235 ]
 [5.8911533]
 [5.60925  ]
 [5.407698 ]
 [5.4596887]], R is [[6.36422014]
 [6.93074131]
 [7.4476738 ]
 [7.88079309]
 [8.10002422]].
[2019-04-09 15:01:18,491] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01716232 0.05985237 0.05899713 0.00800808 0.24190205 0.19431943
 0.14169203 0.04987421 0.07811604 0.10796839 0.04210792], sum to 1.0000
[2019-04-09 15:01:18,491] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0619
[2019-04-09 15:01:18,553] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.3, 79.0, 0.0, 0.0, 19.0, 23.70148295844908, -0.009259592261106007, 0.0, 1.0, 35.0, 44.68671560481825], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2617200.0000, 
sim time next is 2618400.0000, 
raw observation next is [-7.300000000000001, 79.0, 0.0, 0.0, 22.5, 23.75272120978606, -0.03776083312638368, 1.0, 1.0, 20.0, 37.7394128705173], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.79, 0.0, 0.0, 0.375, 0.47939343414883834, 0.4874130556245388, 1.0, 1.0, 0.1, 0.37739412870517297], 
reward next is 0.6226, 
noisyNet noise sample is [array([0.70255435], dtype=float32), -1.0855612]. 
=============================================
[2019-04-09 15:01:18,927] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00884941 0.05707923 0.05901891 0.00683379 0.28807586 0.16417606
 0.14822146 0.03662178 0.09080904 0.07186329 0.0684512 ], sum to 1.0000
[2019-04-09 15:01:18,927] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3208
[2019-04-09 15:01:18,956] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1333333333333333, 44.33333333333333, 172.6666666666667, 197.5, 22.5, 24.82881636660811, 0.2341686549096832, 1.0, 1.0, 40.0, 23.62649818743362], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2641200.0000, 
sim time next is 2642400.0000, 
raw observation next is [0.5, 43.0, 190.0, 170.5, 22.5, 24.98395755306535, 0.253095074570988, 1.0, 1.0, 35.0, 21.106382071586893], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.43, 0.6333333333333333, 0.18839779005524862, 0.375, 0.5819964627554457, 0.584365024856996, 1.0, 1.0, 0.4, 0.21106382071586893], 
reward next is 0.7889, 
noisyNet noise sample is [array([-1.0777196], dtype=float32), 1.1393495]. 
=============================================
[2019-04-09 15:01:18,963] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01127346 0.04137478 0.09637172 0.0066959  0.33951768 0.15958858
 0.15342484 0.03943476 0.07314281 0.04846417 0.03071133], sum to 1.0000
[2019-04-09 15:01:18,967] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0148
[2019-04-09 15:01:18,994] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.066666666666667, 48.0, 0.0, 0.0, 19.0, 25.14154858764633, 0.3501619504888637, 0.0, 1.0, 25.0, 30.081328403048886], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2578800.0000, 
sim time next is 2580000.0000, 
raw observation next is [-2.433333333333333, 52.0, 0.0, 0.0, 19.0, 24.95613895337717, 0.3164073441806337, 0.0, 1.0, 20.0, 27.28999801967077], 
processed observation next is [1.0, 0.8695652173913043, 0.3951985226223454, 0.52, 0.0, 0.0, 0.08333333333333333, 0.5796782461147642, 0.6054691147268779, 0.0, 1.0, 0.1, 0.2728999801967077], 
reward next is 0.7271, 
noisyNet noise sample is [array([-0.4215956], dtype=float32), 0.51597214]. 
=============================================
[2019-04-09 15:01:19,016] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[5.6017795]
 [6.111313 ]
 [6.0500636]
 [5.9840684]
 [6.158484 ]], R is [[6.4995656 ]
 [7.13375664]
 [7.72150517]
 [8.20515251]
 [8.76781368]].
[2019-04-09 15:01:19,144] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.00977538 0.04518507 0.06396997 0.00601872 0.28222284 0.20247619
 0.20484065 0.0384448  0.06269893 0.0450775  0.03928987], sum to 1.0000
[2019-04-09 15:01:19,152] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4468
[2019-04-09 15:01:19,185] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.266666666666667, 63.0, 164.0, 257.6666666666667, 22.5, 24.28465709678447, 0.07473330922634869, 1.0, 1.0, 45.0, 34.905585717130975], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2630400.0000, 
sim time next is 2631600.0000, 
raw observation next is [-3.9, 62.0, 188.0, 223.0, 22.5, 24.3752516214148, 0.09054662856603524, 1.0, 1.0, 40.0, 26.545038842114895], 
processed observation next is [1.0, 0.4782608695652174, 0.3545706371191136, 0.62, 0.6266666666666667, 0.24640883977900552, 0.375, 0.5312709684512334, 0.5301822095220118, 1.0, 1.0, 0.5, 0.26545038842114893], 
reward next is 0.7345, 
noisyNet noise sample is [array([-0.6434899], dtype=float32), -0.64587337]. 
=============================================
[2019-04-09 15:01:19,458] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0088579  0.05004447 0.07295265 0.00709327 0.3127713  0.16508447
 0.16566078 0.02723476 0.08484831 0.07539553 0.03005653], sum to 1.0000
[2019-04-09 15:01:19,458] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2036
[2019-04-09 15:01:19,501] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5333333333333334, 41.66666666666667, 229.6666666666667, 62.83333333333334, 22.5, 25.26826396358695, 0.3057725440803566, 1.0, 1.0, 60.0, 54.57373947596971], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2547600.0000, 
sim time next is 2548800.0000, 
raw observation next is [1.1, 39.0, 225.0, 46.5, 22.5, 25.68502802795141, 0.3616884140247603, 1.0, 1.0, 35.0, 33.74624111479965], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.39, 0.75, 0.05138121546961326, 0.375, 0.6404190023292843, 0.6205628046749201, 1.0, 1.0, 0.4, 0.3374624111479965], 
reward next is 0.6625, 
noisyNet noise sample is [array([-2.5258067], dtype=float32), 0.7938235]. 
=============================================
[2019-04-09 15:01:19,537] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01869595 0.05675345 0.06428554 0.01314628 0.29214454 0.11828025
 0.17132515 0.04106902 0.09817153 0.08032779 0.04580041], sum to 1.0000
[2019-04-09 15:01:19,540] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5877
[2019-04-09 15:01:19,584] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 35.66666666666667, 0.0, 0.0, 19.0, 22.5576694347198, -0.2553753290214615, 0.0, 1.0, 60.0, 53.45659589472868], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2496000.0000, 
sim time next is 2497200.0000, 
raw observation next is [-1.2, 34.33333333333334, 0.0, 0.0, 19.0, 22.76044755063948, -0.2181727700450521, 0.0, 1.0, 45.0, 35.55377195910223], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.34333333333333343, 0.0, 0.0, 0.08333333333333333, 0.39670396255329, 0.4272757433183159, 0.0, 1.0, 0.6, 0.35553771959102226], 
reward next is 0.6445, 
noisyNet noise sample is [array([-0.78557473], dtype=float32), -1.8975214]. 
=============================================
[2019-04-09 15:01:19,891] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00924494 0.04227182 0.06462209 0.00576333 0.2802622  0.25302294
 0.1535229  0.03107535 0.06344014 0.06619655 0.03057773], sum to 1.0000
[2019-04-09 15:01:19,891] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6446
[2019-04-09 15:01:19,950] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [2.2, 30.0, 194.5, 252.0, 22.5, 26.28875054907339, 0.4574652261820971, 1.0, 1.0, 20.0, 29.617951453105754], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2552400.0000, 
sim time next is 2553600.0000, 
raw observation next is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 22.5, 26.15800538981051, 0.4837597061143182, 1.0, 1.0, 40.0, 20.889290715182952], 
processed observation next is [1.0, 0.5652173913043478, 0.538319482917821, 0.28666666666666674, 0.5961111111111109, 0.44788213627992646, 0.375, 0.679833782484209, 0.6612532353714394, 1.0, 1.0, 0.5, 0.2088929071518295], 
reward next is 0.7911, 
noisyNet noise sample is [array([-0.4074424], dtype=float32), -0.1401116]. 
=============================================
[2019-04-09 15:01:20,051] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01028605 0.04223486 0.09388231 0.00449695 0.2836877  0.16949576
 0.21057399 0.01997008 0.09137175 0.04432217 0.02967833], sum to 1.0000
[2019-04-09 15:01:20,052] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5923
[2019-04-09 15:01:20,080] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.8, 56.00000000000001, 0.0, 0.0, 22.5, 25.19513639980878, 0.290207785406716, 1.0, 1.0, 35.0, 28.853719874766014], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2658000.0000, 
sim time next is 2659200.0000, 
raw observation next is [-1.0, 58.0, 0.0, 0.0, 22.5, 24.7348722033393, 0.2399871214195416, 1.0, 1.0, 40.0, 25.26608828229375], 
processed observation next is [1.0, 0.782608695652174, 0.4349030470914128, 0.58, 0.0, 0.0, 0.375, 0.561239350278275, 0.5799957071398473, 1.0, 1.0, 0.5, 0.2526608828229375], 
reward next is 0.7473, 
noisyNet noise sample is [array([1.5455204], dtype=float32), -0.10484321]. 
=============================================
[2019-04-09 15:01:20,369] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00853193 0.03420902 0.06060169 0.00440535 0.32987958 0.14824551
 0.21848726 0.02490481 0.07613122 0.07844945 0.01615413], sum to 1.0000
[2019-04-09 15:01:20,370] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0413
[2019-04-09 15:01:20,414] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.333333333333333, 71.0, 0.0, 0.0, 19.0, 24.11568306932239, 0.1110864875507266, 0.0, 1.0, 45.0, 38.30755701670756], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2677200.0000, 
sim time next is 2678400.0000, 
raw observation next is [-7.0, 72.0, 0.0, 0.0, 19.0, 23.9508849782499, 0.06174813720453207, 0.0, 1.0, 20.0, 34.565256013871235], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.72, 0.0, 0.0, 0.08333333333333333, 0.4959070815208249, 0.5205827124015107, 0.0, 1.0, 0.1, 0.3456525601387124], 
reward next is 0.6543, 
noisyNet noise sample is [array([0.11809225], dtype=float32), 0.12659654]. 
=============================================
[2019-04-09 15:01:20,459] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02257586 0.06332316 0.07708067 0.01015801 0.24841574 0.18988742
 0.1506985  0.0488241  0.07833524 0.07735147 0.03334975], sum to 1.0000
[2019-04-09 15:01:20,459] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4672
[2019-04-09 15:01:20,502] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.7, 47.33333333333333, 0.0, 0.0, 19.0, 23.37530052132473, -0.1680470342859622, 0.0, 1.0, 45.0, 33.18030122666131], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2518800.0000, 
sim time next is 2520000.0000, 
raw observation next is [-1.7, 49.0, 0.0, 0.0, 19.0, 23.36045386373279, -0.1932215636379081, 0.0, 1.0, 40.0, 29.664374676493015], 
processed observation next is [1.0, 0.17391304347826086, 0.4155124653739613, 0.49, 0.0, 0.0, 0.08333333333333333, 0.44670448864439916, 0.4355928121206973, 0.0, 1.0, 0.5, 0.29664374676493016], 
reward next is 0.7034, 
noisyNet noise sample is [array([-0.44324192], dtype=float32), 0.40994847]. 
=============================================
[2019-04-09 15:01:20,510] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[5.0288386]
 [4.988414 ]
 [4.925117 ]
 [4.75266  ]
 [4.788919 ]], R is [[5.65287924]
 [6.26454782]
 [6.69200325]
 [7.31721926]
 [8.01164913]].
[2019-04-09 15:01:20,686] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01313664 0.04467769 0.07516272 0.00824186 0.2440509  0.24664377
 0.13504076 0.0425252  0.07786267 0.06275091 0.04990683], sum to 1.0000
[2019-04-09 15:01:20,708] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5356
[2019-04-09 15:01:20,745] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.100000000000001, 77.66666666666667, 65.33333333333334, 1.333333333333333, 22.5, 22.45096062271571, -0.2750011560882973, 1.0, 1.0, 40.0, 27.99648866075664], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2622000.0000, 
sim time next is 2623200.0000, 
raw observation next is [-6.9, 76.33333333333334, 79.66666666666667, 15.16666666666666, 22.5, 22.78234887734341, -0.2143893010160665, 1.0, 1.0, 40.0, 29.2264029978315], 
processed observation next is [1.0, 0.34782608695652173, 0.27146814404432135, 0.7633333333333334, 0.26555555555555554, 0.01675874769797421, 0.375, 0.3985290731119508, 0.4285368996613112, 1.0, 1.0, 0.5, 0.292264029978315], 
reward next is 0.7077, 
noisyNet noise sample is [array([-0.818686], dtype=float32), 0.18208331]. 
=============================================
[2019-04-09 15:01:20,862] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0078583  0.02840164 0.07261728 0.00370861 0.423148   0.15430447
 0.1480381  0.02482297 0.06016821 0.04928933 0.02764305], sum to 1.0000
[2019-04-09 15:01:20,865] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8423
[2019-04-09 15:01:20,896] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00973685 0.03909596 0.08325065 0.00462575 0.37784556 0.13526042
 0.17784706 0.02667958 0.06694755 0.04925568 0.02945491], sum to 1.0000
[2019-04-09 15:01:20,897] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4312
[2019-04-09 15:01:20,921] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 22.5, 25.31954397699563, 0.3111236385233423, 1.0, 1.0, 35.0, 32.177441587447106], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2553600.0000, 
sim time next is 2554800.0000, 
raw observation next is [3.266666666666667, 27.33333333333334, 169.0, 447.5, 22.5, 25.66638820669717, 0.3597537682071694, 1.0, 1.0, 35.0, 26.997863361752824], 
processed observation next is [1.0, 0.5652173913043478, 0.5530932594644506, 0.2733333333333334, 0.5633333333333334, 0.494475138121547, 0.375, 0.638865683891431, 0.6199179227357231, 1.0, 1.0, 0.4, 0.2699786336175282], 
reward next is 0.7300, 
noisyNet noise sample is [array([-1.6102333], dtype=float32), -0.45145854]. 
=============================================
[2019-04-09 15:01:20,925] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [0.5, 49.0, 141.6666666666667, 192.3333333333333, 22.5, 26.03504433277424, 0.3857346720015469, 1.0, 1.0, 35.0, 41.76659070958665], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2648400.0000, 
sim time next is 2649600.0000, 
raw observation next is [0.5, 50.0, 115.0, 165.0, 22.5, 24.88878667179997, 0.3832844592822573, 1.0, 1.0, 50.0, 36.800653186365544], 
processed observation next is [1.0, 0.6956521739130435, 0.4764542936288089, 0.5, 0.38333333333333336, 0.18232044198895028, 0.375, 0.5740655559833309, 0.6277614864274191, 1.0, 1.0, 0.7, 0.36800653186365545], 
reward next is 0.6320, 
noisyNet noise sample is [array([0.63582754], dtype=float32), 0.13201171]. 
=============================================
[2019-04-09 15:01:21,440] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01191014 0.03773019 0.09442782 0.00748454 0.3693655  0.132791
 0.14715713 0.03332315 0.05877207 0.07826041 0.02877797], sum to 1.0000
[2019-04-09 15:01:21,441] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3848
[2019-04-09 15:01:21,484] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.1, 60.0, 0.0, 0.0, 19.0, 24.58682565436336, 0.2146264513577174, 0.0, 1.0, 35.0, 24.35931320261285], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2589600.0000, 
sim time next is 2590800.0000, 
raw observation next is [-4.3, 61.0, 0.0, 0.0, 19.0, 24.39859297946498, 0.168310167701832, 0.0, 1.0, 35.0, 22.01217524143373], 
processed observation next is [1.0, 1.0, 0.34349030470914127, 0.61, 0.0, 0.0, 0.08333333333333333, 0.5332160816220816, 0.556103389233944, 0.0, 1.0, 0.4, 0.22012175241433732], 
reward next is 0.7799, 
noisyNet noise sample is [array([0.75340086], dtype=float32), 0.1909714]. 
=============================================
[2019-04-09 15:01:23,206] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.01325909 0.04713497 0.0838344  0.00695642 0.29320902 0.19478706
 0.13728343 0.03865989 0.0592033  0.08704444 0.03862798], sum to 1.0000
[2019-04-09 15:01:23,206] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8121
[2019-04-09 15:01:23,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00802869 0.04476223 0.07077685 0.00564739 0.3434405  0.18333788
 0.14498223 0.0320554  0.09147862 0.04808369 0.02740645], sum to 1.0000
[2019-04-09 15:01:23,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4836
[2019-04-09 15:01:23,358] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 19.0, 21.6352239032692, -0.4623322919582824, 0.0, 1.0, 50.0, 40.446813737704055], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2703600.0000, 
sim time next is 2704800.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 22.5, 21.50220929178415, -0.4097645337170801, 0.0, 1.0, 50.0, 55.813933502366574], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.375, 0.29185077431534595, 0.3634118220943066, 0.0, 1.0, 0.7, 0.5581393350236658], 
reward next is 0.4419, 
noisyNet noise sample is [array([-0.87672687], dtype=float32), -0.3968962]. 
=============================================
[2019-04-09 15:01:23,398] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.0, 68.0, 116.1666666666667, 691.8333333333334, 22.5, 25.12874524768542, 0.2367642535453385, 1.0, 1.0, 40.0, 33.653083072625684], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2716800.0000, 
sim time next is 2718000.0000, 
raw observation next is [-9.0, 64.0, 114.5, 727.5, 22.5, 25.2192053467728, 0.158507128731237, 1.0, 1.0, 40.0, 28.23749506474354], 
processed observation next is [1.0, 0.4782608695652174, 0.21329639889196678, 0.64, 0.38166666666666665, 0.8038674033149171, 0.375, 0.6016004455644, 0.552835709577079, 1.0, 1.0, 0.5, 0.2823749506474354], 
reward next is 0.7176, 
noisyNet noise sample is [array([1.7280313], dtype=float32), -0.7627996]. 
=============================================
[2019-04-09 15:01:23,402] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[6.4929457]
 [6.2678795]
 [6.4365287]
 [6.2106624]
 [5.8524323]], R is [[7.17387533]
 [7.76560593]
 [8.28358936]
 [8.57177925]
 [9.13436794]].
[2019-04-09 15:01:24,584] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0047735  0.04281794 0.07093586 0.00341808 0.44339103 0.12796944
 0.14037171 0.02889202 0.06929354 0.04783643 0.02030041], sum to 1.0000
[2019-04-09 15:01:24,584] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1932
[2019-04-09 15:01:24,663] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 61.00000000000001, 0.0, 0.0, 22.5, 26.24411307256996, 0.5886541842560273, 0.0, 1.0, 35.0, 28.59155562304869], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2661600.0000, 
sim time next is 2662800.0000, 
raw observation next is [-1.2, 62.0, 0.0, 0.0, 22.5, 26.0726154762218, 0.552545945493141, 1.0, 1.0, 35.0, 25.88561232418244], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.62, 0.0, 0.0, 0.375, 0.6727179563518165, 0.684181981831047, 1.0, 1.0, 0.4, 0.2588561232418244], 
reward next is 0.7411, 
noisyNet noise sample is [array([0.60618293], dtype=float32), 1.3135226]. 
=============================================
[2019-04-09 15:01:24,961] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0067671  0.0464397  0.06483093 0.00409888 0.36270547 0.13763808
 0.15777665 0.03633511 0.09300719 0.06686398 0.02353681], sum to 1.0000
[2019-04-09 15:01:24,962] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7972
[2019-04-09 15:01:25,088] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-4.266666666666667, 54.66666666666667, 99.33333333333333, 713.1666666666667, 22.5, 26.66775786666904, 0.6643619570573459, 1.0, 1.0, 25.0, 26.09362033621955], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2731200.0000, 
sim time next is 2732400.0000, 
raw observation next is [-4.0, 54.0, 94.0, 673.5, 22.5, 26.63274571347545, 0.7272822485959513, 1.0, 1.0, 50.0, 42.15074339896891], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.54, 0.31333333333333335, 0.7441988950276243, 0.375, 0.7193954761229543, 0.7424274161986504, 1.0, 1.0, 0.7, 0.42150743398968915], 
reward next is 0.5785, 
noisyNet noise sample is [array([-1.7002182], dtype=float32), -0.39220938]. 
=============================================
[2019-04-09 15:01:25,418] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00278265 0.03117854 0.06749994 0.00356505 0.39704365 0.13270053
 0.1809766  0.01660463 0.10387978 0.04072972 0.02303888], sum to 1.0000
[2019-04-09 15:01:25,418] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1974
[2019-04-09 15:01:25,612] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 50.0, 70.0, 534.0, 22.5, 27.77528833171787, 0.66207289111566, 1.0, 1.0, 55.0, 60.58448820559964], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2736000.0000, 
sim time next is 2737200.0000, 
raw observation next is [-3.0, 50.0, 59.33333333333333, 480.6666666666667, 22.5, 27.14811911233528, 0.8136628141669227, 1.0, 1.0, 35.0, 56.48977710114527], 
processed observation next is [1.0, 0.6956521739130435, 0.3795013850415513, 0.5, 0.19777777777777777, 0.5311233885819522, 0.375, 0.7623432593612733, 0.7712209380556408, 1.0, 1.0, 0.4, 0.5648977710114527], 
reward next is 0.4351, 
noisyNet noise sample is [array([0.6145778], dtype=float32), -0.93772554]. 
=============================================
[2019-04-09 15:01:26,221] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01726651 0.05525987 0.07959895 0.00624711 0.32669923 0.15807602
 0.14302285 0.04165082 0.06727694 0.06733719 0.03756453], sum to 1.0000
[2019-04-09 15:01:26,221] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6395
[2019-04-09 15:01:26,249] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-15.33333333333334, 83.0, 0.0, 0.0, 19.0, 22.16270019691004, -0.3016340542979771, 0.0, 1.0, 35.0, 38.14006657024286], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2697600.0000, 
sim time next is 2698800.0000, 
raw observation next is [-15.66666666666667, 83.0, 0.0, 0.0, 19.0, 22.17716162594842, -0.3426717553107465, 0.0, 1.0, 40.0, 33.32737498760953], 
processed observation next is [1.0, 0.21739130434782608, 0.02862419205909501, 0.83, 0.0, 0.0, 0.08333333333333333, 0.3480968021623682, 0.38577608156308446, 0.0, 1.0, 0.5, 0.3332737498760953], 
reward next is 0.6667, 
noisyNet noise sample is [array([0.87219214], dtype=float32), 1.4479495]. 
=============================================
[2019-04-09 15:01:26,503] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01010547 0.03123697 0.07416421 0.00417231 0.34944946 0.12095764
 0.19164376 0.04193199 0.08264598 0.07314569 0.0205466 ], sum to 1.0000
[2019-04-09 15:01:26,504] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9582
[2019-04-09 15:01:26,554] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-4.366666666666666, 69.0, 0.0, 0.0, 19.0, 24.06298916649425, 0.1042850345149918, 0.0, 1.0, 35.0, 30.266176396084507], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2673600.0000, 
sim time next is 2674800.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 19.0, 23.75338768150781, 0.1000253339361259, 0.0, 1.0, 60.0, 64.5976038439672], 
processed observation next is [1.0, 1.0, 0.32409972299168976, 0.69, 0.0, 0.0, 0.08333333333333333, 0.4794489734589842, 0.5333417779787086, 0.0, 1.0, 0.9, 0.6459760384396721], 
reward next is 0.3540, 
noisyNet noise sample is [array([-1.3999757], dtype=float32), -0.6269115]. 
=============================================
[2019-04-09 15:01:26,990] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01878298 0.05889752 0.08510508 0.00691256 0.2295829  0.13275467
 0.23869734 0.03045855 0.09705409 0.07299461 0.02875972], sum to 1.0000
[2019-04-09 15:01:26,990] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3927
[2019-04-09 15:01:27,007] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00967052 0.03437785 0.06460789 0.00383109 0.3524707  0.15444584
 0.19237109 0.02929224 0.08178802 0.05304827 0.02409652], sum to 1.0000
[2019-04-09 15:01:27,007] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9754
[2019-04-09 15:01:27,042] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.666666666666668, 71.33333333333334, 0.0, 0.0, 19.0, 23.08991786327168, -0.07039545067128762, 0.0, 1.0, 20.0, 37.679517050112885], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2683200.0000, 
sim time next is 2684400.0000, 
raw observation next is [-10.33333333333333, 73.66666666666667, 0.0, 0.0, 19.0, 22.98366613455063, -0.1147451089035996, 0.0, 1.0, 35.0, 32.03185742553063], 
processed observation next is [1.0, 0.043478260869565216, 0.17636195752539252, 0.7366666666666667, 0.0, 0.0, 0.08333333333333333, 0.4153055112125526, 0.4617516303654668, 0.0, 1.0, 0.4, 0.3203185742553063], 
reward next is 0.6797, 
noisyNet noise sample is [array([-3.3222215], dtype=float32), 0.050799675]. 
=============================================
[2019-04-09 15:01:27,074] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-6.0, 60.66666666666666, 0.0, 0.0, 19.0, 24.42613615240642, 0.199715771190957, 0.0, 1.0, 35.0, 36.1903871841998], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 2756400.0000, 
sim time next is 2757600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 24.31899324940867, 0.220890975006981, 0.0, 1.0, 60.0, 67.66908039213592], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.5265827707840559, 0.573630325002327, 0.0, 1.0, 0.9, 0.6766908039213593], 
reward next is 0.3233, 
noisyNet noise sample is [array([-0.2623805], dtype=float32), -1.1094143]. 
=============================================
[2019-04-09 15:01:27,285] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.01208571 0.06287726 0.05950217 0.00749793 0.33500332 0.18318826
 0.12570521 0.03759987 0.07541993 0.06859796 0.03252234], sum to 1.0000
[2019-04-09 15:01:27,291] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6695
[2019-04-09 15:01:27,302] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00519186 0.02571146 0.06353345 0.00302025 0.404407   0.13371567
 0.18865965 0.02658713 0.07329365 0.04140868 0.03447121], sum to 1.0000
[2019-04-09 15:01:27,303] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2818
[2019-04-09 15:01:27,326] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8, 56.00000000000001, 0.0, 0.0, 22.5, 25.14501149379906, 0.3355016578675733, 1.0, 1.0, 45.0, 33.32721331030288], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2658000.0000, 
sim time next is 2659200.0000, 
raw observation next is [-1.0, 58.0, 0.0, 0.0, 22.5, 24.85555267928449, 0.3045223824155203, 1.0, 1.0, 35.0, 25.51715741089975], 
processed observation next is [1.0, 0.782608695652174, 0.4349030470914128, 0.58, 0.0, 0.0, 0.375, 0.5712960566070407, 0.6015074608051735, 1.0, 1.0, 0.4, 0.2551715741089975], 
reward next is 0.7448, 
noisyNet noise sample is [array([-1.4815373], dtype=float32), 1.7973177]. 
=============================================
[2019-04-09 15:01:27,332] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 23.34125879047156, -0.1261941849280128, 0.0, 1.0, 35.0, 31.360271015346612], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2785200.0000, 
sim time next is 2786400.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 23.24640429618383, -0.1496910350144318, 0.0, 1.0, 45.0, 28.339117809777406], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.43720035801531915, 0.45010298832852275, 0.0, 1.0, 0.6, 0.28339117809777403], 
reward next is 0.7166, 
noisyNet noise sample is [array([-1.009259], dtype=float32), 1.0751721]. 
=============================================
[2019-04-09 15:01:27,437] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.01200547 0.06406441 0.05935639 0.00747786 0.32537484 0.18484494
 0.12623857 0.03975691 0.07468531 0.07237429 0.03382107], sum to 1.0000
[2019-04-09 15:01:27,437] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0255
[2019-04-09 15:01:27,488] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 22.53598734772655, -0.3355744617993373, 0.0, 1.0, 35.0, 41.201355093876515], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 2788800.0000, 
sim time next is 2790000.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 21.93108384629079, -0.4487377525408274, 0.0, 1.0, 20.0, 38.047208202437226], 
processed observation next is [1.0, 0.30434782608695654, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.3275903205242325, 0.35042074915305754, 0.0, 1.0, 0.1, 0.38047208202437227], 
reward next is 0.6195, 
noisyNet noise sample is [array([-1.009259], dtype=float32), 1.0751721]. 
=============================================
[2019-04-09 15:01:27,517] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[5.3731465]
 [5.385918 ]
 [5.5726585]
 [5.3993044]
 [5.568955 ]], R is [[5.97505856]
 [6.50329447]
 [7.10579157]
 [7.75134277]
 [8.36022663]].
[2019-04-09 15:01:29,739] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01319782 0.04677921 0.07698364 0.00555605 0.30546504 0.1593109
 0.15739024 0.05439108 0.10182676 0.04665679 0.03244248], sum to 1.0000
[2019-04-09 15:01:29,745] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4103
[2019-04-09 15:01:29,789] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 23.13226386882318, -0.1334829479042654, 0.0, 1.0, 35.0, 21.791499100608057], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2774400.0000, 
sim time next is 2775600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 23.0017394876081, -0.1763988535022714, 0.0, 1.0, 35.0, 19.35869810613535], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.41681162396734156, 0.44120038216590957, 0.0, 1.0, 0.4, 0.1935869810613535], 
reward next is 0.8064, 
noisyNet noise sample is [array([1.1649379], dtype=float32), 2.2121422]. 
=============================================
[2019-04-09 15:01:30,330] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00551852 0.04850908 0.04389582 0.00280468 0.2714404  0.23948348
 0.16617379 0.02560526 0.08808328 0.07224296 0.03624279], sum to 1.0000
[2019-04-09 15:01:30,331] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9677
[2019-04-09 15:01:30,366] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.6, 25.0, 83.0, 38.0, 22.5, 25.62986117130907, 0.4066094180186322, 1.0, 1.0, 25.0, 23.803571117853203], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2822400.0000, 
sim time next is 2823600.0000, 
raw observation next is [6.4, 26.0, 75.0, 63.33333333333334, 22.5, 25.85178508674614, 0.4630252801703251, 1.0, 1.0, 65.0, 56.84591072354026], 
processed observation next is [1.0, 0.6956521739130435, 0.6398891966759004, 0.26, 0.25, 0.0699815837937385, 0.375, 0.6543154238955117, 0.654341760056775, 1.0, 1.0, 1.0, 0.5684591072354026], 
reward next is 0.4315, 
noisyNet noise sample is [array([0.7909815], dtype=float32), -1.3115529]. 
=============================================
[2019-04-09 15:01:30,867] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.00523028 0.02747804 0.06000014 0.00283587 0.30481035 0.12414023
 0.28059724 0.03068515 0.09633555 0.04110764 0.02677946], sum to 1.0000
[2019-04-09 15:01:30,868] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6866
[2019-04-09 15:01:31,023] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.866666666666667, 24.33333333333333, 98.83333333333333, 0.0, 22.5, 25.72819483161469, 0.3851483535190003, 1.0, 1.0, 35.0, 33.39002392209381], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2820000.0000, 
sim time next is 2821200.0000, 
raw observation next is [6.733333333333333, 24.66666666666666, 91.0, 12.66666666666666, 22.5, 25.98829686858357, 0.3819597935192484, 1.0, 1.0, 45.0, 62.68106943483373], 
processed observation next is [1.0, 0.6521739130434783, 0.649122807017544, 0.24666666666666662, 0.30333333333333334, 0.013996316758747691, 0.375, 0.6656914057152976, 0.6273199311730828, 1.0, 1.0, 0.6, 0.6268106943483374], 
reward next is 0.3732, 
noisyNet noise sample is [array([-0.6609164], dtype=float32), 0.2890839]. 
=============================================
[2019-04-09 15:01:31,347] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01195213 0.04909241 0.07389619 0.00366649 0.34806582 0.16074967
 0.1282799  0.03509649 0.09690915 0.0693708  0.02292097], sum to 1.0000
[2019-04-09 15:01:31,365] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2147
[2019-04-09 15:01:31,402] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 19.0, 23.37190266737619, -0.03564333258149324, 0.0, 1.0, 50.0, 51.65242053989943], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2853600.0000, 
sim time next is 2854800.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 19.0, 23.44374907922688, -0.03606271890280961, 0.0, 1.0, 35.0, 37.905769533423005], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.72, 0.0, 0.0, 0.08333333333333333, 0.45364575660224, 0.4879790936990635, 0.0, 1.0, 0.4, 0.37905769533423006], 
reward next is 0.6209, 
noisyNet noise sample is [array([0.13110729], dtype=float32), 0.4216974]. 
=============================================
[2019-04-09 15:01:31,776] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00450664 0.03312042 0.04923015 0.00294786 0.39854977 0.20179968
 0.14124833 0.02667737 0.08556972 0.04175382 0.01459614], sum to 1.0000
[2019-04-09 15:01:31,776] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3984
[2019-04-09 15:01:31,892] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 50.0, 45.5, 387.1666666666667, 22.5, 26.29919669478329, 0.6097772789469186, 1.0, 1.0, 45.0, 55.5937476766644], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2738400.0000, 
sim time next is 2739600.0000, 
raw observation next is [-3.0, 50.0, 28.5, 253.5, 22.5, 26.67029509360344, 0.5956247316895967, 1.0, 1.0, 35.0, 42.45035785153719], 
processed observation next is [1.0, 0.7391304347826086, 0.3795013850415513, 0.5, 0.095, 0.28011049723756903, 0.375, 0.7225245911336199, 0.6985415772298657, 1.0, 1.0, 0.4, 0.4245035785153719], 
reward next is 0.5755, 
noisyNet noise sample is [array([-0.00746147], dtype=float32), 0.036221266]. 
=============================================
[2019-04-09 15:01:32,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01000516 0.03874543 0.05746562 0.00851924 0.3449773  0.15309279
 0.1387919  0.03337731 0.123559   0.06156489 0.0299013 ], sum to 1.0000
[2019-04-09 15:01:32,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9841
[2019-04-09 15:01:32,574] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 19.0, 23.29148721009777, -0.1016253090144162, 0.0, 1.0, 45.0, 34.08008678492736], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 2780400.0000, 
sim time next is 2781600.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 19.0, 23.32563395316904, -0.06885169148863655, 0.0, 1.0, 65.0, 64.90180379539726], 
processed observation next is [1.0, 0.17391304347826086, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.08333333333333333, 0.4438028294307532, 0.4770494361704545, 0.0, 1.0, 1.0, 0.6490180379539726], 
reward next is 0.3510, 
noisyNet noise sample is [array([1.6850591], dtype=float32), 0.4144575]. 
=============================================
[2019-04-09 15:01:32,769] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00926661 0.05282388 0.07163398 0.00440323 0.35234565 0.15261301
 0.13109303 0.03907455 0.08430162 0.07550531 0.02693912], sum to 1.0000
[2019-04-09 15:01:32,770] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8492
[2019-04-09 15:01:32,795] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 23.55671275225846, -0.09202081150760417, 0.0, 1.0, 35.0, 30.644203506718682], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2785200.0000, 
sim time next is 2786400.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 23.48440997162941, -0.06813051725112132, 0.0, 1.0, 55.0, 47.52657702443072], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.4570341643024509, 0.47728982758295957, 0.0, 1.0, 0.8, 0.4752657702443072], 
reward next is 0.5247, 
noisyNet noise sample is [array([-0.82654184], dtype=float32), 0.21808773]. 
=============================================
[2019-04-09 15:01:33,967] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01286099 0.04714002 0.07473528 0.00628483 0.34051916 0.12433557
 0.15202814 0.05275835 0.08641804 0.07229788 0.03062176], sum to 1.0000
[2019-04-09 15:01:33,968] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8000
[2019-04-09 15:01:34,008] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 23.4557306862739, -0.04574941165081981, 0.0, 1.0, 40.0, 28.14681685637613], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 2770800.0000, 
sim time next is 2772000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 23.36444944193007, -0.03659179200780665, 0.0, 1.0, 50.0, 50.06849608196774], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.4470374534941725, 0.48780273599739776, 0.0, 1.0, 0.7, 0.5006849608196774], 
reward next is 0.4993, 
noisyNet noise sample is [array([-0.03132179], dtype=float32), -1.2078081]. 
=============================================
[2019-04-09 15:01:34,037] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[6.305273 ]
 [6.4561157]
 [6.540736 ]
 [6.4660225]
 [6.7704945]], R is [[6.76865292]
 [7.41949844]
 [8.14833069]
 [8.85003185]
 [9.52128506]].
[2019-04-09 15:01:34,295] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01258378 0.043648   0.07592326 0.00375821 0.2938109  0.22383796
 0.16589628 0.03011016 0.06656397 0.05891422 0.02495325], sum to 1.0000
[2019-04-09 15:01:34,295] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3101
[2019-04-09 15:01:34,330] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 19.0, 23.54299035391204, -0.04475919469322887, 0.0, 1.0, 35.0, 31.204663274601945], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2876400.0000, 
sim time next is 2877600.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 22.5, 23.36460341091145, -0.07517531146013795, 1.0, 1.0, 35.0, 27.110710668109224], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.0, 0.0, 0.375, 0.4470502842426208, 0.4749415628466207, 1.0, 1.0, 0.4, 0.2711071066810922], 
reward next is 0.7289, 
noisyNet noise sample is [array([-1.928382], dtype=float32), -0.11972653]. 
=============================================
[2019-04-09 15:01:35,371] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0089168  0.03859928 0.08082534 0.00203988 0.39280838 0.20841616
 0.11469059 0.02938395 0.05355536 0.04833824 0.02242612], sum to 1.0000
[2019-04-09 15:01:35,371] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8803
[2019-04-09 15:01:35,433] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 19.0, 24.29417586723751, 0.134554392418859, 0.0, 1.0, 35.0, 35.039047489471805], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2869200.0000, 
sim time next is 2870400.0000, 
raw observation next is [1.0, 95.33333333333334, 0.0, 0.0, 19.0, 24.1977049794007, 0.1854530726573652, 0.0, 1.0, 55.0, 64.56579403058089], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.5164754149500584, 0.5618176908857884, 0.0, 1.0, 0.8, 0.6456579403058089], 
reward next is 0.3543, 
noisyNet noise sample is [array([0.33400184], dtype=float32), 0.2758013]. 
=============================================
[2019-04-09 15:01:35,908] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00687189 0.02699927 0.06265537 0.0037928  0.3513699  0.1280968
 0.21486112 0.01973177 0.09411299 0.0709372  0.02057082], sum to 1.0000
[2019-04-09 15:01:35,909] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2654
[2019-04-09 15:01:35,964] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 19.0, 24.19311130478822, 0.2593042892162217, 0.0, 1.0, 60.0, 62.10359633160307], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2936400.0000, 
sim time next is 2937600.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 24.48156917712573, 0.2615953614705, 0.0, 1.0, 35.0, 40.83231734495299], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.5401307647604776, 0.5871984538235, 0.0, 1.0, 0.4, 0.4083231734495299], 
reward next is 0.5917, 
noisyNet noise sample is [array([1.5914601], dtype=float32), -0.4008907]. 
=============================================
[2019-04-09 15:01:36,303] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00525733 0.02830933 0.04966212 0.00328707 0.42701825 0.19173732
 0.14538237 0.02222068 0.0733459  0.0380477  0.01573198], sum to 1.0000
[2019-04-09 15:01:36,304] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0562
[2019-04-09 15:01:36,319] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00693071 0.03592783 0.07952099 0.00255946 0.30456346 0.20063701
 0.19566904 0.01969021 0.06344949 0.06517138 0.02588044], sum to 1.0000
[2019-04-09 15:01:36,319] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3707
[2019-04-09 15:01:36,363] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [4.666666666666667, 33.33333333333334, 237.1666666666667, 258.3333333333333, 22.5, 25.35113048529085, 0.2557286034379865, 1.0, 1.0, 40.0, 37.471596743123214], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2812800.0000, 
sim time next is 2814000.0000, 
raw observation next is [5.333333333333334, 31.66666666666667, 227.1666666666667, 144.1666666666667, 22.5, 24.53911257362265, 0.235776069299377, 1.0, 1.0, 25.0, 33.21966907102495], 
processed observation next is [1.0, 0.5652173913043478, 0.6103416435826409, 0.3166666666666667, 0.7572222222222224, 0.15930018416206268, 0.375, 0.5449260478018875, 0.5785920230997923, 1.0, 1.0, 0.2, 0.33219669071024954], 
reward next is 0.6678, 
noisyNet noise sample is [array([1.7663046], dtype=float32), -0.4987521]. 
=============================================
[2019-04-09 15:01:36,368] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[8.594503]
 [8.508992]
 [8.542944]
 [8.47918 ]
 [8.385039]], R is [[ 9.02701569]
 [ 9.56202984]
 [10.00471783]
 [10.59994793]
 [11.10731697]].
[2019-04-09 15:01:36,384] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.666666666666667, 93.0, 87.5, 134.5, 22.5, 22.99042794909518, -0.1208375004045762, 1.0, 1.0, 40.0, 39.50613434729583], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2881200.0000, 
sim time next is 2882400.0000, 
raw observation next is [1.333333333333333, 93.0, 95.66666666666666, 130.0, 22.5, 23.38028079019148, -0.1052474349014595, 1.0, 1.0, 35.0, 31.61372600384897], 
processed observation next is [1.0, 0.34782608695652173, 0.4995383194829178, 0.93, 0.31888888888888883, 0.143646408839779, 0.375, 0.44835673251595676, 0.46491752169951345, 1.0, 1.0, 0.4, 0.3161372600384897], 
reward next is 0.6839, 
noisyNet noise sample is [array([-0.95864743], dtype=float32), -0.92693317]. 
=============================================
[2019-04-09 15:01:36,506] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.01834353 0.04823068 0.05758549 0.00745934 0.37608194 0.12490196
 0.14737402 0.0502673  0.06994727 0.06580885 0.03399957], sum to 1.0000
[2019-04-09 15:01:36,506] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8429
[2019-04-09 15:01:36,539] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 22.01988900649662, -0.3219555777417489, 0.0, 1.0, 35.0, 39.68940077866341], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 2961600.0000, 
sim time next is 2962800.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 21.86155127920574, -0.3144789286964474, 0.0, 1.0, 55.0, 73.79037062551373], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.3217959399338118, 0.39517369043451755, 0.0, 1.0, 0.8, 0.7379037062551373], 
reward next is 0.2621, 
noisyNet noise sample is [array([-0.10062981], dtype=float32), -0.89759064]. 
=============================================
[2019-04-09 15:01:36,634] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.01458259 0.04242706 0.06397135 0.00904746 0.34105688 0.15601237
 0.15777658 0.03633356 0.07598623 0.07353632 0.02926959], sum to 1.0000
[2019-04-09 15:01:36,636] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6935
[2019-04-09 15:01:36,692] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 67.0, 191.0, 67.33333333333331, 19.0, 22.50908562326941, -0.2363586289701772, 0.0, 1.0, 45.0, 35.23187168311878], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2976000.0000, 
sim time next is 2977200.0000, 
raw observation next is [-3.0, 65.0, 217.0, 154.0, 19.0, 22.4244372974042, -0.2390541270297896, 0.0, 1.0, 45.0, 33.73472430009446], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.7233333333333334, 0.17016574585635358, 0.08333333333333333, 0.3687031081170167, 0.4203152909900701, 0.0, 1.0, 0.6, 0.33734724300094465], 
reward next is 0.6627, 
noisyNet noise sample is [array([-0.5703393], dtype=float32), -1.5849509]. 
=============================================
[2019-04-09 15:01:37,641] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 38902: loss 22.2543
[2019-04-09 15:01:37,641] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 38902: learning rate 0.0000
[2019-04-09 15:01:37,880] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0039943  0.01933472 0.07054003 0.00236007 0.34535068 0.19246805
 0.21675894 0.02065874 0.05601575 0.05254595 0.01997277], sum to 1.0000
[2019-04-09 15:01:37,880] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6415
[2019-04-09 15:01:37,924] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 97.66666666666667, 73.16666666666666, 0.0, 22.5, 25.74376807102222, 0.3792041282945859, 1.0, 1.0, 45.0, 36.849643830462746], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2888400.0000, 
sim time next is 2889600.0000, 
raw observation next is [0.6666666666666666, 95.33333333333334, 79.5, 0.0, 22.5, 25.77518448222788, 0.3926108267167502, 1.0, 1.0, 45.0, 37.715894085414135], 
processed observation next is [1.0, 0.43478260869565216, 0.4810710987996307, 0.9533333333333335, 0.265, 0.0, 0.375, 0.6479320401856565, 0.6308702755722501, 1.0, 1.0, 0.6, 0.37715894085414137], 
reward next is 0.6228, 
noisyNet noise sample is [array([-0.41690657], dtype=float32), -0.5575226]. 
=============================================
[2019-04-09 15:01:38,072] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 39049: loss 20.4521
[2019-04-09 15:01:38,073] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 39049: learning rate 0.0000
[2019-04-09 15:01:38,157] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00325119 0.01546169 0.04383056 0.000948   0.53749377 0.14467049
 0.13561518 0.01874084 0.05165848 0.02911462 0.01921511], sum to 1.0000
[2019-04-09 15:01:38,158] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0308
[2019-04-09 15:01:38,203] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.666666666666667, 100.0, 173.1666666666667, 0.0, 22.5, 26.1272047370897, 0.5081978359298751, 1.0, 1.0, 35.0, 25.247290902502748], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2896800.0000, 
sim time next is 2898000.0000, 
raw observation next is [2.0, 100.0, 169.5, 0.0, 22.5, 26.0683651105581, 0.5061455125469472, 1.0, 1.0, 25.0, 22.24062668485781], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.565, 0.0, 0.375, 0.6723637592131751, 0.6687151708489824, 1.0, 1.0, 0.2, 0.2224062668485781], 
reward next is 0.7776, 
noisyNet noise sample is [array([1.4670933], dtype=float32), 2.0807216]. 
=============================================
[2019-04-09 15:01:38,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[9.090384]
 [9.16366 ]
 [9.291393]
 [9.128136]
 [9.168673]], R is [[ 9.80158901]
 [10.45110035]
 [11.0489893 ]
 [11.61147213]
 [12.1211071 ]].
[2019-04-09 15:01:38,301] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 39129: loss 21.0768
[2019-04-09 15:01:38,309] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 39130: learning rate 0.0000
[2019-04-09 15:01:38,480] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01006434 0.03926513 0.0667209  0.00388097 0.40325472 0.11679589
 0.16335362 0.02787233 0.06826417 0.0655655  0.03496242], sum to 1.0000
[2019-04-09 15:01:38,482] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4084
[2019-04-09 15:01:38,527] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 19.0, 22.96316260243288, -0.1458949328944754, 0.0, 1.0, 35.0, 32.304282192228854], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2865600.0000, 
sim time next is 2866800.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 19.0, 22.79970049777859, -0.1834522132285883, 0.0, 1.0, 35.0, 31.152041588458864], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.08333333333333333, 0.3999750414815493, 0.4388492622571372, 0.0, 1.0, 0.4, 0.3115204158845886], 
reward next is 0.6885, 
noisyNet noise sample is [array([1.5472329], dtype=float32), -0.9162257]. 
=============================================
[2019-04-09 15:01:38,637] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00780607 0.04236394 0.05983024 0.00299521 0.3853657  0.14290065
 0.14079109 0.03257861 0.08541815 0.07499915 0.02495119], sum to 1.0000
[2019-04-09 15:01:38,658] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7165
[2019-04-09 15:01:38,683] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 19.0, 22.45681648693566, -0.1973263558564527, 0.0, 1.0, 45.0, 52.844426457028064], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2869200.0000, 
sim time next is 2870400.0000, 
raw observation next is [1.0, 95.33333333333334, 0.0, 0.0, 19.0, 22.60636683614058, -0.1477309077573003, 0.0, 1.0, 45.0, 47.9618145915116], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.383863903011715, 0.45075636408089986, 0.0, 1.0, 0.6, 0.479618145915116], 
reward next is 0.5204, 
noisyNet noise sample is [array([-0.6551285], dtype=float32), -0.092483334]. 
=============================================
[2019-04-09 15:01:38,851] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00931746 0.06098946 0.05904985 0.00714524 0.36277544 0.14710996
 0.143321   0.0389331  0.07701208 0.06584539 0.02850109], sum to 1.0000
[2019-04-09 15:01:38,852] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7471
[2019-04-09 15:01:38,915] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 22.86819823361947, -0.119760785899065, 0.0, 1.0, 60.0, 68.2449884986112], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2960400.0000, 
sim time next is 2961600.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 22.91378727193171, -0.1345540136764614, 0.0, 1.0, 45.0, 55.90589724060614], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.40948227266097587, 0.4551486621078462, 0.0, 1.0, 0.6, 0.5590589724060614], 
reward next is 0.4409, 
noisyNet noise sample is [array([-0.10440478], dtype=float32), 1.4405227]. 
=============================================
[2019-04-09 15:01:38,952] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39341: loss 29.2500
[2019-04-09 15:01:38,954] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39341: learning rate 0.0000
[2019-04-09 15:01:39,065] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0035437  0.02027586 0.0300655  0.00116335 0.5426994  0.12453095
 0.12714647 0.01837734 0.0848957  0.02849813 0.01880349], sum to 1.0000
[2019-04-09 15:01:39,070] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2704
[2019-04-09 15:01:39,111] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 87.33333333333334, 0.0, 0.0, 22.5, 25.3576892106297, 0.4434418959464241, 0.0, 1.0, 20.0, 24.730840102141542], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2920800.0000, 
sim time next is 2922000.0000, 
raw observation next is [-1.0, 82.66666666666667, 0.0, 0.0, 22.5, 25.03242680115535, 0.395451109382068, 1.0, 1.0, 35.0, 21.860136974922867], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.8266666666666667, 0.0, 0.0, 0.375, 0.5860355667629458, 0.6318170364606893, 1.0, 1.0, 0.4, 0.21860136974922867], 
reward next is 0.7814, 
noisyNet noise sample is [array([-0.479318], dtype=float32), 0.68088007]. 
=============================================
[2019-04-09 15:01:39,123] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[9.053621]
 [9.346461]
 [9.442208]
 [9.334781]
 [9.440429]], R is [[10.02276707]
 [10.67523098]
 [11.30713844]
 [11.88557243]
 [12.44930172]].
[2019-04-09 15:01:39,269] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.01013748 0.03977896 0.05539675 0.00558805 0.41123962 0.13190623
 0.19418319 0.02706727 0.05438155 0.04501233 0.02530856], sum to 1.0000
[2019-04-09 15:01:39,269] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1625
[2019-04-09 15:01:39,302] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.00000000000001, 102.5, 759.1666666666667, 19.0, 23.45222869808587, 0.03923251050191749, 0.0, 1.0, 25.0, 30.678672020287962], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2989200.0000, 
sim time next is 2990400.0000, 
raw observation next is [-2.0, 60.0, 98.0, 737.0, 19.0, 23.54589244678272, 0.03490412922328005, 0.0, 1.0, 35.0, 26.90519933309986], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6, 0.32666666666666666, 0.8143646408839779, 0.08333333333333333, 0.46215770389856, 0.5116347097410934, 0.0, 1.0, 0.4, 0.2690519933309986], 
reward next is 0.7309, 
noisyNet noise sample is [array([0.4723287], dtype=float32), -1.6287429]. 
=============================================
[2019-04-09 15:01:39,352] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01648382 0.0549246  0.0660213  0.00867446 0.36847907 0.13581346
 0.16659996 0.03584025 0.06557246 0.05187536 0.02971528], sum to 1.0000
[2019-04-09 15:01:39,352] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5124
[2019-04-09 15:01:39,391] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 21.09724053114836, -0.5060774704677699, 0.0, 1.0, 45.0, 51.93843931988037], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2964000.0000, 
sim time next is 2965200.0000, 
raw observation next is [-4.0, 77.0, 14.0, 15.66666666666666, 19.0, 21.02822981834987, -0.5223191898805236, 0.0, 1.0, 35.0, 39.62731540824015], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.77, 0.04666666666666667, 0.017311233885819514, 0.08333333333333333, 0.25235248486248923, 0.3258936033731588, 0.0, 1.0, 0.4, 0.3962731540824015], 
reward next is 0.6037, 
noisyNet noise sample is [array([-0.10116353], dtype=float32), -0.17026852]. 
=============================================
[2019-04-09 15:01:39,429] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00433498 0.01397771 0.06179551 0.00164762 0.38422573 0.22124764
 0.1617107  0.02619568 0.0544939  0.05073658 0.01963397], sum to 1.0000
[2019-04-09 15:01:39,429] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6868
[2019-04-09 15:01:39,472] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 100.0, 63.5, 0.0, 22.5, 25.35996021201197, 0.2784935276802222, 1.0, 1.0, 35.0, 27.782188006285466], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2887200.0000, 
sim time next is 2888400.0000, 
raw observation next is [0.3333333333333333, 97.66666666666667, 73.16666666666666, 0.0, 22.5, 25.30374750603592, 0.2858455288701328, 1.0, 1.0, 40.0, 32.719081883586455], 
processed observation next is [1.0, 0.43478260869565216, 0.4718374884579871, 0.9766666666666667, 0.24388888888888885, 0.0, 0.375, 0.6086456255029932, 0.5952818429567109, 1.0, 1.0, 0.5, 0.32719081883586454], 
reward next is 0.6728, 
noisyNet noise sample is [array([-0.38063443], dtype=float32), -0.25796694]. 
=============================================
[2019-04-09 15:01:39,608] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2500, global step 39569: loss 27.6536
[2019-04-09 15:01:39,609] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2500, global step 39569: learning rate 0.0000
[2019-04-09 15:01:40,081] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00919103 0.03091716 0.05976944 0.00442693 0.40439814 0.14614052
 0.17271912 0.03070978 0.06988176 0.04697943 0.02486664], sum to 1.0000
[2019-04-09 15:01:40,085] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0593
[2019-04-09 15:01:40,109] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.0, 60.0, 98.0, 737.0, 19.0, 23.67144975818688, 0.0897980108266685, 0.0, 1.0, 40.0, 20.358017849709547], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2990400.0000, 
sim time next is 2991600.0000, 
raw observation next is [-2.0, 60.0, 92.0, 709.0, 19.0, 23.57392873403785, 0.07240759298246477, 0.0, 1.0, 25.0, 18.312944719920694], 
processed observation next is [0.0, 0.6521739130434783, 0.40720221606648205, 0.6, 0.30666666666666664, 0.7834254143646409, 0.08333333333333333, 0.4644940611698208, 0.5241358643274883, 0.0, 1.0, 0.2, 0.18312944719920693], 
reward next is 0.8169, 
noisyNet noise sample is [array([-0.74522805], dtype=float32), -1.1878322]. 
=============================================
[2019-04-09 15:01:40,224] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0075482  0.05299389 0.05621252 0.00341493 0.34810463 0.11909094
 0.21229763 0.03377258 0.08913203 0.05603609 0.02139658], sum to 1.0000
[2019-04-09 15:01:40,224] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1223
[2019-04-09 15:01:40,283] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.333333333333333, 84.66666666666667, 0.0, 0.0, 19.0, 23.78547285285004, 0.1506064282498271, 0.0, 1.0, 60.0, 62.40139555902972], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2946000.0000, 
sim time next is 2947200.0000, 
raw observation next is [-2.666666666666667, 84.33333333333334, 0.0, 0.0, 19.0, 23.97065498300733, 0.1708951414769233, 0.0, 1.0, 35.0, 42.003711522552386], 
processed observation next is [0.0, 0.08695652173913043, 0.38873499538319484, 0.8433333333333334, 0.0, 0.0, 0.08333333333333333, 0.49755458191727736, 0.5569650471589744, 0.0, 1.0, 0.4, 0.42003711522552384], 
reward next is 0.5800, 
noisyNet noise sample is [array([0.24232846], dtype=float32), -0.9436054]. 
=============================================
[2019-04-09 15:01:40,400] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39834: loss 20.1152
[2019-04-09 15:01:40,403] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39834: learning rate 0.0000
[2019-04-09 15:01:40,480] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 39856: loss 24.8482
[2019-04-09 15:01:40,481] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 39856: learning rate 0.0000
[2019-04-09 15:01:40,700] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 39932: loss 23.5449
[2019-04-09 15:01:40,703] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 39933: learning rate 0.0000
[2019-04-09 15:01:40,750] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39953: loss 22.5179
[2019-04-09 15:01:40,753] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39953: learning rate 0.0000
[2019-04-09 15:01:40,820] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01009358 0.0368708  0.05564596 0.00623529 0.4150375  0.14082348
 0.16191345 0.03790067 0.06409617 0.04650658 0.0248765 ], sum to 1.0000
[2019-04-09 15:01:40,820] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4782
[2019-04-09 15:01:40,843] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 23.24949768961524, -0.1050958011060523, 0.0, 1.0, 45.0, 29.808635852343777], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3002400.0000, 
sim time next is 3003600.0000, 
raw observation next is [-2.0, 60.00000000000001, 0.0, 0.0, 19.0, 23.06861875542175, -0.1397921937969779, 0.0, 1.0, 35.0, 23.62138982488566], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6000000000000001, 0.0, 0.0, 0.08333333333333333, 0.42238489628514575, 0.453402602067674, 0.0, 1.0, 0.4, 0.2362138982488566], 
reward next is 0.7638, 
noisyNet noise sample is [array([0.4610299], dtype=float32), -0.8275741]. 
=============================================
[2019-04-09 15:01:40,896] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-09 15:01:40,896] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:01:40,897] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:01:40,899] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run5
[2019-04-09 15:01:40,913] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:01:40,915] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:01:40,916] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:01:40,918] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:01:40,921] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run5
[2019-04-09 15:01:40,942] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run5
[2019-04-09 15:02:22,212] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.02083165], dtype=float32), 0.035346933]
[2019-04-09 15:02:22,212] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [4.4, 74.0, 207.5, 230.5, 22.5, 26.74059420227555, 0.808474515480476, 1.0, 1.0, 35.0, 15.478753069766999]
[2019-04-09 15:02:22,213] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 15:02:22,214] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [0.00454552 0.02612123 0.05757093 0.00232006 0.46367082 0.14430188
 0.15776922 0.02117682 0.0647078  0.0395617  0.01825409], sampled 0.9599703111612522
[2019-04-09 15:03:13,610] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2980.2914 125219.5022 960.5207
[2019-04-09 15:03:13,642] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:13,642] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:13,642] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:13,642] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:13,642] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:13,809] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:13,809] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:13,809] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:13,809] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:13,809] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:24,788] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2891.9698 134361.0828 690.3808
[2019-04-09 15:03:24,820] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:24,820] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:24,820] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:24,820] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:24,820] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:24,929] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:24,929] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:24,929] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:24,929] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:24,929] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:26,805] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2867.9611 136396.3963 445.4095
[2019-04-09 15:03:26,826] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:26,826] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:26,826] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:26,826] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:26,826] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:03:26,941] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:26,941] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:26,941] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:26,941] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:26,941] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:03:27,828] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 40000, evaluation results [40000.0, 2891.9698354362527, 134361.0828440248, 690.3807956855379, 2980.291361130714, 125219.50215630393, 960.5206758406453, 2867.9610934490006, 136396.39625321366, 445.40947043685344]
[2019-04-09 15:03:27,844] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.01010668 0.04044722 0.06507929 0.00723307 0.42677003 0.14573108
 0.14826691 0.02860113 0.05685606 0.05195414 0.01895439], sum to 1.0000
[2019-04-09 15:03:27,845] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6100
[2019-04-09 15:03:27,881] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 77.0, 42.0, 29.0, 19.0, 22.49976650116806, -0.1434096892186315, 0.0, 1.0, 55.0, 69.36689346037652], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2966400.0000, 
sim time next is 2967600.0000, 
raw observation next is [-4.0, 75.0, 70.0, 42.33333333333334, 19.0, 22.65030734105756, -0.1262072668738919, 0.0, 1.0, 40.0, 46.93228544587626], 
processed observation next is [0.0, 0.34782608695652173, 0.3518005540166205, 0.75, 0.23333333333333334, 0.04677716390423574, 0.08333333333333333, 0.38752561175479655, 0.45793091104203604, 0.0, 1.0, 0.5, 0.4693228544587626], 
reward next is 0.5307, 
noisyNet noise sample is [array([0.37985364], dtype=float32), -0.4876981]. 
=============================================
[2019-04-09 15:03:28,195] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40119: loss 25.8822
[2019-04-09 15:03:28,196] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40119: learning rate 0.0000
[2019-04-09 15:03:28,511] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00632049 0.03473881 0.07782193 0.00385773 0.44207564 0.14167672
 0.13967414 0.02868612 0.05646376 0.04901994 0.01966467], sum to 1.0000
[2019-04-09 15:03:28,511] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0718
[2019-04-09 15:03:28,563] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0087133  0.02687727 0.05568745 0.00432751 0.4583764  0.1088272
 0.16897626 0.02630492 0.06354345 0.05399765 0.02436853], sum to 1.0000
[2019-04-09 15:03:28,566] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3266
[2019-04-09 15:03:28,567] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.0, 55.0, 60.5, 506.1666666666666, 19.0, 23.35191679564061, 0.006559263263977534, 0.0, 1.0, 35.0, 32.91554599235703], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 2996400.0000, 
sim time next is 2997600.0000, 
raw observation next is [-1.0, 55.0, 47.66666666666667, 411.5, 19.0, 23.4928927211595, -0.005361610117073225, 0.0, 1.0, 25.0, 28.744230243575796], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.55, 0.1588888888888889, 0.45469613259668507, 0.08333333333333333, 0.45774106009662496, 0.49821279662764223, 0.0, 1.0, 0.2, 0.28744230243575797], 
reward next is 0.7126, 
noisyNet noise sample is [array([-1.6708938], dtype=float32), 0.38223338]. 
=============================================
[2019-04-09 15:03:28,609] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 22.91833809408822, -0.2094104442336109, 0.0, 1.0, 40.0, 28.281646401565666], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3022800.0000, 
sim time next is 3024000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 22.76188194886992, -0.2486500969284594, 0.0, 1.0, 35.0, 25.148880402521478], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.39682349573916004, 0.41711663435718016, 0.0, 1.0, 0.4, 0.2514888040252148], 
reward next is 0.7485, 
noisyNet noise sample is [array([-0.4265947], dtype=float32), -0.9421194]. 
=============================================
[2019-04-09 15:03:28,614] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[6.7387943]
 [6.7184186]
 [6.8694344]
 [6.741003 ]
 [6.7919054]], R is [[7.27998543]
 [7.92436934]
 [8.48730087]
 [9.10347271]
 [9.67951488]].
[2019-04-09 15:03:28,659] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2500, global step 40273: loss 19.0974
[2019-04-09 15:03:28,671] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2500, global step 40273: learning rate 0.0000
[2019-04-09 15:03:28,800] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.01253276 0.04635892 0.0649986  0.00585606 0.43096772 0.12983967
 0.12796831 0.03598693 0.06080897 0.05937488 0.02530722], sum to 1.0000
[2019-04-09 15:03:28,801] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9959
[2019-04-09 15:03:28,861] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 60.66666666666666, 85.66666666666667, 405.0, 19.0, 22.67332186622847, -0.2604093561234123, 0.0, 1.0, 20.0, 30.664750233237747], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3055200.0000, 
sim time next is 3056400.0000, 
raw observation next is [-6.0, 59.0, 91.0, 497.0, 19.0, 22.6180613412751, -0.2172133604634448, 0.0, 1.0, 65.0, 65.47528349592136], 
processed observation next is [0.0, 0.391304347826087, 0.296398891966759, 0.59, 0.30333333333333334, 0.549171270718232, 0.08333333333333333, 0.3848384451062585, 0.42759554651218507, 0.0, 1.0, 1.0, 0.6547528349592135], 
reward next is 0.3452, 
noisyNet noise sample is [array([2.1769154], dtype=float32), 0.9399949]. 
=============================================
[2019-04-09 15:03:29,105] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40436: loss 15.4354
[2019-04-09 15:03:29,112] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40437: learning rate 0.0000
[2019-04-09 15:03:29,280] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 40495: loss 16.8733
[2019-04-09 15:03:29,281] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 40495: learning rate 0.0000
[2019-04-09 15:03:29,307] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00802255 0.02791607 0.06530628 0.00350977 0.42213804 0.13658684
 0.18178335 0.02222501 0.05778197 0.05639202 0.01833803], sum to 1.0000
[2019-04-09 15:03:29,307] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5750
[2019-04-09 15:03:29,333] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 19.0, 23.76600412189337, -0.03992538913605063, 0.0, 1.0, 45.0, 30.956444242772474], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3109200.0000, 
sim time next is 3110400.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 19.0, 23.61709896371119, -0.06989434740462265, 0.0, 1.0, 40.0, 25.29192578298042], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.08333333333333333, 0.4680915803092658, 0.4767018841984591, 0.0, 1.0, 0.5, 0.25291925782980423], 
reward next is 0.7471, 
noisyNet noise sample is [array([-0.36558923], dtype=float32), 1.2404864]. 
=============================================
[2019-04-09 15:03:29,387] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01155465 0.05269142 0.06794607 0.00741691 0.3299266  0.13493475
 0.17449786 0.03340355 0.09020033 0.07041116 0.02701671], sum to 1.0000
[2019-04-09 15:03:29,391] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4851
[2019-04-09 15:03:29,419] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 21.7835821224303, -0.4488325942705815, 0.0, 1.0, 45.0, 32.417786445515596], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3049200.0000, 
sim time next is 3050400.0000, 
raw observation next is [-6.0, 72.66666666666667, 0.0, 0.0, 19.0, 21.6561449287076, -0.4796586501010615, 0.0, 1.0, 40.0, 25.489774396070906], 
processed observation next is [0.0, 0.30434782608695654, 0.296398891966759, 0.7266666666666667, 0.0, 0.0, 0.08333333333333333, 0.3046787440589667, 0.34011378329964614, 0.0, 1.0, 0.5, 0.2548977439607091], 
reward next is 0.7451, 
noisyNet noise sample is [array([-0.8731821], dtype=float32), -0.19611594]. 
=============================================
[2019-04-09 15:03:29,454] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00683111 0.03303599 0.0517849  0.00382052 0.46696928 0.11428382
 0.15560924 0.03144764 0.05821872 0.05329532 0.02470348], sum to 1.0000
[2019-04-09 15:03:29,458] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5980
[2019-04-09 15:03:29,480] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 24.79354134701324, 0.2324465748978009, 0.0, 1.0, 40.0, 29.76466765686324], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3004800.0000, 
sim time next is 3006000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 24.59337728492912, 0.1884012083636538, 0.0, 1.0, 40.0, 26.983646414193935], 
processed observation next is [0.0, 0.8260869565217391, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.5494481070774265, 0.5628004027878846, 0.0, 1.0, 0.5, 0.26983646414193935], 
reward next is 0.7302, 
noisyNet noise sample is [array([0.6348208], dtype=float32), -1.0498995]. 
=============================================
[2019-04-09 15:03:29,487] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[7.047873 ]
 [6.8657603]
 [7.1115828]
 [7.0067935]
 [6.871309 ]], R is [[7.4837389 ]
 [8.11125469]
 [8.69974613]
 [9.24402332]
 [9.61784172]].
[2019-04-09 15:03:29,881] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.0074427  0.03563775 0.06189809 0.00584829 0.41315377 0.14115332
 0.17021812 0.02388499 0.06548043 0.05481392 0.02046853], sum to 1.0000
[2019-04-09 15:03:29,882] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8698
[2019-04-09 15:03:29,928] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [0.3333333333333334, 61.33333333333334, 48.66666666666666, 419.6666666666667, 19.0, 24.97074452436366, 0.2588034970968925, 0.0, 1.0, 35.0, 32.90906682695058], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3084000.0000, 
sim time next is 3085200.0000, 
raw observation next is [0.0, 72.0, 32.0, 287.0, 19.0, 24.88953621776016, 0.2525301879400243, 0.0, 1.0, 55.0, 41.429492727773635], 
processed observation next is [0.0, 0.7391304347826086, 0.46260387811634357, 0.72, 0.10666666666666667, 0.31712707182320443, 0.08333333333333333, 0.57412801814668, 0.5841767293133414, 0.0, 1.0, 0.8, 0.41429492727773637], 
reward next is 0.5857, 
noisyNet noise sample is [array([-1.117587], dtype=float32), 1.7091271]. 
=============================================
[2019-04-09 15:03:30,010] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00500828 0.02006953 0.05957626 0.00195064 0.41500068 0.19758834
 0.13449164 0.02536305 0.06613387 0.06229975 0.01251792], sum to 1.0000
[2019-04-09 15:03:30,010] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5960
[2019-04-09 15:03:30,029] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 19.0, 24.78283985000834, 0.2957531924501042, 0.0, 1.0, 35.0, 24.50389777096893], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2938800.0000, 
sim time next is 2940000.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 24.56806901298543, 0.2548658386856722, 0.0, 1.0, 40.0, 26.23473179914987], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.5473390844154524, 0.5849552795618908, 0.0, 1.0, 0.5, 0.2623473179914987], 
reward next is 0.7377, 
noisyNet noise sample is [array([1.211502], dtype=float32), 1.0683818]. 
=============================================
[2019-04-09 15:03:30,035] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[8.622838]
 [8.994232]
 [9.419669]
 [9.29599 ]
 [9.572772]], R is [[ 9.0075798 ]
 [ 9.67246532]
 [10.30359554]
 [10.89776993]
 [11.470047  ]].
[2019-04-09 15:03:30,307] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00232946 0.02000534 0.04315098 0.00075945 0.561985   0.12207451
 0.12571226 0.01250083 0.07230458 0.02481043 0.01436711], sum to 1.0000
[2019-04-09 15:03:30,307] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5360
[2019-04-09 15:03:30,358] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.666666666666667, 95.33333333333334, 113.1666666666667, 820.0, 22.5, 26.50312500935391, 0.6733532152218045, 1.0, 1.0, 35.0, 12.267478017309], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3154800.0000, 
sim time next is 3156000.0000, 
raw observation next is [7.333333333333333, 97.66666666666666, 112.8333333333333, 820.1666666666667, 22.5, 25.91522998260778, 0.6005118016736748, 1.0, 1.0, 35.0, 12.164362796203305], 
processed observation next is [1.0, 0.5217391304347826, 0.6657433056325024, 0.9766666666666666, 0.376111111111111, 0.9062615101289135, 0.375, 0.6596024985506483, 0.7001706005578917, 1.0, 1.0, 0.4, 0.12164362796203304], 
reward next is 0.8784, 
noisyNet noise sample is [array([1.8003391], dtype=float32), -1.1088794]. 
=============================================
[2019-04-09 15:03:30,363] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[9.898952]
 [9.927228]
 [9.875812]
 [9.861982]
 [9.804556]], R is [[10.76080513]
 [11.53052235]
 [12.26518726]
 [12.95598125]
 [13.66461468]].
[2019-04-09 15:03:30,426] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 40913: loss 19.6987
[2019-04-09 15:03:30,438] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 40914: learning rate 0.0000
[2019-04-09 15:03:30,514] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 40942: loss 17.2782
[2019-04-09 15:03:30,515] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 40942: learning rate 0.0000
[2019-04-09 15:03:30,527] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01215884 0.07880423 0.05278268 0.00931958 0.33980817 0.12437287
 0.20262744 0.03296237 0.06619932 0.05286212 0.02810234], sum to 1.0000
[2019-04-09 15:03:30,528] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4692
[2019-04-09 15:03:30,564] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.666666666666667, 75.0, 0.0, 0.0, 19.0, 22.97653016158525, -0.1509093103614454, 0.0, 1.0, 35.0, 37.23520398189329], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3033600.0000, 
sim time next is 3034800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 23.00299938893167, -0.1725318861983728, 0.0, 1.0, 35.0, 31.64187093647135], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.41691661574430583, 0.44248937126720905, 0.0, 1.0, 0.4, 0.3164187093647135], 
reward next is 0.6836, 
noisyNet noise sample is [array([-0.6651026], dtype=float32), -1.1542599]. 
=============================================
[2019-04-09 15:03:31,358] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01182727 0.04795211 0.08331011 0.00630556 0.36497745 0.11578468
 0.1887498  0.03311266 0.08097064 0.04694359 0.02006611], sum to 1.0000
[2019-04-09 15:03:31,362] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0072
[2019-04-09 15:03:31,380] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 19.0, 21.91713570361327, -0.4265339416333335, 0.0, 1.0, 35.0, 25.160666791144983], 
current ob forecast is [], 
actual action is [0, 15.0], 
sim time this is 3027600.0000, 
sim time next is 3028800.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 19.0, 21.70981428157121, -0.5452399748469686, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.08333333333333333, 0.30915119013093406, 0.3182533417176771, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4527234], dtype=float32), -0.14864407]. 
=============================================
[2019-04-09 15:03:31,572] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00105497 0.01398482 0.02859323 0.0006447  0.6239832  0.15059783
 0.10406021 0.01452517 0.03100103 0.0248464  0.00670837], sum to 1.0000
[2019-04-09 15:03:31,573] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9052
[2019-04-09 15:03:31,624] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.866666666666667, 99.66666666666667, 86.83333333333333, 693.0, 22.5, 27.23678806029017, 0.9860759359165358, 1.0, 1.0, 35.0, 14.402776046482975], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3165600.0000, 
sim time next is 3166800.0000, 
raw observation next is [6.733333333333333, 99.33333333333334, 79.66666666666666, 649.0, 22.5, 27.28902516630428, 1.040363598694257, 1.0, 1.0, 45.0, 20.911788433514264], 
processed observation next is [1.0, 0.6521739130434783, 0.649122807017544, 0.9933333333333334, 0.26555555555555554, 0.7171270718232045, 0.375, 0.7740854305253567, 0.846787866231419, 1.0, 1.0, 0.6, 0.20911788433514264], 
reward next is 0.7909, 
noisyNet noise sample is [array([0.2181038], dtype=float32), 1.1194916]. 
=============================================
[2019-04-09 15:03:32,014] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00557678 0.04148266 0.07217486 0.00450513 0.35381666 0.16464508
 0.18955308 0.02555098 0.06638433 0.05458376 0.02172669], sum to 1.0000
[2019-04-09 15:03:32,014] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6292
[2019-04-09 15:03:32,037] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.666666666666667, 58.33333333333334, 86.0, 681.0, 19.0, 24.04881988842659, 0.17688106884225, 0.0, 1.0, 35.0, 22.11876044116208], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 2992800.0000, 
sim time next is 2994000.0000, 
raw observation next is [-1.333333333333333, 56.66666666666667, 78.5, 634.8333333333334, 19.0, 23.99267322706971, 0.1608068392189081, 0.0, 1.0, 40.0, 19.673474766253293], 
processed observation next is [0.0, 0.6521739130434783, 0.42566943674976926, 0.5666666666666668, 0.26166666666666666, 0.7014732965009208, 0.08333333333333333, 0.4993894355891424, 0.553602279739636, 0.0, 1.0, 0.5, 0.19673474766253293], 
reward next is 0.8033, 
noisyNet noise sample is [array([0.41614336], dtype=float32), 1.192219]. 
=============================================
[2019-04-09 15:03:32,041] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00679394 0.03014531 0.05056394 0.00472894 0.35687009 0.14285582
 0.20005982 0.02914196 0.09717676 0.0596736  0.02198985], sum to 1.0000
[2019-04-09 15:03:32,041] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3161
[2019-04-09 15:03:32,048] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[7.782717 ]
 [7.7873173]
 [7.5618644]
 [7.7948766]
 [7.5247073]], R is [[ 8.33195972]
 [ 9.02745247]
 [ 9.65430355]
 [10.32539177]
 [10.96156311]].
[2019-04-09 15:03:32,077] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.7333333333333334, 85.33333333333334, 0.0, 0.0, 19.0, 24.52036753924089, 0.1321101896518518, 0.0, 1.0, 45.0, 34.1300122320004], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3090000.0000, 
sim time next is 3091200.0000, 
raw observation next is [-0.8666666666666667, 88.66666666666666, 0.0, 0.0, 19.0, 24.43351732748122, 0.1016191685363457, 0.0, 1.0, 35.0, 31.04117139094066], 
processed observation next is [0.0, 0.782608695652174, 0.4385964912280702, 0.8866666666666666, 0.0, 0.0, 0.08333333333333333, 0.5361264439567682, 0.5338730561787819, 0.0, 1.0, 0.4, 0.31041171390940664], 
reward next is 0.6896, 
noisyNet noise sample is [array([1.4950191], dtype=float32), -0.45773226]. 
=============================================
[2019-04-09 15:03:32,196] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 41564: loss 18.9460
[2019-04-09 15:03:32,197] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 41564: learning rate 0.0000
[2019-04-09 15:03:33,495] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01604488 0.05498934 0.06687347 0.00899953 0.28063592 0.1283195
 0.16506357 0.05765207 0.10395528 0.08069994 0.03676645], sum to 1.0000
[2019-04-09 15:03:33,499] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1434
[2019-04-09 15:03:33,521] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 20.99027134679167, -0.5758759726840997, 0.0, 1.0, 45.0, 34.94556781536501], 
current ob forecast is [], 
actual action is [1, 30.0], 
sim time this is 3039600.0000, 
sim time next is 3040800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 20.99831891247648, -0.5884073434976473, 0.0, 1.0, 30.0, 27.049239179418826], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.24985990937304012, 0.30386421883411757, 0.0, 1.0, 0.3, 0.27049239179418827], 
reward next is 0.7295, 
noisyNet noise sample is [array([-0.25323525], dtype=float32), 1.087982]. 
=============================================
[2019-04-09 15:03:33,794] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00201576 0.01439193 0.03379671 0.00080798 0.51243913 0.10818255
 0.22461638 0.01229264 0.04646712 0.02783026 0.01715948], sum to 1.0000
[2019-04-09 15:03:33,795] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2553
[2019-04-09 15:03:33,828] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 92.0, 102.3333333333333, 669.5, 22.5, 25.80023543578206, 0.592884585199216, 1.0, 1.0, 45.0, 31.1399405180741], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3231600.0000, 
sim time next is 3232800.0000, 
raw observation next is [-3.0, 92.0, 105.0, 702.5, 22.5, 25.99827460150928, 0.6273210464506637, 1.0, 1.0, 35.0, 24.253938373389385], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.92, 0.35, 0.7762430939226519, 0.375, 0.6665228834591067, 0.7091070154835545, 1.0, 1.0, 0.4, 0.24253938373389386], 
reward next is 0.7575, 
noisyNet noise sample is [array([0.71670634], dtype=float32), -0.6665393]. 
=============================================
[2019-04-09 15:03:33,857] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00104709 0.02092037 0.04959524 0.00045429 0.45065513 0.1961087
 0.17864838 0.01385013 0.0617741  0.01641687 0.01052963], sum to 1.0000
[2019-04-09 15:03:33,857] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0204
[2019-04-09 15:03:33,909] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.4, 99.33333333333334, 62.33333333333333, 529.0, 22.5, 27.95574447518896, 1.050165877192432, 1.0, 1.0, 40.0, 17.07142064686258], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 3169200.0000, 
sim time next is 3170400.0000, 
raw observation next is [6.2, 99.66666666666666, 49.66666666666667, 435.1666666666667, 22.5, 27.94319751547627, 1.092491337972444, 1.0, 1.0, 20.0, 23.694013369144855], 
processed observation next is [1.0, 0.6956521739130435, 0.6343490304709142, 0.9966666666666666, 0.16555555555555557, 0.4808471454880295, 0.375, 0.8285997929563559, 0.8641637793241479, 1.0, 1.0, 0.1, 0.23694013369144856], 
reward next is 0.7631, 
noisyNet noise sample is [array([-0.67209035], dtype=float32), -0.07950704]. 
=============================================
[2019-04-09 15:03:34,016] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01476319 0.05089499 0.07092473 0.00658987 0.37455484 0.13611755
 0.1233332  0.04424147 0.07709967 0.06824109 0.03323938], sum to 1.0000
[2019-04-09 15:03:34,016] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3026
[2019-04-09 15:03:34,036] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.333333333333333, 57.33333333333334, 96.33333333333333, 589.0000000000001, 19.0, 21.4386904161746, -0.4937824654684792, 0.0, 1.0, 25.0, 21.868373458674075], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3057600.0000, 
sim time next is 3058800.0000, 
raw observation next is [-4.666666666666666, 55.66666666666667, 100.1666666666667, 655.6666666666667, 19.0, 21.37265211401544, -0.4984316810123111, 0.0, 1.0, 35.0, 19.784445555424973], 
processed observation next is [0.0, 0.391304347826087, 0.33333333333333337, 0.5566666666666668, 0.333888888888889, 0.7244935543278086, 0.08333333333333333, 0.2810543428346201, 0.33385610632922963, 0.0, 1.0, 0.4, 0.19784445555424973], 
reward next is 0.8022, 
noisyNet noise sample is [array([-1.1163278], dtype=float32), -0.10999943]. 
=============================================
[2019-04-09 15:03:34,152] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00096962 0.01235629 0.0314015  0.00063646 0.55732894 0.22239698
 0.09773306 0.01135193 0.04012068 0.01608486 0.0096197 ], sum to 1.0000
[2019-04-09 15:03:34,158] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7549
[2019-04-09 15:03:34,204] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.133333333333333, 74.0, 113.3333333333333, 813.0, 22.5, 26.98046768094964, 0.8528582920282811, 1.0, 1.0, 25.0, 37.71442751051283], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3238800.0000, 
sim time next is 3240000.0000, 
raw observation next is [-2.0, 71.0, 114.0, 817.0, 22.5, 26.86030513023126, 0.841996377530414, 1.0, 1.0, 40.0, 25.82012069058836], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.71, 0.38, 0.9027624309392265, 0.375, 0.7383587608526051, 0.7806654591768046, 1.0, 1.0, 0.5, 0.2582012069058836], 
reward next is 0.7418, 
noisyNet noise sample is [array([-0.40271378], dtype=float32), 0.8511047]. 
=============================================
[2019-04-09 15:03:34,224] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[10.626004]
 [10.759375]
 [10.699597]
 [10.415898]
 [10.281646]], R is [[11.61185646]
 [12.11859417]
 [12.67247868]
 [13.06397247]
 [13.61162949]].
[2019-04-09 15:03:34,568] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00127962 0.0049612  0.04206222 0.00056343 0.5543726  0.11061101
 0.19009073 0.0106918  0.06030212 0.01528998 0.00977528], sum to 1.0000
[2019-04-09 15:03:34,569] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6182
[2019-04-09 15:03:34,587] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [3.333333333333333, 100.0, 0.0, 0.0, 22.5, 27.18710095078238, 0.9584528169753647, 0.0, 1.0, 25.0, 27.99801366568819], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3181200.0000, 
sim time next is 3182400.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 22.5, 27.08750990779305, 0.9485566408047529, 1.0, 1.0, 40.0, 25.735988332302405], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.375, 0.7572924923160874, 0.8161855469349176, 1.0, 1.0, 0.5, 0.25735988332302406], 
reward next is 0.7426, 
noisyNet noise sample is [array([1.7844387], dtype=float32), -0.1855172]. 
=============================================
[2019-04-09 15:03:34,612] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00336318 0.02432511 0.052618   0.00118935 0.452161   0.11916868
 0.13704956 0.0161828  0.12883952 0.05002017 0.01508263], sum to 1.0000
[2019-04-09 15:03:34,612] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2336
[2019-04-09 15:03:34,642] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 23.79351713859918, 0.1913201374221221, 0.0, 1.0, 45.0, 39.2578030425917], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3279600.0000, 
sim time next is 3280800.0000, 
raw observation next is [-6.333333333333333, 89.33333333333334, 0.0, 0.0, 19.0, 23.75959222943177, 0.1679390033981174, 0.0, 1.0, 35.0, 26.48077888680877], 
processed observation next is [1.0, 1.0, 0.28716528162511545, 0.8933333333333334, 0.0, 0.0, 0.08333333333333333, 0.47996601911931425, 0.5559796677993725, 0.0, 1.0, 0.4, 0.2648077888680877], 
reward next is 0.7352, 
noisyNet noise sample is [array([0.34636998], dtype=float32), -0.2503646]. 
=============================================
[2019-04-09 15:03:35,387] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0374464e-03 6.4347903e-03 3.1990871e-02 4.7098249e-04 5.3175771e-01
 1.7022164e-01 1.4660095e-01 1.1185229e-02 6.3894771e-02 2.8398253e-02
 8.0073625e-03], sum to 1.0000
[2019-04-09 15:03:35,392] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5434
[2019-04-09 15:03:35,442] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 100.0, 110.1666666666667, 798.8333333333334, 22.5, 26.24682206388855, 0.7817535107272578, 1.0, 1.0, 45.0, 25.762290916146803], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3159600.0000, 
sim time next is 3160800.0000, 
raw observation next is [7.0, 100.0, 106.5, 784.5, 22.5, 27.01267878063066, 0.8628063883312449, 1.0, 1.0, 35.0, 17.927656534042196], 
processed observation next is [1.0, 0.6086956521739131, 0.6565096952908588, 1.0, 0.355, 0.8668508287292818, 0.375, 0.7510565650525551, 0.7876021294437483, 1.0, 1.0, 0.4, 0.17927656534042197], 
reward next is 0.8207, 
noisyNet noise sample is [array([1.2792275], dtype=float32), -0.24829744]. 
=============================================
[2019-04-09 15:03:35,460] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00257302 0.01622381 0.05348251 0.00109087 0.53022367 0.10782939
 0.16279979 0.0281351  0.04690803 0.03696622 0.01376753], sum to 1.0000
[2019-04-09 15:03:35,466] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4519
[2019-04-09 15:03:35,490] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333334, 100.0, 0.0, 0.0, 19.0, 25.98873405978606, 0.5988628756017188, 0.0, 1.0, 35.0, 22.80898356481547], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3202800.0000, 
sim time next is 3204000.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 19.0, 25.6297268389619, 0.5424356809425157, 0.0, 1.0, 35.0, 20.328367615876743], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6358105699134917, 0.6808118936475053, 0.0, 1.0, 0.4, 0.20328367615876744], 
reward next is 0.7967, 
noisyNet noise sample is [array([0.99480724], dtype=float32), -1.1188534]. 
=============================================
[2019-04-09 15:03:35,522] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[10.154992]
 [10.130885]
 [10.257864]
 [10.348201]
 [10.395434]], R is [[10.67602062]
 [11.34117126]
 [11.98943996]
 [12.60811996]
 [13.19296455]].
[2019-04-09 15:03:35,597] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3177206e-03 9.3536628e-03 3.1569201e-02 3.5344716e-04 6.1600006e-01
 1.1529218e-01 1.4846957e-01 9.2218043e-03 3.6293745e-02 2.2656988e-02
 9.4715646e-03], sum to 1.0000
[2019-04-09 15:03:35,599] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5242
[2019-04-09 15:03:35,638] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 22.5, 26.77742601955066, 0.7955863949362784, 1.0, 1.0, 40.0, 29.473323727871787], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3264000.0000, 
sim time next is 3265200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 22.5, 26.65122096486553, 0.7660257576799553, 1.0, 1.0, 35.0, 25.69929611990154], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.65, 0.0, 0.0, 0.375, 0.7209350804054608, 0.7553419192266517, 1.0, 1.0, 0.4, 0.2569929611990154], 
reward next is 0.7430, 
noisyNet noise sample is [array([-0.28260922], dtype=float32), -1.4556907]. 
=============================================
[2019-04-09 15:03:35,654] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00316423 0.02166893 0.05600896 0.00173576 0.44334844 0.11385062
 0.19660935 0.01511068 0.09928615 0.0342362  0.01498074], sum to 1.0000
[2019-04-09 15:03:35,654] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9766
[2019-04-09 15:03:35,698] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-5.333333333333334, 84.66666666666667, 0.0, 0.0, 19.0, 25.10686801053983, 0.4904632398854512, 0.0, 1.0, 45.0, 32.42482070927833], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3273600.0000, 
sim time next is 3274800.0000, 
raw observation next is [-5.666666666666667, 88.33333333333333, 0.0, 0.0, 19.0, 25.08387322068151, 0.5278883444357204, 0.0, 1.0, 60.0, 63.392158308714656], 
processed observation next is [1.0, 0.9130434782608695, 0.30563250230840255, 0.8833333333333333, 0.0, 0.0, 0.08333333333333333, 0.5903227683901259, 0.6759627814785735, 0.0, 1.0, 0.9, 0.6339215830871465], 
reward next is 0.3661, 
noisyNet noise sample is [array([0.09688166], dtype=float32), 0.5566709]. 
=============================================
[2019-04-09 15:03:35,804] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.9560767e-04 7.8618517e-03 1.6188238e-02 3.7269699e-04 7.0822048e-01
 7.7456169e-02 8.5784875e-02 1.3295733e-02 5.4329701e-02 1.8227812e-02
 1.7766818e-02], sum to 1.0000
[2019-04-09 15:03:35,804] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9468
[2019-04-09 15:03:35,875] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [7.666666666666667, 95.33333333333334, 113.1666666666667, 820.0, 22.5, 25.7840275494485, 0.4898108074983533, 1.0, 1.0, 35.0, 11.999562880067451], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3154800.0000, 
sim time next is 3156000.0000, 
raw observation next is [7.333333333333333, 97.66666666666666, 112.8333333333333, 820.1666666666667, 22.5, 25.79333624369132, 0.4454430244532828, 1.0, 1.0, 55.0, 45.42555387827683], 
processed observation next is [1.0, 0.5217391304347826, 0.6657433056325024, 0.9766666666666666, 0.376111111111111, 0.9062615101289135, 0.375, 0.6494446869742768, 0.6484810081510942, 1.0, 1.0, 0.8, 0.4542555387827683], 
reward next is 0.5457, 
noisyNet noise sample is [array([-2.447018], dtype=float32), 0.57928336]. 
=============================================
[2019-04-09 15:03:35,893] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[11.189866]
 [11.123005]
 [11.108823]
 [10.933623]
 [10.91682 ]], R is [[11.83046246]
 [12.59216213]
 [13.32143116]
 [14.02361679]
 [14.69646358]].
[2019-04-09 15:03:36,570] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0025584  0.01617122 0.03345012 0.00134461 0.45246482 0.1614778
 0.20166361 0.02450346 0.0622304  0.03205723 0.01207838], sum to 1.0000
[2019-04-09 15:03:36,570] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7019
[2019-04-09 15:03:36,606] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 74.66666666666667, 0.0, 0.0, 19.0, 24.38671430324749, 0.2658669767157736, 0.0, 1.0, 35.0, 28.998748975684236], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3285600.0000, 
sim time next is 3286800.0000, 
raw observation next is [-7.0, 70.0, 0.0, 0.0, 19.0, 24.26475691175665, 0.2292028128525238, 0.0, 1.0, 45.0, 33.80980318257809], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.7, 0.0, 0.0, 0.08333333333333333, 0.5220630759797208, 0.576400937617508, 0.0, 1.0, 0.6, 0.33809803182578085], 
reward next is 0.6619, 
noisyNet noise sample is [array([-0.05988982], dtype=float32), -0.5986755]. 
=============================================
[2019-04-09 15:03:36,829] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00502044 0.03250454 0.03801486 0.002113   0.45909888 0.18058904
 0.13872825 0.02437321 0.04471133 0.05561925 0.01922718], sum to 1.0000
[2019-04-09 15:03:36,829] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9778
[2019-04-09 15:03:36,871] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 19.0, 23.4323838346845, 0.05366795148953845, 0.0, 1.0, 45.0, 37.00413541142343], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3292800.0000, 
sim time next is 3294000.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 19.0, 23.35737179442414, 0.08347714505647043, 0.0, 1.0, 60.0, 66.04144900213774], 
processed observation next is [1.0, 0.13043478260869565, 0.24099722991689754, 0.77, 0.0, 0.0, 0.08333333333333333, 0.44644764953534494, 0.5278257150188235, 0.0, 1.0, 0.9, 0.6604144900213774], 
reward next is 0.3396, 
noisyNet noise sample is [array([0.41673145], dtype=float32), 0.80373657]. 
=============================================
[2019-04-09 15:03:36,887] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[8.838707 ]
 [8.995154 ]
 [9.28027  ]
 [9.505212 ]
 [9.6374035]], R is [[ 9.06503105]
 [ 9.6043396 ]
 [10.21569729]
 [10.88928127]
 [11.53587055]].
[2019-04-09 15:03:37,156] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.6586567e-04 9.2611080e-03 3.1442229e-02 4.2679009e-04 5.3562343e-01
 9.1388784e-02 2.2636209e-01 9.3749408e-03 4.9483240e-02 3.4714144e-02
 1.1057360e-02], sum to 1.0000
[2019-04-09 15:03:37,156] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7862
[2019-04-09 15:03:37,206] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 19.0, 26.90646151646473, 0.9092990843579599, 0.0, 1.0, 60.0, 36.145412483870686], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3186000.0000, 
sim time next is 3187200.0000, 
raw observation next is [2.666666666666667, 100.0, 0.0, 0.0, 19.0, 26.92075764077558, 0.9058469604713696, 0.0, 1.0, 45.0, 30.178677151779688], 
processed observation next is [1.0, 0.9130434782608695, 0.5364727608494922, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7433964700646317, 0.8019489868237898, 0.0, 1.0, 0.6, 0.30178677151779687], 
reward next is 0.6982, 
noisyNet noise sample is [array([0.17802781], dtype=float32), -0.2124255]. 
=============================================
[2019-04-09 15:03:37,549] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00386466 0.02273226 0.04188057 0.00121534 0.54655755 0.12026442
 0.13854232 0.01069495 0.05873653 0.04389264 0.0116188 ], sum to 1.0000
[2019-04-09 15:03:37,552] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4905
[2019-04-09 15:03:37,603] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.333333333333334, 84.66666666666667, 0.0, 0.0, 19.0, 24.89896456473906, 0.4098936318746467, 0.0, 1.0, 35.0, 20.984793409980554], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3273600.0000, 
sim time next is 3274800.0000, 
raw observation next is [-5.666666666666667, 88.33333333333333, 0.0, 0.0, 19.0, 24.69869159276277, 0.3660354890715822, 0.0, 1.0, 35.0, 19.108989247956814], 
processed observation next is [1.0, 0.9130434782608695, 0.30563250230840255, 0.8833333333333333, 0.0, 0.0, 0.08333333333333333, 0.5582242993968975, 0.6220118296905274, 0.0, 1.0, 0.4, 0.19108989247956815], 
reward next is 0.8089, 
noisyNet noise sample is [array([1.445353], dtype=float32), 0.95774496]. 
=============================================
[2019-04-09 15:03:39,095] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.7362528e-04 4.9746921e-03 3.1999346e-02 4.3712850e-04 6.7950994e-01
 5.8756933e-02 1.6493741e-01 1.0002824e-02 2.3701489e-02 1.8429751e-02
 6.3768760e-03], sum to 1.0000
[2019-04-09 15:03:39,095] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0932
[2019-04-09 15:03:39,165] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.333333333333333, 51.66666666666667, 19.16666666666666, 194.3333333333333, 22.5, 26.42569188680689, 0.5741829155352803, 1.0, 1.0, 45.0, 27.80849021633356], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3345600.0000, 
sim time next is 3346800.0000, 
raw observation next is [-2.666666666666667, 53.33333333333333, 9.166666666666668, 110.8333333333333, 22.5, 26.09805556611747, 0.4356104997838737, 1.0, 1.0, 35.0, 23.316512070887526], 
processed observation next is [1.0, 0.7391304347826086, 0.38873499538319484, 0.5333333333333333, 0.030555555555555558, 0.12246777163904232, 0.375, 0.6748379638431224, 0.6452034999279579, 1.0, 1.0, 0.4, 0.23316512070887527], 
reward next is 0.7668, 
noisyNet noise sample is [array([0.63876885], dtype=float32), -1.2157365]. 
=============================================
[2019-04-09 15:03:39,672] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00173753 0.01054594 0.0473407  0.0007192  0.52333975 0.11719091
 0.17324468 0.01116997 0.06899592 0.03720405 0.00851132], sum to 1.0000
[2019-04-09 15:03:39,674] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3618
[2019-04-09 15:03:39,690] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00524902 0.02301448 0.03997646 0.00180445 0.41326267 0.22093855
 0.13632004 0.03104107 0.06600007 0.04373883 0.01865438], sum to 1.0000
[2019-04-09 15:03:39,691] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8620
[2019-04-09 15:03:39,713] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.333333333333334, 74.33333333333334, 0.0, 0.0, 19.0, 25.91131174078176, 0.6492233159277012, 0.0, 1.0, 40.0, 24.00949479824938], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3270000.0000, 
sim time next is 3271200.0000, 
raw observation next is [-4.666666666666666, 77.66666666666667, 0.0, 0.0, 19.0, 25.73596755759376, 0.6067055688600022, 0.0, 1.0, 35.0, 21.53278118957463], 
processed observation next is [1.0, 0.8695652173913043, 0.33333333333333337, 0.7766666666666667, 0.0, 0.0, 0.08333333333333333, 0.6446639631328134, 0.7022351896200006, 0.0, 1.0, 0.4, 0.21532781189574632], 
reward next is 0.7847, 
noisyNet noise sample is [array([0.24401587], dtype=float32), -1.0087959]. 
=============================================
[2019-04-09 15:03:39,721] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-8.9, 77.0, 0.0, 0.0, 19.0, 23.14663653580817, -0.07803685400071238, 0.0, 1.0, 35.0, 23.404766021495043], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3297600.0000, 
sim time next is 3298800.0000, 
raw observation next is [-9.266666666666667, 76.66666666666667, 0.0, 0.0, 19.0, 22.89536650392012, -0.06310811323236272, 0.0, 1.0, 50.0, 49.65031052199903], 
processed observation next is [1.0, 0.17391304347826086, 0.20590951061865187, 0.7666666666666667, 0.0, 0.0, 0.08333333333333333, 0.40794720866000994, 0.47896396225587906, 0.0, 1.0, 0.7, 0.4965031052199903], 
reward next is 0.5035, 
noisyNet noise sample is [array([0.788332], dtype=float32), -1.1530423]. 
=============================================
[2019-04-09 15:03:40,355] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.00174871 0.01368118 0.07667111 0.00068372 0.5251097  0.11335959
 0.12269606 0.01148055 0.07866932 0.04591583 0.00998416], sum to 1.0000
[2019-04-09 15:03:40,355] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9787
[2019-04-09 15:03:40,380] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-3.666666666666667, 61.66666666666666, 0.0, 0.0, 19.0, 23.97557180855209, 0.1804867808432381, 0.0, 1.0, 25.0, 28.52327195704584], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3357600.0000, 
sim time next is 3358800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 23.88849620803655, 0.2035046265988743, 0.0, 1.0, 60.0, 55.49331338065755], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.49070801733637914, 0.5678348755329581, 0.0, 1.0, 0.9, 0.5549331338065755], 
reward next is 0.4451, 
noisyNet noise sample is [array([-0.99316853], dtype=float32), 0.54175484]. 
=============================================
[2019-04-09 15:03:40,976] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00077514 0.00946333 0.02627533 0.0006193  0.44002497 0.1874823
 0.22725727 0.01350494 0.05132623 0.03270018 0.01057106], sum to 1.0000
[2019-04-09 15:03:40,980] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3814
[2019-04-09 15:03:41,065] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.666666666666667, 53.33333333333333, 9.166666666666668, 110.8333333333333, 22.5, 26.72492935892764, 0.7752331206970098, 1.0, 1.0, 45.0, 38.65895718528465], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3346800.0000, 
sim time next is 3348000.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 22.5, 26.97552458749013, 0.7991321298994912, 1.0, 1.0, 35.0, 26.45411375791644], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 0.375, 0.7479603822908443, 0.7663773766331637, 1.0, 1.0, 0.4, 0.26454113757916436], 
reward next is 0.7355, 
noisyNet noise sample is [array([-0.37848303], dtype=float32), -1.9480693]. 
=============================================
[2019-04-09 15:03:41,068] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[11.889849]
 [12.007198]
 [11.775309]
 [11.743742]
 [11.912933]], R is [[12.35079384]
 [12.84069633]
 [13.44861317]
 [14.04334545]
 [14.60531616]].
[2019-04-09 15:03:41,609] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00256149 0.01561381 0.03991885 0.00069894 0.48029876 0.14001413
 0.17899469 0.01867905 0.05828704 0.04285797 0.02207531], sum to 1.0000
[2019-04-09 15:03:41,611] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7642
[2019-04-09 15:03:41,673] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.666666666666667, 68.0, 109.8333333333333, 719.1666666666667, 22.5, 24.56572104746983, 0.1994404842052236, 1.0, 1.0, 35.0, 21.22616606774314], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3320400.0000, 
sim time next is 3321600.0000, 
raw observation next is [-7.333333333333333, 66.0, 111.8333333333333, 749.6666666666667, 22.5, 24.5216095225803, 0.2390684827155683, 1.0, 1.0, 45.0, 34.444563059021746], 
processed observation next is [1.0, 0.43478260869565216, 0.25946445060018475, 0.66, 0.37277777777777765, 0.8283609576427257, 0.375, 0.5434674602150249, 0.5796894942385228, 1.0, 1.0, 0.6, 0.3444456305902175], 
reward next is 0.6556, 
noisyNet noise sample is [array([1.2043202], dtype=float32), 0.00048867293]. 
=============================================
[2019-04-09 15:03:42,535] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.01914105e-03 1.11246267e-02 2.66458187e-02 5.91803342e-04
 6.78469419e-01 9.64320749e-02 1.10123746e-01 1.36414384e-02
 3.94368507e-02 1.54960249e-02 7.01909838e-03], sum to 1.0000
[2019-04-09 15:03:42,535] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3539
[2019-04-09 15:03:42,593] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.666666666666667, 48.66666666666666, 110.0, 770.6666666666667, 22.5, 26.32808796262249, 0.5883084944143683, 1.0, 1.0, 35.0, 20.885241295285276], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3408000.0000, 
sim time next is 3409200.0000, 
raw observation next is [3.0, 49.0, 112.0, 784.0, 22.5, 26.43824029703012, 0.4625986953432035, 1.0, 1.0, 35.0, 28.715118602578006], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.49, 0.37333333333333335, 0.8662983425414365, 0.375, 0.7031866914191767, 0.6541995651144011, 1.0, 1.0, 0.4, 0.28715118602578005], 
reward next is 0.7128, 
noisyNet noise sample is [array([1.4061868], dtype=float32), 0.31830412]. 
=============================================
[2019-04-09 15:03:42,644] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8406059e-03 1.6195480e-02 4.7379497e-02 4.1688222e-04 5.2206683e-01
 1.2848319e-01 1.5615596e-01 1.4984325e-02 8.0478832e-02 2.2909142e-02
 9.0892510e-03], sum to 1.0000
[2019-04-09 15:03:42,645] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9235
[2019-04-09 15:03:42,678] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.666666666666667, 61.66666666666666, 0.0, 0.0, 19.0, 24.32433145196583, 0.2502972781610255, 0.0, 1.0, 45.0, 32.015217454592175], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3357600.0000, 
sim time next is 3358800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 24.29259866310499, 0.2823374326572037, 0.0, 1.0, 55.0, 53.758887714946695], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.5243832219254158, 0.5941124775524013, 0.0, 1.0, 0.8, 0.5375888771494669], 
reward next is 0.4624, 
noisyNet noise sample is [array([-0.35600394], dtype=float32), -1.0598998]. 
=============================================
[2019-04-09 15:03:43,441] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00461272 0.01433345 0.02431547 0.00198419 0.43965185 0.18070482
 0.20668407 0.03008161 0.04454333 0.03332947 0.01975906], sum to 1.0000
[2019-04-09 15:03:43,463] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6004
[2019-04-09 15:03:43,495] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 19.0, 21.86859948181408, -0.3633624322212299, 0.0, 1.0, 35.0, 24.865408781165826], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3307200.0000, 
sim time next is 3308400.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 19.0, 21.66955880620885, -0.3912234720015718, 0.0, 1.0, 40.0, 29.66425489932955], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.76, 0.0, 0.0, 0.08333333333333333, 0.30579656718407094, 0.3695921759994761, 0.0, 1.0, 0.5, 0.2966425489932955], 
reward next is 0.7034, 
noisyNet noise sample is [array([0.5615834], dtype=float32), -1.498943]. 
=============================================
[2019-04-09 15:03:44,122] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00326358 0.02512627 0.03222532 0.0011386  0.445499   0.18537155
 0.1560689  0.01792347 0.06896438 0.04506767 0.01935123], sum to 1.0000
[2019-04-09 15:03:44,122] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7178
[2019-04-09 15:03:44,174] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 63.33333333333333, 0.0, 0.0, 19.0, 22.98276835819244, -0.1853454760255233, 0.0, 1.0, 45.0, 34.65241056554107], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3393600.0000, 
sim time next is 3394800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 22.7425590769605, -0.2155700181464408, 0.0, 1.0, 45.0, 34.80316498904741], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.395213256413375, 0.4281433272845197, 0.0, 1.0, 0.6, 0.3480316498904741], 
reward next is 0.6520, 
noisyNet noise sample is [array([1.5616387], dtype=float32), -1.6853462]. 
=============================================
[2019-04-09 15:03:44,196] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.2480371e-04 1.8348014e-02 2.9131107e-02 3.7392889e-04 6.6023093e-01
 9.8576672e-02 1.2001116e-01 7.7905096e-03 2.1559091e-02 2.9569298e-02
 1.3684449e-02], sum to 1.0000
[2019-04-09 15:03:44,205] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8660
[2019-04-09 15:03:44,267] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00230742 0.02093009 0.04138824 0.00130508 0.44661352 0.16141523
 0.16795601 0.02901614 0.05625152 0.04297001 0.02984669], sum to 1.0000
[2019-04-09 15:03:44,267] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4768
[2019-04-09 15:03:44,286] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 108.0, 790.5, 22.5, 26.00227040193494, 0.5454506166600582, 1.0, 1.0, 35.0, 30.715192758341516], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3420000.0000, 
sim time next is 3421200.0000, 
raw observation next is [3.0, 52.0, 104.6666666666667, 780.1666666666667, 22.5, 26.17758511162276, 0.4672813382166836, 1.0, 1.0, 45.0, 38.01151515316502], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.52, 0.348888888888889, 0.8620626151012892, 0.375, 0.6814654259685634, 0.6557604460722278, 1.0, 1.0, 0.6, 0.3801151515316502], 
reward next is 0.6199, 
noisyNet noise sample is [array([1.3607513], dtype=float32), -0.44445428]. 
=============================================
[2019-04-09 15:03:44,291] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6666666666666667, 68.66666666666667, 0.0, 0.0, 19.0, 24.57828249122795, 0.2196718509501063, 0.0, 1.0, 45.0, 31.706133854241756], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3471600.0000, 
sim time next is 3472800.0000, 
raw observation next is [0.3333333333333334, 70.33333333333334, 0.0, 0.0, 19.0, 24.36545662505564, 0.1807389717641186, 0.0, 1.0, 35.0, 25.29112469110547], 
processed observation next is [1.0, 0.17391304347826086, 0.4718374884579871, 0.7033333333333335, 0.0, 0.0, 0.08333333333333333, 0.5304547187546366, 0.5602463239213729, 0.0, 1.0, 0.4, 0.2529112469110547], 
reward next is 0.7471, 
noisyNet noise sample is [array([0.1442329], dtype=float32), 0.1434427]. 
=============================================
[2019-04-09 15:03:44,583] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.31562655e-03 1.12347230e-02 2.29354557e-02 5.29815617e-04
 6.16945028e-01 1.15321755e-01 1.36775643e-01 8.31389800e-03
 4.64945212e-02 2.96452548e-02 1.04882848e-02], sum to 1.0000
[2019-04-09 15:03:44,584] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9066
[2019-04-09 15:03:44,630] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 19.0, 25.4384205718405, 0.447705921593114, 0.0, 1.0, 35.0, 23.84955036190935], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 3454800.0000, 
sim time next is 3456000.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.0, 25.23390898974039, 0.4477396768946426, 0.0, 1.0, 50.0, 38.77761139723038], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6028257491450324, 0.6492465589648809, 0.0, 1.0, 0.7, 0.3877761139723038], 
reward next is 0.6122, 
noisyNet noise sample is [array([1.8305341], dtype=float32), -1.3009691]. 
=============================================
[2019-04-09 15:03:44,636] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[11.490774]
 [11.796778]
 [11.725834]
 [11.748482]
 [12.049039]], R is [[12.07153606]
 [12.7123251 ]
 [13.32170868]
 [13.89637852]
 [14.4092207 ]].
[2019-04-09 15:03:44,686] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.00278032 0.0216218  0.04830952 0.00188274 0.4340029  0.16817279
 0.1440653  0.02516011 0.07459466 0.04879302 0.03061679], sum to 1.0000
[2019-04-09 15:03:44,687] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2167
[2019-04-09 15:03:44,726] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 70.33333333333334, 0.0, 0.0, 19.0, 23.70115431324846, 0.0671771435380523, 0.0, 1.0, 35.0, 20.122620040076423], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3468000.0000, 
sim time next is 3469200.0000, 
raw observation next is [1.0, 68.66666666666667, 0.0, 0.0, 19.0, 23.6727160676929, 0.05919523539782282, 0.0, 1.0, 35.0, 20.237680384515897], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.6866666666666668, 0.0, 0.0, 0.08333333333333333, 0.4727263389744083, 0.5197317451326077, 0.0, 1.0, 0.4, 0.20237680384515896], 
reward next is 0.7976, 
noisyNet noise sample is [array([0.6433452], dtype=float32), -0.3024761]. 
=============================================
[2019-04-09 15:03:44,841] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0014371  0.01621916 0.03086969 0.00083836 0.57102484 0.13595152
 0.11814466 0.01013134 0.06719493 0.03648759 0.01170077], sum to 1.0000
[2019-04-09 15:03:44,841] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9399
[2019-04-09 15:03:44,870] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 25.12037915272176, 0.3405964868814159, 0.0, 1.0, 40.0, 30.461144871849946], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3370800.0000, 
sim time next is 3372000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 25.00930487285072, 0.3674437129491083, 0.0, 1.0, 55.0, 47.33847767858658], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.5841087394042267, 0.6224812376497028, 0.0, 1.0, 0.8, 0.47338477678586577], 
reward next is 0.5266, 
noisyNet noise sample is [array([1.0209504], dtype=float32), -1.4163461]. 
=============================================
[2019-04-09 15:03:44,873] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[10.777585]
 [11.122022]
 [11.451663]
 [10.999479]
 [11.421313]], R is [[11.41865063]
 [11.99985313]
 [12.5322504 ]
 [12.90198612]
 [13.49860573]].
[2019-04-09 15:03:45,218] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.6044258e-04 7.2777271e-03 2.3835892e-02 3.1951669e-04 6.3186890e-01
 1.8096142e-01 1.0690914e-01 1.0119166e-02 1.9234763e-02 1.2919858e-02
 5.7931421e-03], sum to 1.0000
[2019-04-09 15:03:45,218] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4540
[2019-04-09 15:03:45,327] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.666666666666666, 54.00000000000001, 117.6666666666667, 808.8333333333334, 22.5, 25.23095564783514, 0.3810224589080166, 1.0, 1.0, 45.0, 43.73029151134081], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3327600.0000, 
sim time next is 3328800.0000, 
raw observation next is [-5.333333333333333, 54.0, 117.3333333333333, 809.1666666666667, 22.5, 25.37843296688391, 0.3870316340049708, 1.0, 1.0, 35.0, 26.461300123051863], 
processed observation next is [1.0, 0.5217391304347826, 0.3148661126500462, 0.54, 0.391111111111111, 0.8941068139963169, 0.375, 0.6148694139069925, 0.6290105446683236, 1.0, 1.0, 0.4, 0.2646130012305186], 
reward next is 0.7354, 
noisyNet noise sample is [array([-0.18061088], dtype=float32), -0.11239213]. 
=============================================
[2019-04-09 15:03:45,389] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00304419 0.01433951 0.04402645 0.00104493 0.54704756 0.09494179
 0.17091388 0.02049592 0.05092891 0.0390937  0.01412313], sum to 1.0000
[2019-04-09 15:03:45,389] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4182
[2019-04-09 15:03:45,433] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 19.0, 24.2880672540746, 0.08918816491961128, 0.0, 1.0, 35.0, 29.057109821620084], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3391200.0000, 
sim time next is 3392400.0000, 
raw observation next is [-3.0, 61.66666666666667, 0.0, 0.0, 19.0, 24.04194649582431, 0.04768157247912752, 0.0, 1.0, 35.0, 25.168906109906334], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6166666666666667, 0.0, 0.0, 0.08333333333333333, 0.5034955413186925, 0.5158938574930425, 0.0, 1.0, 0.4, 0.25168906109906336], 
reward next is 0.7483, 
noisyNet noise sample is [array([0.39899647], dtype=float32), 1.3221616]. 
=============================================
[2019-04-09 15:03:45,955] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00154063 0.01190832 0.02458993 0.00087021 0.6455505  0.1044689
 0.1046358  0.0152025  0.043859   0.03482253 0.01255169], sum to 1.0000
[2019-04-09 15:03:45,957] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1780
[2019-04-09 15:03:45,989] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 71.0, 89.83333333333334, 444.1666666666666, 22.5, 24.50595895393037, 0.2472281910632094, 1.0, 1.0, 35.0, 23.637526269885967], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3487200.0000, 
sim time next is 3488400.0000, 
raw observation next is [-1.0, 71.0, 93.5, 534.5, 22.5, 24.74185391192086, 0.2805700728054358, 1.0, 1.0, 35.0, 23.023719159877423], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.71, 0.31166666666666665, 0.5906077348066299, 0.375, 0.5618211593267383, 0.593523357601812, 1.0, 1.0, 0.4, 0.23023719159877423], 
reward next is 0.7698, 
noisyNet noise sample is [array([0.29786354], dtype=float32), -0.0033726988]. 
=============================================
[2019-04-09 15:03:46,515] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00084528 0.01409496 0.05847182 0.00067351 0.5545092  0.15840003
 0.1229841  0.0068132  0.05370984 0.01995064 0.00954749], sum to 1.0000
[2019-04-09 15:03:46,515] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8460
[2019-04-09 15:03:46,598] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00101828 0.01643126 0.02970114 0.00080696 0.59121865 0.14367321
 0.10423101 0.01569876 0.05969583 0.0299101  0.00761476], sum to 1.0000
[2019-04-09 15:03:46,599] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9963
[2019-04-09 15:03:46,636] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 50.0, 35.5, 317.0, 22.5, 26.51287878211019, 0.6106831846776585, 1.0, 1.0, 35.0, 18.11886563430059], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3344400.0000, 
sim time next is 3345600.0000, 
raw observation next is [-2.333333333333333, 51.66666666666667, 19.16666666666666, 194.3333333333333, 22.5, 26.53641026062069, 0.472995452105458, 1.0, 1.0, 45.0, 45.050313354262585], 
processed observation next is [1.0, 0.7391304347826086, 0.3979686057248385, 0.5166666666666667, 0.06388888888888887, 0.21473296500920805, 0.375, 0.7113675217183909, 0.6576651507018193, 1.0, 1.0, 0.6, 0.45050313354262583], 
reward next is 0.5495, 
noisyNet noise sample is [array([-0.57585686], dtype=float32), 2.136947]. 
=============================================
[2019-04-09 15:03:46,816] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 19.0, 24.95755731141151, 0.3454765383526247, 0.0, 1.0, 35.0, 29.278353805134508], 
current ob forecast is [], 
actual action is [1, 65.0], 
sim time this is 3456000.0000, 
sim time next is 3457200.0000, 
raw observation next is [1.0, 83.66666666666667, 0.0, 0.0, 19.0, 24.8989942009657, 0.3968691400313282, 0.0, 1.0, 65.0, 61.54892816215086], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.5749161834138082, 0.632289713343776, 0.0, 1.0, 1.0, 0.6154892816215086], 
reward next is 0.3845, 
noisyNet noise sample is [array([-1.0283842], dtype=float32), -0.2938596]. 
=============================================
[2019-04-09 15:03:47,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00216502 0.01476404 0.03178392 0.00083878 0.5638191  0.13090032
 0.13808136 0.02069228 0.05291606 0.02779283 0.01624621], sum to 1.0000
[2019-04-09 15:03:47,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6770
[2019-04-09 15:03:47,181] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.3333333333333334, 63.66666666666667, 100.6666666666667, 686.6666666666667, 22.5, 25.02599435483884, 0.3382744757827057, 1.0, 1.0, 35.0, 23.253009364148316], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3490800.0000, 
sim time next is 3492000.0000, 
raw observation next is [0.0, 60.0, 104.0, 720.0, 22.5, 25.32365067043374, 0.3991063279984888, 1.0, 1.0, 40.0, 27.12755683387869], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.6, 0.3466666666666667, 0.7955801104972375, 0.375, 0.6103042225361449, 0.633035442666163, 1.0, 1.0, 0.5, 0.2712755683387869], 
reward next is 0.7287, 
noisyNet noise sample is [array([0.5081898], dtype=float32), 0.43070322]. 
=============================================
[2019-04-09 15:03:47,193] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[10.981777]
 [10.790373]
 [10.807323]
 [10.45242 ]
 [10.080191]], R is [[11.91627312]
 [12.56458092]
 [13.13445759]
 [13.71210098]
 [14.15652466]].
[2019-04-09 15:03:47,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.7148725e-04 6.8019624e-03 2.6868016e-02 3.1249586e-04 5.9225059e-01
 1.3462435e-01 1.8655822e-01 5.4309834e-03 2.3130156e-02 1.4212752e-02
 9.0390053e-03], sum to 1.0000
[2019-04-09 15:03:47,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3561
[2019-04-09 15:03:47,312] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6666666666666666, 60.66666666666667, 110.0, 776.6666666666667, 22.5, 25.53590299217804, 0.4468004653705961, 1.0, 1.0, 35.0, 18.25379755935363], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3494400.0000, 
sim time next is 3495600.0000, 
raw observation next is [1.0, 61.0, 112.0, 790.0, 22.5, 25.50984409689781, 0.4412274607937063, 1.0, 1.0, 35.0, 18.205583281531382], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.61, 0.37333333333333335, 0.8729281767955801, 0.375, 0.625820341408151, 0.6470758202645688, 1.0, 1.0, 0.4, 0.18205583281531382], 
reward next is 0.8179, 
noisyNet noise sample is [array([-0.79467154], dtype=float32), -0.3053666]. 
=============================================
[2019-04-09 15:03:47,414] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6903089e-04 7.2223782e-03 3.4652896e-02 2.1415346e-04 5.4304051e-01
 2.0435934e-01 1.3347428e-01 7.2284350e-03 5.4476455e-02 8.1400899e-03
 6.8224161e-03], sum to 1.0000
[2019-04-09 15:03:47,415] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1708
[2019-04-09 15:03:47,468] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 67.0, 52.83333333333334, 447.6666666666667, 22.5, 27.32340009072416, 0.5956702733071044, 1.0, 1.0, 35.0, 25.434935195367707], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3429600.0000, 
sim time next is 3430800.0000, 
raw observation next is [2.0, 67.0, 36.5, 317.0, 22.5, 26.77347516094621, 0.7907561778942416, 1.0, 1.0, 35.0, 22.428500320112768], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.12166666666666667, 0.35027624309392263, 0.375, 0.7311229300788508, 0.7635853926314139, 1.0, 1.0, 0.4, 0.22428500320112768], 
reward next is 0.7757, 
noisyNet noise sample is [array([-0.45922557], dtype=float32), 2.2701569]. 
=============================================
[2019-04-09 15:03:47,579] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.09075238e-04 6.54300209e-03 2.83941142e-02 3.01591645e-04
 5.76132715e-01 1.32371396e-01 1.95893228e-01 5.56016713e-03
 3.43522765e-02 1.36134885e-02 6.32896181e-03], sum to 1.0000
[2019-04-09 15:03:47,580] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1965
[2019-04-09 15:03:47,668] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 61.0, 112.0, 790.0, 22.5, 26.37799027990521, 0.6394082479004558, 1.0, 1.0, 40.0, 25.48416079952819], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3495600.0000, 
sim time next is 3496800.0000, 
raw observation next is [1.333333333333333, 59.66666666666667, 114.0, 803.3333333333333, 22.5, 25.84112987467169, 0.581471427848739, 1.0, 1.0, 35.0, 23.349027336467525], 
processed observation next is [1.0, 0.4782608695652174, 0.4995383194829178, 0.5966666666666667, 0.38, 0.8876611418047882, 0.375, 0.6534274895559742, 0.6938238092829131, 1.0, 1.0, 0.4, 0.23349027336467526], 
reward next is 0.7665, 
noisyNet noise sample is [array([0.02978881], dtype=float32), -0.106784955]. 
=============================================
[2019-04-09 15:03:47,764] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.8119314e-04 5.3181895e-03 1.9286865e-02 2.3883881e-04 4.8215351e-01
 1.9604360e-01 2.1464986e-01 1.0289175e-02 4.2766355e-02 1.6651148e-02
 1.1921243e-02], sum to 1.0000
[2019-04-09 15:03:47,767] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8322
[2019-04-09 15:03:47,824] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 58.0, 93.5, 739.5, 22.5, 25.56048304823189, 0.6776607388399966, 1.0, 1.0, 55.0, 48.34263429249707], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3423600.0000, 
sim time next is 3424800.0000, 
raw observation next is [2.666666666666667, 61.0, 87.16666666666666, 715.8333333333334, 22.5, 27.17740985022026, 0.8132564521156563, 1.0, 1.0, 45.0, 31.854149228407827], 
processed observation next is [1.0, 0.6521739130434783, 0.5364727608494922, 0.61, 0.2905555555555555, 0.7909760589318601, 0.375, 0.7647841541850218, 0.7710854840385521, 1.0, 1.0, 0.6, 0.3185414922840783], 
reward next is 0.6815, 
noisyNet noise sample is [array([0.23795696], dtype=float32), -0.09187273]. 
=============================================
[2019-04-09 15:03:48,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00234988 0.01538374 0.02909071 0.00112682 0.47982436 0.12521832
 0.22164924 0.01730867 0.05753464 0.03500018 0.01551347], sum to 1.0000
[2019-04-09 15:03:48,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8641
[2019-04-09 15:03:48,119] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 23.9533926163979, 0.1254055792527977, 0.0, 1.0, 60.0, 60.08551294990583], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3374400.0000, 
sim time next is 3375600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 23.99362851105219, 0.135820562791253, 0.0, 1.0, 45.0, 35.50819205925514], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.4994690425876825, 0.5452735209304177, 0.0, 1.0, 0.6, 0.3550819205925514], 
reward next is 0.6449, 
noisyNet noise sample is [array([-0.50649905], dtype=float32), 0.049236827]. 
=============================================
[2019-04-09 15:03:48,332] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.8848755e-04 3.3759878e-03 1.7451104e-02 1.6109714e-04 6.8250442e-01
 8.1623502e-02 1.5978788e-01 6.1245961e-03 2.4350295e-02 1.5398500e-02
 8.4341234e-03], sum to 1.0000
[2019-04-09 15:03:48,332] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7559
[2019-04-09 15:03:48,360] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 22.5, 26.35448956720766, 0.6673768302083136, 1.0, 1.0, 25.0, 24.516328491782758], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3438000.0000, 
sim time next is 3439200.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 22.5, 26.0567062869485, 0.612783197843544, 0.0, 1.0, 40.0, 26.2567532155324], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.79, 0.0, 0.0, 0.375, 0.6713921905790418, 0.704261065947848, 0.0, 1.0, 0.5, 0.26256753215532397], 
reward next is 0.7374, 
noisyNet noise sample is [array([-0.5436952], dtype=float32), 1.091038]. 
=============================================
[2019-04-09 15:03:48,395] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00316455 0.02847556 0.04943502 0.00145987 0.3917699  0.25546396
 0.13640115 0.01534794 0.06153254 0.03928809 0.0176614 ], sum to 1.0000
[2019-04-09 15:03:48,403] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3494
[2019-04-09 15:03:48,426] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 68.66666666666667, 0.0, 0.0, 19.0, 24.12015775187836, 0.1841056007897652, 0.0, 1.0, 35.0, 35.1376165582779], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3469200.0000, 
sim time next is 3470400.0000, 
raw observation next is [1.0, 67.0, 0.0, 0.0, 19.0, 24.30524537266101, 0.1674512855631729, 0.0, 1.0, 35.0, 28.83030415698412], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.67, 0.0, 0.0, 0.08333333333333333, 0.5254371143884174, 0.5558170951877243, 0.0, 1.0, 0.4, 0.2883030415698412], 
reward next is 0.7117, 
noisyNet noise sample is [array([-0.67127734], dtype=float32), 1.9280082]. 
=============================================
[2019-04-09 15:03:48,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00387355 0.01695433 0.03247261 0.00131495 0.43075716 0.16537224
 0.23960124 0.00853777 0.03796701 0.05004548 0.0131036 ], sum to 1.0000
[2019-04-09 15:03:48,553] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3759
[2019-04-09 15:03:48,595] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 19.0, 23.62970499033447, -0.009713927284358853, 0.0, 1.0, 35.0, 29.30503360740203], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3387600.0000, 
sim time next is 3388800.0000, 
raw observation next is [-4.333333333333334, 67.33333333333334, 0.0, 0.0, 19.0, 23.52693599922718, -0.06385851534440133, 0.0, 1.0, 35.0, 27.82970365055916], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.6733333333333335, 0.0, 0.0, 0.08333333333333333, 0.4605779999355984, 0.47871382821853287, 0.0, 1.0, 0.4, 0.2782970365055916], 
reward next is 0.7217, 
noisyNet noise sample is [array([-0.22966516], dtype=float32), 0.39749104]. 
=============================================
[2019-04-09 15:03:48,957] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 46753: loss 16.4020
[2019-04-09 15:03:48,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 46753: learning rate 0.0000
[2019-04-09 15:03:49,395] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0817877e-04 2.2930086e-03 1.8727666e-02 1.8553583e-04 5.6725723e-01
 2.0865089e-01 1.7254370e-01 3.8306734e-03 1.3386535e-02 1.0614924e-02
 2.2016964e-03], sum to 1.0000
[2019-04-09 15:03:49,396] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4431
[2019-04-09 15:03:49,427] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 55.0, 99.83333333333334, 763.1666666666667, 22.5, 26.67134086957607, 0.7114048143368272, 1.0, 1.0, 35.0, 23.41791037224079], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3422400.0000, 
sim time next is 3423600.0000, 
raw observation next is [3.0, 58.0, 93.5, 739.5, 22.5, 26.82137397867811, 0.7354959051493125, 1.0, 1.0, 35.0, 19.088222394341525], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.58, 0.31166666666666665, 0.8171270718232044, 0.375, 0.7351144982231759, 0.7451653017164376, 1.0, 1.0, 0.4, 0.19088222394341525], 
reward next is 0.8091, 
noisyNet noise sample is [array([-0.1121187], dtype=float32), -0.5537384]. 
=============================================
[2019-04-09 15:03:49,508] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3000, global step 46961: loss 31.0128
[2019-04-09 15:03:49,511] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3000, global step 46962: learning rate 0.0000
[2019-04-09 15:03:49,606] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00825753 0.02648471 0.05169093 0.00283854 0.44916272 0.12302177
 0.18511747 0.02601267 0.05915066 0.0501978  0.01806524], sum to 1.0000
[2019-04-09 15:03:49,606] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7066
[2019-04-09 15:03:49,631] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 23.50711111980183, 0.06932377742986597, 0.0, 1.0, 45.0, 35.26714852141369], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3556800.0000, 
sim time next is 3558000.0000, 
raw observation next is [-4.333333333333334, 69.0, 0.0, 0.0, 19.0, 23.4700090922384, 0.0721692957889678, 0.0, 1.0, 45.0, 34.942966401756394], 
processed observation next is [0.0, 0.17391304347826086, 0.3425669436749769, 0.69, 0.0, 0.0, 0.08333333333333333, 0.4558340910198666, 0.5240564319296559, 0.0, 1.0, 0.6, 0.3494296640175639], 
reward next is 0.6506, 
noisyNet noise sample is [array([1.1589042], dtype=float32), -0.9091814]. 
=============================================
[2019-04-09 15:03:49,653] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[7.9984603]
 [8.070831 ]
 [7.909515 ]
 [7.840421 ]
 [8.659095 ]], R is [[ 8.63261032]
 [ 9.19361305]
 [ 9.83103371]
 [10.51830769]
 [11.17886925]].
[2019-04-09 15:03:49,707] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0070638  0.03152277 0.04861265 0.00360077 0.39728516 0.14253187
 0.19042237 0.04187831 0.04717711 0.06174467 0.02816041], sum to 1.0000
[2019-04-09 15:03:49,707] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8167
[2019-04-09 15:03:49,731] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 69.0, 0.0, 0.0, 19.0, 23.62781386764376, 0.1091712396396207, 0.0, 1.0, 35.0, 35.64068361322695], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3558000.0000, 
sim time next is 3559200.0000, 
raw observation next is [-4.666666666666666, 67.0, 0.0, 0.0, 19.0, 23.76415976342093, 0.09346156942940016, 0.0, 1.0, 45.0, 29.055439684521335], 
processed observation next is [0.0, 0.17391304347826086, 0.33333333333333337, 0.67, 0.0, 0.0, 0.08333333333333333, 0.4803466469517443, 0.5311538564764667, 0.0, 1.0, 0.6, 0.29055439684521334], 
reward next is 0.7094, 
noisyNet noise sample is [array([0.10594171], dtype=float32), -0.8707634]. 
=============================================
[2019-04-09 15:03:50,023] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5206749e-04 5.6236037e-03 2.9987710e-02 1.5502445e-04 6.0635841e-01
 1.4420234e-01 1.3297987e-01 6.1377753e-03 4.4289321e-02 2.0894382e-02
 9.0194587e-03], sum to 1.0000
[2019-04-09 15:03:50,027] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6189
[2019-04-09 15:03:50,051] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 22.5, 25.79932266426137, 0.5469149950976822, 1.0, 1.0, 35.0, 17.17612039296638], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3438000.0000, 
sim time next is 3439200.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 22.5, 25.52487231250446, 0.5011377088838832, 0.0, 1.0, 35.0, 19.492015366440988], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.79, 0.0, 0.0, 0.375, 0.627072692708705, 0.6670459029612944, 0.0, 1.0, 0.4, 0.19492015366440987], 
reward next is 0.8051, 
noisyNet noise sample is [array([-0.5385762], dtype=float32), -1.3068647]. 
=============================================
[2019-04-09 15:03:50,294] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 47251: loss 18.6605
[2019-04-09 15:03:50,295] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 47251: learning rate 0.0000
[2019-04-09 15:03:50,328] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00486118 0.02578163 0.05547935 0.00320996 0.40432388 0.17473964
 0.17533766 0.03438186 0.07186366 0.0342465  0.01577459], sum to 1.0000
[2019-04-09 15:03:50,329] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6111
[2019-04-09 15:03:50,347] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [8.066666666666666, 28.33333333333334, 0.0, 0.0, 19.0, 24.39869910365507, 0.1053553572657772, 0.0, 1.0, 45.0, 28.74279451550375], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3642000.0000, 
sim time next is 3643200.0000, 
raw observation next is [8.0, 29.0, 0.0, 0.0, 19.0, 24.34606297916823, 0.08751793152233449, 0.0, 1.0, 40.0, 22.42152046644282], 
processed observation next is [0.0, 0.17391304347826086, 0.6842105263157896, 0.29, 0.0, 0.0, 0.08333333333333333, 0.5288385815973525, 0.5291726438407781, 0.0, 1.0, 0.5, 0.2242152046644282], 
reward next is 0.7758, 
noisyNet noise sample is [array([-1.6174363], dtype=float32), -0.4069632]. 
=============================================
[2019-04-09 15:03:50,351] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00198118 0.00890269 0.03632601 0.00061594 0.38247895 0.15831073
 0.27891693 0.01515792 0.0796748  0.02542856 0.01220636], sum to 1.0000
[2019-04-09 15:03:50,354] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5310
[2019-04-09 15:03:50,398] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 71.0, 73.83333333333334, 350.3333333333333, 22.5, 23.84618597326274, 0.1443045258776657, 1.0, 1.0, 40.0, 35.489636176337086], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3486000.0000, 
sim time next is 3487200.0000, 
raw observation next is [-1.0, 71.0, 89.83333333333334, 444.1666666666666, 22.5, 24.26073209528754, 0.2269021527861983, 1.0, 1.0, 40.0, 33.10471589646724], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.29944444444444446, 0.49079189686924485, 0.375, 0.5217276746072951, 0.5756340509287328, 1.0, 1.0, 0.5, 0.3310471589646724], 
reward next is 0.6690, 
noisyNet noise sample is [array([-0.03839016], dtype=float32), -0.6289088]. 
=============================================
[2019-04-09 15:03:50,474] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.40386755e-04 5.33136772e-03 1.50722712e-02 8.63652604e-05
 6.73842967e-01 1.14723414e-01 1.22285299e-01 4.76491963e-03
 4.96448912e-02 8.54732096e-03 5.46079362e-03], sum to 1.0000
[2019-04-09 15:03:50,474] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8281
[2019-04-09 15:03:50,540] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 20.16666666666666, 186.3333333333333, 22.5, 27.39917267620372, 0.6664292001223163, 1.0, 1.0, 35.0, 23.42713209724303], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3432000.0000, 
sim time next is 3433200.0000, 
raw observation next is [2.0, 67.0, 10.0, 100.8333333333333, 22.5, 24.82361209547678, 0.59920140775553, 1.0, 1.0, 45.0, 33.36257512378841], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.03333333333333333, 0.11141804788213625, 0.375, 0.5686343412897316, 0.6997338025851767, 1.0, 1.0, 0.6, 0.3336257512378841], 
reward next is 0.6664, 
noisyNet noise sample is [array([-0.6690702], dtype=float32), -0.31422037]. 
=============================================
[2019-04-09 15:03:50,872] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47483: loss 18.2108
[2019-04-09 15:03:50,873] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47483: learning rate 0.0000
[2019-04-09 15:03:50,955] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 47515: loss 20.4618
[2019-04-09 15:03:50,956] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 47515: learning rate 0.0000
[2019-04-09 15:03:50,964] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47520: loss 39.9572
[2019-04-09 15:03:50,966] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47521: learning rate 0.0000
[2019-04-09 15:03:51,105] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 47579: loss 16.2391
[2019-04-09 15:03:51,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 47579: learning rate 0.0000
[2019-04-09 15:03:51,150] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47598: loss 18.8902
[2019-04-09 15:03:51,153] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47599: learning rate 0.0000
[2019-04-09 15:03:51,507] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00301384 0.02129765 0.03415476 0.00186012 0.47572222 0.13497122
 0.19289441 0.01588914 0.07870097 0.02923269 0.012263  ], sum to 1.0000
[2019-04-09 15:03:51,513] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0716
[2019-04-09 15:03:51,538] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 24.18140341409122, 0.1472464041504759, 0.0, 1.0, 35.0, 22.397600922708634], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3610800.0000, 
sim time next is 3612000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 24.11978052350611, 0.1181386678421041, 0.0, 1.0, 35.0, 19.870233623652503], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.5099817102921756, 0.5393795559473681, 0.0, 1.0, 0.4, 0.19870233623652503], 
reward next is 0.8013, 
noisyNet noise sample is [array([0.6027667], dtype=float32), 0.6567239]. 
=============================================
[2019-04-09 15:03:51,560] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[9.136261]
 [9.303723]
 [9.249156]
 [9.223654]
 [9.339167]], R is [[ 9.85175705]
 [10.5292635 ]
 [11.13131237]
 [11.81793308]
 [12.47466469]].
[2019-04-09 15:03:51,564] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00348011 0.01962131 0.06231939 0.002224   0.42480487 0.19611591
 0.15712681 0.02498068 0.0420151  0.05121879 0.01609306], sum to 1.0000
[2019-04-09 15:03:51,565] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4406
[2019-04-09 15:03:51,591] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00125493 0.0229661  0.02671017 0.00058983 0.55111575 0.09902766
 0.15511276 0.01207575 0.0867539  0.03561788 0.00877523], sum to 1.0000
[2019-04-09 15:03:51,591] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8268
[2019-04-09 15:03:51,599] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.333333333333333, 66.66666666666666, 101.8333333333333, 689.6666666666666, 19.0, 23.27598099189885, 0.0089612255031805, 0.0, 1.0, 25.0, 31.967396796737457], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3577200.0000, 
sim time next is 3578400.0000, 
raw observation next is [-5.0, 65.0, 105.5, 717.0, 19.0, 23.32122340684761, 0.00610817902355026, 0.0, 1.0, 35.0, 27.974063672666027], 
processed observation next is [0.0, 0.43478260869565216, 0.32409972299168976, 0.65, 0.3516666666666667, 0.7922651933701658, 0.08333333333333333, 0.44343528390396764, 0.5020360596745167, 0.0, 1.0, 0.4, 0.2797406367266603], 
reward next is 0.7203, 
noisyNet noise sample is [array([-0.26198146], dtype=float32), -0.13500732]. 
=============================================
[2019-04-09 15:03:51,615] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 19.0, 24.58269732389245, 0.2817040100954863, 0.0, 1.0, 45.0, 32.312244739182795], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3462000.0000, 
sim time next is 3463200.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 19.0, 24.60143873901507, 0.2777087104222173, 0.0, 1.0, 45.0, 32.99945550089063], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.08333333333333333, 0.5501198949179225, 0.5925695701407391, 0.0, 1.0, 0.6, 0.32999455500890634], 
reward next is 0.6700, 
noisyNet noise sample is [array([1.5266482], dtype=float32), -0.4769156]. 
=============================================
[2019-04-09 15:03:51,716] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47831: loss 27.4768
[2019-04-09 15:03:51,729] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0687233e-03 8.4122792e-03 4.5930121e-02 2.0056141e-04 5.9062374e-01
 1.4133587e-01 7.8399234e-02 1.6735850e-02 7.2363749e-02 3.7316624e-02
 7.6133097e-03], sum to 1.0000
[2019-04-09 15:03:51,729] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47831: learning rate 0.0000
[2019-04-09 15:03:51,729] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6759
[2019-04-09 15:03:51,757] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00359399 0.01569897 0.05434102 0.00309745 0.4787675  0.13602735
 0.17590523 0.02116143 0.05787329 0.03155075 0.02198307], sum to 1.0000
[2019-04-09 15:03:51,757] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3506
[2019-04-09 15:03:51,776] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.0, 66.0, 0.0, 0.0, 19.0, 25.70603567572974, 0.5750745343781781, 0.0, 1.0, 40.0, 30.98073795263837], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3538800.0000, 
sim time next is 3540000.0000, 
raw observation next is [-1.333333333333333, 64.0, 0.0, 0.0, 19.0, 25.65185104905811, 0.5363852073164099, 0.0, 1.0, 40.0, 27.63912106261931], 
processed observation next is [1.0, 1.0, 0.42566943674976926, 0.64, 0.0, 0.0, 0.08333333333333333, 0.6376542540881758, 0.6787950691054699, 0.0, 1.0, 0.5, 0.2763912106261931], 
reward next is 0.7236, 
noisyNet noise sample is [array([-0.05968487], dtype=float32), -1.0407029]. 
=============================================
[2019-04-09 15:03:51,781] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[13.273014]
 [13.353724]
 [13.550696]
 [13.253142]
 [13.202225]], R is [[13.95864582]
 [14.50925159]
 [14.93197918]
 [15.26746845]
 [15.94177055]].
[2019-04-09 15:03:51,793] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 19.0, 23.45714027351461, 0.02328245234399521, 0.0, 1.0, 35.0, 41.619818426531516], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3567600.0000, 
sim time next is 3568800.0000, 
raw observation next is [-6.333333333333333, 70.0, 0.0, 0.0, 19.0, 23.537838802375, 0.005426961446128258, 0.0, 1.0, 35.0, 35.33845410544008], 
processed observation next is [0.0, 0.30434782608695654, 0.28716528162511545, 0.7, 0.0, 0.0, 0.08333333333333333, 0.4614865668645833, 0.5018089871487094, 0.0, 1.0, 0.4, 0.35338454105440076], 
reward next is 0.6466, 
noisyNet noise sample is [array([0.8019147], dtype=float32), 0.95295024]. 
=============================================
[2019-04-09 15:03:52,027] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.9026433e-04 6.4201709e-03 2.5318740e-02 2.7989279e-04 3.4874967e-01
 4.0206054e-01 1.2691711e-01 1.2812439e-02 3.4588553e-02 3.3762086e-02
 8.5005434e-03], sum to 1.0000
[2019-04-09 15:03:52,028] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2624
[2019-04-09 15:03:52,087] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 61.0, 112.0, 790.0, 22.5, 26.15323707145445, 0.489441457452743, 1.0, 1.0, 35.0, 29.348948109884958], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3495600.0000, 
sim time next is 3496800.0000, 
raw observation next is [1.333333333333333, 59.66666666666667, 114.0, 803.3333333333333, 22.5, 25.24358615420125, 0.5166155978778505, 1.0, 1.0, 35.0, 26.308461729238736], 
processed observation next is [1.0, 0.4782608695652174, 0.4995383194829178, 0.5966666666666667, 0.38, 0.8876611418047882, 0.375, 0.6036321795167708, 0.6722051992926169, 1.0, 1.0, 0.4, 0.26308461729238736], 
reward next is 0.7369, 
noisyNet noise sample is [array([-0.19793458], dtype=float32), 0.6238166]. 
=============================================
[2019-04-09 15:03:52,095] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0039053  0.02230494 0.05363894 0.00183556 0.42875534 0.11127742
 0.23410578 0.01559495 0.05813873 0.05318886 0.01725424], sum to 1.0000
[2019-04-09 15:03:52,097] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2608
[2019-04-09 15:03:52,126] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 19.0, 23.7467464944628, 0.009250543798156726, 0.0, 1.0, 40.0, 24.20275802434288], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3633600.0000, 
sim time next is 3634800.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 19.0, 23.77944014667681, 0.002968676204208431, 0.0, 1.0, 35.0, 18.350029703289124], 
processed observation next is [0.0, 0.043478260869565216, 0.7119113573407203, 0.25, 0.0, 0.0, 0.08333333333333333, 0.4816200122230674, 0.5009895587347362, 0.0, 1.0, 0.4, 0.18350029703289125], 
reward next is 0.8165, 
noisyNet noise sample is [array([0.03242747], dtype=float32), 0.17648801]. 
=============================================
[2019-04-09 15:03:52,264] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48048: loss 19.5662
[2019-04-09 15:03:52,266] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48048: learning rate 0.0000
[2019-04-09 15:03:52,276] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2949629e-04 7.4647772e-03 4.8652880e-02 1.2670722e-04 5.4855371e-01
 1.1901801e-01 1.9975029e-01 1.1619894e-02 4.5014795e-02 1.2236441e-02
 7.0330841e-03], sum to 1.0000
[2019-04-09 15:03:52,290] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7284
[2019-04-09 15:03:52,318] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 55.33333333333334, 115.8333333333333, 820.8333333333334, 22.5, 26.02480801757569, 0.590578878217355, 1.0, 1.0, 35.0, 21.0891547663917], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3500400.0000, 
sim time next is 3501600.0000, 
raw observation next is [2.0, 53.66666666666667, 115.8333333333333, 820.1666666666667, 22.5, 26.04274508063207, 0.4865415034310108, 1.0, 1.0, 45.0, 29.440850784875998], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.5366666666666667, 0.386111111111111, 0.9062615101289135, 0.375, 0.6702287567193391, 0.6621805011436702, 1.0, 1.0, 0.6, 0.29440850784876], 
reward next is 0.7056, 
noisyNet noise sample is [array([0.10795206], dtype=float32), -1.0231657]. 
=============================================
[2019-04-09 15:03:52,658] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00552107 0.02468485 0.05765902 0.00291881 0.45986193 0.11758266
 0.19791827 0.02568923 0.05561685 0.03752563 0.01502168], sum to 1.0000
[2019-04-09 15:03:52,658] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4189
[2019-04-09 15:03:52,699] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.666666666666668, 28.66666666666667, 0.0, 0.0, 19.0, 24.75474925641402, 0.1803087217826178, 0.0, 1.0, 45.0, 26.683464659674335], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3655200.0000, 
sim time next is 3656400.0000, 
raw observation next is [8.333333333333334, 30.33333333333334, 18.16666666666666, 168.0, 19.0, 24.67288293143656, 0.1832520374953263, 0.0, 1.0, 35.0, 20.738819033849623], 
processed observation next is [0.0, 0.30434782608695654, 0.6934441366574331, 0.3033333333333334, 0.060555555555555536, 0.1856353591160221, 0.08333333333333333, 0.5560735776197134, 0.5610840124984421, 0.0, 1.0, 0.4, 0.20738819033849623], 
reward next is 0.7926, 
noisyNet noise sample is [array([1.3394146], dtype=float32), -1.6143794]. 
=============================================
[2019-04-09 15:03:52,702] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00586504 0.02483993 0.03340833 0.00344293 0.41530257 0.17062294
 0.17670467 0.02595495 0.06856227 0.04358849 0.03170792], sum to 1.0000
[2019-04-09 15:03:52,705] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9378
[2019-04-09 15:03:52,759] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [9.333333333333334, 26.33333333333334, 0.0, 0.0, 19.0, 23.86352387044996, -0.01028966284376236, 0.0, 1.0, 35.0, 22.139648936801912], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3648000.0000, 
sim time next is 3649200.0000, 
raw observation next is [9.666666666666666, 25.66666666666666, 0.0, 0.0, 19.0, 23.88303642634583, 0.0369277189596174, 0.0, 1.0, 60.0, 57.70162935981352], 
processed observation next is [0.0, 0.21739130434782608, 0.7303785780240075, 0.2566666666666666, 0.0, 0.0, 0.08333333333333333, 0.49025303552881905, 0.5123092396532057, 0.0, 1.0, 0.9, 0.5770162935981352], 
reward next is 0.4230, 
noisyNet noise sample is [array([-1.4360787], dtype=float32), -0.5319805]. 
=============================================
[2019-04-09 15:03:52,851] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00399019 0.02671379 0.051735   0.00250186 0.50481576 0.08339023
 0.17018808 0.02068329 0.06845707 0.04704993 0.02047474], sum to 1.0000
[2019-04-09 15:03:52,853] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4898
[2019-04-09 15:03:52,882] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 19.0, 23.4501782184348, -0.002082538546772752, 0.0, 1.0, 35.0, 21.213529510019747], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3565200.0000, 
sim time next is 3566400.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 19.0, 23.19379575290224, -0.05417167875989684, 0.0, 1.0, 35.0, 19.236194293867477], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.08333333333333333, 0.43281631274185334, 0.481942773746701, 0.0, 1.0, 0.4, 0.19236194293867478], 
reward next is 0.8076, 
noisyNet noise sample is [array([-2.2814956], dtype=float32), 0.6817279]. 
=============================================
[2019-04-09 15:03:53,029] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48359: loss 11.5701
[2019-04-09 15:03:53,031] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48360: learning rate 0.0000
[2019-04-09 15:03:53,161] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00109667 0.01765377 0.03499764 0.00065416 0.41468123 0.20167243
 0.2222367  0.01525792 0.04147856 0.03811885 0.012152  ], sum to 1.0000
[2019-04-09 15:03:53,162] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8010
[2019-04-09 15:03:53,219] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.333333333333334, 31.0, 115.1666666666667, 807.1666666666666, 19.0, 26.12897339125898, 0.5769006336634469, 0.0, 1.0, 35.0, 15.721931845682729], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3669600.0000, 
sim time next is 3670800.0000, 
raw observation next is [6.666666666666666, 38.0, 116.1666666666667, 818.1666666666666, 19.0, 26.09750847603908, 0.5788262477268978, 0.0, 1.0, 45.0, 21.51547743649408], 
processed observation next is [0.0, 0.4782608695652174, 0.6472760849492153, 0.38, 0.38722222222222236, 0.9040515653775322, 0.08333333333333333, 0.6747923730032568, 0.6929420825756326, 0.0, 1.0, 0.6, 0.2151547743649408], 
reward next is 0.7848, 
noisyNet noise sample is [array([-2.4490833], dtype=float32), 0.07205092]. 
=============================================
[2019-04-09 15:03:53,380] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00203452 0.01314839 0.03395685 0.00132166 0.49897406 0.15172824
 0.18353757 0.01517844 0.05033982 0.04161193 0.00816851], sum to 1.0000
[2019-04-09 15:03:53,381] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00402556 0.02496609 0.04005906 0.00180916 0.5286532  0.07125843
 0.1987303  0.01516447 0.05261015 0.05306984 0.00965371], sum to 1.0000
[2019-04-09 15:03:53,382] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0477
[2019-04-09 15:03:53,382] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9170
[2019-04-09 15:03:53,417] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [11.33333333333333, 26.66666666666667, 109.3333333333333, 746.3333333333334, 19.0, 25.55454601320992, 0.4431994377994162, 0.0, 1.0, 35.0, 18.92221869670737], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 3666000.0000, 
sim time next is 3667200.0000, 
raw observation next is [11.66666666666667, 25.33333333333334, 111.8333333333333, 771.8333333333334, 19.0, 25.57943503984587, 0.4579475520573905, 0.0, 1.0, 25.0, 16.67527595654385], 
processed observation next is [0.0, 0.43478260869565216, 0.785780240073869, 0.2533333333333334, 0.37277777777777765, 0.8528545119705341, 0.08333333333333333, 0.6316195866538225, 0.6526491840191302, 0.0, 1.0, 0.2, 0.1667527595654385], 
reward next is 0.8332, 
noisyNet noise sample is [array([-1.9375912], dtype=float32), -1.5601726]. 
=============================================
[2019-04-09 15:03:53,419] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.333333333333333, 70.0, 0.0, 0.0, 19.0, 23.4928395864654, -0.01642546528344455, 0.0, 1.0, 45.0, 33.3643849040958], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3568800.0000, 
sim time next is 3570000.0000, 
raw observation next is [-6.666666666666666, 70.0, 17.16666666666666, 171.6666666666667, 19.0, 23.36383853663016, 0.01388701059497605, 0.0, 1.0, 55.0, 49.96998566410574], 
processed observation next is [0.0, 0.30434782608695654, 0.2779316712834719, 0.7, 0.0572222222222222, 0.18968692449355437, 0.08333333333333333, 0.44698654471917987, 0.5046290035316586, 0.0, 1.0, 0.8, 0.4996998566410574], 
reward next is 0.5003, 
noisyNet noise sample is [array([0.3117691], dtype=float32), -0.36477208]. 
=============================================
[2019-04-09 15:03:53,439] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[9.087713]
 [8.875991]
 [8.751272]
 [8.978727]
 [8.703652]], R is [[ 9.22729206]
 [ 9.80137539]
 [10.37763309]
 [10.88919735]
 [11.15263557]].
[2019-04-09 15:03:53,734] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48649: loss 26.9161
[2019-04-09 15:03:53,739] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48649: learning rate 0.0000
[2019-04-09 15:03:53,835] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00248665 0.01736119 0.03114761 0.00080411 0.57853365 0.10727334
 0.1408725  0.00972923 0.0718876  0.02742227 0.01248188], sum to 1.0000
[2019-04-09 15:03:53,835] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1863
[2019-04-09 15:03:53,863] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00176326 0.01476766 0.03152447 0.00115572 0.5306946  0.10029452
 0.21756314 0.01445755 0.05798371 0.02049531 0.00929996], sum to 1.0000
[2019-04-09 15:03:53,864] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7642
[2019-04-09 15:03:53,887] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.0, 24.0, 113.5, 789.5, 19.0, 25.18280836203184, 0.4370109842542956, 0.0, 1.0, 60.0, 46.26225327977501], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3668400.0000, 
sim time next is 3669600.0000, 
raw observation next is [9.333333333333334, 31.0, 115.1666666666667, 807.1666666666666, 19.0, 25.66180518614877, 0.5042037552656143, 0.0, 1.0, 35.0, 28.44222437198028], 
processed observation next is [0.0, 0.4782608695652174, 0.7211449676823639, 0.31, 0.383888888888889, 0.8918968692449355, 0.08333333333333333, 0.6384837655123974, 0.6680679184218715, 0.0, 1.0, 0.4, 0.2844222437198028], 
reward next is 0.7156, 
noisyNet noise sample is [array([1.1017447], dtype=float32), -0.23370762]. 
=============================================
[2019-04-09 15:03:53,889] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 67.0, 0.0, 0.0, 19.0, 25.16367855582714, 0.3626304287521946, 0.0, 1.0, 40.0, 23.95704257495212], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3712800.0000, 
sim time next is 3714000.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 19.0, 25.03892349908936, 0.3466396452650087, 0.0, 1.0, 45.0, 31.546011794174007], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5865769582574467, 0.6155465484216696, 0.0, 1.0, 0.6, 0.31546011794174006], 
reward next is 0.6845, 
noisyNet noise sample is [array([-0.1729998], dtype=float32), 0.41657734]. 
=============================================
[2019-04-09 15:03:53,890] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00238556 0.01750509 0.03183788 0.00080043 0.5729648  0.11177714
 0.14662103 0.00928151 0.06787937 0.02518182 0.01376536], sum to 1.0000
[2019-04-09 15:03:53,890] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8379
[2019-04-09 15:03:53,902] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[11.028648]
 [11.193743]
 [11.310324]
 [11.094145]
 [11.181015]], R is [[11.85120106]
 [12.49311829]
 [13.05320263]
 [13.73305511]
 [14.39040279]].
[2019-04-09 15:03:53,923] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.333333333333334, 31.0, 115.1666666666667, 807.1666666666666, 19.0, 25.66180518614877, 0.5042037552656143, 0.0, 1.0, 35.0, 28.44222437198028], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3669600.0000, 
sim time next is 3670800.0000, 
raw observation next is [6.666666666666666, 38.0, 116.1666666666667, 818.1666666666666, 19.0, 25.87897078821817, 0.5161575783991378, 0.0, 1.0, 45.0, 23.461025394177817], 
processed observation next is [0.0, 0.4782608695652174, 0.6472760849492153, 0.38, 0.38722222222222236, 0.9040515653775322, 0.08333333333333333, 0.6565808990181807, 0.6720525261330459, 0.0, 1.0, 0.6, 0.23461025394177817], 
reward next is 0.7654, 
noisyNet noise sample is [array([1.1017447], dtype=float32), -0.23370762]. 
=============================================
[2019-04-09 15:03:53,979] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 48750: loss 23.6477
[2019-04-09 15:03:53,981] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 48750: learning rate 0.0000
[2019-04-09 15:03:54,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00190572 0.0084807  0.02389322 0.00073378 0.5661276  0.06450728
 0.22136758 0.01003306 0.07340439 0.02132238 0.00822434], sum to 1.0000
[2019-04-09 15:03:54,287] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1429
[2019-04-09 15:03:54,322] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.333333333333333, 62.33333333333333, 0.0, 0.0, 19.0, 26.30137305511833, 0.607325480517007, 0.0, 1.0, 45.0, 28.140160065437968], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3703200.0000, 
sim time next is 3704400.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 19.0, 26.18744930770045, 0.5856002250086104, 0.0, 1.0, 35.0, 22.225485163951046], 
processed observation next is [0.0, 0.9130434782608695, 0.518005540166205, 0.62, 0.0, 0.0, 0.08333333333333333, 0.6822874423083709, 0.6952000750028701, 0.0, 1.0, 0.4, 0.22225485163951045], 
reward next is 0.7777, 
noisyNet noise sample is [array([-0.49993345], dtype=float32), 0.96144235]. 
=============================================
[2019-04-09 15:03:54,328] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.65999736e-04 7.25162169e-03 1.62859168e-02 5.99242339e-04
 5.10988116e-01 1.22099675e-01 2.58313984e-01 1.54741239e-02
 3.80023569e-02 2.21161228e-02 8.40281975e-03], sum to 1.0000
[2019-04-09 15:03:54,343] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9042
[2019-04-09 15:03:54,382] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.00000000000001, 0.0, 0.0, 19.0, 24.51980001361022, 0.2903283180426377, 0.0, 1.0, 35.0, 21.28661829901985], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3543600.0000, 
sim time next is 3544800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 24.41567942024961, 0.2802265037352897, 0.0, 1.0, 45.0, 34.57021518056613], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.5346399516874675, 0.59340883457843, 0.0, 1.0, 0.6, 0.34570215180566133], 
reward next is 0.6543, 
noisyNet noise sample is [array([-1.1802957], dtype=float32), -0.116951175]. 
=============================================
[2019-04-09 15:03:54,470] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3000, global step 48958: loss 19.7293
[2019-04-09 15:03:54,481] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3000, global step 48963: learning rate 0.0000
[2019-04-09 15:03:54,792] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00110241 0.00711106 0.01330215 0.00073888 0.5921368  0.0665254
 0.19991916 0.01022683 0.05917927 0.03795994 0.01179812], sum to 1.0000
[2019-04-09 15:03:54,794] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6095
[2019-04-09 15:03:54,824] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 25.26590661297324, 0.3393797063111948, 0.0, 1.0, 35.0, 19.91157542248282], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3609600.0000, 
sim time next is 3610800.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 25.10616893275369, 0.3073810750223411, 0.0, 1.0, 35.0, 18.272705720681454], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.5921807443961408, 0.6024603583407804, 0.0, 1.0, 0.4, 0.18272705720681454], 
reward next is 0.8173, 
noisyNet noise sample is [array([0.15515463], dtype=float32), -1.0254924]. 
=============================================
[2019-04-09 15:03:54,923] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00343924 0.02375915 0.05118275 0.00327513 0.36973017 0.16110899
 0.20747052 0.0242903  0.076176   0.06373621 0.01583162], sum to 1.0000
[2019-04-09 15:03:54,924] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9725
[2019-04-09 15:03:54,963] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [8.2, 27.0, 0.0, 0.0, 19.0, 24.33321129111831, 0.09722264291711764, 0.0, 1.0, 40.0, 19.698969526460147], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 3639600.0000, 
sim time next is 3640800.0000, 
raw observation next is [8.133333333333333, 27.66666666666667, 0.0, 0.0, 19.0, 24.41394436814375, 0.1719356123676374, 0.0, 1.0, 60.0, 51.64590838034117], 
processed observation next is [0.0, 0.13043478260869565, 0.687903970452447, 0.2766666666666667, 0.0, 0.0, 0.08333333333333333, 0.5344953640119791, 0.5573118707892125, 0.0, 1.0, 0.9, 0.5164590838034117], 
reward next is 0.4835, 
noisyNet noise sample is [array([-0.04259781], dtype=float32), -0.74710786]. 
=============================================
[2019-04-09 15:03:55,014] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00160454 0.0200653  0.04000488 0.00071511 0.60331625 0.11472889
 0.11407752 0.01284455 0.05018544 0.03327567 0.00918185], sum to 1.0000
[2019-04-09 15:03:55,017] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8368
[2019-04-09 15:03:55,037] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 23.90176010327353, 0.04019712853894311, 0.0, 1.0, 35.0, 23.025420304390522], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3724800.0000, 
sim time next is 3726000.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 23.7615235133206, 0.03090526630331691, 0.0, 1.0, 45.0, 34.372483538518146], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.4801269594433834, 0.510301755434439, 0.0, 1.0, 0.6, 0.34372483538518145], 
reward next is 0.6563, 
noisyNet noise sample is [array([-0.2096634], dtype=float32), -0.23047742]. 
=============================================
[2019-04-09 15:03:55,040] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[10.7389555]
 [10.84488  ]
 [10.834168 ]
 [10.5299425]
 [10.83212  ]], R is [[11.7507    ]
 [12.40293884]
 [13.01979828]
 [13.54902458]
 [14.18799877]].
[2019-04-09 15:03:55,107] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00177002 0.00913957 0.02609555 0.00077867 0.63653755 0.08411754
 0.12330858 0.01416971 0.05495123 0.03983107 0.00930054], sum to 1.0000
[2019-04-09 15:03:55,108] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3096
[2019-04-09 15:03:55,135] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 24.72470062009795, 0.256402667519843, 0.0, 1.0, 35.0, 21.614922360536294], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3722400.0000, 
sim time next is 3723600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 24.65136828065265, 0.2223320896332817, 0.0, 1.0, 35.0, 19.834440292069495], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.5542806900543876, 0.5741106965444273, 0.0, 1.0, 0.4, 0.19834440292069494], 
reward next is 0.8017, 
noisyNet noise sample is [array([1.1020616], dtype=float32), -1.2351215]. 
=============================================
[2019-04-09 15:03:55,574] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 49412: loss 24.0194
[2019-04-09 15:03:55,576] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 49412: learning rate 0.0000
[2019-04-09 15:03:56,017] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1293340e-03 1.2159736e-02 3.1134170e-02 5.6730025e-04 6.3062376e-01
 7.3960148e-02 1.6294521e-01 1.5936704e-02 3.2181568e-02 2.7644712e-02
 1.1717382e-02], sum to 1.0000
[2019-04-09 15:03:56,020] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9527
[2019-04-09 15:03:56,041] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 24.02826399286555, 0.1131537755770921, 0.0, 1.0, 45.0, 29.38355573086158], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3613200.0000, 
sim time next is 3614400.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 24.04341211579546, 0.1576294831016194, 0.0, 1.0, 55.0, 51.20326490251244], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.5036176763162882, 0.5525431610338731, 0.0, 1.0, 0.8, 0.5120326490251244], 
reward next is 0.4880, 
noisyNet noise sample is [array([3.203467], dtype=float32), 0.11126923]. 
=============================================
[2019-04-09 15:03:56,230] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 49683: loss 20.4235
[2019-04-09 15:03:56,231] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 49683: learning rate 0.0000
[2019-04-09 15:03:57,020] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-09 15:03:57,022] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:03:57,022] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:03:57,025] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run6
[2019-04-09 15:03:57,037] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:03:57,040] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:03:57,040] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:03:57,041] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:03:57,043] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run6
[2019-04-09 15:03:57,044] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run6
[2019-04-09 15:05:22,593] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.02860505], dtype=float32), 0.044967998]
[2019-04-09 15:05:22,593] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [0.6666666666666667, 95.33333333333334, 58.33333333333333, 25.99999999999999, 22.5, 22.72695794307765, -0.2542762153047201, 1.0, 1.0, 35.0, 32.10418880246067]
[2019-04-09 15:05:22,593] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 15:05:22,594] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [1.0296651e-03 8.4523046e-03 2.9149637e-02 5.6205323e-04 5.6657034e-01
 1.1234325e-01 1.9656050e-01 1.1605047e-02 3.9722305e-02 2.4088481e-02
 9.9164443e-03], sampled 0.21566925938292236
[2019-04-09 15:05:40,762] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 3037.1459 117242.9707 739.4120
[2019-04-09 15:05:40,784] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:40,784] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:40,784] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:40,784] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:40,784] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:40,784] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:40,918] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:40,918] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:40,918] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:40,918] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:40,918] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:40,918] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,277] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2973.1638 125847.3763 458.9208
[2019-04-09 15:05:57,311] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,311] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,311] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,311] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,311] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,311] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:05:57,427] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,427] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,427] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,427] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,427] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:57,427] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:05:58,135] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.02860505], dtype=float32), 0.044967998]
[2019-04-09 15:05:58,135] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [0.4612098636666667, 89.27959497, 57.27125739999998, 0.0, 22.5, 25.64898872468763, 0.441679441669969, 1.0, 1.0, 25.0, 20.020366934944157]
[2019-04-09 15:05:58,135] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:05:58,136] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [6.6271820e-04 6.0515655e-03 2.5129121e-02 3.1610217e-04 6.3034081e-01
 9.7491302e-02 1.6894305e-01 8.0589680e-03 3.4978285e-02 2.1154344e-02
 6.8736905e-03], sampled 0.8799956552494855
[2019-04-09 15:06:03,902] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2944.7212 127544.8458 281.6779
[2019-04-09 15:06:03,922] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:03,922] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:03,922] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:03,922] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:03,922] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:03,922] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:06:04,029] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,029] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,029] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,029] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,029] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,029] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:06:04,924] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 50000, evaluation results [50000.0, 2973.163794314355, 125847.37631841948, 458.9207622370676, 3037.1459060756674, 117242.97071574425, 739.412031168985, 2944.7212248109777, 127544.84578411732, 281.67794402094086]
[2019-04-09 15:06:05,180] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3025490e-04 6.2600677e-03 1.6798789e-02 5.0555846e-05 7.0880681e-01
 7.4303903e-02 1.4827214e-01 5.3352285e-03 2.8131135e-02 9.5551265e-03
 2.1559880e-03], sum to 1.0000
[2019-04-09 15:06:05,181] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8602
[2019-04-09 15:06:05,215] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 25.25208787720642, 0.4679485422304481, 0.0, 1.0, 35.0, 25.230176071264385], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3790800.0000, 
sim time next is 3792000.0000, 
raw observation next is [-3.0, 73.0, 0.0, 0.0, 19.0, 25.15114928745208, 0.45662335531793, 0.0, 1.0, 45.0, 32.90234290914907], 
processed observation next is [1.0, 0.9130434782608695, 0.3795013850415513, 0.73, 0.0, 0.0, 0.08333333333333333, 0.5959291072876732, 0.6522077851059767, 0.0, 1.0, 0.6, 0.3290234290914907], 
reward next is 0.6710, 
noisyNet noise sample is [array([1.1430521], dtype=float32), -0.88626015]. 
=============================================
[2019-04-09 15:06:05,237] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[14.906536 ]
 [15.267254 ]
 [15.291424 ]
 [15.4478445]
 [15.741556 ]], R is [[15.16511345]
 [15.7611599 ]
 [16.27041435]
 [16.89446449]
 [17.49278259]].
[2019-04-09 15:06:05,308] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.62711153e-04 3.83680221e-03 4.61592898e-02 4.93704225e-04
 6.70954823e-01 6.04388826e-02 1.12930804e-01 6.39084540e-03
 7.88848996e-02 1.41131254e-02 5.23411669e-03], sum to 1.0000
[2019-04-09 15:06:05,308] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2020
[2019-04-09 15:06:05,311] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5329078e-04 2.6614787e-03 2.6847627e-02 1.3096696e-04 5.8444726e-01
 7.7138022e-02 2.2462371e-01 6.0516396e-03 6.5058745e-02 7.9873567e-03
 4.8999260e-03], sum to 1.0000
[2019-04-09 15:06:05,311] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8678
[2019-04-09 15:06:05,327] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 24.76743295759119, 0.3310407787097192, 0.0, 1.0, 35.0, 17.56211561186657], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3798000.0000, 
sim time next is 3799200.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 19.0, 24.51235685804035, 0.2851352025888672, 0.0, 1.0, 35.0, 20.26404599775234], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5426964048366957, 0.5950450675296224, 0.0, 1.0, 0.4, 0.20264045997752342], 
reward next is 0.7974, 
noisyNet noise sample is [array([1.3171339], dtype=float32), 0.5270576]. 
=============================================
[2019-04-09 15:06:05,344] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 22.5, 25.56594662873035, 0.5399063604662342, 1.0, 1.0, 35.0, 19.97096254211811], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3787200.0000, 
sim time next is 3788400.0000, 
raw observation next is [-2.333333333333333, 67.0, 0.0, 0.0, 19.0, 25.38213279670569, 0.54255132492816, 0.0, 1.0, 45.0, 36.089316857812946], 
processed observation next is [1.0, 0.8695652173913043, 0.3979686057248385, 0.67, 0.0, 0.0, 0.08333333333333333, 0.6151777330588075, 0.68085044164272, 0.0, 1.0, 0.6, 0.3608931685781295], 
reward next is 0.6391, 
noisyNet noise sample is [array([1.5490497], dtype=float32), 2.1350214]. 
=============================================
[2019-04-09 15:06:05,965] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.5160930e-04 4.6763108e-03 1.3543729e-02 8.1704216e-05 6.5270692e-01
 1.6692071e-01 1.2498393e-01 7.7099404e-03 1.5867699e-02 1.0197204e-02
 3.1603505e-03], sum to 1.0000
[2019-04-09 15:06:05,966] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8821
[2019-04-09 15:06:05,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8081805e-04 4.0873946e-03 1.2079744e-02 9.9888835e-05 5.7541490e-01
 1.2533544e-01 2.2332019e-01 8.7854536e-03 3.2514956e-02 1.3365686e-02
 4.8154918e-03], sum to 1.0000
[2019-04-09 15:06:05,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9716
[2019-04-09 15:06:05,992] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 107.1666666666667, 789.0, 22.5, 26.35078205995625, 0.6340174349280489, 1.0, 1.0, 40.0, 19.48024405592771], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3853200.0000, 
sim time next is 3854400.0000, 
raw observation next is [2.0, 48.0, 102.8333333333333, 771.1666666666667, 22.5, 26.51927895741276, 0.6838994159209272, 1.0, 1.0, 45.0, 28.538621423956876], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.3427777777777777, 0.8521178637200737, 0.375, 0.7099399131177299, 0.7279664719736424, 1.0, 1.0, 0.6, 0.2853862142395688], 
reward next is 0.7146, 
noisyNet noise sample is [array([-1.0429535], dtype=float32), -0.5241688]. 
=============================================
[2019-04-09 15:06:06,007] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 22.5, 25.4551943615066, 0.4979599444813975, 1.0, 1.0, 35.0, 20.55158348979158], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3787200.0000, 
sim time next is 3788400.0000, 
raw observation next is [-2.333333333333333, 67.0, 0.0, 0.0, 19.0, 25.27220285969233, 0.5095063494520391, 0.0, 1.0, 55.0, 56.60774417591098], 
processed observation next is [1.0, 0.8695652173913043, 0.3979686057248385, 0.67, 0.0, 0.0, 0.08333333333333333, 0.606016904974361, 0.6698354498173463, 0.0, 1.0, 0.8, 0.5660774417591098], 
reward next is 0.4339, 
noisyNet noise sample is [array([-1.5980275], dtype=float32), -0.24170873]. 
=============================================
[2019-04-09 15:06:06,071] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.90954281e-04 6.21403474e-03 2.56215446e-02 3.57182987e-04
 4.62575346e-01 1.31134555e-01 2.97158837e-01 1.32844215e-02
 3.81355807e-02 1.79644916e-02 6.66296482e-03], sum to 1.0000
[2019-04-09 15:06:06,072] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5744
[2019-04-09 15:06:06,099] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 23.69922031734252, 0.0903599537166027, 0.0, 1.0, 35.0, 19.838618378554003], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3889200.0000, 
sim time next is 3890400.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 23.57366549236696, 0.07021864048294381, 0.0, 1.0, 40.0, 25.614089893594105], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.46447212436391333, 0.5234062134943146, 0.0, 1.0, 0.5, 0.25614089893594105], 
reward next is 0.7439, 
noisyNet noise sample is [array([-0.04073153], dtype=float32), -0.15016802]. 
=============================================
[2019-04-09 15:06:06,417] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.82137456e-04 6.49568904e-03 2.33592596e-02 2.63791153e-04
 6.57436490e-01 1.34622872e-01 1.15307517e-01 9.82248224e-03
 2.76046377e-02 1.41480295e-02 1.05571179e-02], sum to 1.0000
[2019-04-09 15:06:06,421] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3117
[2019-04-09 15:06:06,455] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 75.0, 90.83333333333334, 470.0, 22.5, 24.68917799862306, 0.2060473398931185, 1.0, 1.0, 40.0, 27.838396000748062], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3746400.0000, 
sim time next is 3747600.0000, 
raw observation next is [-4.0, 77.0, 94.5, 552.0, 22.5, 24.76774418511424, 0.237173726971117, 1.0, 1.0, 35.0, 25.669447595897253], 
processed observation next is [1.0, 0.391304347826087, 0.3518005540166205, 0.77, 0.315, 0.6099447513812155, 0.375, 0.5639786820928533, 0.5790579089903723, 1.0, 1.0, 0.4, 0.25669447595897255], 
reward next is 0.7433, 
noisyNet noise sample is [array([-0.20007834], dtype=float32), 1.0771477]. 
=============================================
[2019-04-09 15:06:06,759] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5259864e-04 3.8984455e-03 1.7126804e-02 1.6416481e-04 5.6417888e-01
 9.0658978e-02 2.4853471e-01 9.0223299e-03 4.8320260e-02 1.0737532e-02
 6.9051734e-03], sum to 1.0000
[2019-04-09 15:06:06,759] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3182
[2019-04-09 15:06:06,781] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 60.0, 113.5, 798.5, 22.5, 24.79049237567309, 0.280762992480297, 1.0, 1.0, 35.0, 18.275066467377705], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3841200.0000, 
sim time next is 3842400.0000, 
raw observation next is [-1.0, 60.00000000000001, 115.8333333333333, 814.1666666666666, 22.5, 24.90061838610686, 0.1426813146029897, 1.0, 1.0, 35.0, 15.572106846379079], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6000000000000001, 0.386111111111111, 0.8996316758747698, 0.375, 0.5750515321755717, 0.5475604382009965, 1.0, 1.0, 0.4, 0.15572106846379077], 
reward next is 0.8443, 
noisyNet noise sample is [array([-0.94190025], dtype=float32), 0.45750597]. 
=============================================
[2019-04-09 15:06:06,830] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.82497053e-04 9.18915588e-03 3.80072035e-02 4.02822043e-04
 4.72297609e-01 1.40250102e-01 2.52748013e-01 1.44447405e-02
 4.53827903e-02 1.90516897e-02 7.44332979e-03], sum to 1.0000
[2019-04-09 15:06:06,830] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4157
[2019-04-09 15:06:06,847] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 23.97499097891729, 0.09489480980956756, 0.0, 1.0, 35.0, 19.921776353807807], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3715200.0000, 
sim time next is 3716400.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 19.0, 23.86095837238664, 0.06705787365624334, 0.0, 1.0, 35.0, 19.997923923611175], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.08333333333333333, 0.48841319769888675, 0.5223526245520811, 0.0, 1.0, 0.4, 0.19997923923611174], 
reward next is 0.8000, 
noisyNet noise sample is [array([-0.6257147], dtype=float32), 0.07899048]. 
=============================================
[2019-04-09 15:06:06,951] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7703373e-04 5.9298691e-03 4.4513728e-02 2.0582932e-04 5.0920105e-01
 9.7070716e-02 2.2791144e-01 7.6315133e-03 8.7378234e-02 1.3418961e-02
 6.2615895e-03], sum to 1.0000
[2019-04-09 15:06:06,952] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6401
[2019-04-09 15:06:06,970] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.666666666666667, 69.0, 0.0, 0.0, 19.0, 24.91584215891808, 0.393235279166589, 0.0, 1.0, 35.0, 18.753983025121016], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3789600.0000, 
sim time next is 3790800.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 19.0, 24.73759968749228, 0.3745204954804551, 0.0, 1.0, 40.0, 27.895687524229885], 
processed observation next is [1.0, 0.9130434782608695, 0.3795013850415513, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5614666406243568, 0.6248401651601517, 0.0, 1.0, 0.5, 0.27895687524229884], 
reward next is 0.7210, 
noisyNet noise sample is [array([-0.5261022], dtype=float32), 0.27769864]. 
=============================================
[2019-04-09 15:06:07,357] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2763505e-04 2.4624846e-03 2.6107270e-02 3.9947165e-05 6.7040133e-01
 1.0290141e-01 1.4581281e-01 6.4037405e-03 2.8101293e-02 1.2323472e-02
 5.3186473e-03], sum to 1.0000
[2019-04-09 15:06:07,363] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3068
[2019-04-09 15:06:07,428] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 48.0, 96.5, 749.5, 22.5, 26.20397555100021, 0.6648861360015864, 1.0, 1.0, 35.0, 18.229807760551317], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3855600.0000, 
sim time next is 3856800.0000, 
raw observation next is [2.333333333333333, 47.0, 90.16666666666666, 727.8333333333333, 22.5, 25.92659733910997, 0.6537886387964035, 1.0, 1.0, 35.0, 15.977077183435995], 
processed observation next is [1.0, 0.6521739130434783, 0.5272391505078486, 0.47, 0.3005555555555555, 0.8042357274401473, 0.375, 0.6605497782591643, 0.7179295462654678, 1.0, 1.0, 0.4, 0.15977077183435995], 
reward next is 0.8402, 
noisyNet noise sample is [array([1.0073148], dtype=float32), -0.36869746]. 
=============================================
[2019-04-09 15:06:07,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.6342118e-04 5.4820585e-03 1.7428344e-02 1.9805152e-04 4.5091176e-01
 1.2146149e-01 3.3124298e-01 9.9154739e-03 3.8024697e-02 1.9868664e-02
 4.6030576e-03], sum to 1.0000
[2019-04-09 15:06:07,892] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5963
[2019-04-09 15:06:07,917] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.1397278e-04 9.4721653e-03 5.1131394e-02 1.3683406e-04 6.6266650e-01
 1.2049559e-01 7.8813329e-02 9.5883636e-03 4.7053475e-02 1.5508719e-02
 4.5196302e-03], sum to 1.0000
[2019-04-09 15:06:07,919] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 24.02036367356829, 0.1443912854635088, 0.0, 1.0, 40.0, 27.66357478051828], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3807600.0000, 
sim time next is 3808800.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 24.06813850910663, 0.1250351789395034, 0.0, 1.0, 40.0, 25.93327630207029], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.5056782090922193, 0.5416783929798344, 0.0, 1.0, 0.5, 0.2593327630207029], 
reward next is 0.7407, 
noisyNet noise sample is [array([-0.13785239], dtype=float32), -1.3015932]. 
=============================================
[2019-04-09 15:06:07,929] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3151
[2019-04-09 15:06:07,934] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.11508016e-04 1.94761215e-03 2.25646775e-02 5.58176107e-05
 5.93508303e-01 1.32067576e-01 2.11067080e-01 2.56095966e-03
 2.05978807e-02 9.96970106e-03 5.54892886e-03], sum to 1.0000
[2019-04-09 15:06:07,935] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1392
[2019-04-09 15:06:07,950] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 63.33333333333333, 0.0, 0.0, 19.0, 25.19773292531684, 0.4184137625369073, 0.0, 1.0, 45.0, 29.628758423952505], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3886800.0000, 
sim time next is 3888000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 25.03124556019002, 0.3830340780513498, 0.0, 1.0, 35.0, 23.05553476079655], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.5859371300158349, 0.6276780260171165, 0.0, 1.0, 0.4, 0.2305553476079655], 
reward next is 0.7694, 
noisyNet noise sample is [array([-0.08342215], dtype=float32), -1.3501362]. 
=============================================
[2019-04-09 15:06:07,960] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[14.391395]
 [14.022716]
 [14.379734]
 [14.595583]
 [14.440718]], R is [[14.56773376]
 [15.12576866]
 [15.75984573]
 [16.366642  ]
 [16.94207191]].
[2019-04-09 15:06:07,970] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 34.0, 77.0, 630.0, 22.5, 26.18146765391657, 0.6118421547605088, 1.0, 1.0, 45.0, 28.970909161188246], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3945600.0000, 
sim time next is 3946800.0000, 
raw observation next is [-4.333333333333334, 35.33333333333334, 69.66666666666666, 567.3333333333334, 22.5, 26.49229955771544, 0.3495323243023285, 1.0, 1.0, 35.0, 23.100990615979683], 
processed observation next is [1.0, 0.6956521739130435, 0.3425669436749769, 0.35333333333333344, 0.2322222222222222, 0.6268876611418048, 0.375, 0.7076916298096201, 0.6165107747674429, 1.0, 1.0, 0.4, 0.23100990615979683], 
reward next is 0.7690, 
noisyNet noise sample is [array([-0.20163997], dtype=float32), -3.1255918]. 
=============================================
[2019-04-09 15:06:07,980] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.3809242e-05 3.4511813e-03 2.6337266e-02 1.4044814e-04 7.0255202e-01
 9.2916839e-02 1.1429785e-01 3.2785535e-03 4.5759581e-02 7.6783667e-03
 3.5141655e-03], sum to 1.0000
[2019-04-09 15:06:07,982] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6111
[2019-04-09 15:06:08,015] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.0, 40.5, 343.0, 22.5, 27.29812606457827, 0.5432330528617065, 1.0, 1.0, 35.0, 19.25850346585825], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3776400.0000, 
sim time next is 3777600.0000, 
raw observation next is [-0.6666666666666666, 63.66666666666667, 24.83333333333333, 212.3333333333333, 22.5, 26.62957876265636, 0.6556610648764004, 1.0, 1.0, 35.0, 17.65153448482964], 
processed observation next is [1.0, 0.7391304347826086, 0.44413665743305636, 0.6366666666666667, 0.08277777777777776, 0.23462246777163898, 0.375, 0.7191315635546968, 0.7185536882921335, 1.0, 1.0, 0.4, 0.1765153448482964], 
reward next is 0.8235, 
noisyNet noise sample is [array([-0.01814801], dtype=float32), -0.6468907]. 
=============================================
[2019-04-09 15:06:08,148] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3414307e-03 2.0309625e-02 2.6916109e-02 4.7192094e-04 5.3321886e-01
 1.9344297e-01 1.1991540e-01 1.6593181e-02 4.3975707e-02 2.9921744e-02
 1.3893024e-02], sum to 1.0000
[2019-04-09 15:06:08,157] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8196
[2019-04-09 15:06:08,232] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 23.60905587127161, 0.0597203476152447, 0.0, 1.0, 40.0, 34.81090318042763], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3902400.0000, 
sim time next is 3903600.0000, 
raw observation next is [-3.333333333333333, 73.0, 0.0, 0.0, 19.0, 23.82173414044783, 0.05534326190660246, 0.0, 1.0, 45.0, 32.339161070929585], 
processed observation next is [1.0, 0.17391304347826086, 0.37026777469990774, 0.73, 0.0, 0.0, 0.08333333333333333, 0.48514451170398587, 0.5184477539688676, 0.0, 1.0, 0.6, 0.32339161070929584], 
reward next is 0.6766, 
noisyNet noise sample is [array([0.29918456], dtype=float32), -0.66875064]. 
=============================================
[2019-04-09 15:06:08,403] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00097463 0.01557878 0.03567578 0.00069591 0.3762556  0.23140344
 0.23494141 0.01655859 0.05387113 0.02402497 0.01001976], sum to 1.0000
[2019-04-09 15:06:08,414] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8913
[2019-04-09 15:06:08,470] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 23.61045402903906, -0.007760848527297294, 0.0, 1.0, 35.0, 23.917395916531383], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3906000.0000, 
sim time next is 3907200.0000, 
raw observation next is [-4.666666666666667, 71.0, 0.0, 0.0, 19.0, 23.49427498014986, -0.02070194138311766, 0.0, 1.0, 45.0, 34.449753956221734], 
processed observation next is [1.0, 0.21739130434782608, 0.3333333333333333, 0.71, 0.0, 0.0, 0.08333333333333333, 0.4578562483458217, 0.49309935287229406, 0.0, 1.0, 0.6, 0.34449753956221735], 
reward next is 0.6555, 
noisyNet noise sample is [array([2.4856741], dtype=float32), 0.28799665]. 
=============================================
[2019-04-09 15:06:08,986] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00158289 0.01281236 0.02905943 0.00081467 0.46778053 0.14292896
 0.23914255 0.0116931  0.05750617 0.02783486 0.00884451], sum to 1.0000
[2019-04-09 15:06:09,001] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8641
[2019-04-09 15:06:09,024] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 23.5434701843262, 0.02163329865469012, 0.0, 1.0, 40.0, 28.90641634472115], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3812400.0000, 
sim time next is 3813600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 23.41210525595012, 0.02230083496888348, 0.0, 1.0, 45.0, 37.19164275015436], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.45100877132917666, 0.5074336116562945, 0.0, 1.0, 0.6, 0.3719164275015436], 
reward next is 0.6281, 
noisyNet noise sample is [array([0.21328972], dtype=float32), 0.31309474]. 
=============================================
[2019-04-09 15:06:09,180] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.3731267e-04 2.1900828e-03 1.8952670e-02 4.1436630e-05 5.4767972e-01
 1.0625327e-01 2.9130682e-01 2.3295728e-03 2.2236265e-02 7.2502890e-03
 1.6225363e-03], sum to 1.0000
[2019-04-09 15:06:09,180] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8873
[2019-04-09 15:06:09,261] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 34.0, 77.0, 630.0, 22.5, 26.69606960673834, 0.6970847508196245, 1.0, 1.0, 35.0, 22.864551094729293], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3945600.0000, 
sim time next is 3946800.0000, 
raw observation next is [-4.333333333333334, 35.33333333333334, 69.66666666666666, 567.3333333333334, 22.5, 26.74545219828782, 0.6806786918176405, 1.0, 1.0, 45.0, 27.939904095927197], 
processed observation next is [1.0, 0.6956521739130435, 0.3425669436749769, 0.35333333333333344, 0.2322222222222222, 0.6268876611418048, 0.375, 0.7287876831906516, 0.7268928972725468, 1.0, 1.0, 0.6, 0.279399040959272], 
reward next is 0.7206, 
noisyNet noise sample is [array([-0.6873797], dtype=float32), 0.9578851]. 
=============================================
[2019-04-09 15:06:09,420] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0514401e-04 2.1833384e-03 1.0595851e-02 6.3431755e-05 6.0669929e-01
 7.5190730e-02 2.5768355e-01 5.2620140e-03 3.0930124e-02 8.1822304e-03
 3.1043009e-03], sum to 1.0000
[2019-04-09 15:06:09,421] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2217
[2019-04-09 15:06:09,477] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 22.5, 25.93420842069645, 0.6401474552680132, 1.0, 1.0, 55.0, 49.601959576938526], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3781200.0000, 
sim time next is 3782400.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 22.5, 26.29546075742338, 0.6695982456542339, 1.0, 1.0, 35.0, 31.438831523245405], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.375, 0.6912883964519484, 0.723199415218078, 1.0, 1.0, 0.4, 0.31438831523245403], 
reward next is 0.6856, 
noisyNet noise sample is [array([1.4690593], dtype=float32), -0.9422663]. 
=============================================
[2019-04-09 15:06:09,919] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.1310497e-05 1.9188424e-03 5.3183176e-03 2.0668400e-05 7.4317217e-01
 1.1432733e-01 1.0719460e-01 1.9432784e-03 1.8306790e-02 4.8420280e-03
 2.9047332e-03], sum to 1.0000
[2019-04-09 15:06:09,921] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5643
[2019-04-09 15:06:10,027] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 42.33333333333334, 56.33333333333334, 489.0, 22.5, 26.79708549979496, 0.7274246636998593, 1.0, 1.0, 35.0, 15.93078977592427], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3861600.0000, 
sim time next is 3862800.0000, 
raw observation next is [3.0, 41.0, 41.0, 365.0, 22.5, 26.61839773315715, 0.4784648105024233, 1.0, 1.0, 35.0, 16.29231210092733], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.41, 0.13666666666666666, 0.40331491712707185, 0.375, 0.718199811096429, 0.6594882701674744, 1.0, 1.0, 0.4, 0.1629231210092733], 
reward next is 0.8371, 
noisyNet noise sample is [array([-1.3639627], dtype=float32), -1.6064705]. 
=============================================
[2019-04-09 15:06:10,271] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9187411e-04 4.2265151e-03 3.6545221e-02 2.5845988e-04 6.4564776e-01
 9.7369157e-02 1.5920249e-01 5.3289086e-03 3.2743216e-02 1.3629326e-02
 4.7570313e-03], sum to 1.0000
[2019-04-09 15:06:10,272] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5220
[2019-04-09 15:06:10,319] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.666666666666667, 49.0, 110.8333333333333, 761.1666666666667, 22.5, 24.88044704490111, 0.2906656145567668, 1.0, 1.0, 35.0, 25.703435532331298], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3925200.0000, 
sim time next is 3926400.0000, 
raw observation next is [-6.333333333333334, 49.0, 114.1666666666667, 780.8333333333334, 22.5, 25.10659483141092, 0.1633377717204048, 1.0, 1.0, 35.0, 22.24548664808426], 
processed observation next is [1.0, 0.43478260869565216, 0.28716528162511545, 0.49, 0.38055555555555565, 0.8627992633517496, 0.375, 0.59221623595091, 0.5544459239068016, 1.0, 1.0, 0.4, 0.2224548664808426], 
reward next is 0.7775, 
noisyNet noise sample is [array([-1.462388], dtype=float32), -0.20366034]. 
=============================================
[2019-04-09 15:06:10,550] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.65524087e-04 4.82915528e-03 1.02665480e-02 1.58483002e-04
 5.13582706e-01 1.08763166e-01 3.05794656e-01 8.88692960e-03
 2.59622764e-02 1.37183154e-02 7.67215854e-03], sum to 1.0000
[2019-04-09 15:06:10,551] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5517
[2019-04-09 15:06:10,593] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.666666666666668, 51.66666666666666, 0.0, 0.0, 19.0, 24.88100679651252, 0.2832882768084995, 0.0, 1.0, 45.0, 34.5225820469943], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3969600.0000, 
sim time next is 3970800.0000, 
raw observation next is [-9.0, 53.0, 0.0, 0.0, 19.0, 24.54216488587235, 0.2164062719702241, 0.0, 1.0, 40.0, 29.987149258074282], 
processed observation next is [1.0, 1.0, 0.21329639889196678, 0.53, 0.0, 0.0, 0.08333333333333333, 0.5451804071560291, 0.5721354239900748, 0.0, 1.0, 0.5, 0.2998714925807428], 
reward next is 0.7001, 
noisyNet noise sample is [array([-1.4411862], dtype=float32), -0.74475247]. 
=============================================
[2019-04-09 15:06:10,818] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.7820386e-05 2.3000957e-03 1.8289443e-02 1.0949350e-04 5.9278488e-01
 1.1115159e-01 2.2224151e-01 6.6393847e-03 2.8168345e-02 1.3441800e-02
 4.7855889e-03], sum to 1.0000
[2019-04-09 15:06:10,819] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9291
[2019-04-09 15:06:10,887] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 38.0, 102.1666666666667, 777.3333333333334, 22.5, 25.72174352251393, 0.4883746353896363, 1.0, 1.0, 35.0, 16.587370650231964], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3940800.0000, 
sim time next is 3942000.0000, 
raw observation next is [-4.0, 38.0, 96.5, 756.0, 22.5, 25.83658096155974, 0.5423694855261099, 1.0, 1.0, 45.0, 29.862442133381016], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.38, 0.32166666666666666, 0.8353591160220994, 0.375, 0.6530484134633117, 0.6807898285087033, 1.0, 1.0, 0.6, 0.2986244213338102], 
reward next is 0.7014, 
noisyNet noise sample is [array([0.40055242], dtype=float32), -0.063444756]. 
=============================================
[2019-04-09 15:06:10,899] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[15.2867985]
 [15.485506 ]
 [15.447765 ]
 [15.241234 ]
 [15.417549 ]], R is [[16.06498718]
 [16.73846436]
 [17.38753319]
 [18.00128365]
 [18.57343102]].
[2019-04-09 15:06:11,316] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.2408765e-05 3.7790483e-03 9.1056777e-03 7.7137694e-05 6.6965115e-01
 1.0768877e-01 1.5830666e-01 4.8774588e-03 2.8278470e-02 1.4277337e-02
 3.8658830e-03], sum to 1.0000
[2019-04-09 15:06:11,316] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3375
[2019-04-09 15:06:11,392] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.333333333333334, 35.33333333333334, 69.66666666666666, 567.3333333333334, 22.5, 26.22840098368695, 0.2979012411758604, 1.0, 1.0, 35.0, 15.573153199063515], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3946800.0000, 
sim time next is 3948000.0000, 
raw observation next is [-4.666666666666666, 36.66666666666666, 58.16666666666666, 474.8333333333334, 22.5, 25.68966015587509, 0.5125281788355466, 1.0, 1.0, 35.0, 17.401251969499803], 
processed observation next is [1.0, 0.6956521739130435, 0.33333333333333337, 0.3666666666666666, 0.19388888888888886, 0.5246777163904237, 0.375, 0.6408050129895907, 0.6708427262785155, 1.0, 1.0, 0.4, 0.17401251969499804], 
reward next is 0.8260, 
noisyNet noise sample is [array([-0.7160111], dtype=float32), -0.38679892]. 
=============================================
[2019-04-09 15:06:11,426] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[15.576626]
 [15.60722 ]
 [15.576821]
 [15.663534]
 [15.733833]], R is [[16.2568512 ]
 [16.93855095]
 [17.6193428 ]
 [18.28380394]
 [18.90939522]].
[2019-04-09 15:06:11,910] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2578726e-04 3.6262830e-03 1.4538369e-02 1.5749135e-04 5.0169909e-01
 2.1262336e-01 1.8717194e-01 8.6486610e-03 3.9553225e-02 2.1897458e-02
 9.6584074e-03], sum to 1.0000
[2019-04-09 15:06:11,911] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9135
[2019-04-09 15:06:11,981] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 24.64999929618885, 0.2979976475578713, 0.0, 1.0, 40.0, 23.49085990503427], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3888000.0000, 
sim time next is 3889200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 24.51814309615357, 0.2785506310736858, 0.0, 1.0, 45.0, 30.57890192465046], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.5431785913461308, 0.5928502103578953, 0.0, 1.0, 0.6, 0.3057890192465046], 
reward next is 0.6942, 
noisyNet noise sample is [array([-2.0450559], dtype=float32), 1.1953634]. 
=============================================
[2019-04-09 15:06:12,120] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.4271851e-05 1.5404820e-03 1.1348132e-02 4.5474262e-05 6.8257087e-01
 1.3185559e-01 1.4224273e-01 3.0281027e-03 1.8009555e-02 7.5074667e-03
 1.7673492e-03], sum to 1.0000
[2019-04-09 15:06:12,120] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9524
[2019-04-09 15:06:12,170] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 46.0, 83.16666666666666, 689.3333333333333, 22.5, 26.81311654186983, 0.7450469834865815, 1.0, 1.0, 35.0, 14.886222415412025], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3858000.0000, 
sim time next is 3859200.0000, 
raw observation next is [3.0, 45.0, 75.5, 634.0, 22.5, 26.93786463268275, 0.8001811577760463, 1.0, 1.0, 45.0, 27.78914548508822], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.25166666666666665, 0.7005524861878453, 0.375, 0.7448220527235625, 0.7667270525920155, 1.0, 1.0, 0.6, 0.2778914548508822], 
reward next is 0.7221, 
noisyNet noise sample is [array([1.486445], dtype=float32), 0.78269255]. 
=============================================
[2019-04-09 15:06:12,634] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2013632e-04 5.2303993e-03 2.0578306e-02 1.2872236e-04 7.6132715e-01
 5.8712874e-02 7.0763759e-02 6.6732215e-03 5.7058971e-02 1.3075431e-02
 6.0311309e-03], sum to 1.0000
[2019-04-09 15:06:12,637] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8069
[2019-04-09 15:06:12,732] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-8.333333333333334, 50.33333333333334, 0.0, 0.0, 19.0, 24.22684304250351, 0.2090828645003518, 0.0, 1.0, 55.0, 52.25840345204115], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3968400.0000, 
sim time next is 3969600.0000, 
raw observation next is [-8.666666666666668, 51.66666666666666, 0.0, 0.0, 19.0, 24.1977585138532, 0.1881577388124132, 0.0, 1.0, 40.0, 36.24156788226598], 
processed observation next is [1.0, 0.9565217391304348, 0.22253000923361033, 0.5166666666666666, 0.0, 0.0, 0.08333333333333333, 0.5164798761544332, 0.5627192462708044, 0.0, 1.0, 0.5, 0.36241567882265985], 
reward next is 0.6376, 
noisyNet noise sample is [array([0.46507585], dtype=float32), 1.243256]. 
=============================================
[2019-04-09 15:06:12,801] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5595033e-04 2.2743535e-03 1.3158449e-02 4.9739825e-05 7.6279342e-01
 6.4685315e-02 1.0316008e-01 4.3399679e-03 3.7896361e-02 9.7474307e-03
 1.7388485e-03], sum to 1.0000
[2019-04-09 15:06:12,802] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4907
[2019-04-09 15:06:12,843] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 19.0, 24.22157522507194, 0.2345392612398438, 0.0, 1.0, 35.0, 23.402047507850988], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3962400.0000, 
sim time next is 3963600.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 19.0, 24.07914387966271, 0.1963720920845509, 0.0, 1.0, 35.0, 21.438104813535496], 
processed observation next is [1.0, 0.9130434782608695, 0.2686980609418283, 0.45, 0.0, 0.0, 0.08333333333333333, 0.5065953233052257, 0.5654573640281836, 0.0, 1.0, 0.4, 0.21438104813535497], 
reward next is 0.7856, 
noisyNet noise sample is [array([0.02831845], dtype=float32), 0.75949115]. 
=============================================
[2019-04-09 15:06:12,899] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.2293374e-04 7.7112750e-03 1.6131550e-02 2.1762092e-04 5.2271432e-01
 1.6284701e-01 2.1264219e-01 9.0791127e-03 4.0875521e-02 2.1539748e-02
 5.4186410e-03], sum to 1.0000
[2019-04-09 15:06:12,901] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9640
[2019-04-09 15:06:12,943] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 24.59813051425074, 0.2493033771141105, 0.0, 1.0, 40.0, 25.9564493925072], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3891600.0000, 
sim time next is 3892800.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 24.58671050338859, 0.2800713463072417, 0.0, 1.0, 55.0, 50.1634941730007], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.5488925419490492, 0.5933571154357472, 0.0, 1.0, 0.8, 0.501634941730007], 
reward next is 0.4984, 
noisyNet noise sample is [array([1.4393591], dtype=float32), 1.3629391]. 
=============================================
[2019-04-09 15:06:12,959] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.55134688e-04 8.82296171e-03 3.11724618e-02 3.32226628e-04
 6.76812291e-01 8.40392634e-02 1.00463115e-01 1.05773304e-02
 4.42186669e-02 3.40312123e-02 8.57529324e-03], sum to 1.0000
[2019-04-09 15:06:12,961] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7211
[2019-04-09 15:06:13,010] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 19.0, 22.4895916645502, -0.1945935721589656, 0.0, 1.0, 35.0, 23.73784366593911], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 3910800.0000, 
sim time next is 3912000.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 19.0, 22.45308667167633, -0.2040542049847224, 0.0, 1.0, 40.0, 31.316062806086784], 
processed observation next is [1.0, 0.2608695652173913, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.08333333333333333, 0.3710905559730276, 0.4319819316717592, 0.0, 1.0, 0.5, 0.31316062806086786], 
reward next is 0.6868, 
noisyNet noise sample is [array([-0.1074805], dtype=float32), 0.025482638]. 
=============================================
[2019-04-09 15:06:13,029] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[11.726554]
 [11.836127]
 [11.756413]
 [11.996498]
 [12.085047]], R is [[12.26892948]
 [12.90886211]
 [13.46789455]
 [14.33321571]
 [15.01943493]].
[2019-04-09 15:06:13,416] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7365744e-04 1.1560938e-02 2.3025891e-02 2.1416046e-04 6.2571347e-01
 8.1354178e-02 2.0818308e-01 3.5357699e-03 2.6651265e-02 1.3097698e-02
 6.2899389e-03], sum to 1.0000
[2019-04-09 15:06:13,416] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9002
[2019-04-09 15:06:13,515] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.333333333333334, 50.33333333333334, 0.0, 0.0, 19.0, 23.94419798745236, 0.1799333652398478, 0.0, 1.0, 55.0, 53.44666565722549], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3968400.0000, 
sim time next is 3969600.0000, 
raw observation next is [-8.666666666666668, 51.66666666666666, 0.0, 0.0, 19.0, 24.02950970807219, 0.1703258698572052, 0.0, 1.0, 45.0, 37.18691876369288], 
processed observation next is [1.0, 0.9565217391304348, 0.22253000923361033, 0.5166666666666666, 0.0, 0.0, 0.08333333333333333, 0.5024591423393492, 0.5567752899524018, 0.0, 1.0, 0.6, 0.3718691876369288], 
reward next is 0.6281, 
noisyNet noise sample is [array([0.10260522], dtype=float32), -0.3163395]. 
=============================================
[2019-04-09 15:06:13,532] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.9921774e-04 9.9190166e-03 3.2609910e-02 4.5658607e-04 5.0662798e-01
 6.8920530e-02 2.6388520e-01 1.4070470e-02 6.4905845e-02 2.6073601e-02
 1.1531672e-02], sum to 1.0000
[2019-04-09 15:06:13,532] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1052
[2019-04-09 15:06:13,562] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.00178644 0.01192679 0.02013412 0.00108089 0.57953936 0.13313621
 0.1640844  0.00961179 0.03077083 0.03153349 0.01639566], sum to 1.0000
[2019-04-09 15:06:13,562] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9174
[2019-04-09 15:06:13,569] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 19.0, 22.65538311578842, -0.1628336693571781, 0.0, 1.0, 35.0, 36.06512790491016], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3984000.0000, 
sim time next is 3985200.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 19.0, 22.70501015775998, -0.171866638206478, 0.0, 1.0, 35.0, 31.556997833113392], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 0.08333333333333333, 0.3920841798133316, 0.4427111205978407, 0.0, 1.0, 0.4, 0.3155699783311339], 
reward next is 0.6844, 
noisyNet noise sample is [array([-1.7236996], dtype=float32), -0.055856075]. 
=============================================
[2019-04-09 15:06:13,751] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-13.66666666666667, 67.0, 0.0, 0.0, 22.5, 20.27272528453384, -0.7458342861393311, 1.0, 1.0, 35.0, 31.664630376253292], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4000800.0000, 
sim time next is 4002000.0000, 
raw observation next is [-13.33333333333333, 65.0, 15.5, 73.99999999999999, 22.5, 20.2683455968025, -0.6299623998403637, 1.0, 1.0, 50.0, 65.01113242224379], 
processed observation next is [1.0, 0.30434782608695654, 0.09325946445060027, 0.65, 0.051666666666666666, 0.08176795580110496, 0.375, 0.18902879973354172, 0.2900125333865454, 1.0, 1.0, 0.7, 0.6501113242224379], 
reward next is 0.4176, 
noisyNet noise sample is [array([2.4642272], dtype=float32), -1.3297155]. 
=============================================
[2019-04-09 15:06:13,756] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00142228 0.00973678 0.02354207 0.00051954 0.4928894  0.13412024
 0.20764573 0.01893081 0.0728336  0.02346021 0.01489932], sum to 1.0000
[2019-04-09 15:06:13,756] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0023
[2019-04-09 15:06:13,758] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[11.400238]
 [11.199206]
 [11.249915]
 [11.455772]
 [11.424294]], R is [[11.91473484]
 [11.82335949]
 [12.45924282]
 [13.09495163]
 [13.72795105]].
[2019-04-09 15:06:13,886] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-13.33333333333333, 65.0, 15.5, 73.99999999999999, 22.5, 21.33145610256646, -0.5082080101256407, 1.0, 1.0, 25.0, 34.74109706430566], 
current ob forecast is [], 
actual action is [1, 20.0], 
sim time this is 4002000.0000, 
sim time next is 4003200.0000, 
raw observation next is [-13.0, 63.0, 46.5, 222.0, 22.5, 21.46668875973257, -0.4903079098133427, 1.0, 1.0, 20.0, 26.688234840666876], 
processed observation next is [1.0, 0.34782608695652173, 0.10249307479224376, 0.63, 0.155, 0.24530386740331492, 0.375, 0.28889072997771414, 0.3365640300622191, 1.0, 1.0, 0.1, 0.26688234840666875], 
reward next is 0.7331, 
noisyNet noise sample is [array([1.3141564], dtype=float32), 1.123243]. 
=============================================
[2019-04-09 15:06:13,988] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.1056577e-04 6.2250406e-03 4.8122283e-02 2.7249017e-04 4.5279545e-01
 1.1603400e-01 2.7574116e-01 9.4731310e-03 3.3835772e-02 4.5040473e-02
 1.1749546e-02], sum to 1.0000
[2019-04-09 15:06:13,989] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3813
[2019-04-09 15:06:14,020] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 19.0, 23.46721558929018, -0.05762901365390365, 0.0, 1.0, 35.0, 23.788935411294638], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3912000.0000, 
sim time next is 3913200.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 23.06997637810783, -0.1211035229587998, 0.0, 1.0, 35.0, 21.16091859012462], 
processed observation next is [1.0, 0.30434782608695654, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.42249803150898596, 0.4596321590137334, 0.0, 1.0, 0.4, 0.2116091859012462], 
reward next is 0.7884, 
noisyNet noise sample is [array([-1.0957584], dtype=float32), -0.3439116]. 
=============================================
[2019-04-09 15:06:14,604] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4200845e-04 7.9976860e-03 2.5562160e-02 1.6048283e-04 4.9714512e-01
 1.3742036e-01 2.1768112e-01 8.3243987e-03 8.1520714e-02 1.6336095e-02
 7.5097815e-03], sum to 1.0000
[2019-04-09 15:06:14,605] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7336
[2019-04-09 15:06:14,631] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 61.66666666666667, 0.0, 0.0, 19.0, 24.62223679644125, 0.2881940255520229, 0.0, 1.0, 25.0, 20.389538310597565], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3885600.0000, 
sim time next is 3886800.0000, 
raw observation next is [-1.666666666666667, 63.33333333333333, 0.0, 0.0, 19.0, 24.4814022163523, 0.2749570385312785, 0.0, 1.0, 45.0, 31.07793227183647], 
processed observation next is [1.0, 1.0, 0.4164358264081256, 0.6333333333333333, 0.0, 0.0, 0.08333333333333333, 0.5401168513626917, 0.5916523461770928, 0.0, 1.0, 0.6, 0.3107793227183647], 
reward next is 0.6892, 
noisyNet noise sample is [array([-0.94586277], dtype=float32), -0.40681425]. 
=============================================
[2019-04-09 15:06:15,936] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.1316920e-04 3.7636040e-03 2.0730982e-02 7.5560951e-05 6.7094266e-01
 7.8038037e-02 1.7043915e-01 5.7459953e-03 3.5193734e-02 1.1347977e-02
 3.5091322e-03], sum to 1.0000
[2019-04-09 15:06:15,936] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3746
[2019-04-09 15:06:15,937] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3247432e-04 3.4500016e-03 1.5544954e-02 2.0866399e-04 6.7459881e-01
 1.0320523e-01 1.5195435e-01 9.1588162e-03 2.1390844e-02 1.5122253e-02
 4.7335145e-03], sum to 1.0000
[2019-04-09 15:06:15,938] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9628
[2019-04-09 15:06:15,979] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 49.0, 118.8333333333333, 804.1666666666666, 22.5, 25.71333225288014, 0.4998482657156435, 1.0, 1.0, 40.0, 23.050162311032963], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3928800.0000, 
sim time next is 3930000.0000, 
raw observation next is [-6.0, 49.0, 119.8333333333333, 814.1666666666666, 22.5, 25.72361767028917, 0.5001710180753708, 1.0, 1.0, 35.0, 20.069227613830577], 
processed observation next is [1.0, 0.4782608695652174, 0.296398891966759, 0.49, 0.3994444444444443, 0.8996316758747698, 0.375, 0.6436348058574307, 0.6667236726917903, 1.0, 1.0, 0.4, 0.20069227613830576], 
reward next is 0.7993, 
noisyNet noise sample is [array([0.3488019], dtype=float32), 0.90984577]. 
=============================================
[2019-04-09 15:06:16,003] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[16.493517]
 [16.657164]
 [16.416101]
 [16.18198 ]
 [16.200941]], R is [[17.27132416]
 [17.86811066]
 [18.40481758]
 [18.96784592]
 [19.49090195]].
[2019-04-09 15:06:16,054] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 19.0, 23.018953402677, -0.02930619340216462, 0.0, 1.0, 55.0, 56.88116483515884], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 3910800.0000, 
sim time next is 3912000.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 19.0, 23.45097023691245, 0.01392768647974326, 0.0, 1.0, 55.0, 50.93939316583209], 
processed observation next is [1.0, 0.2608695652173913, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.08333333333333333, 0.4542475197427042, 0.5046425621599144, 0.0, 1.0, 0.8, 0.5093939316583209], 
reward next is 0.4906, 
noisyNet noise sample is [array([-1.5368223], dtype=float32), -1.2480649]. 
=============================================
[2019-04-09 15:06:16,099] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[13.168906 ]
 [13.002914 ]
 [13.0215645]
 [13.006481 ]
 [12.899548 ]], R is [[13.62226963]
 [13.91723537]
 [14.4172802 ]
 [15.07924843]
 [15.71557426]].
[2019-04-09 15:06:17,190] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.80846531e-05 1.38254534e-03 1.25888186e-02 3.41211489e-05
 7.10774362e-01 1.18207917e-01 1.09885596e-01 4.18758998e-03
 2.25372724e-02 1.24665275e-02 7.85707217e-03], sum to 1.0000
[2019-04-09 15:06:17,190] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0929
[2019-04-09 15:06:17,323] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 20.0, 96.5, 753.0, 22.5, 27.46058590099905, 0.8231501130334689, 1.0, 1.0, 35.0, 35.203072365330186], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4028400.0000, 
sim time next is 4029600.0000, 
raw observation next is [-1.666666666666667, 20.66666666666667, 91.5, 725.6666666666667, 22.5, 27.57478140949904, 0.8526362791266843, 1.0, 1.0, 35.0, 24.88825058746118], 
processed observation next is [1.0, 0.6521739130434783, 0.4164358264081256, 0.20666666666666672, 0.305, 0.801841620626151, 0.375, 0.7978984507915866, 0.7842120930422282, 1.0, 1.0, 0.4, 0.2488825058746118], 
reward next is 0.7511, 
noisyNet noise sample is [array([-0.7370163], dtype=float32), 0.48200086]. 
=============================================
[2019-04-09 15:06:17,507] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5723311e-04 3.1087149e-03 1.7032266e-02 6.5800923e-05 5.4077041e-01
 9.9563316e-02 2.8565910e-01 5.2170325e-03 3.0132517e-02 1.2303160e-02
 5.9905197e-03], sum to 1.0000
[2019-04-09 15:06:17,507] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7632
[2019-04-09 15:06:17,625] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 31.66666666666666, 117.3333333333333, 839.1666666666667, 22.5, 25.06727599789549, 0.1283067485661304, 1.0, 1.0, 35.0, 19.503514142310305], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4020000.0000, 
sim time next is 4021200.0000, 
raw observation next is [-4.0, 29.0, 116.0, 835.5, 22.5, 24.87071629044852, 0.3144609673878, 1.0, 1.0, 45.0, 41.574459145285616], 
processed observation next is [1.0, 0.5652173913043478, 0.3518005540166205, 0.29, 0.38666666666666666, 0.9232044198895027, 0.375, 0.57255969087071, 0.6048203224626, 1.0, 1.0, 0.6, 0.41574459145285614], 
reward next is 0.5843, 
noisyNet noise sample is [array([0.41677183], dtype=float32), 1.1203527]. 
=============================================
[2019-04-09 15:06:18,379] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00055082 0.01331586 0.02908528 0.00046425 0.3977448  0.14574711
 0.25469163 0.02324017 0.07332083 0.03805509 0.02378422], sum to 1.0000
[2019-04-09 15:06:18,380] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6843
[2019-04-09 15:06:18,479] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 35.33333333333334, 0.0, 0.0, 19.0, 23.4509508067163, -0.07891897192488778, 0.0, 1.0, 40.0, 25.521788084902024], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4080000.0000, 
sim time next is 4081200.0000, 
raw observation next is [-4.0, 36.66666666666666, 0.0, 0.0, 19.0, 23.38222951870366, -0.08893046866605485, 0.0, 1.0, 45.0, 33.42841199942328], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.3666666666666666, 0.0, 0.0, 0.08333333333333333, 0.44851912655863835, 0.4703565104446484, 0.0, 1.0, 0.6, 0.3342841199942328], 
reward next is 0.6657, 
noisyNet noise sample is [array([0.36560667], dtype=float32), -1.077998]. 
=============================================
[2019-04-09 15:06:18,792] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1380002e-04 3.5994202e-03 1.8702596e-02 1.5907550e-04 4.4014382e-01
 2.3482338e-01 1.8251900e-01 7.3268609e-03 6.8169959e-02 3.8459852e-02
 5.4822271e-03], sum to 1.0000
[2019-04-09 15:06:18,793] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0184
[2019-04-09 15:06:18,823] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.333333333333334, 33.0, 0.0, 0.0, 19.0, 24.38475193720139, 0.1437494843525301, 0.0, 1.0, 40.0, 25.470603756458303], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4054800.0000, 
sim time next is 4056000.0000, 
raw observation next is [-5.666666666666667, 35.0, 0.0, 0.0, 19.0, 24.078798390378, 0.08884722651173778, 0.0, 1.0, 25.0, 20.17296121183301], 
processed observation next is [1.0, 0.9565217391304348, 0.30563250230840255, 0.35, 0.0, 0.0, 0.08333333333333333, 0.5065665325315001, 0.5296157421705793, 0.0, 1.0, 0.2, 0.2017296121183301], 
reward next is 0.7983, 
noisyNet noise sample is [array([0.29372564], dtype=float32), -0.39179042]. 
=============================================
[2019-04-09 15:06:18,846] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[14.894565]
 [15.045506]
 [15.345798]
 [15.295893]
 [15.471843]], R is [[15.37954903]
 [15.9710474 ]
 [16.53733063]
 [17.12962151]
 [17.5988369 ]].
[2019-04-09 15:06:19,222] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1140060e-03 8.1213135e-03 4.3715723e-02 2.9091281e-04 6.3825101e-01
 7.5162724e-02 1.7293051e-01 1.2147953e-02 2.9020172e-02 1.4398184e-02
 4.8474097e-03], sum to 1.0000
[2019-04-09 15:06:19,222] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8060
[2019-04-09 15:06:19,331] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 19.0, 23.08395498215241, -0.04985387085443462, 0.0, 1.0, 55.0, 55.7455878807644], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4065600.0000, 
sim time next is 4066800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 19.0, 23.60577648930786, -0.00188489443726896, 0.0, 1.0, 45.0, 36.24300090151701], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.08333333333333333, 0.467148040775655, 0.4993717018542437, 0.0, 1.0, 0.6, 0.3624300090151701], 
reward next is 0.6376, 
noisyNet noise sample is [array([-0.25867373], dtype=float32), 0.5328992]. 
=============================================
[2019-04-09 15:06:20,879] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.1824862e-04 4.3484215e-03 2.8642440e-02 2.6764540e-04 6.0986066e-01
 1.1742633e-01 1.5406384e-01 9.5191542e-03 4.3792140e-02 2.4069484e-02
 7.5916043e-03], sum to 1.0000
[2019-04-09 15:06:20,879] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4854
[2019-04-09 15:06:20,914] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 34.0, 0.0, 0.0, 19.0, 23.90895579558478, 0.01743073775401778, 0.0, 1.0, 40.0, 24.91336638766412], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4078800.0000, 
sim time next is 4080000.0000, 
raw observation next is [-4.0, 35.33333333333334, 0.0, 0.0, 19.0, 23.83003537459465, -0.02660534860881172, 0.0, 1.0, 35.0, 23.21988584577987], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.35333333333333344, 0.0, 0.0, 0.08333333333333333, 0.4858362812162209, 0.4911315504637294, 0.0, 1.0, 0.4, 0.2321988584577987], 
reward next is 0.7678, 
noisyNet noise sample is [array([-1.2560691], dtype=float32), -0.5361701]. 
=============================================
[2019-04-09 15:06:20,947] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[13.87228  ]
 [14.032918 ]
 [14.205382 ]
 [13.8582945]
 [13.943099 ]], R is [[14.56386948]
 [15.16909695]
 [15.69846058]
 [16.23106384]
 [16.80884552]].
[2019-04-09 15:06:21,076] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.0934573e-04 5.1184036e-03 1.8986745e-02 2.6318149e-04 5.5522799e-01
 1.6341111e-01 1.7421091e-01 8.8020982e-03 3.7952870e-02 2.4294406e-02
 1.1122983e-02], sum to 1.0000
[2019-04-09 15:06:21,076] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5113
[2019-04-09 15:06:21,130] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 41.0, 0.0, 0.0, 19.0, 24.03190611967369, 0.05332004489087903, 0.0, 1.0, 55.0, 45.348281552669356], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4086000.0000, 
sim time next is 4087200.0000, 
raw observation next is [-4.666666666666667, 38.66666666666667, 0.0, 0.0, 22.5, 24.10241472335363, 0.05098330264863974, 0.0, 1.0, 35.0, 34.991456960881365], 
processed observation next is [1.0, 0.30434782608695654, 0.3333333333333333, 0.3866666666666667, 0.0, 0.0, 0.375, 0.5085345602794691, 0.5169944342162133, 0.0, 1.0, 0.4, 0.3499145696088137], 
reward next is 0.6501, 
noisyNet noise sample is [array([-0.86921036], dtype=float32), 0.77186936]. 
=============================================
[2019-04-09 15:06:21,167] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5746369e-05 1.1860579e-03 9.3864864e-03 3.1951447e-05 6.5401191e-01
 1.2608148e-01 1.6300848e-01 1.3480198e-03 3.3651970e-02 1.0465996e-02
 7.9199352e-04], sum to 1.0000
[2019-04-09 15:06:21,167] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8642
[2019-04-09 15:06:21,299] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 24.0, 43.5, 370.5, 22.5, 27.12096372799784, 0.579595758048821, 1.0, 1.0, 55.0, 55.79798979484318], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4035600.0000, 
sim time next is 4036800.0000, 
raw observation next is [-2.333333333333333, 24.66666666666666, 27.83333333333333, 252.1666666666667, 22.5, 26.9739060273783, 0.7033701838312482, 1.0, 1.0, 45.0, 32.85799866025528], 
processed observation next is [1.0, 0.7391304347826086, 0.3979686057248385, 0.24666666666666662, 0.09277777777777776, 0.2786372007366483, 0.375, 0.7478255022815249, 0.7344567279437494, 1.0, 1.0, 0.6, 0.3285799866025528], 
reward next is 0.6714, 
noisyNet noise sample is [array([1.4300008], dtype=float32), -0.8322894]. 
=============================================
[2019-04-09 15:06:21,742] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2932826e-04 6.4572198e-03 2.1805525e-02 4.5423571e-04 4.5437354e-01
 1.4390554e-01 2.7961615e-01 9.0406053e-03 5.0654151e-02 2.2178583e-02
 1.0685141e-02], sum to 1.0000
[2019-04-09 15:06:21,748] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5918
[2019-04-09 15:06:21,786] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.333333333333334, 39.0, 0.0, 0.0, 19.0, 22.50696089970528, -0.2998106350217433, 0.0, 1.0, 35.0, 23.301435796431498], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 4083600.0000, 
sim time next is 4084800.0000, 
raw observation next is [-4.666666666666666, 40.0, 0.0, 0.0, 19.0, 22.39933948042875, -0.3221818822703595, 0.0, 1.0, 40.0, 26.762052826824004], 
processed observation next is [1.0, 0.2608695652173913, 0.33333333333333337, 0.4, 0.0, 0.0, 0.08333333333333333, 0.36661162336906256, 0.3926060392432135, 0.0, 1.0, 0.5, 0.26762052826824], 
reward next is 0.7324, 
noisyNet noise sample is [array([-0.95402974], dtype=float32), 1.0974987]. 
=============================================
[2019-04-09 15:06:22,132] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5878533e-04 4.3109106e-03 2.0277878e-02 2.3075686e-04 6.2851769e-01
 8.1909671e-02 1.7978008e-01 1.1679987e-02 4.7458682e-02 1.8721888e-02
 6.6537252e-03], sum to 1.0000
[2019-04-09 15:06:22,132] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9191
[2019-04-09 15:06:22,210] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-2.666666666666667, 37.0, 102.0, 644.0, 22.5, 24.16107777879317, 0.07120746466335993, 1.0, 1.0, 35.0, 15.282746104920555], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4094400.0000, 
sim time next is 4095600.0000, 
raw observation next is [-2.333333333333333, 36.0, 105.6666666666667, 694.0, 22.5, 24.54901599703955, 0.1927704780569152, 1.0, 1.0, 50.0, 42.83956913776467], 
processed observation next is [1.0, 0.391304347826087, 0.3979686057248385, 0.36, 0.3522222222222223, 0.7668508287292818, 0.375, 0.5457513330866292, 0.5642568260189718, 1.0, 1.0, 0.7, 0.4283956913776467], 
reward next is 0.5716, 
noisyNet noise sample is [array([-1.420651], dtype=float32), -1.4445035]. 
=============================================
[2019-04-09 15:06:22,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.3211577e-04 9.0003936e-03 1.8665217e-02 6.1787193e-04 6.3473755e-01
 7.8967586e-02 1.5763946e-01 1.2885778e-02 4.1792575e-02 3.3822332e-02
 1.1039197e-02], sum to 1.0000
[2019-04-09 15:06:22,351] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4244
[2019-04-09 15:06:22,354] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.7813020e-05 7.9098949e-04 6.4935400e-03 1.9529300e-05 8.0105495e-01
 6.3936219e-02 1.0193253e-01 3.2281603e-03 1.2163249e-02 9.3982369e-03
 9.0468244e-04], sum to 1.0000
[2019-04-09 15:06:22,354] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9344
[2019-04-09 15:06:22,431] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.666666666666667, 48.66666666666666, 0.0, 0.0, 19.0, 24.52751222148471, 0.2764123060846121, 0.0, 1.0, 65.0, 57.38401719100326], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4156800.0000, 
sim time next is 4158000.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 19.0, 24.6027175930733, 0.28769731276472, 0.0, 1.0, 35.0, 33.65967771752372], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.08333333333333333, 0.5502264660894417, 0.5958991042549067, 0.0, 1.0, 0.4, 0.3365967771752372], 
reward next is 0.6634, 
noisyNet noise sample is [array([0.49623317], dtype=float32), 1.0659794]. 
=============================================
[2019-04-09 15:06:22,436] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[12.314081]
 [12.313576]
 [13.163868]
 [13.689135]
 [13.841066]], R is [[12.00000668]
 [12.30616665]
 [12.92774582]
 [13.58957291]
 [14.22688293]].
[2019-04-09 15:06:22,459] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 30.33333333333333, 114.3333333333333, 824.0, 22.5, 25.7338448236406, 0.5040379924391084, 1.0, 1.0, 35.0, 13.854989866630525], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4110000.0000, 
sim time next is 4111200.0000, 
raw observation next is [3.0, 31.0, 111.0, 812.0, 22.5, 25.90100718797703, 0.5831209460202013, 1.0, 1.0, 45.0, 28.2549130691038], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.31, 0.37, 0.8972375690607735, 0.375, 0.6584172656647524, 0.6943736486734005, 1.0, 1.0, 0.6, 0.282549130691038], 
reward next is 0.7175, 
noisyNet noise sample is [array([0.2117159], dtype=float32), -2.048873]. 
=============================================
[2019-04-09 15:06:22,756] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.4691413e-05 9.6974539e-04 6.1054928e-03 4.5347770e-05 6.8982434e-01
 8.2907185e-02 1.8237619e-01 4.0391805e-03 2.0682605e-02 9.4997408e-03
 3.5255298e-03], sum to 1.0000
[2019-04-09 15:06:22,758] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9650
[2019-04-09 15:06:22,867] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [-1.666666666666667, 20.66666666666667, 91.5, 725.6666666666667, 22.5, 27.21651981362802, 0.776489907642798, 1.0, 1.0, 35.0, 21.748860798207684], 
current ob forecast is [], 
actual action is [1, 50.0], 
sim time this is 4029600.0000, 
sim time next is 4030800.0000, 
raw observation next is [-1.333333333333333, 21.33333333333334, 85.33333333333334, 684.6666666666667, 22.5, 27.46974978190627, 0.8534290472489546, 1.0, 1.0, 50.0, 39.41462400441689], 
processed observation next is [1.0, 0.6521739130434783, 0.42566943674976926, 0.2133333333333334, 0.2844444444444445, 0.7565377532228362, 0.375, 0.7891458151588558, 0.7844763490829849, 1.0, 1.0, 0.7, 0.3941462400441689], 
reward next is 0.6059, 
noisyNet noise sample is [array([-0.17439428], dtype=float32), 0.34899765]. 
=============================================
[2019-04-09 15:06:23,274] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1641431e-03 8.5606892e-03 4.0723521e-02 3.6143709e-04 5.7222497e-01
 1.4095734e-01 1.4088617e-01 7.4421181e-03 5.4727960e-02 2.2062214e-02
 1.0889439e-02], sum to 1.0000
[2019-04-09 15:06:23,275] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3428
[2019-04-09 15:06:23,360] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 38.0, 0.0, 0.0, 19.0, 23.97652110911549, 0.02865213457662354, 0.0, 1.0, 45.0, 31.570005502004797], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4071600.0000, 
sim time next is 4072800.0000, 
raw observation next is [-5.0, 39.0, 0.0, 0.0, 19.0, 23.92488521827177, -0.007900495903827742, 0.0, 1.0, 35.0, 28.29645982361314], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.39, 0.0, 0.0, 0.08333333333333333, 0.4937404348559807, 0.49736650136539073, 0.0, 1.0, 0.4, 0.2829645982361314], 
reward next is 0.7170, 
noisyNet noise sample is [array([0.41015044], dtype=float32), 0.23637292]. 
=============================================
[2019-04-09 15:06:23,908] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.4425083e-05 1.1250887e-03 8.1183910e-03 4.0934126e-05 7.7129459e-01
 6.0120307e-02 1.2779120e-01 2.3785122e-03 1.9255985e-02 8.1796618e-03
 1.6309154e-03], sum to 1.0000
[2019-04-09 15:06:23,909] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1010
[2019-04-09 15:06:23,961] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 38.66666666666666, 0.0, 0.0, 19.0, 26.19312309799573, 0.6404973605221657, 0.0, 1.0, 35.0, 20.759428336392965], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4138800.0000, 
sim time next is 4140000.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 19.0, 26.1021198514405, 0.6112860015683977, 0.0, 1.0, 35.0, 18.971358807792157], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.4, 0.0, 0.0, 0.08333333333333333, 0.6751766542867083, 0.7037620005227992, 0.0, 1.0, 0.4, 0.18971358807792157], 
reward next is 0.8103, 
noisyNet noise sample is [array([-0.5155884], dtype=float32), 0.8563957]. 
=============================================
[2019-04-09 15:06:23,965] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[16.284395]
 [16.778423]
 [16.982445]
 [16.896847]
 [17.180777]], R is [[17.09440041]
 [17.71586227]
 [18.30958748]
 [18.87176323]
 [19.37785149]].
[2019-04-09 15:06:24,021] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.1186981e-04 1.7694857e-03 1.2560307e-02 3.6759542e-05 7.0480698e-01
 8.7239228e-02 1.2266685e-01 4.5763748e-03 5.7041895e-02 6.7573953e-03
 2.4327901e-03], sum to 1.0000
[2019-04-09 15:06:24,041] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7258
[2019-04-09 15:06:24,091] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.6666666666666666, 42.33333333333334, 0.0, 0.0, 19.0, 25.56608666486698, 0.4330296517148515, 0.0, 1.0, 35.0, 15.40732582168073], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 4146000.0000, 
sim time next is 4147200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 25.36447394465015, 0.4069702104302954, 0.0, 1.0, 40.0, 23.84076170677587], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.6137061620541792, 0.6356567368100985, 0.0, 1.0, 0.5, 0.23840761706775868], 
reward next is 0.7616, 
noisyNet noise sample is [array([0.62509996], dtype=float32), -1.3613834]. 
=============================================
[2019-04-09 15:06:24,220] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.57936262e-04 6.53109420e-03 3.67334299e-02 2.03858668e-04
 6.47955179e-01 7.45584369e-02 1.59941241e-01 1.19226305e-02
 4.28216271e-02 1.47241009e-02 4.05051187e-03], sum to 1.0000
[2019-04-09 15:06:24,220] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6429
[2019-04-09 15:06:24,257] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 19.0, 23.61938592058149, 0.02971114076625963, 0.0, 1.0, 35.0, 17.19609082281222], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4152000.0000, 
sim time next is 4153200.0000, 
raw observation next is [-1.666666666666667, 43.66666666666666, 0.0, 0.0, 19.0, 23.49695477274001, -0.003225149168823077, 0.0, 1.0, 35.0, 17.267319948251654], 
processed observation next is [0.0, 0.043478260869565216, 0.4164358264081256, 0.4366666666666666, 0.0, 0.0, 0.08333333333333333, 0.4580795643950009, 0.49892495027705897, 0.0, 1.0, 0.4, 0.17267319948251653], 
reward next is 0.8273, 
noisyNet noise sample is [array([-0.6865732], dtype=float32), -0.16837497]. 
=============================================
[2019-04-09 15:06:24,512] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00308509 0.01210143 0.05240873 0.00108472 0.325727   0.16549937
 0.31101105 0.01930154 0.05551249 0.04214956 0.01211907], sum to 1.0000
[2019-04-09 15:06:24,513] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7356
[2019-04-09 15:06:24,544] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 52.66666666666667, 0.0, 0.0, 19.0, 23.09088395294789, -0.08727966600759544, 0.0, 1.0, 35.0, 25.58180509772682], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4166400.0000, 
sim time next is 4167600.0000, 
raw observation next is [-4.0, 51.33333333333333, 0.0, 0.0, 19.0, 23.01589438399994, -0.1021104426322251, 0.0, 1.0, 45.0, 32.62221412822234], 
processed observation next is [0.0, 0.21739130434782608, 0.3518005540166205, 0.5133333333333333, 0.0, 0.0, 0.08333333333333333, 0.41799119866666157, 0.4659631857892583, 0.0, 1.0, 0.6, 0.3262221412822234], 
reward next is 0.6738, 
noisyNet noise sample is [array([1.2571692], dtype=float32), -0.33756977]. 
=============================================
[2019-04-09 15:06:24,755] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 55033: loss 15.9150
[2019-04-09 15:06:24,756] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 55033: learning rate 0.0000
[2019-04-09 15:06:24,875] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.4953932e-05 2.2038016e-03 1.5895020e-02 2.4659596e-05 4.1529602e-01
 1.1524065e-01 3.7991345e-01 4.7367765e-03 4.0556766e-02 2.3052234e-02
 3.0056941e-03], sum to 1.0000
[2019-04-09 15:06:24,876] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9624
[2019-04-09 15:06:25,003] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.0, 26.0, 109.0, 812.0, 22.5, 26.70495740828549, 0.6166779215649566, 1.0, 1.0, 35.0, 26.293415755619975], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4024800.0000, 
sim time next is 4026000.0000, 
raw observation next is [-2.666666666666667, 24.0, 105.6666666666667, 800.0, 22.5, 26.84087286541454, 0.6024832777557529, 1.0, 1.0, 55.0, 60.19920243101133], 
processed observation next is [1.0, 0.6086956521739131, 0.38873499538319484, 0.24, 0.3522222222222223, 0.8839779005524862, 0.375, 0.7367394054512116, 0.7008277592519176, 1.0, 1.0, 0.8, 0.6019920243101133], 
reward next is 0.3980, 
noisyNet noise sample is [array([0.12731943], dtype=float32), 1.5508131]. 
=============================================
[2019-04-09 15:06:25,006] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[18.498043]
 [18.199053]
 [18.172949]
 [17.961578]
 [17.496912]], R is [[18.8850708 ]
 [19.43328667]
 [19.91927147]
 [20.26032829]
 [20.48978806]].
[2019-04-09 15:06:25,453] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00123361 0.00945352 0.0308222  0.00071503 0.55536294 0.13997452
 0.16453308 0.01126857 0.04515092 0.02628859 0.015197  ], sum to 1.0000
[2019-04-09 15:06:25,455] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7422
[2019-04-09 15:06:25,513] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 52.33333333333334, 15.33333333333333, 81.33333333333331, 19.0, 22.83054082174868, -0.1984026798707922, 0.0, 1.0, 35.0, 20.81818305876067], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4174800.0000, 
sim time next is 4176000.0000, 
raw observation next is [-5.0, 54.0, 46.0, 244.0, 19.0, 22.65667717614998, -0.1807444315389023, 0.0, 1.0, 45.0, 33.378142969400045], 
processed observation next is [0.0, 0.34782608695652173, 0.32409972299168976, 0.54, 0.15333333333333332, 0.2696132596685083, 0.08333333333333333, 0.3880564313458317, 0.43975185615369927, 0.0, 1.0, 0.6, 0.33378142969400043], 
reward next is 0.6662, 
noisyNet noise sample is [array([0.7136687], dtype=float32), 1.141583]. 
=============================================
[2019-04-09 15:06:25,548] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[11.631452 ]
 [11.571331 ]
 [11.5132675]
 [12.031485 ]
 [11.840073 ]], R is [[12.11841965]
 [12.78905296]
 [13.43367767]
 [14.05035686]
 [14.63674831]].
[2019-04-09 15:06:25,642] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00126516 0.01294287 0.02545956 0.00061416 0.60054153 0.06908405
 0.20306519 0.01958099 0.03860965 0.02381161 0.00502531], sum to 1.0000
[2019-04-09 15:06:25,643] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1513
[2019-04-09 15:06:25,709] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 50.66666666666667, 0.0, 0.0, 19.0, 22.95876932104162, -0.1328036222977896, 0.0, 1.0, 35.0, 26.438670242253544], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4173600.0000, 
sim time next is 4174800.0000, 
raw observation next is [-5.0, 52.33333333333334, 15.33333333333333, 81.33333333333331, 19.0, 22.85474862875564, -0.1567953065136195, 0.0, 1.0, 35.0, 23.592625812396644], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.5233333333333334, 0.0511111111111111, 0.0898710865561694, 0.08333333333333333, 0.4045623857296367, 0.44773489782879344, 0.0, 1.0, 0.4, 0.23592625812396645], 
reward next is 0.7641, 
noisyNet noise sample is [array([0.49373513], dtype=float32), 0.6550394]. 
=============================================
[2019-04-09 15:06:25,734] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6873033e-05 3.1007107e-03 1.2953716e-02 4.9494643e-05 5.8708674e-01
 8.8413611e-02 2.5636464e-01 5.2798791e-03 3.0507572e-02 1.1552286e-02
 4.6544783e-03], sum to 1.0000
[2019-04-09 15:06:25,734] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1713
[2019-04-09 15:06:25,769] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.333333333333333, 32.33333333333334, 107.6666666666667, 800.0, 22.5, 26.81655975490175, 0.7378901786014301, 1.0, 1.0, 55.0, 41.923090398777816], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4112400.0000, 
sim time next is 4113600.0000, 
raw observation next is [3.666666666666667, 33.66666666666666, 104.0, 777.5, 22.5, 27.10369611943952, 0.7825372514243095, 1.0, 1.0, 35.0, 25.132496776676142], 
processed observation next is [1.0, 0.6086956521739131, 0.564173591874423, 0.33666666666666656, 0.3466666666666667, 0.8591160220994475, 0.375, 0.7586413432866266, 0.7608457504747698, 1.0, 1.0, 0.4, 0.25132496776676144], 
reward next is 0.7487, 
noisyNet noise sample is [array([-0.23363732], dtype=float32), -0.46208742]. 
=============================================
[2019-04-09 15:06:25,800] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.1749516e-04 1.3925657e-02 2.8442889e-02 7.6674350e-04 6.7124581e-01
 8.1641555e-02 1.3345633e-01 9.9253561e-03 2.4311695e-02 2.6149083e-02
 9.5173763e-03], sum to 1.0000
[2019-04-09 15:06:25,800] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6955
[2019-04-09 15:06:25,874] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 45.0, 100.0, 574.0, 19.0, 22.71716103213713, -0.1328246354518973, 0.0, 1.0, 45.0, 31.90941548545681], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4179600.0000, 
sim time next is 4180800.0000, 
raw observation next is [-3.333333333333333, 41.66666666666667, 105.3333333333333, 631.3333333333333, 19.0, 22.7802999568268, -0.1089788616771483, 0.0, 1.0, 35.0, 24.777348534281117], 
processed observation next is [0.0, 0.391304347826087, 0.37026777469990774, 0.41666666666666674, 0.351111111111111, 0.6976058931860036, 0.08333333333333333, 0.39835832973556656, 0.4636737127742839, 0.0, 1.0, 0.4, 0.24777348534281118], 
reward next is 0.7522, 
noisyNet noise sample is [array([0.30047634], dtype=float32), -0.437247]. 
=============================================
[2019-04-09 15:06:26,169] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4130269e-04 8.1048561e-03 1.2830174e-02 2.4889613e-04 7.7477932e-01
 6.8122767e-02 8.4108680e-02 5.0681806e-03 2.0829240e-02 2.1175966e-02
 4.2905998e-03], sum to 1.0000
[2019-04-09 15:06:26,171] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5689
[2019-04-09 15:06:26,230] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.333333333333333, 31.33333333333334, 118.1666666666667, 842.8333333333334, 19.0, 23.45626802047565, 0.1010250868099744, 0.0, 1.0, 55.0, 46.48970779738906], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4191600.0000, 
sim time next is 4192800.0000, 
raw observation next is [1.666666666666667, 32.66666666666666, 134.0, 817.3333333333334, 19.0, 23.93926359523637, 0.1692281102715432, 0.0, 1.0, 35.0, 26.918622800817246], 
processed observation next is [0.0, 0.5217391304347826, 0.5087719298245615, 0.32666666666666655, 0.44666666666666666, 0.9031307550644567, 0.08333333333333333, 0.4949386329363641, 0.5564093700905144, 0.0, 1.0, 0.4, 0.2691862280081725], 
reward next is 0.7308, 
noisyNet noise sample is [array([0.38560522], dtype=float32), -0.5234054]. 
=============================================
[2019-04-09 15:06:26,233] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4915813e-04 8.5926158e-03 1.3681252e-02 2.6382165e-04 7.6897329e-01
 7.2285183e-02 8.4023885e-02 5.2059200e-03 2.0926546e-02 2.0967077e-02
 4.6312129e-03], sum to 1.0000
[2019-04-09 15:06:26,233] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7487
[2019-04-09 15:06:26,236] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55398: loss 28.1646
[2019-04-09 15:06:26,237] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55398: learning rate 0.0000
[2019-04-09 15:06:26,252] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3500, global step 55402: loss 10.9762
[2019-04-09 15:06:26,253] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3500, global step 55402: learning rate 0.0000
[2019-04-09 15:06:26,284] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.666666666666667, 32.66666666666666, 134.0, 817.3333333333334, 19.0, 23.93926359523637, 0.1692281102715432, 0.0, 1.0, 35.0, 26.918622800817246], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4192800.0000, 
sim time next is 4194000.0000, 
raw observation next is [2.0, 34.0, 166.0, 758.0, 19.0, 24.20421163688357, 0.1967620305702201, 0.0, 1.0, 35.0, 22.147824928969712], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.34, 0.5533333333333333, 0.8375690607734807, 0.08333333333333333, 0.5170176364069642, 0.5655873435234067, 0.0, 1.0, 0.4, 0.22147824928969712], 
reward next is 0.7785, 
noisyNet noise sample is [array([0.38560522], dtype=float32), -0.5234054]. 
=============================================
[2019-04-09 15:06:26,304] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[12.212753]
 [12.291093]
 [11.900353]
 [11.87691 ]
 [11.750275]], R is [[12.99912548]
 [13.59994793]
 [13.99905109]
 [14.7132864 ]
 [15.39888382]].
[2019-04-09 15:06:26,315] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.5736194e-04 7.1978783e-03 1.9750016e-02 5.5668206e-04 5.8555579e-01
 1.2515438e-01 1.5550159e-01 8.5853552e-03 5.2774329e-02 3.2909535e-02
 1.1257067e-02], sum to 1.0000
[2019-04-09 15:06:26,316] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2635
[2019-04-09 15:06:26,377] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 35.0, 113.0, 755.0, 19.0, 23.58802863325105, 0.02369040926114624, 0.0, 1.0, 35.0, 23.838790263153157], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4184400.0000, 
sim time next is 4185600.0000, 
raw observation next is [-1.333333333333333, 35.0, 114.8333333333333, 782.0, 19.0, 23.57868084904258, 0.01583201379023128, 0.0, 1.0, 35.0, 20.810499722318788], 
processed observation next is [0.0, 0.43478260869565216, 0.42566943674976926, 0.35, 0.38277777777777766, 0.8640883977900552, 0.08333333333333333, 0.4648900707535484, 0.5052773379300771, 0.0, 1.0, 0.4, 0.20810499722318787], 
reward next is 0.7919, 
noisyNet noise sample is [array([-1.3545896], dtype=float32), 0.21361054]. 
=============================================
[2019-04-09 15:06:26,674] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 55502: loss 17.4636
[2019-04-09 15:06:26,674] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 55502: learning rate 0.0000
[2019-04-09 15:06:26,882] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55562: loss 18.0002
[2019-04-09 15:06:26,883] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55562: learning rate 0.0000
[2019-04-09 15:06:26,959] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.0922528e-04 2.6527683e-03 8.7934965e-03 1.9804649e-04 8.2320952e-01
 4.4514105e-02 8.6709753e-02 6.4649279e-03 1.6389431e-02 6.3069584e-03
 4.5517692e-03], sum to 1.0000
[2019-04-09 15:06:26,960] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5082
[2019-04-09 15:06:26,985] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 55585: loss 11.7543
[2019-04-09 15:06:26,986] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 55586: learning rate 0.0000
[2019-04-09 15:06:27,032] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.39038408e-04 3.98200750e-03 1.29393237e-02 1.49467131e-04
 7.57284939e-01 5.77168651e-02 1.16691224e-01 4.28218907e-03
 1.55468080e-02 2.52198633e-02 5.84830996e-03], sum to 1.0000
[2019-04-09 15:06:27,032] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1276
[2019-04-09 15:06:27,063] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.666666666666667, 39.33333333333333, 144.6666666666667, 540.0, 19.0, 23.92904496253578, 0.1246593138002893, 0.0, 1.0, 35.0, 12.795907944700996], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4203600.0000, 
sim time next is 4204800.0000, 
raw observation next is [3.0, 37.0, 114.0, 544.0, 19.0, 23.93351206018287, 0.1305821647093574, 0.0, 1.0, 35.0, 15.36487577764149], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.37, 0.38, 0.6011049723756906, 0.08333333333333333, 0.49445933834857253, 0.5435273882364525, 0.0, 1.0, 0.4, 0.1536487577764149], 
reward next is 0.8464, 
noisyNet noise sample is [array([-1.3243723], dtype=float32), -0.9133413]. 
=============================================
[2019-04-09 15:06:27,068] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.5, 41.66666666666667, 0.0, 0.0, 19.0, 24.95592304840823, 0.2294957363500745, 0.0, 1.0, 35.0, 19.528036891639395], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4214400.0000, 
sim time next is 4215600.0000, 
raw observation next is [1.4, 42.0, 0.0, 0.0, 19.0, 24.7589839914373, 0.1922679353307274, 0.0, 1.0, 35.0, 18.052513602892866], 
processed observation next is [0.0, 0.8260869565217391, 0.5013850415512465, 0.42, 0.0, 0.0, 0.08333333333333333, 0.5632486659531084, 0.5640893117769091, 0.0, 1.0, 0.4, 0.18052513602892867], 
reward next is 0.8195, 
noisyNet noise sample is [array([0.04294484], dtype=float32), -0.66843307]. 
=============================================
[2019-04-09 15:06:27,196] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 55640: loss 13.1602
[2019-04-09 15:06:27,196] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 55640: learning rate 0.0000
[2019-04-09 15:06:27,300] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 55664: loss 12.5441
[2019-04-09 15:06:27,301] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 55664: learning rate 0.0000
[2019-04-09 15:06:27,660] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55752: loss 17.1177
[2019-04-09 15:06:27,663] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55752: learning rate 0.0000
[2019-04-09 15:06:27,824] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55796: loss 18.3416
[2019-04-09 15:06:27,824] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55796: learning rate 0.0000
[2019-04-09 15:06:28,212] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.8263356e-04 7.2104093e-03 3.8675841e-02 4.3936382e-04 6.3429856e-01
 8.4268644e-02 1.6590382e-01 1.2151178e-02 3.2819077e-02 1.8520588e-02
 5.2298671e-03], sum to 1.0000
[2019-04-09 15:06:28,212] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8369
[2019-04-09 15:06:28,251] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 19.0, 23.50780819488609, -0.06524580935364273, 0.0, 1.0, 25.0, 25.854204197663073], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4237200.0000, 
sim time next is 4238400.0000, 
raw observation next is [2.333333333333333, 47.0, 0.0, 0.0, 19.0, 23.47898157066342, -0.06695738141087844, 0.0, 1.0, 45.0, 32.91712358205862], 
processed observation next is [0.0, 0.043478260869565216, 0.5272391505078486, 0.47, 0.0, 0.0, 0.08333333333333333, 0.4565817975552851, 0.47768087286304056, 0.0, 1.0, 0.6, 0.3291712358205862], 
reward next is 0.6708, 
noisyNet noise sample is [array([-0.57094955], dtype=float32), 1.6302823]. 
=============================================
[2019-04-09 15:06:28,500] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.8207344e-06 5.0861144e-04 8.1242984e-03 9.1375123e-06 7.5882620e-01
 7.1084581e-02 1.3535970e-01 1.3687336e-03 1.8225079e-02 5.1688063e-03
 1.3160479e-03], sum to 1.0000
[2019-04-09 15:06:28,501] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4115
[2019-04-09 15:06:28,550] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 35.0, 0.0, 0.0, 22.5, 26.97637931327661, 0.6179182898407222, 1.0, 1.0, 35.0, 14.432632059531915], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4126800.0000, 
sim time next is 4128000.0000, 
raw observation next is [3.0, 36.0, 0.0, 0.0, 22.5, 24.75271680994643, 0.4456172843505259, 1.0, 1.0, 35.0, 14.08138174249262], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.36, 0.0, 0.0, 0.375, 0.5627264008288693, 0.6485390947835087, 1.0, 1.0, 0.4, 0.1408138174249262], 
reward next is 0.8592, 
noisyNet noise sample is [array([0.43475088], dtype=float32), -0.14264992]. 
=============================================
[2019-04-09 15:06:28,558] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[20.173529]
 [20.39398 ]
 [20.40425 ]
 [20.497736]
 [20.546589]], R is [[20.71335602]
 [21.36189461]
 [22.00020409]
 [22.60828781]
 [23.18425941]].
[2019-04-09 15:06:28,861] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0009654  0.00814538 0.02758824 0.00060606 0.57730925 0.0721565
 0.21574272 0.00956659 0.05559691 0.02482895 0.00749398], sum to 1.0000
[2019-04-09 15:06:28,933] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8845
[2019-04-09 15:06:29,015] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 23.23784926188447, -0.1151304294171028, 0.0, 1.0, 35.0, 20.001141724489422], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4242000.0000, 
sim time next is 4243200.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 23.16413099881744, -0.09431262053707119, 0.0, 1.0, 45.0, 36.4344181116638], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.4303442499014534, 0.4685624598209763, 0.0, 1.0, 0.6, 0.364344181116638], 
reward next is 0.6357, 
noisyNet noise sample is [array([-0.2522147], dtype=float32), 1.1056305]. 
=============================================
[2019-04-09 15:06:29,279] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 56160: loss 30.3910
[2019-04-09 15:06:29,282] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 56161: learning rate 0.0000
[2019-04-09 15:06:29,425] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4477733e-04 4.5249402e-03 1.2362196e-02 1.7970617e-04 7.7178931e-01
 7.0105545e-02 9.4004512e-02 6.6309487e-03 2.3547921e-02 1.0198788e-02
 6.4113620e-03], sum to 1.0000
[2019-04-09 15:06:29,425] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0377
[2019-04-09 15:06:29,482] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 19.0, 23.29323933419244, -0.1266307002670774, 0.0, 1.0, 35.0, 20.584925981863307], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4233600.0000, 
sim time next is 4234800.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 19.0, 23.1625471264302, -0.1580765292384166, 0.0, 1.0, 35.0, 18.921712129897134], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.08333333333333333, 0.4302122605358501, 0.4473078235871945, 0.0, 1.0, 0.4, 0.18921712129897134], 
reward next is 0.8108, 
noisyNet noise sample is [array([0.00469058], dtype=float32), -1.9558407]. 
=============================================
[2019-04-09 15:06:29,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.2623434e-04 7.8605842e-03 4.2603098e-02 3.8065348e-04 4.8472521e-01
 8.8430889e-02 2.7190673e-01 1.5707996e-02 6.1923627e-02 1.8459108e-02
 7.3758541e-03], sum to 1.0000
[2019-04-09 15:06:29,705] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2981
[2019-04-09 15:06:29,743] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 19.0, 23.05880216831778, -0.1408745070900131, 0.0, 1.0, 45.0, 34.43258185635613], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4237200.0000, 
sim time next is 4238400.0000, 
raw observation next is [2.333333333333333, 47.0, 0.0, 0.0, 19.0, 23.0965009586201, -0.1474972134214136, 0.0, 1.0, 35.0, 26.03215870840922], 
processed observation next is [0.0, 0.043478260869565216, 0.5272391505078486, 0.47, 0.0, 0.0, 0.08333333333333333, 0.4247084132183418, 0.4508342621928621, 0.0, 1.0, 0.4, 0.2603215870840922], 
reward next is 0.7397, 
noisyNet noise sample is [array([0.6242975], dtype=float32), 0.35314667]. 
=============================================
[2019-04-09 15:06:29,907] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5313542e-04 2.6203892e-03 9.4676409e-03 7.2491297e-05 6.7468619e-01
 5.9376758e-02 2.1390818e-01 2.3917316e-03 2.0621791e-02 1.4015571e-02
 2.6861024e-03], sum to 1.0000
[2019-04-09 15:06:29,908] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6413
[2019-04-09 15:06:29,974] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.6, 60.0, 47.0, 392.0, 19.0, 25.61998782083788, 0.4773610860138512, 0.0, 1.0, 35.0, 21.191021722554893], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4294800.0000, 
sim time next is 4296000.0000, 
raw observation next is [6.466666666666667, 61.33333333333334, 31.66666666666666, 282.6666666666666, 19.0, 25.6743544475396, 0.4605583151263497, 0.0, 1.0, 35.0, 18.388458093495373], 
processed observation next is [0.0, 0.7391304347826086, 0.6417359187442291, 0.6133333333333334, 0.10555555555555554, 0.3123388581952117, 0.08333333333333333, 0.6395295372949666, 0.6535194383754499, 0.0, 1.0, 0.4, 0.18388458093495375], 
reward next is 0.8161, 
noisyNet noise sample is [array([0.06394329], dtype=float32), 0.47130272]. 
=============================================
[2019-04-09 15:06:29,995] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[15.223829 ]
 [15.3339615]
 [15.117436 ]
 [15.006344 ]
 [15.056253 ]], R is [[15.90703297]
 [16.5360527 ]
 [17.08539391]
 [17.78205299]
 [18.45854378]].
[2019-04-09 15:06:30,100] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56388: loss 22.4138
[2019-04-09 15:06:30,103] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56389: learning rate 0.0000
[2019-04-09 15:06:30,139] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8256112e-04 2.8605487e-03 2.7661312e-02 2.4771821e-04 6.3434374e-01
 7.2613306e-02 1.9641876e-01 6.4541558e-03 2.7626052e-02 2.8122684e-02
 3.2691322e-03], sum to 1.0000
[2019-04-09 15:06:30,140] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7492
[2019-04-09 15:06:30,192] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 40.0, 200.5, 379.0, 19.0, 24.60055141808611, 0.3468107409163957, 0.0, 1.0, 60.0, 50.00182742238733], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4197600.0000, 
sim time next is 4198800.0000, 
raw observation next is [2.0, 41.33333333333334, 191.5, 185.6666666666666, 19.0, 25.11351175794575, 0.3851840018653334, 0.0, 1.0, 45.0, 31.826342351378404], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.41333333333333344, 0.6383333333333333, 0.20515653775322276, 0.08333333333333333, 0.5927926464954792, 0.6283946672884445, 0.0, 1.0, 0.6, 0.31826342351378406], 
reward next is 0.6817, 
noisyNet noise sample is [array([1.2372593], dtype=float32), 0.6110171]. 
=============================================
[2019-04-09 15:06:30,266] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 56435: loss 20.9888
[2019-04-09 15:06:30,268] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 56435: learning rate 0.0000
[2019-04-09 15:06:31,226] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9596708e-04 6.1875945e-03 1.4262574e-02 1.0226071e-04 7.1102220e-01
 9.9374391e-02 1.2929021e-01 5.2361432e-03 1.5682811e-02 1.1442798e-02
 7.2030588e-03], sum to 1.0000
[2019-04-09 15:06:31,230] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1309
[2019-04-09 15:06:31,309] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 19.0, 24.7728715621996, 0.2295574005018252, 0.0, 1.0, 35.0, 22.78496091457993], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4220400.0000, 
sim time next is 4221600.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 19.0, 24.63422738244618, 0.1945668300763983, 0.0, 1.0, 35.0, 20.42894771631012], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.08333333333333333, 0.552852281870515, 0.5648556100254661, 0.0, 1.0, 0.4, 0.2042894771631012], 
reward next is 0.7957, 
noisyNet noise sample is [array([-0.8663466], dtype=float32), 0.1796473]. 
=============================================
[2019-04-09 15:06:31,682] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7318547e-04 1.1188404e-02 3.8789645e-02 4.4827137e-04 5.7455838e-01
 1.0678983e-01 1.8790463e-01 1.4947040e-02 3.5560448e-02 2.2878526e-02
 6.4616092e-03], sum to 1.0000
[2019-04-09 15:06:31,682] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1001
[2019-04-09 15:06:31,716] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.666666666666666, 54.66666666666667, 190.1666666666667, 640.3333333333334, 19.0, 23.24602148548136, -0.0771613885646928, 0.0, 1.0, 35.0, 18.68274919715395], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4272000.0000, 
sim time next is 4273200.0000, 
raw observation next is [5.0, 55.0, 162.5, 713.0, 19.0, 23.25559710176643, -0.06854117568377599, 0.0, 1.0, 35.0, 16.12869239431155], 
processed observation next is [0.0, 0.4782608695652174, 0.6011080332409973, 0.55, 0.5416666666666666, 0.7878453038674034, 0.08333333333333333, 0.43796642514720246, 0.4771529414387414, 0.0, 1.0, 0.4, 0.1612869239431155], 
reward next is 0.8387, 
noisyNet noise sample is [array([-0.89081454], dtype=float32), 0.19224401]. 
=============================================
[2019-04-09 15:06:31,884] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6767782e-04 9.5473332e-03 2.1694668e-02 2.1037059e-04 7.4369037e-01
 6.1442964e-02 9.4328374e-02 7.2590834e-03 2.9964583e-02 2.6970131e-02
 4.6244212e-03], sum to 1.0000
[2019-04-09 15:06:31,884] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4721
[2019-04-09 15:06:31,975] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 19.0, 23.71548790763225, -0.005264026274062708, 0.0, 1.0, 35.0, 34.55859261454651], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4237200.0000, 
sim time next is 4238400.0000, 
raw observation next is [2.333333333333333, 47.0, 0.0, 0.0, 19.0, 23.84975179396002, -0.009543176460641414, 0.0, 1.0, 35.0, 30.998359960354968], 
processed observation next is [0.0, 0.043478260869565216, 0.5272391505078486, 0.47, 0.0, 0.0, 0.08333333333333333, 0.48747931616333506, 0.49681894117978614, 0.0, 1.0, 0.4, 0.30998359960354965], 
reward next is 0.6900, 
noisyNet noise sample is [array([-0.98814315], dtype=float32), -0.2229063]. 
=============================================
[2019-04-09 15:06:32,138] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7102352e-04 4.3452857e-03 1.4700243e-02 1.3723492e-04 6.8546146e-01
 5.8124281e-02 1.7664817e-01 5.3132982e-03 3.2839585e-02 1.5095231e-02
 6.9641392e-03], sum to 1.0000
[2019-04-09 15:06:32,138] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3692
[2019-04-09 15:06:32,188] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 39.0, 0.0, 0.0, 19.0, 23.94417268096552, 0.1137086930900343, 0.0, 1.0, 35.0, 20.71070313022474], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4150800.0000, 
sim time next is 4152000.0000, 
raw observation next is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 19.0, 23.83257551541954, 0.07814111667683692, 0.0, 1.0, 35.0, 18.903065948253598], 
processed observation next is [0.0, 0.043478260869565216, 0.42566943674976926, 0.41333333333333344, 0.0, 0.0, 0.08333333333333333, 0.4860479596182949, 0.526047038892279, 0.0, 1.0, 0.4, 0.18903065948253597], 
reward next is 0.8110, 
noisyNet noise sample is [array([0.25870365], dtype=float32), -0.06975227]. 
=============================================
[2019-04-09 15:06:32,242] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[15.213988]
 [16.034721]
 [16.85239 ]
 [17.170792]
 [18.048449]], R is [[15.70119667]
 [16.33707809]
 [16.93894958]
 [17.44867134]
 [18.11967278]].
[2019-04-09 15:06:32,541] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.48847670e-04 2.64999783e-03 1.24855787e-02 6.99362281e-05
 5.78633964e-01 1.16523825e-01 2.46112853e-01 1.98603095e-03
 3.48859690e-02 4.17425949e-03 2.32871785e-03], sum to 1.0000
[2019-04-09 15:06:32,556] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7787
[2019-04-09 15:06:32,579] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3500, global step 57043: loss 14.3811
[2019-04-09 15:06:32,580] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3500, global step 57043: learning rate 0.0000
[2019-04-09 15:06:32,597] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.4, 73.0, 0.0, 0.0, 19.0, 24.78298304648135, 0.2625579249051198, 0.0, 1.0, 40.0, 23.337340187716087], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4305600.0000, 
sim time next is 4306800.0000, 
raw observation next is [5.300000000000001, 73.0, 0.0, 0.0, 19.0, 24.81163480518966, 0.2699797717541334, 0.0, 1.0, 45.0, 30.070058312558153], 
processed observation next is [0.0, 0.8695652173913043, 0.6094182825484765, 0.73, 0.0, 0.0, 0.08333333333333333, 0.5676362337658049, 0.5899932572513779, 0.0, 1.0, 0.6, 0.3007005831255815], 
reward next is 0.6993, 
noisyNet noise sample is [array([-0.4705665], dtype=float32), 1.2609814]. 
=============================================
[2019-04-09 15:06:33,214] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 57217: loss 14.4863
[2019-04-09 15:06:33,216] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 57217: learning rate 0.0000
[2019-04-09 15:06:33,219] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4610933e-04 2.7849777e-03 1.7444743e-02 1.8842098e-04 7.7115059e-01
 7.9675689e-02 1.0075906e-01 4.1434569e-03 1.3539903e-02 7.4766567e-03
 2.4903184e-03], sum to 1.0000
[2019-04-09 15:06:33,219] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6494
[2019-04-09 15:06:33,295] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 42.66666666666667, 182.5, 163.8333333333333, 19.0, 23.94547535696501, 0.07621734613355657, 0.0, 1.0, 35.0, 19.6419027462745], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4200000.0000, 
sim time next is 4201200.0000, 
raw observation next is [2.0, 44.0, 173.5, 313.5, 19.0, 23.84221516791749, 0.07465799732851623, 0.0, 1.0, 35.0, 17.81468134556591], 
processed observation next is [0.0, 0.6521739130434783, 0.518005540166205, 0.44, 0.5783333333333334, 0.34640883977900555, 0.08333333333333333, 0.48685126399312423, 0.5248859991095054, 0.0, 1.0, 0.4, 0.17814681345565908], 
reward next is 0.8219, 
noisyNet noise sample is [array([0.17375503], dtype=float32), 0.5484608]. 
=============================================
[2019-04-09 15:06:33,493] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2840127e-04 2.1809801e-03 1.7356969e-02 7.9124446e-05 7.3553038e-01
 4.8520070e-02 1.6250280e-01 4.8972028e-03 1.6427470e-02 1.1188747e-02
 1.1878280e-03], sum to 1.0000
[2019-04-09 15:06:33,494] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1493
[2019-04-09 15:06:33,533] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.1, 73.0, 0.0, 0.0, 19.0, 24.61243379901191, 0.2072734153407757, 0.0, 1.0, 45.0, 29.95575567831271], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4309200.0000, 
sim time next is 4310400.0000, 
raw observation next is [5.0, 74.0, 0.0, 0.0, 19.0, 24.58260977830876, 0.195766110817205, 0.0, 1.0, 35.0, 23.376562328492085], 
processed observation next is [0.0, 0.9130434782608695, 0.6011080332409973, 0.74, 0.0, 0.0, 0.08333333333333333, 0.5485508148590634, 0.5652553702724017, 0.0, 1.0, 0.4, 0.23376562328492084], 
reward next is 0.7662, 
noisyNet noise sample is [array([-0.4309147], dtype=float32), 0.7521116]. 
=============================================
[2019-04-09 15:06:33,843] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8383740e-04 1.8463130e-03 1.4013558e-02 9.7838223e-05 6.3145745e-01
 1.2123264e-01 1.8189651e-01 4.7464841e-03 1.8532580e-02 2.4763579e-02
 1.2291645e-03], sum to 1.0000
[2019-04-09 15:06:33,856] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9653
[2019-04-09 15:06:33,949] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [6.6, 60.0, 47.0, 392.0, 19.0, 25.7563591098701, 0.4836926496784077, 0.0, 1.0, 35.0, 22.962168442797676], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4294800.0000, 
sim time next is 4296000.0000, 
raw observation next is [6.466666666666667, 61.33333333333334, 31.66666666666666, 282.6666666666666, 19.0, 25.81867000984852, 0.5264600770977232, 0.0, 1.0, 55.0, 45.24461449904307], 
processed observation next is [0.0, 0.7391304347826086, 0.6417359187442291, 0.6133333333333334, 0.10555555555555554, 0.3123388581952117, 0.08333333333333333, 0.6515558341540434, 0.6754866923659076, 0.0, 1.0, 0.8, 0.4524461449904307], 
reward next is 0.5476, 
noisyNet noise sample is [array([0.00449326], dtype=float32), -1.3778933]. 
=============================================
[2019-04-09 15:06:33,997] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3983879e-04 8.1022205e-03 2.1875707e-02 4.0165719e-04 6.2134737e-01
 9.7140595e-02 1.6636053e-01 1.4068934e-02 4.3506626e-02 2.0252654e-02
 6.6038365e-03], sum to 1.0000
[2019-04-09 15:06:33,998] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8322
[2019-04-09 15:06:34,036] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[16.95222 ]
 [16.98744 ]
 [16.959059]
 [16.747053]
 [16.846546]], R is [[17.81092072]
 [18.40319061]
 [18.9581337 ]
 [19.36007881]
 [20.01782227]].
[2019-04-09 15:06:34,060] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 35.0, 113.0, 755.0, 19.0, 22.95005051424911, -0.004313321601932076, 0.0, 1.0, 55.0, 47.042269676742634], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4184400.0000, 
sim time next is 4185600.0000, 
raw observation next is [-1.333333333333333, 35.0, 114.8333333333333, 782.0, 19.0, 23.48641044109814, 0.06318966974534157, 0.0, 1.0, 45.0, 30.22748880997473], 
processed observation next is [0.0, 0.43478260869565216, 0.42566943674976926, 0.35, 0.38277777777777766, 0.8640883977900552, 0.08333333333333333, 0.4572008700915117, 0.5210632232484472, 0.0, 1.0, 0.6, 0.3022748880997473], 
reward next is 0.6977, 
noisyNet noise sample is [array([-0.66276336], dtype=float32), -1.3567251]. 
=============================================
[2019-04-09 15:06:34,523] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.5297412e-05 2.0949410e-03 1.9892149e-02 1.3027989e-04 6.9987851e-01
 6.3091107e-02 1.7706667e-01 4.9185622e-03 2.2399999e-02 7.3819505e-03
 3.0605954e-03], sum to 1.0000
[2019-04-09 15:06:34,523] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2347
[2019-04-09 15:06:34,588] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 52.0, 120.1666666666667, 842.8333333333334, 19.0, 24.14022974470758, 0.1840412574398864, 0.0, 1.0, 45.0, 27.964312231041134], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4278000.0000, 
sim time next is 4279200.0000, 
raw observation next is [7.0, 52.0, 131.3333333333333, 825.3333333333334, 19.0, 24.31453214471497, 0.2245688763792642, 0.0, 1.0, 35.0, 20.09837888892168], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.4377777777777776, 0.9119705340699816, 0.08333333333333333, 0.5262110120595809, 0.5748562921264214, 0.0, 1.0, 0.4, 0.2009837888892168], 
reward next is 0.7990, 
noisyNet noise sample is [array([-0.99384385], dtype=float32), -2.3287482]. 
=============================================
[2019-04-09 15:06:35,019] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 57623: loss 18.2300
[2019-04-09 15:06:35,020] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 57623: learning rate 0.0000
[2019-04-09 15:06:36,037] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0746411e-03 1.2428863e-02 2.4537778e-02 4.6902747e-04 5.2097523e-01
 1.2206742e-01 2.2520697e-01 1.4110286e-02 5.2317128e-02 1.9886749e-02
 6.9259126e-03], sum to 1.0000
[2019-04-09 15:06:36,037] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8296
[2019-04-09 15:06:36,106] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 55.0, 26.5, 19.0, 22.36325076272824, -0.315267993951763, 0.0, 1.0, 35.0, 20.13013818492408], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4262400.0000, 
sim time next is 4263600.0000, 
raw observation next is [3.0, 50.33333333333334, 91.66666666666667, 44.16666666666667, 19.0, 22.32983585552805, -0.2746537771339592, 0.0, 1.0, 45.0, 37.46960992669343], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.5033333333333334, 0.3055555555555556, 0.04880294659300185, 0.08333333333333333, 0.36081965462733745, 0.40844874095534695, 0.0, 1.0, 0.6, 0.3746960992669343], 
reward next is 0.6253, 
noisyNet noise sample is [array([-0.2572985], dtype=float32), 1.3087544]. 
=============================================
[2019-04-09 15:06:36,236] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.5358433e-06 5.6137203e-04 4.7528208e-03 2.8299576e-06 7.5328034e-01
 7.4640714e-02 1.6062364e-01 5.2964088e-04 2.7675980e-03 1.9149395e-03
 9.2160492e-04], sum to 1.0000
[2019-04-09 15:06:36,261] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2177
[2019-04-09 15:06:36,358] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.1, 31.66666666666666, 179.6666666666667, 524.1666666666667, 22.5, 27.70964494215045, 1.043303115891215, 1.0, 1.0, 45.0, 7.871566394779622], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4372800.0000, 
sim time next is 4374000.0000, 
raw observation next is [13.9, 32.0, 149.0, 314.5, 22.5, 28.40250958875527, 1.085650909012452, 1.0, 1.0, 35.0, 7.453262778570844], 
processed observation next is [1.0, 0.6521739130434783, 0.847645429362881, 0.32, 0.49666666666666665, 0.3475138121546961, 0.375, 0.8668757990629391, 0.8618836363374841, 1.0, 1.0, 0.4, 0.07453262778570843], 
reward next is 0.9255, 
noisyNet noise sample is [array([-0.4806107], dtype=float32), 0.14254841]. 
=============================================
[2019-04-09 15:06:36,361] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[22.460192]
 [22.524305]
 [22.41293 ]
 [22.213825]
 [22.21973 ]], R is [[23.27523804]
 [23.96376991]
 [24.51047325]
 [25.15034294]
 [25.80357933]].
[2019-04-09 15:06:36,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.4109206e-06 4.6639855e-04 4.4706357e-03 1.6525229e-06 8.0459321e-01
 3.8013741e-02 1.3435721e-01 1.4438663e-03 1.2517622e-02 3.2817428e-03
 8.4554811e-04], sum to 1.0000
[2019-04-09 15:06:36,573] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3293
[2019-04-09 15:06:36,622] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.0, 58.0, 0.0, 0.0, 22.5, 27.59908126005643, 1.011955135307305, 0.0, 1.0, 35.0, 20.88840531325304], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4392000.0000, 
sim time next is 4393200.0000, 
raw observation next is [10.73333333333333, 58.33333333333334, 0.0, 0.0, 19.0, 27.57957342141546, 1.002603877270447, 0.0, 1.0, 35.0, 19.348680741955917], 
processed observation next is [1.0, 0.8695652173913043, 0.7599261311172669, 0.5833333333333335, 0.0, 0.0, 0.08333333333333333, 0.798297785117955, 0.8342012924234824, 0.0, 1.0, 0.4, 0.19348680741955918], 
reward next is 0.8065, 
noisyNet noise sample is [array([0.9206079], dtype=float32), -0.7252308]. 
=============================================
[2019-04-09 15:06:36,815] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.9599052e-04 6.6567636e-03 9.3042897e-03 1.5518919e-04 7.4163157e-01
 8.4089488e-02 1.1758353e-01 5.2935537e-03 2.0964887e-02 9.8865293e-03
 4.2382488e-03], sum to 1.0000
[2019-04-09 15:06:36,815] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6515
[2019-04-09 15:06:36,865] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.666666666666667, 54.0, 134.8333333333333, 785.6666666666666, 19.0, 23.58043462163509, 0.01116710884728019, 0.0, 1.0, 35.0, 19.957677429569458], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4274400.0000, 
sim time next is 4275600.0000, 
raw observation next is [6.333333333333333, 53.0, 120.8333333333333, 826.1666666666666, 19.0, 23.66509257958793, 0.02278689994179502, 0.0, 1.0, 35.0, 17.311376216263177], 
processed observation next is [0.0, 0.4782608695652174, 0.6380424746075716, 0.53, 0.4027777777777777, 0.9128913443830571, 0.08333333333333333, 0.47209104829899423, 0.5075956333139316, 0.0, 1.0, 0.4, 0.17311376216263177], 
reward next is 0.8269, 
noisyNet noise sample is [array([-2.2354443], dtype=float32), -0.7075525]. 
=============================================
[2019-04-09 15:06:37,328] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.8138750e-06 8.7998743e-04 8.8670999e-03 3.2863134e-06 5.5102861e-01
 1.7034866e-01 2.5046366e-01 1.0188289e-03 9.8783672e-03 5.8798045e-03
 1.6229165e-03], sum to 1.0000
[2019-04-09 15:06:37,328] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3717
[2019-04-09 15:06:37,392] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.73333333333333, 28.66666666666666, 117.5, 851.1666666666667, 22.5, 27.37538069604557, 0.7526823055257084, 1.0, 1.0, 35.0, 17.659196709973372], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4365600.0000, 
sim time next is 4366800.0000, 
raw observation next is [14.6, 29.0, 116.5, 847.5, 22.5, 26.36747193991586, 0.7736752309139097, 1.0, 1.0, 35.0, 15.067608442666135], 
processed observation next is [1.0, 0.5652173913043478, 0.8670360110803325, 0.29, 0.3883333333333333, 0.93646408839779, 0.375, 0.6972893283263216, 0.7578917436379699, 1.0, 1.0, 0.4, 0.15067608442666136], 
reward next is 0.8493, 
noisyNet noise sample is [array([0.69154334], dtype=float32), 0.014161273]. 
=============================================
[2019-04-09 15:06:37,438] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4255758e-05 1.0414177e-03 1.0530962e-02 9.8478067e-06 4.0221882e-01
 5.3784516e-02 5.1901853e-01 2.2983265e-03 9.1012828e-03 1.6502815e-03
 3.3181167e-04], sum to 1.0000
[2019-04-09 15:06:37,439] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6915
[2019-04-09 15:06:37,507] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.933333333333334, 59.66666666666667, 0.0, 0.0, 19.0, 27.64073552887918, 1.023500272377452, 0.0, 1.0, 45.0, 22.759238510153473], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4396800.0000, 
sim time next is 4398000.0000, 
raw observation next is [9.666666666666668, 60.33333333333333, 0.0, 0.0, 19.0, 27.60216620348175, 1.017185693582962, 0.0, 1.0, 45.0, 23.37167259176998], 
processed observation next is [1.0, 0.9130434782608695, 0.7303785780240075, 0.6033333333333333, 0.0, 0.0, 0.08333333333333333, 0.8001805169568126, 0.8390618978609874, 0.0, 1.0, 0.6, 0.2337167259176998], 
reward next is 0.7663, 
noisyNet noise sample is [array([1.1286132], dtype=float32), 0.69723594]. 
=============================================
[2019-04-09 15:06:37,576] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[21.677975]
 [21.56899 ]
 [21.820742]
 [22.121004]
 [22.053663]], R is [[22.03894806]
 [22.59096718]
 [23.1811657 ]
 [23.72525406]
 [24.30011177]].
[2019-04-09 15:06:38,887] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0696288e-04 1.3434031e-03 1.5774703e-02 5.3709686e-05 6.7485750e-01
 6.7537032e-02 2.0606297e-01 5.1053893e-03 1.4332229e-02 1.1302269e-02
 3.5237905e-03], sum to 1.0000
[2019-04-09 15:06:38,887] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8560
[2019-04-09 15:06:38,889] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.9361497e-05 1.6460364e-03 1.0905190e-02 3.0649575e-05 7.7490467e-01
 6.5809697e-02 1.1659269e-01 2.2880286e-03 2.1017980e-02 4.4993567e-03
 2.2563885e-03], sum to 1.0000
[2019-04-09 15:06:38,889] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5512
[2019-04-09 15:06:38,927] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.1, 66.0, 0.0, 0.0, 19.0, 26.15029016234091, 0.6733872523127666, 0.0, 1.0, 35.0, 23.074278724149885], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4413600.0000, 
sim time next is 4414800.0000, 
raw observation next is [5.733333333333334, 66.33333333333334, 0.0, 0.0, 19.0, 26.18646815295337, 0.6771082297169798, 0.0, 1.0, 45.0, 29.762173983274096], 
processed observation next is [1.0, 0.08695652173913043, 0.6214219759926132, 0.6633333333333334, 0.0, 0.0, 0.08333333333333333, 0.6822056794127809, 0.7257027432389932, 0.0, 1.0, 0.6, 0.297621739832741], 
reward next is 0.7024, 
noisyNet noise sample is [array([-0.12221742], dtype=float32), 0.7025139]. 
=============================================
[2019-04-09 15:06:38,942] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.066666666666666, 64.33333333333333, 0.0, 0.0, 19.0, 25.98753974204441, 0.6700283795199535, 0.0, 1.0, 35.0, 16.95382594248513], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4408800.0000, 
sim time next is 4410000.0000, 
raw observation next is [6.8, 65.0, 0.0, 0.0, 19.0, 26.02360611975904, 0.653790112607098, 0.0, 1.0, 35.0, 15.202857257640606], 
processed observation next is [1.0, 0.043478260869565216, 0.6509695290858727, 0.65, 0.0, 0.0, 0.08333333333333333, 0.6686338433132534, 0.7179300375356993, 0.0, 1.0, 0.4, 0.15202857257640606], 
reward next is 0.8480, 
noisyNet noise sample is [array([-0.30206013], dtype=float32), -0.015269186]. 
=============================================
[2019-04-09 15:06:39,020] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[19.295729]
 [19.891886]
 [20.362215]
 [20.628935]
 [20.803175]], R is [[19.72029305]
 [20.35355186]
 [20.92184258]
 [21.52486992]
 [22.17181587]].
[2019-04-09 15:06:39,120] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9120764e-04 2.7198629e-03 1.4038018e-02 4.1952062e-05 5.0276655e-01
 3.0237669e-01 1.1398138e-01 3.6052440e-03 4.1840643e-02 1.5811684e-02
 2.6267231e-03], sum to 1.0000
[2019-04-09 15:06:39,120] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9150
[2019-04-09 15:06:39,186] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.9, 75.0, 0.0, 0.0, 19.0, 24.7468905960162, 0.1815170294613717, 0.0, 1.0, 35.0, 29.22976945234879], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4345200.0000, 
sim time next is 4346400.0000, 
raw observation next is [2.933333333333334, 74.66666666666667, 0.0, 0.0, 22.5, 24.7232290989217, 0.1623245428796667, 0.0, 1.0, 45.0, 27.66316489364614], 
processed observation next is [1.0, 0.30434782608695654, 0.543859649122807, 0.7466666666666667, 0.0, 0.0, 0.375, 0.5602690915768083, 0.5541081809598889, 0.0, 1.0, 0.6, 0.2766316489364614], 
reward next is 0.7234, 
noisyNet noise sample is [array([1.6280798], dtype=float32), 0.22734138]. 
=============================================
[2019-04-09 15:06:39,363] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4149425e-04 1.7371464e-03 2.6945280e-02 6.3489475e-05 6.8648893e-01
 4.0840179e-02 2.0303759e-01 4.6767215e-03 2.4840631e-02 8.6401980e-03
 2.5883494e-03], sum to 1.0000
[2019-04-09 15:06:39,363] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8872
[2019-04-09 15:06:39,438] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.300000000000001, 73.0, 0.0, 0.0, 19.0, 24.80643898658476, 0.2626660395392906, 0.0, 1.0, 35.0, 22.508758416114297], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4306800.0000, 
sim time next is 4308000.0000, 
raw observation next is [5.199999999999999, 73.0, 0.0, 0.0, 19.0, 24.78402531194772, 0.2628217438011137, 0.0, 1.0, 45.0, 29.779455061668152], 
processed observation next is [0.0, 0.8695652173913043, 0.6066481994459835, 0.73, 0.0, 0.0, 0.08333333333333333, 0.56533544266231, 0.5876072479337046, 0.0, 1.0, 0.6, 0.29779455061668153], 
reward next is 0.7022, 
noisyNet noise sample is [array([-1.1643596], dtype=float32), 1.3215102]. 
=============================================
[2019-04-09 15:06:39,472] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[16.85936 ]
 [16.989716]
 [16.88453 ]
 [17.21627 ]
 [17.363787]], R is [[17.35523796]
 [17.95659828]
 [18.47433281]
 [19.12375259]
 [19.75313187]].
[2019-04-09 15:06:40,042] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.8531399e-06 2.2201390e-04 4.2535705e-03 5.0528415e-06 7.7453345e-01
 3.5842158e-02 1.6441821e-01 9.8296639e-04 1.6803222e-02 2.1197700e-03
 8.1268267e-04], sum to 1.0000
[2019-04-09 15:06:40,075] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9292961e-05 1.2301767e-03 5.6750290e-03 3.1774409e-05 6.9606119e-01
 1.3038376e-01 1.4381050e-01 1.4185407e-03 1.1217938e-02 8.7048151e-03
 1.3969700e-03], sum to 1.0000
[2019-04-09 15:06:40,075] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8540
[2019-04-09 15:06:40,088] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1037
[2019-04-09 15:06:40,158] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 92.0, 180.3333333333333, 5.0, 22.5, 26.58305427823925, 0.746885672984355, 1.0, 1.0, 35.0, 21.174343203615862], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4454400.0000, 
sim time next is 4455600.0000, 
raw observation next is [0.0, 92.0, 177.5, 5.0, 22.5, 26.68911367825255, 0.7440212182019378, 1.0, 1.0, 35.0, 19.27490131264281], 
processed observation next is [1.0, 0.5652173913043478, 0.46260387811634357, 0.92, 0.5916666666666667, 0.0055248618784530384, 0.375, 0.7240928065210458, 0.7480070727339793, 1.0, 1.0, 0.4, 0.19274901312642811], 
reward next is 0.8073, 
noisyNet noise sample is [array([-0.25514668], dtype=float32), 0.5181037]. 
=============================================
[2019-04-09 15:06:40,168] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.8, 68.0, 0.0, 0.0, 19.0, 25.59687362828646, 0.5516990056580787, 0.0, 1.0, 35.0, 23.789110512606598], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4424400.0000, 
sim time next is 4425600.0000, 
raw observation next is [3.533333333333334, 68.0, 0.0, 0.0, 19.0, 25.70798929426418, 0.5772435637773108, 0.0, 1.0, 45.0, 32.793300730988214], 
processed observation next is [1.0, 0.21739130434782608, 0.5604801477377656, 0.68, 0.0, 0.0, 0.08333333333333333, 0.6423324411886817, 0.6924145212591036, 0.0, 1.0, 0.6, 0.3279330073098821], 
reward next is 0.6721, 
noisyNet noise sample is [array([-0.43764162], dtype=float32), -0.13058306]. 
=============================================
[2019-04-09 15:06:40,314] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3784748e-05 1.1576044e-03 5.5094943e-03 2.9392808e-05 6.9739819e-01
 1.2876734e-01 1.4511515e-01 1.3394450e-03 1.0921524e-02 8.3737317e-03
 1.3243142e-03], sum to 1.0000
[2019-04-09 15:06:40,316] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8777
[2019-04-09 15:06:40,345] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.266666666666667, 68.0, 0.0, 0.0, 19.0, 25.71234886063952, 0.5518903359106256, 0.0, 1.0, 35.0, 24.542006216169334], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4426800.0000, 
sim time next is 4428000.0000, 
raw observation next is [3.0, 68.0, 0.0, 0.0, 19.0, 25.6134453851371, 0.5471966549153581, 0.0, 1.0, 45.0, 31.736966889810112], 
processed observation next is [1.0, 0.2608695652173913, 0.5457063711911359, 0.68, 0.0, 0.0, 0.08333333333333333, 0.6344537820947584, 0.682398884971786, 0.0, 1.0, 0.6, 0.31736966889810114], 
reward next is 0.6826, 
noisyNet noise sample is [array([-0.43764162], dtype=float32), -0.13058306]. 
=============================================
[2019-04-09 15:06:40,354] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[17.746922]
 [17.727127]
 [17.584263]
 [17.70334 ]
 [17.601545]], R is [[18.23081779]
 [18.80308914]
 [19.28712654]
 [19.8563652 ]
 [20.32112885]].
[2019-04-09 15:06:40,536] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1332686e-05 7.5575750e-04 6.9743767e-03 5.5969304e-06 6.8126076e-01
 7.2810657e-02 2.1186529e-01 1.4417304e-03 1.9921241e-02 4.2468794e-03
 6.9630385e-04], sum to 1.0000
[2019-04-09 15:06:40,536] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6828
[2019-04-09 15:06:40,660] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.2, 84.66666666666667, 157.5, 64.5, 22.5, 27.18458271778751, 0.8610983660881555, 1.0, 1.0, 45.0, 33.150416531855214], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4440000.0000, 
sim time next is 4441200.0000, 
raw observation next is [1.1, 85.33333333333333, 179.3333333333333, 50.16666666666666, 22.5, 27.53205936499117, 0.8941765857401381, 1.0, 1.0, 35.0, 25.411407338082917], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8533333333333333, 0.5977777777777776, 0.05543278084714548, 0.375, 0.7943382804159308, 0.7980588619133794, 1.0, 1.0, 0.4, 0.25411407338082914], 
reward next is 0.7459, 
noisyNet noise sample is [array([1.6623707], dtype=float32), -1.3834269]. 
=============================================
[2019-04-09 15:06:40,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0010899e-05 5.3584494e-04 5.0492696e-03 1.0990976e-05 7.2727549e-01
 8.4008515e-02 1.5133511e-01 1.7543861e-03 2.1805596e-02 6.4103962e-03
 1.7843860e-03], sum to 1.0000
[2019-04-09 15:06:40,697] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0688
[2019-04-09 15:06:40,774] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3666666666666667, 72.33333333333334, 0.0, 0.0, 19.0, 23.97964971824578, 0.176801678799796, 0.0, 1.0, 35.0, 22.50439433377802], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4490400.0000, 
sim time next is 4491600.0000, 
raw observation next is [-0.4333333333333333, 72.66666666666667, 0.0, 0.0, 19.0, 23.79420738317771, 0.1336417240266086, 0.0, 1.0, 35.0, 20.53307046490457], 
processed observation next is [1.0, 1.0, 0.45060018467220686, 0.7266666666666667, 0.0, 0.0, 0.08333333333333333, 0.48285061526480916, 0.5445472413422029, 0.0, 1.0, 0.4, 0.20533070464904568], 
reward next is 0.7947, 
noisyNet noise sample is [array([-0.15465687], dtype=float32), -0.21237361]. 
=============================================
[2019-04-09 15:06:40,803] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3188183e-05 6.2030979e-04 4.9237567e-03 2.1223900e-06 7.1950442e-01
 6.2187441e-02 1.9691080e-01 1.9679461e-03 9.9485675e-03 3.0835643e-03
 8.3783240e-04], sum to 1.0000
[2019-04-09 15:06:40,803] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8850
[2019-04-09 15:06:40,823] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.4, 44.0, 0.0, 0.0, 22.5, 28.21005140946706, 1.083024539494625, 1.0, 1.0, 35.0, 10.858908097620724], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4384800.0000, 
sim time next is 4386000.0000, 
raw observation next is [12.26666666666667, 46.0, 0.0, 0.0, 22.5, 28.28272949043819, 1.100968008677581, 1.0, 1.0, 45.0, 16.149346713419085], 
processed observation next is [1.0, 0.782608695652174, 0.8024007386888276, 0.46, 0.0, 0.0, 0.375, 0.8568941242031825, 0.8669893362258604, 1.0, 1.0, 0.6, 0.16149346713419085], 
reward next is 0.8385, 
noisyNet noise sample is [array([-1.0657597], dtype=float32), 0.76461834]. 
=============================================
[2019-04-09 15:06:40,836] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[22.927084]
 [22.922152]
 [22.960857]
 [23.106642]
 [22.96056 ]], R is [[23.41682625]
 [24.07406998]
 [24.69193077]
 [25.29316902]
 [25.7566433 ]].
[2019-04-09 15:06:41,113] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4684318e-06 1.6638542e-04 1.7478507e-03 1.6221350e-06 8.5751355e-01
 2.2382813e-02 1.0101317e-01 6.2610762e-04 1.5118203e-02 9.8853069e-04
 4.3928326e-04], sum to 1.0000
[2019-04-09 15:06:41,114] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3389
[2019-04-09 15:06:41,178] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 89.66666666666667, 103.5, 0.9999999999999998, 22.5, 27.07514294575346, 0.6723839953867415, 1.0, 1.0, 35.0, 23.39792884891233], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4458000.0000, 
sim time next is 4459200.0000, 
raw observation next is [0.0, 87.33333333333334, 82.66666666666667, 0.0, 22.5, 25.50002755597335, 0.5796338054837088, 1.0, 1.0, 35.0, 22.169628236020714], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.8733333333333334, 0.27555555555555555, 0.0, 0.375, 0.6250022963311125, 0.6932112684945696, 1.0, 1.0, 0.4, 0.22169628236020714], 
reward next is 0.7783, 
noisyNet noise sample is [array([-0.1576766], dtype=float32), 0.34275547]. 
=============================================
[2019-04-09 15:06:41,192] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0833794e-05 2.9469241e-04 4.2128903e-03 6.9394496e-06 7.4890906e-01
 5.7302371e-02 1.7358278e-01 1.0517030e-03 1.1296531e-02 2.7786428e-03
 5.5355509e-04], sum to 1.0000
[2019-04-09 15:06:41,192] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4117
[2019-04-09 15:06:41,232] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 86.0, 236.6666666666667, 126.8333333333333, 22.5, 26.44647738457108, 0.6731696622031226, 1.0, 1.0, 35.0, 16.51503778943242], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4443600.0000, 
sim time next is 4444800.0000, 
raw observation next is [1.0, 86.0, 232.8333333333333, 121.6666666666667, 22.5, 26.53326484125997, 0.6870213090786669, 1.0, 1.0, 35.0, 18.382478092408846], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.776111111111111, 0.134438305709024, 0.375, 0.711105403438331, 0.7290071030262223, 1.0, 1.0, 0.4, 0.18382478092408847], 
reward next is 0.8162, 
noisyNet noise sample is [array([-0.25520283], dtype=float32), -0.3900374]. 
=============================================
[2019-04-09 15:06:41,997] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.2797800e-05 8.0313178e-04 7.4754427e-03 2.0234362e-05 7.5868559e-01
 3.4312516e-02 1.7864761e-01 2.5655355e-03 1.2897647e-02 2.2902465e-03
 2.2792127e-03], sum to 1.0000
[2019-04-09 15:06:41,997] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1941
[2019-04-09 15:06:42,051] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.1, 68.33333333333334, 76.66666666666667, 409.1666666666667, 22.5, 23.96032934836825, 0.1402049993809788, 1.0, 1.0, 35.0, 25.27881339473914], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4350000.0000, 
sim time next is 4351200.0000, 
raw observation next is [5.199999999999999, 62.66666666666667, 94.5, 522.0, 22.5, 24.65206631046397, 0.2250541262076635, 1.0, 1.0, 35.0, 21.118912029773334], 
processed observation next is [1.0, 0.34782608695652173, 0.6066481994459835, 0.6266666666666667, 0.315, 0.5767955801104973, 0.375, 0.5543388592053308, 0.5750180420692211, 1.0, 1.0, 0.4, 0.21118912029773335], 
reward next is 0.7888, 
noisyNet noise sample is [array([-1.6320122], dtype=float32), 0.17132594]. 
=============================================
[2019-04-09 15:06:42,385] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.30559099e-04 2.43155891e-03 4.19099443e-03 8.37071275e-05
 6.30080700e-01 1.00879274e-01 2.11732402e-01 5.25919162e-03
 3.16310413e-02 1.09347859e-02 2.64588138e-03], sum to 1.0000
[2019-04-09 15:06:42,385] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6718
[2019-04-09 15:06:42,421] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-0.9333333333333333, 71.66666666666667, 0.0, 0.0, 22.5, 23.05274274638711, -0.07469534256028151, 1.0, 1.0, 35.0, 26.274853445995014], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 4519200.0000, 
sim time next is 4520400.0000, 
raw observation next is [-0.8666666666666667, 72.33333333333333, 18.5, 11.0, 22.5, 23.11895183636354, -0.09023029181210995, 1.0, 1.0, 40.0, 24.61084520939565], 
processed observation next is [1.0, 0.30434782608695654, 0.4385964912280702, 0.7233333333333333, 0.06166666666666667, 0.012154696132596685, 0.375, 0.4265793196969616, 0.46992323606263, 1.0, 1.0, 0.5, 0.2461084520939565], 
reward next is 0.7539, 
noisyNet noise sample is [array([-0.1446954], dtype=float32), 0.433209]. 
=============================================
[2019-04-09 15:06:42,614] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.4099429e-06 8.7112968e-04 4.8924936e-03 1.4647981e-05 7.4599522e-01
 1.1563160e-01 1.1032500e-01 2.5842418e-03 1.6599890e-02 2.0967622e-03
 9.8056823e-04], sum to 1.0000
[2019-04-09 15:06:42,615] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9538
[2019-04-09 15:06:42,691] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.2826438e-05 2.8358591e-03 9.8164482e-03 2.7387834e-05 6.9509012e-01
 1.3135855e-01 1.2563218e-01 3.7055351e-03 1.7434159e-02 1.0897854e-02
 3.1190463e-03], sum to 1.0000
[2019-04-09 15:06:42,691] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4785
[2019-04-09 15:06:42,700] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 78.0, 37.0, 27.5, 22.5, 25.76678902911094, 0.5483374183534795, 1.0, 1.0, 35.0, 21.97416865897121], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4467600.0000, 
sim time next is 4468800.0000, 
raw observation next is [0.0, 76.0, 29.0, 45.83333333333334, 22.5, 26.04304685727748, 0.5972782776946108, 1.0, 1.0, 45.0, 40.69944215779062], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.76, 0.09666666666666666, 0.050644567219152864, 0.375, 0.6702539047731234, 0.6990927592315369, 1.0, 1.0, 0.6, 0.4069944215779062], 
reward next is 0.5930, 
noisyNet noise sample is [array([1.5504326], dtype=float32), -0.35420227]. 
=============================================
[2019-04-09 15:06:42,727] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8666666666666667, 73.0, 0.0, 0.0, 19.0, 23.7335502304828, 0.09774814717742684, 0.0, 1.0, 35.0, 22.284399767203418], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4502400.0000, 
sim time next is 4503600.0000, 
raw observation next is [-1.0, 73.0, 0.0, 0.0, 19.0, 23.65543474541866, 0.07571843334375487, 0.0, 1.0, 35.0, 20.215509705056345], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.73, 0.0, 0.0, 0.08333333333333333, 0.4712862287848883, 0.5252394777812516, 0.0, 1.0, 0.4, 0.20215509705056345], 
reward next is 0.7978, 
noisyNet noise sample is [array([1.5448914], dtype=float32), -0.053739697]. 
=============================================
[2019-04-09 15:06:42,864] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3957747e-06 2.2322194e-04 4.2540170e-03 2.2021834e-06 8.8972789e-01
 5.4445188e-02 4.0204857e-02 3.5462840e-04 6.2220646e-03 3.5382288e-03
 1.0242370e-03], sum to 1.0000
[2019-04-09 15:06:42,885] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6363
[2019-04-09 15:06:42,909] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 74.0, 20.83333333333334, 45.83333333333334, 22.5, 25.96866036760536, 0.5897307763286598, 1.0, 1.0, 35.0, 26.706807914832265], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4470000.0000, 
sim time next is 4471200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 25.91335999067726, 0.6098455306671198, 1.0, 1.0, 35.0, 24.278271217173206], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.6594466658897717, 0.7032818435557067, 1.0, 1.0, 0.4, 0.24278271217173206], 
reward next is 0.7572, 
noisyNet noise sample is [array([-1.2892042], dtype=float32), -0.018916834]. 
=============================================
[2019-04-09 15:06:43,159] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.73957105e-05 1.01956283e-03 6.07938878e-03 1.21911235e-05
 7.77318776e-01 5.41337207e-02 1.11552313e-01 7.70521536e-03
 2.73463987e-02 1.25251748e-02 2.28985492e-03], sum to 1.0000
[2019-04-09 15:06:43,159] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7292
[2019-04-09 15:06:43,179] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.033333333333333, 67.66666666666666, 0.0, 0.0, 19.0, 25.9745449385521, 0.5955726593578048, 0.0, 1.0, 35.0, 21.22047877959185], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4423200.0000, 
sim time next is 4424400.0000, 
raw observation next is [3.8, 68.0, 0.0, 0.0, 19.0, 25.82857912710471, 0.5561873830821191, 0.0, 1.0, 35.0, 19.42330571753382], 
processed observation next is [1.0, 0.21739130434782608, 0.5678670360110805, 0.68, 0.0, 0.0, 0.08333333333333333, 0.6523815939253925, 0.6853957943607064, 0.0, 1.0, 0.4, 0.19423305717533823], 
reward next is 0.8058, 
noisyNet noise sample is [array([1.246555], dtype=float32), -0.26007128]. 
=============================================
[2019-04-09 15:06:43,649] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.20031300e-05 5.96730737e-04 1.05309607e-02 1.02322865e-05
 7.84101784e-01 3.59563418e-02 1.53104216e-01 1.41024147e-03
 8.92826356e-03 3.33948550e-03 2.00975104e-03], sum to 1.0000
[2019-04-09 15:06:43,649] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4143
[2019-04-09 15:06:43,722] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3, 72.0, 0.0, 0.0, 19.0, 23.5883982883121, 0.16739122033512, 0.0, 1.0, 60.0, 55.72120520128526], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4489200.0000, 
sim time next is 4490400.0000, 
raw observation next is [-0.3666666666666667, 72.33333333333334, 0.0, 0.0, 19.0, 23.67887143671638, 0.1785381650108695, 0.0, 1.0, 35.0, 31.759204763439083], 
processed observation next is [1.0, 1.0, 0.4524469067405356, 0.7233333333333334, 0.0, 0.0, 0.08333333333333333, 0.47323928639303176, 0.5595127216702899, 0.0, 1.0, 0.4, 0.3175920476343908], 
reward next is 0.6824, 
noisyNet noise sample is [array([-1.3271548], dtype=float32), 0.6478048]. 
=============================================
[2019-04-09 15:06:43,789] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1921154e-05 1.2094466e-03 7.1287570e-03 4.4580567e-05 7.4393308e-01
 4.8953049e-02 1.4554884e-01 1.0731322e-03 4.0920690e-02 8.4002642e-03
 2.7562431e-03], sum to 1.0000
[2019-04-09 15:06:43,789] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5524
[2019-04-09 15:06:43,869] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 19.0, 23.56994874712112, 0.1255233668021886, 0.0, 1.0, 45.0, 40.10032387301456], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4496400.0000, 
sim time next is 4497600.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 19.0, 23.74385598726703, 0.1256326950525412, 0.0, 1.0, 35.0, 27.048392646766345], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.08333333333333333, 0.4786546656055857, 0.5418775650175137, 0.0, 1.0, 0.4, 0.27048392646766345], 
reward next is 0.7295, 
noisyNet noise sample is [array([1.896295], dtype=float32), 0.36640525]. 
=============================================
[2019-04-09 15:06:44,200] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.9189282e-05 8.9895749e-04 6.2938863e-03 4.2609070e-05 8.0253983e-01
 6.2185884e-02 1.0247043e-01 2.6268356e-03 1.4547974e-02 5.7473038e-03
 2.5669995e-03], sum to 1.0000
[2019-04-09 15:06:44,200] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1191
[2019-04-09 15:06:44,231] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.9666666666666668, 73.0, 0.0, 0.0, 19.0, 23.78803597812085, 0.06874877193569914, 0.0, 1.0, 35.0, 20.833737308542453], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4504800.0000, 
sim time next is 4506000.0000, 
raw observation next is [-0.9333333333333333, 73.0, 0.0, 0.0, 19.0, 23.62442819854166, 0.03521588287777668, 0.0, 1.0, 35.0, 18.980851409771503], 
processed observation next is [1.0, 0.13043478260869565, 0.4367497691597415, 0.73, 0.0, 0.0, 0.08333333333333333, 0.46870234987847176, 0.5117386276259256, 0.0, 1.0, 0.4, 0.18980851409771504], 
reward next is 0.8102, 
noisyNet noise sample is [array([1.0578154], dtype=float32), 0.25215426]. 
=============================================
[2019-04-09 15:06:44,245] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[16.76446 ]
 [16.76022 ]
 [17.12786 ]
 [17.232721]
 [17.652052]], R is [[17.5191021 ]
 [18.13557434]
 [18.72481537]
 [19.2833252 ]
 [19.80848312]].
[2019-04-09 15:06:45,025] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1633597e-05 5.8342738e-04 2.4995394e-03 8.3677623e-06 7.2073191e-01
 6.1160136e-02 1.8852015e-01 3.3774739e-03 1.8839052e-02 3.3289916e-03
 9.3936315e-04], sum to 1.0000
[2019-04-09 15:06:45,036] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3753
[2019-04-09 15:06:45,103] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 19.0, 23.90271206112515, 0.1673417615815783, 0.0, 1.0, 35.0, 17.791071632881348], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4572000.0000, 
sim time next is 4573200.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 19.0, 23.82325699184042, 0.0863204619597104, 0.0, 1.0, 35.0, 18.456851889562444], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.61, 0.0, 0.0, 0.08333333333333333, 0.4852714159867017, 0.5287734873199035, 0.0, 1.0, 0.4, 0.18456851889562442], 
reward next is 0.8154, 
noisyNet noise sample is [array([-0.37468734], dtype=float32), 1.0436952]. 
=============================================
[2019-04-09 15:06:45,355] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0568306e-05 6.6943868e-04 9.5053688e-03 5.5941291e-06 7.3522621e-01
 9.4064713e-02 1.3740060e-01 1.9187927e-03 1.6777663e-02 2.9586598e-03
 1.4424457e-03], sum to 1.0000
[2019-04-09 15:06:45,356] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7496
[2019-04-09 15:06:45,385] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 24.61458400488651, 0.3721347670278689, 0.0, 1.0, 35.0, 20.633561617985695], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 4483200.0000, 
sim time next is 4484400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 24.50739832622707, 0.3585533417906692, 0.0, 1.0, 40.0, 27.591826445784186], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.5422831938522558, 0.6195177805968898, 0.0, 1.0, 0.5, 0.27591826445784184], 
reward next is 0.7241, 
noisyNet noise sample is [array([-0.3780594], dtype=float32), -0.6699394]. 
=============================================
[2019-04-09 15:06:45,394] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-09 15:06:45,395] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:06:45,395] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:06:45,396] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:06:45,397] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:06:45,397] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:06:45,400] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run7
[2019-04-09 15:06:45,439] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:06:45,441] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run7
[2019-04-09 15:06:45,461] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run7
[2019-04-09 15:07:50,206] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.03634919], dtype=float32), 0.054576095]
[2019-04-09 15:07:50,206] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [1.466666666666667, 36.0, 220.3333333333333, 30.16666666666666, 22.5, 24.77744431509747, 0.04560937309389338, 1.0, 1.0, 45.0, 39.57906038598452]
[2019-04-09 15:07:50,207] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 15:07:50,208] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [6.0415303e-05 1.5500403e-03 9.7219329e-03 3.3879864e-05 7.1124673e-01
 5.3844288e-02 1.9512080e-01 2.7387445e-03 1.6349344e-02 7.2297896e-03
 2.1040468e-03], sampled 0.25787043790657715
[2019-04-09 15:08:22,626] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 3072.9404 109928.3333 591.0632
[2019-04-09 15:08:22,654] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:22,654] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:22,654] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:22,654] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:22,654] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:22,654] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:22,654] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:22,825] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:22,825] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:22,825] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:22,825] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:22,825] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:22,825] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:22,825] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:42,036] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 3012.3678 117184.9733 183.6394
[2019-04-09 15:08:42,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:42,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:42,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:42,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:42,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:42,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:42,057] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:42,206] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:42,206] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:42,206] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:42,206] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:42,206] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:42,206] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:42,206] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:45,199] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2961.8963 119164.9667 -21.6181
[2019-04-09 15:08:45,218] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:45,218] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:45,218] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:45,218] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:45,218] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:45,218] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:45,218] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:08:45,325] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:45,325] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:45,325] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:45,325] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:45,325] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:45,325] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:45,325] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:08:46,220] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 60000, evaluation results [60000.0, 3012.3678329833638, 117184.97333228422, 183.6393646958565, 3072.9404200444137, 109928.33330947878, 591.0631925004641, 2961.8963066263423, 119164.96665837981, -21.618115435972626]
[2019-04-09 15:08:46,240] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.61942387e-05 1.06255827e-03 5.44176111e-03 8.38406322e-06
 7.94476092e-01 6.92348257e-02 1.16927266e-01 2.35570525e-03
 6.81098457e-03 2.95398664e-03 7.02196267e-04], sum to 1.0000
[2019-04-09 15:08:46,240] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6155
[2019-04-09 15:08:46,257] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6666666666666666, 64.66666666666667, 139.3333333333333, 2.999999999999999, 22.5, 24.20214123278824, 0.1144605772287871, 1.0, 1.0, 35.0, 19.225934157003053], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4527600.0000, 
sim time next is 4528800.0000, 
raw observation next is [1.0, 61.0, 172.0, 9.0, 22.5, 24.34968756794344, 0.1479350650541919, 1.0, 1.0, 35.0, 19.01832175784184], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.61, 0.5733333333333334, 0.009944751381215469, 0.375, 0.5291406306619534, 0.5493116883513973, 1.0, 1.0, 0.4, 0.19018321757841838], 
reward next is 0.8098, 
noisyNet noise sample is [array([-1.6233236], dtype=float32), 1.8966407]. 
=============================================
[2019-04-09 15:08:47,183] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6772144e-06 1.5316418e-04 2.1591713e-03 1.7874373e-06 7.6290625e-01
 4.7108326e-02 1.6691671e-01 1.6722210e-03 1.5896814e-02 2.1708442e-03
 1.0110572e-03], sum to 1.0000
[2019-04-09 15:08:47,188] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4912
[2019-04-09 15:08:47,236] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 25.45700492803819, 0.5440244380809519, 1.0, 1.0, 35.0, 33.68771019621267], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4560000.0000, 
sim time next is 4561200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 25.84812920330987, 0.5853448179374546, 1.0, 1.0, 35.0, 28.48012186814106], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.6540107669424892, 0.6951149393124849, 1.0, 1.0, 0.4, 0.2848012186814106], 
reward next is 0.7152, 
noisyNet noise sample is [array([-0.23146182], dtype=float32), 2.4804947]. 
=============================================
[2019-04-09 15:08:47,284] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.7444162e-05 1.6636816e-03 3.6077355e-03 4.1826861e-05 7.7465212e-01
 4.3801647e-02 1.5822184e-01 4.7602425e-03 8.8440115e-03 2.7719142e-03
 1.5675796e-03], sum to 1.0000
[2019-04-09 15:08:47,284] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9406
[2019-04-09 15:08:47,333] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 102.5, 142.5, 22.5, 22.39320898841305, -0.1865678048241096, 1.0, 1.0, 35.0, 18.34489535092503], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4609200.0000, 
sim time next is 4610400.0000, 
raw observation next is [-2.0, 71.0, 129.8333333333333, 227.3333333333333, 22.5, 23.22443082636221, -0.03333916401682027, 1.0, 1.0, 45.0, 35.89576181845021], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.4327777777777776, 0.2511970534069981, 0.375, 0.4353692355301841, 0.48888694532772653, 1.0, 1.0, 0.6, 0.3589576181845021], 
reward next is 0.6410, 
noisyNet noise sample is [array([-0.19419675], dtype=float32), -1.2448047]. 
=============================================
[2019-04-09 15:08:47,634] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9791959e-06 2.7049400e-04 2.4081983e-03 2.5013576e-06 8.7003791e-01
 4.2967558e-02 6.8101756e-02 7.9334277e-04 1.3143514e-02 2.0080169e-03
 2.6363248e-04], sum to 1.0000
[2019-04-09 15:08:47,634] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5180
[2019-04-09 15:08:47,662] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.0, 23.33333333333334, 23.33333333333334, 22.5, 25.70392497462056, 0.5031109318547873, 1.0, 1.0, 35.0, 19.196785471490927], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4556400.0000, 
sim time next is 4557600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 25.54292568674194, 0.4650551195787078, 1.0, 1.0, 35.0, 18.555511674829994], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.6285771405618282, 0.6550183731929026, 1.0, 1.0, 0.4, 0.18555511674829994], 
reward next is 0.8144, 
noisyNet noise sample is [array([0.50499547], dtype=float32), 1.9183838]. 
=============================================
[2019-04-09 15:08:47,730] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6670356e-06 5.8411871e-04 7.1682995e-03 1.4613096e-06 8.2876641e-01
 5.5026896e-02 9.9890925e-02 8.2093652e-04 6.9971327e-03 4.8458658e-04
 2.5551740e-04], sum to 1.0000
[2019-04-09 15:08:47,730] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4101
[2019-04-09 15:08:47,748] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 25.20924231794686, 0.415419628501243, 1.0, 1.0, 35.0, 17.793259947602436], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4560000.0000, 
sim time next is 4561200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 25.0948585630094, 0.4093494703405867, 1.0, 1.0, 35.0, 18.081695673898466], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.5912382135841167, 0.6364498234468622, 1.0, 1.0, 0.4, 0.18081695673898465], 
reward next is 0.8192, 
noisyNet noise sample is [array([1.2533301], dtype=float32), 0.50184125]. 
=============================================
[2019-04-09 15:08:48,029] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.8959883e-06 3.9896078e-04 2.3774423e-03 6.9308589e-06 9.2073143e-01
 3.3309191e-02 3.1096905e-02 7.6259172e-04 6.4777867e-03 3.4267243e-03
 1.4041063e-03], sum to 1.0000
[2019-04-09 15:08:48,029] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9944
[2019-04-09 15:08:48,072] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.666666666666667, 58.33333333333334, 0.0, 0.0, 19.0, 24.6224504101996, 0.3226824527241088, 0.0, 1.0, 35.0, 17.63375053936555], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 4569600.0000, 
sim time next is 4570800.0000, 
raw observation next is [1.333333333333333, 59.66666666666667, 0.0, 0.0, 19.0, 24.56451384523573, 0.354396471398941, 0.0, 1.0, 55.0, 53.98199407724188], 
processed observation next is [1.0, 0.9130434782608695, 0.4995383194829178, 0.5966666666666667, 0.0, 0.0, 0.08333333333333333, 0.5470428204363108, 0.6181321571329803, 0.0, 1.0, 0.8, 0.5398199407724188], 
reward next is 0.4602, 
noisyNet noise sample is [array([0.5369969], dtype=float32), 0.6024625]. 
=============================================
[2019-04-09 15:08:48,094] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.3063242e-06 1.8151723e-04 8.6818310e-04 3.1706322e-06 6.5208322e-01
 4.4510193e-02 2.9284903e-01 4.0283959e-04 5.6307297e-03 3.1895465e-03
 2.7316209e-04], sum to 1.0000
[2019-04-09 15:08:48,097] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2161
[2019-04-09 15:08:48,110] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1586772e-05 2.4659338e-04 7.7996906e-03 2.7305184e-06 8.5125524e-01
 3.9735787e-02 8.7314919e-02 1.7553966e-03 9.7375298e-03 1.7292822e-03
 4.1128241e-04], sum to 1.0000
[2019-04-09 15:08:48,111] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6869
[2019-04-09 15:08:48,115] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 85.33333333333333, 179.3333333333333, 50.16666666666666, 22.5, 26.59781735454955, 0.6962962836171157, 1.0, 1.0, 35.0, 21.193782271000707], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4441200.0000, 
sim time next is 4442400.0000, 
raw observation next is [1.0, 86.0, 208.0, 88.5, 22.5, 26.63836792584683, 0.705451865663926, 1.0, 1.0, 35.0, 19.004942481267744], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.6933333333333334, 0.09779005524861878, 0.375, 0.7198639938205691, 0.7351506218879753, 1.0, 1.0, 0.4, 0.19004942481267745], 
reward next is 0.8100, 
noisyNet noise sample is [array([0.12791808], dtype=float32), 0.04230104]. 
=============================================
[2019-04-09 15:08:48,154] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.333333333333333, 59.66666666666667, 204.6666666666667, 15.0, 22.5, 24.85457930660385, 0.2496670580280463, 1.0, 1.0, 35.0, 15.900267206831906], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4530000.0000, 
sim time next is 4531200.0000, 
raw observation next is [1.666666666666667, 58.33333333333334, 203.8333333333333, 15.0, 22.5, 24.92942322867825, 0.264293182928649, 1.0, 1.0, 35.0, 20.494425092369816], 
processed observation next is [1.0, 0.43478260869565216, 0.5087719298245615, 0.5833333333333335, 0.6794444444444443, 0.016574585635359115, 0.375, 0.5774519357231874, 0.588097727642883, 1.0, 1.0, 0.4, 0.20494425092369817], 
reward next is 0.7951, 
noisyNet noise sample is [array([-0.5823122], dtype=float32), 0.22105475]. 
=============================================
[2019-04-09 15:08:48,333] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.9377915e-06 8.9994493e-05 3.2705099e-03 9.5926725e-07 9.5314389e-01
 1.3400877e-02 2.5290916e-02 4.0428931e-04 3.4447589e-03 6.6788902e-04
 2.8390344e-04], sum to 1.0000
[2019-04-09 15:08:48,336] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6465
[2019-04-09 15:08:48,392] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.666666666666667, 50.0, 121.6666666666667, 837.3333333333334, 22.5, 25.69483169909764, 0.5419338664189542, 1.0, 1.0, 35.0, 20.19148223543943], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4621200.0000, 
sim time next is 4622400.0000, 
raw observation next is [3.0, 49.0, 121.0, 846.0, 22.5, 26.07758679396505, 0.5801884253463193, 1.0, 1.0, 35.0, 17.700987782314762], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.49, 0.4033333333333333, 0.9348066298342541, 0.375, 0.6731322328304209, 0.6933961417821064, 1.0, 1.0, 0.4, 0.17700987782314762], 
reward next is 0.8230, 
noisyNet noise sample is [array([1.4344039], dtype=float32), -0.9870693]. 
=============================================
[2019-04-09 15:08:48,508] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9521051e-06 3.6300227e-04 4.4499640e-03 2.1294061e-06 6.2778771e-01
 7.9240993e-02 2.7518243e-01 1.3168604e-03 8.8777188e-03 2.3734670e-03
 4.0168868e-04], sum to 1.0000
[2019-04-09 15:08:48,510] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2480
[2019-04-09 15:08:48,559] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 49.33333333333333, 125.5, 0.0, 22.5, 26.07190073566721, 0.374094436869676, 1.0, 1.0, 45.0, 27.563936620559435], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4537200.0000, 
sim time next is 4538400.0000, 
raw observation next is [2.0, 50.66666666666666, 147.0, 7.999999999999998, 22.5, 24.41227700769443, 0.276307843784174, 1.0, 1.0, 35.0, 25.53578398977146], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.5066666666666666, 0.49, 0.008839779005524859, 0.375, 0.5343564173078693, 0.5921026145947247, 1.0, 1.0, 0.4, 0.2553578398977146], 
reward next is 0.7446, 
noisyNet noise sample is [array([0.35045207], dtype=float32), -0.4602197]. 
=============================================
[2019-04-09 15:08:48,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.9078931e-05 1.1395689e-03 3.6740401e-03 2.3908526e-05 7.7881938e-01
 6.5162547e-02 1.2435840e-01 3.5378670e-03 1.3518662e-02 7.7585923e-03
 1.9480773e-03], sum to 1.0000
[2019-04-09 15:08:48,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6445
[2019-04-09 15:08:48,749] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 65.66666666666667, 0.0, 0.0, 19.0, 24.05656722798308, 0.1334795337935945, 0.0, 1.0, 35.0, 16.76204965458424], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4587600.0000, 
sim time next is 4588800.0000, 
raw observation next is [-0.8, 66.33333333333333, 0.0, 0.0, 19.0, 24.03367266530086, 0.09224118493570854, 0.0, 1.0, 35.0, 15.96490718057929], 
processed observation next is [1.0, 0.08695652173913043, 0.4404432132963989, 0.6633333333333333, 0.0, 0.0, 0.08333333333333333, 0.5028060554417383, 0.5307470616452362, 0.0, 1.0, 0.4, 0.1596490718057929], 
reward next is 0.8404, 
noisyNet noise sample is [array([0.50638443], dtype=float32), 0.40256912]. 
=============================================
[2019-04-09 15:08:48,886] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2607324e-05 8.5211365e-04 3.6517265e-03 7.5419825e-06 5.6592667e-01
 2.3200789e-02 3.8861433e-01 1.0953248e-03 1.0788631e-02 4.9039265e-03
 9.3632267e-04], sum to 1.0000
[2019-04-09 15:08:48,887] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1066
[2019-04-09 15:08:48,919] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5666666666666667, 73.0, 0.0, 0.0, 19.0, 23.36581934371126, 0.05505226834070617, 0.0, 1.0, 25.0, 17.984688787713928], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4495200.0000, 
sim time next is 4496400.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 19.0, 23.32902599516783, 0.05275304276616938, 0.0, 1.0, 35.0, 22.85159561725084], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.08333333333333333, 0.4440854995973191, 0.5175843475887231, 0.0, 1.0, 0.4, 0.22851595617250842], 
reward next is 0.7715, 
noisyNet noise sample is [array([0.56374234], dtype=float32), -0.5040949]. 
=============================================
[2019-04-09 15:08:48,925] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4301419e-06 2.2860838e-04 3.0778141e-03 2.3227863e-06 6.3571984e-01
 7.6273501e-02 2.7597383e-01 1.0567165e-03 6.1520659e-03 8.8359276e-04
 6.2928186e-04], sum to 1.0000
[2019-04-09 15:08:48,938] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5691
[2019-04-09 15:08:48,994] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6666666666666666, 57.33333333333334, 134.8333333333333, 724.0, 22.5, 25.76912081850644, 0.4823861055407604, 1.0, 1.0, 40.0, 31.831742707436696], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4616400.0000, 
sim time next is 4617600.0000, 
raw observation next is [1.333333333333333, 54.66666666666667, 127.8333333333333, 778.0, 22.5, 26.00607669913695, 0.5274882078603342, 1.0, 1.0, 35.0, 28.041050188448473], 
processed observation next is [1.0, 0.43478260869565216, 0.4995383194829178, 0.5466666666666667, 0.426111111111111, 0.8596685082872928, 0.375, 0.6671730582614126, 0.6758294026201114, 1.0, 1.0, 0.4, 0.28041050188448474], 
reward next is 0.7196, 
noisyNet noise sample is [array([2.4310627], dtype=float32), -0.41108352]. 
=============================================
[2019-04-09 15:08:49,133] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.3106468e-05 1.3628927e-03 1.2460821e-02 2.9673331e-05 5.3848070e-01
 6.7289323e-02 3.4007379e-01 2.6957537e-03 1.9347858e-02 1.4541138e-02
 3.6348242e-03], sum to 1.0000
[2019-04-09 15:08:49,133] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5795
[2019-04-09 15:08:49,167] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.333333333333333, 73.0, 20.5, 28.49999999999999, 22.5, 22.25588023451044, -0.2272138318925782, 1.0, 1.0, 45.0, 33.14723250607992], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 4606800.0000, 
sim time next is 4608000.0000, 
raw observation next is [-2.0, 71.0, 61.5, 85.5, 22.5, 22.56458443064444, -0.1875700332734959, 1.0, 1.0, 40.0, 23.78190048887094], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.205, 0.09447513812154697, 0.375, 0.38038203588703673, 0.43747665557550136, 1.0, 1.0, 0.5, 0.23781900488870938], 
reward next is 0.7622, 
noisyNet noise sample is [array([-1.237507], dtype=float32), -0.817346]. 
=============================================
[2019-04-09 15:08:49,174] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[19.080742]
 [18.492905]
 [18.320118]
 [18.38416 ]
 [18.169025]], R is [[19.66000557]
 [20.13193321]
 [20.64702034]
 [21.19889832]
 [21.64374161]].
[2019-04-09 15:08:50,009] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.15991534e-04 2.05797353e-03 5.69405966e-03 2.44671282e-05
 6.94059968e-01 5.82196936e-02 1.99927196e-01 9.04927868e-03
 1.85516048e-02 9.32786521e-03 2.97187618e-03], sum to 1.0000
[2019-04-09 15:08:50,009] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3100
[2019-04-09 15:08:50,041] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 69.66666666666667, 0.0, 0.0, 19.0, 23.21313217171735, -0.1102254809524056, 0.0, 1.0, 35.0, 16.281912451306706], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4594800.0000, 
sim time next is 4596000.0000, 
raw observation next is [-1.833333333333333, 70.33333333333333, 0.0, 0.0, 19.0, 23.02674409225136, -0.1439080612266216, 0.0, 1.0, 35.0, 14.88448494655313], 
processed observation next is [1.0, 0.17391304347826086, 0.41181902123730385, 0.7033333333333333, 0.0, 0.0, 0.08333333333333333, 0.4188953410209466, 0.45203064625779277, 0.0, 1.0, 0.4, 0.1488448494655313], 
reward next is 0.8512, 
noisyNet noise sample is [array([0.8142125], dtype=float32), 1.4947957]. 
=============================================
[2019-04-09 15:08:50,058] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[18.303709]
 [18.177086]
 [18.003801]
 [18.31288 ]
 [18.066813]], R is [[18.74843597]
 [19.39813232]
 [20.02615547]
 [20.63015747]
 [21.20723915]].
[2019-04-09 15:08:50,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5238986e-05 1.0168289e-03 4.7950526e-03 9.2503624e-06 8.3390355e-01
 3.8574472e-02 1.0694427e-01 1.0525911e-03 8.9369537e-03 3.3142155e-03
 1.4375187e-03], sum to 1.0000
[2019-04-09 15:08:50,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0221
[2019-04-09 15:08:50,279] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.24598550e-05 8.42313690e-04 6.52573630e-03 1.24180315e-05
 7.83325911e-01 3.64055932e-02 1.47195891e-01 2.48971768e-03
 1.48219364e-02 5.55768562e-03 2.79028295e-03], sum to 1.0000
[2019-04-09 15:08:50,299] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9369
[2019-04-09 15:08:50,280] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6666666666666667, 97.33333333333334, 0.0, 0.0, 22.5, 24.12699127597393, 0.1233168528705074, 1.0, 1.0, 35.0, 20.941158399894285], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4692000.0000, 
sim time next is 4693200.0000, 
raw observation next is [-0.3333333333333334, 94.66666666666666, 10.5, 0.0, 22.5, 24.08260703517613, 0.1045214663117408, 1.0, 1.0, 35.0, 18.914721468853024], 
processed observation next is [1.0, 0.30434782608695654, 0.4533702677747, 0.9466666666666665, 0.035, 0.0, 0.375, 0.5068839195980109, 0.5348404887705803, 1.0, 1.0, 0.4, 0.18914721468853024], 
reward next is 0.8109, 
noisyNet noise sample is [array([1.5236908], dtype=float32), 2.4750984]. 
=============================================
[2019-04-09 15:08:50,331] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2, 65.0, 0.0, 0.0, 19.0, 23.21490753254007, -0.05071472760403459, 0.0, 1.0, 35.0, 13.90477363596237], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4586400.0000, 
sim time next is 4587600.0000, 
raw observation next is [-0.5, 65.66666666666667, 0.0, 0.0, 19.0, 23.15796208833798, -0.01459034218143738, 0.0, 1.0, 45.0, 33.1435117907636], 
processed observation next is [1.0, 0.08695652173913043, 0.44875346260387816, 0.6566666666666667, 0.0, 0.0, 0.08333333333333333, 0.42983017402816515, 0.49513655260618755, 0.0, 1.0, 0.6, 0.33143511790763597], 
reward next is 0.6686, 
noisyNet noise sample is [array([-0.8141465], dtype=float32), -0.24527791]. 
=============================================
[2019-04-09 15:08:50,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.1673280e-05 5.9276464e-04 8.7228538e-03 3.8969811e-05 7.7290082e-01
 7.8325555e-02 1.2628680e-01 2.2341344e-03 6.0658101e-03 2.8826131e-03
 1.8880098e-03], sum to 1.0000
[2019-04-09 15:08:50,392] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6248
[2019-04-09 15:08:50,436] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.9666666666666668, 73.0, 0.0, 0.0, 19.0, 23.61392807703611, 0.1304619382318579, 0.0, 1.0, 55.0, 58.23514508151471], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4504800.0000, 
sim time next is 4506000.0000, 
raw observation next is [-0.9333333333333333, 73.0, 0.0, 0.0, 19.0, 24.05019264825761, 0.1736471789954731, 0.0, 1.0, 35.0, 37.48564958581718], 
processed observation next is [1.0, 0.13043478260869565, 0.4367497691597415, 0.73, 0.0, 0.0, 0.08333333333333333, 0.5041827206881342, 0.557882392998491, 0.0, 1.0, 0.4, 0.37485649585817177], 
reward next is 0.6251, 
noisyNet noise sample is [array([-0.3714633], dtype=float32), -0.06758063]. 
=============================================
[2019-04-09 15:08:50,448] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[18.964462]
 [18.814743]
 [18.912457]
 [19.050863]
 [19.630348]], R is [[19.44455719]
 [19.66776085]
 [20.11501122]
 [20.6730442 ]
 [21.19403648]].
[2019-04-09 15:08:50,828] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6835596e-06 6.9471251e-05 1.6054211e-03 1.4195034e-06 9.0989304e-01
 4.9837794e-02 3.3655096e-02 2.4427913e-04 3.9029892e-03 4.5494171e-04
 3.3276342e-04], sum to 1.0000
[2019-04-09 15:08:50,828] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4616
[2019-04-09 15:08:50,869] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7497090e-05 5.2861054e-04 1.9001700e-02 3.0864735e-06 6.3964725e-01
 1.1827656e-01 2.0788306e-01 5.9692562e-04 8.2648750e-03 3.7938091e-03
 1.9866151e-03], sum to 1.0000
[2019-04-09 15:08:50,870] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.333333333333333, 47.0, 145.6666666666667, 21.33333333333333, 22.5, 25.48683514441277, 0.4189559868667672, 1.0, 1.0, 35.0, 21.470468566577956], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4549200.0000, 
sim time next is 4550400.0000, 
raw observation next is [2.0, 48.0, 131.0, 40.0, 22.5, 25.32975675445873, 0.4082455426822861, 1.0, 1.0, 35.0, 18.987275193565075], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.48, 0.43666666666666665, 0.04419889502762431, 0.375, 0.610813062871561, 0.636081847560762, 1.0, 1.0, 0.4, 0.18987275193565076], 
reward next is 0.8101, 
noisyNet noise sample is [array([-0.9485574], dtype=float32), -0.6276589]. 
=============================================
[2019-04-09 15:08:50,873] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1070
[2019-04-09 15:08:50,904] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 19.0, 25.1664784336074, 0.4061659808347983, 0.0, 1.0, 35.0, 21.971935085672726], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4660800.0000, 
sim time next is 4662000.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 19.0, 25.0207441854883, 0.3713864324201056, 0.0, 1.0, 35.0, 19.787972645686036], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.57, 0.0, 0.0, 0.08333333333333333, 0.5850620154573584, 0.6237954774733686, 0.0, 1.0, 0.4, 0.19787972645686036], 
reward next is 0.8021, 
noisyNet noise sample is [array([0.47778308], dtype=float32), 1.9223074]. 
=============================================
[2019-04-09 15:08:50,923] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[21.99233 ]
 [21.928059]
 [22.099983]
 [22.434935]
 [22.11093 ]], R is [[22.40350723]
 [22.95975304]
 [23.4417057 ]
 [23.9298439 ]
 [24.47434425]].
[2019-04-09 15:08:50,983] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8066190e-06 3.5490800e-04 3.5755394e-03 6.3981258e-07 7.4580157e-01
 1.0694000e-02 2.3008564e-01 2.5298735e-04 6.2158755e-03 2.5808313e-03
 4.3328732e-04], sum to 1.0000
[2019-04-09 15:08:50,987] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7479
[2019-04-09 15:08:51,035] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 19.0, 25.01560029175206, 0.4654122837610086, 0.0, 1.0, 45.0, 30.07692142404691], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4654800.0000, 
sim time next is 4656000.0000, 
raw observation next is [2.0, 53.66666666666667, 0.0, 0.0, 19.0, 25.10973963711971, 0.4725599734868831, 0.0, 1.0, 35.0, 21.38922918816275], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.5366666666666667, 0.0, 0.0, 0.08333333333333333, 0.5924783030933091, 0.6575199911622943, 0.0, 1.0, 0.4, 0.2138922918816275], 
reward next is 0.7861, 
noisyNet noise sample is [array([-0.2801547], dtype=float32), -0.35877687]. 
=============================================
[2019-04-09 15:08:51,043] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[22.878632]
 [23.439554]
 [23.340422]
 [23.825054]
 [24.332962]], R is [[22.8857193 ]
 [23.35609245]
 [23.9340477 ]
 [24.50823784]
 [25.08101463]].
[2019-04-09 15:08:51,945] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4959629e-06 1.6016192e-04 1.2165069e-03 1.0894857e-06 8.9616537e-01
 3.2634463e-02 5.8494724e-02 9.9752843e-04 9.2741428e-03 8.7067444e-04
 1.8379946e-04], sum to 1.0000
[2019-04-09 15:08:51,946] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3290
[2019-04-09 15:08:51,971] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 92.0, 161.5, 3.0, 22.5, 25.32849446580705, 0.3538754768596759, 1.0, 1.0, 35.0, 21.629754973492286], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4701600.0000, 
sim time next is 4702800.0000, 
raw observation next is [0.0, 92.0, 192.5, 5.0, 22.5, 25.47273093168447, 0.3798690286745061, 1.0, 1.0, 35.0, 18.892043690280538], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.92, 0.6416666666666667, 0.0055248618784530384, 0.375, 0.6227275776403726, 0.6266230095581687, 1.0, 1.0, 0.4, 0.18892043690280538], 
reward next is 0.8111, 
noisyNet noise sample is [array([-0.40818563], dtype=float32), -0.21837217]. 
=============================================
[2019-04-09 15:08:52,631] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4985796e-05 5.2589295e-04 3.4345319e-03 6.9391190e-06 8.2862306e-01
 1.5350681e-02 1.4236477e-01 1.3205511e-03 4.4643129e-03 2.9744168e-03
 9.1981894e-04], sum to 1.0000
[2019-04-09 15:08:52,631] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5719
[2019-04-09 15:08:52,652] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 58.66666666666666, 0.0, 0.0, 19.0, 24.24575069315586, 0.1679047605324712, 0.0, 1.0, 35.0, 15.120905319429724], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4671600.0000, 
sim time next is 4672800.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 19.0, 24.06401740856128, 0.1342718256543718, 0.0, 1.0, 35.0, 17.337534233960525], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.08333333333333333, 0.5053347840467733, 0.5447572752181239, 0.0, 1.0, 0.4, 0.17337534233960525], 
reward next is 0.8266, 
noisyNet noise sample is [array([0.82680297], dtype=float32), 0.25365523]. 
=============================================
[2019-04-09 15:08:52,925] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5622991e-04 2.3016045e-03 1.0419620e-02 1.8892295e-05 5.7632846e-01
 9.3286768e-02 2.6551953e-01 6.2832264e-03 2.9697219e-02 1.0175998e-02
 5.8124652e-03], sum to 1.0000
[2019-04-09 15:08:52,925] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5113
[2019-04-09 15:08:52,950] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6666666666666666, 97.33333333333333, 0.0, 0.0, 19.0, 23.58248244489675, 0.02333101271117266, 0.0, 1.0, 35.0, 20.81742181835407], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4682400.0000, 
sim time next is 4683600.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 23.60178806944716, -0.004885391419526139, 0.0, 1.0, 35.0, 18.435270667505627], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.46681567245393, 0.4983715361934913, 0.0, 1.0, 0.4, 0.18435270667505627], 
reward next is 0.8156, 
noisyNet noise sample is [array([-0.22140665], dtype=float32), -0.7524506]. 
=============================================
[2019-04-09 15:08:52,978] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7093003e-07 1.3421050e-04 2.0459243e-03 2.5471431e-07 7.9498976e-01
 1.6721128e-02 1.8292630e-01 4.9100563e-05 1.7772069e-03 1.2858504e-03
 6.9993861e-05], sum to 1.0000
[2019-04-09 15:08:52,978] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9206
[2019-04-09 15:08:53,006] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 72.0, 171.5, 3.0, 22.5, 25.27468042575654, 0.3673351387486332, 1.0, 1.0, 35.0, 23.926401432927165], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4719600.0000, 
sim time next is 4720800.0000, 
raw observation next is [1.0, 72.0, 155.1666666666667, 0.9999999999999998, 22.5, 25.39106734209592, 0.4117941691325995, 1.0, 1.0, 45.0, 41.501046479866574], 
processed observation next is [1.0, 0.6521739130434783, 0.4903047091412743, 0.72, 0.5172222222222224, 0.0011049723756906074, 0.375, 0.6159222785079933, 0.6372647230441998, 1.0, 1.0, 0.6, 0.4150104647986657], 
reward next is 0.5850, 
noisyNet noise sample is [array([-1.0542501], dtype=float32), 0.3299198]. 
=============================================
[2019-04-09 15:08:53,777] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.5146803e-07 1.1788691e-04 5.0050241e-04 7.3426963e-07 9.2026073e-01
 1.9740950e-02 5.3458326e-02 4.9714552e-04 4.6948781e-03 2.4466065e-04
 4.8331599e-04], sum to 1.0000
[2019-04-09 15:08:53,778] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0158
[2019-04-09 15:08:53,803] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.800000000000001, 49.33333333333334, 192.3333333333333, 634.6666666666666, 22.5, 26.12256153342664, 0.6784327541078244, 1.0, 1.0, 45.0, 29.518109940215453], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4630800.0000, 
sim time next is 4632000.0000, 
raw observation next is [4.9, 49.66666666666666, 201.6666666666667, 520.6666666666666, 22.5, 26.68021121699321, 0.7491322358989655, 1.0, 1.0, 35.0, 16.746998461399205], 
processed observation next is [1.0, 0.6086956521739131, 0.5983379501385043, 0.4966666666666666, 0.6722222222222224, 0.5753222836095764, 0.375, 0.7233509347494342, 0.7497107452996552, 1.0, 1.0, 0.4, 0.16746998461399204], 
reward next is 0.8325, 
noisyNet noise sample is [array([0.73893636], dtype=float32), -0.26209128]. 
=============================================
[2019-04-09 15:08:53,826] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[26.380045]
 [26.117487]
 [26.117142]
 [26.099022]
 [26.232859]], R is [[26.99807739]
 [27.43291473]
 [28.04471397]
 [28.6480484 ]
 [29.26877975]].
[2019-04-09 15:08:53,851] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 62935: loss 14.9092
[2019-04-09 15:08:53,852] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 62935: learning rate 0.0000
[2019-04-09 15:08:54,215] A3C_AGENT_WORKER-Thread-8 INFO:Local step 4000, global step 63081: loss 10.5593
[2019-04-09 15:08:54,228] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 4000, global step 63083: learning rate 0.0000
[2019-04-09 15:08:54,526] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.33716639e-06 4.11533169e-04 5.41380374e-03 3.40997985e-06
 7.98609614e-01 1.67260114e-02 1.61691666e-01 2.51649064e-03
 1.30424015e-02 9.99016687e-04 5.80805237e-04], sum to 1.0000
[2019-04-09 15:08:54,527] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4040
[2019-04-09 15:08:54,549] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 55.33333333333334, 0.0, 0.0, 19.0, 25.06439384969996, 0.4318549965777393, 0.0, 1.0, 35.0, 21.792871636931842], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4663200.0000, 
sim time next is 4664400.0000, 
raw observation next is [2.0, 53.66666666666667, 0.0, 0.0, 19.0, 25.06092301748983, 0.4031805349968356, 0.0, 1.0, 35.0, 19.27386641962401], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.5366666666666667, 0.0, 0.0, 0.08333333333333333, 0.5884102514574859, 0.6343935116656119, 0.0, 1.0, 0.4, 0.1927386641962401], 
reward next is 0.8073, 
noisyNet noise sample is [array([0.6530268], dtype=float32), -0.2663036]. 
=============================================
[2019-04-09 15:08:54,557] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.6860440e-06 4.1636307e-04 5.4394244e-03 3.5805797e-06 7.9410881e-01
 1.7456662e-02 1.6493636e-01 2.5758536e-03 1.3422214e-02 1.0398832e-03
 5.9519429e-04], sum to 1.0000
[2019-04-09 15:08:54,561] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1991
[2019-04-09 15:08:54,558] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0913830e-05 6.2940386e-04 6.4649428e-03 4.6642162e-06 6.3149869e-01
 5.8920737e-02 2.8100681e-01 1.2151154e-03 1.2773259e-02 6.3143363e-03
 1.1610552e-03], sum to 1.0000
[2019-04-09 15:08:54,562] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0512
[2019-04-09 15:08:54,579] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 77.0, 0.0, 0.0, 19.0, 22.68088319286238, -0.128101782966999, 0.0, 1.0, 35.0, 20.987244043405084], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4748400.0000, 
sim time next is 4749600.0000, 
raw observation next is [-3.333333333333333, 79.33333333333334, 0.0, 0.0, 19.0, 22.5048409633627, -0.1653338982072862, 0.0, 1.0, 35.0, 21.13680896098657], 
processed observation next is [1.0, 1.0, 0.37026777469990774, 0.7933333333333334, 0.0, 0.0, 0.08333333333333333, 0.3754034136135583, 0.44488870059757124, 0.0, 1.0, 0.4, 0.2113680896098657], 
reward next is 0.7886, 
noisyNet noise sample is [array([-0.24484491], dtype=float32), 1.2017037]. 
=============================================
[2019-04-09 15:08:54,609] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 53.66666666666667, 0.0, 0.0, 19.0, 25.06092301748983, 0.4031805349968356, 0.0, 1.0, 35.0, 19.27386641962401], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4664400.0000, 
sim time next is 4665600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 19.0, 24.94139550272612, 0.3695029112730824, 0.0, 1.0, 35.0, 17.56380779523122], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 0.08333333333333333, 0.5784496252271767, 0.6231676370910274, 0.0, 1.0, 0.4, 0.1756380779523122], 
reward next is 0.8244, 
noisyNet noise sample is [array([0.6530268], dtype=float32), -0.2663036]. 
=============================================
[2019-04-09 15:08:54,636] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0441559e-05 9.5201592e-04 3.1954830e-03 1.4301854e-05 9.1628516e-01
 1.6311610e-02 4.1001458e-02 1.8665156e-03 1.6358899e-02 3.3852959e-03
 5.8877823e-04], sum to 1.0000
[2019-04-09 15:08:54,636] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8001
[2019-04-09 15:08:54,661] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 22.52996667163932, -0.1457533930854929, 0.0, 1.0, 45.0, 35.274172735074444], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4758000.0000, 
sim time next is 4759200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 22.48704528563399, -0.1577422420729492, 0.0, 1.0, 35.0, 26.411223345670983], 
processed observation next is [0.0, 0.08695652173913043, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.3739204404694991, 0.4474192526423502, 0.0, 1.0, 0.4, 0.2641122334567098], 
reward next is 0.7359, 
noisyNet noise sample is [array([0.00235526], dtype=float32), 2.8966515]. 
=============================================
[2019-04-09 15:08:54,858] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.94783555e-04 3.21025727e-03 1.66491196e-02 1.14169750e-04
 7.34199941e-01 8.30233172e-02 1.13427386e-01 3.14464862e-03
 2.62672957e-02 1.69295650e-02 2.83953943e-03], sum to 1.0000
[2019-04-09 15:08:54,861] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8223
[2019-04-09 15:08:54,911] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 69.0, 170.5, 472.5, 19.0, 21.51237761367225, -0.3368098099620429, 0.0, 1.0, 35.0, 26.837752553022916], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 4786800.0000, 
sim time next is 4788000.0000, 
raw observation next is [-3.0, 65.0, 163.5, 575.5, 19.0, 21.78274505858895, -0.3159361523688029, 0.0, 1.0, 40.0, 25.162112837480777], 
processed observation next is [0.0, 0.43478260869565216, 0.3795013850415513, 0.65, 0.545, 0.6359116022099448, 0.08333333333333333, 0.3152287548824126, 0.394687949210399, 0.0, 1.0, 0.5, 0.25162112837480777], 
reward next is 0.7484, 
noisyNet noise sample is [array([-2.1207924], dtype=float32), 1.24387]. 
=============================================
[2019-04-09 15:08:54,915] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2359958e-06 5.0613351e-05 4.4226847e-03 3.1065332e-07 9.1068083e-01
 2.0242808e-02 5.7747722e-02 1.0235265e-03 3.8345989e-03 1.7344831e-03
 2.6119934e-04], sum to 1.0000
[2019-04-09 15:08:54,917] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[16.252611]
 [16.318693]
 [16.089539]
 [15.901276]
 [15.856604]], R is [[17.01591301]
 [17.57737732]
 [18.0574379 ]
 [18.66999435]
 [19.25020218]].
[2019-04-09 15:08:54,918] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8180
[2019-04-09 15:08:54,953] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5164956e-04 3.9992905e-03 7.9283249e-03 1.0264197e-04 6.2152648e-01
 4.1531943e-02 2.9111794e-01 6.6609792e-03 1.6028620e-02 9.1513423e-03
 1.6008860e-03], sum to 1.0000
[2019-04-09 15:08:54,953] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6013
[2019-04-09 15:08:54,981] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 72.0, 123.5, 5.5, 22.5, 25.14747512447536, 0.3382731202520796, 1.0, 1.0, 35.0, 23.44956238672924], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4723200.0000, 
sim time next is 4724400.0000, 
raw observation next is [1.0, 72.0, 107.8333333333333, 9.166666666666668, 22.5, 25.2329573716688, 0.2419641824212299, 1.0, 1.0, 35.0, 22.887737704930355], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.72, 0.35944444444444434, 0.010128913443830573, 0.375, 0.6027464476390666, 0.5806547274737434, 1.0, 1.0, 0.4, 0.22887737704930355], 
reward next is 0.7711, 
noisyNet noise sample is [array([1.6841067], dtype=float32), 0.31221783]. 
=============================================
[2019-04-09 15:08:54,987] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 21.79067564138476, -0.3914952248996822, 0.0, 1.0, 45.0, 32.69215838493139], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4772400.0000, 
sim time next is 4773600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 21.71123568463658, -0.4243898349386614, 0.0, 1.0, 35.0, 29.39373047672356], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.3092696403863817, 0.35853672168711287, 0.0, 1.0, 0.4, 0.2939373047672356], 
reward next is 0.7061, 
noisyNet noise sample is [array([0.78628415], dtype=float32), 2.0611374]. 
=============================================
[2019-04-09 15:08:55,031] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 63413: loss 20.8225
[2019-04-09 15:08:55,033] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 63413: learning rate 0.0000
[2019-04-09 15:08:55,042] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5061213e-04 6.7590480e-03 2.6093559e-02 2.7829988e-04 4.7498235e-01
 5.1322032e-02 3.5824412e-01 1.1948838e-02 3.5633504e-02 2.6211036e-02
 8.1766667e-03], sum to 1.0000
[2019-04-09 15:08:55,042] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5722
[2019-04-09 15:08:55,061] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 21.2281688444688, -0.4692794743345072, 0.0, 1.0, 35.0, 23.202705183017645], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4764000.0000, 
sim time next is 4765200.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 21.13441710452808, -0.4540871348959376, 0.0, 1.0, 45.0, 42.292764375141616], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.26120142537734, 0.3486376217013541, 0.0, 1.0, 0.6, 0.42292764375141617], 
reward next is 0.5771, 
noisyNet noise sample is [array([-1.6657016], dtype=float32), -0.6092308]. 
=============================================
[2019-04-09 15:08:55,317] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63524: loss 18.0564
[2019-04-09 15:08:55,329] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 63525: learning rate 0.0000
[2019-04-09 15:08:55,343] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 63537: loss 12.8381
[2019-04-09 15:08:55,344] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 63537: learning rate 0.0000
[2019-04-09 15:08:55,511] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.10148785e-05 1.13748305e-03 5.58256730e-03 7.33591696e-06
 6.58926785e-01 1.19279318e-01 1.71399355e-01 8.12574464e-04
 3.52184735e-02 6.60781236e-03 1.01727492e-03], sum to 1.0000
[2019-04-09 15:08:55,511] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1555
[2019-04-09 15:08:55,543] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 55.33333333333334, 0.0, 0.0, 19.0, 25.22427266615999, 0.4875321081146438, 0.0, 1.0, 35.0, 18.663059037041783], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4657200.0000, 
sim time next is 4658400.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 19.0, 25.14139427132434, 0.4701465328364973, 0.0, 1.0, 35.0, 18.88579420775917], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.57, 0.0, 0.0, 0.08333333333333333, 0.5951161892770284, 0.6567155109454991, 0.0, 1.0, 0.4, 0.18885794207759168], 
reward next is 0.8111, 
noisyNet noise sample is [array([-0.16287315], dtype=float32), -1.3295301]. 
=============================================
[2019-04-09 15:08:55,634] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.3269140e-06 1.2802951e-04 3.2271170e-03 2.5210468e-06 8.0873829e-01
 3.7326969e-02 1.2833039e-01 5.7073223e-04 1.4776489e-02 6.2165787e-03
 6.7649886e-04], sum to 1.0000
[2019-04-09 15:08:55,635] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0669
[2019-04-09 15:08:55,655] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 79.33333333333333, 0.0, 0.0, 19.0, 22.55481545615891, -0.1460496720576968, 0.0, 1.0, 35.0, 22.119906224221655], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4747200.0000, 
sim time next is 4748400.0000, 
raw observation next is [-3.0, 77.0, 0.0, 0.0, 19.0, 22.40081369334378, -0.1752521649034521, 0.0, 1.0, 35.0, 22.28919700390947], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.77, 0.0, 0.0, 0.08333333333333333, 0.3667344744453149, 0.44158261169884927, 0.0, 1.0, 0.4, 0.22289197003909472], 
reward next is 0.7771, 
noisyNet noise sample is [array([0.02651845], dtype=float32), 0.18541576]. 
=============================================
[2019-04-09 15:08:55,682] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4185528e-04 2.9141926e-03 1.1100476e-02 1.4044785e-04 6.8361217e-01
 7.4591339e-02 1.6150287e-01 5.9301760e-03 4.4310465e-02 1.1235925e-02
 4.3201023e-03], sum to 1.0000
[2019-04-09 15:08:55,682] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1171
[2019-04-09 15:08:55,721] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 19.0, 20.96848570840773, -0.5798198994004143, 0.0, 1.0, 55.0, 55.444539519149956], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4861200.0000, 
sim time next is 4862400.0000, 
raw observation next is [-3.666666666666667, 63.33333333333333, 0.0, 0.0, 19.0, 21.0911752040102, -0.5597967335995832, 0.0, 1.0, 35.0, 31.5340261094673], 
processed observation next is [0.0, 0.2608695652173913, 0.3610341643582641, 0.6333333333333333, 0.0, 0.0, 0.08333333333333333, 0.25759793366751654, 0.31340108880013895, 0.0, 1.0, 0.4, 0.315340261094673], 
reward next is 0.6847, 
noisyNet noise sample is [array([-0.15721326], dtype=float32), -1.5864562]. 
=============================================
[2019-04-09 15:08:55,769] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 63713: loss 14.8570
[2019-04-09 15:08:55,772] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 63714: learning rate 0.0000
[2019-04-09 15:08:55,809] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 63730: loss 13.7578
[2019-04-09 15:08:55,810] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 63730: learning rate 0.0000
[2019-04-09 15:08:55,866] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63752: loss 18.5347
[2019-04-09 15:08:55,869] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63754: learning rate 0.0000
[2019-04-09 15:08:55,974] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1215938e-04 2.6433868e-03 1.0210870e-02 5.9472321e-05 6.1547548e-01
 4.5124453e-02 2.9053637e-01 3.7917437e-03 1.5859002e-02 1.3048408e-02
 3.1386523e-03], sum to 1.0000
[2019-04-09 15:08:55,977] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2267
[2019-04-09 15:08:56,003] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 21.51470214106813, -0.4089807838421485, 0.0, 1.0, 45.0, 35.98674898207101], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4773600.0000, 
sim time next is 4774800.0000, 
raw observation next is [-6.066666666666666, 92.33333333333334, 0.0, 0.0, 19.0, 21.44308777110703, -0.4289768057451857, 0.0, 1.0, 35.0, 27.18664757215693], 
processed observation next is [0.0, 0.2608695652173913, 0.2945521698984303, 0.9233333333333335, 0.0, 0.0, 0.08333333333333333, 0.2869239809255859, 0.3570077314182714, 0.0, 1.0, 0.4, 0.2718664757215693], 
reward next is 0.7281, 
noisyNet noise sample is [array([-0.79965883], dtype=float32), -0.13036141]. 
=============================================
[2019-04-09 15:08:56,153] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 63869: loss 13.1954
[2019-04-09 15:08:56,154] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 63869: learning rate 0.0000
[2019-04-09 15:08:56,277] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63920: loss 27.2032
[2019-04-09 15:08:56,289] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2011029e-04 1.1645448e-03 1.2817299e-02 6.9714602e-05 6.0609496e-01
 6.6759899e-02 2.6583076e-01 6.3566533e-03 2.8506139e-02 9.7464072e-03
 2.5335453e-03], sum to 1.0000
[2019-04-09 15:08:56,289] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63920: learning rate 0.0000
[2019-04-09 15:08:56,289] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2217
[2019-04-09 15:08:56,319] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 51.0, 0.0, 0.0, 19.0, 22.93892308744821, -0.1367925369970321, 0.0, 1.0, 35.0, 19.432651551170217], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4827600.0000, 
sim time next is 4828800.0000, 
raw observation next is [-0.3333333333333333, 52.33333333333334, 0.0, 0.0, 19.0, 22.85855741390755, -0.1632432498105273, 0.0, 1.0, 35.0, 17.448171936716165], 
processed observation next is [0.0, 0.9130434782608695, 0.4533702677747, 0.5233333333333334, 0.0, 0.0, 0.08333333333333333, 0.40487978449229595, 0.44558558339649085, 0.0, 1.0, 0.4, 0.17448171936716164], 
reward next is 0.8255, 
noisyNet noise sample is [array([-0.89515907], dtype=float32), 0.47325075]. 
=============================================
[2019-04-09 15:08:56,828] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6297759e-05 1.9765717e-03 8.8711325e-03 3.1964944e-05 8.6321664e-01
 3.3075582e-02 8.1555985e-02 1.4040407e-03 3.9215251e-03 4.7466070e-03
 1.1636721e-03], sum to 1.0000
[2019-04-09 15:08:56,828] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2042
[2019-04-09 15:08:56,857] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 64146: loss 12.1032
[2019-04-09 15:08:56,860] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 64147: learning rate 0.0000
[2019-04-09 15:08:56,861] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 19.0, 23.05770897366426, -0.09216949230886119, 0.0, 1.0, 35.0, 23.903161193330845], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4824000.0000, 
sim time next is 4825200.0000, 
raw observation next is [0.6666666666666667, 48.33333333333334, 0.0, 0.0, 19.0, 23.0666810046222, -0.1105632871417705, 0.0, 1.0, 35.0, 21.17897819160462], 
processed observation next is [0.0, 0.8695652173913043, 0.4810710987996307, 0.48333333333333345, 0.0, 0.0, 0.08333333333333333, 0.42222341705185, 0.46314557095274317, 0.0, 1.0, 0.4, 0.21178978191604622], 
reward next is 0.7882, 
noisyNet noise sample is [array([-0.04719809], dtype=float32), 0.9269309]. 
=============================================
[2019-04-09 15:08:56,915] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7812403e-05 3.4349418e-04 2.9762543e-03 5.4906750e-06 8.3938229e-01
 5.2813713e-02 8.8886671e-02 1.3329231e-03 1.1646385e-02 1.7882031e-03
 7.9671410e-04], sum to 1.0000
[2019-04-09 15:08:56,918] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0738
[2019-04-09 15:08:56,950] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 45.0, 112.1666666666667, 334.5, 19.0, 23.50473473556086, -0.01705935727662055, 0.0, 1.0, 35.0, 18.559206778485617], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4897200.0000, 
sim time next is 4898400.0000, 
raw observation next is [3.0, 45.0, 90.5, 295.3333333333334, 19.0, 23.6037247900716, -0.02936147644285173, 0.0, 1.0, 35.0, 17.440249857431404], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.3016666666666667, 0.3263351749539596, 0.08333333333333333, 0.46697706583929993, 0.49021284118571606, 0.0, 1.0, 0.4, 0.17440249857431403], 
reward next is 0.8256, 
noisyNet noise sample is [array([0.3162582], dtype=float32), 2.0259542]. 
=============================================
[2019-04-09 15:08:56,990] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.5397635e-05 1.2102270e-03 5.2041691e-03 3.2781409e-05 6.4454991e-01
 4.8070762e-02 2.8037524e-01 1.7740036e-03 1.2139202e-02 5.7375743e-03
 8.5080130e-04], sum to 1.0000
[2019-04-09 15:08:56,991] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6061
[2019-04-09 15:08:57,064] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.4, 45.0, 276.5, 389.0, 19.0, 22.68664197595433, -0.1577884798554703, 0.0, 1.0, 35.0, 32.090531567607385], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4885200.0000, 
sim time next is 4886400.0000, 
raw observation next is [1.6, 44.66666666666667, 273.5, 388.3333333333334, 19.0, 22.99082994562903, -0.1277250122004853, 0.0, 1.0, 35.0, 25.972441482239432], 
processed observation next is [0.0, 0.5652173913043478, 0.5069252077562327, 0.4466666666666667, 0.9116666666666666, 0.42909760589318613, 0.08333333333333333, 0.4159024954690859, 0.45742499593317154, 0.0, 1.0, 0.4, 0.25972441482239433], 
reward next is 0.7403, 
noisyNet noise sample is [array([0.4611075], dtype=float32), -0.79295635]. 
=============================================
[2019-04-09 15:08:57,132] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.1396710e-05 3.5470820e-03 1.0910933e-02 1.1389730e-04 6.8181342e-01
 4.4964436e-02 2.0743737e-01 6.6606272e-03 2.4071911e-02 1.7402431e-02
 3.0265588e-03], sum to 1.0000
[2019-04-09 15:08:57,133] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3250
[2019-04-09 15:08:57,159] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.00000000000001, 0.0, 0.0, 19.0, 22.51011550428479, -0.2922475031601558, 0.0, 1.0, 40.0, 25.205758933867237], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4839600.0000, 
sim time next is 4840800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 22.33606747693299, -0.3282205108543741, 0.0, 1.0, 35.0, 20.788796749548386], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.36133895641108243, 0.39059316304854197, 0.0, 1.0, 0.4, 0.20788796749548386], 
reward next is 0.7921, 
noisyNet noise sample is [array([1.1313446], dtype=float32), -0.83972096]. 
=============================================
[2019-04-09 15:08:57,286] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64313: loss 14.9697
[2019-04-09 15:08:57,286] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64313: learning rate 0.0000
[2019-04-09 15:08:57,445] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.5745587e-06 2.5207619e-04 6.3047609e-03 2.5640795e-06 8.2032675e-01
 5.6037106e-02 9.9807881e-02 1.0427702e-03 1.2790869e-02 2.2833198e-03
 1.1433400e-03], sum to 1.0000
[2019-04-09 15:08:57,447] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 64381: loss 15.8602
[2019-04-09 15:08:57,447] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 64381: learning rate 0.0000
[2019-04-09 15:08:57,450] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6060
[2019-04-09 15:08:57,484] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 79.33333333333334, 0.0, 0.0, 19.0, 22.52542826454157, -0.1428587381259191, 0.0, 1.0, 35.0, 21.33561904338983], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4749600.0000, 
sim time next is 4750800.0000, 
raw observation next is [-3.666666666666667, 81.66666666666667, 0.0, 0.0, 19.0, 22.36750107730916, -0.1765792141144497, 0.0, 1.0, 35.0, 21.71433005352521], 
processed observation next is [1.0, 1.0, 0.3610341643582641, 0.8166666666666668, 0.0, 0.0, 0.08333333333333333, 0.3639584231090967, 0.4411402619618501, 0.0, 1.0, 0.4, 0.2171433005352521], 
reward next is 0.7829, 
noisyNet noise sample is [array([0.29843962], dtype=float32), 0.49155802]. 
=============================================
[2019-04-09 15:08:58,044] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6091218e-06 1.4708791e-04 5.2836440e-03 3.4210134e-07 7.8352088e-01
 2.7737949e-02 1.7101294e-01 2.5383313e-04 7.3535019e-03 4.3496732e-03
 3.3748450e-04], sum to 1.0000
[2019-04-09 15:08:58,045] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2238
[2019-04-09 15:08:58,050] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.5604836e-05 1.1155034e-03 1.3094754e-02 2.4882613e-05 7.3608768e-01
 2.4075344e-02 1.8954520e-01 3.4287730e-03 2.3023831e-02 8.3534084e-03
 1.2049660e-03], sum to 1.0000
[2019-04-09 15:08:58,050] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7026
[2019-04-09 15:08:58,073] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1180840e-05 7.9854869e-04 8.9720804e-03 6.3780193e-05 7.8486079e-01
 2.0315399e-02 1.6272420e-01 2.7355603e-03 1.2420995e-02 5.9101651e-03
 1.1572479e-03], sum to 1.0000
[2019-04-09 15:08:58,074] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3739
[2019-04-09 15:08:58,079] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 19.0, 22.75167918131141, -0.2291484311405648, 0.0, 1.0, 35.0, 21.014738130148146], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4916400.0000, 
sim time next is 4917600.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 19.0, 22.71021020741528, -0.2548789387734154, 0.0, 1.0, 35.0, 19.124948896517097], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.36, 0.0, 0.0, 0.08333333333333333, 0.39251751728460665, 0.4150403537421949, 0.0, 1.0, 0.4, 0.19124948896517097], 
reward next is 0.8088, 
noisyNet noise sample is [array([0.50295544], dtype=float32), 1.8057195]. 
=============================================
[2019-04-09 15:08:58,085] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 19.0, 23.84593465967252, 0.1552928687608441, 0.0, 1.0, 35.0, 20.554888300140057], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4741200.0000, 
sim time next is 4742400.0000, 
raw observation next is [-2.333333333333333, 84.66666666666667, 0.0, 0.0, 19.0, 23.66960498174685, 0.1234949837642311, 0.0, 1.0, 35.0, 20.482754327897524], 
processed observation next is [1.0, 0.9130434782608695, 0.3979686057248385, 0.8466666666666667, 0.0, 0.0, 0.08333333333333333, 0.47246708181223757, 0.5411649945880771, 0.0, 1.0, 0.4, 0.20482754327897523], 
reward next is 0.7952, 
noisyNet noise sample is [array([0.9956994], dtype=float32), -2.4852629]. 
=============================================
[2019-04-09 15:08:58,095] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6666666666666667, 48.33333333333334, 0.0, 0.0, 19.0, 22.62027678985307, -0.2409475806863296, 0.0, 1.0, 35.0, 19.079028412343334], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4825200.0000, 
sim time next is 4826400.0000, 
raw observation next is [0.3333333333333334, 49.66666666666667, 0.0, 0.0, 19.0, 22.47142101556069, -0.2672047767827774, 0.0, 1.0, 35.0, 19.326579388319395], 
processed observation next is [0.0, 0.8695652173913043, 0.4718374884579871, 0.4966666666666667, 0.0, 0.0, 0.08333333333333333, 0.37261841796339085, 0.41093174107240754, 0.0, 1.0, 0.4, 0.19326579388319395], 
reward next is 0.8067, 
noisyNet noise sample is [array([0.47589776], dtype=float32), -1.3189715]. 
=============================================
[2019-04-09 15:08:58,150] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8145874e-05 1.4923323e-03 7.2922339e-03 1.9052295e-05 7.2073150e-01
 2.4508690e-02 2.1675666e-01 3.8789937e-03 1.5221256e-02 8.6655626e-03
 1.3955524e-03], sum to 1.0000
[2019-04-09 15:08:58,150] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7564
[2019-04-09 15:08:58,185] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 19.0, 23.38778296837731, -0.05661209880597914, 0.0, 1.0, 35.0, 16.897983916154807], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4820400.0000, 
sim time next is 4821600.0000, 
raw observation next is [1.0, 44.33333333333334, 0.0, 0.0, 19.0, 23.25125386771987, -0.03830491353357802, 0.0, 1.0, 45.0, 32.074991798765765], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4433333333333334, 0.0, 0.0, 0.08333333333333333, 0.43760448897665594, 0.48723169548880735, 0.0, 1.0, 0.6, 0.3207499179876577], 
reward next is 0.6793, 
noisyNet noise sample is [array([1.4872719], dtype=float32), -0.3508634]. 
=============================================
[2019-04-09 15:08:58,193] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9862715e-04 3.4674262e-03 8.4948987e-03 6.2789339e-05 5.7780188e-01
 5.2001845e-02 3.2191885e-01 7.0013879e-03 1.3296120e-02 1.1554588e-02
 4.2016162e-03], sum to 1.0000
[2019-04-09 15:08:58,193] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4567
[2019-04-09 15:08:58,214] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 19.0, 21.80170231334224, -0.4593552297862731, 0.0, 1.0, 35.0, 22.884137539544007], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4860000.0000, 
sim time next is 4861200.0000, 
raw observation next is [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 19.0, 21.65150742615013, -0.4957223672980033, 0.0, 1.0, 35.0, 20.79839602868564], 
processed observation next is [0.0, 0.2608695652173913, 0.37026777469990774, 0.6166666666666667, 0.0, 0.0, 0.08333333333333333, 0.30429228551251075, 0.33475921090066557, 0.0, 1.0, 0.4, 0.2079839602868564], 
reward next is 0.7920, 
noisyNet noise sample is [array([0.36156598], dtype=float32), 1.2996836]. 
=============================================
[2019-04-09 15:08:58,379] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2091398e-04 3.7878421e-03 6.7238212e-03 4.7352132e-05 7.2428286e-01
 5.1105034e-02 1.8608722e-01 4.1225045e-03 1.3173489e-02 7.9465285e-03
 2.6024347e-03], sum to 1.0000
[2019-04-09 15:08:58,386] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1228
[2019-04-09 15:08:58,422] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 253.5, 171.5, 19.0, 21.89608690425602, -0.3731080611731454, 0.0, 1.0, 35.0, 18.711405624877575], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4874400.0000, 
sim time next is 4875600.0000, 
raw observation next is [-1.466666666666667, 57.33333333333334, 284.5, 166.5, 19.0, 21.8653664143846, -0.3749618045865483, 0.0, 1.0, 35.0, 16.6214569115901], 
processed observation next is [0.0, 0.43478260869565216, 0.42197599261311175, 0.5733333333333335, 0.9483333333333334, 0.1839779005524862, 0.08333333333333333, 0.32211386786538326, 0.37501273180448386, 0.0, 1.0, 0.4, 0.16621456911590102], 
reward next is 0.8338, 
noisyNet noise sample is [array([-0.5014072], dtype=float32), -0.5648766]. 
=============================================
[2019-04-09 15:08:58,548] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5428080e-04 2.7491832e-03 1.2282820e-02 6.2861254e-05 7.3100817e-01
 3.1778552e-02 1.8243760e-01 4.3859421e-03 1.9429917e-02 1.2139147e-02
 3.5716367e-03], sum to 1.0000
[2019-04-09 15:08:58,548] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6650
[2019-04-09 15:08:58,568] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 21.16899017830166, -0.5064119302310611, 0.0, 1.0, 35.0, 21.946074337985564], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4773600.0000, 
sim time next is 4774800.0000, 
raw observation next is [-6.066666666666666, 92.33333333333334, 0.0, 0.0, 19.0, 21.01605173409551, -0.5445307525218086, 0.0, 1.0, 35.0, 23.95870070767821], 
processed observation next is [0.0, 0.2608695652173913, 0.2945521698984303, 0.9233333333333335, 0.0, 0.0, 0.08333333333333333, 0.2513376445079591, 0.31848974915939715, 0.0, 1.0, 0.4, 0.23958700707678213], 
reward next is 0.7604, 
noisyNet noise sample is [array([-0.33827895], dtype=float32), -1.3312584]. 
=============================================
[2019-04-09 15:08:58,807] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0793719e-05 4.3265861e-03 5.2744686e-03 2.5168119e-05 8.6764008e-01
 2.4013733e-02 8.2154445e-02 2.2432604e-03 8.2205525e-03 5.0440924e-03
 1.0267466e-03], sum to 1.0000
[2019-04-09 15:08:58,808] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4918
[2019-04-09 15:08:58,837] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 44.0, 254.0, 381.0, 19.0, 22.81934515036417, -0.157770191436534, 0.0, 1.0, 35.0, 14.04177886559831], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4888800.0000, 
sim time next is 4890000.0000, 
raw observation next is [2.333333333333333, 44.33333333333334, 242.0, 376.3333333333334, 19.0, 22.79184224202163, -0.1606491366803782, 0.0, 1.0, 35.0, 14.170766563754064], 
processed observation next is [0.0, 0.6086956521739131, 0.5272391505078486, 0.4433333333333334, 0.8066666666666666, 0.4158379373848988, 0.08333333333333333, 0.39932018683513587, 0.4464502877732073, 0.0, 1.0, 0.4, 0.14170766563754064], 
reward next is 0.8583, 
noisyNet noise sample is [array([-0.0363045], dtype=float32), 0.94947994]. 
=============================================
[2019-04-09 15:08:58,867] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[18.902231]
 [18.819038]
 [18.706009]
 [18.6681  ]
 [18.58378 ]], R is [[19.47817612]
 [20.14297676]
 [20.78726768]
 [21.40736771]
 [21.97991753]].
[2019-04-09 15:08:59,236] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4000, global step 65094: loss 13.1681
[2019-04-09 15:08:59,238] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4000, global step 65094: learning rate 0.0000
[2019-04-09 15:08:59,484] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 65181: loss 17.9508
[2019-04-09 15:08:59,488] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 65183: learning rate 0.0000
[2019-04-09 15:09:00,009] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 65379: loss 11.8221
[2019-04-09 15:09:00,010] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 65379: learning rate 0.0000
[2019-04-09 15:09:00,522] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7557576e-05 1.2610173e-03 8.2508270e-03 5.5708984e-05 5.8078206e-01
 2.5242370e-02 3.4251785e-01 4.1435529e-03 2.8360095e-02 7.7642105e-03
 1.5647988e-03], sum to 1.0000
[2019-04-09 15:09:00,522] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3855
[2019-04-09 15:09:00,565] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3333333333333333, 45.33333333333334, 0.0, 0.0, 19.0, 22.38319648328802, -0.3551935718685661, 0.0, 1.0, 35.0, 25.73681655536144], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4929600.0000, 
sim time next is 4930800.0000, 
raw observation next is [-0.6666666666666666, 47.66666666666666, 0.0, 0.0, 19.0, 22.36934535980718, -0.3724543511592031, 0.0, 1.0, 35.0, 22.837287756683956], 
processed observation next is [1.0, 0.043478260869565216, 0.44413665743305636, 0.47666666666666657, 0.0, 0.0, 0.08333333333333333, 0.36411211331726506, 0.375848549613599, 0.0, 1.0, 0.4, 0.22837287756683955], 
reward next is 0.7716, 
noisyNet noise sample is [array([1.0153084], dtype=float32), -1.118609]. 
=============================================
[2019-04-09 15:09:00,830] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7873580e-08 5.0224065e-05 7.2276493e-04 1.4915572e-07 8.8542563e-01
 3.8932044e-02 6.6132344e-02 6.0420058e-05 6.9622966e-03 1.6048597e-03
 1.0918729e-04], sum to 1.0000
[2019-04-09 15:09:00,830] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6734
[2019-04-09 15:09:00,873] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.0, 26.0, 110.3333333333333, 825.8333333333333, 22.5, 27.16475252783068, 0.733454614396969, 1.0, 1.0, 35.0, 15.60381484742571], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4976400.0000, 
sim time next is 4977600.0000, 
raw observation next is [8.0, 26.0, 106.1666666666667, 811.5, 22.5, 27.43823111446029, 0.7813646320240778, 1.0, 1.0, 35.0, 13.169893172377245], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.353888888888889, 0.8966850828729281, 0.375, 0.7865192595383576, 0.7604548773413593, 1.0, 1.0, 0.4, 0.13169893172377245], 
reward next is 0.8683, 
noisyNet noise sample is [array([-0.86069965], dtype=float32), 0.69374734]. 
=============================================
[2019-04-09 15:09:01,550] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.1160935e-04 3.1121639e-03 2.1244161e-02 3.9870756e-05 6.8479401e-01
 4.8072714e-02 2.0135623e-01 2.8397555e-03 2.3249291e-02 1.0666565e-02
 4.5137047e-03], sum to 1.0000
[2019-04-09 15:09:01,550] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1464
[2019-04-09 15:09:01,614] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 67.0, 0.0, 0.0, 19.0, 21.68580971764481, -0.5171096007086156, 0.0, 1.0, 35.0, 26.758273341160837], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4864800.0000, 
sim time next is 4866000.0000, 
raw observation next is [-4.0, 69.0, 23.5, 52.16666666666666, 19.0, 21.51556017905665, -0.5458201569648411, 0.0, 1.0, 35.0, 24.195642261709907], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.69, 0.07833333333333334, 0.05764272559852669, 0.08333333333333333, 0.29296334825472076, 0.3180599476783863, 0.0, 1.0, 0.4, 0.24195642261709907], 
reward next is 0.7580, 
noisyNet noise sample is [array([1.0154128], dtype=float32), 0.41071272]. 
=============================================
[2019-04-09 15:09:01,635] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[15.718156]
 [15.983429]
 [15.911292]
 [15.5941  ]
 [15.673741]], R is [[16.62333488]
 [17.18951988]
 [17.72069359]
 [18.21294785]
 [18.63326263]].
[2019-04-09 15:09:02,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8021196e-07 7.0397131e-05 6.7823811e-04 3.1377519e-07 8.9399469e-01
 2.7993487e-02 7.2324798e-02 3.0675798e-04 3.3707197e-03 7.6656480e-04
 4.9372885e-04], sum to 1.0000
[2019-04-09 15:09:02,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5945
[2019-04-09 15:09:02,140] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.666666666666667, 29.66666666666667, 115.5, 788.6666666666667, 22.5, 25.12740335398964, 0.2145428336738365, 1.0, 1.0, 35.0, 26.242069117877598], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4962000.0000, 
sim time next is 4963200.0000, 
raw observation next is [2.333333333333333, 29.33333333333334, 117.8333333333333, 810.0, 22.5, 25.34074987060969, 0.2625564136016809, 1.0, 1.0, 35.0, 21.280924006918855], 
processed observation next is [1.0, 0.43478260869565216, 0.5272391505078486, 0.2933333333333334, 0.39277777777777767, 0.8950276243093923, 0.375, 0.611729155884141, 0.5875188045338936, 1.0, 1.0, 0.4, 0.21280924006918855], 
reward next is 0.7872, 
noisyNet noise sample is [array([1.4070541], dtype=float32), -0.40757763]. 
=============================================
[2019-04-09 15:09:02,306] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.0816980e-05 4.7974358e-04 2.4469299e-03 1.6471906e-05 9.1474092e-01
 2.3342559e-02 4.8449762e-02 9.2104572e-04 6.7741815e-03 1.9339890e-03
 8.6356065e-04], sum to 1.0000
[2019-04-09 15:09:02,306] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1211
[2019-04-09 15:09:02,432] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5333333333333332, 48.66666666666667, 282.6666666666667, 321.6666666666667, 19.0, 22.57252913327235, -0.246093476820636, 0.0, 1.0, 35.0, 22.680726257998], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4880400.0000, 
sim time next is 4881600.0000, 
raw observation next is [1.0, 47.0, 282.0, 349.0, 19.0, 22.53412438591339, -0.2522260666218706, 0.0, 1.0, 35.0, 20.86234459351199], 
processed observation next is [0.0, 0.5217391304347826, 0.4903047091412743, 0.47, 0.94, 0.3856353591160221, 0.08333333333333333, 0.37784369882611585, 0.4159246444593765, 0.0, 1.0, 0.4, 0.20862344593511992], 
reward next is 0.7914, 
noisyNet noise sample is [array([-0.40587533], dtype=float32), 1.1309115]. 
=============================================
[2019-04-09 15:09:02,628] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.5523634e-05 1.3344550e-03 6.4142477e-03 7.8906323e-06 7.2699869e-01
 6.1221469e-02 1.7369647e-01 1.6381254e-03 2.1272670e-02 6.3211666e-03
 1.0792348e-03], sum to 1.0000
[2019-04-09 15:09:02,628] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8124
[2019-04-09 15:09:02,671] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.266666666666667, 45.66666666666667, 279.5, 389.6666666666666, 19.0, 22.5381623441479, -0.2465018722144061, 0.0, 1.0, 35.0, 15.597170253693129], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4884000.0000, 
sim time next is 4885200.0000, 
raw observation next is [1.4, 45.0, 276.5, 389.0, 19.0, 22.50576705016976, -0.2140947119650593, 0.0, 1.0, 45.0, 28.78610022384421], 
processed observation next is [0.0, 0.5652173913043478, 0.5013850415512465, 0.45, 0.9216666666666666, 0.4298342541436464, 0.08333333333333333, 0.37548058751414654, 0.4286350960116469, 0.0, 1.0, 0.6, 0.2878610022384421], 
reward next is 0.7121, 
noisyNet noise sample is [array([0.27069256], dtype=float32), -0.10342486]. 
=============================================
[2019-04-09 15:09:02,900] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.1101813e-05 1.7270677e-04 3.0143370e-03 2.2554011e-06 8.0893105e-01
 2.3394056e-02 1.5279083e-01 8.4474497e-04 6.0922010e-03 4.1645044e-03
 5.8212603e-04], sum to 1.0000
[2019-04-09 15:09:02,900] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7567
[2019-04-09 15:09:02,928] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 19.0, 24.61370488469899, 0.2131140968874108, 0.0, 1.0, 35.0, 18.460527039839917], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 5014800.0000, 
sim time next is 5016000.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 19.0, 24.51074844774974, 0.1905998786547407, 0.0, 1.0, 35.0, 18.678638525395243], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.4, 0.0, 0.0, 0.08333333333333333, 0.5425623706458117, 0.5635332928849136, 0.0, 1.0, 0.4, 0.18678638525395244], 
reward next is 0.8132, 
noisyNet noise sample is [array([-3.0576015], dtype=float32), 1.7795622]. 
=============================================
[2019-04-09 15:09:02,939] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[22.979662]
 [23.279795]
 [23.972715]
 [24.625648]
 [24.916452]], R is [[23.32168579]
 [23.90386391]
 [24.48209381]
 [25.0550766 ]
 [25.61720276]].
[2019-04-09 15:09:03,505] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.9054214e-07 1.7809767e-05 2.3631668e-03 1.0833448e-07 5.8817965e-01
 6.9571204e-02 3.3466220e-01 6.4647451e-05 4.3732170e-03 6.9223868e-04
 7.5349220e-05], sum to 1.0000
[2019-04-09 15:09:03,505] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6857
[2019-04-09 15:09:03,553] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2012293e-07 7.0541188e-05 8.7198376e-04 5.4361447e-08 7.8571707e-01
 1.6468866e-02 1.9219160e-01 1.8706382e-04 3.3604202e-03 3.4140260e-04
 7.9068454e-04], sum to 1.0000
[2019-04-09 15:09:03,553] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7691
[2019-04-09 15:09:03,604] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.0, 26.0, 100.5, 796.5, 22.5, 26.99201420000729, 0.7567771293083211, 1.0, 1.0, 35.0, 15.05398315975468], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4978800.0000, 
sim time next is 4980000.0000, 
raw observation next is [8.333333333333334, 25.66666666666667, 94.83333333333334, 781.5, 22.5, 27.51840134786647, 0.8795523251457729, 1.0, 1.0, 45.0, 32.55501117972429], 
processed observation next is [1.0, 0.6521739130434783, 0.6934441366574331, 0.2566666666666667, 0.3161111111111111, 0.86353591160221, 0.375, 0.7932001123222058, 0.7931841083819243, 1.0, 1.0, 0.6, 0.3255501117972429], 
reward next is 0.6744, 
noisyNet noise sample is [array([-0.58274513], dtype=float32), -0.37740496]. 
=============================================
[2019-04-09 15:09:03,607] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[29.579563]
 [29.377455]
 [29.420444]
 [29.268793]
 [28.706688]], R is [[30.03193665]
 [30.58107758]
 [31.09747505]
 [31.58431053]
 [31.92815018]].
[2019-04-09 15:09:03,610] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 25.0, 17.0, 152.0, 22.5, 27.32191112563714, 0.8274495156299942, 1.0, 1.0, 35.0, 12.229644312381705], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4989600.0000, 
sim time next is 4990800.0000, 
raw observation next is [6.0, 24.33333333333334, 0.0, 0.0, 22.5, 27.38580841863062, 0.830644417327426, 1.0, 1.0, 35.0, 10.856558623813426], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2433333333333334, 0.0, 0.0, 0.375, 0.7821507015525517, 0.7768814724424753, 1.0, 1.0, 0.4, 0.10856558623813425], 
reward next is 0.8914, 
noisyNet noise sample is [array([0.73293847], dtype=float32), -0.30656773]. 
=============================================
[2019-04-09 15:09:03,813] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6083615e-06 2.3323366e-04 5.8954484e-03 9.7375835e-07 7.8299677e-01
 6.4099997e-02 1.2470605e-01 8.1673276e-04 1.6789718e-02 3.9839218e-03
 4.7461834e-04], sum to 1.0000
[2019-04-09 15:09:03,813] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2392
[2019-04-09 15:09:03,866] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.3698111e-05 8.0769893e-04 2.8600534e-03 8.3937075e-06 8.2316774e-01
 1.8748723e-02 1.2720348e-01 1.6698176e-03 2.2400293e-02 2.1744510e-03
 9.4548619e-04], sum to 1.0000
[2019-04-09 15:09:03,866] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0442
[2019-04-09 15:09:03,884] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 19.0, 22.78220732433221, -0.2380912361642324, 0.0, 1.0, 35.0, 16.21522018672629], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4916400.0000, 
sim time next is 4917600.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 19.0, 22.64202769679689, -0.2678633129722948, 0.0, 1.0, 35.0, 18.628677951673858], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.36, 0.0, 0.0, 0.08333333333333333, 0.3868356413997409, 0.41071222900923504, 0.0, 1.0, 0.4, 0.18628677951673858], 
reward next is 0.8137, 
noisyNet noise sample is [array([1.5518291], dtype=float32), -1.3321505]. 
=============================================
[2019-04-09 15:09:03,911] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3333333333333334, 36.0, 105.5, 690.8333333333333, 22.5, 23.575606841801, -0.06837941996796895, 1.0, 1.0, 45.0, 45.86541780893765], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4958400.0000, 
sim time next is 4959600.0000, 
raw observation next is [0.3333333333333333, 33.0, 109.5, 731.3333333333333, 22.5, 24.23588629696261, 0.02785646890132705, 1.0, 1.0, 35.0, 27.628528912662965], 
processed observation next is [1.0, 0.391304347826087, 0.4718374884579871, 0.33, 0.365, 0.8081031307550643, 0.375, 0.5196571914135509, 0.5092854896337756, 1.0, 1.0, 0.4, 0.27628528912662964], 
reward next is 0.7237, 
noisyNet noise sample is [array([1.101386], dtype=float32), -0.5123186]. 
=============================================
[2019-04-09 15:09:03,939] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.4377920e-07 1.6109034e-04 4.2896546e-04 4.2729206e-07 7.5202876e-01
 4.0880542e-02 1.9371286e-01 6.0677278e-04 9.4286995e-03 1.6479910e-03
 1.1031012e-03], sum to 1.0000
[2019-04-09 15:09:03,940] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1535
[2019-04-09 15:09:03,987] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 22.5, 26.25813312639529, 0.638974485821179, 1.0, 1.0, 35.0, 17.67087608541291], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4995600.0000, 
sim time next is 4996800.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 22.5, 26.27501190930411, 0.6368102337041538, 0.0, 1.0, 35.0, 20.67847439673622], 
processed observation next is [1.0, 0.8695652173913043, 0.6288088642659281, 0.23, 0.0, 0.0, 0.375, 0.6895843257753423, 0.7122700779013846, 0.0, 1.0, 0.4, 0.2067847439673622], 
reward next is 0.7932, 
noisyNet noise sample is [array([0.694735], dtype=float32), 1.0625553]. 
=============================================
[2019-04-09 15:09:04,037] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.1510228e-06 2.1495177e-04 1.4901378e-03 4.4403705e-06 8.6428726e-01
 1.5783619e-02 1.1290075e-01 5.6186202e-04 2.9338230e-03 1.5492735e-03
 2.6866462e-04], sum to 1.0000
[2019-04-09 15:09:04,038] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8149
[2019-04-09 15:09:04,082] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 52.33333333333334, 0.0, 0.0, 19.0, 24.03706383730269, 0.07015382030055356, 0.0, 1.0, 35.0, 18.15798619569917], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 5034000.0000, 
sim time next is 5035200.0000, 
raw observation next is [-2.333333333333333, 58.66666666666666, 0.0, 0.0, 19.0, 23.81619737734873, 0.03526078135147195, 0.0, 1.0, 35.0, 18.447888870551857], 
processed observation next is [1.0, 0.2608695652173913, 0.3979686057248385, 0.5866666666666666, 0.0, 0.0, 0.08333333333333333, 0.48468311477906073, 0.511753593783824, 0.0, 1.0, 0.4, 0.18447888870551857], 
reward next is 0.8155, 
noisyNet noise sample is [array([0.5309168], dtype=float32), -1.1590981]. 
=============================================
[2019-04-09 15:09:04,310] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.1463018e-05 5.5666774e-04 2.6196891e-03 3.2417865e-05 7.5132668e-01
 2.9143009e-02 1.8353485e-01 1.6293616e-03 2.1845063e-02 8.5589048e-03
 7.3180132e-04], sum to 1.0000
[2019-04-09 15:09:04,310] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2829
[2019-04-09 15:09:04,349] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333334, 42.0, 0.0, 0.0, 19.0, 22.83748059415543, -0.208348263225901, 0.0, 1.0, 35.0, 30.209907545244366], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4927200.0000, 
sim time next is 4928400.0000, 
raw observation next is [0.0, 43.0, 0.0, 0.0, 19.0, 23.09985015269638, -0.2065533206279881, 0.0, 1.0, 35.0, 25.271554897978035], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.43, 0.0, 0.0, 0.08333333333333333, 0.4249875127246983, 0.43114889312400395, 0.0, 1.0, 0.4, 0.2527155489797803], 
reward next is 0.7473, 
noisyNet noise sample is [array([-1.3593724], dtype=float32), 1.6910886]. 
=============================================
[2019-04-09 15:09:04,788] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0214519e-07 2.5210515e-05 3.6606472e-04 8.3813617e-08 8.9181679e-01
 1.7837347e-02 8.6964443e-02 1.6364988e-04 2.1358163e-03 6.3890906e-04
 5.1565090e-05], sum to 1.0000
[2019-04-09 15:09:04,789] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5170
[2019-04-09 15:09:04,881] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 25.0, 17.0, 152.0, 22.5, 27.76674829059198, 0.8997566686612407, 1.0, 1.0, 35.0, 16.887807901763356], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4989600.0000, 
sim time next is 4990800.0000, 
raw observation next is [6.0, 24.33333333333334, 0.0, 0.0, 22.5, 27.80284028605089, 0.8934926274506885, 1.0, 1.0, 35.0, 14.942068627788835], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2433333333333334, 0.0, 0.0, 0.375, 0.8169033571709076, 0.7978308758168962, 1.0, 1.0, 0.4, 0.14942068627788835], 
reward next is 0.8506, 
noisyNet noise sample is [array([0.28297782], dtype=float32), -0.3406471]. 
=============================================
[2019-04-09 15:09:04,908] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0187576e-06 3.0217414e-05 1.9225163e-03 2.4421755e-07 7.9037505e-01
 2.5423812e-02 1.7304276e-01 2.7671483e-04 8.0510294e-03 6.3893199e-04
 2.3760360e-04], sum to 1.0000
[2019-04-09 15:09:04,909] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8260
[2019-04-09 15:09:04,939] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3328435e-07 7.3364012e-05 1.6234004e-03 4.1169145e-07 7.2452903e-01
 3.4127641e-02 2.1874236e-01 4.0316686e-04 1.9379316e-02 9.7235572e-04
 1.4875332e-04], sum to 1.0000
[2019-04-09 15:09:04,939] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2372
[2019-04-09 15:09:04,954] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333333, 25.0, 0.0, 0.0, 19.0, 26.44905814975145, 0.6807574996345415, 0.0, 1.0, 35.0, 18.89602114826361], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4998000.0000, 
sim time next is 4999200.0000, 
raw observation next is [4.666666666666666, 27.0, 0.0, 0.0, 19.0, 26.45061387094186, 0.7069385150108585, 0.0, 1.0, 45.0, 28.488522369834214], 
processed observation next is [1.0, 0.8695652173913043, 0.5918744228993538, 0.27, 0.0, 0.0, 0.08333333333333333, 0.7042178225784884, 0.7356461716702861, 0.0, 1.0, 0.6, 0.2848852236983421], 
reward next is 0.7151, 
noisyNet noise sample is [array([1.0872701], dtype=float32), -0.53162754]. 
=============================================
[2019-04-09 15:09:05,016] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 22.5, 27.28486047470679, 0.8626097199795946, 1.0, 1.0, 35.0, 22.308677420361988], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4995600.0000, 
sim time next is 4996800.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 22.5, 27.33973450144648, 0.855907981828735, 0.0, 1.0, 35.0, 19.499234644212677], 
processed observation next is [1.0, 0.8695652173913043, 0.6288088642659281, 0.23, 0.0, 0.0, 0.375, 0.7783112084538732, 0.7853026606095783, 0.0, 1.0, 0.4, 0.19499234644212676], 
reward next is 0.8050, 
noisyNet noise sample is [array([1.1064306], dtype=float32), -0.041562192]. 
=============================================
[2019-04-09 15:09:05,132] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3036237e-05 1.0772867e-03 2.3491383e-03 3.8964640e-06 7.3230070e-01
 3.1938296e-02 1.9541468e-01 2.1181547e-03 2.6014354e-02 6.8906476e-03
 1.8797209e-03], sum to 1.0000
[2019-04-09 15:09:05,136] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9840
[2019-04-09 15:09:05,194] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 19.0, 24.88197606256927, 0.2553905757108841, 0.0, 1.0, 35.0, 23.334188610859982], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 5024400.0000, 
sim time next is 5025600.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 19.0, 24.79098027944581, 0.2716081270723003, 0.0, 1.0, 55.0, 50.00332243587021], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.55, 0.0, 0.0, 0.08333333333333333, 0.5659150232871509, 0.5905360423574334, 0.0, 1.0, 0.8, 0.5000332243587021], 
reward next is 0.5000, 
noisyNet noise sample is [array([-0.14263912], dtype=float32), 0.32099313]. 
=============================================
[2019-04-09 15:09:05,356] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7869635e-07 5.0393348e-05 9.7228348e-04 5.8123398e-08 9.1586840e-01
 4.0995780e-02 3.9855026e-02 7.3756913e-05 1.8732713e-03 1.9383275e-04
 1.1685837e-04], sum to 1.0000
[2019-04-09 15:09:05,357] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8305
[2019-04-09 15:09:05,387] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.1505008e-07 1.2278759e-04 5.8885911e-03 2.7775530e-07 7.9631048e-01
 1.3114723e-02 1.7933173e-01 1.8772345e-04 3.8328867e-03 1.0822640e-03
 1.2778951e-04], sum to 1.0000
[2019-04-09 15:09:05,387] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4057
[2019-04-09 15:09:05,411] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.333333333333334, 25.66666666666666, 65.66666666666667, 584.8333333333334, 22.5, 27.69656747437479, 0.866832264180409, 1.0, 1.0, 35.0, 14.34259604033217], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4984800.0000, 
sim time next is 4986000.0000, 
raw observation next is [8.0, 26.0, 53.0, 472.5, 22.5, 27.68666193296986, 0.8538737746327798, 1.0, 1.0, 35.0, 12.927734259576525], 
processed observation next is [1.0, 0.7391304347826086, 0.6842105263157896, 0.26, 0.17666666666666667, 0.5220994475138122, 0.375, 0.8072218277474882, 0.7846245915442599, 1.0, 1.0, 0.4, 0.12927734259576526], 
reward next is 0.8707, 
noisyNet noise sample is [array([-0.65167457], dtype=float32), 0.26525843]. 
=============================================
[2019-04-09 15:09:05,413] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[28.974163]
 [28.806581]
 [28.816767]
 [28.782537]
 [28.744392]], R is [[29.56838036]
 [30.12927246]
 [30.73171425]
 [31.30705452]
 [31.86091614]].
[2019-04-09 15:09:05,419] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.0, 29.0, 0.0, 0.0, 19.0, 26.41040170362828, 0.7001203987591694, 0.0, 1.0, 45.0, 29.07777250574027], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 5000400.0000, 
sim time next is 5001600.0000, 
raw observation next is [3.666666666666667, 31.66666666666667, 0.0, 0.0, 19.0, 26.41349876234047, 0.6945010348711117, 0.0, 1.0, 35.0, 20.823428107465567], 
processed observation next is [1.0, 0.9130434782608695, 0.564173591874423, 0.3166666666666667, 0.0, 0.0, 0.08333333333333333, 0.7011248968617059, 0.7315003449570372, 0.0, 1.0, 0.4, 0.20823428107465566], 
reward next is 0.7918, 
noisyNet noise sample is [array([-0.42379352], dtype=float32), -0.75141686]. 
=============================================
[2019-04-09 15:09:05,746] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.9241659e-08 2.2904815e-05 6.5244990e-04 2.6987886e-08 8.1293505e-01
 6.8328222e-03 1.7740570e-01 8.2854982e-05 1.5973251e-03 3.9229865e-04
 7.8452846e-05], sum to 1.0000
[2019-04-09 15:09:05,746] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7065
[2019-04-09 15:09:05,786] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.0, 26.0, 53.0, 472.5, 22.5, 27.71161817477888, 0.8883111459052495, 1.0, 1.0, 35.0, 12.548471774774594], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4986000.0000, 
sim time next is 4987200.0000, 
raw observation next is [7.333333333333334, 25.66666666666667, 40.33333333333333, 360.1666666666666, 22.5, 27.87899509109675, 0.8313007204680437, 1.0, 1.0, 35.0, 16.142810368380736], 
processed observation next is [1.0, 0.7391304347826086, 0.6657433056325024, 0.2566666666666667, 0.13444444444444442, 0.3979742173112338, 0.375, 0.8232495909247293, 0.7771002401560145, 1.0, 1.0, 0.4, 0.16142810368380736], 
reward next is 0.8386, 
noisyNet noise sample is [array([0.6464308], dtype=float32), 0.37056714]. 
=============================================
[2019-04-09 15:09:05,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.41698984e-07 1.74169945e-05 9.82557074e-04 1.76588742e-07
 8.09203207e-01 1.24671105e-02 1.74098641e-01 2.94725818e-04
 2.16902560e-03 6.60369522e-04 1.06550091e-04], sum to 1.0000
[2019-04-09 15:09:05,821] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7567341e-06 2.5109042e-04 3.6907133e-03 1.4553635e-06 8.1615776e-01
 1.0486686e-02 1.6084461e-01 1.2978060e-03 4.1724630e-03 2.8763169e-03
 2.1833555e-04], sum to 1.0000
[2019-04-09 15:09:05,822] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8855
[2019-04-09 15:09:05,823] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5383
[2019-04-09 15:09:05,844] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 19.0, 25.38859274841004, 0.378844457505415, 0.0, 1.0, 35.0, 18.132960954972617], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 5017200.0000, 
sim time next is 5018400.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 19.0, 25.24641104543858, 0.3426056228817635, 0.0, 1.0, 35.0, 16.4945284126404], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.4, 0.0, 0.0, 0.08333333333333333, 0.6038675871198818, 0.6142018742939211, 0.0, 1.0, 0.4, 0.164945284126404], 
reward next is 0.8351, 
noisyNet noise sample is [array([0.5687559], dtype=float32), -0.26649326]. 
=============================================
[2019-04-09 15:09:05,910] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 24.33333333333334, 0.0, 0.0, 22.5, 27.749448626824, 0.8775746818148895, 1.0, 1.0, 35.0, 16.20221787723774], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4990800.0000, 
sim time next is 4992000.0000, 
raw observation next is [6.0, 23.66666666666666, 0.0, 0.0, 22.5, 27.62217074277138, 0.8992767893739521, 1.0, 1.0, 45.0, 33.496118424223965], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2366666666666666, 0.0, 0.0, 0.375, 0.8018475618976151, 0.7997589297913174, 1.0, 1.0, 0.6, 0.33496118424223964], 
reward next is 0.6650, 
noisyNet noise sample is [array([-0.08928668], dtype=float32), -0.48730057]. 
=============================================
[2019-04-09 15:09:05,918] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[29.949627]
 [29.987675]
 [30.178308]
 [30.188404]
 [29.94632 ]], R is [[30.16701126]
 [30.70331955]
 [31.21239471]
 [31.68625259]
 [31.98762894]].
[2019-04-09 15:09:06,174] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.6429517e-06 1.8479410e-04 1.2213079e-03 1.1382945e-06 7.9438651e-01
 2.9738329e-02 1.6922371e-01 3.4954736e-04 2.5533284e-03 1.3184472e-03
 1.0213255e-03], sum to 1.0000
[2019-04-09 15:09:06,174] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3032
[2019-04-09 15:09:06,282] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 30.0, 112.5, 760.0, 22.5, 24.68685760720906, 0.1052936718541513, 1.0, 1.0, 35.0, 22.691789873440385], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4960800.0000, 
sim time next is 4962000.0000, 
raw observation next is [1.666666666666667, 29.66666666666667, 115.5, 788.6666666666667, 22.5, 24.87749636967347, 0.1401783521805159, 1.0, 1.0, 35.0, 19.029067788876656], 
processed observation next is [1.0, 0.43478260869565216, 0.5087719298245615, 0.2966666666666667, 0.385, 0.8714548802946593, 0.375, 0.5731246974727892, 0.5467261173935053, 1.0, 1.0, 0.4, 0.19029067788876655], 
reward next is 0.8097, 
noisyNet noise sample is [array([-1.0488971], dtype=float32), 0.85660523]. 
=============================================
[2019-04-09 15:09:06,288] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[25.31122 ]
 [24.861572]
 [24.539894]
 [23.288212]
 [22.63595 ]], R is [[26.75609589]
 [27.26161766]
 [27.71497726]
 [27.97421074]
 [28.47128677]].
[2019-04-09 15:09:06,440] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:06,448] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-8-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:06,621] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:06,654] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-8-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:06,987] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.89422882e-07 8.39676522e-06 2.48075608e-04 2.04987902e-08
 8.98112416e-01 6.63825637e-03 9.31268558e-02 1.17232936e-04
 1.23004487e-03 4.38346615e-04 7.96569439e-05], sum to 1.0000
[2019-04-09 15:09:06,990] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9225
[2019-04-09 15:09:07,123] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666666, 24.33333333333334, 122.0, 864.1666666666667, 22.5, 26.22849265408886, 0.3915132419631111, 1.0, 1.0, 35.0, 16.60110928020898], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4970400.0000, 
sim time next is 4971600.0000, 
raw observation next is [7.0, 24.0, 120.0, 862.5, 22.5, 25.1998291604377, 0.4790055222628301, 1.0, 1.0, 45.0, 38.02517849232626], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 0.24, 0.4, 0.9530386740331491, 0.375, 0.5999857633698085, 0.6596685074209434, 1.0, 1.0, 0.6, 0.3802517849232626], 
reward next is 0.6197, 
noisyNet noise sample is [array([-0.12064666], dtype=float32), -0.61977094]. 
=============================================
[2019-04-09 15:09:07,223] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5578471e-05 4.4287628e-04 4.8101353e-03 3.0197248e-06 6.4010519e-01
 5.2859519e-02 2.8175578e-01 2.1372263e-03 9.9178115e-03 7.5089233e-03
 4.4404311e-04], sum to 1.0000
[2019-04-09 15:09:07,223] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9847
[2019-04-09 15:09:07,274] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 19.0, 24.93655763785073, 0.328096371326477, 0.0, 1.0, 45.0, 32.43735984898149], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 5017200.0000, 
sim time next is 5018400.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 19.0, 24.99872987686613, 0.3881782413700334, 0.0, 1.0, 55.0, 54.457583234713084], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.4, 0.0, 0.0, 0.08333333333333333, 0.5832274897388441, 0.6293927471233445, 0.0, 1.0, 0.8, 0.5445758323471308], 
reward next is 0.4554, 
noisyNet noise sample is [array([0.37234354], dtype=float32), 1.0607512]. 
=============================================
[2019-04-09 15:09:07,394] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9677648e-07 4.5533267e-05 1.6210601e-03 1.4815002e-07 6.8342847e-01
 7.6535262e-02 2.3465453e-01 6.0843292e-04 1.7569227e-03 1.1366204e-03
 2.1286143e-04], sum to 1.0000
[2019-04-09 15:09:07,394] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0189
[2019-04-09 15:09:07,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.6036507e-08 2.0774534e-05 1.7492619e-04 2.1216994e-08 9.6299332e-01
 5.9534064e-03 2.9727776e-02 2.2502722e-05 9.8778552e-04 9.8083627e-05
 2.1285978e-05], sum to 1.0000
[2019-04-09 15:09:07,414] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8961
[2019-04-09 15:09:07,442] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:07,442] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:07,443] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res11/Eplus-env-sub_run2
[2019-04-09 15:09:07,459] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:07,462] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:07,464] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.333333333333334, 25.66666666666667, 123.8333333333333, 861.6666666666667, 22.5, 26.34780505367546, 0.6221593330306227, 1.0, 1.0, 35.0, 9.57819958987628], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 5055600.0000, 
sim time next is 5056800.0000, 
raw observation next is [8.666666666666668, 25.33333333333333, 123.0, 864.1666666666667, 22.5, 26.47046917885356, 0.6586105635223587, 1.0, 1.0, 35.0, 9.795126271512533], 
processed observation next is [1.0, 0.5217391304347826, 0.7026777469990768, 0.2533333333333333, 0.41, 0.9548802946593002, 0.375, 0.7058724315711299, 0.719536854507453, 1.0, 1.0, 0.4, 0.09795126271512533], 
reward next is 0.9020, 
noisyNet noise sample is [array([-0.5235112], dtype=float32), -0.34206507]. 
=============================================
[2019-04-09 15:09:07,469] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.0, 26.0, 113.0, 839.5, 22.5, 26.31005852427097, 0.5785166922156915, 1.0, 1.0, 35.0, 22.448792057011925], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4975200.0000, 
sim time next is 4976400.0000, 
raw observation next is [8.0, 26.0, 110.3333333333333, 825.8333333333333, 22.5, 26.91626522373399, 0.6526184068362024, 1.0, 1.0, 35.0, 17.689952553804986], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.36777777777777765, 0.9125230202578268, 0.375, 0.7430221019778326, 0.7175394689454008, 1.0, 1.0, 0.4, 0.17689952553804986], 
reward next is 0.8231, 
noisyNet noise sample is [array([-0.2337859], dtype=float32), 0.13270621]. 
=============================================
[2019-04-09 15:09:07,482] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res8/Eplus-env-sub_run2
[2019-04-09 15:09:07,539] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.1079861e-06 4.0934412e-04 6.8908418e-03 1.6665875e-06 8.1996858e-01
 3.3695012e-02 1.2783663e-01 8.5134816e-04 3.7919625e-03 5.9637227e-03
 5.8279792e-04], sum to 1.0000
[2019-04-09 15:09:07,541] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0830
[2019-04-09 15:09:07,572] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 51.66666666666666, 0.0, 0.0, 19.0, 24.23752418788935, 0.1131325772520888, 0.0, 1.0, 35.0, 17.091333051845503], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 5028000.0000, 
sim time next is 5029200.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 19.0, 24.08231185330049, 0.08516275358977536, 0.0, 1.0, 35.0, 19.40690699434132], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 0.5, 0.0, 0.0, 0.08333333333333333, 0.5068593211083741, 0.5283875845299252, 0.0, 1.0, 0.4, 0.1940690699434132], 
reward next is 0.8059, 
noisyNet noise sample is [array([1.0837988], dtype=float32), -0.39516333]. 
=============================================
[2019-04-09 15:09:07,734] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:07,803] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2811287e-07 3.6208858e-06 1.5228668e-03 2.0811466e-08 9.5136946e-01
 1.3604324e-02 3.2223217e-02 4.2388256e-05 1.0281758e-03 1.9467623e-04
 1.1095242e-05], sum to 1.0000
[2019-04-09 15:09:07,827] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3064
[2019-04-09 15:09:07,870] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.0, 19.0, 103.5, 782.0, 22.5, 27.00660997882465, 0.9020481239714918, 1.0, 1.0, 35.0, 11.91897321830998], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 5065200.0000, 
sim time next is 5066400.0000, 
raw observation next is [12.0, 19.0, 98.5, 757.3333333333334, 22.5, 27.93071182470207, 0.9938913464088733, 1.0, 1.0, 35.0, 8.562081700562558], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.19, 0.3283333333333333, 0.8368324125230203, 0.375, 0.8275593187251724, 0.8312971154696244, 1.0, 1.0, 0.4, 0.08562081700562559], 
reward next is 0.9144, 
noisyNet noise sample is [array([-0.05158889], dtype=float32), -0.4098384]. 
=============================================
[2019-04-09 15:09:07,907] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:07,926] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:07,968] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:08,003] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9341244e-06 1.1706149e-04 1.4317263e-03 8.2044363e-07 7.6038671e-01
 1.6419414e-02 2.1704061e-01 2.8315134e-04 3.2199267e-03 7.7635731e-04
 3.2032683e-04], sum to 1.0000
[2019-04-09 15:09:08,003] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2166
[2019-04-09 15:09:08,031] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 47.0, 104.5, 615.5, 22.5, 24.64689799330519, 0.2005673877360959, 1.0, 1.0, 35.0, 19.214835996636403], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 5043600.0000, 
sim time next is 5044800.0000, 
raw observation next is [1.666666666666667, 45.0, 109.5, 670.5, 22.5, 24.81307146605178, 0.2548486049559116, 1.0, 1.0, 35.0, 16.951608513260194], 
processed observation next is [1.0, 0.391304347826087, 0.5087719298245615, 0.45, 0.365, 0.7408839779005525, 0.375, 0.5677559555043151, 0.5849495349853039, 1.0, 1.0, 0.4, 0.16951608513260194], 
reward next is 0.8305, 
noisyNet noise sample is [array([-1.1923901], dtype=float32), -0.90420395]. 
=============================================
[2019-04-09 15:09:08,119] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:08,170] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:08,451] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:08,622] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:08,623] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:08,719] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:08,733] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9329354e-08 7.2395909e-05 1.3527613e-03 8.0521787e-08 9.0315050e-01
 1.0002724e-02 8.3087064e-02 1.2978145e-04 1.5367180e-03 6.1203272e-04
 5.5835397e-05], sum to 1.0000
[2019-04-09 15:09:08,733] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0817
[2019-04-09 15:09:08,735] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:08,735] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:08,737] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res5/Eplus-env-sub_run2
[2019-04-09 15:09:08,757] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.33333333333333, 21.66666666666666, 116.8333333333333, 853.1666666666667, 22.5, 27.22777918310178, 0.8332964996853526, 1.0, 1.0, 35.0, 10.611487663020425], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 5060400.0000, 
sim time next is 5061600.0000, 
raw observation next is [11.0, 20.0, 114.5, 839.5, 22.5, 27.58614348447538, 0.8880901437660245, 1.0, 1.0, 35.0, 8.839442716251682], 
processed observation next is [1.0, 0.6086956521739131, 0.7673130193905818, 0.2, 0.38166666666666665, 0.9276243093922651, 0.375, 0.7988452903729483, 0.7960300479220082, 1.0, 1.0, 0.4, 0.08839442716251682], 
reward next is 0.9116, 
noisyNet noise sample is [array([-0.49712527], dtype=float32), 0.8056378]. 
=============================================
[2019-04-09 15:09:08,789] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:08,887] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:08,925] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:08,925] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:08,927] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res10/Eplus-env-sub_run2
[2019-04-09 15:09:08,950] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:08,951] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:08,969] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:08,969] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:08,971] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res2/Eplus-env-sub_run2
[2019-04-09 15:09:09,111] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:09,112] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:09,463] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:09,463] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:09,465] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res3/Eplus-env-sub_run2
[2019-04-09 15:09:09,497] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:09,624] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:09,624] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:09,639] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res6/Eplus-env-sub_run2
[2019-04-09 15:09:09,661] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:09,720] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:09,721] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:09,722] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res16/Eplus-env-sub_run2
[2019-04-09 15:09:09,957] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:09,957] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:09,961] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res17/Eplus-env-sub_run2
[2019-04-09 15:09:10,014] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:10,015] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:10,017] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res12/Eplus-env-sub_run2
[2019-04-09 15:09:10,106] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:10,288] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:10,301] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:10,466] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:10,498] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:10,498] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:10,500] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res4/Eplus-env-sub_run2
[2019-04-09 15:09:11,107] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:11,107] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:11,109] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res15/Eplus-env-sub_run2
[2019-04-09 15:09:11,273] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:11,273] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:11,275] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res9/Eplus-env-sub_run2
[2019-04-09 15:09:11,957] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.2097891e-08 5.0530907e-05 2.9656367e-04 9.8177381e-08 7.2849226e-01
 2.1107679e-02 2.4091427e-01 3.2721116e-04 7.9245130e-03 8.1177038e-04
 7.5056618e-05], sum to 1.0000
[2019-04-09 15:09:11,957] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6424
[2019-04-09 15:09:11,959] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.6499859e-08 1.4132972e-06 2.3545040e-04 2.3598338e-08 8.9127481e-01
 1.7337756e-02 8.7146848e-02 7.1809140e-05 2.7723280e-03 1.1444215e-03
 1.5163457e-05], sum to 1.0000
[2019-04-09 15:09:11,959] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3788
[2019-04-09 15:09:11,982] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.33333333333333, 17.0, 30.0, 243.3333333333333, 22.5, 28.66714383723183, 1.134370908220634, 1.0, 1.0, 35.0, 10.074359765175007], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 5074800.0000, 
sim time next is 5076000.0000, 
raw observation next is [11.0, 17.0, 18.0, 146.0, 22.5, 28.58073361698632, 1.095591130008805, 1.0, 1.0, 35.0, 7.140404887566207], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.06, 0.16132596685082873, 0.375, 0.8817278014155265, 0.8651970433362685, 1.0, 1.0, 0.4, 0.07140404887566207], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.30032203], dtype=float32), 0.5263053]. 
=============================================
[2019-04-09 15:09:11,986] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[32.605465]
 [32.72636 ]
 [32.64938 ]
 [32.609013]
 [32.476124]], R is [[33.05723572]
 [33.62592316]
 [34.2195549 ]
 [34.81484222]
 [35.26280212]].
[2019-04-09 15:09:12,012] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.0, 17.0, 56.0, 438.5, 22.5, 28.62948869039891, 1.113302320746959, 1.0, 1.0, 35.0, 13.101908001101599], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 5072400.0000, 
sim time next is 5073600.0000, 
raw observation next is [11.66666666666667, 17.0, 42.66666666666666, 340.8333333333333, 22.5, 28.68815595944201, 0.9128438687022783, 1.0, 1.0, 35.0, 10.644627263817666], 
processed observation next is [1.0, 0.7391304347826086, 0.785780240073869, 0.17, 0.1422222222222222, 0.3766114180478821, 0.375, 0.8906796632868342, 0.8042812895674261, 1.0, 1.0, 0.4, 0.10644627263817666], 
reward next is 0.8936, 
noisyNet noise sample is [array([0.3653347], dtype=float32), -0.9567664]. 
=============================================
[2019-04-09 15:09:12,381] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:12,713] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:12,717] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:09:12,740] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:13,054] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:13,088] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:09:13,382] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:13,382] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:13,384] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res14/Eplus-env-sub_run2
[2019-04-09 15:09:13,714] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:13,714] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:13,716] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res13/Eplus-env-sub_run2
[2019-04-09 15:09:13,761] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-09 15:09:13,761] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:13,763] EPLUS_ENV_Part4-Light-Pit-Train-v3_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res7/Eplus-env-sub_run2
[2019-04-09 15:09:20,124] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6718458e-05 1.5520892e-03 1.1182895e-03 9.1141565e-06 7.4894160e-01
 2.7934892e-02 2.0594455e-01 9.2346827e-04 1.1224453e-02 1.7370172e-03
 5.7783932e-04], sum to 1.0000
[2019-04-09 15:09:20,124] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8070
[2019-04-09 15:09:20,161] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.699999999999999, 93.0, 0.0, 0.0, 19.0, 21.2426087254893, -0.5133842936881613, 0.0, 1.0, 35.0, 19.583311524165754], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 24000.0000, 
sim time next is 25200.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 21.22670092535883, -0.4781111561071436, 0.0, 1.0, 45.0, 35.83426402304235], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.26889174377990255, 0.34062961463095215, 0.0, 1.0, 0.6, 0.3583426402304235], 
reward next is 0.6417, 
noisyNet noise sample is [array([-1.2200329], dtype=float32), -1.3196023]. 
=============================================
[2019-04-09 15:09:22,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2863698e-05 3.6761322e-04 6.8510980e-03 2.8707009e-06 7.5458592e-01
 1.9957140e-02 2.0924753e-01 1.0240140e-03 4.3777432e-03 2.8138550e-03
 7.5930299e-04], sum to 1.0000
[2019-04-09 15:09:22,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2276
[2019-04-09 15:09:22,102] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 20.75822964792346, -0.6114406921564272, 0.0, 1.0, 25.0, 17.32409599048857], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 86400.0000, 
sim time next is 87600.0000, 
raw observation next is [-0.2, 93.66666666666667, 0.0, 0.0, 19.0, 20.62054190490194, -0.6301782397123392, 0.0, 1.0, 35.0, 21.154029224937617], 
processed observation next is [1.0, 0.0, 0.4570637119113574, 0.9366666666666668, 0.0, 0.0, 0.08333333333333333, 0.21837849207516177, 0.2899405867625536, 0.0, 1.0, 0.4, 0.21154029224937618], 
reward next is 0.7885, 
noisyNet noise sample is [array([0.2245957], dtype=float32), 0.01841357]. 
=============================================
[2019-04-09 15:09:22,511] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00482098 0.02551762 0.03973512 0.00256314 0.43761253 0.1214281
 0.23042624 0.01580375 0.05590875 0.04734176 0.01884193], sum to 1.0000
[2019-04-09 15:09:22,512] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0489
[2019-04-09 15:09:22,547] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [7.200000000000001, 96.0, 0.0, 0.0, 19.0, 21.25200962631444, -0.4858188721482184, 0.0, 1.0, 35.0, 25.870403848029376], 
current ob forecast is [], 
actual action is [1, 25.0], 
sim time this is 4800.0000, 
sim time next is 6000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 21.3669710846033, -0.4829730582011214, 0.0, 1.0, 25.0, 22.63821631645662], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.28058092371694165, 0.3390089805996262, 0.0, 1.0, 0.2, 0.22638216316456622], 
reward next is 0.7736, 
noisyNet noise sample is [array([0.28188196], dtype=float32), 0.58707196]. 
=============================================
[2019-04-09 15:09:22,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[8.798194 ]
 [7.4372764]
 [5.3254647]
 [3.6735427]
 [2.3894627]], R is [[10.90998554]
 [11.54218197]
 [12.07644558]
 [12.67842865]
 [13.27271938]].
[2019-04-09 15:09:23,257] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4665791e-05 1.5483208e-03 1.1907299e-02 6.6807506e-06 5.8238357e-01
 3.4858573e-02 3.5490265e-01 9.1845705e-04 6.2732324e-03 5.8858856e-03
 1.3006350e-03], sum to 1.0000
[2019-04-09 15:09:23,257] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5932
[2019-04-09 15:09:23,299] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.200000000000001, 88.0, 0.0, 0.0, 19.0, 21.30079968146081, -0.4797820716256756, 0.0, 1.0, 35.0, 20.630417716126708], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 66000.0000, 
sim time next is 67200.0000, 
raw observation next is [4.0, 87.0, 0.0, 0.0, 19.0, 21.19923700343689, -0.4973214763470127, 0.0, 1.0, 35.0, 20.741895010604082], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.87, 0.0, 0.0, 0.08333333333333333, 0.266603083619741, 0.3342261745509958, 0.0, 1.0, 0.4, 0.20741895010604083], 
reward next is 0.7926, 
noisyNet noise sample is [array([-0.82350636], dtype=float32), 0.83876616]. 
=============================================
[2019-04-09 15:09:23,337] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2471637e-05 3.9958081e-04 2.1418973e-03 3.2362957e-06 8.1354618e-01
 2.3228351e-02 1.4656056e-01 5.9604226e-04 9.8743774e-03 3.1218801e-03
 5.1542645e-04], sum to 1.0000
[2019-04-09 15:09:23,337] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0922
[2019-04-09 15:09:23,371] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.366666666666667, 95.0, 0.0, 0.0, 19.0, 22.6578733293252, -0.2412645425454623, 0.0, 1.0, 45.0, 28.986880889868893], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 12000.0000, 
sim time next is 13200.0000, 
raw observation next is [7.533333333333333, 94.0, 0.0, 0.0, 19.0, 22.62049868737022, -0.2537605932721053, 0.0, 1.0, 35.0, 26.044287230990776], 
processed observation next is [0.0, 0.13043478260869565, 0.6712834718374886, 0.94, 0.0, 0.0, 0.08333333333333333, 0.3850415572808516, 0.4154131355759649, 0.0, 1.0, 0.4, 0.26044287230990776], 
reward next is 0.7396, 
noisyNet noise sample is [array([1.9169021], dtype=float32), -0.57766634]. 
=============================================
[2019-04-09 15:09:23,618] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6291437e-05 2.4217923e-04 6.0937237e-03 5.4433140e-06 7.3879665e-01
 2.7618196e-02 2.1593207e-01 1.8359524e-03 6.4030173e-03 2.4806508e-03
 5.7588285e-04], sum to 1.0000
[2019-04-09 15:09:23,618] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6444
[2019-04-09 15:09:23,647] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.699999999999999, 93.0, 0.0, 0.0, 19.0, 21.64970275215146, -0.4168026295294067, 0.0, 1.0, 45.0, 32.42701998822371], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 27600.0000, 
sim time next is 28800.0000, 
raw observation next is [7.7, 93.0, 10.5, 0.0, 19.0, 21.68219206593172, -0.4041063509882546, 0.0, 1.0, 35.0, 24.684683678534448], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.035, 0.0, 0.08333333333333333, 0.30684933882764326, 0.36529788300391514, 0.0, 1.0, 0.4, 0.24684683678534447], 
reward next is 0.7532, 
noisyNet noise sample is [array([-0.9975138], dtype=float32), 0.033284504]. 
=============================================
[2019-04-09 15:09:25,829] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0520237e-05 6.5806840e-04 2.7593386e-03 6.5568793e-06 8.6686665e-01
 2.1401383e-02 9.0473250e-02 8.2195434e-04 1.0595127e-02 4.6913498e-03
 1.6958808e-03], sum to 1.0000
[2019-04-09 15:09:25,834] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9629
[2019-04-09 15:09:25,862] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 67.5, 0.0, 19.0, 21.50957074043015, -0.4544649976744567, 0.0, 1.0, 35.0, 17.946956829705037], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 39600.0000, 
sim time next is 40800.0000, 
raw observation next is [7.7, 93.0, 72.5, 0.0, 19.0, 21.50772420210744, -0.426219581440604, 0.0, 1.0, 45.0, 33.73251395180296], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.24166666666666667, 0.0, 0.08333333333333333, 0.29231035017561996, 0.35792680618646533, 0.0, 1.0, 0.6, 0.33732513951802956], 
reward next is 0.6627, 
noisyNet noise sample is [array([0.783968], dtype=float32), 0.015582969]. 
=============================================
[2019-04-09 15:09:25,995] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5107041e-05 3.1795914e-04 2.6730348e-03 3.2632220e-06 8.4955525e-01
 1.9714640e-02 1.1867543e-01 1.9423529e-03 4.3347236e-03 2.1341578e-03
 6.3410215e-04], sum to 1.0000
[2019-04-09 15:09:26,008] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5948
[2019-04-09 15:09:26,039] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.2, 81.66666666666667, 0.0, 0.0, 19.0, 20.44942887100368, -0.7257724714637542, 0.0, 1.0, 35.0, 24.56829635348567], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 99600.0000, 
sim time next is 100800.0000, 
raw observation next is [-3.4, 79.0, 0.0, 0.0, 19.0, 20.2544020959533, -0.7634352917294273, 0.0, 1.0, 35.0, 25.300623639953322], 
processed observation next is [1.0, 0.17391304347826086, 0.368421052631579, 0.79, 0.0, 0.0, 0.08333333333333333, 0.18786684132944162, 0.24552156942352424, 0.0, 1.0, 0.4, 0.25300623639953324], 
reward next is 0.7470, 
noisyNet noise sample is [array([-1.0675025], dtype=float32), -0.14165579]. 
=============================================
[2019-04-09 15:09:26,196] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9100295e-06 2.6612059e-04 2.4975303e-03 1.1798778e-06 7.2718680e-01
 1.5819049e-02 2.4523936e-01 9.8284590e-04 4.0967478e-03 3.1588923e-03
 7.4659422e-04], sum to 1.0000
[2019-04-09 15:09:26,202] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4247
[2019-04-09 15:09:26,232] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 86.0, 83.0, 0.0, 19.0, 21.85602579772888, -0.3499343139301221, 0.0, 1.0, 35.0, 24.497438228845613], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 50400.0000, 
sim time next is 51600.0000, 
raw observation next is [7.533333333333333, 86.0, 80.33333333333334, 0.0, 19.0, 21.91262857087846, -0.3509970128334882, 0.0, 1.0, 35.0, 21.64781571054116], 
processed observation next is [0.0, 0.6086956521739131, 0.6712834718374886, 0.86, 0.26777777777777784, 0.0, 0.08333333333333333, 0.3260523809065384, 0.38300099572217056, 0.0, 1.0, 0.4, 0.2164781571054116], 
reward next is 0.7835, 
noisyNet noise sample is [array([0.40101272], dtype=float32), 1.6146457]. 
=============================================
[2019-04-09 15:09:26,680] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.3781428e-07 2.0672237e-04 1.6054631e-03 7.9307377e-08 7.1208447e-01
 2.7016118e-02 2.5357562e-01 7.8771601e-04 2.7101960e-03 1.7210157e-03
 2.9183098e-04], sum to 1.0000
[2019-04-09 15:09:26,681] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2596
[2019-04-09 15:09:26,778] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.2, 69.33333333333334, 175.0, 111.3333333333333, 22.5, 22.54481997158254, -0.3522929238145924, 1.0, 1.0, 35.0, 27.335731353972427], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 128400.0000, 
sim time next is 129600.0000, 
raw observation next is [-8.4, 61.0, 157.0, 308.0, 22.5, 22.56786873056616, -0.3383912884540201, 1.0, 1.0, 35.0, 25.980236261323483], 
processed observation next is [1.0, 0.5217391304347826, 0.2299168975069252, 0.61, 0.5233333333333333, 0.34033149171270716, 0.375, 0.3806557275471801, 0.38720290384865996, 1.0, 1.0, 0.4, 0.25980236261323486], 
reward next is 0.7402, 
noisyNet noise sample is [array([1.0254738], dtype=float32), 0.2754057]. 
=============================================
[2019-04-09 15:09:27,859] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.13407202e-06 1.02742335e-04 6.24324952e-04 1.09050359e-06
 9.13319767e-01 1.02327634e-02 7.28168041e-02 1.74017521e-04
 1.20886485e-03 1.18115556e-03 3.37404374e-04], sum to 1.0000
[2019-04-09 15:09:27,860] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0761
[2019-04-09 15:09:27,877] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.0, 87.0, 0.0, 0.0, 19.0, 21.68840926465551, -0.3982070819726904, 0.0, 1.0, 35.0, 20.41357066997436], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 67200.0000, 
sim time next is 68400.0000, 
raw observation next is [3.8, 86.0, 0.0, 0.0, 19.0, 21.59410169704715, -0.4235958463909578, 0.0, 1.0, 35.0, 18.587250737831518], 
processed observation next is [0.0, 0.8260869565217391, 0.5678670360110805, 0.86, 0.0, 0.0, 0.08333333333333333, 0.2995084747539292, 0.35880138453634736, 0.0, 1.0, 0.4, 0.18587250737831518], 
reward next is 0.8141, 
noisyNet noise sample is [array([0.88097847], dtype=float32), -0.072488]. 
=============================================
[2019-04-09 15:09:28,264] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.6832265e-06 9.6339121e-05 4.2439960e-03 1.0500314e-06 6.5635830e-01
 3.2584488e-02 2.8276584e-01 7.8406330e-04 1.6457060e-02 5.3242804e-03
 1.3819782e-03], sum to 1.0000
[2019-04-09 15:09:28,264] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3661
[2019-04-09 15:09:28,407] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.84935175e-07 3.62285173e-05 1.28674190e-04 2.14023203e-07
 6.72959387e-01 4.91245650e-02 2.74016500e-01 2.64087139e-04
 2.38340674e-03 9.74379771e-04 1.12414586e-04], sum to 1.0000
[2019-04-09 15:09:28,407] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0554
[2019-04-09 15:09:28,420] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.799999999999999, 82.0, 189.0, 32.16666666666666, 22.5, 22.47626112193148, -0.375893026152229, 1.0, 1.0, 35.0, 35.62679860584039], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 124800.0000, 
sim time next is 126000.0000, 
raw observation next is [-7.8, 86.0, 187.0, 24.5, 22.5, 22.47363329679711, -0.3698706816390926, 1.0, 1.0, 35.0, 28.08211712778721], 
processed observation next is [1.0, 0.4782608695652174, 0.24653739612188366, 0.86, 0.6233333333333333, 0.02707182320441989, 0.375, 0.3728027747330926, 0.3767097727869691, 1.0, 1.0, 0.4, 0.2808211712778721], 
reward next is 0.7192, 
noisyNet noise sample is [array([-1.2271967], dtype=float32), 0.75106287]. 
=============================================
[2019-04-09 15:09:28,424] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[26.813692]
 [26.733902]
 [25.578169]
 [24.70799 ]
 [24.948984]], R is [[27.92796135]
 [28.29241371]
 [28.43736267]
 [28.85582733]
 [29.27895927]].
[2019-04-09 15:09:28,484] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.433333333333334, 61.0, 137.5, 503.8333333333334, 22.5, 22.90702585907548, -0.200370565124544, 1.0, 1.0, 35.0, 32.203726117522066], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 134400.0000, 
sim time next is 135600.0000, 
raw observation next is [-7.066666666666666, 61.0, 140.5, 421.0000000000001, 22.5, 23.20326306763031, -0.17817603918543, 1.0, 1.0, 35.0, 26.53124163721139], 
processed observation next is [1.0, 0.5652173913043478, 0.26685133887349954, 0.61, 0.4683333333333333, 0.46519337016574597, 0.375, 0.43360525563585917, 0.44060798693819, 1.0, 1.0, 0.4, 0.2653124163721139], 
reward next is 0.7347, 
noisyNet noise sample is [array([1.2375224], dtype=float32), -0.28023848]. 
=============================================
[2019-04-09 15:09:28,820] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.2955352e-06 1.7423996e-04 3.0317388e-03 1.4166324e-06 8.5678047e-01
 2.4810780e-02 1.1044794e-01 7.7888864e-04 2.2223827e-03 1.2423504e-03
 5.0459756e-04], sum to 1.0000
[2019-04-09 15:09:28,820] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9106
[2019-04-09 15:09:28,938] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.566666666666666, 78.0, 41.5, 246.6666666666667, 22.5, 19.6519928815504, -0.9585525994825538, 1.0, 1.0, 35.0, 36.299704838757805], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 204000.0000, 
sim time next is 205200.0000, 
raw observation next is [-8.4, 78.0, 56.5, 148.0, 22.5, 20.12520299276866, -0.8789472753618571, 1.0, 1.0, 45.0, 49.771803641627876], 
processed observation next is [1.0, 0.391304347826087, 0.2299168975069252, 0.78, 0.18833333333333332, 0.16353591160220995, 0.375, 0.17710024939738833, 0.20701757487938097, 1.0, 1.0, 0.6, 0.49771803641627876], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.37167153], dtype=float32), 0.42354923]. 
=============================================
[2019-04-09 15:09:30,741] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-09 15:09:30,742] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:09:30,742] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:30,744] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run8
[2019-04-09 15:09:30,777] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:09:30,777] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:30,779] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run8
[2019-04-09 15:09:30,817] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:09:30,817] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:09:30,822] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run8
[2019-04-09 15:09:53,435] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:NoisyNet noise sample: [array([0.04322645], dtype=float32), 0.063808]
[2019-04-09 15:09:53,435] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation this: [-1.066666666666667, 80.33333333333333, 131.3333333333333, 429.1666666666666, 19.0, 20.34342310929441, -0.7620891284239507, 0.0, 1.0, 35.0, 25.7042067332595]
[2019-04-09 15:09:53,435] A3C_EVAL-Part4-Light-Pit-Train-v3 DEBUG:Observation forecast: []
[2019-04-09 15:09:53,436] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Softmax [1.5301952e-05 4.9561617e-04 3.3106874e-03 5.8954579e-06 7.7222443e-01
 2.7398096e-02 1.8291852e-01 1.1058043e-03 7.8015183e-03 3.9046428e-03
 8.1953115e-04], sampled 0.7199860793994174
[2019-04-09 15:11:13,849] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 3065.5000 105608.3609 411.2735
[2019-04-09 15:11:13,871] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:13,871] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:13,871] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:13,871] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:13,871] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:13,871] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:13,871] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:13,871] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:13,971] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:13,971] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:13,971] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:13,971] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:13,971] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:13,971] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:13,971] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:13,971] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:28,639] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2993.0359 111902.8643 -31.7418
[2019-04-09 15:11:28,662] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:28,662] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:28,662] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:28,662] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:28,662] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:28,662] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:28,662] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:28,662] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:28,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:28,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:28,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:28,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:28,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:28,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:28,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:28,800] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:35,727] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2900.1480 113577.8302 -184.9948
[2019-04-09 15:11:35,754] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:35,754] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:35,754] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:35,754] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:35,754] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:35,754] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:35,754] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:35,754] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:11:35,862] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:35,862] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:35,862] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:35,862] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:35,862] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:35,862] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:35,862] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:35,862] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:11:36,756] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 70000, evaluation results [70000.0, 2993.0359080645603, 111902.86426305551, -31.74182311003953, 3065.4999575246, 105608.36092813383, 411.2734654621189, 2900.148028265608, 113577.83022207013, -184.9948070575196]
[2019-04-09 15:11:36,790] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2252514e-07 1.5363954e-05 6.1778665e-05 8.1258214e-08 8.3214223e-01
 4.3111248e-03 1.6093421e-01 4.9436410e-05 1.6300844e-03 7.5275777e-04
 1.0278788e-04], sum to 1.0000
[2019-04-09 15:11:36,790] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7652
[2019-04-09 15:11:36,940] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.7, 61.0, 104.5, 75.5, 22.5, 23.28317020001101, -0.2530147438214495, 1.0, 1.0, 45.0, 56.57084972333351], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 140400.0000, 
sim time next is 141600.0000, 
raw observation next is [-6.700000000000001, 62.0, 75.5, 55.16666666666666, 22.5, 23.30276038927209, -0.1607611779535025, 1.0, 1.0, 35.0, 33.61801903627971], 
processed observation next is [1.0, 0.6521739130434783, 0.2770083102493075, 0.62, 0.25166666666666665, 0.06095764272559852, 0.375, 0.4418966991060076, 0.4464129406821658, 1.0, 1.0, 0.4, 0.3361801903627971], 
reward next is 0.6638, 
noisyNet noise sample is [array([2.330393], dtype=float32), 0.91687536]. 
=============================================
[2019-04-09 15:11:37,069] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3443159e-05 3.1394133e-04 2.7132630e-03 4.7772278e-06 7.7341527e-01
 4.8181590e-02 1.5718126e-01 8.2079263e-04 1.2771871e-02 4.0042452e-03
 5.6962995e-04], sum to 1.0000
[2019-04-09 15:11:37,069] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5960
[2019-04-09 15:11:37,142] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 19.0, 19.21359886307105, -1.066499713401922, 0.0, 1.0, 35.0, 23.341165289504318], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 192000.0000, 
sim time next is 193200.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 19.03040117263275, -1.108539401364576, 0.0, 1.0, 35.0, 25.630372701958144], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.08586676438606251, 0.13048686621180802, 0.0, 1.0, 0.4, 0.25630372701958143], 
reward next is 0.7437, 
noisyNet noise sample is [array([0.9441983], dtype=float32), 0.11894329]. 
=============================================
[2019-04-09 15:11:37,755] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9496469e-05 2.0001449e-04 1.2827914e-02 3.6521642e-06 5.8482414e-01
 7.0139520e-02 3.0741233e-01 6.3333329e-04 8.0715884e-03 1.5152718e-02
 7.1521802e-04], sum to 1.0000
[2019-04-09 15:11:37,755] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3083
[2019-04-09 15:11:37,785] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.12006217e-06 7.09972301e-05 1.78750546e-03 5.17330193e-07
 8.38855922e-01 4.53244485e-02 1.07806474e-01 5.91192045e-04
 3.05533875e-03 2.40568281e-03 1.00872960e-04], sum to 1.0000
[2019-04-09 15:11:37,786] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0398
[2019-04-09 15:11:37,902] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 19.0, 21.50182709121353, -0.5194926922627968, 0.0, 1.0, 35.0, 29.280308040685043], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 162000.0000, 
sim time next is 163200.0000, 
raw observation next is [-8.4, 69.0, 0.0, 0.0, 19.0, 21.21551992220369, -0.580656177689248, 0.0, 1.0, 35.0, 26.234537680495073], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.69, 0.0, 0.0, 0.08333333333333333, 0.26795999351697414, 0.30644794077025067, 0.0, 1.0, 0.4, 0.26234537680495074], 
reward next is 0.7377, 
noisyNet noise sample is [array([-1.1372508], dtype=float32), 0.5768985]. 
=============================================
[2019-04-09 15:11:37,928] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 22.5, 19.24839587116933, -1.022111726592466, 1.0, 1.0, 45.0, 52.06132901671651], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 200400.0000, 
sim time next is 201600.0000, 
raw observation next is [-8.9, 78.0, 17.0, 157.0, 22.5, 19.67478299014942, -0.961258583512386, 1.0, 1.0, 35.0, 36.826648950858115], 
processed observation next is [1.0, 0.34782608695652173, 0.21606648199445982, 0.78, 0.056666666666666664, 0.1734806629834254, 0.375, 0.1395652491791184, 0.179580472162538, 1.0, 1.0, 0.4, 0.36826648950858115], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4748265], dtype=float32), -1.1865956]. 
=============================================
[2019-04-09 15:11:38,029] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5770207e-06 1.7167788e-04 3.5132663e-03 2.5958741e-07 8.8590765e-01
 1.2532064e-02 8.8488765e-02 3.8904173e-04 7.5185071e-03 1.2950754e-03
 1.8214500e-04], sum to 1.0000
[2019-04-09 15:11:38,032] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2027
[2019-04-09 15:11:38,052] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 70.0, 0.0, 0.0, 19.0, 20.56754562867302, -0.6892315464398563, 0.0, 1.0, 35.0, 27.04711526588861], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 164400.0000, 
sim time next is 165600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 20.45406894614099, -0.715174407283, 0.0, 1.0, 35.0, 27.106710025521103], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.20450574551174908, 0.26160853090566666, 0.0, 1.0, 0.4, 0.271067100255211], 
reward next is 0.7289, 
noisyNet noise sample is [array([-0.27926674], dtype=float32), 0.6735056]. 
=============================================
[2019-04-09 15:11:38,141] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.71531724e-07 1.29588525e-05 4.31529217e-04 5.23754231e-08
 8.96434665e-01 1.99430902e-02 8.06123167e-02 5.75917402e-05
 1.93217036e-03 4.71761363e-04 1.03628561e-04], sum to 1.0000
[2019-04-09 15:11:38,142] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4073
[2019-04-09 15:11:38,247] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.2, 61.0, 139.0, 504.6666666666667, 22.5, 21.91648104231153, -0.4216613360080433, 1.0, 1.0, 45.0, 56.911922926189774], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 130800.0000, 
sim time next is 132000.0000, 
raw observation next is [-8.0, 61.0, 131.5, 583.1666666666666, 22.5, 22.18627238669365, -0.3752979444509681, 1.0, 1.0, 35.0, 34.912670169889836], 
processed observation next is [1.0, 0.5217391304347826, 0.24099722991689754, 0.61, 0.43833333333333335, 0.6443830570902394, 0.375, 0.3488560322244707, 0.3749006851830106, 1.0, 1.0, 0.4, 0.34912670169889837], 
reward next is 0.6509, 
noisyNet noise sample is [array([0.37567568], dtype=float32), -1.2316691]. 
=============================================
[2019-04-09 15:11:38,255] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[28.481413]
 [27.727428]
 [27.444975]
 [27.178022]
 [26.881437]], R is [[29.24047852]
 [29.37895584]
 [29.78232193]
 [30.18808174]
 [30.56775284]].
[2019-04-09 15:11:38,783] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5697768e-06 3.0569619e-05 6.1853003e-04 1.4956282e-07 8.4987813e-01
 8.4837144e-03 1.3738105e-01 1.3010881e-04 2.7109219e-03 6.3115061e-04
 1.3415111e-04], sum to 1.0000
[2019-04-09 15:11:38,794] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3836
[2019-04-09 15:11:38,846] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 19.0, 20.52818632244232, -0.7282340530023252, 0.0, 1.0, 35.0, 25.058356787568513], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 170400.0000, 
sim time next is 171600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 20.36820748446035, -0.7608023272831227, 0.0, 1.0, 35.0, 26.075342566407947], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.1973506237050291, 0.2463992242389591, 0.0, 1.0, 0.4, 0.2607534256640795], 
reward next is 0.7392, 
noisyNet noise sample is [array([0.36677182], dtype=float32), -0.54264057]. 
=============================================
[2019-04-09 15:11:39,209] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5937511e-06 1.4310128e-04 1.8429359e-03 1.2628354e-06 7.9570127e-01
 3.1867206e-02 1.6035479e-01 3.3695772e-04 6.2963455e-03 2.9254057e-03
 5.2816182e-04], sum to 1.0000
[2019-04-09 15:11:39,209] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5961
[2019-04-09 15:11:39,257] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.900000000000002, 74.0, 0.0, 0.0, 19.0, 19.91351073606397, -0.8623803587717122, 0.0, 1.0, 40.0, 32.634638725697], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 177600.0000, 
sim time next is 178800.0000, 
raw observation next is [-8.9, 74.0, 0.0, 0.0, 19.0, 19.81552535855071, -0.8874873988570533, 0.0, 1.0, 35.0, 25.64983215127164], 
processed observation next is [1.0, 0.043478260869565216, 0.21606648199445982, 0.74, 0.0, 0.0, 0.08333333333333333, 0.15129377987922568, 0.2041708670476489, 0.0, 1.0, 0.4, 0.2564983215127164], 
reward next is 0.7435, 
noisyNet noise sample is [array([-1.657829], dtype=float32), -2.0765703]. 
=============================================
[2019-04-09 15:11:39,391] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.9007978e-06 1.5641531e-04 7.8559289e-04 2.7581029e-06 7.2883964e-01
 7.9691507e-02 1.7676957e-01 1.3497480e-03 9.6849427e-03 2.1026903e-03
 6.0927146e-04], sum to 1.0000
[2019-04-09 15:11:39,391] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7092
[2019-04-09 15:11:39,474] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.733333333333334, 78.0, 28.33333333333334, 249.6666666666667, 22.5, 19.52758539742862, -0.9878265124020013, 1.0, 1.0, 35.0, 30.206969748157583], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 202800.0000, 
sim time next is 204000.0000, 
raw observation next is [-8.566666666666666, 78.0, 41.5, 246.6666666666667, 22.5, 19.84216313877814, -0.9497066527046204, 1.0, 1.0, 35.0, 27.19398889029045], 
processed observation next is [1.0, 0.34782608695652173, 0.22530009233610343, 0.78, 0.13833333333333334, 0.27255985267034993, 0.375, 0.15351359489817837, 0.18343111576512652, 1.0, 1.0, 0.4, 0.2719398889029045], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3368765], dtype=float32), 0.19697759]. 
=============================================
[2019-04-09 15:11:39,478] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[24.116613]
 [23.154764]
 [22.757282]
 [21.98771 ]
 [21.740568]], R is [[24.41109276]
 [24.16698265]
 [23.92531395]
 [23.68606186]
 [23.44920158]].
[2019-04-09 15:11:39,486] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.0462244e-06 1.2732609e-04 1.6489963e-03 6.8444871e-07 6.3111115e-01
 1.7068813e-02 3.4058332e-01 5.6945521e-04 5.1249699e-03 3.4058329e-03
 3.5344352e-04], sum to 1.0000
[2019-04-09 15:11:39,487] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0243
[2019-04-09 15:11:39,608] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 78.0, 56.5, 148.0, 22.5, 20.45865185177731, -0.8081616579914321, 1.0, 1.0, 35.0, 35.59730292610405], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 205200.0000, 
sim time next is 206400.0000, 
raw observation next is [-8.033333333333333, 77.0, 71.50000000000001, 49.33333333333332, 22.5, 20.90194783344076, -0.7638202937570262, 1.0, 1.0, 35.0, 29.211154946602605], 
processed observation next is [1.0, 0.391304347826087, 0.24007386888273316, 0.77, 0.23833333333333337, 0.05451197053406997, 0.375, 0.24182898612006345, 0.2453932354143246, 1.0, 1.0, 0.4, 0.292111549466026], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.88755465], dtype=float32), -0.8835608]. 
=============================================
[2019-04-09 15:11:39,620] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.2101361e-06 2.1664782e-04 2.2222428e-03 1.3029072e-06 8.3055830e-01
 5.7050204e-03 1.5423536e-01 3.9604335e-04 4.5839120e-03 1.7011858e-03
 3.7685648e-04], sum to 1.0000
[2019-04-09 15:11:39,620] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0064
[2019-04-09 15:11:39,652] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.1, 69.66666666666666, 0.0, 0.0, 19.0, 20.16944050437098, -0.8576677323321081, 0.0, 1.0, 35.0, 27.67672071139243], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 265200.0000, 
sim time next is 266400.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 19.0, 20.11060300768455, -0.8892529864543329, 0.0, 1.0, 35.0, 25.11705611277899], 
processed observation next is [1.0, 0.08695652173913043, 0.26038781163434904, 0.71, 0.0, 0.0, 0.08333333333333333, 0.17588358397371243, 0.2035823378485557, 0.0, 1.0, 0.4, 0.2511705611277899], 
reward next is 0.7488, 
noisyNet noise sample is [array([0.5644367], dtype=float32), 1.1628743]. 
=============================================
[2019-04-09 15:11:40,073] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.6052069e-07 3.4414159e-05 1.3506792e-03 6.0783105e-07 7.7703953e-01
 8.2102101e-03 2.0900102e-01 8.4395148e-04 2.8095308e-03 5.2575336e-04
 1.8405938e-04], sum to 1.0000
[2019-04-09 15:11:40,074] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4750
[2019-04-09 15:11:40,171] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.300000000000001, 67.66666666666667, 0.0, 0.0, 22.5, 22.66221767081777, -0.3202248208213008, 1.0, 1.0, 35.0, 34.81693491593428], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 148800.0000, 
sim time next is 150000.0000, 
raw observation next is [-7.3, 64.33333333333333, 0.0, 0.0, 22.5, 22.55484481328163, -0.3715679101022578, 1.0, 1.0, 35.0, 29.935790398415275], 
processed observation next is [1.0, 0.7391304347826086, 0.26038781163434904, 0.6433333333333333, 0.0, 0.0, 0.375, 0.37957040110680246, 0.37614402996591406, 1.0, 1.0, 0.4, 0.29935790398415274], 
reward next is 0.7006, 
noisyNet noise sample is [array([0.10595715], dtype=float32), -0.53868383]. 
=============================================
[2019-04-09 15:11:40,195] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[30.135506]
 [30.51288 ]
 [30.200077]
 [30.160006]
 [30.310158]], R is [[30.37531853]
 [30.7233963 ]
 [30.82692719]
 [31.21678543]
 [31.60644341]].
[2019-04-09 15:11:40,981] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2102284e-06 1.1674372e-04 1.6952894e-03 1.8964604e-07 8.4780812e-01
 1.8975539e-02 1.2519343e-01 4.8155617e-04 1.8486158e-03 3.8079047e-03
 7.1315699e-05], sum to 1.0000
[2019-04-09 15:11:40,982] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0670
[2019-04-09 15:11:41,016] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 79.0, 0.0, 0.0, 19.0, 20.23038884136994, -0.796476786996346, 0.0, 1.0, 45.0, 41.163368142788514], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 259200.0000, 
sim time next is 260400.0000, 
raw observation next is [-5.233333333333334, 75.0, 0.0, 0.0, 19.0, 20.20825151130668, -0.799958219392566, 0.0, 1.0, 35.0, 29.558409975955982], 
processed observation next is [1.0, 0.0, 0.31763619575253926, 0.75, 0.0, 0.0, 0.08333333333333333, 0.18402095927555676, 0.233347260202478, 0.0, 1.0, 0.4, 0.2955840997595598], 
reward next is 0.7044, 
noisyNet noise sample is [array([-4.143281], dtype=float32), -2.606748]. 
=============================================
[2019-04-09 15:11:41,661] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.2460690e-07 1.4830771e-04 1.0744953e-03 8.0124119e-07 6.9301665e-01
 1.1823253e-02 2.8259575e-01 5.7623943e-04 7.9386234e-03 2.6885602e-03
 1.3655412e-04], sum to 1.0000
[2019-04-09 15:11:41,661] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9881
[2019-04-09 15:11:41,790] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 75.0, 101.5, 0.0, 22.5, 20.98052035421433, -0.7758418822042027, 1.0, 1.0, 35.0, 29.819854602033587], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 208800.0000, 
sim time next is 210000.0000, 
raw observation next is [-6.933333333333334, 74.0, 116.5, 0.0, 22.5, 21.17862944498578, -0.6833646884965647, 1.0, 1.0, 45.0, 55.86708036931606], 
processed observation next is [1.0, 0.43478260869565216, 0.270544783010157, 0.74, 0.3883333333333333, 0.0, 0.375, 0.2648857870821484, 0.2722117705011451, 1.0, 1.0, 0.6, 0.5586708036931606], 
reward next is 0.3131, 
noisyNet noise sample is [array([-0.73206776], dtype=float32), -0.33541337]. 
=============================================
[2019-04-09 15:11:41,793] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[26.346888]
 [26.201336]
 [25.583803]
 [24.837595]
 [24.328753]], R is [[27.46576118]
 [27.19110298]
 [26.91919136]
 [26.64999962]
 [26.38349915]].
[2019-04-09 15:11:42,129] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.46900379e-07 3.02896151e-05 3.30725987e-03 1.47515223e-07
 8.34140658e-01 2.05592532e-02 1.38993278e-01 1.75381181e-04
 2.20846990e-03 4.64031647e-04 1.20854376e-04], sum to 1.0000
[2019-04-09 15:11:42,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7022
[2019-04-09 15:11:42,242] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.7261444e-06 4.6686741e-04 6.1491914e-03 5.7086877e-06 8.3389562e-01
 2.7060041e-02 1.1559776e-01 1.2493215e-03 9.5197763e-03 4.1488442e-03
 1.9001761e-03], sum to 1.0000
[2019-04-09 15:11:42,242] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0795
[2019-04-09 15:11:42,276] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.3197859e-08 1.2495386e-05 3.0570611e-04 9.3167785e-09 8.7861431e-01
 1.3359955e-02 1.0447070e-01 4.8518537e-05 2.2214369e-03 7.9952896e-04
 1.6724669e-04], sum to 1.0000
[2019-04-09 15:11:42,276] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2064
[2019-04-09 15:11:42,280] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.800000000000001, 69.66666666666667, 148.1666666666667, 0.0, 22.5, 21.82294076321106, -0.6878998620384341, 1.0, 1.0, 35.0, 26.884181941969654], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 213600.0000, 
sim time next is 214800.0000, 
raw observation next is [-5.4, 67.33333333333333, 149.0, 0.0, 22.5, 21.05963862257948, -0.6933526552158832, 1.0, 1.0, 35.0, 28.824877358133563], 
processed observation next is [1.0, 0.4782608695652174, 0.31301939058171746, 0.6733333333333333, 0.49666666666666665, 0.0, 0.375, 0.2549698852149567, 0.26888244826137225, 1.0, 1.0, 0.4, 0.2882487735813356], 
reward next is 0.3365, 
noisyNet noise sample is [array([0.62083644], dtype=float32), -0.20737536]. 
=============================================
[2019-04-09 15:11:42,294] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.900000000000002, 75.33333333333334, 0.0, 0.0, 19.0, 19.6893535826297, -0.9349814165137803, 0.0, 1.0, 35.0, 24.73814341439165], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 181200.0000, 
sim time next is 182400.0000, 
raw observation next is [-8.9, 76.66666666666667, 0.0, 0.0, 19.0, 19.68192361094838, -0.9761256739437552, 0.0, 1.0, 35.0, 23.88809065037837], 
processed observation next is [1.0, 0.08695652173913043, 0.21606648199445982, 0.7666666666666667, 0.0, 0.0, 0.08333333333333333, 0.1401603009123651, 0.17462477535208162, 0.0, 1.0, 0.4, 0.23888090650378369], 
reward next is 0.7611, 
noisyNet noise sample is [array([1.7080511], dtype=float32), -1.019449]. 
=============================================
[2019-04-09 15:11:42,380] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.766666666666667, 63.0, 143.6666666666667, 0.0, 22.5, 22.69599083727512, -0.4047324206087689, 1.0, 1.0, 35.0, 27.395430247729422], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 222000.0000, 
sim time next is 223200.0000, 
raw observation next is [-3.4, 62.0, 133.0, 0.0, 22.5, 22.50612392357325, -0.4298187094209494, 1.0, 1.0, 35.0, 26.10689716650825], 
processed observation next is [1.0, 0.6086956521739131, 0.368421052631579, 0.62, 0.44333333333333336, 0.0, 0.375, 0.37551032696443737, 0.35672709685968357, 1.0, 1.0, 0.4, 0.2610689716650825], 
reward next is 0.7389, 
noisyNet noise sample is [array([-0.47087646], dtype=float32), -0.19966167]. 
=============================================
[2019-04-09 15:11:42,872] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.3507612e-07 7.6709155e-05 2.3913227e-03 1.5711208e-07 7.5091225e-01
 1.3337849e-02 2.2819284e-01 3.9009174e-04 2.1791991e-03 2.1852581e-03
 3.3342952e-04], sum to 1.0000
[2019-04-09 15:11:42,872] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4852
[2019-04-09 15:11:42,960] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 22.68949769132409, -0.5031926831740902, 1.0, 1.0, 35.0, 26.179970227451527], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 234000.0000, 
sim time next is 235200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 22.1267690063187, -0.5166883187952268, 1.0, 1.0, 35.0, 28.411630240185083], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.343897417193225, 0.3277705604015911, 1.0, 1.0, 0.4, 0.2841163024018508], 
reward next is 0.7304, 
noisyNet noise sample is [array([0.1981596], dtype=float32), -0.19040446]. 
=============================================
[2019-04-09 15:11:43,006] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1471628e-06 4.5708468e-05 4.1487045e-04 4.1895476e-07 8.3913594e-01
 1.7248293e-02 1.3022946e-01 4.6110217e-04 1.0607333e-02 1.7260738e-03
 1.2966854e-04], sum to 1.0000
[2019-04-09 15:11:43,007] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5868
[2019-04-09 15:11:43,051] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.1, 81.0, 0.0, 0.0, 19.0, 20.17501658317256, -0.8339137858618538, 0.0, 1.0, 35.0, 26.116297045516312], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 256800.0000, 
sim time next is 258000.0000, 
raw observation next is [-4.3, 80.0, 0.0, 0.0, 19.0, 20.06143433535123, -0.8566039487861055, 0.0, 1.0, 35.0, 26.291378622903526], 
processed observation next is [1.0, 1.0, 0.34349030470914127, 0.8, 0.0, 0.0, 0.08333333333333333, 0.17178619461260247, 0.21446535040463152, 0.0, 1.0, 0.4, 0.2629137862290353], 
reward next is 0.7371, 
noisyNet noise sample is [array([0.72997993], dtype=float32), -0.89457]. 
=============================================
[2019-04-09 15:11:43,063] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[26.920855]
 [27.27427 ]
 [27.561823]
 [27.66984 ]
 [28.379112]], R is [[27.36738396]
 [27.83254623]
 [28.29528236]
 [28.75545883]
 [29.21340561]].
[2019-04-09 15:11:43,682] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0351690e-07 5.2827098e-05 1.6771935e-04 6.8935442e-08 5.6310582e-01
 2.2621211e-02 4.1091445e-01 2.2051740e-04 2.1617210e-03 6.3404191e-04
 1.2147076e-04], sum to 1.0000
[2019-04-09 15:11:43,682] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5344
[2019-04-09 15:11:43,804] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 21.56950469793079, -0.553859260551437, 1.0, 1.0, 35.0, 29.533621850354194], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 241200.0000, 
sim time next is 242400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 21.55717354467157, -0.5871688178275273, 0.0, 1.0, 35.0, 29.74144322120863], 
processed observation next is [1.0, 0.8260869565217391, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.2964311287226309, 0.30427706072415756, 0.0, 1.0, 0.4, 0.2974144322120863], 
reward next is 0.7026, 
noisyNet noise sample is [array([0.9109826], dtype=float32), 0.8890819]. 
=============================================
[2019-04-09 15:11:44,425] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9497634e-06 5.8771595e-05 3.3286519e-03 1.0712635e-06 7.9060495e-01
 4.2445552e-02 1.5700217e-01 3.2976572e-04 5.1853708e-03 4.6496085e-04
 5.7673256e-04], sum to 1.0000
[2019-04-09 15:11:44,425] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2717
[2019-04-09 15:11:44,545] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.933333333333334, 74.0, 116.5, 0.0, 22.5, 21.2694216192155, -0.7262796148907142, 1.0, 1.0, 35.0, 29.386311628288585], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 210000.0000, 
sim time next is 211200.0000, 
raw observation next is [-6.566666666666666, 73.0, 128.8333333333333, 0.0, 22.5, 21.26066830232071, -0.7163145294767449, 1.0, 1.0, 35.0, 30.361898112379755], 
processed observation next is [1.0, 0.43478260869565216, 0.28070175438596495, 0.73, 0.4294444444444443, 0.0, 0.375, 0.2717223585267258, 0.26122849017441835, 1.0, 1.0, 0.4, 0.30361898112379754], 
reward next is 0.2104, 
noisyNet noise sample is [array([1.3389801], dtype=float32), 1.2884657]. 
=============================================
[2019-04-09 15:11:45,058] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.2425960e-07 1.3365762e-04 1.8876952e-03 4.0212973e-07 8.2167578e-01
 1.2001828e-02 1.6101317e-01 1.2989687e-04 2.4566255e-03 5.9528375e-04
 1.0509753e-04], sum to 1.0000
[2019-04-09 15:11:45,058] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9508
[2019-04-09 15:11:45,138] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 19.0, 20.84125183702431, -0.6950691893342872, 0.0, 1.0, 35.0, 27.80459095224751], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 246000.0000, 
sim time next is 247200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 20.71650218282638, -0.6714654705885034, 0.0, 1.0, 45.0, 44.47014186470877], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.22637518190219819, 0.27617817647049886, 0.0, 1.0, 0.6, 0.44470141864708773], 
reward next is 0.5553, 
noisyNet noise sample is [array([-0.03482279], dtype=float32), -1.0793867]. 
=============================================
[2019-04-09 15:11:45,324] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3085214e-06 2.1206468e-04 2.3662196e-03 6.9243919e-07 8.1231451e-01
 1.4391795e-02 1.6640840e-01 1.8307778e-04 3.0124981e-03 9.5232553e-04
 1.5702302e-04], sum to 1.0000
[2019-04-09 15:11:45,325] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2020
[2019-04-09 15:11:45,363] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.566666666666667, 68.33333333333334, 0.0, 0.0, 19.0, 20.78839220523937, -0.702155870796601, 0.0, 1.0, 35.0, 25.782314075413673], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 249600.0000, 
sim time next is 250800.0000, 
raw observation next is [-3.733333333333333, 71.66666666666667, 0.0, 0.0, 19.0, 20.66322140549788, -0.7365291434177691, 0.0, 1.0, 35.0, 23.42401888599735], 
processed observation next is [1.0, 0.9130434782608695, 0.35918744228993543, 0.7166666666666667, 0.0, 0.0, 0.08333333333333333, 0.22193511712482325, 0.2544902855274103, 0.0, 1.0, 0.4, 0.2342401888599735], 
reward next is 0.7658, 
noisyNet noise sample is [array([-0.03482279], dtype=float32), -1.0793867]. 
=============================================
[2019-04-09 15:11:45,555] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0742313e-05 3.3723525e-04 7.1549104e-03 6.3313551e-06 6.9598013e-01
 4.0238313e-02 2.3717965e-01 7.3708559e-04 1.1448353e-02 5.9777750e-03
 9.2956692e-04], sum to 1.0000
[2019-04-09 15:11:45,567] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5492
[2019-04-09 15:11:45,627] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.23333333333333, 68.0, 0.0, 0.0, 19.0, 19.39344739331702, -1.019304866112587, 0.0, 1.0, 35.0, 32.63500616212596], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 276000.0000, 
sim time next is 277200.0000, 
raw observation next is [-10.6, 67.0, 0.0, 0.0, 19.0, 19.22803821364421, -1.06250655891898, 0.0, 1.0, 35.0, 29.24869527604687], 
processed observation next is [1.0, 0.21739130434782608, 0.1689750692520776, 0.67, 0.0, 0.0, 0.08333333333333333, 0.10233651780368429, 0.1458311470270067, 0.0, 1.0, 0.4, 0.2924869527604687], 
reward next is 0.7075, 
noisyNet noise sample is [array([1.3161292], dtype=float32), 1.1132885]. 
=============================================
[2019-04-09 15:11:45,879] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8296728e-07 6.7064061e-06 9.3490897e-05 2.4079457e-08 8.5551137e-01
 1.5918456e-02 1.2707318e-01 9.1248832e-05 9.4092556e-04 2.7847177e-04
 8.5872125e-05], sum to 1.0000
[2019-04-09 15:11:45,879] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2419
[2019-04-09 15:11:45,981] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 21.37534391837093, -0.6134334302146972, 1.0, 1.0, 35.0, 28.910294240582836], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 243600.0000, 
sim time next is 244800.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 21.13612284357195, -0.6477925762932307, 0.0, 1.0, 35.0, 29.416282816616675], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.26134357029766253, 0.28406914123558974, 0.0, 1.0, 0.4, 0.29416282816616673], 
reward next is 0.7058, 
noisyNet noise sample is [array([0.1430204], dtype=float32), 0.8468668]. 
=============================================
[2019-04-09 15:11:46,110] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4276452e-05 9.1495243e-04 2.9171065e-03 4.7403119e-06 6.9973713e-01
 3.5584636e-02 2.3961067e-01 9.8105555e-04 7.5548152e-03 1.0604828e-02
 2.0758619e-03], sum to 1.0000
[2019-04-09 15:11:46,110] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6569
[2019-04-09 15:11:46,222] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.46666666666667, 68.0, 0.0, 0.0, 22.5, 18.74319988708752, -1.148578910534698, 1.0, 1.0, 35.0, 44.25062713572629], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 285600.0000, 
sim time next is 286800.0000, 
raw observation next is [-12.63333333333333, 69.0, 0.0, 0.0, 22.5, 18.82986903925035, -1.146721200711749, 1.0, 1.0, 35.0, 35.6150127527748], 
processed observation next is [1.0, 0.30434782608695654, 0.11265004616805181, 0.69, 0.0, 0.0, 0.375, 0.06915575327086258, 0.1177595997627503, 1.0, 1.0, 0.4, 0.35615012752774805], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6083807], dtype=float32), 0.2756358]. 
=============================================
[2019-04-09 15:11:47,019] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.79429514e-07 1.02818405e-04 8.62827699e-04 5.99508780e-08
 8.14976394e-01 1.11945383e-02 1.70963272e-01 2.10291721e-04
 9.14003293e-04 7.09108775e-04 6.63843603e-05], sum to 1.0000
[2019-04-09 15:11:47,020] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0515
[2019-04-09 15:11:47,146] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 19.0, 20.89113939072004, -0.724443924353141, 0.0, 1.0, 35.0, 25.015856689228364], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 246000.0000, 
sim time next is 247200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 20.60251499835902, -0.7747183341596159, 0.0, 1.0, 35.0, 25.328328377483828], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.21687624986325163, 0.24176055528012805, 0.0, 1.0, 0.4, 0.2532832837748383], 
reward next is 0.7467, 
noisyNet noise sample is [array([1.3680222], dtype=float32), -1.1102536]. 
=============================================
[2019-04-09 15:11:47,229] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.7514190e-06 2.8975579e-04 2.9597075e-03 5.7725142e-06 6.8357241e-01
 3.8836218e-02 2.5533524e-01 9.3027251e-04 3.8234419e-03 9.9779684e-03
 4.2604590e-03], sum to 1.0000
[2019-04-09 15:11:47,229] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8194
[2019-04-09 15:11:47,259] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.366666666666667, 68.33333333333333, 0.0, 0.0, 19.0, 20.21946707035295, -0.8374595068178737, 0.0, 1.0, 45.0, 40.342803478779665], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 268800.0000, 
sim time next is 270000.0000, 
raw observation next is [-8.9, 67.0, 0.0, 0.0, 19.0, 20.19323113981331, -0.8391628292318846, 0.0, 1.0, 45.0, 41.16679782964114], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.67, 0.0, 0.0, 0.08333333333333333, 0.18276926165110918, 0.22027905692270514, 0.0, 1.0, 0.6, 0.4116679782964114], 
reward next is 0.5883, 
noisyNet noise sample is [array([-0.4423606], dtype=float32), 0.8635909]. 
=============================================
[2019-04-09 15:11:47,277] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[23.599056]
 [23.70163 ]
 [24.346292]
 [24.997906]
 [25.227774]], R is [[23.33484077]
 [23.6980648 ]
 [24.18985558]
 [24.64189529]
 [24.99450111]].
[2019-04-09 15:11:47,305] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.8424123e-07 5.2224485e-05 1.8981135e-03 4.8733051e-07 6.8319285e-01
 6.3216135e-02 2.4603152e-01 1.5383614e-04 3.4231516e-03 1.7672902e-03
 2.6366935e-04], sum to 1.0000
[2019-04-09 15:11:47,305] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0038
[2019-04-09 15:11:47,388] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 22.73818656576774, -0.3883778096319602, 1.0, 1.0, 35.0, 34.1492168957918], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 235200.0000, 
sim time next is 236400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 22.70001761341384, -0.447231505849144, 1.0, 1.0, 35.0, 28.44057682054719], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.39166813445115345, 0.3509228313836186, 1.0, 1.0, 0.4, 0.2844057682054719], 
reward next is 0.7156, 
noisyNet noise sample is [array([0.30422008], dtype=float32), -1.4875968]. 
=============================================
[2019-04-09 15:11:47,708] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.06715795e-07 1.00987263e-05 2.94944039e-04 8.14256254e-08
 8.33595753e-01 1.55689511e-02 1.46501869e-01 1.68850864e-04
 3.21030268e-03 5.25428681e-04 1.23735081e-04], sum to 1.0000
[2019-04-09 15:11:47,708] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2255
[2019-04-09 15:11:47,844] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 21.5778427271906, -0.6192385602280922, 1.0, 1.0, 35.0, 28.96047919956424], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 240000.0000, 
sim time next is 241200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 21.22823777183029, -0.6356709206398538, 1.0, 1.0, 35.0, 29.609054282244355], 
processed observation next is [1.0, 0.8260869565217391, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.26901981431919086, 0.28810969312004875, 1.0, 1.0, 0.4, 0.29609054282244357], 
reward next is 0.5701, 
noisyNet noise sample is [array([0.20084414], dtype=float32), 0.3504776]. 
=============================================
[2019-04-09 15:11:48,126] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.9887444e-06 1.9102221e-04 1.4744289e-03 4.6035625e-06 8.0158234e-01
 1.7583562e-02 1.6969712e-01 5.8105506e-04 5.8243042e-03 1.8593948e-03
 1.1931484e-03], sum to 1.0000
[2019-04-09 15:11:48,127] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2956
[2019-04-09 15:11:48,245] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.63333333333333, 69.0, 0.0, 0.0, 22.5, 18.76938006171938, -1.08396590501558, 1.0, 1.0, 45.0, 58.18954126230792], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 286800.0000, 
sim time next is 288000.0000, 
raw observation next is [-12.8, 70.0, 15.0, 205.5, 22.5, 19.38389882648835, -1.026601338205755, 1.0, 1.0, 35.0, 37.81029567121948], 
processed observation next is [1.0, 0.34782608695652173, 0.1080332409972299, 0.7, 0.05, 0.22707182320441988, 0.375, 0.11532490220736236, 0.15779955393141498, 1.0, 1.0, 0.4, 0.3781029567121948], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.4691944], dtype=float32), -1.5452778]. 
=============================================
[2019-04-09 15:11:48,252] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[22.711905]
 [22.027542]
 [21.812376]
 [21.557613]
 [21.603767]], R is [[22.97870445]
 [22.74891853]
 [22.52142906]
 [22.29621506]
 [22.07325363]].
[2019-04-09 15:11:48,365] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.17838265e-07 2.31596605e-05 6.99102820e-04 9.90663764e-08
 9.29898798e-01 4.78117401e-03 6.12196922e-02 1.03659266e-04
 2.75895768e-03 3.82591505e-04 1.32236368e-04], sum to 1.0000
[2019-04-09 15:11:48,366] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6595
[2019-04-09 15:11:48,444] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 19.0, 20.57062356968308, -0.7600985246094827, 0.0, 1.0, 35.0, 27.9116473639779], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 246000.0000, 
sim time next is 247200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 20.41145794282075, -0.7920914427064684, 0.0, 1.0, 35.0, 25.96841830022787], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.20095482856839583, 0.2359695190978439, 0.0, 1.0, 0.4, 0.2596841830022787], 
reward next is 0.7403, 
noisyNet noise sample is [array([-0.8300035], dtype=float32), 0.035304446]. 
=============================================
[2019-04-09 15:11:49,186] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9154211e-07 6.9912843e-05 2.5712154e-04 1.4814707e-07 8.2941741e-01
 1.6473038e-02 1.5008652e-01 7.7390891e-05 2.5575168e-03 9.0406329e-04
 1.5660161e-04], sum to 1.0000
[2019-04-09 15:11:49,186] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5652
[2019-04-09 15:11:49,343] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.8, 70.0, 0.0, 0.0, 22.5, 20.45464873161163, -0.7373195999806147, 0.0, 1.0, 45.0, 60.99833277817317], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 331200.0000, 
sim time next is 332400.0000, 
raw observation next is [-12.8, 72.33333333333334, 0.0, 0.0, 19.0, 20.60933109279564, -0.7481490668282262, 0.0, 1.0, 35.0, 35.390068472875726], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7233333333333334, 0.0, 0.0, 0.08333333333333333, 0.2174442577329699, 0.2506169777239246, 0.0, 1.0, 0.4, 0.35390068472875724], 
reward next is 0.6461, 
noisyNet noise sample is [array([2.1290104], dtype=float32), 0.0062316335]. 
=============================================
[2019-04-09 15:11:49,523] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.5493254e-08 9.2266773e-06 7.7388412e-04 1.0877542e-07 9.0193617e-01
 9.9155838e-03 8.3955497e-02 5.5571570e-04 2.5713637e-03 1.8690634e-04
 9.5413910e-05], sum to 1.0000
[2019-04-09 15:11:49,523] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8539
[2019-04-09 15:11:49,613] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.866666666666667, 45.66666666666667, 85.0, 736.8333333333334, 22.5, 21.22846058704816, -0.6691401731721077, 1.0, 1.0, 35.0, 27.91105722269876], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 304800.0000, 
sim time next is 306000.0000, 
raw observation next is [-9.5, 44.0, 89.0, 694.5, 22.5, 21.14536686824701, -0.6681532220480784, 1.0, 1.0, 35.0, 28.78409286172183], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.2966666666666667, 0.7674033149171271, 0.375, 0.2621139056872508, 0.2772822593173072, 1.0, 1.0, 0.4, 0.28784092861721833], 
reward next is 0.4534, 
noisyNet noise sample is [array([0.52073306], dtype=float32), 0.55707717]. 
=============================================
[2019-04-09 15:11:49,622] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[29.544704]
 [29.329445]
 [29.073078]
 [28.854872]
 [28.3777  ]], R is [[29.54732513]
 [29.70384216]
 [29.98512459]
 [30.3620472 ]
 [30.52601433]].
[2019-04-09 15:11:49,704] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3394977e-07 3.7960603e-05 6.3951063e-04 1.8071094e-07 9.3847293e-01
 1.4684968e-02 4.3412518e-02 1.5028236e-04 1.9933030e-03 5.0849468e-04
 9.9539291e-05], sum to 1.0000
[2019-04-09 15:11:49,704] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4800
[2019-04-09 15:11:49,738] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.24320865e-06 3.33172211e-04 1.87559123e-03 1.63271125e-06
 8.34772348e-01 4.55378108e-02 1.06373675e-01 7.52914639e-04
 8.07487220e-03 1.79761299e-03 4.74179629e-04], sum to 1.0000
[2019-04-09 15:11:49,738] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6137
[2019-04-09 15:11:49,780] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.7, 70.0, 0.0, 0.0, 19.0, 18.57726325989804, -1.213886385042566, 0.0, 1.0, 35.0, 28.20631340427047], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 280800.0000, 
sim time next is 282000.0000, 
raw observation next is [-11.9, 69.0, 0.0, 0.0, 19.0, 18.35793662698324, -1.266925493310199, 0.0, 1.0, 35.0, 29.511575400923263], 
processed observation next is [1.0, 0.2608695652173913, 0.13296398891966757, 0.69, 0.0, 0.0, 0.08333333333333333, 0.02982805224860326, 0.07769150222993366, 0.0, 1.0, 0.4, 0.2951157540092326], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7299738], dtype=float32), -0.64657855]. 
=============================================
[2019-04-09 15:11:49,790] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[22.535742]
 [22.314   ]
 [22.41734 ]
 [22.732618]
 [22.137333]], R is [[21.79296875]
 [21.8166275 ]
 [22.17108345]
 [22.32938957]
 [22.10609627]].
[2019-04-09 15:11:49,805] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.6, 56.33333333333334, 102.8333333333333, 633.6666666666667, 22.5, 21.42780986235023, -0.6442343700490042, 1.0, 1.0, 35.0, 30.361585339549134], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 300000.0000, 
sim time next is 301200.0000, 
raw observation next is [-10.6, 52.66666666666667, 102.1666666666667, 674.6666666666666, 22.5, 21.46756511698376, -0.6275975481647048, 1.0, 1.0, 35.0, 30.278924425347938], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.5266666666666667, 0.34055555555555567, 0.74548802946593, 0.375, 0.2889637597486467, 0.2908008172784317, 1.0, 1.0, 0.4, 0.30278924425347936], 
reward next is 0.5913, 
noisyNet noise sample is [array([-2.7077248], dtype=float32), -2.2674365]. 
=============================================
[2019-04-09 15:11:49,968] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5035564e-07 7.9056634e-05 1.5666573e-03 1.1231279e-07 8.8312227e-01
 1.3008537e-02 9.8148301e-02 1.5924254e-04 2.9680810e-03 7.1420491e-04
 2.3332075e-04], sum to 1.0000
[2019-04-09 15:11:49,968] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3122
[2019-04-09 15:11:50,060] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.6, 49.0, 12.0, 123.0, 22.5, 23.02090550649064, -0.3357702256131587, 1.0, 1.0, 35.0, 29.343442931465468], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 320400.0000, 
sim time next is 321600.0000, 
raw observation next is [-10.96666666666667, 51.66666666666667, 0.0, 0.0, 22.5, 22.82068044143594, -0.4120792402291183, 1.0, 1.0, 35.0, 27.661458995373124], 
processed observation next is [1.0, 0.7391304347826086, 0.15881809787626952, 0.5166666666666667, 0.0, 0.0, 0.375, 0.40172337011966164, 0.3626402532569606, 1.0, 1.0, 0.4, 0.2766145899537312], 
reward next is 0.7234, 
noisyNet noise sample is [array([0.28869888], dtype=float32), 0.8759887]. 
=============================================
[2019-04-09 15:11:50,267] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5200816e-05 1.2843133e-03 4.8050657e-03 7.1887303e-06 8.2404786e-01
 4.9078364e-02 9.5031142e-02 1.3629466e-03 1.9091673e-02 4.4559394e-03
 8.2019699e-04], sum to 1.0000
[2019-04-09 15:11:50,268] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5901
[2019-04-09 15:11:50,326] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 19.0, 18.44871864244225, -1.260187925880637, 0.0, 1.0, 35.0, 30.953227958517274], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 352800.0000, 
sim time next is 354000.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 19.0, 18.2094443601368, -1.310990770582603, 0.0, 1.0, 35.0, 31.60927118338006], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.08333333333333333, 0.01745369667806666, 0.06300307647246568, 0.0, 1.0, 0.4, 0.3160927118338006], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.94009596], dtype=float32), 1.0761793]. 
=============================================
[2019-04-09 15:11:50,340] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[20.504015]
 [21.071531]
 [21.564299]
 [21.851778]
 [22.38417 ]], R is [[19.81822586]
 [19.62004471]
 [19.42384529]
 [19.30516243]
 [19.21566391]].
[2019-04-09 15:11:50,760] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.9879263e-06 3.1924466e-04 2.7231197e-03 1.7880270e-06 7.0314658e-01
 3.6720805e-02 2.4675295e-01 4.7056781e-04 7.3538469e-03 1.7096224e-03
 7.9646282e-04], sum to 1.0000
[2019-04-09 15:11:50,760] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5035
[2019-04-09 15:11:50,900] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.46666666666667, 68.0, 40.83333333333333, 385.5, 22.5, 19.81517439262015, -0.9121379927733716, 1.0, 1.0, 45.0, 52.78448798809673], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 290400.0000, 
sim time next is 291600.0000, 
raw observation next is [-12.3, 67.0, 62.5, 384.5, 22.5, 20.47419041839823, -0.8621083100527525, 1.0, 1.0, 35.0, 37.190463051075014], 
processed observation next is [1.0, 0.391304347826087, 0.12188365650969527, 0.67, 0.20833333333333334, 0.4248618784530387, 0.375, 0.20618253486651916, 0.21263056331574917, 1.0, 1.0, 0.4, 0.3719046305107501], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0514686], dtype=float32), -1.5233067]. 
=============================================
[2019-04-09 15:11:51,439] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5533354e-07 1.4973197e-05 2.3561527e-04 6.5246148e-08 7.9760897e-01
 4.3623019e-03 1.9650559e-01 1.3217638e-04 7.7722949e-04 3.3798293e-04
 2.4980034e-05], sum to 1.0000
[2019-04-09 15:11:51,439] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5273
[2019-04-09 15:11:51,594] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.63333333333333, 67.66666666666667, 0.0, 0.0, 22.5, 20.63307415255391, -0.7831463033668394, 1.0, 1.0, 35.0, 29.92026634564029], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 330000.0000, 
sim time next is 331200.0000, 
raw observation next is [-12.8, 70.0, 0.0, 0.0, 22.5, 20.29397730432391, -0.8372754833057381, 0.0, 1.0, 35.0, 32.13649972015273], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7, 0.0, 0.0, 0.375, 0.19116477536032583, 0.22090817223142065, 0.0, 1.0, 0.4, 0.3213649972015273], 
reward next is 0.6786, 
noisyNet noise sample is [array([-0.6155841], dtype=float32), 0.3410307]. 
=============================================
[2019-04-09 15:11:53,154] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2476058e-07 8.9596455e-05 3.4448737e-04 3.6185341e-08 8.2445413e-01
 1.1010542e-02 1.6166443e-01 3.7333262e-05 1.1779884e-03 1.0710716e-03
 1.5022595e-04], sum to 1.0000
[2019-04-09 15:11:53,155] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4309
[2019-04-09 15:11:53,269] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 43.33333333333334, 84.16666666666667, 624.3333333333334, 22.5, 22.01441572166042, -0.4864871783532782, 1.0, 1.0, 35.0, 29.622890219988157], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 310800.0000, 
sim time next is 312000.0000, 
raw observation next is [-9.5, 42.66666666666667, 80.0, 598.6666666666667, 22.5, 22.19592267874395, -0.4601906118406474, 1.0, 1.0, 35.0, 29.717119544045065], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4266666666666667, 0.26666666666666666, 0.661510128913444, 0.375, 0.3496602232286626, 0.3466031293864509, 1.0, 1.0, 0.4, 0.2971711954404507], 
reward next is 0.7028, 
noisyNet noise sample is [array([-1.2007546], dtype=float32), -1.0603535]. 
=============================================
[2019-04-09 15:11:53,311] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[30.911629]
 [30.940475]
 [31.174175]
 [31.154055]
 [30.908457]], R is [[31.29845238]
 [31.6892395 ]
 [32.09696579]
 [32.48691559]
 [32.89489365]].
[2019-04-09 15:11:53,744] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.3807954e-06 2.1158038e-04 9.8092004e-04 7.6618733e-07 5.0672191e-01
 2.3972930e-02 4.6301582e-01 5.6700362e-04 2.4214128e-03 1.2647526e-03
 8.3847262e-04], sum to 1.0000
[2019-04-09 15:11:53,745] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7758
[2019-04-09 15:11:53,941] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.5, 66.0, 55.0, 733.5, 22.5, 19.28754434508067, -1.135047504138971, 1.0, 1.0, 35.0, 33.90387086734369], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 381600.0000, 
sim time next is 382800.0000, 
raw observation next is [-14.13333333333333, 64.0, 65.66666666666667, 730.5, 22.5, 19.60150805277983, -1.008551513401942, 1.0, 1.0, 45.0, 58.192100014809455], 
processed observation next is [1.0, 0.43478260869565216, 0.07109879963065568, 0.64, 0.2188888888888889, 0.8071823204419889, 0.375, 0.13345900439831926, 0.16381616219935266, 1.0, 1.0, 0.6, 0.5819210001480946], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9573364], dtype=float32), -0.3069626]. 
=============================================
[2019-04-09 15:11:55,266] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.0170157e-06 3.3335539e-04 3.2591939e-03 2.1382409e-06 7.3917276e-01
 4.7533557e-02 1.9466193e-01 6.7915191e-04 9.8448051e-03 3.1150368e-03
 1.3900914e-03], sum to 1.0000
[2019-04-09 15:11:55,267] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6967
[2019-04-09 15:11:55,449] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-16.36666666666667, 79.0, 0.0, 0.0, 22.5, 17.41686177248677, -1.478524003771214, 1.0, 1.0, 35.0, 38.789551603603925], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 372000.0000, 
sim time next is 373200.0000, 
raw observation next is [-16.53333333333333, 80.0, 0.0, 0.0, 22.5, 17.50680945007267, -1.4968193069917, 1.0, 1.0, 35.0, 36.60276402124146], 
processed observation next is [1.0, 0.30434782608695654, 0.0046168051708218244, 0.8, 0.0, 0.0, 0.375, -0.041099212493944215, 0.00106023100276668, 1.0, 1.0, 0.4, 0.36602764021241463], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0035636], dtype=float32), -0.63308847]. 
=============================================
[2019-04-09 15:11:56,944] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.6781897e-06 2.1528521e-04 1.6288774e-03 5.2989304e-07 8.0890650e-01
 3.2679178e-02 1.4682901e-01 2.3253585e-04 7.6566809e-03 1.5368707e-03
 3.1282392e-04], sum to 1.0000
[2019-04-09 15:11:56,944] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1309
[2019-04-09 15:11:57,062] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.533333333333333, 26.0, 127.8333333333333, 0.0, 22.5, 21.2660732807816, -0.8991543630648028, 1.0, 1.0, 35.0, 26.42113129988206], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 476400.0000, 
sim time next is 477600.0000, 
raw observation next is [-1.366666666666667, 27.0, 127.3333333333333, 0.0, 22.5, 20.69697128148333, -0.900528618657432, 1.0, 1.0, 35.0, 27.87017508609867], 
processed observation next is [1.0, 0.5217391304347826, 0.42474607571560485, 0.27, 0.42444444444444435, 0.0, 0.375, 0.2247476067902774, 0.19982379378085602, 1.0, 1.0, 0.4, 0.2787017508609867], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.02475131], dtype=float32), 1.6426415]. 
=============================================
[2019-04-09 15:11:57,722] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0080488e-07 5.1099527e-05 1.1507733e-03 4.2470518e-07 7.9231280e-01
 6.5719937e-03 1.9692163e-01 3.2312871e-04 1.9324186e-03 6.9201348e-04
 4.3284348e-05], sum to 1.0000
[2019-04-09 15:11:57,722] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2999
[2019-04-09 15:11:57,744] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.8733290e-06 4.7036496e-04 5.5716657e-03 4.6598821e-06 3.8068131e-01
 3.6601171e-02 5.6223154e-01 3.9383999e-04 9.5157204e-03 3.4045645e-03
 1.1152879e-03], sum to 1.0000
[2019-04-09 15:11:57,745] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4218
[2019-04-09 15:11:57,780] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.53333333333333, 54.33333333333334, 0.0, 0.0, 19.0, 18.40493901750055, -1.282601020103377, 0.0, 1.0, 35.0, 27.180314555279285], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 433200.0000, 
sim time next is 434400.0000, 
raw observation next is [-11.36666666666667, 54.66666666666667, 0.0, 0.0, 19.0, 18.27698186755561, -1.293666787381485, 0.0, 1.0, 40.0, 35.7388059865825], 
processed observation next is [1.0, 0.0, 0.14773776546629722, 0.5466666666666667, 0.0, 0.0, 0.08333333333333333, 0.023081822296300736, 0.06877773753950496, 0.0, 1.0, 0.5, 0.357388059865825], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.98715144], dtype=float32), 0.6389801]. 
=============================================
[2019-04-09 15:11:57,801] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1666727e-05 5.3193502e-04 6.0792528e-03 5.8880846e-06 3.6147106e-01
 3.7845936e-02 5.7873893e-01 4.2487847e-04 1.0037801e-02 3.6851054e-03
 1.1675743e-03], sum to 1.0000
[2019-04-09 15:11:57,802] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1495
[2019-04-09 15:11:57,830] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.2, 57.00000000000001, 60.16666666666666, 758.1666666666667, 22.5, 20.27279075211273, -0.970644616998206, 1.0, 1.0, 35.0, 26.75913100024055], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 386400.0000, 
sim time next is 387600.0000, 
raw observation next is [-13.0, 54.0, 58.0, 787.5, 22.5, 20.2615090808287, -0.9588879427993485, 1.0, 1.0, 35.0, 29.14537352145999], 
processed observation next is [1.0, 0.4782608695652174, 0.10249307479224376, 0.54, 0.19333333333333333, 0.8701657458563536, 0.375, 0.18845909006905823, 0.1803706857335505, 1.0, 1.0, 0.4, 0.2914537352145999], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.53638744], dtype=float32), -0.112601995]. 
=============================================
[2019-04-09 15:11:57,852] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.36666666666667, 54.66666666666667, 0.0, 0.0, 19.0, 18.27698186755561, -1.293666787381485, 0.0, 1.0, 40.0, 35.7388059865825], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 434400.0000, 
sim time next is 435600.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 19.0, 18.28395477405724, -1.299374579308499, 0.0, 1.0, 35.0, 26.92921261531657], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 0.08333333333333333, 0.02366289783810327, 0.06687514023050034, 0.0, 1.0, 0.4, 0.26929212615316567], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.98715144], dtype=float32), 0.6389801]. 
=============================================
[2019-04-09 15:11:57,893] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.5077913e-08 2.0909989e-05 7.1908883e-04 5.8482300e-08 8.3811319e-01
 3.4082513e-02 1.2432995e-01 5.3317523e-05 2.5295541e-03 7.2906791e-05
 7.8356134e-05], sum to 1.0000
[2019-04-09 15:11:57,893] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3980
[2019-04-09 15:11:57,989] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.3, 39.33333333333334, 38.83333333333334, 727.8333333333334, 22.5, 21.45765922819935, -0.5891833148547119, 1.0, 1.0, 35.0, 27.189403756325994], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 400800.0000, 
sim time next is 402000.0000, 
raw observation next is [-9.100000000000001, 38.66666666666667, 34.33333333333334, 656.3333333333334, 22.5, 21.89524031163079, -0.5368223628586702, 1.0, 1.0, 35.0, 27.82328971583066], 
processed observation next is [1.0, 0.6521739130434783, 0.21052631578947364, 0.3866666666666667, 0.11444444444444447, 0.7252302025782689, 0.375, 0.32460335930256584, 0.32105921238044327, 1.0, 1.0, 0.4, 0.2782328971583066], 
reward next is 0.7411, 
noisyNet noise sample is [array([0.539236], dtype=float32), 0.6287698]. 
=============================================
[2019-04-09 15:11:58,043] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[30.546541]
 [30.683298]
 [30.564545]
 [30.405762]
 [30.487864]], R is [[31.09264183]
 [31.47955704]
 [31.88950539]
 [32.30625916]
 [32.6661911 ]].
[2019-04-09 15:11:59,654] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3354237e-07 4.2397849e-05 8.2381739e-04 2.8630478e-07 6.0595417e-01
 5.1160090e-02 3.3967745e-01 1.3322177e-04 1.6935237e-03 3.6090691e-04
 1.5391207e-04], sum to 1.0000
[2019-04-09 15:11:59,654] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2726
[2019-04-09 15:11:59,744] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 22.5, 21.61468498335997, -0.6574576788101447, 1.0, 1.0, 35.0, 34.61691331038263], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 412800.0000, 
sim time next is 414000.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 22.5, 21.28030750091065, -0.7416262254975502, 1.0, 1.0, 35.0, 32.17872739419211], 
processed observation next is [1.0, 0.8260869565217391, 0.1994459833795014, 0.4, 0.0, 0.0, 0.375, 0.2733589584092207, 0.25279125816748327, 1.0, 1.0, 0.4, 0.3217872739419211], 
reward next is 0.0551, 
noisyNet noise sample is [array([1.3456995], dtype=float32), -0.41178355]. 
=============================================
[2019-04-09 15:11:59,749] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[29.3673  ]
 [29.546665]
 [29.684847]
 [29.497927]
 [29.52391 ]], R is [[29.1046257 ]
 [29.2887516 ]
 [29.52015686]
 [29.76573944]
 [29.99416351]].
[2019-04-09 15:12:01,067] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2937729e-06 1.7707958e-04 6.4688944e-03 5.0312116e-07 6.8351310e-01
 3.1092955e-02 2.7037734e-01 4.2394357e-04 4.1602575e-03 3.3850262e-03
 3.9857652e-04], sum to 1.0000
[2019-04-09 15:12:01,067] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9213
[2019-04-09 15:12:01,173] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.6, 47.0, 0.0, 0.0, 19.0, 19.35032490072211, -1.068528668716369, 0.0, 1.0, 35.0, 27.854840608879776], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 421200.0000, 
sim time next is 422400.0000, 
raw observation next is [-10.6, 47.66666666666667, 0.0, 0.0, 19.0, 19.18500165250016, -1.057864266184241, 0.0, 1.0, 45.0, 47.380196146686174], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.47666666666666674, 0.0, 0.0, 0.08333333333333333, 0.09875013770834655, 0.14737857793858633, 0.0, 1.0, 0.6, 0.47380196146686177], 
reward next is 0.5262, 
noisyNet noise sample is [array([0.40853405], dtype=float32), 0.5910483]. 
=============================================
[2019-04-09 15:12:01,667] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.26265295e-06 4.03408776e-04 6.99952198e-03 4.96205485e-06
 7.68172920e-01 1.63874030e-02 1.91355646e-01 2.26426311e-03
 1.10653825e-02 2.52977945e-03 8.08456913e-04], sum to 1.0000
[2019-04-09 15:12:01,667] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7245
[2019-04-09 15:12:01,691] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.36666666666667, 54.66666666666667, 0.0, 0.0, 19.0, 18.7800424087212, -1.221858578019455, 0.0, 1.0, 35.0, 25.160890614637708], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 434400.0000, 
sim time next is 435600.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 19.0, 18.65043210605747, -1.250680107302133, 0.0, 1.0, 35.0, 24.461242509048404], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 0.08333333333333333, 0.05420267550478908, 0.083106630899289, 0.0, 1.0, 0.4, 0.24461242509048403], 
reward next is 0.4376, 
noisyNet noise sample is [array([1.1870881], dtype=float32), -0.42641377]. 
=============================================
[2019-04-09 15:12:02,084] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.09035398e-06 1.09576635e-04 1.53666281e-03 7.48407331e-07
 5.03348112e-01 6.01873882e-02 4.27754581e-01 1.40391450e-04
 4.79168026e-03 1.73678179e-03 3.93112830e-04], sum to 1.0000
[2019-04-09 15:12:02,084] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9909
[2019-04-09 15:12:02,239] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.9, 26.0, 123.1666666666667, 0.0, 22.5, 21.39848350065379, -0.7415896046101914, 1.0, 1.0, 45.0, 48.45299531138845], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 474000.0000, 
sim time next is 475200.0000, 
raw observation next is [-1.7, 25.0, 125.5, 0.0, 22.5, 21.61844384157624, -0.6912762767592738, 1.0, 1.0, 45.0, 45.454768246496016], 
processed observation next is [1.0, 0.5217391304347826, 0.4155124653739613, 0.25, 0.41833333333333333, 0.0, 0.375, 0.30153698679802005, 0.2695745744135754, 1.0, 1.0, 0.6, 0.45454768246496013], 
reward next is 0.3078, 
noisyNet noise sample is [array([0.2032724], dtype=float32), -0.2877538]. 
=============================================
[2019-04-09 15:12:02,602] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.16147416e-05 4.13618487e-04 1.10494364e-02 1.47062865e-05
 7.79363930e-01 4.74729352e-02 1.37183383e-01 1.47197302e-03
 1.15749063e-02 1.03718154e-02 1.05167681e-03], sum to 1.0000
[2019-04-09 15:12:02,602] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8039
[2019-04-09 15:12:02,642] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 19.0, 18.17174492498659, -1.324233372060044, 0.0, 1.0, 40.0, 36.02102213331007], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 439200.0000, 
sim time next is 440400.0000, 
raw observation next is [-11.0, 53.0, 0.0, 0.0, 19.0, 18.1298999879805, -1.328519713302796, 0.0, 1.0, 40.0, 36.20981957107549], 
processed observation next is [1.0, 0.08695652173913043, 0.15789473684210528, 0.53, 0.0, 0.0, 0.08333333333333333, 0.01082499899837508, 0.05716009556573468, 0.0, 1.0, 0.5, 0.3620981957107549], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.124706], dtype=float32), -0.32929406]. 
=============================================
[2019-04-09 15:12:02,676] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0738724e-05 1.1836450e-03 1.3952455e-02 1.9973855e-05 5.8747602e-01
 6.5634042e-02 2.8897178e-01 3.2142419e-03 2.5229177e-02 1.1540744e-02
 2.7471692e-03], sum to 1.0000
[2019-04-09 15:12:02,676] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5936
[2019-04-09 15:12:02,725] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.23333333333333, 49.33333333333334, 0.0, 0.0, 19.0, 18.31646702091723, -1.281152299527717, 0.0, 1.0, 65.0, 65.8387158185964], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 451200.0000, 
sim time next is 452400.0000, 
raw observation next is [-9.866666666666667, 46.66666666666667, 0.0, 0.0, 19.0, 18.46157385466912, -1.266022966074976, 0.0, 1.0, 35.0, 40.8563560249325], 
processed observation next is [1.0, 0.21739130434782608, 0.18928901200369344, 0.46666666666666673, 0.0, 0.0, 0.08333333333333333, 0.038464487889093256, 0.07799234464167466, 0.0, 1.0, 0.4, 0.408563560249325], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3014035], dtype=float32), 1.9279227]. 
=============================================
[2019-04-09 15:12:03,542] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.5995226e-07 9.1381853e-06 4.2180659e-04 5.3938230e-08 8.1339717e-01
 7.6504988e-03 1.7660627e-01 1.6072289e-04 9.6273440e-04 7.4997492e-04
 4.1391606e-05], sum to 1.0000
[2019-04-09 15:12:03,542] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5521
[2019-04-09 15:12:03,654] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.666666666666668, 40.66666666666667, 0.0, 0.0, 22.5, 21.35603399384589, -0.6923201210360261, 1.0, 1.0, 35.0, 31.072332175964213], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 415200.0000, 
sim time next is 416400.0000, 
raw observation next is [-9.833333333333332, 41.33333333333334, 0.0, 0.0, 22.5, 20.93301249728788, -0.7720094126607308, 1.0, 1.0, 35.0, 28.581466968396363], 
processed observation next is [1.0, 0.8260869565217391, 0.19021237303785785, 0.41333333333333344, 0.0, 0.0, 0.375, 0.24441770810732333, 0.24266352911308975, 1.0, 1.0, 0.4, 0.28581466968396363], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09649929], dtype=float32), -0.21072514]. 
=============================================
[2019-04-09 15:12:04,280] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0424129e-05 7.2039192e-04 1.1976628e-02 1.5106681e-05 7.1874470e-01
 3.2950934e-02 2.0949048e-01 1.6020591e-03 1.4957185e-02 7.7415314e-03
 1.7705549e-03], sum to 1.0000
[2019-04-09 15:12:04,280] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9596
[2019-04-09 15:12:04,330] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 53.0, 0.0, 0.0, 19.0, 18.82340380024944, -1.193018387051171, 0.0, 1.0, 35.0, 31.784029913879248], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 440400.0000, 
sim time next is 441600.0000, 
raw observation next is [-10.8, 51.0, 0.0, 0.0, 19.0, 18.81112517683226, -1.19948523693256, 0.0, 1.0, 45.0, 41.12827432887873], 
processed observation next is [1.0, 0.08695652173913043, 0.1634349030470914, 0.51, 0.0, 0.0, 0.08333333333333333, 0.06759376473602163, 0.10017158768914669, 0.0, 1.0, 0.6, 0.41128274328878733], 
reward next is 0.6014, 
noisyNet noise sample is [array([0.40075433], dtype=float32), 1.6122864]. 
=============================================
[2019-04-09 15:12:05,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.58352725e-06 1.03162216e-04 8.05716321e-04 1.85919498e-06
 7.99409628e-01 4.95166406e-02 1.44057989e-01 2.57043575e-04
 2.79965065e-03 2.82379845e-03 2.21945797e-04], sum to 1.0000
[2019-04-09 15:12:05,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7291
[2019-04-09 15:12:06,205] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.066666666666666, 32.33333333333334, 68.0, 0.0, 22.5, 19.98864106636124, -1.002205795792626, 1.0, 1.0, 45.0, 57.22347133802751], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 466800.0000, 
sim time next is 468000.0000, 
raw observation next is [-4.5, 32.0, 80.0, 0.0, 22.5, 20.64336587143657, -0.9332677886594861, 1.0, 1.0, 35.0, 34.777011888423885], 
processed observation next is [1.0, 0.43478260869565216, 0.3379501385041552, 0.32, 0.26666666666666666, 0.0, 0.375, 0.2202804892863807, 0.18891073711350462, 1.0, 1.0, 0.4, 0.34777011888423887], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.11193761], dtype=float32), -0.533103]. 
=============================================
[2019-04-09 15:12:06,213] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[25.335192]
 [24.339437]
 [23.834303]
 [23.12453 ]
 [22.807503]], R is [[25.50332451]
 [25.24829102]
 [24.99580765]
 [24.74584961]
 [24.49839211]].
[2019-04-09 15:12:06,482] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.0177266e-06 4.3487293e-04 1.5605682e-03 2.7569040e-06 7.4391466e-01
 2.4915738e-02 2.1674711e-01 9.1226457e-04 6.6415258e-03 4.4739502e-03
 3.8954365e-04], sum to 1.0000
[2019-04-09 15:12:06,482] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1083
[2019-04-09 15:12:06,502] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 83.0, 83.0, 138.0, 19.0, 19.84939938688869, -0.9013907148447601, 0.0, 1.0, 35.0, 22.54305501686554], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 558000.0000, 
sim time next is 559200.0000, 
raw observation next is [-0.6666666666666667, 82.33333333333334, 87.0, 136.0, 19.0, 19.82541120584483, -0.9144676433569652, 0.0, 1.0, 35.0, 20.43915058198768], 
processed observation next is [0.0, 0.4782608695652174, 0.44413665743305636, 0.8233333333333335, 0.29, 0.15027624309392265, 0.08333333333333333, 0.15211760048706915, 0.19517745221434493, 0.0, 1.0, 0.4, 0.20439150581987678], 
reward next is 0.7956, 
noisyNet noise sample is [array([1.5461962], dtype=float32), -1.004256]. 
=============================================
[2019-04-09 15:12:06,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1799325e-06 1.5262910e-04 2.3937826e-03 6.0574365e-07 5.4315245e-01
 1.9407209e-02 4.2965379e-01 3.4321690e-04 2.8978987e-03 1.5740215e-03
 4.1922941e-04], sum to 1.0000
[2019-04-09 15:12:06,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7279
[2019-04-09 15:12:07,043] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.1, 27.0, 118.0, 0.0, 22.5, 21.60755715594031, -0.743169415029772, 1.0, 1.0, 35.0, 33.42386703987309], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 472800.0000, 
sim time next is 474000.0000, 
raw observation next is [-1.9, 26.0, 123.1666666666667, 0.0, 22.5, 21.66866706277953, -0.700470248470291, 1.0, 1.0, 45.0, 44.428123790543026], 
processed observation next is [1.0, 0.4782608695652174, 0.4099722991689751, 0.26, 0.4105555555555557, 0.0, 0.375, 0.3057222552316275, 0.26650991717656963, 1.0, 1.0, 0.6, 0.4442812379054303], 
reward next is 0.2690, 
noisyNet noise sample is [array([0.48307052], dtype=float32), 2.3201206]. 
=============================================
[2019-04-09 15:12:07,051] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[27.652086]
 [27.307148]
 [26.942434]
 [26.262564]
 [25.35729 ]], R is [[28.2861824 ]
 [28.04808617]
 [27.76760483]
 [27.4899292 ]
 [27.21503067]].
[2019-04-09 15:12:07,281] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3803967e-06 1.8242988e-04 3.9825477e-03 5.8195207e-07 7.6746660e-01
 2.3743941e-02 2.0094153e-01 4.0575847e-04 1.8763720e-03 1.0597140e-03
 3.3715367e-04], sum to 1.0000
[2019-04-09 15:12:07,281] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1334
[2019-04-09 15:12:07,319] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 83.0, 104.5, 138.6666666666667, 19.0, 20.56169252544987, -0.7447827957518398, 0.0, 1.0, 35.0, 25.11478636851825], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 573600.0000, 
sim time next is 574800.0000, 
raw observation next is [-1.2, 83.0, 90.16666666666667, 68.33333333333333, 19.0, 20.40893407524158, -0.7772632032874043, 0.0, 1.0, 35.0, 22.06711034425022], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.3005555555555556, 0.07550644567219153, 0.08333333333333333, 0.20074450627013155, 0.24091226557086523, 0.0, 1.0, 0.4, 0.2206711034425022], 
reward next is 0.7793, 
noisyNet noise sample is [array([1.4102978], dtype=float32), -0.8829663]. 
=============================================
[2019-04-09 15:12:07,936] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0541628e-04 3.2244637e-03 5.6987968e-03 3.4701719e-05 6.4343524e-01
 3.6154732e-02 2.7377504e-01 2.0061668e-03 2.5426341e-02 8.7639811e-03
 1.3751505e-03], sum to 1.0000
[2019-04-09 15:12:07,965] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3997
[2019-04-09 15:12:08,039] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 79.0, 9.5, 0.0, 19.0, 17.95775131576949, -1.290622699463365, 0.0, 1.0, 45.0, 41.635025738653496], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 633600.0000, 
sim time next is 634800.0000, 
raw observation next is [-4.300000000000001, 76.33333333333334, 15.83333333333333, 0.0, 19.0, 18.08905151222913, -1.252142505041688, 0.0, 1.0, 35.0, 42.090522203147344], 
processed observation next is [0.0, 0.34782608695652173, 0.34349030470914127, 0.7633333333333334, 0.05277777777777777, 0.0, 0.08333333333333333, 0.007420959352427481, 0.08261916498610404, 0.0, 1.0, 0.4, 0.42090522203147346], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3304749], dtype=float32), -0.6609117]. 
=============================================
[2019-04-09 15:12:08,149] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.5113608e-06 5.9609563e-04 1.7891049e-03 1.9736378e-06 7.6164657e-01
 1.4741192e-02 2.1215877e-01 6.4161594e-04 7.1741454e-03 9.2193560e-04
 3.2496822e-04], sum to 1.0000
[2019-04-09 15:12:08,152] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6556
[2019-04-09 15:12:08,225] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.3, 88.0, 0.0, 0.0, 19.0, 21.14647826568918, -0.6613912471690444, 0.0, 1.0, 35.0, 23.33944699941437], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 525600.0000, 
sim time next is 526800.0000, 
raw observation next is [4.133333333333333, 87.33333333333334, 0.0, 0.0, 19.0, 21.04357990571938, -0.6862860825026407, 0.0, 1.0, 35.0, 21.19842418470886], 
processed observation next is [0.0, 0.08695652173913043, 0.577100646352724, 0.8733333333333334, 0.0, 0.0, 0.08333333333333333, 0.2536316588099483, 0.2712379724991198, 0.0, 1.0, 0.4, 0.21198424184708858], 
reward next is 0.7880, 
noisyNet noise sample is [array([1.6127808], dtype=float32), 1.977238]. 
=============================================
[2019-04-09 15:12:08,455] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2874604e-07 1.0741492e-05 9.3330156e-05 1.7118076e-08 9.1429907e-01
 9.6107731e-03 7.4933231e-02 1.4465324e-04 7.5385050e-04 1.2655153e-04
 2.7376258e-05], sum to 1.0000
[2019-04-09 15:12:08,455] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1658
[2019-04-09 15:12:08,512] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.633333333333333, 96.66666666666666, 0.0, 0.0, 19.0, 21.2496607518097, -0.6081694691768792, 0.0, 1.0, 35.0, 27.07752236345261], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 517200.0000, 
sim time next is 518400.0000, 
raw observation next is [3.8, 97.0, 0.0, 0.0, 19.0, 21.31732959185588, -0.6201189710420482, 0.0, 1.0, 35.0, 24.004223040026435], 
processed observation next is [0.0, 0.0, 0.5678670360110805, 0.97, 0.0, 0.0, 0.08333333333333333, 0.2764441326546567, 0.2932936763193173, 0.0, 1.0, 0.4, 0.24004223040026434], 
reward next is 0.7600, 
noisyNet noise sample is [array([-1.2864805], dtype=float32), -0.32321435]. 
=============================================
[2019-04-09 15:12:08,845] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0677304e-06 3.8680743e-04 2.2068080e-03 1.0423838e-06 4.8731175e-01
 1.5962241e-02 4.7827861e-01 1.0548991e-03 1.0583764e-02 3.5047166e-03
 7.0832163e-04], sum to 1.0000
[2019-04-09 15:12:08,847] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3094
[2019-04-09 15:12:08,885] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.2, 94.33333333333334, 0.0, 0.0, 19.0, 21.11486988841047, -0.6524408015801958, 0.0, 1.0, 35.0, 27.26556365868062], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 519600.0000, 
sim time next is 520800.0000, 
raw observation next is [4.6, 91.66666666666666, 0.0, 0.0, 19.0, 21.11823939534356, -0.6646901271701929, 0.0, 1.0, 35.0, 24.176308743945313], 
processed observation next is [0.0, 0.0, 0.590027700831025, 0.9166666666666665, 0.0, 0.0, 0.08333333333333333, 0.2598532829452968, 0.2784366242766024, 0.0, 1.0, 0.4, 0.24176308743945313], 
reward next is 0.7582, 
noisyNet noise sample is [array([-0.37132442], dtype=float32), 0.65634066]. 
=============================================
[2019-04-09 15:12:09,458] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.6094861e-05 1.6202595e-03 1.6849929e-03 9.8733826e-06 7.4451667e-01
 5.0948616e-02 1.8072197e-01 2.9354559e-03 5.7691834e-03 1.0547348e-02
 1.1895248e-03], sum to 1.0000
[2019-04-09 15:12:09,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9876
[2019-04-09 15:12:09,463] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0383962e-05 2.4213131e-04 3.0131100e-03 3.6693013e-06 8.4436530e-01
 3.8686775e-02 8.9977115e-02 1.1064271e-03 1.8224278e-02 2.9443137e-03
 1.4264625e-03], sum to 1.0000
[2019-04-09 15:12:09,463] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3662
[2019-04-09 15:12:09,485] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 84.33333333333333, 79.0, 140.0, 19.0, 20.35595124097228, -0.7733920320192529, 0.0, 1.0, 35.0, 24.663379130693876], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 556800.0000, 
sim time next is 558000.0000, 
raw observation next is [-0.6, 83.0, 83.0, 138.0, 19.0, 20.30559514771812, -0.790810320193671, 0.0, 1.0, 35.0, 22.326968962735513], 
processed observation next is [0.0, 0.4782608695652174, 0.44598337950138506, 0.83, 0.27666666666666667, 0.15248618784530388, 0.08333333333333333, 0.1921329289765099, 0.23639655993544298, 0.0, 1.0, 0.4, 0.22326968962735513], 
reward next is 0.7767, 
noisyNet noise sample is [array([-0.38344526], dtype=float32), 0.8040486]. 
=============================================
[2019-04-09 15:12:09,493] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[22.500422]
 [22.222843]
 [22.207426]
 [22.153458]
 [21.7875  ]], R is [[22.96764946]
 [23.49133873]
 [23.97894287]
 [24.37307167]
 [24.76768494]].
[2019-04-09 15:12:09,508] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [1.433333333333334, 86.0, 0.0, 0.0, 19.0, 19.8122486944702, -0.9508125873163519, 0.0, 1.0, 35.0, 23.92143054354017], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 537600.0000, 
sim time next is 538800.0000, 
raw observation next is [1.266666666666667, 87.0, 0.0, 0.0, 19.0, 19.70082581409264, -0.8934463604644943, 0.0, 1.0, 55.0, 53.882396268033176], 
processed observation next is [0.0, 0.21739130434782608, 0.4976915974145891, 0.87, 0.0, 0.0, 0.08333333333333333, 0.1417354845077199, 0.20218454651183523, 0.0, 1.0, 0.8, 0.5388239626803317], 
reward next is 0.4612, 
noisyNet noise sample is [array([-0.15347709], dtype=float32), 0.7261799]. 
=============================================
[2019-04-09 15:12:09,961] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0357481e-07 3.3930573e-05 5.0110376e-04 2.8065829e-08 6.7548805e-01
 1.6008087e-02 3.0747238e-01 2.2366912e-05 2.3062207e-04 1.9617060e-04
 4.7193011e-05], sum to 1.0000
[2019-04-09 15:12:09,961] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1064
[2019-04-09 15:12:10,074] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 22.5, 21.57848566222723, -0.5526202310160716, 1.0, 1.0, 45.0, 46.55897774786207], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 502800.0000, 
sim time next is 504000.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 22.5, 21.7780876342846, -0.5375054789003134, 1.0, 1.0, 35.0, 30.880740148669727], 
processed observation next is [1.0, 0.8695652173913043, 0.49307479224376743, 0.96, 0.0, 0.0, 0.375, 0.31484063619038327, 0.3208315070332289, 1.0, 1.0, 0.4, 0.3088074014866973], 
reward next is 0.7150, 
noisyNet noise sample is [array([0.10804658], dtype=float32), 2.6394694]. 
=============================================
[2019-04-09 15:12:10,079] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[32.746387]
 [32.343925]
 [31.978235]
 [32.279755]
 [31.9652  ]], R is [[32.91183472]
 [33.17082214]
 [33.60447311]
 [34.00474548]
 [34.35808945]].
[2019-04-09 15:12:10,653] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7479034e-06 2.5237532e-04 4.7976901e-03 1.3025009e-06 8.1934828e-01
 2.9382607e-02 1.3992137e-01 4.8006853e-04 3.5932849e-03 1.7053045e-03
 5.1289174e-04], sum to 1.0000
[2019-04-09 15:12:10,654] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4923
[2019-04-09 15:12:10,673] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 83.0, 70.5, 59.0, 19.0, 20.49623779911015, -0.7313695241902569, 0.0, 1.0, 35.0, 23.569336923568272], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 576000.0000, 
sim time next is 577200.0000, 
raw observation next is [-1.366666666666667, 84.33333333333334, 50.83333333333334, 49.66666666666666, 19.0, 20.44992205302264, -0.7560862000609577, 0.0, 1.0, 35.0, 21.546465501798295], 
processed observation next is [0.0, 0.6956521739130435, 0.42474607571560485, 0.8433333333333334, 0.16944444444444448, 0.05488029465930017, 0.08333333333333333, 0.20416017108521997, 0.24797126664634742, 0.0, 1.0, 0.4, 0.21546465501798295], 
reward next is 0.7845, 
noisyNet noise sample is [array([0.8339073], dtype=float32), -0.39103562]. 
=============================================
[2019-04-09 15:12:11,045] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.3854370e-06 1.4962479e-04 1.5249900e-03 2.0067082e-06 7.0201474e-01
 3.3387613e-02 2.5208303e-01 1.0617613e-03 7.8058229e-03 1.4393593e-03
 5.2678474e-04], sum to 1.0000
[2019-04-09 15:12:11,045] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5528
[2019-04-09 15:12:11,076] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.366666666666667, 84.33333333333334, 50.83333333333334, 49.66666666666666, 19.0, 20.01950610191908, -0.8278561147969566, 0.0, 1.0, 35.0, 28.000671744583435], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 577200.0000, 
sim time next is 578400.0000, 
raw observation next is [-1.533333333333333, 85.66666666666667, 34.16666666666667, 37.5, 19.0, 20.04072089957378, -0.8459768680644757, 0.0, 1.0, 35.0, 25.02014735161368], 
processed observation next is [0.0, 0.6956521739130435, 0.42012927054478305, 0.8566666666666667, 0.1138888888888889, 0.04143646408839779, 0.08333333333333333, 0.17006007496448175, 0.21800771064517477, 0.0, 1.0, 0.4, 0.2502014735161368], 
reward next is 0.7498, 
noisyNet noise sample is [array([-0.46041048], dtype=float32), 2.3856177]. 
=============================================
[2019-04-09 15:12:11,186] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.7201596e-07 2.5341851e-05 3.4253646e-04 7.8151835e-08 6.2738502e-01
 7.6248404e-03 3.6162284e-01 6.5736458e-05 1.6194697e-03 1.2247714e-03
 8.8503984e-05], sum to 1.0000
[2019-04-09 15:12:11,187] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0379
[2019-04-09 15:12:11,226] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.8, 97.0, 0.0, 0.0, 19.0, 20.82668836073151, -0.7272965608670444, 0.0, 1.0, 35.0, 21.71903777430545], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 518400.0000, 
sim time next is 519600.0000, 
raw observation next is [4.2, 94.33333333333334, 0.0, 0.0, 19.0, 20.75366099002426, -0.7395380280365068, 0.0, 1.0, 35.0, 21.83159841047143], 
processed observation next is [0.0, 0.0, 0.5789473684210527, 0.9433333333333335, 0.0, 0.0, 0.08333333333333333, 0.22947174916868848, 0.2534873239878311, 0.0, 1.0, 0.4, 0.21831598410471428], 
reward next is 0.7817, 
noisyNet noise sample is [array([-1.326117], dtype=float32), 0.52771246]. 
=============================================
[2019-04-09 15:12:11,629] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.2256196e-06 2.2598330e-04 1.2029929e-03 3.2664752e-06 6.9676203e-01
 2.2982614e-02 2.6556391e-01 8.0095034e-04 9.1067366e-03 2.6556344e-03
 6.9066393e-04], sum to 1.0000
[2019-04-09 15:12:11,630] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1268
[2019-04-09 15:12:11,647] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 19.0, 19.10397660795061, -1.045321805086549, 0.0, 1.0, 35.0, 29.247854750417673], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 596400.0000, 
sim time next is 597600.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 19.0, 19.1023643766448, -1.064564285443061, 0.0, 1.0, 35.0, 25.75859623108881], 
processed observation next is [0.0, 0.9565217391304348, 0.38504155124653744, 0.83, 0.0, 0.0, 0.08333333333333333, 0.09186369805373336, 0.1451452381856463, 0.0, 1.0, 0.4, 0.2575859623108881], 
reward next is 0.7424, 
noisyNet noise sample is [array([0.49424613], dtype=float32), 0.10547719]. 
=============================================
[2019-04-09 15:12:11,760] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.1453852e-05 5.0934067e-04 6.2661483e-03 1.9954390e-05 6.5095401e-01
 3.6595818e-02 2.6291102e-01 2.9528760e-03 2.3968564e-02 1.4730977e-02
 1.0499123e-03], sum to 1.0000
[2019-04-09 15:12:11,760] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7365
[2019-04-09 15:12:11,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0590589e-05 1.0107447e-03 2.4579654e-03 8.2078968e-06 6.2195075e-01
 1.7126683e-02 3.4325129e-01 9.7001606e-04 7.3227640e-03 5.1282095e-03
 7.4272917e-04], sum to 1.0000
[2019-04-09 15:12:11,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8806
[2019-04-09 15:12:11,820] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 19.9948094459303, -0.857692297151599, 0.0, 1.0, 35.0, 27.960274366801716], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 546000.0000, 
sim time next is 547200.0000, 
raw observation next is [0.5, 92.0, 17.5, 54.5, 19.0, 20.08502256701584, -0.8263028359160204, 0.0, 1.0, 45.0, 36.896243182385746], 
processed observation next is [0.0, 0.34782608695652173, 0.4764542936288089, 0.92, 0.058333333333333334, 0.06022099447513812, 0.08333333333333333, 0.1737518805846534, 0.22456572136132655, 0.0, 1.0, 0.6, 0.36896243182385746], 
reward next is 0.6310, 
noisyNet noise sample is [array([-0.30267808], dtype=float32), 1.2654872]. 
=============================================
[2019-04-09 15:12:11,864] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.933333333333333, 59.33333333333334, 170.3333333333333, 94.16666666666666, 19.0, 19.85544456234447, -0.9398474262204326, 0.0, 1.0, 35.0, 38.59403488342518], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 652800.0000, 
sim time next is 654000.0000, 
raw observation next is [-1.566666666666667, 59.66666666666667, 165.1666666666667, 86.83333333333333, 19.0, 20.11011210357631, -0.9040890493036625, 0.0, 1.0, 45.0, 45.170860420724296], 
processed observation next is [0.0, 0.5652173913043478, 0.4192059095106187, 0.5966666666666667, 0.5505555555555557, 0.09594843462246777, 0.08333333333333333, 0.17584267529802577, 0.19863698356544582, 0.0, 1.0, 0.6, 0.45170860420724296], 
reward next is 0.5483, 
noisyNet noise sample is [array([0.55087304], dtype=float32), -2.1950598]. 
=============================================
[2019-04-09 15:12:11,868] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[20.671087]
 [20.652355]
 [19.788895]
 [19.955935]
 [19.693846]], R is [[21.45042801]
 [21.84998322]
 [21.97638893]
 [22.46053123]
 [22.96278763]].
[2019-04-09 15:12:12,063] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9730325e-05 9.4415492e-04 2.0570806e-03 1.9916271e-05 6.4253682e-01
 3.3703480e-02 3.1090376e-01 1.6707375e-03 3.7665090e-03 2.8087797e-03
 1.5689881e-03], sum to 1.0000
[2019-04-09 15:12:12,063] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4216
[2019-04-09 15:12:12,115] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.899999999999999, 78.66666666666667, 0.0, 0.0, 19.0, 19.09537701046147, -1.06927508806713, 0.0, 1.0, 45.0, 38.40699293613144], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 614400.0000, 
sim time next is 615600.0000, 
raw observation next is [-3.9, 75.0, 0.0, 0.0, 19.0, 19.10823291033278, -1.074212661711488, 0.0, 1.0, 35.0, 29.488560563137014], 
processed observation next is [0.0, 0.13043478260869565, 0.3545706371191136, 0.75, 0.0, 0.0, 0.08333333333333333, 0.09235274252773173, 0.14192911276283735, 0.0, 1.0, 0.4, 0.2948856056313701], 
reward next is 0.7051, 
noisyNet noise sample is [array([-0.6194649], dtype=float32), 0.53171253]. 
=============================================
[2019-04-09 15:12:12,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.6570863e-06 2.7753998e-04 2.1343229e-03 1.3059147e-06 8.1368303e-01
 1.1686836e-02 1.6686511e-01 2.0533818e-04 3.4388183e-03 1.2596555e-03
 4.4341051e-04], sum to 1.0000
[2019-04-09 15:12:12,247] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2445
[2019-04-09 15:12:12,283] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 19.0, 20.37288954216534, -0.8593845050196434, 0.0, 1.0, 40.0, 29.96034109364355], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 686400.0000, 
sim time next is 687600.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 19.0, 20.26781340530435, -0.8854341150795456, 0.0, 1.0, 35.0, 23.02603338490164], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.08333333333333333, 0.18898445044202905, 0.20485529497348479, 0.0, 1.0, 0.4, 0.23026033384901642], 
reward next is 0.7697, 
noisyNet noise sample is [array([-1.2111031], dtype=float32), -1.398187]. 
=============================================
[2019-04-09 15:12:13,662] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.0684719e-06 3.6644604e-04 1.3258480e-03 7.9883603e-06 8.9033061e-01
 1.8070774e-02 8.0910295e-02 5.4027210e-04 4.7775479e-03 3.1071692e-03
 5.5597973e-04], sum to 1.0000
[2019-04-09 15:12:13,664] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7251
[2019-04-09 15:12:13,730] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 19.0, 19.48604082066151, -0.9755139275427748, 0.0, 1.0, 35.0, 23.777941965368036], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 590400.0000, 
sim time next is 591600.0000, 
raw observation next is [-2.8, 85.66666666666667, 0.0, 0.0, 19.0, 19.35330880096978, -1.002340351473577, 0.0, 1.0, 35.0, 24.098433499947205], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.11277573341414844, 0.16588654950880768, 0.0, 1.0, 0.4, 0.24098433499947206], 
reward next is 0.7590, 
noisyNet noise sample is [array([-0.49051222], dtype=float32), 0.8001378]. 
=============================================
[2019-04-09 15:12:14,152] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.8589532e-06 3.3557587e-04 3.7372469e-03 2.5142317e-06 4.5390454e-01
 3.3401001e-02 4.9189648e-01 1.2541116e-03 1.0854205e-02 4.1004508e-03
 5.0496258e-04], sum to 1.0000
[2019-04-09 15:12:14,152] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8767
[2019-04-09 15:12:14,203] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 87.0, 0.0, 0.0, 19.0, 19.50814465420495, -0.9702027801483268, 0.0, 1.0, 35.0, 29.15450591728292], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 604800.0000, 
sim time next is 606000.0000, 
raw observation next is [-3.566666666666667, 86.66666666666667, 0.0, 0.0, 19.0, 19.48587688467164, -0.970142566302712, 0.0, 1.0, 45.0, 37.48102599478656], 
processed observation next is [0.0, 0.0, 0.3638042474607572, 0.8666666666666667, 0.0, 0.0, 0.08333333333333333, 0.12382307372263668, 0.17661914456576266, 0.0, 1.0, 0.6, 0.37481025994786565], 
reward next is 0.6252, 
noisyNet noise sample is [array([0.8377348], dtype=float32), -0.63771313]. 
=============================================
[2019-04-09 15:12:14,207] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[22.819283]
 [23.39823 ]
 [23.069403]
 [23.264395]
 [23.290815]], R is [[22.65094185]
 [23.13288689]
 [23.52421188]
 [24.00088692]
 [24.37342644]].
[2019-04-09 15:12:14,673] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1819604e-05 7.2126143e-04 2.1886732e-03 4.0597811e-06 7.6898611e-01
 4.5158543e-02 1.6282742e-01 1.5029884e-03 1.4566928e-02 2.8973250e-03
 1.1248895e-03], sum to 1.0000
[2019-04-09 15:12:14,673] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9895
[2019-04-09 15:12:14,818] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-3.166666666666667, 63.66666666666667, 90.83333333333334, 31.66666666666667, 19.0, 19.57588444802743, -0.9457876460177316, 0.0, 1.0, 55.0, 68.89781935652302], 
current ob forecast is [], 
actual action is [1, 55.0], 
sim time this is 645600.0000, 
sim time next is 646800.0000, 
raw observation next is [-2.933333333333334, 62.33333333333333, 92.83333333333334, 48.33333333333333, 19.0, 20.20395986812516, -0.7990407732850046, 0.0, 1.0, 55.0, 73.79821519492316], 
processed observation next is [0.0, 0.4782608695652174, 0.38134810710988, 0.6233333333333333, 0.30944444444444447, 0.05340699815837937, 0.08333333333333333, 0.18366332234376337, 0.23365307557166515, 0.0, 1.0, 0.8, 0.7379821519492316], 
reward next is 0.2620, 
noisyNet noise sample is [array([1.1782093], dtype=float32), -0.40597388]. 
=============================================
[2019-04-09 15:12:15,276] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.0927104e-05 1.1708962e-03 6.3943584e-03 1.9218221e-05 7.2982043e-01
 2.9727980e-02 2.0871110e-01 3.5159080e-03 1.0239572e-02 8.9945756e-03
 1.3351004e-03], sum to 1.0000
[2019-04-09 15:12:15,276] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7909
[2019-04-09 15:12:15,306] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 72.66666666666667, 0.0, 0.0, 19.0, 18.62544575601694, -1.197861654847218, 0.0, 1.0, 35.0, 27.544875376366363], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 620400.0000, 
sim time next is 621600.0000, 
raw observation next is [-4.5, 70.33333333333333, 0.0, 0.0, 19.0, 18.53352188094741, -1.193903009916679, 0.0, 1.0, 45.0, 40.45704647319031], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.7033333333333333, 0.0, 0.0, 0.08333333333333333, 0.04446015674561762, 0.10203233002777368, 0.0, 1.0, 0.6, 0.4045704647319031], 
reward next is 0.1025, 
noisyNet noise sample is [array([0.2818495], dtype=float32), -0.29899335]. 
=============================================
[2019-04-09 15:12:16,283] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9459318e-05 3.7434397e-04 3.3769996e-03 7.2009707e-06 6.9804102e-01
 4.0006202e-02 2.4272749e-01 6.5919990e-04 1.0599077e-02 3.2337536e-03
 9.5524010e-04], sum to 1.0000
[2019-04-09 15:12:16,283] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8857
[2019-04-09 15:12:16,424] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 69.0, 115.6666666666667, 42.5, 19.0, 19.13389499643081, -1.109413995066004, 0.0, 1.0, 35.0, 29.465423075757116], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 638400.0000, 
sim time next is 639600.0000, 
raw observation next is [-3.899999999999999, 67.0, 129.1666666666667, 42.5, 19.0, 19.13019353323464, -1.063333612638015, 0.0, 1.0, 45.0, 50.84804429226818], 
processed observation next is [0.0, 0.391304347826087, 0.35457063711911363, 0.67, 0.4305555555555557, 0.04696132596685083, 0.08333333333333333, 0.09418279443621991, 0.14555546245399498, 0.0, 1.0, 0.6, 0.5084804429226818], 
reward next is 0.4915, 
noisyNet noise sample is [array([1.292607], dtype=float32), 0.46670222]. 
=============================================
[2019-04-09 15:12:16,763] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.8761820e-06 1.2723892e-04 1.2352117e-03 2.1468243e-06 6.3930476e-01
 1.2450858e-02 3.3668756e-01 7.1161869e-04 6.7509292e-03 2.2565611e-03
 4.6317570e-04], sum to 1.0000
[2019-04-09 15:12:16,764] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7221
[2019-04-09 15:12:16,885] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8, 55.00000000000001, 36.33333333333333, 18.83333333333333, 19.0, 20.79098743435354, -0.7711250490672169, 0.0, 1.0, 35.0, 27.53739178023815], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 663600.0000, 
sim time next is 664800.0000, 
raw observation next is [-1.0, 56.0, 22.5, 12.83333333333333, 19.0, 20.72477771979788, -0.7211737060083049, 0.0, 1.0, 45.0, 56.05319499548867], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.56, 0.075, 0.014180478821362795, 0.08333333333333333, 0.22706480998315678, 0.2596087646638984, 0.0, 1.0, 0.6, 0.5605319499548866], 
reward next is 0.4395, 
noisyNet noise sample is [array([-0.26976568], dtype=float32), 0.2450662]. 
=============================================
[2019-04-09 15:12:17,672] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.26686705e-08 2.36834308e-06 1.15307426e-04 1.83014315e-08
 5.65215886e-01 6.45294599e-03 4.27041799e-01 1.93380129e-05
 1.03381218e-03 9.12450050e-05 2.72409980e-05], sum to 1.0000
[2019-04-09 15:12:17,672] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3992
[2019-04-09 15:12:17,820] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 60.0, 91.83333333333333, 724.0, 22.5, 22.578412115317, -0.3836759971011088, 1.0, 1.0, 45.0, 47.29677122313979], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 733200.0000, 
sim time next is 734400.0000, 
raw observation next is [-0.6, 57.0, 107.5, 614.0, 22.5, 22.94659535168734, -0.3381621151983813, 1.0, 1.0, 35.0, 28.82670236461125], 
processed observation next is [1.0, 0.5217391304347826, 0.44598337950138506, 0.57, 0.35833333333333334, 0.6784530386740332, 0.375, 0.4122162793072783, 0.38727929493387286, 1.0, 1.0, 0.4, 0.2882670236461125], 
reward next is 0.7117, 
noisyNet noise sample is [array([-0.5710016], dtype=float32), 0.50516826]. 
=============================================
[2019-04-09 15:12:18,258] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8730798e-06 7.2012644e-04 1.1441618e-03 1.2611744e-06 7.1383148e-01
 4.8359204e-02 2.1810710e-01 6.2239193e-04 1.3420320e-02 2.7805811e-03
 1.0073873e-03], sum to 1.0000
[2019-04-09 15:12:18,259] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3310
[2019-04-09 15:12:18,303] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 19.0, 19.95977949280525, -0.9725491006282233, 0.0, 1.0, 35.0, 20.851007019076874], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 700800.0000, 
sim time next is 702000.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 19.0, 19.79007020967614, -0.9996593679076197, 0.0, 1.0, 35.0, 25.38080324954357], 
processed observation next is [1.0, 0.13043478260869565, 0.368421052631579, 0.75, 0.0, 0.0, 0.08333333333333333, 0.14917251747301177, 0.1667802106974601, 0.0, 1.0, 0.4, 0.2538080324954357], 
reward next is 0.7462, 
noisyNet noise sample is [array([1.4445677], dtype=float32), 1.7417638]. 
=============================================
[2019-04-09 15:12:18,313] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[24.577211]
 [24.837847]
 [24.92151 ]
 [24.696444]
 [25.269756]], R is [[25.68164062]
 [26.21631432]
 [26.7263813 ]
 [27.20903969]
 [27.65683746]].
[2019-04-09 15:12:21,723] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.6253987e-06 1.9038796e-04 2.6261252e-03 4.9512932e-06 6.5255785e-01
 2.4353942e-02 3.0785811e-01 1.1918782e-03 6.1112656e-03 4.0328568e-03
 1.0689064e-03], sum to 1.0000
[2019-04-09 15:12:21,723] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9596
[2019-04-09 15:12:21,756] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 75.0, 0.0, 0.0, 19.0, 19.90864552182975, -0.9594595300207382, 0.0, 1.0, 35.0, 27.8755521882552], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 705600.0000, 
sim time next is 706800.0000, 
raw observation next is [-2.633333333333333, 75.33333333333334, 0.0, 0.0, 19.0, 19.9746743872988, -0.9277688448636474, 0.0, 1.0, 45.0, 37.96433802101945], 
processed observation next is [1.0, 0.17391304347826086, 0.38965835641735924, 0.7533333333333334, 0.0, 0.0, 0.08333333333333333, 0.16455619894156678, 0.1907437183787842, 0.0, 1.0, 0.6, 0.3796433802101945], 
reward next is 0.6204, 
noisyNet noise sample is [array([0.47336823], dtype=float32), 0.64086735]. 
=============================================
[2019-04-09 15:12:22,405] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4953281e-08 9.4975230e-06 1.9336559e-04 3.2638283e-09 4.3736404e-01
 9.6114893e-03 5.5254656e-01 2.5378322e-05 8.3148057e-05 1.4964925e-04
 1.6907672e-05], sum to 1.0000
[2019-04-09 15:12:22,406] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0773
[2019-04-09 15:12:22,506] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 46.66666666666667, 87.5, 763.1666666666667, 22.5, 23.78321042423815, -0.09297211042470811, 1.0, 1.0, 45.0, 38.659096080082094], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 740400.0000, 
sim time next is 741600.0000, 
raw observation next is [0.5, 45.0, 84.5, 743.5, 22.5, 24.05028835700849, -0.05653534100486891, 1.0, 1.0, 35.0, 28.054450223290267], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.45, 0.2816666666666667, 0.8215469613259668, 0.375, 0.5041906964173742, 0.48115488633171033, 1.0, 1.0, 0.4, 0.28054450223290267], 
reward next is 0.7195, 
noisyNet noise sample is [array([-0.01139785], dtype=float32), -1.4816649]. 
=============================================
[2019-04-09 15:12:23,202] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.3490939e-09 8.9636560e-06 3.6701218e-05 7.5508790e-09 7.9950285e-01
 1.9736479e-03 1.9718401e-01 2.4948460e-05 1.1196480e-03 1.0465889e-04
 4.4558270e-05], sum to 1.0000
[2019-04-09 15:12:23,202] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5290
[2019-04-09 15:12:23,295] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 81.33333333333334, 64.33333333333333, 0.0, 22.5, 23.1341063012147, -0.3292492413875316, 1.0, 1.0, 35.0, 22.68735854682101], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 829200.0000, 
sim time next is 830400.0000, 
raw observation next is [-3.899999999999999, 83.66666666666666, 57.33333333333334, 0.0, 22.5, 22.79493156908845, -0.3680124448152615, 1.0, 1.0, 35.0, 23.001198098045524], 
processed observation next is [1.0, 0.6086956521739131, 0.35457063711911363, 0.8366666666666666, 0.19111111111111115, 0.0, 0.375, 0.3995776307573709, 0.3773291850615795, 1.0, 1.0, 0.4, 0.23001198098045525], 
reward next is 0.7700, 
noisyNet noise sample is [array([1.7524729], dtype=float32), -1.1502639]. 
=============================================
[2019-04-09 15:12:23,430] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.4507922e-06 2.6889160e-04 3.4678658e-03 1.5568635e-06 6.8954355e-01
 4.5690782e-02 2.4436760e-01 1.0846670e-03 1.0661948e-02 3.7022929e-03
 1.2045532e-03], sum to 1.0000
[2019-04-09 15:12:23,430] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9029
[2019-04-09 15:12:23,477] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.100000000000001, 69.66666666666667, 0.0, 0.0, 19.0, 19.46705792113622, -1.001136134428539, 0.0, 1.0, 35.0, 25.346149928698285], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 800400.0000, 
sim time next is 801600.0000, 
raw observation next is [-6.9, 68.33333333333333, 0.0, 0.0, 19.0, 19.48149465172596, -0.9806339400189019, 0.0, 1.0, 45.0, 38.457456047437205], 
processed observation next is [1.0, 0.2608695652173913, 0.27146814404432135, 0.6833333333333332, 0.0, 0.0, 0.08333333333333333, 0.12345788764382999, 0.17312201999369936, 0.0, 1.0, 0.6, 0.38457456047437205], 
reward next is 0.6154, 
noisyNet noise sample is [array([1.0657934], dtype=float32), -0.64958185]. 
=============================================
[2019-04-09 15:12:24,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3962958e-08 1.2394156e-05 5.4699584e-04 1.0554842e-08 7.1219051e-01
 4.8281080e-03 2.8050324e-01 3.1619689e-05 1.5912364e-03 2.2049790e-04
 7.5365555e-05], sum to 1.0000
[2019-04-09 15:12:24,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2045
[2019-04-09 15:12:24,239] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.166666666666667, 54.66666666666667, 0.0, 0.0, 22.5, 24.40049468604582, -0.002853756945396237, 1.0, 1.0, 35.0, 29.388072350789237], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 753600.0000, 
sim time next is 754800.0000, 
raw observation next is [-3.533333333333333, 55.33333333333333, 0.0, 0.0, 22.5, 24.09632376484417, -0.1956354790813693, 1.0, 1.0, 35.0, 25.547149328192845], 
processed observation next is [1.0, 0.7391304347826086, 0.36472760849492153, 0.5533333333333332, 0.0, 0.0, 0.375, 0.5080269804036807, 0.4347881736395436, 1.0, 1.0, 0.4, 0.2554714932819284], 
reward next is 0.7445, 
noisyNet noise sample is [array([1.1339554], dtype=float32), -1.3708131]. 
=============================================
[2019-04-09 15:12:24,382] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.5952943e-09 2.2101790e-06 1.8146816e-04 6.2474097e-09 6.5898901e-01
 2.6634324e-03 3.3725405e-01 7.7545970e-05 4.4922795e-04 3.7039875e-04
 1.2621951e-05], sum to 1.0000
[2019-04-09 15:12:24,382] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8991
[2019-04-09 15:12:24,518] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2333333333333334, 54.66666666666667, 123.1666666666667, 504.0, 22.5, 22.96617214744048, -0.3297316223489068, 1.0, 1.0, 35.0, 23.89114610663426], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 735600.0000, 
sim time next is 736800.0000, 
raw observation next is [0.1333333333333333, 52.33333333333333, 124.0, 503.0, 22.5, 23.00160135310767, -0.2524896891042098, 1.0, 1.0, 45.0, 48.9665586812726], 
processed observation next is [1.0, 0.5217391304347826, 0.46629732225300097, 0.5233333333333333, 0.41333333333333333, 0.5558011049723757, 0.375, 0.4168001127589725, 0.4158367702985967, 1.0, 1.0, 0.6, 0.48966558681272604], 
reward next is 0.5103, 
noisyNet noise sample is [array([-0.8104629], dtype=float32), 0.00059668766]. 
=============================================
[2019-04-09 15:12:25,673] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.2886917e-07 1.2610640e-04 5.7530019e-04 2.3339865e-07 9.0415341e-01
 1.6220961e-02 7.5314127e-02 2.6224944e-04 2.5544099e-03 7.0462050e-04
 8.7856220e-05], sum to 1.0000
[2019-04-09 15:12:25,673] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9227
[2019-04-09 15:12:25,714] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.466666666666667, 71.0, 0.0, 0.0, 19.0, 20.5518585927255, -0.7465017640192316, 0.0, 1.0, 35.0, 23.399882734651577], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 782400.0000, 
sim time next is 783600.0000, 
raw observation next is [-7.633333333333333, 71.0, 0.0, 0.0, 19.0, 20.47326274594551, -0.7309657775519942, 0.0, 1.0, 45.0, 42.63826322871506], 
processed observation next is [1.0, 0.043478260869565216, 0.2511542012927055, 0.71, 0.0, 0.0, 0.08333333333333333, 0.20610522882879265, 0.25634474081600195, 0.0, 1.0, 0.6, 0.4263826322871506], 
reward next is 0.5736, 
noisyNet noise sample is [array([-0.36675844], dtype=float32), -0.49289525]. 
=============================================
[2019-04-09 15:12:26,404] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1150338e-06 1.9301481e-04 3.5830413e-03 1.4588619e-06 4.3748236e-01
 3.6586769e-02 5.1455802e-01 6.3805486e-04 4.3075215e-03 2.1119048e-03
 5.3565908e-04], sum to 1.0000
[2019-04-09 15:12:26,404] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9772
[2019-04-09 15:12:26,433] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 19.0, 20.34806324024643, -0.8093764982988209, 0.0, 1.0, 35.0, 21.20016378677991], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 781200.0000, 
sim time next is 782400.0000, 
raw observation next is [-7.466666666666667, 71.0, 0.0, 0.0, 19.0, 20.21299047312059, -0.803747866492951, 0.0, 1.0, 45.0, 38.09475319687023], 
processed observation next is [1.0, 0.043478260869565216, 0.25577100646352724, 0.71, 0.0, 0.0, 0.08333333333333333, 0.18441587276004925, 0.23208404450234965, 0.0, 1.0, 0.6, 0.3809475319687023], 
reward next is 0.6191, 
noisyNet noise sample is [array([1.083314], dtype=float32), 0.5187128]. 
=============================================
[2019-04-09 15:12:26,524] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6796567e-06 2.9163173e-04 8.8421470e-03 8.5710872e-07 7.8667808e-01
 1.6380595e-02 1.7340977e-01 7.9046714e-04 6.6202679e-03 6.0321875e-03
 9.5131370e-04], sum to 1.0000
[2019-04-09 15:12:26,524] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4513
[2019-04-09 15:12:26,586] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.9, 68.33333333333333, 0.0, 0.0, 19.0, 19.37817707408252, -0.9856505226531908, 0.0, 1.0, 45.0, 43.365745728285475], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 801600.0000, 
sim time next is 802800.0000, 
raw observation next is [-6.7, 67.0, 0.0, 0.0, 19.0, 19.41920457153383, -0.990114666833079, 0.0, 1.0, 35.0, 28.73233141522189], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.67, 0.0, 0.0, 0.08333333333333333, 0.11826704762781907, 0.169961777722307, 0.0, 1.0, 0.4, 0.2873233141522189], 
reward next is 0.7127, 
noisyNet noise sample is [array([1.9783627], dtype=float32), 0.26221818]. 
=============================================
[2019-04-09 15:12:26,927] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6536690e-06 1.8013446e-04 2.5977786e-03 1.3010545e-06 4.9842733e-01
 2.1492152e-02 4.6940535e-01 8.1718800e-04 4.7272556e-03 1.9218760e-03
 4.2803990e-04], sum to 1.0000
[2019-04-09 15:12:26,927] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0169
[2019-04-09 15:12:26,964] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.9, 68.33333333333333, 0.0, 0.0, 19.0, 19.65708991811654, -0.9586384406187248, 0.0, 1.0, 45.0, 37.53451886825523], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 801600.0000, 
sim time next is 802800.0000, 
raw observation next is [-6.7, 67.0, 0.0, 0.0, 19.0, 19.6262556913246, -0.9763927761141297, 0.0, 1.0, 35.0, 28.512620741675242], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.67, 0.0, 0.0, 0.08333333333333333, 0.13552130761038347, 0.17453574129529012, 0.0, 1.0, 0.4, 0.28512620741675243], 
reward next is 0.7149, 
noisyNet noise sample is [array([-1.2152156], dtype=float32), 0.23792405]. 
=============================================
[2019-04-09 15:12:28,154] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.0381336e-06 2.7743331e-04 2.5172769e-03 1.0154793e-06 7.6400697e-01
 4.7924101e-02 1.7576686e-01 6.6422968e-04 4.4559198e-03 3.2130098e-03
 1.1681560e-03], sum to 1.0000
[2019-04-09 15:12:28,154] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2137
[2019-04-09 15:12:28,215] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 19.0, 19.78655299719647, -0.9335455379418797, 0.0, 1.0, 45.0, 37.383022079865604], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 795600.0000, 
sim time next is 796800.0000, 
raw observation next is [-7.300000000000001, 71.0, 0.0, 0.0, 19.0, 19.70230959228721, -0.956852904605192, 0.0, 1.0, 35.0, 28.581487713026682], 
processed observation next is [1.0, 0.21739130434782608, 0.26038781163434904, 0.71, 0.0, 0.0, 0.08333333333333333, 0.1418591326906009, 0.18104903179826934, 0.0, 1.0, 0.4, 0.2858148771302668], 
reward next is 0.7142, 
noisyNet noise sample is [array([-1.1058488], dtype=float32), 0.7582901]. 
=============================================
[2019-04-09 15:12:28,394] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1173220e-07 1.8041996e-05 5.7277089e-04 9.7409576e-08 9.4512755e-01
 1.4284465e-03 5.1758200e-02 2.3060894e-04 6.1550195e-04 1.9002709e-04
 5.8572918e-05], sum to 1.0000
[2019-04-09 15:12:28,394] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3094
[2019-04-09 15:12:28,419] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.1, 79.66666666666667, 0.0, 0.0, 19.0, 20.32999160414587, -0.8249469570942599, 0.0, 1.0, 35.0, 22.638467067958594], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 868800.0000, 
sim time next is 870000.0000, 
raw observation next is [-1.9, 79.33333333333334, 0.0, 0.0, 19.0, 20.30675115578555, -0.834540274287099, 0.0, 1.0, 35.0, 22.70718676826267], 
processed observation next is [1.0, 0.043478260869565216, 0.4099722991689751, 0.7933333333333334, 0.0, 0.0, 0.08333333333333333, 0.19222926298212903, 0.221819908570967, 0.0, 1.0, 0.4, 0.2270718676826267], 
reward next is 0.7729, 
noisyNet noise sample is [array([-1.2005374], dtype=float32), -3.0922205]. 
=============================================
[2019-04-09 15:12:28,429] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[28.442162]
 [28.652191]
 [29.03819 ]
 [29.828766]
 [30.696398]], R is [[28.43379402]
 [28.92307091]
 [29.40815353]
 [29.88863754]
 [30.36378098]].
[2019-04-09 15:12:29,110] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.8511333e-08 5.5859282e-06 1.8552945e-04 2.3042942e-08 8.6726260e-01
 2.0088919e-02 1.1013584e-01 4.1201194e-05 1.9916575e-03 2.3790510e-04
 5.0636449e-05], sum to 1.0000
[2019-04-09 15:12:29,110] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8218
[2019-04-09 15:12:29,219] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 71.0, 108.1666666666667, 0.0, 22.5, 22.3579687626724, -0.498215037284284, 1.0, 1.0, 35.0, 25.741427436882923], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 819600.0000, 
sim time next is 820800.0000, 
raw observation next is [-4.5, 71.0, 104.5, 0.0, 22.5, 22.11146342471948, -0.5244676860709817, 1.0, 1.0, 35.0, 26.37868176495645], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.71, 0.34833333333333333, 0.0, 0.375, 0.34262195205995677, 0.3251774379763394, 1.0, 1.0, 0.4, 0.2637868176495645], 
reward next is 0.7525, 
noisyNet noise sample is [array([-0.14596263], dtype=float32), -0.6273385]. 
=============================================
[2019-04-09 15:12:29,545] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2151543e-09 1.9896504e-06 6.7008470e-05 1.2866660e-08 9.5901853e-01
 1.1159434e-02 2.8551390e-02 8.0737038e-05 1.0108646e-03 9.5169053e-05
 1.4956217e-05], sum to 1.0000
[2019-04-09 15:12:29,545] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5859
[2019-04-09 15:12:29,707] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.899999999999999, 84.66666666666666, 24.16666666666667, 0.0, 22.5, 23.22302754264945, -0.2721062766835414, 1.0, 1.0, 35.0, 23.227682268254796], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 837600.0000, 
sim time next is 838800.0000, 
raw observation next is [-3.9, 86.0, 14.5, 0.0, 22.5, 23.05024932797237, -0.2932305821840185, 1.0, 1.0, 35.0, 24.161668538549677], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.86, 0.04833333333333333, 0.0, 0.375, 0.4208541106643642, 0.4022564726053272, 1.0, 1.0, 0.4, 0.24161668538549677], 
reward next is 0.7584, 
noisyNet noise sample is [array([-2.0682242], dtype=float32), -0.8365999]. 
=============================================
[2019-04-09 15:12:30,714] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9737539e-08 3.7285170e-06 2.0284235e-04 2.6866951e-08 5.4090524e-01
 9.6671525e-03 4.4856608e-01 1.7446002e-04 2.4889197e-04 2.0387750e-04
 2.7704671e-05], sum to 1.0000
[2019-04-09 15:12:30,714] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4550
[2019-04-09 15:12:30,814] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.633333333333334, 73.66666666666667, 82.66666666666666, 0.0, 22.5, 22.60982372749293, -0.4308813742423101, 1.0, 1.0, 45.0, 43.18597114373786], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 814800.0000, 
sim time next is 816000.0000, 
raw observation next is [-5.066666666666666, 72.33333333333333, 90.83333333333333, 0.0, 22.5, 22.71028570843688, -0.4164493653033094, 1.0, 1.0, 35.0, 31.375461305743357], 
processed observation next is [1.0, 0.43478260869565216, 0.32225300092336107, 0.7233333333333333, 0.30277777777777776, 0.0, 0.375, 0.39252380903640677, 0.36118354489889687, 1.0, 1.0, 0.4, 0.3137546130574336], 
reward next is 0.6862, 
noisyNet noise sample is [array([0.3812011], dtype=float32), 1.0179335]. 
=============================================
[2019-04-09 15:12:30,817] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[33.193275]
 [32.38171 ]
 [31.796875]
 [30.86731 ]
 [29.769867]], R is [[34.12932587]
 [34.35617447]
 [34.56787109]
 [34.81275177]
 [35.12179947]].
[2019-04-09 15:12:31,382] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.03001035e-07 1.06358175e-05 3.05092341e-04 1.12647946e-07
 5.40969670e-01 7.49056228e-03 4.49675351e-01 1.65655918e-04
 5.41486021e-04 7.93450105e-04 4.73261643e-05], sum to 1.0000
[2019-04-09 15:12:31,382] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7015
[2019-04-09 15:12:31,410] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.1, 79.66666666666667, 0.0, 0.0, 19.0, 21.04451722912135, -0.6409897616987666, 0.0, 1.0, 45.0, 35.453956277068485], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 868800.0000, 
sim time next is 870000.0000, 
raw observation next is [-1.9, 79.33333333333334, 0.0, 0.0, 19.0, 21.14515792332867, -0.618852911670894, 0.0, 1.0, 45.0, 36.11685379300802], 
processed observation next is [1.0, 0.043478260869565216, 0.4099722991689751, 0.7933333333333334, 0.0, 0.0, 0.08333333333333333, 0.2620964936107226, 0.293715696109702, 0.0, 1.0, 0.6, 0.3611685379300802], 
reward next is 0.6388, 
noisyNet noise sample is [array([1.5191004], dtype=float32), -0.14881906]. 
=============================================
[2019-04-09 15:12:31,417] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[30.870571]
 [31.170813]
 [31.546503]
 [32.328   ]
 [33.210316]], R is [[30.74616432]
 [31.08416367]
 [31.56406593]
 [32.01702881]
 [32.43197632]].
[2019-04-09 15:12:31,910] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5582042e-06 1.3281258e-04 9.3255210e-04 5.0245774e-07 6.8083429e-01
 3.8363863e-02 2.7347237e-01 1.2508666e-03 3.3666748e-03 1.5098464e-03
 1.3476610e-04], sum to 1.0000
[2019-04-09 15:12:31,914] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2113
[2019-04-09 15:12:31,946] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 19.0, 20.50276536978524, -0.7676129083730917, 0.0, 1.0, 45.0, 36.676232426924486], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 872400.0000, 
sim time next is 873600.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 19.0, 20.48050425015391, -0.784616971256708, 0.0, 1.0, 35.0, 27.564256686734133], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.79, 0.0, 0.0, 0.08333333333333333, 0.2067086875128258, 0.2384610095810973, 0.0, 1.0, 0.4, 0.27564256686734134], 
reward next is 0.7244, 
noisyNet noise sample is [array([-1.8488222], dtype=float32), 0.0012207909]. 
=============================================
[2019-04-09 15:12:32,435] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.5050721e-09 2.7917802e-06 2.7341468e-03 7.3401445e-09 2.8249171e-01
 2.2296283e-02 6.8983233e-01 2.0386853e-05 2.0116372e-03 5.8276055e-04
 2.8000155e-05], sum to 1.0000
[2019-04-09 15:12:32,435] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6334
[2019-04-09 15:12:32,677] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 92.66666666666666, 98.16666666666667, 0.0, 22.5, 23.67165696349779, -0.1832400408446179, 1.0, 1.0, 35.0, 23.324330084051226], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 906000.0000, 
sim time next is 907200.0000, 
raw observation next is [2.7, 97.0, 100.5, 0.0, 22.5, 23.46514975392261, -0.1312265106494508, 1.0, 1.0, 45.0, 45.44423676448248], 
processed observation next is [1.0, 0.5217391304347826, 0.5373961218836566, 0.97, 0.335, 0.0, 0.375, 0.4554291461602175, 0.4562578297835164, 1.0, 1.0, 0.6, 0.4544423676448248], 
reward next is 0.5456, 
noisyNet noise sample is [array([1.2920728], dtype=float32), -1.7300917]. 
=============================================
[2019-04-09 15:12:32,848] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8855486e-07 5.7529028e-06 2.2415302e-04 4.8712629e-08 6.0872597e-01
 2.1446442e-02 3.6802909e-01 4.9151258e-05 1.1404086e-03 3.2745249e-04
 5.1309064e-05], sum to 1.0000
[2019-04-09 15:12:32,850] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6975
[2019-04-09 15:12:32,907] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.633333333333333, 79.33333333333334, 0.0, 0.0, 19.0, 20.83839875883177, -0.6835405399483552, 0.0, 1.0, 35.0, 22.33474007441371], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 861600.0000, 
sim time next is 862800.0000, 
raw observation next is [-2.466666666666667, 79.66666666666666, 0.0, 0.0, 19.0, 20.7372798995955, -0.66254578174372, 0.0, 1.0, 45.0, 41.406492408162634], 
processed observation next is [1.0, 1.0, 0.39427516158818104, 0.7966666666666665, 0.0, 0.0, 0.08333333333333333, 0.22810665829962495, 0.27915140608542666, 0.0, 1.0, 0.6, 0.4140649240816263], 
reward next is 0.5859, 
noisyNet noise sample is [array([1.5160978], dtype=float32), -0.3637599]. 
=============================================
[2019-04-09 15:12:33,102] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5439214e-07 2.9152183e-05 1.2271223e-04 3.8621675e-08 7.5498796e-01
 3.1143297e-02 2.0959167e-01 1.4908370e-04 3.2217640e-03 7.1668834e-04
 3.7435751e-05], sum to 1.0000
[2019-04-09 15:12:33,102] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4046
[2019-04-09 15:12:33,138] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.466666666666667, 79.66666666666666, 0.0, 0.0, 19.0, 21.0243614933507, -0.6455031009494059, 0.0, 1.0, 35.0, 27.461959748093005], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 862800.0000, 
sim time next is 864000.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 19.0, 20.99397301133597, -0.6629246516591927, 0.0, 1.0, 35.0, 24.24358423017572], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.08333333333333333, 0.24949775094466423, 0.2790251161136024, 0.0, 1.0, 0.4, 0.24243584230175722], 
reward next is 0.7576, 
noisyNet noise sample is [array([0.47513637], dtype=float32), -0.41774422]. 
=============================================
[2019-04-09 15:12:33,151] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[32.914024]
 [33.14946 ]
 [33.290966]
 [33.090027]
 [33.25813 ]], R is [[33.15378952]
 [33.54763031]
 [33.85280991]
 [34.15993881]
 [34.5372467 ]].
[2019-04-09 15:12:33,279] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5504915e-08 2.3574696e-06 9.0392728e-05 8.5604723e-09 8.9541024e-01
 4.6682539e-03 9.8177068e-02 4.5268513e-05 1.2539238e-03 2.7464482e-04
 7.7809644e-05], sum to 1.0000
[2019-04-09 15:12:33,289] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8381
[2019-04-09 15:12:33,334] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.65989742e-08 8.65150741e-06 1.68812810e-04 4.63614676e-08
 5.83090067e-01 2.87677255e-02 3.86071444e-01 1.09647954e-04
 1.39291433e-03 3.56094650e-04 3.44498840e-05], sum to 1.0000
[2019-04-09 15:12:33,334] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2968
[2019-04-09 15:12:33,391] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.8, 93.0, 95.0, 0.0, 22.5, 24.17338295484941, -0.02419390517870052, 1.0, 1.0, 35.0, 23.57481256733465], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 913200.0000, 
sim time next is 914400.0000, 
raw observation next is [3.8, 93.0, 93.0, 0.0, 22.5, 24.37268389839054, -0.09003695322246445, 1.0, 1.0, 35.0, 20.75558666622495], 
processed observation next is [1.0, 0.6086956521739131, 0.5678670360110805, 0.93, 0.31, 0.0, 0.375, 0.531056991532545, 0.4699876822591785, 1.0, 1.0, 0.4, 0.2075558666622495], 
reward next is 0.7924, 
noisyNet noise sample is [array([-0.5649719], dtype=float32), -1.6193396]. 
=============================================
[2019-04-09 15:12:33,406] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.466666666666667, 79.66666666666666, 0.0, 0.0, 19.0, 21.24844630452298, -0.5874746895898214, 0.0, 1.0, 45.0, 35.121605980163736], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 862800.0000, 
sim time next is 864000.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 19.0, 21.26290051379009, -0.594231255265054, 0.0, 1.0, 35.0, 27.22506313355803], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.08333333333333333, 0.2719083761491741, 0.30192291491164863, 0.0, 1.0, 0.4, 0.2722506313355803], 
reward next is 0.7277, 
noisyNet noise sample is [array([1.3501474], dtype=float32), -1.2872685]. 
=============================================
[2019-04-09 15:12:33,456] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[33.496994]
 [32.985325]
 [33.449947]
 [33.83165 ]
 [33.45112 ]], R is [[32.90831757]
 [33.22801971]
 [33.61109924]
 [33.90031052]
 [34.03609848]].
[2019-04-09 15:12:33,556] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.2662976e-08 4.7274258e-05 6.0392864e-04 1.4661264e-08 7.9171908e-01
 8.2521671e-03 1.9795384e-01 3.1794905e-05 1.0551488e-03 2.7371242e-04
 6.2952109e-05], sum to 1.0000
[2019-04-09 15:12:33,607] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5346
[2019-04-09 15:12:33,657] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 19.0, 21.26290051379009, -0.594231255265054, 0.0, 1.0, 35.0, 27.22506313355803], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 864000.0000, 
sim time next is 865200.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 19.0, 21.19056306821778, -0.6174909894757012, 0.0, 1.0, 35.0, 23.946501792794713], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.08333333333333333, 0.26588025568481505, 0.29416967017476625, 0.0, 1.0, 0.4, 0.23946501792794714], 
reward next is 0.7605, 
noisyNet noise sample is [array([0.05751239], dtype=float32), -0.98350006]. 
=============================================
[2019-04-09 15:12:33,747] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.91722050e-07 7.18190786e-05 4.00739489e-04 1.16096174e-07
 8.13051164e-01 9.43894964e-03 1.73969477e-01 2.95125879e-04
 1.33726932e-03 1.37002987e-03 6.45993568e-05], sum to 1.0000
[2019-04-09 15:12:33,748] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7213
[2019-04-09 15:12:33,768] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.9, 79.33333333333334, 0.0, 0.0, 19.0, 20.71216610027208, -0.7377637400779556, 0.0, 1.0, 35.0, 21.234755053718153], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 870000.0000, 
sim time next is 871200.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 19.0, 20.52809138745659, -0.7602419605125713, 0.0, 1.0, 35.0, 21.940542410455805], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.79, 0.0, 0.0, 0.08333333333333333, 0.21067428228804927, 0.24658601316247622, 0.0, 1.0, 0.4, 0.21940542410455804], 
reward next is 0.7806, 
noisyNet noise sample is [array([-0.7704601], dtype=float32), -0.21308777]. 
=============================================
[2019-04-09 15:12:34,098] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-09 15:12:34,108] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:12:34,117] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:12:34,118] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:12:34,118] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:12:34,119] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:12:34,119] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:12:34,122] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run9
[2019-04-09 15:12:34,139] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run9
[2019-04-09 15:12:34,154] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run9
[2019-04-09 15:13:42,846] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.04774871], dtype=float32), 0.0691454]
[2019-04-09 15:13:42,847] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-1.712745866, 35.37835969, 212.9724351, 0.0, 22.5, 22.41213528177358, -0.4596826156243241, 1.0, 1.0, 35.0, 28.42066566994189]
[2019-04-09 15:13:42,848] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:13:42,849] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [1.8970613e-07 2.1979429e-05 4.8210577e-04 1.0017558e-07 6.9823849e-01
 1.1337977e-02 2.8740588e-01 1.2854565e-04 1.6910727e-03 6.0977309e-04
 8.3897830e-05], sampled 0.8776007110899698
[2019-04-09 15:14:13,344] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 3053.5239 108434.9393 537.9585
[2019-04-09 15:14:13,364] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:13,364] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:13,364] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:13,364] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:13,364] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:13,364] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:13,364] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:13,364] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:13,364] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:13,500] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:13,500] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:13,500] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:13,500] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:13,500] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:13,500] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:13,500] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:13,500] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:13,500] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:29,887] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2979.5415 115031.2651 170.3715
[2019-04-09 15:14:29,908] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:29,908] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:29,908] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:29,908] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:29,908] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:29,908] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:29,908] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:29,908] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:29,908] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:30,014] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:30,014] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:30,014] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:30,014] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:30,014] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:30,014] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:30,014] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:30,014] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:30,014] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:34,140] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2890.4883 117128.0930 -75.0330
[2019-04-09 15:14:34,161] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:34,161] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:34,161] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:34,161] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:34,161] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:34,161] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:34,161] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:34,161] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:34,161] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:14:34,274] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:34,274] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:34,274] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:34,274] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:34,274] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:34,274] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:34,274] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:34,274] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:34,274] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:14:35,163] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 80000, evaluation results [80000.0, 2979.5415184240987, 115031.26506568374, 170.3714504585456, 3053.523860416066, 108434.93930875207, 537.9585497864758, 2890.488280287469, 117128.09301159478, -75.03299308574121]
[2019-04-09 15:14:36,349] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.7295391e-10 6.0750739e-07 8.1916960e-06 5.0720739e-10 4.1687173e-01
 4.9191073e-04 5.8237016e-01 1.9763265e-05 1.7989743e-04 5.4368447e-05
 3.3644249e-06], sum to 1.0000
[2019-04-09 15:14:36,349] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6739
[2019-04-09 15:14:36,446] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.2, 93.0, 84.0, 0.0, 22.5, 24.2856727245946, -0.006812391428591617, 1.0, 1.0, 45.0, 38.47710018763907], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 916800.0000, 
sim time next is 918000.0000, 
raw observation next is [4.4, 93.0, 72.0, 0.0, 22.5, 24.40941053643811, 0.03061637957368042, 1.0, 1.0, 45.0, 38.11391056573397], 
processed observation next is [1.0, 0.6521739130434783, 0.5844875346260389, 0.93, 0.24, 0.0, 0.375, 0.5341175447031757, 0.5102054598578935, 1.0, 1.0, 0.6, 0.38113910565733966], 
reward next is 0.6189, 
noisyNet noise sample is [array([1.5445384], dtype=float32), 0.6290712]. 
=============================================
[2019-04-09 15:14:36,452] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[40.19681 ]
 [39.577854]
 [39.119778]
 [38.833233]
 [38.353046]], R is [[40.70473099]
 [40.91291428]
 [41.11442184]
 [41.28717041]
 [41.36845016]].
[2019-04-09 15:14:36,863] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7351073e-07 2.0461630e-05 1.4426549e-04 3.4297134e-08 8.1589895e-01
 1.3213170e-02 1.6845463e-01 2.0303954e-04 1.7086536e-03 3.2199780e-04
 3.4490182e-05], sum to 1.0000
[2019-04-09 15:14:36,863] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6373
[2019-04-09 15:14:36,894] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.433333333333334, 83.0, 0.0, 0.0, 19.0, 22.72168500228217, -0.1732784118218151, 0.0, 1.0, 35.0, 24.270780767636566], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 967200.0000, 
sim time next is 968400.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 19.0, 22.82091073528802, -0.1746744159939202, 0.0, 1.0, 35.0, 21.155839152220132], 
processed observation next is [1.0, 0.21739130434782608, 0.7063711911357342, 0.83, 0.0, 0.0, 0.08333333333333333, 0.4017425612740017, 0.44177519466869325, 0.0, 1.0, 0.4, 0.21155839152220132], 
reward next is 0.7884, 
noisyNet noise sample is [array([2.2019389], dtype=float32), 0.5090076]. 
=============================================
[2019-04-09 15:14:37,270] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4951078e-08 7.9013665e-07 1.7230888e-04 8.4564835e-09 6.9874847e-01
 4.0740985e-03 2.9475683e-01 4.3575026e-05 2.0610553e-03 8.1646634e-05
 6.1283201e-05], sum to 1.0000
[2019-04-09 15:14:37,270] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8777
[2019-04-09 15:14:37,392] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 84.0, 87.0, 0.0, 22.5, 23.64149226814096, -0.1789237059981642, 1.0, 1.0, 45.0, 38.7762328369608], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 903600.0000, 
sim time next is 904800.0000, 
raw observation next is [1.633333333333334, 88.33333333333334, 93.66666666666667, 0.0, 22.5, 23.76608676866766, -0.1321426897738461, 1.0, 1.0, 45.0, 39.66320204739777], 
processed observation next is [1.0, 0.4782608695652174, 0.5078485687903971, 0.8833333333333334, 0.31222222222222223, 0.0, 0.375, 0.4805072307223049, 0.4559524367420513, 1.0, 1.0, 0.6, 0.39663202047397766], 
reward next is 0.6034, 
noisyNet noise sample is [array([-0.32992253], dtype=float32), -0.3946768]. 
=============================================
[2019-04-09 15:14:37,856] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2243889e-09 7.6960816e-07 1.4081511e-05 1.4064464e-09 8.4107208e-01
 2.1195889e-03 1.5607689e-01 3.3439901e-05 5.9904775e-04 6.4110471e-05
 2.0087175e-05], sum to 1.0000
[2019-04-09 15:14:37,856] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2627
[2019-04-09 15:14:37,872] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.6, 86.0, 108.0, 0.0, 22.5, 25.23430068705592, 0.3007699073787544, 1.0, 1.0, 35.0, 19.9237317174658], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 990000.0000, 
sim time next is 991200.0000, 
raw observation next is [11.8, 86.0, 116.0, 0.0, 22.5, 25.23838472546937, 0.3113125207874146, 1.0, 1.0, 35.0, 17.96348339856142], 
processed observation next is [1.0, 0.4782608695652174, 0.7894736842105264, 0.86, 0.38666666666666666, 0.0, 0.375, 0.6031987271224475, 0.6037708402624715, 1.0, 1.0, 0.4, 0.1796348339856142], 
reward next is 0.8204, 
noisyNet noise sample is [array([1.1545663], dtype=float32), 1.2896638]. 
=============================================
[2019-04-09 15:14:38,253] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4263713e-07 2.5125059e-05 1.3139416e-04 5.2562019e-08 5.2193964e-01
 1.7531449e-02 4.5733714e-01 1.3214942e-04 2.4570650e-03 3.7626663e-04
 6.9533395e-05], sum to 1.0000
[2019-04-09 15:14:38,253] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7245
[2019-04-09 15:14:38,285] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 19.0, 23.34506511628997, -0.06298324808256761, 0.0, 1.0, 35.0, 24.29750120250193], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 972000.0000, 
sim time next is 973200.0000, 
raw observation next is [9.200000000000001, 83.0, 0.0, 0.0, 19.0, 23.5153963068516, -0.05345443007813763, 0.0, 1.0, 45.0, 30.47602580936782], 
processed observation next is [1.0, 0.2608695652173913, 0.7174515235457064, 0.83, 0.0, 0.0, 0.08333333333333333, 0.4596163589042999, 0.4821818566406208, 0.0, 1.0, 0.6, 0.3047602580936782], 
reward next is 0.6952, 
noisyNet noise sample is [array([0.805498], dtype=float32), -1.2355659]. 
=============================================
[2019-04-09 15:14:38,587] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9781170e-08 1.2230739e-05 7.5420452e-05 8.8753849e-09 2.8251863e-01
 2.0014651e-03 7.0853806e-01 1.1797288e-04 6.4302469e-03 1.6459287e-04
 1.4133920e-04], sum to 1.0000
[2019-04-09 15:14:38,589] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9998
[2019-04-09 15:14:38,645] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [9.4, 93.0, 13.5, 0.0, 22.5, 23.37195349024181, -0.0490392415872101, 1.0, 1.0, 45.0, 30.526405318662356], 
current ob forecast is [], 
actual action is [1, 60.0], 
sim time this is 979200.0000, 
sim time next is 980400.0000, 
raw observation next is [9.600000000000001, 92.66666666666667, 22.5, 0.0, 22.5, 23.4245119771979, 0.07472195667859177, 1.0, 1.0, 60.0, 57.795242017092576], 
processed observation next is [1.0, 0.34782608695652173, 0.7285318559556788, 0.9266666666666667, 0.075, 0.0, 0.375, 0.4520426647664915, 0.5249073188928639, 1.0, 1.0, 0.9, 0.5779524201709257], 
reward next is 0.4220, 
noisyNet noise sample is [array([0.33130673], dtype=float32), 0.6943293]. 
=============================================
[2019-04-09 15:14:39,917] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2770594e-08 1.0536309e-05 2.2976397e-04 9.2270040e-09 5.1806420e-01
 3.1598744e-03 4.7697642e-01 5.7949535e-05 9.4020361e-04 3.7729024e-04
 1.8385037e-04], sum to 1.0000
[2019-04-09 15:14:39,917] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3087
[2019-04-09 15:14:39,938] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.333333333333334, 91.33333333333333, 0.0, 0.0, 19.0, 22.20920641057901, -0.2803917898226889, 0.0, 1.0, 45.0, 33.34040687115622], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 952800.0000, 
sim time next is 954000.0000, 
raw observation next is [5.5, 89.0, 0.0, 0.0, 19.0, 22.26751840960575, -0.2762704029307776, 0.0, 1.0, 35.0, 25.59197522934036], 
processed observation next is [1.0, 0.043478260869565216, 0.6149584487534627, 0.89, 0.0, 0.0, 0.08333333333333333, 0.35562653413381246, 0.4079098656897408, 0.0, 1.0, 0.4, 0.2559197522934036], 
reward next is 0.7441, 
noisyNet noise sample is [array([0.7963272], dtype=float32), 0.23836304]. 
=============================================
[2019-04-09 15:14:39,987] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[34.70314 ]
 [35.154427]
 [36.05459 ]
 [36.32711 ]
 [36.361004]], R is [[34.78289032]
 [35.10165787]
 [35.49897766]
 [35.75762939]
 [36.1975174 ]].
[2019-04-09 15:14:40,749] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5024360e-09 1.9152503e-06 4.3780293e-04 1.0180672e-09 7.4745589e-01
 2.5379434e-02 2.2587794e-01 6.4121682e-06 7.5988972e-04 5.3595835e-05
 2.7136426e-05], sum to 1.0000
[2019-04-09 15:14:40,750] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2105
[2019-04-09 15:14:40,858] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 22.5, 23.61427649082461, -0.01554420271963103, 1.0, 1.0, 35.0, 28.061713768786717], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 936000.0000, 
sim time next is 937200.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 19.0, 23.58283216365331, -0.04358467919203316, 0.0, 1.0, 35.0, 23.673194107058315], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.08333333333333333, 0.46523601363777595, 0.48547177360265564, 0.0, 1.0, 0.4, 0.23673194107058315], 
reward next is 0.7633, 
noisyNet noise sample is [array([-0.1243456], dtype=float32), -0.88271075]. 
=============================================
[2019-04-09 15:14:41,072] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9710404e-07 8.0933518e-05 9.6735964e-04 6.9711291e-08 8.1744993e-01
 1.8162288e-02 1.5856408e-01 1.5837493e-04 3.8636769e-03 4.5703090e-04
 2.9619416e-04], sum to 1.0000
[2019-04-09 15:14:41,073] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7717
[2019-04-09 15:14:41,108] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 19.0, 24.6349323430572, 0.292218770071128, 0.0, 1.0, 35.0, 13.517199107502565], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1047600.0000, 
sim time next is 1048800.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 19.0, 24.67806991262437, 0.2921523686492461, 0.0, 1.0, 35.0, 12.315007907593385], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 0.08333333333333333, 0.5565058260520308, 0.597384122883082, 0.0, 1.0, 0.4, 0.12315007907593385], 
reward next is 0.8768, 
noisyNet noise sample is [array([0.3343578], dtype=float32), 1.4919772]. 
=============================================
[2019-04-09 15:14:41,233] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.14206566e-10 5.51613482e-07 1.49608495e-05 2.89025304e-10
 8.88629258e-01 2.78759841e-03 1.07626632e-01 3.66914014e-06
 7.49010476e-04 1.81874639e-04 6.35999186e-06], sum to 1.0000
[2019-04-09 15:14:41,235] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0265
[2019-04-09 15:14:41,279] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 76.33333333333334, 0.0, 0.0, 19.0, 24.2821770165372, 0.2233564811695918, 0.0, 1.0, 35.0, 14.153653106679295], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1027200.0000, 
sim time next is 1028400.0000, 
raw observation next is [14.4, 75.66666666666667, 0.0, 0.0, 19.0, 24.16620404884931, 0.2019836861549947, 0.0, 1.0, 35.0, 12.972616245590874], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7566666666666667, 0.0, 0.0, 0.08333333333333333, 0.5138503374041091, 0.5673278953849982, 0.0, 1.0, 0.4, 0.12972616245590873], 
reward next is 0.8703, 
noisyNet noise sample is [array([-1.4933921], dtype=float32), 0.71648073]. 
=============================================
[2019-04-09 15:14:41,800] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.23061351e-07 4.31310909e-05 3.66364722e-04 1.94726951e-07
 8.59565854e-01 2.79895216e-02 1.09680317e-01 3.08206829e-04
 1.36442226e-03 5.60365093e-04 1.21174744e-04], sum to 1.0000
[2019-04-09 15:14:41,803] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2782
[2019-04-09 15:14:41,844] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 65.0, 161.0, 0.0, 19.0, 26.62477549432502, 0.853613569991862, 0.0, 0.0, 35.0, 18.919758079558076], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1171200.0000, 
sim time next is 1172400.0000, 
raw observation next is [18.3, 65.0, 153.8333333333333, 0.0, 19.0, 26.6800786552568, 0.858445309849213, 0.0, 0.0, 35.0, 16.643610856278322], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.5127777777777777, 0.0, 0.08333333333333333, 0.7233398879380667, 0.7861484366164043, 0.0, 0.0, 0.4, 0.1664361085627832], 
reward next is 0.8336, 
noisyNet noise sample is [array([0.26801422], dtype=float32), -0.12678571]. 
=============================================
[2019-04-09 15:14:42,138] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0068029e-09 5.4705509e-07 1.1253551e-05 2.9178895e-10 9.2525643e-01
 2.5431002e-03 7.2093271e-02 4.1172866e-06 5.2486797e-05 3.5622110e-05
 3.1481416e-06], sum to 1.0000
[2019-04-09 15:14:42,142] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2855
[2019-04-09 15:14:42,178] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 81.0, 106.5, 0.0, 22.5, 25.75383098956205, 0.4947331913090126, 1.0, 1.0, 45.0, 28.305667218262407], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1000800.0000, 
sim time next is 1002000.0000, 
raw observation next is [14.4, 81.0, 98.16666666666667, 0.0, 22.5, 25.93936657561072, 0.5273000948851405, 1.0, 1.0, 35.0, 21.263072215307112], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.32722222222222225, 0.0, 0.375, 0.6616138813008933, 0.6757666982950469, 1.0, 1.0, 0.4, 0.2126307221530711], 
reward next is 0.7874, 
noisyNet noise sample is [array([1.5870272], dtype=float32), 0.34469482]. 
=============================================
[2019-04-09 15:14:42,210] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[45.205555]
 [45.004887]
 [44.517773]
 [44.179924]
 [43.97851 ]], R is [[45.77167511]
 [46.03089905]
 [46.28821564]
 [46.63563156]
 [46.95788956]].
[2019-04-09 15:14:42,726] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4335731e-09 5.1120867e-07 9.7354678e-05 8.3527840e-10 8.1749070e-01
 7.5434619e-03 1.7422278e-01 1.4604936e-05 5.8810005e-04 3.7760441e-05
 4.6680580e-06], sum to 1.0000
[2019-04-09 15:14:42,726] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7023
[2019-04-09 15:14:42,771] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [17.16666666666667, 51.0, 0.0, 0.0, 22.5, 28.23004138201236, 0.9836843331877773, 1.0, 0.0, 45.0, 23.34847239544441], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1099200.0000, 
sim time next is 1100400.0000, 
raw observation next is [16.63333333333333, 52.0, 0.0, 0.0, 22.5, 26.4905739276501, 0.9056774863139986, 1.0, 0.0, 35.0, 18.717968391521975], 
processed observation next is [1.0, 0.7391304347826086, 0.9233610341643583, 0.52, 0.0, 0.0, 0.375, 0.707547827304175, 0.8018924954379996, 1.0, 0.0, 0.4, 0.18717968391521975], 
reward next is 0.8128, 
noisyNet noise sample is [array([1.3348702], dtype=float32), -0.19262533]. 
=============================================
[2019-04-09 15:14:42,959] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5708140e-09 3.3912686e-06 5.4082888e-05 2.2194828e-09 1.8964145e-01
 8.6197155e-03 8.0098814e-01 1.6758268e-05 5.7574641e-04 8.8083645e-05
 1.2560846e-05], sum to 1.0000
[2019-04-09 15:14:42,960] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9158
[2019-04-09 15:14:42,990] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.66666666666667, 0.0, 0.0, 19.0, 24.88387251958814, 0.368875135346605, 0.0, 1.0, 35.0, 20.990914445595745], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1028400.0000, 
sim time next is 1029600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 24.89148673455597, 0.3937487507370034, 0.0, 1.0, 45.0, 26.927324200411846], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.5742905612129974, 0.6312495835790012, 0.0, 1.0, 0.6, 0.26927324200411845], 
reward next is 0.7307, 
noisyNet noise sample is [array([-0.16453642], dtype=float32), 1.2470106]. 
=============================================
[2019-04-09 15:14:43,032] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4373547e-09 3.5333508e-06 3.5377157e-05 2.4574638e-09 8.4193569e-01
 6.3200509e-03 1.4888157e-01 2.4093557e-05 2.5889517e-03 2.0378409e-04
 6.9132234e-06], sum to 1.0000
[2019-04-09 15:14:43,032] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2985
[2019-04-09 15:14:43,074] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 75.66666666666667, 0.0, 0.0, 19.0, 24.60937054960003, 0.3336738206227776, 0.0, 1.0, 45.0, 28.675986039637124], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1028400.0000, 
sim time next is 1029600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 24.69325548545999, 0.3492378110590343, 0.0, 1.0, 35.0, 21.368507331506457], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.5577712904549991, 0.6164126036863448, 0.0, 1.0, 0.4, 0.21368507331506456], 
reward next is 0.7863, 
noisyNet noise sample is [array([-0.55101687], dtype=float32), -0.8312241]. 
=============================================
[2019-04-09 15:14:43,393] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.2481746e-08 6.1144992e-06 9.7343385e-05 9.6368026e-08 8.9649069e-01
 6.4332844e-03 9.5279999e-02 5.5346707e-05 1.4607178e-03 1.3492782e-04
 4.1411004e-05], sum to 1.0000
[2019-04-09 15:14:43,394] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1451
[2019-04-09 15:14:43,420] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.7, 81.33333333333334, 0.0, 0.0, 19.0, 25.51160381521575, 0.5574349549985541, 0.0, 1.0, 35.0, 14.238507725795241], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1149600.0000, 
sim time next is 1150800.0000, 
raw observation next is [12.7, 82.66666666666667, 0.0, 0.0, 19.0, 25.43508387956638, 0.5422061921054571, 0.0, 1.0, 35.0, 12.999280587039244], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.6195903232971984, 0.6807353973684857, 0.0, 1.0, 0.4, 0.12999280587039244], 
reward next is 0.8700, 
noisyNet noise sample is [array([-1.0737863], dtype=float32), 0.7891007]. 
=============================================
[2019-04-09 15:14:43,925] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.79750295e-08 2.69416751e-06 3.55236611e-04 3.22848526e-09
 8.96648645e-01 8.58925283e-03 8.64316449e-02 1.03696904e-04
 7.48645840e-03 3.26033478e-04 5.63580652e-05], sum to 1.0000
[2019-04-09 15:14:43,925] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8523
[2019-04-09 15:14:43,956] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 19.0, 24.6300450938285, 0.3146145117293615, 0.0, 1.0, 35.0, 15.776161651380349], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1062000.0000, 
sim time next is 1063200.0000, 
raw observation next is [12.93333333333334, 81.0, 0.0, 0.0, 22.5, 24.57488220937327, 0.3281967868923342, 1.0, 1.0, 35.0, 12.966292980717725], 
processed observation next is [1.0, 0.30434782608695654, 0.8208679593721148, 0.81, 0.0, 0.0, 0.375, 0.5479068507811059, 0.6093989289641114, 1.0, 1.0, 0.4, 0.12966292980717725], 
reward next is 0.8703, 
noisyNet noise sample is [array([0.5877443], dtype=float32), 0.3927474]. 
=============================================
[2019-04-09 15:14:44,010] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9686924e-07 8.4923064e-05 7.4731983e-04 1.8564023e-07 6.6668713e-01
 2.2900341e-02 3.0693829e-01 9.6867603e-05 1.8204184e-03 6.6179322e-04
 6.2609739e-05], sum to 1.0000
[2019-04-09 15:14:44,016] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2709
[2019-04-09 15:14:44,034] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.6, 79.0, 0.0, 0.0, 19.0, 25.20412984380008, 0.5449910724721284, 0.0, 1.0, 35.0, 21.624419497088166], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1142400.0000, 
sim time next is 1143600.0000, 
raw observation next is [11.6, 81.0, 0.0, 0.0, 19.0, 25.29340516674478, 0.5399510166478227, 0.0, 1.0, 35.0, 18.92560292910412], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.81, 0.0, 0.0, 0.08333333333333333, 0.6077837638953983, 0.679983672215941, 0.0, 1.0, 0.4, 0.18925602929104118], 
reward next is 0.8107, 
noisyNet noise sample is [array([-0.6369937], dtype=float32), -1.2754713]. 
=============================================
[2019-04-09 15:14:44,073] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4160256e-09 6.1068010e-07 5.3742027e-04 1.2280653e-09 5.6780201e-01
 1.4171538e-02 4.1682559e-01 3.2979839e-05 5.5668293e-04 7.1528841e-05
 1.7170989e-06], sum to 1.0000
[2019-04-09 15:14:44,077] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1646
[2019-04-09 15:14:44,107] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 56.0, 176.0, 158.5, 22.5, 26.86551499519206, 0.8330334889654575, 1.0, 0.0, 35.0, 11.933088936586858], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1083600.0000, 
sim time next is 1084800.0000, 
raw observation next is [18.46666666666667, 55.33333333333334, 172.6666666666667, 52.83333333333332, 22.5, 27.04158389186412, 0.8759469684240608, 1.0, 0.0, 35.0, 9.777144228707648], 
processed observation next is [1.0, 0.5652173913043478, 0.9741458910433982, 0.5533333333333335, 0.5755555555555557, 0.058379373848987094, 0.375, 0.7534653243220101, 0.7919823228080203, 1.0, 0.0, 0.4, 0.09777144228707649], 
reward next is 0.9022, 
noisyNet noise sample is [array([-1.453538], dtype=float32), -0.32926816]. 
=============================================
[2019-04-09 15:14:44,169] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0531746e-08 2.9866042e-06 3.7423826e-05 2.6724947e-09 8.9836085e-01
 3.6132641e-03 9.7501263e-02 1.1834145e-05 3.9719819e-04 6.3603882e-05
 1.1560493e-05], sum to 1.0000
[2019-04-09 15:14:44,169] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2481
[2019-04-09 15:14:44,205] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.2, 77.33333333333333, 0.0, 0.0, 19.0, 25.28805124111253, 0.4696016894229969, 0.0, 1.0, 45.0, 26.317625396864024], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1046400.0000, 
sim time next is 1047600.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 19.0, 25.2686382047592, 0.4776196186560968, 0.0, 1.0, 35.0, 20.51512657352927], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6057198503965999, 0.6592065395520322, 0.0, 1.0, 0.4, 0.2051512657352927], 
reward next is 0.7948, 
noisyNet noise sample is [array([0.35583544], dtype=float32), 0.012023698]. 
=============================================
[2019-04-09 15:14:44,321] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1953598e-07 2.2198765e-05 4.6832312e-04 9.9247623e-09 8.4780979e-01
 6.5306588e-03 1.4339153e-01 9.6488853e-05 1.4220547e-03 1.2638292e-04
 1.3249692e-04], sum to 1.0000
[2019-04-09 15:14:44,329] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3230
[2019-04-09 15:14:44,347] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.7, 81.33333333333334, 0.0, 0.0, 19.0, 25.80304918558134, 0.6266176475764221, 0.0, 1.0, 35.0, 14.079280486898565], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1149600.0000, 
sim time next is 1150800.0000, 
raw observation next is [12.7, 82.66666666666667, 0.0, 0.0, 19.0, 25.71616732253386, 0.6091720115340309, 0.0, 1.0, 35.0, 12.883270435724146], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.6430139435444883, 0.7030573371780102, 0.0, 1.0, 0.4, 0.12883270435724145], 
reward next is 0.8712, 
noisyNet noise sample is [array([1.10534], dtype=float32), 0.066258125]. 
=============================================
[2019-04-09 15:14:44,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.3838972e-09 7.9841057e-06 1.9963960e-04 6.2113745e-09 8.0881774e-01
 2.2927101e-03 1.8608223e-01 1.8255203e-04 2.2285036e-03 1.4663881e-04
 4.1946463e-05], sum to 1.0000
[2019-04-09 15:14:44,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2558
[2019-04-09 15:14:44,492] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 78.0, 0.0, 0.0, 19.0, 24.77838303379853, 0.3366389578308907, 0.0, 1.0, 35.0, 21.295970981465505], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1054800.0000, 
sim time next is 1056000.0000, 
raw observation next is [13.63333333333333, 78.66666666666667, 0.0, 0.0, 19.0, 24.76456265191546, 0.3344988142106379, 0.0, 1.0, 35.0, 18.90761921192369], 
processed observation next is [1.0, 0.21739130434782608, 0.840258541089566, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.5637135543262882, 0.6114996047368794, 0.0, 1.0, 0.4, 0.1890761921192369], 
reward next is 0.8109, 
noisyNet noise sample is [array([1.397886], dtype=float32), 1.2851799]. 
=============================================
[2019-04-09 15:14:44,499] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[36.203476]
 [36.321465]
 [35.97966 ]
 [35.67945 ]
 [35.66935 ]], R is [[36.75766754]
 [37.17713165]
 [37.51494217]
 [37.85442352]
 [38.34791565]].
[2019-04-09 15:14:44,907] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.3802870e-09 2.3832449e-06 1.1505764e-04 2.7119038e-09 8.9335227e-01
 5.6560598e-03 9.9480271e-02 1.2161058e-05 1.3111373e-03 4.8654721e-05
 2.2027700e-05], sum to 1.0000
[2019-04-09 15:14:44,907] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9161
[2019-04-09 15:14:44,932] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.93333333333334, 81.0, 0.0, 0.0, 22.5, 24.07495288862866, 0.2110108494188464, 1.0, 1.0, 35.0, 18.374387052208085], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1063200.0000, 
sim time next is 1064400.0000, 
raw observation next is [12.56666666666667, 82.0, 0.0, 0.0, 22.5, 24.21670094919687, 0.2485317765293246, 1.0, 1.0, 45.0, 29.74565205577091], 
processed observation next is [1.0, 0.30434782608695654, 0.8107109879963068, 0.82, 0.0, 0.0, 0.375, 0.5180584124330725, 0.5828439255097749, 1.0, 1.0, 0.6, 0.2974565205577091], 
reward next is 0.7025, 
noisyNet noise sample is [array([0.18399851], dtype=float32), -1.9785925]. 
=============================================
[2019-04-09 15:14:45,201] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.6991142e-09 7.6516142e-07 3.4574349e-04 1.8472299e-09 4.1080406e-01
 1.3771928e-02 5.7225430e-01 4.6699526e-05 2.6105084e-03 1.4707942e-04
 1.8881905e-05], sum to 1.0000
[2019-04-09 15:14:45,203] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3672
[2019-04-09 15:14:45,229] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.93333333333334, 81.0, 0.0, 0.0, 22.5, 24.84233326016452, 0.3893210021907453, 1.0, 1.0, 35.0, 20.57926745212945], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1063200.0000, 
sim time next is 1064400.0000, 
raw observation next is [12.56666666666667, 82.0, 0.0, 0.0, 22.5, 24.97383535163941, 0.3791468324505279, 1.0, 1.0, 35.0, 18.215402766034224], 
processed observation next is [1.0, 0.30434782608695654, 0.8107109879963068, 0.82, 0.0, 0.0, 0.375, 0.5811529459699507, 0.6263822774835093, 1.0, 1.0, 0.4, 0.18215402766034225], 
reward next is 0.8178, 
noisyNet noise sample is [array([0.9720268], dtype=float32), 0.93318504]. 
=============================================
[2019-04-09 15:14:45,381] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.21700898e-08 1.22623305e-05 2.88455572e-04 4.54196947e-09
 7.21567273e-01 8.79135542e-03 2.68212914e-01 2.07344547e-05
 8.89733492e-04 1.97727422e-04 1.94728309e-05], sum to 1.0000
[2019-04-09 15:14:45,381] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1355
[2019-04-09 15:14:45,398] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 24.63671486913668, 0.3257933753757838, 0.0, 1.0, 35.0, 14.49518024664891], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1038000.0000, 
sim time next is 1039200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 24.7539083619969, 0.3132540378081965, 0.0, 1.0, 35.0, 13.139928907741716], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.5628256968330749, 0.6044180126027322, 0.0, 1.0, 0.4, 0.13139928907741716], 
reward next is 0.8686, 
noisyNet noise sample is [array([0.8441223], dtype=float32), -2.7105756]. 
=============================================
[2019-04-09 15:14:45,493] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5319279e-10 1.5180400e-07 3.2116412e-05 1.6587867e-10 6.6092396e-01
 6.7284429e-03 3.3195952e-01 2.5728766e-06 2.8116614e-04 6.2694722e-05
 9.4273073e-06], sum to 1.0000
[2019-04-09 15:14:45,494] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2045
[2019-04-09 15:14:45,534] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [17.73333333333333, 59.0, 179.3333333333333, 264.1666666666667, 22.5, 27.12665815709114, 0.9462066847584514, 1.0, 0.0, 35.0, 17.792311128242066], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1082400.0000, 
sim time next is 1083600.0000, 
raw observation next is [18.3, 56.0, 176.0, 158.5, 22.5, 27.43654161628941, 0.9843886794624997, 1.0, 0.0, 35.0, 14.704154891510925], 
processed observation next is [1.0, 0.5652173913043478, 0.9695290858725764, 0.56, 0.5866666666666667, 0.17513812154696132, 0.375, 0.7863784680241176, 0.8281295598208332, 1.0, 0.0, 0.4, 0.14704154891510923], 
reward next is 0.8530, 
noisyNet noise sample is [array([0.79203624], dtype=float32), 0.88059354]. 
=============================================
[2019-04-09 15:14:45,780] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4429519e-11 8.4447692e-08 5.3082658e-06 9.3718970e-11 9.5589954e-01
 1.9099719e-04 4.3867208e-02 3.2423661e-06 3.2187250e-05 9.2785467e-07
 6.6235759e-07], sum to 1.0000
[2019-04-09 15:14:45,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2708
[2019-04-09 15:14:45,807] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.8, 69.33333333333333, 0.0, 0.0, 19.0, 26.41653598268094, 0.8233702562176336, 0.0, 1.0, 35.0, 18.113540252538648], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1122000.0000, 
sim time next is 1123200.0000, 
raw observation next is [11.6, 71.0, 0.0, 0.0, 19.0, 26.39045282717158, 0.8053849914799805, 0.0, 1.0, 35.0, 16.5109447172729], 
processed observation next is [0.0, 0.0, 0.7839335180055402, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6992044022642983, 0.7684616638266601, 0.0, 1.0, 0.4, 0.165109447172729], 
reward next is 0.8349, 
noisyNet noise sample is [array([0.3216388], dtype=float32), 0.20451605]. 
=============================================
[2019-04-09 15:14:45,995] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9479097e-07 1.5480895e-05 5.5993494e-04 9.2575000e-08 5.6023222e-01
 9.0111727e-03 4.2836067e-01 5.2759846e-05 1.2520807e-03 4.3439234e-04
 8.1013502e-05], sum to 1.0000
[2019-04-09 15:14:45,998] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8963
[2019-04-09 15:14:46,018] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.3, 65.0, 143.5, 0.0, 19.0, 26.535951876313, 0.8501682732266048, 0.0, 0.0, 45.0, 24.38629696577819], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1173600.0000, 
sim time next is 1174800.0000, 
raw observation next is [18.3, 65.0, 133.1666666666667, 0.0, 19.0, 26.65582363847328, 0.8751068117224797, 0.0, 0.0, 45.0, 24.628714685823553], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.44388888888888905, 0.0, 0.08333333333333333, 0.72131863653944, 0.7917022705741599, 0.0, 0.0, 0.6, 0.24628714685823552], 
reward next is 0.7537, 
noisyNet noise sample is [array([-0.07393902], dtype=float32), -0.61566454]. 
=============================================
[2019-04-09 15:14:46,321] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.6508860e-10 9.8091596e-07 1.1124419e-04 3.2331646e-10 4.2682526e-01
 5.7699876e-03 5.6620383e-01 1.1816855e-05 1.0421666e-03 2.6845719e-05
 7.8794956e-06], sum to 1.0000
[2019-04-09 15:14:46,321] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9200
[2019-04-09 15:14:46,335] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.83333333333333, 49.33333333333334, 44.5, 0.0, 22.5, 28.47954530033294, 1.161876082115934, 1.0, 0.0, 35.0, 9.897572648155144], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1095600.0000, 
sim time next is 1096800.0000, 
raw observation next is [18.26666666666667, 49.66666666666666, 29.33333333333334, 0.4999999999999999, 22.5, 28.54323790799909, 1.175585212588394, 1.0, 0.0, 45.0, 11.851027851798861], 
processed observation next is [1.0, 0.6956521739130435, 0.968605724838412, 0.4966666666666666, 0.0977777777777778, 0.0005524861878453037, 0.375, 0.878603158999924, 0.8918617375294646, 1.0, 0.0, 0.6, 0.11851027851798861], 
reward next is 0.8815, 
noisyNet noise sample is [array([0.40389672], dtype=float32), -0.9595952]. 
=============================================
[2019-04-09 15:14:46,783] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.80384663e-09 8.27185090e-07 1.50462933e-04 4.01869116e-09
 8.82224441e-01 4.97724768e-03 1.11482576e-01 1.75952271e-04
 9.71083762e-04 1.57683862e-05 1.65564938e-06], sum to 1.0000
[2019-04-09 15:14:46,784] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3520
[2019-04-09 15:14:46,802] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.0, 67.66666666666667, 0.0, 0.0, 19.0, 26.40139421087578, 0.8281443195765451, 0.0, 1.0, 45.0, 26.531125980648145], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1120800.0000, 
sim time next is 1122000.0000, 
raw observation next is [11.8, 69.33333333333333, 0.0, 0.0, 19.0, 26.40347763811234, 0.8244747785002359, 0.0, 1.0, 35.0, 19.788705039462165], 
processed observation next is [1.0, 1.0, 0.7894736842105264, 0.6933333333333332, 0.0, 0.0, 0.08333333333333333, 0.7002898031760282, 0.7748249261667453, 0.0, 1.0, 0.4, 0.19788705039462165], 
reward next is 0.8021, 
noisyNet noise sample is [array([-0.28329992], dtype=float32), 0.38390535]. 
=============================================
[2019-04-09 15:14:46,809] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[42.96878 ]
 [42.758156]
 [43.285492]
 [43.347668]
 [42.82833 ]], R is [[43.23753738]
 [43.53984833]
 [43.95697784]
 [44.35600662]
 [44.73506927]].
[2019-04-09 15:14:46,841] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4752486e-07 9.7534612e-06 5.6587852e-04 1.5183569e-07 5.6334406e-01
 2.2435756e-03 4.3022007e-01 2.0181571e-04 3.1282390e-03 2.2867418e-04
 5.7511930e-05], sum to 1.0000
[2019-04-09 15:14:46,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2206
[2019-04-09 15:14:46,865] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 19.0, 26.77271082249291, 0.8583095360070593, 0.0, 0.0, 35.0, 11.528070999544891], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1192800.0000, 
sim time next is 1194000.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 19.0, 26.67776159350898, 0.8399619808952382, 0.0, 0.0, 35.0, 10.621918964002285], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.08333333333333333, 0.7231467994590816, 0.7799873269650793, 0.0, 0.0, 0.4, 0.10621918964002285], 
reward next is 0.8938, 
noisyNet noise sample is [array([-0.57321453], dtype=float32), -0.38997263]. 
=============================================
[2019-04-09 15:14:46,886] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[30.512562]
 [30.684622]
 [30.718874]
 [30.916119]
 [31.123701]], R is [[30.88782883]
 [31.46367073]
 [32.02386093]
 [32.56760788]
 [33.09257889]].
[2019-04-09 15:14:46,926] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.5418320e-09 6.9400180e-06 4.7285917e-05 6.3801320e-09 9.0122181e-01
 4.3599857e-03 9.3358107e-02 1.3118145e-05 9.2432898e-04 4.9605333e-05
 1.8791054e-05], sum to 1.0000
[2019-04-09 15:14:46,928] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7783
[2019-04-09 15:14:46,955] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.86666666666667, 75.0, 0.0, 0.0, 19.0, 26.27372504405261, 0.7641547984081852, 0.0, 1.0, 35.0, 14.671837370184146], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1125600.0000, 
sim time next is 1126800.0000, 
raw observation next is [10.5, 77.0, 0.0, 0.0, 19.0, 26.13259602862901, 0.7322868536276513, 0.0, 1.0, 35.0, 13.494449805561695], 
processed observation next is [0.0, 0.043478260869565216, 0.7534626038781165, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6777163357190842, 0.7440956178758839, 0.0, 1.0, 0.4, 0.13494449805561695], 
reward next is 0.8651, 
noisyNet noise sample is [array([1.6843177], dtype=float32), -0.18315656]. 
=============================================
[2019-04-09 15:14:47,257] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.3777529e-08 1.1654654e-05 7.6151337e-05 1.1686685e-07 7.8779179e-01
 3.5995091e-03 2.0749216e-01 3.7870534e-05 6.4610987e-04 3.3049588e-04
 1.3944046e-05], sum to 1.0000
[2019-04-09 15:14:47,260] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2932
[2019-04-09 15:14:47,294] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.6, 77.0, 0.0, 0.0, 19.0, 25.35916129389895, 0.5461148055228243, 0.0, 1.0, 35.0, 15.39449247912107], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1141200.0000, 
sim time next is 1142400.0000, 
raw observation next is [11.6, 79.0, 0.0, 0.0, 19.0, 25.33068852285303, 0.5309733334839698, 0.0, 1.0, 35.0, 14.02586353197924], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.79, 0.0, 0.0, 0.08333333333333333, 0.6108907102377525, 0.6769911111613233, 0.0, 1.0, 0.4, 0.1402586353197924], 
reward next is 0.8597, 
noisyNet noise sample is [array([0.72503555], dtype=float32), -0.4022396]. 
=============================================
[2019-04-09 15:14:47,726] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0635007e-07 3.0647683e-05 3.3519621e-04 9.2112600e-08 5.3874511e-01
 1.4903908e-02 4.4114259e-01 6.2103631e-05 4.1439929e-03 6.0659560e-04
 2.9596056e-05], sum to 1.0000
[2019-04-09 15:14:47,727] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9248
[2019-04-09 15:14:47,753] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.7, 81.33333333333334, 0.0, 0.0, 19.0, 25.84797734106408, 0.6714645604728807, 0.0, 1.0, 45.0, 28.046399516234132], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1149600.0000, 
sim time next is 1150800.0000, 
raw observation next is [12.7, 82.66666666666667, 0.0, 0.0, 19.0, 25.92957754033052, 0.6841036856546263, 0.0, 1.0, 45.0, 27.33232136383146], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.6607981283608767, 0.7280345618848755, 0.0, 1.0, 0.6, 0.27332321363831463], 
reward next is 0.7267, 
noisyNet noise sample is [array([-1.0801568], dtype=float32), 1.4368834]. 
=============================================
[2019-04-09 15:14:47,780] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.61278842e-07 9.15642977e-06 5.55940554e-04 1.03651985e-07
 6.94180250e-01 2.09293514e-02 2.80891240e-01 7.90411214e-05
 3.14151193e-03 1.41943063e-04 7.12469628e-05], sum to 1.0000
[2019-04-09 15:14:47,784] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0988
[2019-04-09 15:14:47,810] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.33333333333333, 81.0, 0.0, 0.0, 19.0, 25.29269002162996, 0.560627012334891, 0.0, 1.0, 35.0, 21.378704331764474], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1147200.0000, 
sim time next is 1148400.0000, 
raw observation next is [12.7, 80.0, 0.0, 0.0, 19.0, 25.36658720826941, 0.5578016106942676, 0.0, 1.0, 35.0, 18.611739067458206], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8, 0.0, 0.0, 0.08333333333333333, 0.6138822673557843, 0.6859338702314225, 0.0, 1.0, 0.4, 0.18611739067458205], 
reward next is 0.8139, 
noisyNet noise sample is [array([0.5267984], dtype=float32), -0.86731]. 
=============================================
[2019-04-09 15:14:47,988] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7822886e-10 6.3955787e-07 5.8825695e-05 7.9224732e-10 9.6095228e-01
 1.3585747e-03 3.7402678e-02 1.4629842e-06 1.8725314e-04 3.2717522e-05
 5.5350420e-06], sum to 1.0000
[2019-04-09 15:14:47,989] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4148
[2019-04-09 15:14:48,011] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 100.0, 55.33333333333333, 0.0, 19.0, 26.36788670022569, 0.8380893058603774, 0.0, 1.0, 35.0, 13.347789041926749], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1264800.0000, 
sim time next is 1266000.0000, 
raw observation next is [13.8, 100.0, 45.66666666666666, 0.0, 19.0, 26.30706229246673, 0.8237721865311868, 0.0, 1.0, 35.0, 12.176860120510785], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.1522222222222222, 0.0, 0.08333333333333333, 0.692255191038894, 0.7745907288437289, 0.0, 1.0, 0.4, 0.12176860120510785], 
reward next is 0.8782, 
noisyNet noise sample is [array([-0.41122493], dtype=float32), 0.73618215]. 
=============================================
[2019-04-09 15:14:48,022] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[37.373306]
 [37.531204]
 [37.472073]
 [37.280586]
 [37.219906]], R is [[37.95711517]
 [38.44406891]
 [38.9119606 ]
 [39.35990143]
 [39.7794838 ]].
[2019-04-09 15:14:48,127] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.3409717e-07 8.8772591e-05 2.0404966e-03 4.3537062e-07 8.4327286e-01
 6.1576962e-02 8.0684043e-02 2.0240661e-04 1.0726345e-02 1.1903376e-03
 2.1659896e-04], sum to 1.0000
[2019-04-09 15:14:48,130] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2780
[2019-04-09 15:14:48,160] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.46666666666667, 64.33333333333333, 169.0, 0.0, 19.0, 25.97388453084006, 0.701676576184619, 0.0, 0.0, 40.0, 18.712091739817573], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1168800.0000, 
sim time next is 1170000.0000, 
raw observation next is [18.3, 65.0, 165.0, 0.0, 19.0, 26.01635198932818, 0.7093908795336424, 0.0, 0.0, 35.0, 13.878397684538195], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.55, 0.0, 0.08333333333333333, 0.6680293324440149, 0.7364636265112141, 0.0, 0.0, 0.4, 0.13878397684538193], 
reward next is 0.8612, 
noisyNet noise sample is [array([-1.552277], dtype=float32), -0.20381398]. 
=============================================
[2019-04-09 15:14:48,186] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[29.008522]
 [28.813683]
 [29.812159]
 [30.351412]
 [31.111977]], R is [[29.70394897]
 [30.21978951]
 [30.81313324]
 [31.38895607]
 [31.94787407]].
[2019-04-09 15:14:48,431] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.2318625e-07 1.8828972e-05 1.1978403e-03 1.0343405e-07 8.7152189e-01
 2.0098178e-02 1.0417035e-01 2.0945417e-04 1.9615206e-03 7.3865458e-04
 8.2524253e-05], sum to 1.0000
[2019-04-09 15:14:48,440] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2737
[2019-04-09 15:14:48,464] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 65.0, 153.8333333333333, 0.0, 19.0, 26.42569072753331, 0.8207207280363874, 0.0, 0.0, 35.0, 16.279966047353774], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1172400.0000, 
sim time next is 1173600.0000, 
raw observation next is [18.3, 65.0, 143.5, 0.0, 19.0, 26.50514518332865, 0.8281837960875782, 0.0, 0.0, 35.0, 14.593175595351958], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.47833333333333333, 0.0, 0.08333333333333333, 0.7087620986107209, 0.7760612653625261, 0.0, 0.0, 0.4, 0.1459317559535196], 
reward next is 0.8541, 
noisyNet noise sample is [array([-0.3777702], dtype=float32), 1.4196508]. 
=============================================
[2019-04-09 15:14:48,503] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.7968343e-09 9.4758019e-07 4.7086109e-05 1.8978061e-09 8.3299321e-01
 7.7756058e-04 1.6597129e-01 9.3659546e-06 1.4316333e-04 5.6079447e-05
 1.3434989e-06], sum to 1.0000
[2019-04-09 15:14:48,504] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7565
[2019-04-09 15:14:48,522] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.4, 96.0, 0.0, 0.0, 19.0, 25.15417947099491, 0.5535442994964413, 0.0, 1.0, 35.0, 15.06167464456859], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1296000.0000, 
sim time next is 1297200.0000, 
raw observation next is [4.200000000000001, 95.0, 0.0, 0.0, 19.0, 24.99063287265117, 0.531128079234208, 0.0, 1.0, 35.0, 18.187938248512616], 
processed observation next is [1.0, 0.0, 0.5789473684210527, 0.95, 0.0, 0.0, 0.08333333333333333, 0.5825527393875974, 0.6770426930780693, 0.0, 1.0, 0.4, 0.18187938248512617], 
reward next is 0.8181, 
noisyNet noise sample is [array([0.24937055], dtype=float32), 0.6249691]. 
=============================================
[2019-04-09 15:14:48,560] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.5251740e-08 4.6498717e-06 3.7199218e-04 1.4011585e-08 8.5217339e-01
 4.3094642e-03 1.4243446e-01 1.5919327e-04 4.2982059e-04 9.6422817e-05
 2.0552345e-05], sum to 1.0000
[2019-04-09 15:14:48,561] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2714
[2019-04-09 15:14:48,588] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.3, 65.0, 153.8333333333333, 0.0, 19.0, 26.73446606815993, 0.8694854026002118, 0.0, 0.0, 35.0, 18.50624009892441], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1172400.0000, 
sim time next is 1173600.0000, 
raw observation next is [18.3, 65.0, 143.5, 0.0, 19.0, 26.77198397594528, 0.8722930378313452, 0.0, 0.0, 35.0, 16.328771091368907], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.47833333333333333, 0.0, 0.08333333333333333, 0.7309986646621066, 0.7907643459437818, 0.0, 0.0, 0.4, 0.16328771091368907], 
reward next is 0.8367, 
noisyNet noise sample is [array([0.15715832], dtype=float32), -0.52990955]. 
=============================================
[2019-04-09 15:14:49,303] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5870182e-08 2.1379396e-06 2.7846202e-04 2.6199155e-08 7.4963993e-01
 2.0699108e-03 2.4677148e-01 7.6101769e-06 1.1139470e-03 7.7459561e-05
 3.9036127e-05], sum to 1.0000
[2019-04-09 15:14:49,304] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9973
[2019-04-09 15:14:49,329] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.7, 92.0, 0.0, 0.0, 19.0, 24.48934459648834, 0.3908502681685729, 0.0, 1.0, 35.0, 19.75761228046437], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1306800.0000, 
sim time next is 1308000.0000, 
raw observation next is [2.533333333333334, 92.0, 0.0, 0.0, 19.0, 24.48139067495743, 0.4015496487697238, 0.0, 1.0, 45.0, 33.647069834587604], 
processed observation next is [1.0, 0.13043478260869565, 0.5327793167128348, 0.92, 0.0, 0.0, 0.08333333333333333, 0.540115889579786, 0.6338498829232413, 0.0, 1.0, 0.6, 0.33647069834587606], 
reward next is 0.6635, 
noisyNet noise sample is [array([-0.6905454], dtype=float32), -0.1194416]. 
=============================================
[2019-04-09 15:14:49,344] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[36.508915]
 [35.96503 ]
 [36.03943 ]
 [36.036583]
 [35.72451 ]], R is [[36.8902626 ]
 [37.32378387]
 [37.73230743]
 [38.10659409]
 [38.36314011]].
[2019-04-09 15:14:49,522] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.40900759e-07 1.18446434e-04 4.26171144e-04 4.74527013e-07
 4.71402556e-01 4.14835960e-02 4.76350307e-01 2.16406421e-04
 8.40598159e-03 1.53853046e-03 5.68022842e-05], sum to 1.0000
[2019-04-09 15:14:49,523] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2944
[2019-04-09 15:14:49,543] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.7, 89.66666666666667, 0.0, 0.0, 19.0, 26.51950075644684, 0.8211236011154796, 0.0, 0.0, 35.0, 19.556881182819723], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1219200.0000, 
sim time next is 1220400.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 19.0, 26.52084061508717, 0.8139374573816961, 0.0, 0.0, 35.0, 17.24798388715524], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.08333333333333333, 0.7100700512572642, 0.7713124857938988, 0.0, 0.0, 0.4, 0.17247983887155238], 
reward next is 0.8275, 
noisyNet noise sample is [array([1.1088837], dtype=float32), -1.9093956]. 
=============================================
[2019-04-09 15:14:49,841] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.3932414e-11 2.4912449e-08 5.1993757e-06 6.3859178e-12 8.9126790e-01
 3.5936185e-04 1.0834901e-01 6.0483131e-07 1.2523143e-05 5.0638523e-06
 3.8691633e-07], sum to 1.0000
[2019-04-09 15:14:49,842] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0250
[2019-04-09 15:14:49,863] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 124.6666666666667, 0.0, 22.5, 26.28225921539022, 0.6497607140782004, 1.0, 1.0, 35.0, 18.45893848525607], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1338000.0000, 
sim time next is 1339200.0000, 
raw observation next is [1.1, 92.0, 120.0, 0.0, 22.5, 26.20988835102556, 0.6486694031974941, 1.0, 1.0, 35.0, 18.873628730670617], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.4, 0.0, 0.375, 0.6841573625854632, 0.7162231343991646, 1.0, 1.0, 0.4, 0.18873628730670616], 
reward next is 0.8113, 
noisyNet noise sample is [array([-0.56180036], dtype=float32), 1.1621435]. 
=============================================
[2019-04-09 15:14:50,175] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.7479349e-10 2.5692460e-08 7.1556831e-05 7.6916418e-11 5.5172163e-01
 1.3995619e-03 4.4360837e-01 3.1047821e-06 3.0280843e-03 1.6598175e-04
 1.7136575e-06], sum to 1.0000
[2019-04-09 15:14:50,182] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8643
[2019-04-09 15:14:50,201] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 24.01636320921086, 0.2296053234967169, 1.0, 1.0, 35.0, 23.610200023171963], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1454400.0000, 
sim time next is 1455600.0000, 
raw observation next is [1.266666666666667, 91.0, 0.0, 0.0, 19.0, 23.97063532155168, 0.2308200869802538, 0.0, 1.0, 45.0, 31.275225920171444], 
processed observation next is [1.0, 0.8695652173913043, 0.4976915974145891, 0.91, 0.0, 0.0, 0.08333333333333333, 0.49755294346263995, 0.5769400289934179, 0.0, 1.0, 0.6, 0.31275225920171446], 
reward next is 0.6872, 
noisyNet noise sample is [array([-0.3794088], dtype=float32), -0.38187063]. 
=============================================
[2019-04-09 15:14:50,277] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9178850e-11 1.3269333e-07 1.4844675e-05 1.9980090e-11 4.9882671e-01
 2.5747970e-03 4.9772260e-01 1.4453987e-06 8.3376258e-04 2.2411541e-05
 3.2706971e-06], sum to 1.0000
[2019-04-09 15:14:50,284] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4252
[2019-04-09 15:14:50,321] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 25.29486434025165, 0.3698583478965148, 1.0, 1.0, 35.0, 26.70905871873877], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1359600.0000, 
sim time next is 1360800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 24.90680632782663, 0.4927899831221602, 1.0, 1.0, 35.0, 23.413814115217274], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.5755671939855524, 0.6642633277073867, 1.0, 1.0, 0.4, 0.23413814115217274], 
reward next is 0.7659, 
noisyNet noise sample is [array([0.49239534], dtype=float32), -0.6453895]. 
=============================================
[2019-04-09 15:14:50,477] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.8717335e-09 1.6166066e-06 5.2088020e-05 9.3969144e-09 4.9948168e-01
 6.7803934e-03 4.9270093e-01 5.5119394e-06 9.1530732e-04 5.9974671e-05
 2.4034041e-06], sum to 1.0000
[2019-04-09 15:14:50,478] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1366
[2019-04-09 15:14:50,503] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.3, 92.0, 0.0, 0.0, 19.0, 25.06072816817297, 0.534176022537793, 0.0, 1.0, 45.0, 32.0719114434885], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1303200.0000, 
sim time next is 1304400.0000, 
raw observation next is [3.1, 92.0, 0.0, 0.0, 19.0, 25.04117046614942, 0.5287658713607042, 0.0, 1.0, 35.0, 24.160264442543355], 
processed observation next is [1.0, 0.08695652173913043, 0.5484764542936289, 0.92, 0.0, 0.0, 0.08333333333333333, 0.5867642055124517, 0.6762552904535681, 0.0, 1.0, 0.4, 0.24160264442543355], 
reward next is 0.7584, 
noisyNet noise sample is [array([-2.4888809], dtype=float32), -0.7490329]. 
=============================================
[2019-04-09 15:14:50,507] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0465604e-08 2.1156957e-06 5.7959955e-05 1.4857456e-08 9.3460107e-01
 1.1992757e-03 6.3721187e-02 1.7828310e-05 2.9163822e-04 8.3767904e-05
 2.5239389e-05], sum to 1.0000
[2019-04-09 15:14:50,508] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0133
[2019-04-09 15:14:50,524] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 100.0, 66.33333333333334, 0.0, 19.0, 26.68436250475431, 0.8916718066464694, 0.0, 0.0, 35.0, 11.45425319352718], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1243200.0000, 
sim time next is 1244400.0000, 
raw observation next is [15.0, 100.0, 74.66666666666667, 0.0, 19.0, 26.6067311000222, 0.879987815686705, 0.0, 0.0, 35.0, 10.267584192441578], 
processed observation next is [0.0, 0.391304347826087, 0.8781163434903049, 1.0, 0.2488888888888889, 0.0, 0.08333333333333333, 0.7172275916685166, 0.7933292718955683, 0.0, 0.0, 0.4, 0.10267584192441578], 
reward next is 0.8973, 
noisyNet noise sample is [array([0.0714055], dtype=float32), -0.98999316]. 
=============================================
[2019-04-09 15:14:50,848] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6172567e-08 1.6468080e-06 5.8253969e-05 2.8042721e-09 7.2589284e-01
 1.6839184e-03 2.7218127e-01 9.0406165e-06 1.3879276e-04 2.6908770e-05
 7.3782012e-06], sum to 1.0000
[2019-04-09 15:14:50,848] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3947
[2019-04-09 15:14:50,879] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 22.5, 23.21813470763102, 0.004799528469602331, 1.0, 1.0, 35.0, 24.747751986126573], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1410000.0000, 
sim time next is 1411200.0000, 
raw observation next is [-0.6, 100.0, 9.0, 0.0, 22.5, 23.18724634390968, -0.009301366853936796, 1.0, 1.0, 35.0, 21.824047227406425], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.03, 0.0, 0.375, 0.4322705286591401, 0.49689954438202105, 1.0, 1.0, 0.4, 0.21824047227406423], 
reward next is 0.7818, 
noisyNet noise sample is [array([-0.20387979], dtype=float32), 0.9805473]. 
=============================================
[2019-04-09 15:14:51,098] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4635292e-10 5.5460430e-07 1.2805671e-04 6.8706341e-10 8.0472517e-01
 1.1757198e-03 1.9347057e-01 2.2143061e-06 4.7903307e-04 1.8051856e-05
 6.4307920e-07], sum to 1.0000
[2019-04-09 15:14:51,100] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7947
[2019-04-09 15:14:51,123] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.8, 100.0, 35.0, 0.0, 19.0, 26.77525289225846, 0.9339545857344586, 0.0, 1.0, 35.0, 11.09702481040129], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1267200.0000, 
sim time next is 1268400.0000, 
raw observation next is [13.26666666666667, 100.0, 24.33333333333333, 0.0, 19.0, 26.66982236522375, 0.9109155970582566, 0.0, 1.0, 35.0, 10.286938581092425], 
processed observation next is [0.0, 0.6956521739130435, 0.8301015697137583, 1.0, 0.08111111111111109, 0.0, 0.08333333333333333, 0.7224851971019793, 0.8036385323527523, 0.0, 1.0, 0.4, 0.10286938581092425], 
reward next is 0.8971, 
noisyNet noise sample is [array([0.49330103], dtype=float32), 1.2657074]. 
=============================================
[2019-04-09 15:14:51,318] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.14668510e-09 2.50242306e-06 1.30005021e-04 2.15287543e-09
 7.52514780e-01 3.69750103e-03 2.43237555e-01 3.63274448e-05
 2.71433761e-04 1.06699204e-04 3.07610208e-06], sum to 1.0000
[2019-04-09 15:14:51,320] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9465
[2019-04-09 15:14:51,340] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.1, 96.0, 0.0, 0.0, 19.0, 25.95049523259652, 0.7641996517512091, 0.0, 1.0, 35.0, 22.694610130590224], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1281600.0000, 
sim time next is 1282800.0000, 
raw observation next is [5.9, 97.33333333333333, 0.0, 0.0, 19.0, 25.90684961911413, 0.7575467114032257, 0.0, 1.0, 45.0, 28.87208597316711], 
processed observation next is [0.0, 0.8695652173913043, 0.626038781163435, 0.9733333333333333, 0.0, 0.0, 0.08333333333333333, 0.6589041349261775, 0.7525155704677419, 0.0, 1.0, 0.6, 0.2887208597316711], 
reward next is 0.7113, 
noisyNet noise sample is [array([-0.6644448], dtype=float32), 0.12842098]. 
=============================================
[2019-04-09 15:14:51,840] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2871534e-10 7.2333549e-08 1.6274491e-05 8.5407487e-11 7.4726927e-01
 2.5427889e-04 2.5234544e-01 2.4328126e-06 6.5191904e-05 4.4867953e-05
 2.1121873e-06], sum to 1.0000
[2019-04-09 15:14:51,841] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4518
[2019-04-09 15:14:51,856] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.266666666666667, 92.0, 0.0, 0.0, 19.0, 23.79547717786255, 0.1134016868081357, 0.0, 1.0, 35.0, 25.405887257358465], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1462800.0000, 
sim time next is 1464000.0000, 
raw observation next is [1.433333333333333, 92.0, 0.0, 0.0, 19.0, 23.55967985631383, 0.07376565013963234, 0.0, 1.0, 35.0, 20.816233562373395], 
processed observation next is [1.0, 0.9565217391304348, 0.502308402585411, 0.92, 0.0, 0.0, 0.08333333333333333, 0.4633066546928193, 0.5245885500465441, 0.0, 1.0, 0.4, 0.20816233562373396], 
reward next is 0.7918, 
noisyNet noise sample is [array([-2.0104885], dtype=float32), -0.46811366]. 
=============================================
[2019-04-09 15:14:51,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[42.856663]
 [43.60393 ]
 [43.292133]
 [44.453873]
 [45.2823  ]], R is [[42.29553223]
 [42.61851883]
 [42.88735199]
 [43.22329712]
 [43.48025513]].
[2019-04-09 15:14:51,962] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0721668e-11 7.4125701e-08 2.7440485e-06 2.1655296e-11 9.1512752e-01
 2.5238388e-04 8.4603779e-02 4.0745351e-07 5.8768592e-06 7.0298238e-06
 1.8506516e-07], sum to 1.0000
[2019-04-09 15:14:51,965] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3731
[2019-04-09 15:14:51,966] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8443898e-10 2.0084639e-07 2.9935018e-05 1.1298471e-09 9.6994466e-01
 7.4373197e-04 2.9112265e-02 7.8628800e-06 1.1519112e-04 4.4328932e-05
 1.7440663e-06], sum to 1.0000
[2019-04-09 15:14:51,967] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4296
[2019-04-09 15:14:51,990] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 76.83333333333334, 0.0, 22.5, 26.20781655281763, 0.6569406714814371, 1.0, 1.0, 35.0, 18.399565422658547], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1347600.0000, 
sim time next is 1348800.0000, 
raw observation next is [1.1, 92.0, 66.5, 0.0, 22.5, 26.19469160806415, 0.6490764005933656, 1.0, 1.0, 35.0, 18.91993625199758], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.22166666666666668, 0.0, 0.375, 0.6828909673386793, 0.7163588001977885, 1.0, 1.0, 0.4, 0.1891993625199758], 
reward next is 0.8108, 
noisyNet noise sample is [array([1.8848791], dtype=float32), -0.20389459]. 
=============================================
[2019-04-09 15:14:52,004] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.200000000000001, 95.0, 0.0, 0.0, 19.0, 25.36171328744367, 0.6289561249341858, 0.0, 1.0, 35.0, 24.613413540812907], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1297200.0000, 
sim time next is 1298400.0000, 
raw observation next is [4.0, 94.0, 0.0, 0.0, 19.0, 25.38591954881236, 0.6123775340647922, 0.0, 1.0, 35.0, 21.907444251926], 
processed observation next is [1.0, 0.0, 0.5734072022160666, 0.94, 0.0, 0.0, 0.08333333333333333, 0.6154932957343634, 0.7041258446882641, 0.0, 1.0, 0.4, 0.21907444251926], 
reward next is 0.7809, 
noisyNet noise sample is [array([-0.8260603], dtype=float32), -1.1998597]. 
=============================================
[2019-04-09 15:14:52,007] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0605946e-11 6.7010490e-08 7.0127880e-06 2.8394448e-11 9.0392172e-01
 1.9265398e-03 9.4069123e-02 8.8739142e-07 6.7292240e-05 7.2443254e-06
 6.7816678e-08], sum to 1.0000
[2019-04-09 15:14:52,008] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8683
[2019-04-09 15:14:52,031] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.33333333333334, 48.5, 0.0, 22.5, 26.39379936963905, 0.7103630027626787, 1.0, 1.0, 35.0, 24.606800287202468], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1351200.0000, 
sim time next is 1352400.0000, 
raw observation next is [1.1, 92.66666666666667, 39.66666666666667, 0.0, 22.5, 26.42283277855881, 0.7056633084233006, 1.0, 1.0, 35.0, 21.94028050540431], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.9266666666666667, 0.13222222222222224, 0.0, 0.375, 0.7019027315465675, 0.7352211028077669, 1.0, 1.0, 0.4, 0.2194028050540431], 
reward next is 0.7806, 
noisyNet noise sample is [array([1.2623233], dtype=float32), -1.5023327]. 
=============================================
[2019-04-09 15:14:52,241] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.7210920e-10 1.1354264e-06 3.5448393e-05 1.8053092e-11 5.4580241e-01
 2.5988400e-03 4.5124736e-01 1.4380154e-06 8.3932311e-05 2.2563689e-04
 3.7784905e-06], sum to 1.0000
[2019-04-09 15:14:52,243] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2204
[2019-04-09 15:14:52,292] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.0, 63.0, 80.0, 682.0, 22.5, 25.27070582751309, 0.5357815651182484, 1.0, 1.0, 45.0, 32.047051282998154], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1519200.0000, 
sim time next is 1520400.0000, 
raw observation next is [10.53333333333333, 59.33333333333334, 76.66666666666666, 669.3333333333333, 22.5, 25.66699313271274, 0.5575381409258129, 1.0, 1.0, 35.0, 19.038942746270344], 
processed observation next is [1.0, 0.6086956521739131, 0.7543859649122806, 0.5933333333333334, 0.25555555555555554, 0.7395948434622467, 0.375, 0.6389160943927283, 0.685846046975271, 1.0, 1.0, 0.4, 0.19038942746270343], 
reward next is 0.8096, 
noisyNet noise sample is [array([-1.2974429], dtype=float32), 0.24045722]. 
=============================================
[2019-04-09 15:14:52,430] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.8969020e-09 4.1129360e-06 3.9620638e-05 1.7198959e-09 6.3644391e-01
 4.0865396e-03 3.5874084e-01 1.1877127e-05 5.8135076e-04 8.9451329e-05
 2.4253438e-06], sum to 1.0000
[2019-04-09 15:14:52,430] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8864
[2019-04-09 15:14:52,453] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 23.55761332390375, 0.151602029157741, 0.0, 1.0, 45.0, 34.337969822837366], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1387200.0000, 
sim time next is 1388400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 23.5883965863362, 0.1430435304410652, 0.0, 1.0, 45.0, 33.62112793156696], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.46569971552801653, 0.5476811768136883, 0.0, 1.0, 0.6, 0.3362112793156696], 
reward next is 0.6638, 
noisyNet noise sample is [array([0.9700041], dtype=float32), -0.68672633]. 
=============================================
[2019-04-09 15:14:52,820] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5500818e-08 7.6274500e-06 8.1090600e-04 7.4145285e-09 3.4460467e-01
 2.6865622e-02 6.2536955e-01 2.5023857e-05 2.2081276e-03 1.0360207e-04
 4.8134730e-06], sum to 1.0000
[2019-04-09 15:14:52,821] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4557
[2019-04-09 15:14:52,855] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 19.0, 23.08694282793433, -0.02110674320768498, 0.0, 1.0, 35.0, 25.25844443981802], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1472400.0000, 
sim time next is 1473600.0000, 
raw observation next is [1.8, 92.0, 0.0, 0.0, 19.0, 23.24611736856623, -0.0002450429513040225, 0.0, 1.0, 45.0, 30.984940404926675], 
processed observation next is [1.0, 0.043478260869565216, 0.5124653739612189, 0.92, 0.0, 0.0, 0.08333333333333333, 0.4371764473805193, 0.49991831901623196, 0.0, 1.0, 0.6, 0.30984940404926675], 
reward next is 0.6902, 
noisyNet noise sample is [array([-0.25961292], dtype=float32), -0.11763331]. 
=============================================
[2019-04-09 15:14:52,919] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1049352e-11 2.4256875e-07 2.3340838e-06 1.6518323e-11 9.6727431e-01
 1.8990161e-04 3.2510590e-02 2.8281860e-07 1.8643264e-05 3.3496633e-06
 4.2864076e-07], sum to 1.0000
[2019-04-09 15:14:52,920] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6380
[2019-04-09 15:14:52,940] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 54.5, 0.0, 22.5, 25.61130105755116, 0.5115971611285811, 1.0, 1.0, 35.0, 20.195491073595058], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1330800.0000, 
sim time next is 1332000.0000, 
raw observation next is [0.5, 92.0, 73.5, 0.0, 22.5, 25.64667850491846, 0.5138234482502254, 1.0, 1.0, 35.0, 18.403823100700183], 
processed observation next is [1.0, 0.43478260869565216, 0.4764542936288089, 0.92, 0.245, 0.0, 0.375, 0.6372232087432049, 0.6712744827500751, 1.0, 1.0, 0.4, 0.18403823100700184], 
reward next is 0.8160, 
noisyNet noise sample is [array([0.17984244], dtype=float32), -1.4117218]. 
=============================================
[2019-04-09 15:14:52,952] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[46.046947]
 [45.503677]
 [44.43388 ]
 [44.16568 ]
 [43.481133]], R is [[46.43051147]
 [46.76425171]
 [47.07337952]
 [47.35072708]
 [47.54323959]].
[2019-04-09 15:14:53,236] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8113820e-08 9.9472509e-06 7.2205825e-05 3.6073668e-09 6.2940919e-01
 4.7711055e-03 3.6497495e-01 1.0779818e-05 4.2629501e-04 3.2314175e-04
 2.4021713e-06], sum to 1.0000
[2019-04-09 15:14:53,239] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0349
[2019-04-09 15:14:53,257] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 23.82711192455432, 0.1830669819330487, 0.0, 1.0, 35.0, 22.624782340863565], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1389600.0000, 
sim time next is 1390800.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 23.70544871083195, 0.152996550049129, 0.0, 1.0, 35.0, 20.522925591307125], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.4754540592359957, 0.5509988500163764, 0.0, 1.0, 0.4, 0.20522925591307126], 
reward next is 0.7948, 
noisyNet noise sample is [array([1.0888747], dtype=float32), 0.3350663]. 
=============================================
[2019-04-09 15:14:53,422] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3412860e-11 1.1214141e-07 2.2811511e-05 2.6766037e-10 3.1067446e-01
 1.3088080e-03 6.8740433e-01 1.6492022e-07 5.6997215e-04 1.9233892e-05
 1.2412839e-07], sum to 1.0000
[2019-04-09 15:14:53,422] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8744
[2019-04-09 15:14:53,444] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.433333333333333, 90.0, 0.0, 0.0, 19.0, 23.81735583797334, 0.1773141390631271, 0.0, 1.0, 45.0, 30.95310181961284], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1456800.0000, 
sim time next is 1458000.0000, 
raw observation next is [1.6, 89.0, 0.0, 0.0, 19.0, 23.81944664514948, 0.1829753522696099, 0.0, 1.0, 45.0, 31.411606348466925], 
processed observation next is [1.0, 0.9130434782608695, 0.5069252077562327, 0.89, 0.0, 0.0, 0.08333333333333333, 0.48495388709578996, 0.5609917840898699, 0.0, 1.0, 0.6, 0.31411606348466925], 
reward next is 0.6859, 
noisyNet noise sample is [array([-0.33024848], dtype=float32), 0.7602798]. 
=============================================
[2019-04-09 15:14:53,456] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[47.244576]
 [47.64691 ]
 [48.59502 ]
 [48.33573 ]
 [48.439266]], R is [[47.63519287]
 [47.84930801]
 [48.1269455 ]
 [48.32733917]
 [48.53232956]].
[2019-04-09 15:14:53,487] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.2283880e-12 6.3369354e-09 3.6209860e-06 1.8742818e-12 8.0219686e-01
 2.7889233e-05 1.9771923e-01 1.0394318e-07 5.0384078e-05 1.7806900e-06
 3.8985011e-08], sum to 1.0000
[2019-04-09 15:14:53,489] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9214
[2019-04-09 15:14:53,508] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.33333333333334, 48.5, 0.0, 22.5, 25.79108417435195, 0.5682330186025261, 1.0, 1.0, 35.0, 19.698263479640147], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1351200.0000, 
sim time next is 1352400.0000, 
raw observation next is [1.1, 92.66666666666667, 39.66666666666667, 0.0, 22.5, 25.8307696387334, 0.6091770276369178, 1.0, 1.0, 45.0, 35.8542490958646], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.9266666666666667, 0.13222222222222224, 0.0, 0.375, 0.6525641365611167, 0.703059009212306, 1.0, 1.0, 0.6, 0.35854249095864604], 
reward next is 0.6415, 
noisyNet noise sample is [array([-0.14031161], dtype=float32), 0.32700124]. 
=============================================
[2019-04-09 15:14:53,702] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8196715e-08 4.8761021e-06 3.1835931e-05 1.7766266e-09 4.6589956e-01
 6.2111700e-03 5.2696228e-01 2.5317077e-05 3.5309722e-04 5.0495082e-04
 6.8274894e-06], sum to 1.0000
[2019-04-09 15:14:53,703] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0074
[2019-04-09 15:14:53,726] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 23.2316331666905, 0.1019963055084053, 0.0, 1.0, 45.0, 36.732747834209846], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1388400.0000, 
sim time next is 1389600.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 23.2409414557269, 0.09590058613568701, 0.0, 1.0, 35.0, 25.30744371989629], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.4367451213105751, 0.531966862045229, 0.0, 1.0, 0.4, 0.2530744371989629], 
reward next is 0.7469, 
noisyNet noise sample is [array([-1.8333248], dtype=float32), -0.37407273]. 
=============================================
[2019-04-09 15:14:53,740] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5879262e-10 1.8442455e-07 3.2301989e-05 1.1972572e-10 8.4100568e-01
 1.2837233e-03 1.5754434e-01 2.3906778e-06 4.4183766e-05 8.3630272e-05
 3.5485709e-06], sum to 1.0000
[2019-04-09 15:14:53,746] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1162
[2019-04-09 15:14:53,773] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 93.0, 0.0, 22.5, 24.20369368534145, 0.188792235184398, 1.0, 1.0, 35.0, 18.00345613605183], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1429200.0000, 
sim time next is 1430400.0000, 
raw observation next is [0.7000000000000001, 92.0, 91.0, 0.0, 22.5, 24.27960162860296, 0.202143273374783, 1.0, 1.0, 35.0, 15.791649778509282], 
processed observation next is [1.0, 0.5652173913043478, 0.4819944598337951, 0.92, 0.30333333333333334, 0.0, 0.375, 0.5233001357169135, 0.5673810911249276, 1.0, 1.0, 0.4, 0.1579164977850928], 
reward next is 0.8421, 
noisyNet noise sample is [array([-0.3490744], dtype=float32), -0.47826213]. 
=============================================
[2019-04-09 15:14:54,646] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.1083476e-08 5.8185187e-06 6.4889318e-05 1.3994013e-08 5.8233577e-01
 6.4368574e-03 4.1048941e-01 6.2104073e-06 3.3597086e-04 3.1873141e-04
 6.3430757e-06], sum to 1.0000
[2019-04-09 15:14:54,646] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2496
[2019-04-09 15:14:54,666] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.2, 93.33333333333334, 0.0, 0.0, 19.0, 23.03592394580863, -0.06289973857752122, 0.0, 1.0, 35.0, 23.143010559600665], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1478400.0000, 
sim time next is 1479600.0000, 
raw observation next is [2.2, 94.0, 0.0, 0.0, 19.0, 22.98866714900423, -0.08830907098717451, 0.0, 1.0, 35.0, 20.442106588974646], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.94, 0.0, 0.0, 0.08333333333333333, 0.4157222624170191, 0.47056364300427517, 0.0, 1.0, 0.4, 0.20442106588974646], 
reward next is 0.7956, 
noisyNet noise sample is [array([-1.0658007], dtype=float32), 0.09107577]. 
=============================================
[2019-04-09 15:14:55,224] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9164079e-10 1.0079553e-07 1.4393695e-05 4.8235277e-10 3.1034249e-01
 8.4930338e-04 6.8843168e-01 2.4435874e-06 3.0787816e-04 4.8019450e-05
 3.7553675e-06], sum to 1.0000
[2019-04-09 15:14:55,224] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7100
[2019-04-09 15:14:55,255] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.6514085e-09 4.1118710e-07 3.8874878e-05 3.1513430e-09 5.4910696e-01
 5.0508804e-03 4.4360957e-01 6.8092333e-05 1.4770861e-03 6.1589410e-04
 3.2221491e-05], sum to 1.0000
[2019-04-09 15:14:55,255] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1501
[2019-04-09 15:14:55,265] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.8, 92.0, 0.0, 0.0, 19.0, 23.63962565412927, 0.07526779407244566, 0.0, 1.0, 45.0, 29.97644415572892], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1473600.0000, 
sim time next is 1474800.0000, 
raw observation next is [2.0, 92.0, 0.0, 0.0, 19.0, 23.62111002984942, 0.06825271704524181, 0.0, 1.0, 45.0, 29.82692963242739], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.92, 0.0, 0.0, 0.08333333333333333, 0.4684258358207849, 0.5227509056817473, 0.0, 1.0, 0.6, 0.2982692963242739], 
reward next is 0.7017, 
noisyNet noise sample is [array([2.0664763], dtype=float32), 0.9649856]. 
=============================================
[2019-04-09 15:14:55,275] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6868247e-09 1.1988148e-07 6.1797597e-05 2.0183931e-11 6.1116099e-01
 2.5062073e-02 3.6349246e-01 2.5931283e-06 1.4263588e-04 7.6048600e-05
 1.2803416e-06], sum to 1.0000
[2019-04-09 15:14:55,275] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4065
[2019-04-09 15:14:55,281] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 23.65254188890746, 0.1234929604648374, 0.0, 1.0, 45.0, 32.37643795094937], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1398000.0000, 
sim time next is 1399200.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 23.6508301683184, 0.1121408617427652, 0.0, 1.0, 35.0, 25.090710269551018], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.4709025140265333, 0.5373802872475885, 0.0, 1.0, 0.4, 0.2509071026955102], 
reward next is 0.7491, 
noisyNet noise sample is [array([1.5508537], dtype=float32), -1.1188993]. 
=============================================
[2019-04-09 15:14:55,309] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 24.14643461154887, 0.2180582736723883, 0.0, 1.0, 35.0, 20.715569819568625], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1453200.0000, 
sim time next is 1454400.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 23.96624079618369, 0.1889203857828138, 1.0, 1.0, 35.0, 18.986932039174025], 
processed observation next is [1.0, 0.8695652173913043, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.4971867330153075, 0.5629734619276047, 1.0, 1.0, 0.4, 0.18986932039174026], 
reward next is 0.8101, 
noisyNet noise sample is [array([0.7464576], dtype=float32), -0.8106111]. 
=============================================
[2019-04-09 15:14:55,342] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3820312e-11 2.2567296e-07 4.5485158e-06 4.8500749e-11 7.8828108e-01
 1.3732193e-03 2.1008503e-01 1.5719580e-06 2.1477269e-04 3.5134217e-05
 4.5064999e-06], sum to 1.0000
[2019-04-09 15:14:55,342] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4287
[2019-04-09 15:14:55,369] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 24.22554421898032, 0.2276091901080671, 0.0, 1.0, 35.0, 19.24667694219363], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1453200.0000, 
sim time next is 1454400.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 24.00951402769215, 0.1942434067593392, 1.0, 1.0, 35.0, 17.746539897478463], 
processed observation next is [1.0, 0.8695652173913043, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.5007928356410124, 0.564747802253113, 1.0, 1.0, 0.4, 0.17746539897478464], 
reward next is 0.8225, 
noisyNet noise sample is [array([-0.5960954], dtype=float32), -0.45476457]. 
=============================================
[2019-04-09 15:14:55,665] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2450070e-11 1.6890541e-08 1.0534142e-05 1.4777543e-10 7.8256428e-01
 7.3814485e-04 2.1662886e-01 9.0066993e-07 2.2475217e-05 3.4435434e-05
 3.4736044e-07], sum to 1.0000
[2019-04-09 15:14:55,665] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2668
[2019-04-09 15:14:55,686] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 19.0, 23.43785054798002, 0.06053095104671515, 0.0, 1.0, 35.0, 23.26986990852042], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1467600.0000, 
sim time next is 1468800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 19.0, 23.3607128164428, 0.03489424431293692, 0.0, 1.0, 35.0, 20.503705438514523], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.08333333333333333, 0.4467260680368999, 0.511631414770979, 0.0, 1.0, 0.4, 0.20503705438514522], 
reward next is 0.7950, 
noisyNet noise sample is [array([0.32641715], dtype=float32), 1.0067344]. 
=============================================
[2019-04-09 15:14:55,692] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5805815e-08 1.7154538e-07 2.3061357e-05 1.2774737e-09 5.1920438e-01
 2.2098846e-03 4.7814909e-01 7.0648794e-06 3.8096050e-04 2.0719042e-05
 4.6856999e-06], sum to 1.0000
[2019-04-09 15:14:55,699] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6492
[2019-04-09 15:14:55,722] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 32.0, 0.0, 22.5, 24.29497340935297, 0.194558871405068, 1.0, 1.0, 35.0, 24.265284905013765], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1414800.0000, 
sim time next is 1416000.0000, 
raw observation next is [-0.4, 98.33333333333334, 41.33333333333334, 0.0, 22.5, 24.43068700907484, 0.2192898116345342, 1.0, 1.0, 45.0, 31.215981650517897], 
processed observation next is [1.0, 0.391304347826087, 0.45152354570637127, 0.9833333333333334, 0.1377777777777778, 0.0, 0.375, 0.5358905840895701, 0.5730966038781781, 1.0, 1.0, 0.6, 0.312159816505179], 
reward next is 0.6878, 
noisyNet noise sample is [array([1.4386404], dtype=float32), 0.13376227]. 
=============================================
[2019-04-09 15:14:55,729] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[44.123978]
 [43.799488]
 [42.874252]
 [41.74722 ]
 [41.26173 ]], R is [[45.65654755]
 [45.9573288 ]
 [46.16571808]
 [46.43796921]
 [46.63199234]].
[2019-04-09 15:14:55,801] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.6747848e-11 1.7902201e-08 6.3952639e-06 1.1466287e-11 6.4072210e-01
 9.9278241e-04 3.5819453e-01 6.2967359e-07 7.7725919e-05 5.7317047e-06
 9.0319269e-08], sum to 1.0000
[2019-04-09 15:14:55,805] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7158
[2019-04-09 15:14:55,834] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.266666666666666, 71.33333333333333, 0.0, 0.0, 22.5, 26.84804806558276, 0.8919399279605843, 1.0, 1.0, 45.0, 26.810369686031095], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1626000.0000, 
sim time next is 1627200.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 22.5, 26.80865033408346, 0.8829313384885927, 1.0, 1.0, 45.0, 26.952231201581515], 
processed observation next is [1.0, 0.8695652173913043, 0.6759002770083103, 0.74, 0.0, 0.0, 0.375, 0.734054194506955, 0.7943104461628643, 1.0, 1.0, 0.6, 0.26952231201581517], 
reward next is 0.7305, 
noisyNet noise sample is [array([0.5406936], dtype=float32), -0.71367073]. 
=============================================
[2019-04-09 15:14:55,839] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5749742e-10 1.6364004e-07 1.6088026e-05 5.0686061e-10 8.4484613e-01
 2.3888489e-03 1.5256533e-01 2.4382957e-06 1.4440612e-04 3.5809484e-05
 9.0423418e-07], sum to 1.0000
[2019-04-09 15:14:55,839] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2933
[2019-04-09 15:14:55,868] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.2, 96.66666666666666, 50.33333333333333, 0.0, 22.5, 24.00731409795709, 0.1469991874584222, 1.0, 1.0, 45.0, 35.78115616731305], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1417200.0000, 
sim time next is 1418400.0000, 
raw observation next is [0.0, 95.0, 59.0, 0.0, 22.5, 24.30225607122721, 0.1887692599889594, 1.0, 1.0, 35.0, 24.08454257401824], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.19666666666666666, 0.0, 0.375, 0.5251880059356008, 0.5629230866629865, 1.0, 1.0, 0.4, 0.2408454257401824], 
reward next is 0.7592, 
noisyNet noise sample is [array([-2.4772358], dtype=float32), 1.8066449]. 
=============================================
[2019-04-09 15:14:56,322] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4308619e-12 8.2918755e-10 1.6349582e-06 2.3329786e-12 7.3634267e-01
 3.0105514e-04 2.6332942e-01 2.2379633e-07 2.4040914e-05 7.0170620e-07
 2.4129019e-07], sum to 1.0000
[2019-04-09 15:14:56,322] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0609
[2019-04-09 15:14:56,342] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.63333333333333, 52.66666666666667, 85.33333333333333, 103.0, 22.5, 26.50113623325257, 0.6848274032768774, 1.0, 1.0, 45.0, 23.72673317955185], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1527600.0000, 
sim time next is 1528800.0000, 
raw observation next is [11.06666666666667, 55.33333333333333, 72.83333333333333, 24.33333333333334, 22.5, 26.68653103291666, 0.7109061745590707, 1.0, 1.0, 35.0, 17.377762099425368], 
processed observation next is [1.0, 0.6956521739130435, 0.7691597414589106, 0.5533333333333332, 0.24277777777777776, 0.026887661141804794, 0.375, 0.7238775860763882, 0.7369687248530236, 1.0, 1.0, 0.4, 0.17377762099425367], 
reward next is 0.8262, 
noisyNet noise sample is [array([-1.5134454], dtype=float32), -0.5421359]. 
=============================================
[2019-04-09 15:14:56,611] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2193519e-11 4.3785835e-08 1.7502338e-06 3.2430017e-11 8.9983642e-01
 1.1495497e-03 9.8978043e-02 9.3757086e-07 2.3975881e-05 8.9220339e-06
 3.3602015e-07], sum to 1.0000
[2019-04-09 15:14:56,615] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5378
[2019-04-09 15:14:56,651] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 92.0, 32.0, 0.0, 22.5, 25.09170441671264, 0.324320652965948, 1.0, 1.0, 35.0, 18.889116145062836], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1440000.0000, 
sim time next is 1441200.0000, 
raw observation next is [1.1, 92.0, 22.66666666666666, 0.0, 22.5, 25.09163327493999, 0.2100181651360788, 1.0, 1.0, 35.0, 18.049766225822015], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.07555555555555554, 0.0, 0.375, 0.5909694395783326, 0.5700060550453596, 1.0, 1.0, 0.4, 0.18049766225822014], 
reward next is 0.8195, 
noisyNet noise sample is [array([-1.2775427], dtype=float32), 0.21645987]. 
=============================================
[2019-04-09 15:14:56,667] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4231229e-11 4.7420997e-08 1.8722102e-06 3.7033807e-11 8.9839154e-01
 1.2382818e-03 1.0033165e-01 1.0183458e-06 2.5744403e-05 9.5458363e-06
 3.7414219e-07], sum to 1.0000
[2019-04-09 15:14:56,667] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9761
[2019-04-09 15:14:56,727] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 22.66666666666666, 0.0, 22.5, 25.09163327493999, 0.2100181651360788, 1.0, 1.0, 35.0, 18.049766225822015], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1441200.0000, 
sim time next is 1442400.0000, 
raw observation next is [1.1, 92.0, 15.0, 0.0, 22.5, 23.79707276044149, 0.2136362269586518, 1.0, 1.0, 45.0, 36.80403862873742], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.05, 0.0, 0.375, 0.4830893967034576, 0.5712120756528839, 1.0, 1.0, 0.6, 0.36804038628737423], 
reward next is 0.6320, 
noisyNet noise sample is [array([-1.2775427], dtype=float32), 0.21645987]. 
=============================================
[2019-04-09 15:14:56,784] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5952556e-09 1.9077039e-07 8.5828669e-06 2.6259400e-10 9.7491169e-01
 4.6724806e-04 2.4485679e-02 1.9683446e-06 1.0965240e-04 1.2278665e-05
 2.7495612e-06], sum to 1.0000
[2019-04-09 15:14:56,787] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0618
[2019-04-09 15:14:56,809] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 100.0, 32.5, 0.0, 22.5, 23.44516852835828, -0.02408455912333077, 1.0, 1.0, 35.0, 19.93646371602256], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1501200.0000, 
sim time next is 1502400.0000, 
raw observation next is [1.8, 100.0, 42.16666666666667, 0.0, 22.5, 23.53243847071142, 0.009775381030739572, 1.0, 1.0, 35.0, 17.98337436654234], 
processed observation next is [1.0, 0.391304347826087, 0.5124653739612189, 1.0, 0.14055555555555557, 0.0, 0.375, 0.4610365392259516, 0.5032584603435798, 1.0, 1.0, 0.4, 0.1798337436654234], 
reward next is 0.8202, 
noisyNet noise sample is [array([1.5145538], dtype=float32), -0.15351859]. 
=============================================
[2019-04-09 15:14:57,826] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0628018e-11 5.3410570e-08 2.7685771e-06 3.0226655e-12 6.3729650e-01
 1.3820785e-04 3.6253998e-01 2.0068688e-07 2.0642881e-05 1.6712994e-06
 5.3660230e-08], sum to 1.0000
[2019-04-09 15:14:57,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6681
[2019-04-09 15:14:57,848] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.8, 60.33333333333334, 0.0, 0.0, 22.5, 25.96672044670686, 0.6375112072577416, 1.0, 1.0, 45.0, 27.139713267691192], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1534800.0000, 
sim time next is 1536000.0000, 
raw observation next is [9.600000000000001, 60.66666666666667, 0.0, 0.0, 22.5, 25.99703749892914, 0.6627685565343236, 1.0, 1.0, 45.0, 26.50831809623207], 
processed observation next is [1.0, 0.782608695652174, 0.7285318559556788, 0.6066666666666667, 0.0, 0.0, 0.375, 0.6664197915774283, 0.7209228521781079, 1.0, 1.0, 0.6, 0.2650831809623207], 
reward next is 0.7349, 
noisyNet noise sample is [array([-0.8245637], dtype=float32), 0.34199804]. 
=============================================
[2019-04-09 15:14:57,858] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[50.304443]
 [50.051643]
 [50.245827]
 [50.613564]
 [50.770645]], R is [[50.46811295]
 [50.69203568]
 [51.07091904]
 [51.44189072]
 [51.81834793]].
[2019-04-09 15:14:58,069] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.73814971e-10 2.86921022e-07 2.92829336e-05 1.29096844e-09
 9.17082429e-01 1.43779733e-03 8.08850527e-02 1.23749805e-05
 2.43241331e-04 3.06751288e-04 2.83995223e-06], sum to 1.0000
[2019-04-09 15:14:58,069] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4960
[2019-04-09 15:14:58,084] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 25.11944845013825, 0.4474122625347516, 0.0, 1.0, 35.0, 15.257516111129942], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1657200.0000, 
sim time next is 1658400.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 25.05446485583143, 0.4193049025302549, 0.0, 1.0, 35.0, 17.48064258463156], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.5878720713192859, 0.6397683008434183, 0.0, 1.0, 0.4, 0.17480642584631562], 
reward next is 0.8252, 
noisyNet noise sample is [array([0.10447486], dtype=float32), 1.1432608]. 
=============================================
[2019-04-09 15:14:58,525] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.2803969e-10 3.4362395e-07 1.6268204e-05 4.8413035e-10 5.5564541e-01
 3.7358985e-03 4.4031635e-01 5.8559255e-07 1.9820272e-04 8.1974067e-05
 5.0642789e-06], sum to 1.0000
[2019-04-09 15:14:58,525] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2982
[2019-04-09 15:14:58,544] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 100.0, 9.0, 0.0, 22.5, 22.56838264977637, -0.1789914500709444, 1.0, 1.0, 35.0, 21.317888537243462], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1497600.0000, 
sim time next is 1498800.0000, 
raw observation next is [1.266666666666667, 100.0, 15.0, 0.0, 22.5, 22.58245904910671, -0.1461347917990618, 1.0, 1.0, 35.0, 19.74689391496525], 
processed observation next is [1.0, 0.34782608695652173, 0.4976915974145891, 1.0, 0.05, 0.0, 0.375, 0.38187158742555916, 0.45128840273364607, 1.0, 1.0, 0.4, 0.1974689391496525], 
reward next is 0.8025, 
noisyNet noise sample is [array([0.37455073], dtype=float32), 0.039387636]. 
=============================================
[2019-04-09 15:14:58,712] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8571977e-10 2.1108477e-07 1.0453183e-05 7.4571547e-12 5.5731362e-01
 1.5193425e-04 4.4245929e-01 6.9962266e-07 5.3885735e-05 9.4684892e-06
 3.8062100e-07], sum to 1.0000
[2019-04-09 15:14:58,714] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0464
[2019-04-09 15:14:58,758] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.4, 93.0, 94.0, 704.0, 22.5, 24.84048424474269, 0.3126494443500789, 1.0, 1.0, 35.0, 16.769621860375857], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1512000.0000, 
sim time next is 1513200.0000, 
raw observation next is [5.333333333333334, 86.33333333333334, 98.0, 701.3333333333334, 22.5, 24.85500664133911, 0.3223655733791866, 1.0, 1.0, 35.0, 14.72274611816232], 
processed observation next is [1.0, 0.5217391304347826, 0.6103416435826409, 0.8633333333333334, 0.32666666666666666, 0.7749539594843463, 0.375, 0.5712505534449258, 0.6074551911263956, 1.0, 1.0, 0.4, 0.1472274611816232], 
reward next is 0.8528, 
noisyNet noise sample is [array([0.17890547], dtype=float32), -1.1088097]. 
=============================================
[2019-04-09 15:14:59,500] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.3501840e-10 9.0355201e-07 3.2799722e-05 6.6072714e-10 7.1383941e-01
 8.8345207e-04 2.8488067e-01 3.7261839e-06 3.4743981e-04 9.5827500e-06
 2.1227411e-06], sum to 1.0000
[2019-04-09 15:14:59,503] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3447
[2019-04-09 15:14:59,525] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 80.0, 0.0, 0.0, 19.0, 23.89551209954629, 0.1562785686704047, 0.0, 1.0, 35.0, 18.858883928790874], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1561200.0000, 
sim time next is 1562400.0000, 
raw observation next is [5.0, 79.0, 0.0, 0.0, 19.0, 23.76147319151904, 0.1339096594961049, 0.0, 1.0, 35.0, 18.963392452112085], 
processed observation next is [1.0, 0.08695652173913043, 0.6011080332409973, 0.79, 0.0, 0.0, 0.08333333333333333, 0.4801227659599201, 0.5446365531653683, 0.0, 1.0, 0.4, 0.18963392452112085], 
reward next is 0.8104, 
noisyNet noise sample is [array([-0.33608487], dtype=float32), 0.82601947]. 
=============================================
[2019-04-09 15:14:59,653] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.12252325e-11 1.36594540e-08 2.51475785e-06 3.07233340e-12
 8.65780592e-01 8.14406667e-05 1.34127662e-01 3.26091026e-07
 2.94077290e-06 4.43328418e-06 1.31199485e-07], sum to 1.0000
[2019-04-09 15:14:59,658] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6866
[2019-04-09 15:14:59,673] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.6383314e-10 2.2047460e-07 1.8772065e-05 1.7722636e-10 7.8945899e-01
 3.6426398e-04 2.0982327e-01 2.6168041e-06 3.0843637e-04 2.2356569e-05
 1.0287131e-06], sum to 1.0000
[2019-04-09 15:14:59,673] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2066
[2019-04-09 15:14:59,690] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [12.93333333333334, 52.00000000000001, 54.66666666666667, 30.83333333333334, 22.5, 26.92024305720341, 0.8223131624414356, 1.0, 1.0, 35.0, 16.423786546062928], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1614000.0000, 
sim time next is 1615200.0000, 
raw observation next is [12.56666666666667, 53.0, 41.83333333333334, 30.83333333333334, 22.5, 27.32308015562928, 0.8628385570672221, 1.0, 1.0, 35.0, 13.936654457835315], 
processed observation next is [1.0, 0.6956521739130435, 0.8107109879963068, 0.53, 0.13944444444444448, 0.03406998158379374, 0.375, 0.7769233463024401, 0.7876128523557407, 1.0, 1.0, 0.4, 0.13936654457835315], 
reward next is 0.8606, 
noisyNet noise sample is [array([-1.5349559], dtype=float32), -0.75340396]. 
=============================================
[2019-04-09 15:14:59,700] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 25.47162328636861, 0.5008410574827847, 0.0, 1.0, 35.0, 15.330121287075183], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1658400.0000, 
sim time next is 1659600.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 25.30990878672014, 0.4792955678242223, 0.0, 1.0, 35.0, 17.89138442821793], 
processed observation next is [1.0, 0.21739130434782608, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.6091590655600116, 0.6597651892747408, 0.0, 1.0, 0.4, 0.1789138442821793], 
reward next is 0.8211, 
noisyNet noise sample is [array([1.279862], dtype=float32), 0.92907673]. 
=============================================
[2019-04-09 15:14:59,754] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.9428296e-09 2.6014166e-06 3.2798089e-05 5.7563527e-09 1.6650060e-01
 3.0663756e-03 8.2820386e-01 1.8680179e-05 1.8417705e-03 3.1710288e-04
 1.6176142e-05], sum to 1.0000
[2019-04-09 15:14:59,754] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2011
[2019-04-09 15:14:59,776] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.633333333333334, 84.66666666666667, 0.0, 0.0, 19.0, 24.01775768530218, 0.1704461539086338, 0.0, 1.0, 35.0, 24.019963245241385], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1570800.0000, 
sim time next is 1572000.0000, 
raw observation next is [4.666666666666667, 84.33333333333334, 0.0, 0.0, 19.0, 24.00218353783341, 0.1631194729397835, 0.0, 1.0, 45.0, 30.800162666116393], 
processed observation next is [1.0, 0.17391304347826086, 0.5918744228993538, 0.8433333333333334, 0.0, 0.0, 0.08333333333333333, 0.5001819614861175, 0.5543731576465946, 0.0, 1.0, 0.6, 0.30800162666116393], 
reward next is 0.6920, 
noisyNet noise sample is [array([1.0276415], dtype=float32), 0.5681415]. 
=============================================
[2019-04-09 15:14:59,789] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[39.27712 ]
 [39.11191 ]
 [38.643166]
 [38.64566 ]
 [38.264297]], R is [[39.82955551]
 [40.19105911]
 [40.47533417]
 [40.83242416]
 [41.0859108 ]].
[2019-04-09 15:15:00,050] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0135328e-10 1.0760344e-07 4.8877296e-06 3.6089656e-10 7.1239227e-01
 5.1107402e-03 2.8207558e-01 2.8725699e-06 3.6447859e-04 4.6346606e-05
 2.7638068e-06], sum to 1.0000
[2019-04-09 15:15:00,051] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9462
[2019-04-09 15:15:00,076] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.533333333333333, 80.0, 31.0, 30.0, 22.5, 24.44721192080134, 0.285278254018703, 1.0, 1.0, 35.0, 18.91112124560927], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1585200.0000, 
sim time next is 1586400.0000, 
raw observation next is [6.066666666666666, 78.0, 50.0, 51.83333333333333, 22.5, 24.89557681256681, 0.3647959075152889, 1.0, 1.0, 45.0, 31.68474135947079], 
processed observation next is [1.0, 0.34782608695652173, 0.6306555863342568, 0.78, 0.16666666666666666, 0.057274401473296495, 0.375, 0.5746314010472341, 0.6215986358384297, 1.0, 1.0, 0.6, 0.3168474135947079], 
reward next is 0.6832, 
noisyNet noise sample is [array([0.5515261], dtype=float32), -0.35157633]. 
=============================================
[2019-04-09 15:15:00,182] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.4713368e-11 3.4467519e-08 2.1854539e-05 1.3672005e-11 3.2747641e-01
 5.0641393e-04 6.7187810e-01 8.7819404e-07 7.8378842e-05 3.7725404e-05
 2.7609630e-07], sum to 1.0000
[2019-04-09 15:15:00,183] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6957
[2019-04-09 15:15:00,199] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6011864e-11 1.1999472e-07 3.7441857e-06 1.3908537e-11 4.4571385e-01
 4.9935980e-04 5.5363280e-01 5.0611726e-07 1.1925849e-04 3.0171332e-05
 1.3861053e-07], sum to 1.0000
[2019-04-09 15:15:00,202] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7030
[2019-04-09 15:15:00,209] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 92.0, 59.5, 0.0, 22.5, 25.83590478359331, 0.5613106763003454, 1.0, 1.0, 45.0, 35.472990353200224], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1677600.0000, 
sim time next is 1678800.0000, 
raw observation next is [1.366666666666667, 92.0, 63.83333333333333, 0.0, 22.5, 25.97120241471429, 0.5919180267300244, 1.0, 1.0, 45.0, 36.448591928456395], 
processed observation next is [1.0, 0.43478260869565216, 0.5004616805170823, 0.92, 0.21277777777777776, 0.0, 0.375, 0.6642668678928576, 0.697306008910008, 1.0, 1.0, 0.6, 0.364485919284564], 
reward next is 0.6355, 
noisyNet noise sample is [array([0.6804896], dtype=float32), -1.0799396]. 
=============================================
[2019-04-09 15:15:00,225] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 49.0, 171.5, 20.66666666666666, 22.5, 26.83020460545317, 0.8004895557347297, 1.0, 1.0, 45.0, 26.218736423369105], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1603200.0000, 
sim time next is 1604400.0000, 
raw observation next is [13.8, 49.0, 170.8333333333333, 0.0, 22.5, 27.09455569510455, 0.8698846010352895, 1.0, 1.0, 45.0, 25.44219710247016], 
processed observation next is [1.0, 0.5652173913043478, 0.844875346260388, 0.49, 0.5694444444444443, 0.0, 0.375, 0.7578796412587124, 0.7899615336784298, 1.0, 1.0, 0.6, 0.2544219710247016], 
reward next is 0.7456, 
noisyNet noise sample is [array([0.48978963], dtype=float32), -0.5262425]. 
=============================================
[2019-04-09 15:15:00,355] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.1722364e-11 2.9646006e-09 3.0824840e-06 2.4029884e-12 6.5456927e-01
 2.8066779e-04 3.4512937e-01 2.4006718e-07 1.6286727e-05 8.5290776e-07
 2.3432182e-07], sum to 1.0000
[2019-04-09 15:15:00,355] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4849
[2019-04-09 15:15:00,382] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.13333333333333, 59.66666666666667, 213.3333333333333, 222.1666666666667, 22.5, 26.59757629713953, 0.7373129145379539, 1.0, 1.0, 45.0, 26.808003890443864], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1596000.0000, 
sim time next is 1597200.0000, 
raw observation next is [10.86666666666667, 58.33333333333334, 204.8333333333333, 228.1666666666667, 22.5, 26.81650207469071, 0.7808605490921389, 1.0, 1.0, 35.0, 20.70242995243141], 
processed observation next is [1.0, 0.4782608695652174, 0.7636195752539245, 0.5833333333333335, 0.6827777777777776, 0.2521178637200737, 0.375, 0.7347085062242259, 0.7602868496973797, 1.0, 1.0, 0.4, 0.2070242995243141], 
reward next is 0.7930, 
noisyNet noise sample is [array([-1.2870557], dtype=float32), 0.8520681]. 
=============================================
[2019-04-09 15:15:00,443] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7239128e-10 1.8897192e-07 1.7271079e-05 1.0061623e-10 9.5122212e-01
 7.6410937e-04 4.7847945e-02 2.6994953e-06 8.8928864e-05 5.4816854e-05
 2.0204907e-06], sum to 1.0000
[2019-04-09 15:15:00,457] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3804
[2019-04-09 15:15:00,635] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 23.33634227152469, 0.1142230777923292, 0.0, 1.0, 45.0, 39.127189356488174], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1728000.0000, 
sim time next is 1729200.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 23.3469461304424, 0.1058439176597363, 0.0, 1.0, 35.0, 26.897104360716483], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.4455788442035334, 0.5352813058865787, 0.0, 1.0, 0.4, 0.26897104360716484], 
reward next is 0.7310, 
noisyNet noise sample is [array([-1.4938471], dtype=float32), -0.2440927]. 
=============================================
[2019-04-09 15:15:00,672] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.13151124e-10 4.56905838e-07 5.16585096e-05 2.05790121e-10
 2.65993237e-01 1.00633909e-03 7.32783616e-01 2.19928588e-05
 1.19144934e-04 2.02208321e-05 3.40237739e-06], sum to 1.0000
[2019-04-09 15:15:00,673] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7669
[2019-04-09 15:15:00,722] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 80.0, 0.0, 0.0, 19.0, 25.14442194129516, 0.4411106343324133, 0.0, 1.0, 45.0, 29.639683124131736], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1561200.0000, 
sim time next is 1562400.0000, 
raw observation next is [5.0, 79.0, 0.0, 0.0, 19.0, 25.05892156581499, 0.4266196292681215, 0.0, 1.0, 45.0, 29.367942036495222], 
processed observation next is [1.0, 0.08695652173913043, 0.6011080332409973, 0.79, 0.0, 0.0, 0.08333333333333333, 0.5882434638179159, 0.6422065430893739, 0.0, 1.0, 0.6, 0.2936794203649522], 
reward next is 0.7063, 
noisyNet noise sample is [array([0.46131852], dtype=float32), 0.83243406]. 
=============================================
[2019-04-09 15:15:00,771] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5751696e-07 3.1524120e-05 3.9130446e-04 1.3066015e-07 7.5017053e-01
 1.1561287e-02 2.3678523e-01 7.2166651e-05 8.4216974e-04 1.1676440e-04
 2.8666052e-05], sum to 1.0000
[2019-04-09 15:15:00,773] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6721
[2019-04-09 15:15:00,792] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.2, 89.66666666666667, 0.0, 0.0, 19.0, 22.35240312932817, -0.1579127409559844, 0.0, 1.0, 35.0, 23.84646054643952], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1740000.0000, 
sim time next is 1741200.0000, 
raw observation next is [-0.4, 88.33333333333334, 0.0, 0.0, 19.0, 22.25901235989264, -0.1797530594936314, 0.0, 1.0, 35.0, 24.04239766727492], 
processed observation next is [0.0, 0.13043478260869565, 0.45152354570637127, 0.8833333333333334, 0.0, 0.0, 0.08333333333333333, 0.35491769665772005, 0.44008231350212285, 0.0, 1.0, 0.4, 0.2404239766727492], 
reward next is 0.7596, 
noisyNet noise sample is [array([-1.4800755], dtype=float32), 0.37147406]. 
=============================================
[2019-04-09 15:15:00,798] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.80526945e-12 7.03670864e-08 9.87055228e-06 1.21032958e-11
 9.09024477e-01 3.49385082e-03 8.73076394e-02 8.24802328e-07
 1.10047775e-04 5.24975767e-05 7.08046741e-07], sum to 1.0000
[2019-04-09 15:15:00,806] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3358
[2019-04-09 15:15:00,821] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-09 15:15:00,822] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:15:00,822] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:15:00,823] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:15:00,824] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:15:00,824] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:15:00,825] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.4, 66.0, 0.0, 0.0, 22.5, 27.2478713212285, 0.9475878595133409, 1.0, 1.0, 35.0, 20.3424164743725], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1623600.0000, 
sim time next is 1624800.0000, 
raw observation next is [8.833333333333334, 68.66666666666667, 0.0, 0.0, 22.5, 27.09207490408621, 0.9181268799270934, 0.0, 1.0, 35.0, 18.55557314008434], 
processed observation next is [1.0, 0.8260869565217391, 0.7072945521698984, 0.6866666666666668, 0.0, 0.0, 0.375, 0.7576729086738508, 0.8060422933090311, 0.0, 1.0, 0.4, 0.18555573140084342], 
reward next is 0.8144, 
noisyNet noise sample is [array([-0.16419375], dtype=float32), -1.8968868]. 
=============================================
[2019-04-09 15:15:00,825] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:15:00,833] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run10
[2019-04-09 15:15:00,851] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run10
[2019-04-09 15:15:00,851] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run10
[2019-04-09 15:16:30,953] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.05165188], dtype=float32), 0.07554027]
[2019-04-09 15:16:30,953] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [-3.866666666666667, 61.00000000000001, 161.8333333333333, 516.0, 22.5, 23.41257068750802, -0.03356226300813579, 1.0, 1.0, 35.0, 22.636843305909995]
[2019-04-09 15:16:30,953] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 15:16:30,954] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [4.0605976e-09 8.9482677e-07 4.9919068e-05 2.7351876e-09 6.3798171e-01
 3.3532130e-03 3.5823247e-01 9.7628590e-06 2.8933695e-04 7.7664088e-05
 4.9915334e-06], sampled 0.353824755585488
[2019-04-09 15:16:43,705] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 3051.8537 109942.4477 577.8009
[2019-04-09 15:16:43,743] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:43,743] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:43,743] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:43,743] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:43,743] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:43,743] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:43,743] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:43,743] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:43,743] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:43,743] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:43,877] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:43,877] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:43,877] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:43,877] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:43,877] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:43,877] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:43,877] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:43,877] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:43,877] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:43,877] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:59,422] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2994.9392 116706.5316 212.5633
[2019-04-09 15:16:59,464] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:59,464] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:59,464] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:59,464] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:59,464] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:59,464] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:59,464] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:59,464] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:59,464] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:59,464] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:16:59,616] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:59,616] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:59,616] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:59,616] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:59,616] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:59,616] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:59,616] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:59,616] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:59,616] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:16:59,616] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:17:07,078] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2906.2260 118323.5201 -2.4929
[2019-04-09 15:17:07,111] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:17:07,111] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:17:07,111] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:17:07,111] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:17:07,111] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:17:07,111] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:17:07,111] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:17:07,111] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:17:07,111] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:17:07,111] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:17:07,260] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:17:07,260] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:17:07,260] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:17:07,260] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:17:07,260] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:17:07,260] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:17:07,260] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:17:07,260] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:17:07,260] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:17:07,260] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:17:08,113] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 90000, evaluation results [90000.0, 2994.939227895391, 116706.53157678919, 212.56325570955102, 3051.8537189491476, 109942.44773547478, 577.8008894538561, 2906.22601668179, 118323.52011100687, -2.4928814935824386]
[2019-04-09 15:17:08,613] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4559624e-12 1.3168463e-08 3.1499160e-06 1.3009069e-11 7.5192398e-01
 7.1084179e-04 2.4733707e-01 7.6285136e-07 1.4886680e-05 9.2109931e-06
 1.3011589e-07], sum to 1.0000
[2019-04-09 15:17:08,613] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6789
[2019-04-09 15:17:08,662] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [9.4, 61.0, 208.0, 168.5, 22.5, 26.34296043085835, 0.6580993583495204, 1.0, 1.0, 35.0, 18.451491075259344], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1594800.0000, 
sim time next is 1596000.0000, 
raw observation next is [10.13333333333333, 59.66666666666667, 213.3333333333333, 222.1666666666667, 22.5, 26.48470665960549, 0.692711448894635, 1.0, 1.0, 35.0, 16.375181514480445], 
processed observation next is [1.0, 0.4782608695652174, 0.7433056325023084, 0.5966666666666667, 0.7111111111111109, 0.24548802946593007, 0.375, 0.7070588883004575, 0.7309038162982117, 1.0, 1.0, 0.4, 0.16375181514480444], 
reward next is 0.8362, 
noisyNet noise sample is [array([1.2476186], dtype=float32), -0.9121727]. 
=============================================
[2019-04-09 15:17:08,673] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[51.774834]
 [51.314205]
 [51.153828]
 [49.86697 ]
 [49.283127]], R is [[52.83466721]
 [53.1218071 ]
 [53.37802505]
 [53.558815  ]
 [53.83497238]].
[2019-04-09 15:17:08,700] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.0085233e-13 6.3954100e-09 3.9322094e-06 5.4301286e-12 8.2689089e-01
 9.5275673e-04 1.7213452e-01 5.8669957e-07 9.0452077e-06 8.0097825e-06
 2.2818764e-07], sum to 1.0000
[2019-04-09 15:17:08,700] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1666
[2019-04-09 15:17:08,747] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 74.0, 0.0, 0.0, 22.5, 26.38829662793565, 0.7576845745773989, 1.0, 1.0, 35.0, 19.002303575734082], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1627200.0000, 
sim time next is 1628400.0000, 
raw observation next is [7.533333333333333, 74.66666666666667, 0.0, 0.0, 19.0, 26.23893766339269, 0.7283574730536184, 0.0, 1.0, 35.0, 17.577560347901716], 
processed observation next is [1.0, 0.8695652173913043, 0.6712834718374886, 0.7466666666666667, 0.0, 0.0, 0.08333333333333333, 0.6865781386160575, 0.7427858243512061, 0.0, 1.0, 0.4, 0.17577560347901716], 
reward next is 0.8242, 
noisyNet noise sample is [array([0.05973821], dtype=float32), 1.0181888]. 
=============================================
[2019-04-09 15:17:08,756] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.4195038e-12 1.7386659e-08 1.6119484e-05 1.1013402e-11 6.2325311e-01
 5.7804100e-03 3.7079325e-01 3.3277300e-07 1.5148263e-04 5.0793456e-06
 1.2318228e-07], sum to 1.0000
[2019-04-09 15:17:08,756] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5978
[2019-04-09 15:17:08,784] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 22.5, 25.33588326513295, 0.5009239440075756, 1.0, 1.0, 45.0, 33.831796541354066], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1708800.0000, 
sim time next is 1710000.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 22.5, 25.18372900210334, 0.4758219675065643, 1.0, 1.0, 35.0, 26.31445655277825], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.88, 0.0, 0.0, 0.375, 0.5986440835086118, 0.6586073225021881, 1.0, 1.0, 0.4, 0.2631445655277825], 
reward next is 0.7369, 
noisyNet noise sample is [array([0.00847495], dtype=float32), -0.53216773]. 
=============================================
[2019-04-09 15:17:08,793] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[53.09367 ]
 [52.855457]
 [52.839756]
 [52.49938 ]
 [52.502068]], R is [[53.47339249]
 [53.6003418 ]
 [53.71504593]
 [53.78837967]
 [54.03932953]].
[2019-04-09 15:17:08,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.6306067e-12 1.2605981e-08 4.4356179e-06 2.3321367e-11 9.3846047e-01
 1.2326380e-03 6.0272656e-02 1.2311792e-06 2.7890073e-05 6.7763153e-07
 6.3613136e-08], sum to 1.0000
[2019-04-09 15:17:08,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5590
[2019-04-09 15:17:08,881] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 22.5, 25.55418027507992, 0.5214409298468711, 1.0, 1.0, 35.0, 20.147802783423185], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1707600.0000, 
sim time next is 1708800.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 22.5, 25.28492535005654, 0.4493438061676768, 1.0, 1.0, 35.0, 19.90734099939006], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 0.375, 0.6070771125047116, 0.6497812687225589, 1.0, 1.0, 0.4, 0.19907340999390058], 
reward next is 0.8009, 
noisyNet noise sample is [array([1.2924879], dtype=float32), 0.876847]. 
=============================================
[2019-04-09 15:17:09,569] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7239871e-09 4.4668013e-06 5.0668012e-05 1.3417264e-09 6.7582458e-01
 3.3272339e-03 3.1917498e-01 1.0370234e-05 1.1788078e-03 4.2340788e-04
 5.4385464e-06], sum to 1.0000
[2019-04-09 15:17:09,569] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7319
[2019-04-09 15:17:09,593] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 24.99805551678252, 0.4596741153604001, 0.0, 1.0, 35.0, 18.076879128609885], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1659600.0000, 
sim time next is 1660800.0000, 
raw observation next is [6.233333333333333, 97.0, 0.0, 0.0, 19.0, 25.15677586252135, 0.4977972773216706, 0.0, 1.0, 45.0, 33.08018255183774], 
processed observation next is [1.0, 0.21739130434782608, 0.6352723915050786, 0.97, 0.0, 0.0, 0.08333333333333333, 0.5963979885434458, 0.6659324257738902, 0.0, 1.0, 0.6, 0.3308018255183774], 
reward next is 0.6692, 
noisyNet noise sample is [array([-1.0886434], dtype=float32), 0.56352645]. 
=============================================
[2019-04-09 15:17:10,029] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7533064e-09 1.3527930e-06 4.6099478e-05 1.2661877e-09 7.1754277e-01
 2.7705056e-03 2.7918026e-01 5.5781406e-06 3.8359140e-04 6.8616195e-05
 1.1911642e-06], sum to 1.0000
[2019-04-09 15:17:10,029] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6320
[2019-04-09 15:17:10,064] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 23.28566277338466, 0.0721061237139836, 0.0, 1.0, 35.0, 24.238466256898334], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1730400.0000, 
sim time next is 1731600.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 23.23805293092569, 0.04062996444514219, 0.0, 1.0, 35.0, 21.99089968184991], 
processed observation next is [0.0, 0.043478260869565216, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.43650441091047415, 0.5135433214817141, 0.0, 1.0, 0.4, 0.2199089968184991], 
reward next is 0.7801, 
noisyNet noise sample is [array([-2.1449413], dtype=float32), 0.31308308]. 
=============================================
[2019-04-09 15:17:10,807] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.7236865e-11 1.8265160e-08 2.4955873e-06 6.5956421e-11 4.8519060e-01
 1.6233989e-03 5.1313317e-01 1.4188299e-06 3.9186198e-05 9.5664273e-06
 1.5015580e-07], sum to 1.0000
[2019-04-09 15:17:10,807] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3663
[2019-04-09 15:17:10,857] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.866666666666667, 92.0, 0.0, 0.0, 22.5, 25.30141642341715, 0.459922492124637, 1.0, 1.0, 35.0, 21.361654958765833], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1669200.0000, 
sim time next is 1670400.0000, 
raw observation next is [3.3, 92.0, 15.5, 0.0, 22.5, 25.22545504889619, 0.4418230992969479, 1.0, 1.0, 35.0, 19.659871593934206], 
processed observation next is [1.0, 0.34782608695652173, 0.554016620498615, 0.92, 0.051666666666666666, 0.0, 0.375, 0.6021212540746825, 0.647274366432316, 1.0, 1.0, 0.4, 0.19659871593934206], 
reward next is 0.8034, 
noisyNet noise sample is [array([1.556964], dtype=float32), -0.5122473]. 
=============================================
[2019-04-09 15:17:10,975] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6299767e-07 3.6329591e-06 7.8667299e-04 5.4087891e-08 4.9273551e-01
 9.8237395e-03 4.9497595e-01 8.3122744e-05 1.2582218e-03 2.9721134e-04
 3.5724031e-05], sum to 1.0000
[2019-04-09 15:17:10,975] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0160
[2019-04-09 15:17:11,009] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.533333333333333, 87.0, 0.0, 0.0, 19.0, 22.38250564471057, -0.1879895872473949, 0.0, 1.0, 35.0, 26.047199995213845], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1752000.0000, 
sim time next is 1753200.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 19.0, 22.25358444894456, -0.2202514459789122, 0.0, 1.0, 35.0, 23.658586387109622], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 0.08333333333333333, 0.35446537074538015, 0.4265828513403626, 0.0, 1.0, 0.4, 0.23658586387109623], 
reward next is 0.7634, 
noisyNet noise sample is [array([-0.6719621], dtype=float32), -0.37307483]. 
=============================================
[2019-04-09 15:17:11,123] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0178336e-06 1.9096096e-05 6.4502121e-04 2.4392250e-07 6.6898382e-01
 1.0453879e-02 3.1698149e-01 1.8808298e-04 1.2911445e-03 1.3164394e-03
 1.1978129e-04], sum to 1.0000
[2019-04-09 15:17:11,123] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2725
[2019-04-09 15:17:11,198] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 76.0, 130.6666666666667, 56.0, 19.0, 18.71729250181062, -1.093958360396236, 0.0, 1.0, 35.0, 27.330909464947773], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1852800.0000, 
sim time next is 1854000.0000, 
raw observation next is [-5.6, 75.0, 152.0, 66.0, 19.0, 18.66795140337319, -1.097819490559872, 0.0, 1.0, 35.0, 27.355238119833352], 
processed observation next is [0.0, 0.4782608695652174, 0.30747922437673136, 0.75, 0.5066666666666667, 0.07292817679558011, 0.08333333333333333, 0.05566261694776594, 0.134060169813376, 0.0, 1.0, 0.4, 0.2735523811983335], 
reward next is 0.4671, 
noisyNet noise sample is [array([-1.3218417], dtype=float32), 0.49141705]. 
=============================================
[2019-04-09 15:17:11,206] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[27.302547]
 [27.27135 ]
 [27.38436 ]
 [26.986088]
 [26.853846]], R is [[27.75076294]
 [28.03478432]
 [28.41736603]
 [28.86863708]
 [29.34586525]].
[2019-04-09 15:17:11,507] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.67521195e-11 1.45868375e-08 3.99354167e-06 6.38586753e-12
 8.56822968e-01 3.08927265e-03 1.39954165e-01 1.97302796e-07
 1.20345045e-04 8.96098663e-06 8.52048174e-08], sum to 1.0000
[2019-04-09 15:17:11,507] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8484
[2019-04-09 15:17:11,561] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4852976e-12 1.6874989e-09 2.0024713e-06 1.0078418e-13 2.7701536e-01
 6.2271472e-05 7.2290558e-01 4.5114895e-08 1.3941659e-05 7.8038909e-07
 8.8058956e-09], sum to 1.0000
[2019-04-09 15:17:11,561] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6895
[2019-04-09 15:17:11,592] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 83.33333333333334, 0.0, 22.5, 26.14875406093212, 0.6237272440871228, 1.0, 1.0, 35.0, 22.280827176182346], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1693200.0000, 
sim time next is 1694400.0000, 
raw observation next is [1.1, 88.0, 75.5, 0.0, 22.5, 25.65368826610612, 0.5308950977446395, 1.0, 1.0, 35.0, 21.923709961010665], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.88, 0.25166666666666665, 0.0, 0.375, 0.6378073555088433, 0.6769650325815465, 1.0, 1.0, 0.4, 0.21923709961010665], 
reward next is 0.7808, 
noisyNet noise sample is [array([0.1250836], dtype=float32), -0.22813208]. 
=============================================
[2019-04-09 15:17:11,613] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 81.0, 41.5, 0.0, 22.5, 26.17160763049927, 0.5288115169541011, 1.0, 1.0, 45.0, 34.170386246860886], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1699200.0000, 
sim time next is 1700400.0000, 
raw observation next is [1.433333333333334, 83.33333333333334, 33.83333333333333, 0.0, 22.5, 25.25752467680409, 0.5373651129008651, 1.0, 1.0, 45.0, 33.26397988083862], 
processed observation next is [1.0, 0.6956521739130435, 0.502308402585411, 0.8333333333333335, 0.11277777777777777, 0.0, 0.375, 0.6047937230670074, 0.6791217043002883, 1.0, 1.0, 0.6, 0.3326397988083862], 
reward next is 0.6674, 
noisyNet noise sample is [array([0.6947759], dtype=float32), 0.8754555]. 
=============================================
[2019-04-09 15:17:11,754] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.5151095e-11 1.7124183e-08 1.6119293e-05 1.2693836e-11 9.4172031e-01
 1.1414060e-03 5.7049982e-02 6.0291990e-07 6.9481153e-05 2.0788409e-06
 3.2422751e-08], sum to 1.0000
[2019-04-09 15:17:11,758] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1956
[2019-04-09 15:17:11,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2732465e-10 6.5604247e-08 7.8325875e-06 1.4786407e-10 9.2485565e-01
 1.5406294e-03 7.3515795e-02 4.7557339e-07 6.8555426e-05 6.3353086e-06
 4.6062319e-06], sum to 1.0000
[2019-04-09 15:17:11,781] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2844
[2019-04-09 15:17:11,793] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 86.66666666666667, 105.8333333333333, 0.0, 22.5, 26.36016800339611, 0.6630570821368998, 1.0, 1.0, 35.0, 25.74685892113949], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1687200.0000, 
sim time next is 1688400.0000, 
raw observation next is [1.1, 88.0, 103.5, 0.0, 22.5, 26.23854407717329, 0.6513916474621056, 1.0, 1.0, 35.0, 22.795833836624766], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.345, 0.0, 0.375, 0.6865453397644409, 0.7171305491540352, 1.0, 1.0, 0.4, 0.22795833836624765], 
reward next is 0.7720, 
noisyNet noise sample is [array([0.32726872], dtype=float32), -1.4296403]. 
=============================================
[2019-04-09 15:17:11,818] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.0, 95.0, 0.0, 0.0, 19.0, 25.52432319036219, 0.5750678060350558, 0.0, 1.0, 35.0, 16.95303365074492], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1647600.0000, 
sim time next is 1648800.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 25.50531423386758, 0.555283383173029, 0.0, 1.0, 35.0, 19.666432859840768], 
processed observation next is [1.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.6254428528222983, 0.6850944610576764, 0.0, 1.0, 0.4, 0.19666432859840768], 
reward next is 0.8033, 
noisyNet noise sample is [array([-0.33747512], dtype=float32), 0.38704923]. 
=============================================
[2019-04-09 15:17:12,061] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.2626990e-08 1.0622789e-05 5.5484305e-04 1.5577653e-08 7.6616961e-01
 5.0861249e-03 2.2515954e-01 3.1685675e-05 2.3752421e-03 6.0442759e-04
 7.7750165e-06], sum to 1.0000
[2019-04-09 15:17:12,061] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4323
[2019-04-09 15:17:12,101] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1333333333333334, 91.0, 0.0, 0.0, 19.0, 23.18700104175424, 0.0540229700568865, 0.0, 1.0, 45.0, 36.07828006396747], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1736400.0000, 
sim time next is 1737600.0000, 
raw observation next is [0.06666666666666668, 91.0, 0.0, 0.0, 19.0, 23.24966041495238, 0.04352841456487563, 0.0, 1.0, 35.0, 28.026415717298836], 
processed observation next is [0.0, 0.08695652173913043, 0.46445060018467227, 0.91, 0.0, 0.0, 0.08333333333333333, 0.4374717012460317, 0.5145094715216252, 0.0, 1.0, 0.4, 0.28026415717298836], 
reward next is 0.7197, 
noisyNet noise sample is [array([1.2905897], dtype=float32), 0.021751927]. 
=============================================
[2019-04-09 15:17:12,715] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3028515e-11 7.2042514e-09 3.1567840e-06 4.4283999e-12 9.4506854e-01
 3.4648430e-04 5.4520369e-02 1.7074732e-06 4.8669717e-05 1.0977345e-05
 7.2095062e-08], sum to 1.0000
[2019-04-09 15:17:12,717] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2142
[2019-04-09 15:17:12,777] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 86.66666666666667, 105.8333333333333, 0.0, 22.5, 25.61882663567813, 0.4958527175479612, 1.0, 1.0, 35.0, 21.41349337440684], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1687200.0000, 
sim time next is 1688400.0000, 
raw observation next is [1.1, 88.0, 103.5, 0.0, 22.5, 25.03849615878802, 0.4181673794501906, 1.0, 1.0, 35.0, 21.79972883699402], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.345, 0.0, 0.375, 0.5865413465656685, 0.6393891264833969, 1.0, 1.0, 0.4, 0.2179972883699402], 
reward next is 0.7820, 
noisyNet noise sample is [array([-1.275496], dtype=float32), 1.1701871]. 
=============================================
[2019-04-09 15:17:13,729] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.82438808e-12 1.17278880e-08 9.07624155e-07 2.24696065e-12
 8.80515456e-01 4.68285318e-04 1.18972786e-01 3.37953281e-07
 3.94921699e-05 2.58045679e-06 8.09160241e-08], sum to 1.0000
[2019-04-09 15:17:13,729] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6980
[2019-04-09 15:17:13,762] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 88.0, 101.1666666666667, 0.0, 22.5, 26.1123453448106, 0.6348713553674927, 1.0, 1.0, 35.0, 20.15942105243225], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1689600.0000, 
sim time next is 1690800.0000, 
raw observation next is [1.1, 88.0, 96.66666666666667, 0.0, 22.5, 26.13606105420135, 0.634701411120747, 1.0, 1.0, 35.0, 18.22350968152081], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.32222222222222224, 0.0, 0.375, 0.6780050878501124, 0.711567137040249, 1.0, 1.0, 0.4, 0.1822350968152081], 
reward next is 0.8178, 
noisyNet noise sample is [array([-0.9251838], dtype=float32), -1.4638644]. 
=============================================
[2019-04-09 15:17:14,054] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3193367e-07 6.0778770e-05 6.0841173e-04 1.0408889e-06 5.9092355e-01
 1.0721152e-02 3.9552617e-01 7.7576296e-05 9.3584566e-04 1.0008081e-03
 1.4448917e-04], sum to 1.0000
[2019-04-09 15:17:14,054] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0039
[2019-04-09 15:17:14,150] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.199999999999999, 84.33333333333333, 0.0, 0.0, 19.0, 20.14568905530005, -0.7631272942252, 0.0, 1.0, 45.0, 41.52472561324429], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1827600.0000, 
sim time next is 1828800.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 19.0, 20.15628974706387, -0.7739040220104728, 0.0, 1.0, 35.0, 31.90405257145737], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.83, 0.0, 0.0, 0.08333333333333333, 0.17969081225532246, 0.24203199266317574, 0.0, 1.0, 0.4, 0.3190405257145737], 
reward next is 0.6810, 
noisyNet noise sample is [array([1.3535799], dtype=float32), -0.3362813]. 
=============================================
[2019-04-09 15:17:14,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1676988e-07 1.3787227e-05 2.2580860e-04 6.1101730e-08 9.1842878e-01
 4.0220143e-03 7.4632294e-02 6.7857611e-05 2.2722518e-03 3.2424933e-04
 1.2741072e-05], sum to 1.0000
[2019-04-09 15:17:14,594] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1051
[2019-04-09 15:17:14,649] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.866666666666667, 84.0, 0.0, 0.0, 19.0, 19.47334951129066, -0.9837176670417214, 0.0, 1.0, 35.0, 29.458335432145354], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1884000.0000, 
sim time next is 1885200.0000, 
raw observation next is [-5.233333333333333, 85.0, 0.0, 0.0, 19.0, 19.42486908603519, -1.009173719772572, 0.0, 1.0, 35.0, 26.47455460126078], 
processed observation next is [0.0, 0.8260869565217391, 0.31763619575253926, 0.85, 0.0, 0.0, 0.08333333333333333, 0.11873909050293256, 0.1636087600758093, 0.0, 1.0, 0.4, 0.2647455460126078], 
reward next is 0.7353, 
noisyNet noise sample is [array([0.83303154], dtype=float32), 0.5789204]. 
=============================================
[2019-04-09 15:17:14,904] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0891308e-06 2.3742404e-04 1.1464648e-03 1.1009237e-06 4.8660189e-01
 3.5435703e-02 4.6705294e-01 3.9536349e-04 6.6912128e-03 2.1491740e-03
 2.8553719e-04], sum to 1.0000
[2019-04-09 15:17:14,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1495
[2019-04-09 15:17:14,951] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 19.32392011199493, -0.970457330612288, 0.0, 1.0, 35.0, 28.12042932960633], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1842000.0000, 
sim time next is 1843200.0000, 
raw observation next is [-6.7, 78.0, 14.0, 0.0, 19.0, 19.22248451528419, -0.999984963492579, 0.0, 1.0, 35.0, 25.45591600358341], 
processed observation next is [0.0, 0.34782608695652173, 0.2770083102493075, 0.78, 0.04666666666666667, 0.0, 0.08333333333333333, 0.10187370960701585, 0.166671678835807, 0.0, 1.0, 0.4, 0.2545591600358341], 
reward next is 0.7454, 
noisyNet noise sample is [array([-0.27880445], dtype=float32), -1.4951688]. 
=============================================
[2019-04-09 15:17:14,997] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0013348e-09 3.6701780e-07 4.8430884e-05 1.2960237e-09 7.5400031e-01
 1.0671116e-03 2.4431111e-01 1.7990078e-05 5.0673418e-04 4.6277462e-05
 1.6272896e-06], sum to 1.0000
[2019-04-09 15:17:14,998] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3228
[2019-04-09 15:17:15,033] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 22.92903527100518, 0.004567487540120434, 0.0, 1.0, 35.0, 27.322846383421265], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1730400.0000, 
sim time next is 1731600.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 22.99230349966336, -0.01517499390365782, 0.0, 1.0, 35.0, 24.035381421810264], 
processed observation next is [0.0, 0.043478260869565216, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.4160252916386134, 0.4949416686987807, 0.0, 1.0, 0.4, 0.24035381421810265], 
reward next is 0.7596, 
noisyNet noise sample is [array([-0.11301165], dtype=float32), -0.50335693]. 
=============================================
[2019-04-09 15:17:15,236] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.37318858e-08 1.54801319e-06 1.65754478e-04 9.13332965e-09
 7.82147110e-01 5.30006783e-03 2.11910933e-01 4.37646049e-05
 3.11309501e-04 1.01095786e-04 1.84439505e-05], sum to 1.0000
[2019-04-09 15:17:15,240] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7440
[2019-04-09 15:17:15,249] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0950605e-09 3.2557202e-06 5.2850173e-05 9.8335795e-10 5.1516426e-01
 1.8540403e-03 4.8178953e-01 4.2330021e-06 9.6782693e-04 1.5775800e-04
 6.1961387e-06], sum to 1.0000
[2019-04-09 15:17:15,249] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9382
[2019-04-09 15:17:15,279] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 22.85698497700932, 0.02085165476879313, 0.0, 1.0, 45.0, 42.11291828164025], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1730400.0000, 
sim time next is 1731600.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 22.98699361767336, 0.03781287886156084, 0.0, 1.0, 45.0, 38.163545743932374], 
processed observation next is [0.0, 0.043478260869565216, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.41558280147278, 0.5126042929538536, 0.0, 1.0, 0.6, 0.38163545743932376], 
reward next is 0.6184, 
noisyNet noise sample is [array([-2.5194633], dtype=float32), 0.6588635]. 
=============================================
[2019-04-09 15:17:15,304] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 87.0, 47.0, 0.0, 19.0, 21.25419651614296, -0.4727969142761461, 0.0, 1.0, 35.0, 23.54624090321476], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1785600.0000, 
sim time next is 1786800.0000, 
raw observation next is [-3.566666666666667, 85.33333333333334, 34.33333333333334, 0.0, 19.0, 21.12927098876778, -0.5033432884915859, 0.0, 1.0, 35.0, 27.665620472608353], 
processed observation next is [0.0, 0.6956521739130435, 0.3638042474607572, 0.8533333333333334, 0.11444444444444447, 0.0, 0.08333333333333333, 0.2607725823973149, 0.332218903836138, 0.0, 1.0, 0.4, 0.27665620472608354], 
reward next is 0.7233, 
noisyNet noise sample is [array([0.746702], dtype=float32), 0.37675458]. 
=============================================
[2019-04-09 15:17:15,397] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.6587107e-08 3.1119143e-06 1.1138482e-04 2.8653782e-08 7.1935761e-01
 9.9176904e-03 2.6908579e-01 1.8222063e-05 1.4339052e-03 4.8514285e-05
 2.3710041e-05], sum to 1.0000
[2019-04-09 15:17:15,398] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9662
[2019-04-09 15:17:15,468] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 83.33333333333334, 0.0, 0.0, 19.0, 20.03848224068415, -0.7722360673634437, 0.0, 1.0, 35.0, 27.199510702517344], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1809600.0000, 
sim time next is 1810800.0000, 
raw observation next is [-5.0, 82.0, 0.0, 0.0, 19.0, 19.97486684606296, -0.749727219915819, 0.0, 1.0, 45.0, 46.81900886260392], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.82, 0.0, 0.0, 0.08333333333333333, 0.16457223717191324, 0.250090926694727, 0.0, 1.0, 0.6, 0.4681900886260392], 
reward next is 0.5318, 
noisyNet noise sample is [array([1.3299626], dtype=float32), -0.91028917]. 
=============================================
[2019-04-09 15:17:15,616] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.5934841e-08 7.2442385e-06 1.1946709e-04 9.1924868e-08 8.6178684e-01
 1.0062556e-02 1.2630886e-01 1.5822184e-04 1.3534646e-03 1.8889851e-04
 1.4272037e-05], sum to 1.0000
[2019-04-09 15:17:15,617] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0669
[2019-04-09 15:17:15,670] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.899999999999999, 82.0, 0.0, 0.0, 19.0, 20.98985567277339, -0.5021194205036595, 0.0, 1.0, 45.0, 40.518167511155184], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1791600.0000, 
sim time next is 1792800.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 19.0, 21.08814385121532, -0.5060707690069669, 0.0, 1.0, 35.0, 30.809442842534416], 
processed observation next is [0.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.08333333333333333, 0.2573453209346101, 0.33130974366434435, 0.0, 1.0, 0.4, 0.3080944284253442], 
reward next is 0.6919, 
noisyNet noise sample is [array([0.1446552], dtype=float32), -0.563522]. 
=============================================
[2019-04-09 15:17:16,867] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1949956e-07 6.6416637e-06 1.8750886e-04 2.1263752e-08 8.6581725e-01
 4.5469701e-03 1.2832108e-01 2.9247718e-05 8.9428009e-04 1.7757234e-04
 1.9348714e-05], sum to 1.0000
[2019-04-09 15:17:16,867] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0812
[2019-04-09 15:17:16,925] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 78.0, 0.0, 0.0, 19.0, 20.57763587766753, -0.6800563862566023, 0.0, 1.0, 35.0, 30.653575381901554], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1818000.0000, 
sim time next is 1819200.0000, 
raw observation next is [-5.733333333333333, 79.66666666666667, 0.0, 0.0, 19.0, 20.4515539798717, -0.7148494765187962, 0.0, 1.0, 35.0, 27.35113851757858], 
processed observation next is [0.0, 0.043478260869565216, 0.30378578024007385, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.2042961649893084, 0.2617168411604013, 0.0, 1.0, 0.4, 0.27351138517578577], 
reward next is 0.7265, 
noisyNet noise sample is [array([-0.0884834], dtype=float32), -0.38463312]. 
=============================================
[2019-04-09 15:17:17,261] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6957989e-08 2.2979777e-06 1.1106119e-04 1.0174688e-08 8.4872574e-01
 3.7447067e-03 1.4714193e-01 1.8559620e-05 1.8273981e-04 6.1559891e-05
 1.1436083e-05], sum to 1.0000
[2019-04-09 15:17:17,261] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1998
[2019-04-09 15:17:17,337] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 82.0, 0.0, 0.0, 19.0, 20.09516604716504, -0.7496427337602031, 0.0, 1.0, 35.0, 27.087270218962587], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1810800.0000, 
sim time next is 1812000.0000, 
raw observation next is [-5.0, 81.0, 0.0, 0.0, 19.0, 20.00299559102761, -0.7695421289969596, 0.0, 1.0, 35.0, 27.174729033798464], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.81, 0.0, 0.0, 0.08333333333333333, 0.16691629925230078, 0.24348595700101347, 0.0, 1.0, 0.4, 0.2717472903379846], 
reward next is 0.7283, 
noisyNet noise sample is [array([-1.5767262], dtype=float32), -1.8193227]. 
=============================================
[2019-04-09 15:17:17,400] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[33.570015]
 [33.715363]
 [33.95876 ]
 [34.03601 ]
 [34.343483]], R is [[33.90715408]
 [34.29721069]
 [34.68429184]
 [35.06910706]
 [35.45412064]].
[2019-04-09 15:17:17,410] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6281294e-07 1.3254815e-05 4.0069444e-04 2.9386133e-08 3.2294682e-01
 9.5653860e-03 6.6434819e-01 7.7977202e-05 2.4392116e-03 1.6268746e-04
 4.5658977e-05], sum to 1.0000
[2019-04-09 15:17:17,411] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9122
[2019-04-09 15:17:17,473] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 87.0, 82.5, 0.0, 19.0, 21.40926300880064, -0.4163589179483875, 0.0, 1.0, 35.0, 30.03240816125633], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1782000.0000, 
sim time next is 1783200.0000, 
raw observation next is [-3.0, 87.0, 71.5, 0.0, 19.0, 21.42006318368933, -0.4112218021887568, 0.0, 1.0, 45.0, 38.15545377329846], 
processed observation next is [0.0, 0.6521739130434783, 0.3795013850415513, 0.87, 0.23833333333333334, 0.0, 0.08333333333333333, 0.28500526530744413, 0.36292606593708104, 0.0, 1.0, 0.6, 0.3815545377329846], 
reward next is 0.6184, 
noisyNet noise sample is [array([0.49966466], dtype=float32), -0.40991032]. 
=============================================
[2019-04-09 15:17:17,898] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6386883e-07 1.5945663e-05 3.2195539e-04 2.6303897e-07 5.2489358e-01
 2.1555953e-02 4.4914708e-01 6.5010448e-05 3.5787018e-03 3.8407050e-04
 3.7125799e-05], sum to 1.0000
[2019-04-09 15:17:17,898] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8231
[2019-04-09 15:17:17,943] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 19.0, 19.00075841583963, -1.100403775217968, 0.0, 1.0, 35.0, 31.16832534394255], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1900800.0000, 
sim time next is 1902000.0000, 
raw observation next is [-7.300000000000001, 79.66666666666667, 0.0, 0.0, 19.0, 18.90800498902481, -1.10136249251069, 0.0, 1.0, 45.0, 39.61251345831923], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.07566708241873421, 0.13287916916310336, 0.0, 1.0, 0.6, 0.39612513458319226], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.54270303], dtype=float32), -0.8981154]. 
=============================================
[2019-04-09 15:17:17,951] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[32.352257]
 [32.033363]
 [32.311394]
 [32.337368]
 [31.69027 ]], R is [[32.28897858]
 [32.6544075 ]
 [32.92775345]
 [33.19921875]
 [33.47182083]].
[2019-04-09 15:17:18,108] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.37240522e-07 2.93863268e-05 3.27792368e-04 2.33896049e-08
 5.09530962e-01 3.69351241e-03 4.84426796e-01 1.00324214e-04
 1.13326160e-03 7.42422300e-04 1.51034901e-05], sum to 1.0000
[2019-04-09 15:17:18,112] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4124
[2019-04-09 15:17:18,195] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 80.33333333333334, 91.0, 14.0, 19.0, 19.61407091173978, -0.9072962606274159, 0.0, 1.0, 45.0, 38.588680677531705], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1869600.0000, 
sim time next is 1870800.0000, 
raw observation next is [-4.5, 77.66666666666667, 64.83333333333333, 0.0, 19.0, 19.72317321035201, -0.9118097309872805, 0.0, 1.0, 35.0, 29.932558275083707], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.7766666666666667, 0.2161111111111111, 0.0, 0.08333333333333333, 0.14359776752933406, 0.19606342300423985, 0.0, 1.0, 0.4, 0.2993255827508371], 
reward next is 0.7007, 
noisyNet noise sample is [array([0.23415215], dtype=float32), 1.0947193]. 
=============================================
[2019-04-09 15:17:18,445] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.8545895e-07 7.0957167e-06 4.1964182e-04 5.1250680e-07 7.4936956e-01
 8.6721303e-03 2.3784837e-01 1.3756790e-04 2.9772373e-03 5.2617909e-04
 4.0851060e-05], sum to 1.0000
[2019-04-09 15:17:18,445] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1931
[2019-04-09 15:17:18,486] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 19.0, 19.54786931149922, -0.872030581643506, 0.0, 1.0, 45.0, 46.30915308514246], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1825200.0000, 
sim time next is 1826400.0000, 
raw observation next is [-6.2, 85.66666666666667, 0.0, 0.0, 19.0, 19.58627939394477, -0.8644766321141523, 0.0, 1.0, 35.0, 31.264171033447667], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.1321899494953976, 0.2118411226286159, 0.0, 1.0, 0.4, 0.3126417103344767], 
reward next is 0.6874, 
noisyNet noise sample is [array([-0.09695493], dtype=float32), -1.1480536]. 
=============================================
[2019-04-09 15:17:20,442] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5360347e-07 2.1429940e-05 1.2360465e-04 6.7428111e-08 7.1311098e-01
 7.2938935e-03 2.7786002e-01 5.2738644e-05 1.2637547e-03 2.3148772e-04
 4.1830732e-05], sum to 1.0000
[2019-04-09 15:17:20,443] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4944
[2019-04-09 15:17:20,466] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.7, 78.0, 14.0, 0.0, 19.0, 19.35364611868218, -0.9531141937879112, 0.0, 1.0, 45.0, 42.05424927307258], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1843200.0000, 
sim time next is 1844400.0000, 
raw observation next is [-6.700000000000001, 78.0, 22.66666666666667, 0.0, 19.0, 19.3923860147393, -0.9629870849322653, 0.0, 1.0, 35.0, 32.00031261174955], 
processed observation next is [0.0, 0.34782608695652173, 0.2770083102493075, 0.78, 0.07555555555555557, 0.0, 0.08333333333333333, 0.11603216789494179, 0.17900430502257825, 0.0, 1.0, 0.4, 0.3200031261174955], 
reward next is 0.6800, 
noisyNet noise sample is [array([-1.0159863], dtype=float32), 0.49385095]. 
=============================================
[2019-04-09 15:17:20,604] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.31634407e-08 1.06735961e-05 6.84211627e-05 1.19144921e-07
 8.63979101e-01 9.35269799e-03 1.21963434e-01 1.49316067e-04
 3.86041566e-03 3.40225932e-04 2.75537779e-04], sum to 1.0000
[2019-04-09 15:17:20,604] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7295
[2019-04-09 15:17:20,638] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.100000000000001, 85.0, 0.0, 0.0, 19.0, 17.74744701277967, -1.384069077796145, 0.0, 1.0, 35.0, 26.276985333210394], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1923600.0000, 
sim time next is 1924800.0000, 
raw observation next is [-9.3, 88.0, 0.0, 0.0, 19.0, 17.6445647355185, -1.405932937513782, 0.0, 1.0, 35.0, 24.778344409045765], 
processed observation next is [1.0, 0.2608695652173913, 0.20498614958448752, 0.88, 0.0, 0.0, 0.08333333333333333, -0.029619605373458313, 0.031355687495406036, 0.0, 1.0, 0.4, 0.24778344409045766], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1082661], dtype=float32), -1.7394849]. 
=============================================
[2019-04-09 15:17:20,767] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.61733418e-08 1.21796620e-06 1.13898466e-04 6.17176887e-09
 6.17259443e-01 5.53348381e-03 3.76249790e-01 5.37636879e-06
 7.91272032e-04 4.20073702e-05 3.47528567e-06], sum to 1.0000
[2019-04-09 15:17:20,767] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4225
[2019-04-09 15:17:20,826] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 85.66666666666667, 0.0, 0.0, 19.0, 20.84573610729264, -0.6675812794586546, 0.0, 1.0, 45.0, 36.76957664809961], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1992000.0000, 
sim time next is 1993200.0000, 
raw observation next is [-5.8, 84.33333333333333, 0.0, 0.0, 19.0, 20.82188032933134, -0.6878667422440446, 0.0, 1.0, 35.0, 28.555210736710535], 
processed observation next is [1.0, 0.043478260869565216, 0.30193905817174516, 0.8433333333333333, 0.0, 0.0, 0.08333333333333333, 0.23515669411094495, 0.2707110859186518, 0.0, 1.0, 0.4, 0.28555210736710535], 
reward next is 0.7144, 
noisyNet noise sample is [array([0.7628356], dtype=float32), 0.7728752]. 
=============================================
[2019-04-09 15:17:21,233] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.04488742e-07 3.12809752e-05 3.83176841e-04 8.99953463e-08
 5.81173718e-01 6.45712810e-03 4.10164803e-01 8.05965246e-05
 1.03198038e-03 5.56782004e-04 1.20284814e-04], sum to 1.0000
[2019-04-09 15:17:21,233] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1327
[2019-04-09 15:17:21,276] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 71.0, 152.0, 40.5, 19.0, 19.41246658440717, -0.9511081118685428, 0.0, 1.0, 35.0, 26.940777638141498], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1857600.0000, 
sim time next is 1858800.0000, 
raw observation next is [-4.833333333333334, 71.0, 130.6666666666667, 13.5, 19.0, 19.35680390240299, -0.9785688787936042, 0.0, 1.0, 35.0, 24.404962365408707], 
processed observation next is [0.0, 0.5217391304347826, 0.32871652816251157, 0.71, 0.4355555555555557, 0.014917127071823204, 0.08333333333333333, 0.11306699186691589, 0.17381037373546526, 0.0, 1.0, 0.4, 0.24404962365408708], 
reward next is 0.7560, 
noisyNet noise sample is [array([-0.9124343], dtype=float32), 0.4011515]. 
=============================================
[2019-04-09 15:17:21,456] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.02706445e-11 1.37732625e-08 7.97469602e-06 1.01719388e-11
 2.89611161e-01 1.34020559e-02 6.96918964e-01 1.70405212e-06
 5.64106595e-05 1.35766015e-06 3.69926482e-07], sum to 1.0000
[2019-04-09 15:17:21,456] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2914
[2019-04-09 15:17:21,536] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-4.5, 71.0, 0.0, 0.0, 22.5, 23.36373017761695, -0.1719535718029126, 1.0, 1.0, 35.0, 30.83389365090233], 
current ob forecast is [], 
actual action is [1, 40.0], 
sim time this is 1969200.0000, 
sim time next is 1970400.0000, 
raw observation next is [-4.866666666666667, 75.0, 0.0, 0.0, 22.5, 23.2822585496278, -0.1978191113505694, 1.0, 1.0, 40.0, 28.461898509002204], 
processed observation next is [1.0, 0.8260869565217391, 0.3277931671283472, 0.75, 0.0, 0.0, 0.375, 0.4401882124689835, 0.43406029621647685, 1.0, 1.0, 0.5, 0.28461898509002204], 
reward next is 0.7154, 
noisyNet noise sample is [array([-0.02241085], dtype=float32), 0.1854234]. 
=============================================
[2019-04-09 15:17:22,356] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0212832e-07 6.4640258e-06 4.0296369e-04 6.0772159e-08 3.2941708e-01
 8.4752589e-03 6.6045970e-01 1.3165024e-05 9.9356868e-04 2.0198058e-04
 2.9660257e-05], sum to 1.0000
[2019-04-09 15:17:22,356] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1683
[2019-04-09 15:17:22,395] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 18.58898660318275, -1.150311313484184, 0.0, 1.0, 45.0, 40.73453673144891], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1890000.0000, 
sim time next is 1891200.0000, 
raw observation next is [-5.8, 80.33333333333334, 0.0, 0.0, 19.0, 18.68460278816855, -1.150190940271924, 0.0, 1.0, 35.0, 31.408023535412244], 
processed observation next is [0.0, 0.9130434782608695, 0.30193905817174516, 0.8033333333333335, 0.0, 0.0, 0.08333333333333333, 0.057050232347379236, 0.11660301990935869, 0.0, 1.0, 0.4, 0.31408023535412244], 
reward next is 0.4861, 
noisyNet noise sample is [array([-1.7633121], dtype=float32), -0.120432444]. 
=============================================
[2019-04-09 15:17:22,818] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.6714992e-08 3.4740842e-06 7.0926260e-05 2.8233575e-08 3.7893882e-01
 1.0979974e-03 6.1897808e-01 1.6741813e-05 7.2410150e-04 1.6514644e-04
 4.5256925e-06], sum to 1.0000
[2019-04-09 15:17:22,818] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1232
[2019-04-09 15:17:22,884] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.233333333333333, 85.0, 0.0, 0.0, 19.0, 19.16641686510667, -1.025262369818726, 0.0, 1.0, 45.0, 39.043122441889764], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1885200.0000, 
sim time next is 1886400.0000, 
raw observation next is [-5.6, 86.0, 0.0, 0.0, 19.0, 19.18557739065032, -1.03193677658536, 0.0, 1.0, 35.0, 29.563130625291087], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.86, 0.0, 0.0, 0.08333333333333333, 0.09879811588752663, 0.15602107447154667, 0.0, 1.0, 0.4, 0.2956313062529109], 
reward next is 0.7044, 
noisyNet noise sample is [array([0.17989546], dtype=float32), -1.6835716]. 
=============================================
[2019-04-09 15:17:23,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1292638e-07 3.0913750e-06 9.8389166e-05 4.3752852e-08 6.7018276e-01
 3.0360934e-03 3.2477814e-01 4.5837540e-05 1.4636663e-03 3.0009105e-04
 9.1827722e-05], sum to 1.0000
[2019-04-09 15:17:23,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8209
[2019-04-09 15:17:23,646] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 19.0, 18.74898328977805, -1.134359692904035, 0.0, 1.0, 45.0, 40.989383128013216], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1900800.0000, 
sim time next is 1902000.0000, 
raw observation next is [-7.300000000000001, 79.66666666666667, 0.0, 0.0, 19.0, 18.78101639496991, -1.124605025779327, 0.0, 1.0, 45.0, 40.303067772900626], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.06508469958082586, 0.12513165807355767, 0.0, 1.0, 0.6, 0.40303067772900625], 
reward next is 0.5817, 
noisyNet noise sample is [array([0.15221605], dtype=float32), -1.2927]. 
=============================================
[2019-04-09 15:17:23,682] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[33.066925]
 [33.03059 ]
 [32.87821 ]
 [32.915462]
 [33.2722  ]], R is [[32.92715073]
 [33.14173126]
 [33.3712883 ]
 [33.77487183]
 [34.12524033]].
[2019-04-09 15:17:23,974] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.07456950e-11 6.84343915e-09 1.04606215e-05 6.22350921e-11
 5.04592121e-01 2.79228319e-03 4.92549181e-01 6.85561588e-07
 5.06748183e-05 3.41017130e-06 1.13924432e-06], sum to 1.0000
[2019-04-09 15:17:23,974] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1619
[2019-04-09 15:17:24,207] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.533333333333333, 70.66666666666667, 25.83333333333333, 0.3333333333333333, 22.5, 23.71637965799549, -0.1495029905611588, 1.0, 1.0, 45.0, 42.754596386171784], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1960800.0000, 
sim time next is 1962000.0000, 
raw observation next is [-3.9, 75.0, 17.5, 1.0, 22.5, 23.62343917047723, -0.1632456908089246, 1.0, 1.0, 35.0, 32.34467658159704], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.75, 0.058333333333333334, 0.0011049723756906078, 0.375, 0.4686199308731025, 0.4455847697303585, 1.0, 1.0, 0.4, 0.3234467658159704], 
reward next is 0.6766, 
noisyNet noise sample is [array([-0.7933124], dtype=float32), -1.3432877]. 
=============================================
[2019-04-09 15:17:24,258] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[50.31276 ]
 [50.480373]
 [49.98074 ]
 [50.234005]
 [50.72872 ]], R is [[50.27899933]
 [50.34866333]
 [50.3792572 ]
 [50.59781265]
 [50.79232025]].
[2019-04-09 15:17:25,173] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.90497963e-12 6.59948762e-09 1.04242702e-06 1.40928621e-12
 1.09574795e-01 7.04537451e-05 8.90342891e-01 1.09769722e-07
 8.41791916e-06 2.19513731e-06 1.89780245e-08], sum to 1.0000
[2019-04-09 15:17:25,173] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4295
[2019-04-09 15:17:25,351] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2174614e-11 9.7999106e-08 1.1070288e-06 9.2069173e-12 8.1927377e-01
 1.1532376e-04 1.8058451e-01 2.8200319e-07 2.2418446e-05 2.4172400e-06
 6.0047846e-08], sum to 1.0000
[2019-04-09 15:17:25,352] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9999
[2019-04-09 15:17:25,401] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 86.0, 87.5, 0.0, 22.5, 24.2701287212675, -0.1083018886735933, 1.0, 1.0, 45.0, 42.36552400964112], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2041200.0000, 
sim time next is 2042400.0000, 
raw observation next is [-4.300000000000001, 84.66666666666667, 76.5, 0.0, 22.5, 23.69187345987566, -0.0733129141368279, 1.0, 1.0, 45.0, 42.79737974142452], 
processed observation next is [1.0, 0.6521739130434783, 0.34349030470914127, 0.8466666666666667, 0.255, 0.0, 0.375, 0.47432278832297153, 0.4755623619543907, 1.0, 1.0, 0.6, 0.42797379741424524], 
reward next is 0.5720, 
noisyNet noise sample is [array([0.38751104], dtype=float32), 0.8557033]. 
=============================================
[2019-04-09 15:17:25,471] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.633333333333333, 77.66666666666667, 0.0, 0.0, 22.5, 22.96208474060921, -0.1750744324844963, 1.0, 1.0, 45.0, 51.6316881844468], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1964400.0000, 
sim time next is 1965600.0000, 
raw observation next is [-5.0, 79.0, 0.0, 0.0, 22.5, 23.50202768660628, -0.1232451839624529, 1.0, 1.0, 45.0, 43.62377132607405], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.79, 0.0, 0.0, 0.375, 0.45850230721718993, 0.4589182720125157, 1.0, 1.0, 0.6, 0.4362377132607405], 
reward next is 0.5638, 
noisyNet noise sample is [array([1.0280224], dtype=float32), -1.5109179]. 
=============================================
[2019-04-09 15:17:26,264] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.26471416e-09 5.59363741e-07 9.02283209e-05 1.64853109e-08
 4.70451325e-01 2.17554066e-03 5.26485443e-01 1.36454655e-05
 7.30711094e-04 4.64898294e-05 6.00562271e-06], sum to 1.0000
[2019-04-09 15:17:26,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8479
[2019-04-09 15:17:26,296] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 19.0, 20.97283165989599, -0.6940457542742425, 0.0, 1.0, 45.0, 35.237150104404364], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2010000.0000, 
sim time next is 2011200.0000, 
raw observation next is [-6.199999999999999, 87.0, 0.0, 0.0, 19.0, 20.7985972556977, -0.7095828942844863, 0.0, 1.0, 45.0, 36.00550105108829], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.23321643797480837, 0.2634723685718379, 0.0, 1.0, 0.6, 0.3600550105108829], 
reward next is 0.6399, 
noisyNet noise sample is [array([0.5793206], dtype=float32), -1.3793555]. 
=============================================
[2019-04-09 15:17:26,748] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5960178e-11 2.1979229e-08 2.5572647e-06 1.0046796e-11 4.0062258e-01
 3.8062403e-04 5.9895080e-01 4.1346263e-07 3.8369712e-05 4.4132889e-06
 2.0157344e-07], sum to 1.0000
[2019-04-09 15:17:26,753] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8229
[2019-04-09 15:17:26,802] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 81.33333333333333, 0.0, 0.0, 19.0, 22.29247198739409, -0.3261464756134536, 0.0, 1.0, 45.0, 36.02994670455587], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1978800.0000, 
sim time next is 1980000.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 19.0, 22.22599846144211, -0.3387351103043346, 0.0, 1.0, 45.0, 35.70755990343136], 
processed observation next is [1.0, 0.9565217391304348, 0.2908587257617729, 0.83, 0.0, 0.0, 0.08333333333333333, 0.35216653845350915, 0.3870882965652218, 0.0, 1.0, 0.6, 0.3570755990343136], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.12786329], dtype=float32), -1.0805371]. 
=============================================
[2019-04-09 15:17:26,805] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[49.353832]
 [49.99    ]
 [50.042736]
 [51.40022 ]
 [52.06665 ]], R is [[48.29192734]
 [48.4487114 ]
 [48.59609604]
 [48.8151741 ]
 [48.97034836]].
[2019-04-09 15:17:27,013] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.7220487e-11 1.6972010e-08 2.9697032e-06 2.5565342e-11 6.3903290e-01
 8.9486252e-04 3.6000544e-01 9.4389884e-07 6.1244442e-05 1.4076746e-06
 1.2131274e-07], sum to 1.0000
[2019-04-09 15:17:27,013] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3813
[2019-04-09 15:17:27,062] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 81.33333333333333, 0.0, 0.0, 19.0, 22.21042773251545, -0.3527469046540081, 0.0, 1.0, 35.0, 20.03495811942744], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1978800.0000, 
sim time next is 1980000.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 19.0, 22.04088854602491, -0.3844881204881834, 0.0, 1.0, 35.0, 22.157068364796963], 
processed observation next is [1.0, 0.9565217391304348, 0.2908587257617729, 0.83, 0.0, 0.0, 0.08333333333333333, 0.33674071216874246, 0.37183729317060554, 0.0, 1.0, 0.4, 0.22157068364796964], 
reward next is 0.7784, 
noisyNet noise sample is [array([0.05210807], dtype=float32), 0.7189756]. 
=============================================
[2019-04-09 15:17:27,066] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[48.01282 ]
 [48.733448]
 [49.613598]
 [50.818108]
 [51.366917]], R is [[47.18235016]
 [47.51017761]
 [47.79862976]
 [48.05866623]
 [48.28411102]].
[2019-04-09 15:17:27,802] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.1355395e-11 2.2003555e-07 3.9804308e-06 1.1743698e-10 8.8073516e-01
 2.1148229e-03 1.1698836e-01 1.3740384e-06 3.1136318e-05 1.2460336e-04
 2.4357712e-07], sum to 1.0000
[2019-04-09 15:17:27,802] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3312
[2019-04-09 15:17:27,875] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 81.33333333333334, 0.0, 0.0, 19.0, 21.55747704070445, -0.4661308726746465, 0.0, 1.0, 45.0, 48.80438163766722], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1974000.0000, 
sim time next is 1975200.0000, 
raw observation next is [-5.6, 79.66666666666667, 0.0, 0.0, 19.0, 21.60978841959088, -0.4699613099896729, 0.0, 1.0, 35.0, 29.85261150186437], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.30081570163257343, 0.3433462300034424, 0.0, 1.0, 0.4, 0.2985261150186437], 
reward next is 0.7015, 
noisyNet noise sample is [array([-1.397873], dtype=float32), 0.43236884]. 
=============================================
[2019-04-09 15:17:27,882] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.1904604e-10 1.0951917e-07 2.6349722e-05 4.9652688e-10 2.0260435e-01
 2.1052183e-04 7.9683030e-01 3.3703811e-06 2.8921201e-04 3.4945911e-05
 9.0348755e-07], sum to 1.0000
[2019-04-09 15:17:27,882] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7975
[2019-04-09 15:17:27,928] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 87.66666666666667, 0.0, 0.0, 19.0, 21.39830056064316, -0.4793138212000449, 0.0, 1.0, 45.0, 43.21587746523497], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2071200.0000, 
sim time next is 2072400.0000, 
raw observation next is [-4.5, 89.33333333333334, 0.0, 0.0, 19.0, 21.43596815361144, -0.4676380659919406, 0.0, 1.0, 45.0, 39.307236935084134], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.8933333333333334, 0.0, 0.0, 0.08333333333333333, 0.2863306794676201, 0.34412064466935316, 0.0, 1.0, 0.6, 0.39307236935084133], 
reward next is 0.6069, 
noisyNet noise sample is [array([0.88474244], dtype=float32), -0.907295]. 
=============================================
[2019-04-09 15:17:28,181] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7915369e-09 4.1781750e-06 5.1941108e-05 1.9595985e-09 7.7435964e-01
 1.7899470e-02 2.0705841e-01 1.0530430e-05 5.5819593e-04 5.3013664e-05
 4.5697388e-06], sum to 1.0000
[2019-04-09 15:17:28,184] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4880
[2019-04-09 15:17:28,385] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 22.5, 20.74713157501599, -0.7626821201146919, 1.0, 1.0, 35.0, 30.446326778900218], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2013600.0000, 
sim time next is 2014800.0000, 
raw observation next is [-6.199999999999999, 87.0, 0.0, 0.0, 22.5, 20.64734856449601, -0.7845768638330615, 1.0, 1.0, 35.0, 26.08639066716555], 
processed observation next is [1.0, 0.30434782608695654, 0.2908587257617729, 0.87, 0.0, 0.0, 0.375, 0.2206123803746675, 0.23847437872231284, 1.0, 1.0, 0.4, 0.2608639066716555], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7255826], dtype=float32), 1.2674363]. 
=============================================
[2019-04-09 15:17:28,421] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.4626715e-11 1.6954610e-08 1.1989255e-06 9.9629046e-12 8.4661376e-01
 6.4902130e-04 1.5270479e-01 2.8475008e-06 2.3552206e-05 4.7088779e-06
 1.2569899e-07], sum to 1.0000
[2019-04-09 15:17:28,422] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2963
[2019-04-09 15:17:28,529] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 62.0, 80.33333333333334, 0.0, 22.5, 23.58127129760198, -0.1983543546035235, 1.0, 1.0, 35.0, 31.13797177292094], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1956000.0000, 
sim time next is 1957200.0000, 
raw observation next is [-2.8, 62.0, 66.66666666666667, 0.0, 22.5, 23.72937594386693, -0.1950870684996315, 1.0, 1.0, 35.0, 25.82216580536652], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.22222222222222224, 0.0, 0.375, 0.4774479953222442, 0.4349709771667895, 1.0, 1.0, 0.4, 0.2582216580536652], 
reward next is 0.7418, 
noisyNet noise sample is [array([1.6233063], dtype=float32), 1.179687]. 
=============================================
[2019-04-09 15:17:29,237] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6443485e-11 5.8662717e-08 3.2095750e-06 2.2841364e-11 3.5312134e-01
 3.7691515e-04 6.4614445e-01 8.7158560e-06 3.3262072e-04 1.2138377e-05
 5.8369540e-07], sum to 1.0000
[2019-04-09 15:17:29,237] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1799
[2019-04-09 15:17:29,410] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.266666666666667, 76.33333333333334, 9.166666666666666, 1.666666666666667, 22.5, 23.48509133534827, -0.2165564722841641, 1.0, 1.0, 45.0, 44.74320965061021], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 1963200.0000, 
sim time next is 1964400.0000, 
raw observation next is [-4.633333333333333, 77.66666666666667, 0.0, 0.0, 22.5, 23.07171591157998, -0.2219444371249563, 1.0, 1.0, 35.0, 32.201100170849344], 
processed observation next is [1.0, 0.7391304347826086, 0.3342566943674977, 0.7766666666666667, 0.0, 0.0, 0.375, 0.4226429926316649, 0.42601852095834786, 1.0, 1.0, 0.4, 0.32201100170849345], 
reward next is 0.6780, 
noisyNet noise sample is [array([-0.02088563], dtype=float32), 0.6049736]. 
=============================================
[2019-04-09 15:17:29,536] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1506936e-09 4.5812297e-07 1.4814936e-05 5.4502242e-10 3.2759738e-01
 1.8867034e-03 6.7040485e-01 2.8715133e-06 5.4121268e-05 3.8256625e-05
 5.3557443e-07], sum to 1.0000
[2019-04-09 15:17:29,536] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1935
[2019-04-09 15:17:29,790] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 22.5, 20.30708434682372, -0.8169786042433285, 1.0, 1.0, 45.0, 47.89255730718146], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2013600.0000, 
sim time next is 2014800.0000, 
raw observation next is [-6.199999999999999, 87.0, 0.0, 0.0, 22.5, 20.64628741630811, -0.7918769095625144, 1.0, 1.0, 35.0, 35.279595109338224], 
processed observation next is [1.0, 0.30434782608695654, 0.2908587257617729, 0.87, 0.0, 0.0, 0.375, 0.22052395135900907, 0.23604103014582853, 1.0, 1.0, 0.4, 0.35279595109338224], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.44213012], dtype=float32), -0.16048487]. 
=============================================
[2019-04-09 15:17:30,532] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.5109337e-12 4.6676707e-09 1.8168374e-06 4.4304314e-11 2.9121596e-01
 4.3485747e-04 7.0828980e-01 2.1185033e-07 5.3542633e-05 3.7783573e-06
 3.8065931e-08], sum to 1.0000
[2019-04-09 15:17:30,532] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9737
[2019-04-09 15:17:30,662] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 81.33333333333334, 0.0, 0.0, 19.0, 22.90542187003637, -0.2288507822911051, 0.0, 1.0, 45.0, 38.91688951415193], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 1974000.0000, 
sim time next is 1975200.0000, 
raw observation next is [-5.6, 79.66666666666667, 0.0, 0.0, 19.0, 22.76498451028742, -0.251494666409328, 0.0, 1.0, 45.0, 35.43996110010811], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.39708204252395163, 0.416168444530224, 0.0, 1.0, 0.6, 0.3543996110010811], 
reward next is 0.6456, 
noisyNet noise sample is [array([-1.1312461], dtype=float32), -1.335633]. 
=============================================
[2019-04-09 15:17:31,041] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9563540e-09 9.6950907e-07 2.0160091e-04 4.1875037e-09 5.2805340e-01
 8.3116582e-03 4.6286145e-01 1.7392538e-05 4.3873917e-04 1.1077962e-04
 3.9634833e-06], sum to 1.0000
[2019-04-09 15:17:31,042] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9339
[2019-04-09 15:17:31,097] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 91.0, 0.0, 0.0, 19.0, 21.45247822349709, -0.5163466573203609, 0.0, 1.0, 45.0, 37.773219638120395], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2088000.0000, 
sim time next is 2089200.0000, 
raw observation next is [-5.8, 89.66666666666667, 0.0, 0.0, 19.0, 21.44494139743018, -0.5456390114217513, 0.0, 1.0, 45.0, 37.369593645062906], 
processed observation next is [1.0, 0.17391304347826086, 0.30193905817174516, 0.8966666666666667, 0.0, 0.0, 0.08333333333333333, 0.28707844978584846, 0.3181203295260829, 0.0, 1.0, 0.6, 0.37369593645062904], 
reward next is 0.6263, 
noisyNet noise sample is [array([-0.96005046], dtype=float32), -1.3110055]. 
=============================================
[2019-04-09 15:17:33,005] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4661034e-10 7.9783050e-08 8.5000165e-06 9.7903415e-11 6.9963163e-01
 6.5053598e-04 2.9942015e-01 3.0057149e-06 2.7788646e-04 7.8481289e-06
 3.0456948e-07], sum to 1.0000
[2019-04-09 15:17:33,005] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4222
[2019-04-09 15:17:33,062] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.1, 83.33333333333334, 0.0, 0.0, 19.0, 22.38407760581738, -0.3628302044401406, 0.0, 1.0, 35.0, 23.6801451285149], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2067600.0000, 
sim time next is 2068800.0000, 
raw observation next is [-4.3, 84.66666666666666, 0.0, 0.0, 19.0, 21.99394698717305, -0.42418271051018, 0.0, 1.0, 35.0, 20.095343303920018], 
processed observation next is [1.0, 0.9565217391304348, 0.34349030470914127, 0.8466666666666666, 0.0, 0.0, 0.08333333333333333, 0.3328289155977542, 0.3586057631632733, 0.0, 1.0, 0.4, 0.20095343303920019], 
reward next is 0.7990, 
noisyNet noise sample is [array([0.11112888], dtype=float32), -1.1272594]. 
=============================================
[2019-04-09 15:17:33,516] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.8040755e-12 2.1779428e-08 3.7954815e-06 3.0409474e-11 4.3193248e-01
 1.0482490e-03 5.6692296e-01 1.4831693e-07 5.6108001e-05 3.6190759e-05
 2.0422140e-08], sum to 1.0000
[2019-04-09 15:17:33,517] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1662
[2019-04-09 15:17:33,710] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 68.0, 14.0, 0.0, 22.5, 24.73562501285222, 0.1214921306944812, 1.0, 1.0, 45.0, 41.220553667682864], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2134800.0000, 
sim time next is 2136000.0000, 
raw observation next is [-4.666666666666667, 69.0, 5.999999999999999, 0.0, 22.5, 24.9751982562126, 0.1000508182917125, 1.0, 1.0, 35.0, 31.742384948964776], 
processed observation next is [1.0, 0.7391304347826086, 0.3333333333333333, 0.69, 0.019999999999999997, 0.0, 0.375, 0.5812665213510501, 0.5333502727639042, 1.0, 1.0, 0.4, 0.31742384948964775], 
reward next is 0.6826, 
noisyNet noise sample is [array([-0.6578643], dtype=float32), -0.7117311]. 
=============================================
[2019-04-09 15:17:33,716] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[51.650345]
 [51.459114]
 [51.06553 ]
 [50.613308]
 [50.4552  ]], R is [[51.93828201]
 [52.00669479]
 [52.06890488]
 [52.12993622]
 [52.18737411]].
[2019-04-09 15:17:33,904] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0479179e-10 3.2981170e-08 1.7020366e-05 5.3648329e-11 2.2780508e-01
 2.1578169e-03 7.6995671e-01 2.1046822e-06 4.9356047e-05 1.1843338e-05
 1.2246234e-07], sum to 1.0000
[2019-04-09 15:17:33,905] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0896
[2019-04-09 15:17:34,012] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.733333333333333, 84.0, 74.5, 0.0, 22.5, 22.48172226679995, -0.4441677122166955, 1.0, 1.0, 35.0, 27.41890810158424], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2022000.0000, 
sim time next is 2023200.0000, 
raw observation next is [-5.6, 83.0, 85.5, 0.0, 22.5, 22.56834599127957, -0.4499404530856623, 1.0, 1.0, 35.0, 24.561088140724898], 
processed observation next is [1.0, 0.43478260869565216, 0.30747922437673136, 0.83, 0.285, 0.0, 0.375, 0.3806954992732976, 0.3500198489714459, 1.0, 1.0, 0.4, 0.245610881407249], 
reward next is 0.7544, 
noisyNet noise sample is [array([0.3236992], dtype=float32), 0.7903203]. 
=============================================
[2019-04-09 15:17:34,121] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.22026500e-10 1.05027574e-07 7.23135236e-06 2.69087322e-11
 2.60103762e-01 2.00714567e-04 7.39637971e-01 6.68600364e-07
 4.30273176e-05 5.85646285e-06 6.51374364e-07], sum to 1.0000
[2019-04-09 15:17:34,122] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3084
[2019-04-09 15:17:34,150] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 84.66666666666667, 0.0, 0.0, 19.0, 22.3201245534559, -0.2883343213086196, 0.0, 1.0, 45.0, 36.4416083882894], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2064000.0000, 
sim time next is 2065200.0000, 
raw observation next is [-3.899999999999999, 83.33333333333334, 0.0, 0.0, 19.0, 22.33693283155867, -0.2797561773771943, 0.0, 1.0, 45.0, 36.84772490627325], 
processed observation next is [1.0, 0.9130434782608695, 0.35457063711911363, 0.8333333333333335, 0.0, 0.0, 0.08333333333333333, 0.3614110692965558, 0.4067479408742685, 0.0, 1.0, 0.6, 0.3684772490627325], 
reward next is 0.6315, 
noisyNet noise sample is [array([0.27564764], dtype=float32), 0.29589123]. 
=============================================
[2019-04-09 15:17:34,331] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3912453e-11 1.6519881e-08 1.1432185e-05 1.6271434e-11 2.2638007e-01
 1.5978336e-03 7.7196813e-01 1.2153347e-06 3.4290944e-05 6.9241901e-06
 5.9598726e-08], sum to 1.0000
[2019-04-09 15:17:34,331] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0199
[2019-04-09 15:17:34,487] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 83.0, 109.5, 0.0, 22.5, 22.8755728997464, -0.3339471966323038, 1.0, 1.0, 45.0, 46.089546274177266], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2025600.0000, 
sim time next is 2026800.0000, 
raw observation next is [-5.6, 83.0, 124.5, 0.0, 22.5, 23.13910824141078, -0.2953592884009184, 1.0, 1.0, 35.0, 31.934358046760675], 
processed observation next is [1.0, 0.4782608695652174, 0.30747922437673136, 0.83, 0.415, 0.0, 0.375, 0.4282590201175651, 0.40154690386636055, 1.0, 1.0, 0.4, 0.31934358046760675], 
reward next is 0.6807, 
noisyNet noise sample is [array([0.3236992], dtype=float32), 0.7903203]. 
=============================================
[2019-04-09 15:17:34,622] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.2731204e-10 8.9801333e-08 4.2159005e-05 5.3901689e-10 2.7904829e-01
 2.0635850e-03 7.1803719e-01 1.2632283e-05 7.6509878e-04 2.5323558e-05
 5.5914361e-06], sum to 1.0000
[2019-04-09 15:17:34,623] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3439
[2019-04-09 15:17:34,666] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 86.0, 0.0, 0.0, 19.0, 21.77360555342807, -0.4379700527775888, 0.0, 1.0, 45.0, 36.39993416724256], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2080800.0000, 
sim time next is 2082000.0000, 
raw observation next is [-4.666666666666667, 86.0, 0.0, 0.0, 19.0, 21.83464017453034, -0.4376871166017693, 0.0, 1.0, 45.0, 36.371819844381974], 
processed observation next is [1.0, 0.08695652173913043, 0.3333333333333333, 0.86, 0.0, 0.0, 0.08333333333333333, 0.31955334787752826, 0.3541042944660769, 0.0, 1.0, 0.6, 0.36371819844381975], 
reward next is 0.6363, 
noisyNet noise sample is [array([2.3951619], dtype=float32), 0.40239006]. 
=============================================
[2019-04-09 15:17:34,671] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[42.61724 ]
 [43.013687]
 [43.808933]
 [43.88521 ]
 [44.03549 ]], R is [[42.08979034]
 [42.30489349]
 [42.51763153]
 [42.72480774]
 [42.93758392]].
[2019-04-09 15:17:35,351] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9260530e-13 2.7651330e-09 5.5121296e-07 3.6136788e-12 7.7950644e-01
 1.1248434e-04 2.2036320e-01 1.7290384e-07 9.8350547e-06 7.2578168e-06
 9.8967007e-08], sum to 1.0000
[2019-04-09 15:17:35,351] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5908
[2019-04-09 15:17:35,508] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 79.0, 152.0, 0.0, 22.5, 23.67970031937157, -0.1455961416052516, 1.0, 1.0, 45.0, 41.48835581414809], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2034000.0000, 
sim time next is 2035200.0000, 
raw observation next is [-4.300000000000001, 79.0, 149.3333333333333, 0.0, 22.5, 23.27234844049642, -0.1886232233486299, 1.0, 1.0, 35.0, 32.25169908177501], 
processed observation next is [1.0, 0.5652173913043478, 0.34349030470914127, 0.79, 0.4977777777777776, 0.0, 0.375, 0.43936237004136824, 0.4371255922171233, 1.0, 1.0, 0.4, 0.3225169908177501], 
reward next is 0.6775, 
noisyNet noise sample is [array([0.47338396], dtype=float32), -0.2928599]. 
=============================================
[2019-04-09 15:17:35,631] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.58166077e-10 1.10080265e-07 1.57758222e-05 1.07242423e-10
 6.45855188e-01 5.03298594e-03 3.48884225e-01 7.69142162e-07
 2.07491030e-04 2.94193228e-06 4.64323676e-07], sum to 1.0000
[2019-04-09 15:17:35,631] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1082
[2019-04-09 15:17:35,724] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 22.5, 23.00705318087729, -0.1577699147696559, 0.0, 1.0, 35.0, 26.318297148512563], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2059200.0000, 
sim time next is 2060400.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 19.0, 22.83532906859333, -0.1854414009667818, 0.0, 1.0, 35.0, 24.577031467081383], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.08333333333333333, 0.4029440890494443, 0.4381861996777394, 0.0, 1.0, 0.4, 0.24577031467081384], 
reward next is 0.7542, 
noisyNet noise sample is [array([-2.7448676], dtype=float32), -0.86352736]. 
=============================================
[2019-04-09 15:17:36,458] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1412625e-12 2.6210389e-09 1.1345356e-06 5.4822222e-13 1.1520724e-01
 3.6828924e-04 8.8441283e-01 1.1443474e-07 8.9820696e-06 1.3400338e-06
 4.3824877e-09], sum to 1.0000
[2019-04-09 15:17:36,458] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4975
[2019-04-09 15:17:36,560] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.1, 83.33333333333334, 64.5, 0.0, 22.5, 24.01798820040639, -0.03608209144265228, 1.0, 1.0, 45.0, 42.551516885620416], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2043600.0000, 
sim time next is 2044800.0000, 
raw observation next is [-3.9, 82.0, 51.5, 0.0, 22.5, 24.20513253718126, -0.02720975561288261, 1.0, 1.0, 45.0, 41.725961266084155], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.17166666666666666, 0.0, 0.375, 0.5170943780984384, 0.4909300814623725, 1.0, 1.0, 0.6, 0.41725961266084155], 
reward next is 0.5827, 
noisyNet noise sample is [array([0.30984354], dtype=float32), 0.41421914]. 
=============================================
[2019-04-09 15:17:36,805] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0981142e-12 1.2549228e-09 4.8186625e-06 5.9269110e-13 2.7225435e-01
 5.7341857e-04 7.2711474e-01 7.4460115e-07 5.1305873e-05 5.2399628e-07
 3.4219035e-08], sum to 1.0000
[2019-04-09 15:17:36,805] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1324
[2019-04-09 15:17:36,887] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 82.0, 6.999999999999999, 0.0, 22.5, 24.24397402052868, -0.04854703316230911, 1.0, 1.0, 35.0, 32.82535772152303], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2049600.0000, 
sim time next is 2050800.0000, 
raw observation next is [-3.899999999999999, 82.0, 0.0, 0.0, 22.5, 23.58549962660864, -0.0979655530016282, 1.0, 1.0, 35.0, 28.255577358471], 
processed observation next is [1.0, 0.7391304347826086, 0.35457063711911363, 0.82, 0.0, 0.0, 0.375, 0.46545830221738677, 0.46734481566612396, 1.0, 1.0, 0.4, 0.28255577358471], 
reward next is 0.7174, 
noisyNet noise sample is [array([-0.4792452], dtype=float32), 1.3614012]. 
=============================================
[2019-04-09 15:17:37,637] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2782356e-09 1.0981281e-06 1.2638968e-04 7.3677251e-09 8.0761528e-01
 5.7296576e-03 1.8591885e-01 4.4730070e-05 5.2474922e-04 3.5177203e-05
 4.0557329e-06], sum to 1.0000
[2019-04-09 15:17:37,637] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9082
[2019-04-09 15:17:37,769] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.633333333333333, 81.0, 89.0, 50.5, 22.5, 21.22410867043021, -0.6268655074343386, 1.0, 1.0, 35.0, 32.40424518087121], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2104800.0000, 
sim time next is 2106000.0000, 
raw observation next is [-7.8, 82.0, 123.0, 77.5, 22.5, 21.60286962197548, -0.5147441975327115, 1.0, 1.0, 45.0, 47.23255686015254], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.82, 0.41, 0.0856353591160221, 0.375, 0.30023913516462325, 0.3284186008224295, 1.0, 1.0, 0.6, 0.4723255686015254], 
reward next is 0.5521, 
noisyNet noise sample is [array([-0.8278209], dtype=float32), 0.13206056]. 
=============================================
[2019-04-09 15:17:37,773] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[41.99647 ]
 [41.688843]
 [39.82397 ]
 [39.33445 ]
 [39.148884]], R is [[43.65127182]
 [43.79763794]
 [43.59222794]
 [43.15630722]
 [42.72474289]].
[2019-04-09 15:17:38,260] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9450435e-12 1.4728601e-08 1.1284183e-05 1.4199655e-12 2.5330222e-01
 7.1213534e-04 7.4590272e-01 3.3062281e-07 6.0300972e-05 1.0903689e-05
 5.1594313e-08], sum to 1.0000
[2019-04-09 15:17:38,260] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8263
[2019-04-09 15:17:38,430] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 84.66666666666666, 0.0, 0.0, 22.5, 23.77481930427891, -0.02096703875336646, 0.0, 1.0, 45.0, 41.789019830636796], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2058000.0000, 
sim time next is 2059200.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 22.5, 23.69422019682655, -0.02551296141404616, 0.0, 1.0, 45.0, 41.6819424513007], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.375, 0.4745183497355458, 0.4914956795286513, 0.0, 1.0, 0.6, 0.416819424513007], 
reward next is 0.5832, 
noisyNet noise sample is [array([0.37964702], dtype=float32), 0.5617277]. 
=============================================
[2019-04-09 15:17:38,527] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.1039936e-12 2.0498485e-08 1.3361263e-05 2.1354481e-12 2.5068614e-01
 7.9530850e-04 7.4843156e-01 3.8363123e-07 6.0424394e-05 1.2694918e-05
 6.6010578e-08], sum to 1.0000
[2019-04-09 15:17:38,527] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5307
[2019-04-09 15:17:38,667] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 19.0, 23.64418859221998, -0.03459176108004806, 0.0, 1.0, 45.0, 38.371417587740765], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2060400.0000, 
sim time next is 2061600.0000, 
raw observation next is [-3.899999999999999, 86.0, 0.0, 0.0, 19.0, 23.53977100048807, -0.05288457502957255, 0.0, 1.0, 45.0, 34.81549170394136], 
processed observation next is [1.0, 0.8695652173913043, 0.35457063711911363, 0.86, 0.0, 0.0, 0.08333333333333333, 0.46164758337400585, 0.4823718083234758, 0.0, 1.0, 0.6, 0.3481549170394136], 
reward next is 0.6518, 
noisyNet noise sample is [array([0.37964702], dtype=float32), 0.5617277]. 
=============================================
[2019-04-09 15:17:38,727] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0715988e-11 7.8758324e-09 7.5622199e-07 1.5413375e-11 4.8321447e-01
 8.4104639e-04 5.1590037e-01 2.5214598e-07 3.8216862e-05 4.7465337e-06
 1.0700794e-07], sum to 1.0000
[2019-04-09 15:17:38,727] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3994
[2019-04-09 15:17:38,867] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.899999999999999, 86.0, 0.0, 0.0, 19.0, 23.06552688729997, -0.1694819085404202, 0.0, 1.0, 35.0, 29.47769584420346], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2061600.0000, 
sim time next is 2062800.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 19.0, 22.85403985425549, -0.2137653365486979, 0.0, 1.0, 35.0, 26.054351981846136], 
processed observation next is [1.0, 0.9130434782608695, 0.3545706371191136, 0.86, 0.0, 0.0, 0.08333333333333333, 0.40450332118795745, 0.42874488781710074, 0.0, 1.0, 0.4, 0.26054351981846136], 
reward next is 0.7395, 
noisyNet noise sample is [array([-0.7317008], dtype=float32), -1.5025543]. 
=============================================
[2019-04-09 15:17:39,442] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.9280127e-10 7.1541064e-07 9.5329116e-05 1.0689756e-09 7.0266628e-01
 1.4175180e-03 2.9538095e-01 3.1940148e-05 3.9125062e-04 1.5007326e-05
 9.9800161e-07], sum to 1.0000
[2019-04-09 15:17:39,447] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2826
[2019-04-09 15:17:39,469] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 89.33333333333334, 0.0, 0.0, 19.0, 21.56320511029863, -0.5078641799534288, 0.0, 1.0, 35.0, 23.268911932651356], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2078400.0000, 
sim time next is 2079600.0000, 
raw observation next is [-4.5, 87.66666666666666, 0.0, 0.0, 19.0, 21.44502154175019, -0.5325744160277837, 0.0, 1.0, 35.0, 23.705390352475106], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.8766666666666666, 0.0, 0.0, 0.08333333333333333, 0.2870851284791825, 0.32247519465740543, 0.0, 1.0, 0.4, 0.23705390352475106], 
reward next is 0.7629, 
noisyNet noise sample is [array([1.9083673], dtype=float32), 0.6297349]. 
=============================================
[2019-04-09 15:17:39,995] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0548772e-08 1.0393402e-06 2.4002527e-05 8.4554097e-09 8.2821959e-01
 2.9298931e-03 1.6833733e-01 2.1803442e-05 3.0885692e-04 1.3633301e-04
 2.1150707e-05], sum to 1.0000
[2019-04-09 15:17:39,996] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4679
[2019-04-09 15:17:40,038] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 88.33333333333334, 0.0, 0.0, 19.0, 21.05951412868716, -0.6490047302164815, 0.0, 1.0, 35.0, 26.87393061951608], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2090400.0000, 
sim time next is 2091600.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 20.80163827055644, -0.7022044783999561, 0.0, 1.0, 35.0, 23.403157261584802], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.23346985587970348, 0.26593184053334795, 0.0, 1.0, 0.4, 0.23403157261584803], 
reward next is 0.7660, 
noisyNet noise sample is [array([-1.1681174], dtype=float32), -0.9815308]. 
=============================================
[2019-04-09 15:17:40,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6943654e-12 1.8304705e-08 6.9968951e-07 1.5388813e-12 5.6736290e-01
 8.5539832e-05 4.3254256e-01 5.3535945e-07 5.7006241e-06 1.8013891e-06
 3.0470656e-07], sum to 1.0000
[2019-04-09 15:17:40,364] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8422
[2019-04-09 15:17:40,482] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 68.0, 129.0, 0.0, 22.5, 24.61278208842143, 0.07710832522647287, 1.0, 1.0, 35.0, 24.626835835231113], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2125200.0000, 
sim time next is 2126400.0000, 
raw observation next is [-5.199999999999999, 68.0, 118.5, 0.0, 22.5, 24.60943919232206, -0.01778743162460335, 1.0, 1.0, 45.0, 42.269833435149636], 
processed observation next is [1.0, 0.6086956521739131, 0.31855955678670367, 0.68, 0.395, 0.0, 0.375, 0.5507865993601717, 0.4940708561251322, 1.0, 1.0, 0.6, 0.42269833435149634], 
reward next is 0.5773, 
noisyNet noise sample is [array([0.15945216], dtype=float32), 0.7658313]. 
=============================================
[2019-04-09 15:17:41,584] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3735602e-11 5.1151305e-09 2.0758564e-06 5.7176620e-12 9.6968636e-02
 5.2633969e-04 9.0246511e-01 8.6750396e-07 2.7347724e-05 9.5193436e-06
 1.2482678e-07], sum to 1.0000
[2019-04-09 15:17:41,584] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2230
[2019-04-09 15:17:41,701] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.1, 69.0, 140.5, 0.0, 22.5, 23.45968032031134, -0.1979059825509922, 1.0, 1.0, 35.0, 25.00362924250804], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2202000.0000, 
sim time next is 2203200.0000, 
raw observation next is [-3.9, 68.0, 135.5, 0.0, 22.5, 23.4553050927316, -0.1447433738915155, 1.0, 1.0, 45.0, 52.18741301706576], 
processed observation next is [1.0, 0.5217391304347826, 0.3545706371191136, 0.68, 0.45166666666666666, 0.0, 0.375, 0.45460875772763326, 0.4517522087028281, 1.0, 1.0, 0.6, 0.5218741301706575], 
reward next is 0.4781, 
noisyNet noise sample is [array([2.0767446], dtype=float32), 1.0306853]. 
=============================================
[2019-04-09 15:17:42,594] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2941178e-11 5.4908940e-09 1.7032779e-06 4.7765277e-12 9.5696175e-01
 5.5466726e-04 4.2447791e-02 1.2043865e-06 2.2255806e-05 1.0412570e-05
 1.7438721e-07], sum to 1.0000
[2019-04-09 15:17:42,594] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8924
[2019-04-09 15:17:42,680] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 75.0, 250.5, 80.5, 22.5, 23.48359168347664, -0.1815146783195049, 1.0, 1.0, 35.0, 24.600956480948312], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2113200.0000, 
sim time next is 2114400.0000, 
raw observation next is [-7.100000000000001, 71.33333333333334, 278.8333333333334, 94.16666666666667, 22.5, 23.48313931594378, -0.1698010854428731, 1.0, 1.0, 35.0, 22.047323915657792], 
processed observation next is [1.0, 0.4782608695652174, 0.26592797783933514, 0.7133333333333334, 0.9294444444444447, 0.10405156537753224, 0.375, 0.4569282763286484, 0.443399638185709, 1.0, 1.0, 0.4, 0.2204732391565779], 
reward next is 0.7795, 
noisyNet noise sample is [array([1.9173217], dtype=float32), 1.3890334]. 
=============================================
[2019-04-09 15:17:43,122] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.84790226e-12 1.91048628e-07 1.86627381e-06 3.09192071e-13
 6.83753490e-01 5.76601196e-05 3.16175252e-01 2.69146966e-07
 1.07159685e-05 4.43699236e-07 4.87137051e-08], sum to 1.0000
[2019-04-09 15:17:43,122] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5634
[2019-04-09 15:17:43,219] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.3, 70.0, 29.66666666666667, 0.0, 22.5, 24.80151273579572, 0.1191611042253119, 1.0, 1.0, 45.0, 43.01301379147886], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2220000.0000, 
sim time next is 2221200.0000, 
raw observation next is [-4.5, 71.0, 19.0, 0.0, 22.5, 24.88667647994172, 0.1221457345311097, 1.0, 1.0, 35.0, 30.9394979866302], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.71, 0.06333333333333334, 0.0, 0.375, 0.5738897066618099, 0.5407152448437033, 1.0, 1.0, 0.4, 0.309394979866302], 
reward next is 0.6906, 
noisyNet noise sample is [array([0.26094854], dtype=float32), 0.6292748]. 
=============================================
[2019-04-09 15:17:45,021] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.8437738e-12 6.0214673e-09 4.6421667e-07 1.5434570e-11 2.4619801e-01
 1.3388699e-04 7.5364399e-01 3.0295126e-08 2.0474143e-05 3.0725494e-06
 2.4745791e-08], sum to 1.0000
[2019-04-09 15:17:45,021] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2646
[2019-04-09 15:17:45,167] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.866666666666666, 70.66666666666666, 0.0, 0.0, 22.5, 24.06801888975079, 0.08095953343360034, 1.0, 1.0, 45.0, 41.982547840374494], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2230800.0000, 
sim time next is 2232000.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 22.5, 24.01950035163794, 0.08362488271669054, 1.0, 1.0, 45.0, 43.395120577741515], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.71, 0.0, 0.0, 0.375, 0.5016250293031618, 0.5278749609055635, 1.0, 1.0, 0.6, 0.43395120577741514], 
reward next is 0.5660, 
noisyNet noise sample is [array([-1.0687784], dtype=float32), 1.8692985]. 
=============================================
[2019-04-09 15:17:45,198] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[55.61957 ]
 [55.960163]
 [56.481426]
 [57.216663]
 [57.672817]], R is [[55.43513489]
 [55.46095657]
 [55.5966568 ]
 [55.61808777]
 [55.63462448]].
[2019-04-09 15:17:45,256] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.4532481e-12 2.9624236e-09 6.2507019e-07 1.1509952e-12 5.0039113e-01
 4.1623588e-04 4.9918333e-01 1.9552864e-07 5.0621006e-06 1.0609911e-06
 2.3541224e-06], sum to 1.0000
[2019-04-09 15:17:45,257] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6855
[2019-04-09 15:17:45,403] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.1, 69.0, 55.33333333333333, 47.49999999999999, 22.5, 25.32796586468647, 0.259652169844205, 1.0, 1.0, 45.0, 42.47596514203322], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2218800.0000, 
sim time next is 2220000.0000, 
raw observation next is [-4.3, 70.0, 29.66666666666667, 0.0, 22.5, 25.07382377851543, 0.1934488014355322, 1.0, 1.0, 45.0, 41.95398304554797], 
processed observation next is [1.0, 0.6956521739130435, 0.34349030470914127, 0.7, 0.0988888888888889, 0.0, 0.375, 0.5894853148762857, 0.5644829338118441, 1.0, 1.0, 0.6, 0.4195398304554797], 
reward next is 0.5805, 
noisyNet noise sample is [array([0.93780416], dtype=float32), 0.08353493]. 
=============================================
[2019-04-09 15:17:45,407] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[56.050762]
 [55.832546]
 [55.524742]
 [55.547646]
 [55.555294]], R is [[56.05577087]
 [56.07045364]
 [56.07155991]
 [56.27201843]
 [56.43765259]].
[2019-04-09 15:17:45,532] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.48692143e-11 3.28866356e-08 8.54176142e-06 1.20068053e-11
 8.70615244e-01 1.68872252e-03 1.27572477e-01 4.50265389e-07
 1.08962420e-04 5.40641895e-06 1.14323086e-07], sum to 1.0000
[2019-04-09 15:17:45,532] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4097
[2019-04-09 15:17:45,682] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.300000000000001, 70.0, 138.6666666666667, 0.0, 22.5, 23.71316533397502, -0.1530913065899351, 1.0, 1.0, 35.0, 26.00688550604088], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2200800.0000, 
sim time next is 2202000.0000, 
raw observation next is [-4.1, 69.0, 140.5, 0.0, 22.5, 23.72976960736828, -0.1472187165921099, 1.0, 1.0, 35.0, 23.33527061558404], 
processed observation next is [1.0, 0.4782608695652174, 0.3490304709141275, 0.69, 0.4683333333333333, 0.0, 0.375, 0.47748080061402326, 0.4509270944692967, 1.0, 1.0, 0.4, 0.2333527061558404], 
reward next is 0.7666, 
noisyNet noise sample is [array([0.21342985], dtype=float32), 1.3108954]. 
=============================================
[2019-04-09 15:17:45,709] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[51.871128]
 [51.14144 ]
 [51.08282 ]
 [50.10204 ]
 [48.84039 ]], R is [[52.73849487]
 [52.95104218]
 [53.11082077]
 [53.14015961]
 [53.34228516]].
[2019-04-09 15:17:46,224] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.8543105e-12 1.4081754e-08 5.4329468e-07 5.4165179e-12 4.2314565e-01
 2.1134655e-03 5.7471871e-01 1.0522484e-06 1.3950835e-05 6.4540586e-06
 1.7567908e-07], sum to 1.0000
[2019-04-09 15:17:46,224] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2826
[2019-04-09 15:17:46,304] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 49.00000000000001, 227.8333333333333, 69.83333333333333, 22.5, 23.85157874082482, -0.07436310370953479, 1.0, 1.0, 35.0, 28.2066812992151], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2294400.0000, 
sim time next is 2295600.0000, 
raw observation next is [-0.9666666666666668, 47.0, 204.3333333333333, 67.5, 22.5, 24.12654914284654, -0.04879515010139829, 1.0, 1.0, 35.0, 23.239675887633155], 
processed observation next is [1.0, 0.5652173913043478, 0.43582640812557716, 0.47, 0.681111111111111, 0.07458563535911603, 0.375, 0.5105457619038782, 0.48373494996620053, 1.0, 1.0, 0.4, 0.23239675887633154], 
reward next is 0.7676, 
noisyNet noise sample is [array([1.6063796], dtype=float32), -0.073128864]. 
=============================================
[2019-04-09 15:17:46,356] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8786213e-10 1.0939763e-07 1.8793489e-05 1.3105824e-09 8.7992102e-01
 1.1544457e-03 1.1884940e-01 3.9350452e-06 2.9036004e-05 1.9786055e-05
 3.5684750e-06], sum to 1.0000
[2019-04-09 15:17:46,357] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4533
[2019-04-09 15:17:46,381] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 80.0, 0.0, 0.0, 19.0, 21.69671384770931, -0.4595927467314239, 0.0, 1.0, 35.0, 23.09315081318406], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2162400.0000, 
sim time next is 2163600.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 19.0, 21.58817858042364, -0.4932257085026704, 0.0, 1.0, 35.0, 23.54478254460755], 
processed observation next is [1.0, 0.043478260869565216, 0.26038781163434904, 0.79, 0.0, 0.0, 0.08333333333333333, 0.29901488170197005, 0.3355914304991099, 0.0, 1.0, 0.4, 0.2354478254460755], 
reward next is 0.7646, 
noisyNet noise sample is [array([1.7246633], dtype=float32), -0.2074354]. 
=============================================
[2019-04-09 15:17:47,243] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2142012e-09 5.1287316e-07 1.2162930e-05 2.2888353e-09 2.4842685e-01
 2.9156837e-04 7.5065792e-01 3.5299112e-05 4.5577419e-04 9.9677680e-05
 2.0212219e-05], sum to 1.0000
[2019-04-09 15:17:47,243] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8716
[2019-04-09 15:17:47,343] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 21.71585451778478, -0.4369184544358992, 0.0, 1.0, 45.0, 38.18573040862759], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2169600.0000, 
sim time next is 2170800.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 19.0, 21.90616204534873, -0.4276365184081377, 0.0, 1.0, 45.0, 36.48382269219623], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.3255135037790608, 0.3574544938639541, 0.0, 1.0, 0.6, 0.36483822692196227], 
reward next is 0.6352, 
noisyNet noise sample is [array([-0.6393114], dtype=float32), 0.030953923]. 
=============================================
[2019-04-09 15:17:47,398] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.05530065e-08 6.23236986e-07 1.53966412e-05 2.92131719e-09
 2.54693419e-01 3.50607792e-04 7.44215369e-01 4.38827046e-05
 5.30255900e-04 1.23284364e-04 2.71105873e-05], sum to 1.0000
[2019-04-09 15:17:47,398] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9784
[2019-04-09 15:17:47,432] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 19.0, 21.90616204534873, -0.4276365184081377, 0.0, 1.0, 45.0, 36.48382269219623], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2170800.0000, 
sim time next is 2172000.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 21.94400629957499, -0.4539456919119065, 0.0, 1.0, 45.0, 36.598189613416096], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.3286671916312492, 0.3486847693626978, 0.0, 1.0, 0.6, 0.365981896134161], 
reward next is 0.6340, 
noisyNet noise sample is [array([-0.6393114], dtype=float32), 0.030953923]. 
=============================================
[2019-04-09 15:17:47,442] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[40.065945]
 [40.72769 ]
 [41.03987 ]
 [41.57022 ]
 [42.863483]], R is [[40.53080368]
 [40.76065826]
 [40.97119141]
 [41.18763351]
 [41.56895447]].
[2019-04-09 15:17:49,733] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.5665867e-10 7.6758738e-08 7.8955663e-06 3.2761729e-10 5.4121840e-01
 4.3279221e-04 4.5820302e-01 5.7991392e-06 8.6854590e-05 4.4782733e-05
 4.0293872e-07], sum to 1.0000
[2019-04-09 15:17:49,733] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1421
[2019-04-09 15:17:49,784] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.700000000000001, 77.0, 0.0, 0.0, 19.0, 22.44527468870705, -0.2712414677621598, 0.0, 1.0, 45.0, 37.38268917526159], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2247600.0000, 
sim time next is 2248800.0000, 
raw observation next is [-6.700000000000001, 76.0, 0.0, 0.0, 19.0, 22.49341568498203, -0.2870165042598629, 0.0, 1.0, 35.0, 28.856406101404488], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.76, 0.0, 0.0, 0.08333333333333333, 0.37445130708183577, 0.404327831913379, 0.0, 1.0, 0.4, 0.28856406101404486], 
reward next is 0.7114, 
noisyNet noise sample is [array([0.21562932], dtype=float32), -1.0298873]. 
=============================================
[2019-04-09 15:17:51,103] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5175276e-08 3.9897468e-06 2.5752376e-04 6.1877463e-09 6.5622324e-01
 8.2402015e-03 3.3316773e-01 8.5965890e-05 1.7919539e-03 2.2305343e-04
 6.3618704e-06], sum to 1.0000
[2019-04-09 15:17:51,103] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1327
[2019-04-09 15:17:51,200] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 22.5, 19.97392908415951, -0.8693893761848349, 1.0, 1.0, 35.0, 29.53128301380859], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2272800.0000, 
sim time next is 2274000.0000, 
raw observation next is [-9.5, 91.0, 9.999999999999998, 19.33333333333334, 22.5, 20.03799423673537, -0.8556217245845325, 1.0, 1.0, 35.0, 29.854909556666474], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.033333333333333326, 0.021362799263351755, 0.375, 0.1698328530612807, 0.21479275847182253, 1.0, 1.0, 0.4, 0.29854909556666476], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6261473], dtype=float32), 1.8715756]. 
=============================================
[2019-04-09 15:17:51,204] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[38.607803]
 [38.09354 ]
 [38.004803]
 [38.24338 ]
 [38.257496]], R is [[38.77906418]
 [38.3912735 ]
 [38.77482986]
 [39.12915421]
 [39.43135452]].
[2019-04-09 15:17:51,318] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3006692e-11 6.1707098e-08 7.2349676e-06 1.9671761e-11 1.3185883e-01
 8.6027611e-04 8.6715573e-01 6.2341519e-07 1.0888078e-04 8.2321694e-06
 9.4631488e-08], sum to 1.0000
[2019-04-09 15:17:51,321] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8844
[2019-04-09 15:17:51,369] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 55.33333333333333, 0.0, 0.0, 19.0, 24.13524691486543, 0.1123988290629215, 0.0, 1.0, 45.0, 30.48192966287068], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2324400.0000, 
sim time next is 2325600.0000, 
raw observation next is [-1.7, 56.0, 0.0, 0.0, 19.0, 24.10372878585722, 0.1059394660056359, 0.0, 1.0, 45.0, 30.077651627012205], 
processed observation next is [1.0, 0.9565217391304348, 0.4155124653739613, 0.56, 0.0, 0.0, 0.08333333333333333, 0.5086440654881018, 0.535313155335212, 0.0, 1.0, 0.6, 0.30077651627012203], 
reward next is 0.6992, 
noisyNet noise sample is [array([0.14325261], dtype=float32), -0.25305012]. 
=============================================
[2019-04-09 15:17:51,379] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8851980e-08 1.4866156e-06 2.8096027e-05 3.0526668e-09 7.2431219e-01
 6.6065387e-04 2.7464053e-01 1.2392128e-06 2.7965548e-04 7.5269294e-05
 8.5736718e-07], sum to 1.0000
[2019-04-09 15:17:51,398] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6303
[2019-04-09 15:17:51,483] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.566666666666666, 88.33333333333334, 0.0, 0.0, 19.0, 21.49657630442839, -0.5333224295156993, 0.0, 1.0, 35.0, 25.47160329001829], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2262000.0000, 
sim time next is 2263200.0000, 
raw observation next is [-8.733333333333334, 89.66666666666667, 0.0, 0.0, 19.0, 21.3597743841467, -0.5803140710409509, 0.0, 1.0, 35.0, 23.14026731088913], 
processed observation next is [1.0, 0.17391304347826086, 0.22068328716528163, 0.8966666666666667, 0.0, 0.0, 0.08333333333333333, 0.27998119867889165, 0.306561976319683, 0.0, 1.0, 0.4, 0.2314026731088913], 
reward next is 0.7686, 
noisyNet noise sample is [array([0.50401366], dtype=float32), 1.3394059]. 
=============================================
[2019-04-09 15:17:51,491] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.27857705e-12 1.50641284e-07 3.68682117e-06 1.00430862e-11
 3.90013099e-01 2.51388905e-04 6.09612286e-01 5.27945815e-07
 1.16209965e-04 2.55133273e-06 2.93591871e-08], sum to 1.0000
[2019-04-09 15:17:51,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4300
[2019-04-09 15:17:51,678] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.8, 61.33333333333333, 177.8333333333333, 104.0, 22.5, 23.76275505933368, -0.1425716292074907, 1.0, 1.0, 35.0, 25.57510447274708], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2288400.0000, 
sim time next is 2289600.0000, 
raw observation next is [-3.2, 58.0, 211.5, 92.0, 22.5, 23.76145430828393, -0.09414540049182123, 1.0, 1.0, 45.0, 41.56235151245629], 
processed observation next is [1.0, 0.5217391304347826, 0.37396121883656513, 0.58, 0.705, 0.10165745856353592, 0.375, 0.4801211923569942, 0.4686181998360596, 1.0, 1.0, 0.6, 0.41562351512456286], 
reward next is 0.5844, 
noisyNet noise sample is [array([-0.1295997], dtype=float32), 1.1925315]. 
=============================================
[2019-04-09 15:17:51,803] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7239178e-10 9.1428715e-10 7.5738075e-07 3.8978733e-11 7.3282319e-01
 6.9522386e-04 2.6646972e-01 3.2044724e-07 9.2386463e-06 1.4338582e-06
 3.4927705e-08], sum to 1.0000
[2019-04-09 15:17:51,803] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3082
[2019-04-09 15:17:51,890] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 53.33333333333334, 0.0, 0.0, 22.5, 24.49838173706672, 0.1195625942426963, 1.0, 1.0, 35.0, 20.233907623576002], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2313600.0000, 
sim time next is 2314800.0000, 
raw observation next is [-1.2, 54.0, 0.0, 0.0, 22.5, 24.34774676818893, 0.08930922735133033, 1.0, 1.0, 35.0, 22.660969896596214], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.54, 0.0, 0.0, 0.375, 0.5289788973490775, 0.5297697424504434, 1.0, 1.0, 0.4, 0.22660969896596214], 
reward next is 0.7734, 
noisyNet noise sample is [array([0.17528377], dtype=float32), 0.877748]. 
=============================================
[2019-04-09 15:17:52,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2737024e-08 3.7732914e-06 2.0900319e-04 8.2947116e-09 5.9746063e-01
 6.9693150e-04 4.0100345e-01 6.2665480e-05 4.6796419e-04 8.7528286e-05
 7.9872880e-06], sum to 1.0000
[2019-04-09 15:17:52,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3894
[2019-04-09 15:17:52,205] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 22.80042921130025, -0.2169022255871371, 0.0, 1.0, 45.0, 32.6724197709705], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2341200.0000, 
sim time next is 2342400.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 19.0, 22.71677370550085, -0.2427945931586627, 0.0, 1.0, 35.0, 24.875193377537435], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.08333333333333333, 0.39306447545840406, 0.41906846894711247, 0.0, 1.0, 0.4, 0.24875193377537436], 
reward next is 0.7512, 
noisyNet noise sample is [array([-0.92028666], dtype=float32), -0.85510767]. 
=============================================
[2019-04-09 15:17:52,871] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.6257625e-08 1.4182344e-06 3.1987773e-05 5.2739320e-09 8.3667630e-01
 7.2148503e-03 1.5568769e-01 7.1499730e-06 2.1753222e-04 1.5810659e-04
 4.9567807e-06], sum to 1.0000
[2019-04-09 15:17:52,871] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9764
[2019-04-09 15:17:52,894] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.2, 86.66666666666667, 0.0, 0.0, 19.0, 21.46073818445394, -0.528680386237724, 0.0, 1.0, 45.0, 39.57957453907742], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2259600.0000, 
sim time next is 2260800.0000, 
raw observation next is [-8.4, 87.0, 0.0, 0.0, 19.0, 21.41179912074847, -0.5333209005111933, 0.0, 1.0, 45.0, 38.38429569754089], 
processed observation next is [1.0, 0.17391304347826086, 0.2299168975069252, 0.87, 0.0, 0.0, 0.08333333333333333, 0.28431659339570575, 0.3222263664962689, 0.0, 1.0, 0.6, 0.38384295697540893], 
reward next is 0.6162, 
noisyNet noise sample is [array([-0.726428], dtype=float32), 1.7508079]. 
=============================================
[2019-04-09 15:17:53,040] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4051500e-12 6.3038121e-09 9.7497741e-07 2.2997771e-12 8.7562698e-01
 1.0942795e-03 1.2326982e-01 2.1898719e-08 6.3052707e-06 1.6229668e-06
 7.1108026e-09], sum to 1.0000
[2019-04-09 15:17:53,040] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2749
[2019-04-09 15:17:53,171] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 43.0, 129.5, 51.0, 22.5, 24.48917836438318, 0.07296135989755942, 1.0, 1.0, 35.0, 23.41453724491052], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2300400.0000, 
sim time next is 2301600.0000, 
raw observation next is [0.7333333333333335, 43.33333333333334, 135.1666666666667, 45.0, 22.5, 24.41292772305811, 0.07331524554132846, 1.0, 1.0, 35.0, 20.318232438139972], 
processed observation next is [1.0, 0.6521739130434783, 0.4829178208679595, 0.4333333333333334, 0.4505555555555557, 0.049723756906077346, 0.375, 0.5344106435881759, 0.5244384151804428, 1.0, 1.0, 0.4, 0.20318232438139971], 
reward next is 0.7968, 
noisyNet noise sample is [array([1.7334863], dtype=float32), -0.5347434]. 
=============================================
[2019-04-09 15:17:53,367] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4526049e-11 8.7954319e-09 3.7622121e-06 1.6935538e-12 1.0342124e-01
 4.2269705e-05 8.9650095e-01 7.9305110e-08 3.0573636e-05 9.3563614e-07
 2.0099881e-07], sum to 1.0000
[2019-04-09 15:17:53,367] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5439
[2019-04-09 15:17:53,551] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.3, 70.0, 29.66666666666667, 0.0, 22.5, 25.06311546808671, 0.1789479969328222, 1.0, 1.0, 45.0, 41.58994987194153], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2220000.0000, 
sim time next is 2221200.0000, 
raw observation next is [-4.5, 71.0, 19.0, 0.0, 22.5, 24.84874904891927, 0.1847203916629384, 1.0, 1.0, 45.0, 41.136638937312426], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.71, 0.06333333333333334, 0.0, 0.375, 0.5707290874099392, 0.5615734638876462, 1.0, 1.0, 0.6, 0.41136638937312425], 
reward next is 0.5886, 
noisyNet noise sample is [array([-0.6559833], dtype=float32), -1.4623002]. 
=============================================
[2019-04-09 15:17:54,063] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6152718e-07 8.5675738e-06 1.6858829e-04 2.9329195e-08 3.6981794e-01
 3.7985889e-03 6.2336946e-01 5.1696490e-05 2.0512147e-03 7.0763938e-04
 2.6111949e-05], sum to 1.0000
[2019-04-09 15:17:54,063] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7732
[2019-04-09 15:17:54,149] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.2, 67.66666666666667, 0.0, 0.0, 19.0, 21.82157926907282, -0.451217217139902, 0.0, 1.0, 35.0, 19.986681521427066], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2352000.0000, 
sim time next is 2353200.0000, 
raw observation next is [-3.0, 66.33333333333333, 0.0, 0.0, 19.0, 21.66613227389634, -0.4497665715159875, 0.0, 1.0, 45.0, 35.621341480548715], 
processed observation next is [0.0, 0.21739130434782608, 0.3795013850415513, 0.6633333333333333, 0.0, 0.0, 0.08333333333333333, 0.3055110228246951, 0.3500778094946708, 0.0, 1.0, 0.6, 0.3562134148054871], 
reward next is 0.6438, 
noisyNet noise sample is [array([-0.36788943], dtype=float32), -0.76593703]. 
=============================================
[2019-04-09 15:17:55,708] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.5593749e-08 2.2556801e-06 3.9084480e-05 8.0407352e-09 2.4012233e-01
 2.8017419e-03 7.5668538e-01 6.0807542e-06 1.7490357e-04 1.6209648e-04
 6.0436437e-06], sum to 1.0000
[2019-04-09 15:17:55,710] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2839
[2019-04-09 15:17:55,754] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 27.0, 70.0, 732.0, 19.0, 21.3028150004945, -0.5496887106989902, 0.0, 1.0, 45.0, 30.001875817107646], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2473200.0000, 
sim time next is 2474400.0000, 
raw observation next is [3.3, 26.66666666666667, 64.66666666666667, 697.3333333333334, 19.0, 21.42102164745008, -0.5266418386609939, 0.0, 1.0, 45.0, 29.355623313134217], 
processed observation next is [0.0, 0.6521739130434783, 0.554016620498615, 0.2666666666666667, 0.21555555555555558, 0.7705340699815838, 0.08333333333333333, 0.28508513728750656, 0.3244527204463354, 0.0, 1.0, 0.6, 0.29355623313134216], 
reward next is 0.7064, 
noisyNet noise sample is [array([-1.7890341], dtype=float32), 1.457825]. 
=============================================
[2019-04-09 15:17:56,045] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0346423e-10 7.1460022e-08 1.4771882e-05 7.1169265e-10 2.3573412e-01
 4.3989029e-03 7.5968581e-01 5.0011749e-06 1.5153161e-04 5.8129121e-06
 3.9753354e-06], sum to 1.0000
[2019-04-09 15:17:56,045] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5615
[2019-04-09 15:17:56,180] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 22.5, 20.76377718642622, -0.6430624784638631, 1.0, 1.0, 45.0, 48.60058652803523], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2272800.0000, 
sim time next is 2274000.0000, 
raw observation next is [-9.5, 91.0, 9.999999999999998, 19.33333333333334, 22.5, 21.33460132959935, -0.5650182069036206, 1.0, 1.0, 45.0, 47.012091621256204], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.033333333333333326, 0.021362799263351755, 0.375, 0.2778834441332793, 0.3116605976987931, 1.0, 1.0, 0.6, 0.470120916212562], 
reward next is 0.5845, 
noisyNet noise sample is [array([0.45685545], dtype=float32), -0.99258727]. 
=============================================
[2019-04-09 15:17:56,241] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[41.27654 ]
 [40.13155 ]
 [40.2182  ]
 [40.63495 ]
 [40.301624]], R is [[41.92256927]
 [41.96798706]
 [42.18434906]
 [42.47855377]
 [42.69012833]].
[2019-04-09 15:17:56,611] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2539629e-08 5.4990187e-06 2.3403170e-04 2.2207217e-08 3.3658698e-01
 1.9326922e-03 6.5837151e-01 4.6416626e-05 2.3963370e-03 3.9579626e-04
 3.0712421e-05], sum to 1.0000
[2019-04-09 15:17:56,625] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3239
[2019-04-09 15:17:56,682] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 19.0, 21.99932402749771, -0.3914295968922087, 0.0, 1.0, 45.0, 34.239220268634725], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2350800.0000, 
sim time next is 2352000.0000, 
raw observation next is [-3.2, 67.66666666666667, 0.0, 0.0, 19.0, 21.92143846238674, -0.411810886499451, 0.0, 1.0, 35.0, 26.81170767938569], 
processed observation next is [0.0, 0.21739130434782608, 0.37396121883656513, 0.6766666666666667, 0.0, 0.0, 0.08333333333333333, 0.3267865385322282, 0.362729704500183, 0.0, 1.0, 0.4, 0.26811707679385693], 
reward next is 0.7319, 
noisyNet noise sample is [array([-0.47263408], dtype=float32), -1.103201]. 
=============================================
[2019-04-09 15:17:56,717] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[33.362736]
 [33.134224]
 [33.262466]
 [32.913593]
 [33.046463]], R is [[33.58397293]
 [33.90574265]
 [34.3009491 ]
 [34.61998367]
 [35.00831223]].
[2019-04-09 15:17:56,859] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4264961e-10 1.9514812e-07 2.9397917e-05 1.2407662e-10 3.0658647e-01
 5.5803277e-04 6.9269150e-01 3.7647098e-06 1.0729607e-04 2.2346785e-05
 9.6253711e-07], sum to 1.0000
[2019-04-09 15:17:56,859] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4102
[2019-04-09 15:17:56,882] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 63.0, 0.0, 0.0, 19.0, 23.37552211343936, -0.08705204031143919, 0.0, 1.0, 45.0, 31.02864174216659], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2331600.0000, 
sim time next is 2332800.0000, 
raw observation next is [-2.3, 65.0, 0.0, 0.0, 19.0, 23.3274287294302, -0.0896383546823963, 0.0, 1.0, 45.0, 31.080084425103372], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.65, 0.0, 0.0, 0.08333333333333333, 0.44395239411918325, 0.47012054843920126, 0.0, 1.0, 0.6, 0.3108008442510337], 
reward next is 0.6892, 
noisyNet noise sample is [array([0.8682286], dtype=float32), 0.16677211]. 
=============================================
[2019-04-09 15:17:57,989] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0184531e-11 8.8107530e-09 8.0695878e-07 2.7931078e-11 3.5822555e-01
 6.6614019e-05 6.4168417e-01 9.0901159e-07 1.6642582e-05 5.1724987e-06
 5.8311358e-08], sum to 1.0000
[2019-04-09 15:17:57,989] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0580
[2019-04-09 15:17:58,030] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.1, 58.0, 0.0, 0.0, 19.0, 24.08244901571882, 0.06763271347304904, 0.0, 1.0, 45.0, 29.926814530743442], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2328000.0000, 
sim time next is 2329200.0000, 
raw observation next is [-2.3, 59.0, 0.0, 0.0, 19.0, 23.93439176571708, 0.0316225902444431, 0.0, 1.0, 35.0, 23.565712429213004], 
processed observation next is [1.0, 1.0, 0.3988919667590028, 0.59, 0.0, 0.0, 0.08333333333333333, 0.49453264714309003, 0.5105408634148144, 0.0, 1.0, 0.4, 0.23565712429213004], 
reward next is 0.7643, 
noisyNet noise sample is [array([0.86673594], dtype=float32), -0.32714504]. 
=============================================
[2019-04-09 15:17:58,425] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.1688974e-09 1.8951250e-06 3.4820754e-05 2.7057370e-08 4.4148412e-01
 1.0983243e-03 5.5690128e-01 2.0116566e-05 3.7258214e-04 8.3098726e-05
 3.7358905e-06], sum to 1.0000
[2019-04-09 15:17:58,439] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7455
[2019-04-09 15:17:58,466] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.2179003e-13 3.3548566e-09 1.5957093e-07 2.9808916e-12 4.0503711e-01
 1.3144483e-04 5.9482574e-01 7.0509309e-08 3.8798084e-06 1.5312144e-06
 1.6184925e-08], sum to 1.0000
[2019-04-09 15:17:58,466] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1651
[2019-04-09 15:17:58,485] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4, 45.66666666666667, 66.83333333333334, 51.0, 19.0, 21.86501827061478, -0.4127379242204501, 0.0, 1.0, 45.0, 33.95977115895713], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2392800.0000, 
sim time next is 2394000.0000, 
raw observation next is [-0.6, 45.0, 42.5, 37.0, 19.0, 21.88591733794019, -0.4171401010564899, 0.0, 1.0, 45.0, 33.91585093854938], 
processed observation next is [0.0, 0.7391304347826086, 0.44598337950138506, 0.45, 0.14166666666666666, 0.04088397790055249, 0.08333333333333333, 0.32382644482834905, 0.3609532996478367, 0.0, 1.0, 0.6, 0.33915850938549386], 
reward next is 0.6608, 
noisyNet noise sample is [array([0.4908094], dtype=float32), 0.04754442]. 
=============================================
[2019-04-09 15:17:58,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[36.378784]
 [36.003212]
 [35.880066]
 [36.136715]
 [35.812145]], R is [[36.62437439]
 [36.91853333]
 [37.20957947]
 [37.48558426]
 [37.7651825 ]].
[2019-04-09 15:17:58,567] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 49.00000000000001, 227.8333333333333, 69.83333333333333, 22.5, 24.14051035994632, 0.04366301532884337, 1.0, 1.0, 35.0, 29.405743819045433], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2294400.0000, 
sim time next is 2295600.0000, 
raw observation next is [-0.9666666666666668, 47.0, 204.3333333333333, 67.5, 22.5, 24.36533480601218, 0.0550623921137773, 1.0, 1.0, 35.0, 24.565619094278738], 
processed observation next is [1.0, 0.5652173913043478, 0.43582640812557716, 0.47, 0.681111111111111, 0.07458563535911603, 0.375, 0.5304445671676815, 0.5183541307045925, 1.0, 1.0, 0.4, 0.24565619094278737], 
reward next is 0.7543, 
noisyNet noise sample is [array([-0.66663826], dtype=float32), -0.08357117]. 
=============================================
[2019-04-09 15:17:58,791] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0872189e-07 3.5552563e-05 4.7084561e-04 8.1300129e-08 3.3182356e-01
 5.2967230e-03 6.6073710e-01 3.3187462e-05 1.0592926e-03 5.2571978e-04
 1.7448765e-05], sum to 1.0000
[2019-04-09 15:17:58,791] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4691
[2019-04-09 15:17:58,824] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.2, 67.66666666666667, 0.0, 0.0, 19.0, 22.09679188946225, -0.3500402205062188, 0.0, 1.0, 45.0, 35.25295221820749], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2352000.0000, 
sim time next is 2353200.0000, 
raw observation next is [-3.0, 66.33333333333333, 0.0, 0.0, 19.0, 22.08865385581666, -0.3577537142557816, 0.0, 1.0, 45.0, 34.848864325260834], 
processed observation next is [0.0, 0.21739130434782608, 0.3795013850415513, 0.6633333333333333, 0.0, 0.0, 0.08333333333333333, 0.34072115465138825, 0.38074876191473944, 0.0, 1.0, 0.6, 0.34848864325260837], 
reward next is 0.6515, 
noisyNet noise sample is [array([0.16826116], dtype=float32), 0.060610846]. 
=============================================
[2019-04-09 15:17:59,042] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6181914e-08 2.0308421e-06 4.5011820e-05 4.1437907e-08 4.2725745e-01
 1.0644405e-02 5.6158477e-01 2.9780176e-05 2.5544318e-04 1.7494221e-04
 6.1699925e-06], sum to 1.0000
[2019-04-09 15:17:59,042] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9730
[2019-04-09 15:17:59,047] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.40614603e-12 2.86226798e-09 2.87977770e-07 2.59553945e-12
 2.77449965e-01 8.01159127e-04 7.21726775e-01 4.97503152e-07
 2.10447870e-05 3.21922442e-07 1.17760495e-08], sum to 1.0000
[2019-04-09 15:17:59,047] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9022
[2019-04-09 15:17:59,064] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 47.0, 98.33333333333333, 284.1666666666667, 19.0, 21.33269463835378, -0.5118435696847782, 0.0, 1.0, 35.0, 20.51891391043734], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2388000.0000, 
sim time next is 2389200.0000, 
raw observation next is [0.0, 47.0, 84.83333333333334, 293.8333333333334, 19.0, 21.34704219214764, -0.4704803228013656, 0.0, 1.0, 45.0, 38.585480443303155], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.2827777777777778, 0.3246777163904237, 0.08333333333333333, 0.27892018267897, 0.3431732257328781, 0.0, 1.0, 0.6, 0.38585480443303155], 
reward next is 0.6141, 
noisyNet noise sample is [array([-0.38246888], dtype=float32), -0.693495]. 
=============================================
[2019-04-09 15:17:59,144] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.7333333333333335, 43.33333333333334, 135.1666666666667, 45.0, 22.5, 25.35851442974636, 0.2910396343037785, 1.0, 1.0, 45.0, 35.77190268492808], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2301600.0000, 
sim time next is 2302800.0000, 
raw observation next is [0.3666666666666668, 43.66666666666667, 121.8333333333333, 35.0, 22.5, 25.4701269523535, 0.3074407358121021, 1.0, 1.0, 45.0, 35.1331360221494], 
processed observation next is [1.0, 0.6521739130434783, 0.4727608494921515, 0.4366666666666667, 0.406111111111111, 0.03867403314917127, 0.375, 0.6225105793627916, 0.6024802452707007, 1.0, 1.0, 0.6, 0.351331360221494], 
reward next is 0.6487, 
noisyNet noise sample is [array([-0.49507987], dtype=float32), 0.25305593]. 
=============================================
[2019-04-09 15:17:59,548] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.31549434e-13 7.39662243e-09 4.17411911e-06 1.05305196e-12
 7.08769500e-01 4.07476764e-04 2.90756941e-01 1.12321594e-07
 5.70363954e-05 4.67119526e-06 2.04561790e-08], sum to 1.0000
[2019-04-09 15:17:59,557] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8471
[2019-04-09 15:17:59,693] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.2854377e-07 5.6995823e-06 1.8758180e-04 4.6656275e-08 1.6296272e-01
 1.4418481e-03 8.3500749e-01 4.7992802e-05 1.8038138e-04 1.3911648e-04
 2.6962374e-05], sum to 1.0000
[2019-04-09 15:17:59,693] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5827
[2019-04-09 15:17:59,708] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 49.0, 23.0, 0.0, 22.5, 25.93547420589096, 0.2534689828286957, 1.0, 1.0, 35.0, 23.299068588585072], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2307600.0000, 
sim time next is 2308800.0000, 
raw observation next is [-0.8, 50.0, 11.0, 0.0, 22.5, 24.88672152918013, 0.2481067122895177, 1.0, 1.0, 45.0, 40.75760880345932], 
processed observation next is [1.0, 0.7391304347826086, 0.4404432132963989, 0.5, 0.03666666666666667, 0.0, 0.375, 0.5738934607650107, 0.5827022374298393, 1.0, 1.0, 0.6, 0.4075760880345932], 
reward next is 0.5924, 
noisyNet noise sample is [array([-0.7355925], dtype=float32), 1.3299719]. 
=============================================
[2019-04-09 15:17:59,761] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 19.5, 0.0, 19.0, 21.65764665453288, -0.4486064422469044, 0.0, 1.0, 45.0, 35.5021121794142], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2361600.0000, 
sim time next is 2362800.0000, 
raw observation next is [-3.4, 69.0, 31.16666666666667, 0.0, 19.0, 21.81283390762529, -0.4561963233296091, 0.0, 1.0, 45.0, 35.19783335819543], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.1038888888888889, 0.0, 0.08333333333333333, 0.31773615896877416, 0.3479345588901303, 0.0, 1.0, 0.6, 0.3519783335819543], 
reward next is 0.6480, 
noisyNet noise sample is [array([-1.0992744], dtype=float32), -0.25405118]. 
=============================================
[2019-04-09 15:18:00,152] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.5335198e-12 2.5827640e-08 5.9004901e-07 3.3743546e-11 9.2554605e-01
 1.3580126e-03 7.3083684e-02 2.0565341e-07 8.6150121e-06 2.8427291e-06
 4.4145384e-08], sum to 1.0000
[2019-04-09 15:18:00,154] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8781
[2019-04-09 15:18:00,187] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 54.66666666666667, 0.0, 0.0, 19.0, 24.54540027843905, 0.1993369067210577, 0.0, 1.0, 35.0, 25.09456672227809], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2323200.0000, 
sim time next is 2324400.0000, 
raw observation next is [-1.7, 55.33333333333333, 0.0, 0.0, 19.0, 24.40236888760596, 0.1717289737100909, 0.0, 1.0, 35.0, 20.752423677836905], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5533333333333332, 0.0, 0.0, 0.08333333333333333, 0.53353074063383, 0.557242991236697, 0.0, 1.0, 0.4, 0.20752423677836906], 
reward next is 0.7925, 
noisyNet noise sample is [array([0.14018892], dtype=float32), -1.6784191]. 
=============================================
[2019-04-09 15:18:00,188] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2761824e-08 3.0867759e-06 5.5736444e-05 2.1469475e-08 8.3680695e-01
 3.7980329e-03 1.5845323e-01 4.2025877e-05 6.9245993e-04 1.4352502e-04
 4.9118121e-06], sum to 1.0000
[2019-04-09 15:18:00,189] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2021
[2019-04-09 15:18:00,198] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-09 15:18:00,199] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:18:00,199] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:18:00,200] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:18:00,200] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:18:00,200] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:18:00,200] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:18:00,202] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run11
[2019-04-09 15:18:00,219] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run11
[2019-04-09 15:18:00,236] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.3, 25.0, 27.0, 161.0, 19.0, 21.96197749405912, -0.4897746779696816, 0.0, 1.0, 35.0, 20.374636677854937], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2480400.0000, 
sim time next is 2481600.0000, 
raw observation next is [2.566666666666667, 26.0, 13.0, 82.33333333333333, 19.0, 21.83716004400347, -0.5279773761171455, 0.0, 1.0, 35.0, 18.824476046226557], 
processed observation next is [0.0, 0.7391304347826086, 0.5337026777469991, 0.26, 0.043333333333333335, 0.09097605893186003, 0.08333333333333333, 0.319763337000289, 0.32400754129428483, 0.0, 1.0, 0.4, 0.18824476046226557], 
reward next is 0.8118, 
noisyNet noise sample is [array([0.9103308], dtype=float32), -0.419178]. 
=============================================
[2019-04-09 15:18:00,238] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run11
[2019-04-09 15:18:20,982] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:NoisyNet noise sample: [array([0.0550216], dtype=float32), 0.07974591]
[2019-04-09 15:18:20,983] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation this: [-4.396305291, 27.4867794, 145.78260315, 0.0, 22.5, 22.15097943145091, -0.5529637577087175, 1.0, 1.0, 45.0, 43.96503483428991]
[2019-04-09 15:18:20,983] A3C_EVAL-Part4-Light-Pit-Test-v6 DEBUG:Observation forecast: []
[2019-04-09 15:18:20,984] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Softmax [1.4909147e-09 3.5820040e-07 1.8510551e-05 1.0845839e-09 3.4483105e-01
 2.0464687e-03 6.5293372e-01 5.1763759e-06 1.3206349e-04 3.1157106e-05
 1.5720482e-06], sampled 0.9371812100976419
[2019-04-09 15:19:35,825] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2990.2889 119122.2723 786.8675
[2019-04-09 15:19:35,845] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:35,845] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:35,845] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:35,845] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:35,845] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:35,845] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:35,845] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:35,845] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:35,845] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:35,845] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:35,845] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:35,951] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:35,951] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:35,951] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:35,951] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:35,951] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:35,951] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:35,951] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:35,951] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:35,951] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:35,951] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:35,951] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:53,644] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2961.5222 126521.5393 516.3181
[2019-04-09 15:19:53,664] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:53,664] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:53,664] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:53,664] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:53,664] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:53,664] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:53,664] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:53,664] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:53,664] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:53,664] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:53,664] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:53,773] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:53,773] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:53,773] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:53,773] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:53,773] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:53,773] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:53,773] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:53,773] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:53,773] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:53,773] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:53,773] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:55,955] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2922.3252 127851.5173 238.9694
[2019-04-09 15:19:55,975] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:55,975] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:55,975] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:55,975] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:55,975] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:55,975] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:55,975] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:55,975] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:55,975] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:55,975] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:55,975] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:19:56,078] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:56,078] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:56,078] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:56,078] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:56,078] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:56,078] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:56,078] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:56,078] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:56,078] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:56,078] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:56,078] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:19:56,977] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 100000, evaluation results [100000.0, 2961.5221517374803, 126521.53929822722, 516.3180588004749, 2990.2889335867217, 119122.2722508991, 786.8675315714781, 2922.3252270393123, 127851.51734487648, 238.96941982215407]
[2019-04-09 15:19:57,121] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.65798813e-08 4.35593984e-06 5.87828516e-04 5.93002945e-08
 6.47702277e-01 6.98152836e-03 3.41292500e-01 1.03070095e-04
 2.97352299e-03 3.01601569e-04 5.31632140e-05], sum to 1.0000
[2019-04-09 15:19:57,129] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8931
[2019-04-09 15:19:57,157] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 28.0, 0.0, 0.0, 19.0, 21.45559705394205, -0.6151278918389913, 0.0, 1.0, 35.0, 16.479788801191788], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2484000.0000, 
sim time next is 2485200.0000, 
raw observation next is [0.7333333333333335, 28.66666666666667, 0.0, 0.0, 19.0, 21.27232140716326, -0.6109160558096166, 0.0, 1.0, 45.0, 32.75518131132202], 
processed observation next is [0.0, 0.782608695652174, 0.4829178208679595, 0.28666666666666674, 0.0, 0.0, 0.08333333333333333, 0.27269345059693845, 0.2963613147301278, 0.0, 1.0, 0.6, 0.3275518131132202], 
reward next is 0.6724, 
noisyNet noise sample is [array([0.4428222], dtype=float32), 1.3819859]. 
=============================================
[2019-04-09 15:19:57,530] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.3489209e-07 5.6433742e-05 1.0846487e-03 5.4104004e-07 5.7940769e-01
 1.0393598e-02 4.0393031e-01 1.0542688e-04 3.7210470e-03 1.1228357e-03
 1.7668499e-04], sum to 1.0000
[2019-04-09 15:19:57,530] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5278
[2019-04-09 15:19:57,553] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.8, 55.0, 0.0, 0.0, 19.0, 19.83541209588022, -0.9301445095908382, 0.0, 1.0, 35.0, 22.0345768903224], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2430000.0000, 
sim time next is 2431200.0000, 
raw observation next is [-8.0, 57.0, 0.0, 0.0, 19.0, 19.61810190024284, -0.9740538436070586, 0.0, 1.0, 35.0, 24.151419977203183], 
processed observation next is [0.0, 0.13043478260869565, 0.24099722991689754, 0.57, 0.0, 0.0, 0.08333333333333333, 0.13484182502023678, 0.1753153854643138, 0.0, 1.0, 0.4, 0.24151419977203184], 
reward next is 0.7585, 
noisyNet noise sample is [array([0.37969843], dtype=float32), 1.4927895]. 
=============================================
[2019-04-09 15:19:58,074] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.15642585e-07 1.48207346e-05 1.77412701e-04 4.22997566e-08
 3.50203484e-01 1.61221158e-02 6.32673204e-01 8.78257342e-05
 3.30329029e-04 3.82514932e-04 8.21293543e-06], sum to 1.0000
[2019-04-09 15:19:58,074] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1208
[2019-04-09 15:19:58,123] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 38.66666666666667, 0.0, 0.0, 19.0, 20.62921760851238, -0.728892602140346, 0.0, 1.0, 45.0, 34.79577237557984], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2506800.0000, 
sim time next is 2508000.0000, 
raw observation next is [-1.7, 39.33333333333333, 0.0, 0.0, 19.0, 20.95177073730868, -0.7275300889866654, 0.0, 1.0, 35.0, 26.012352604954724], 
processed observation next is [1.0, 0.0, 0.4155124653739613, 0.3933333333333333, 0.0, 0.0, 0.08333333333333333, 0.24598089477572346, 0.2574899703377782, 0.0, 1.0, 0.4, 0.26012352604954725], 
reward next is 0.7399, 
noisyNet noise sample is [array([0.6462597], dtype=float32), -0.12622868]. 
=============================================
[2019-04-09 15:19:58,131] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[34.204243]
 [34.563812]
 [34.661266]
 [34.62432 ]
 [34.717297]], R is [[34.37934494]
 [34.68759537]
 [35.07954788]
 [35.37867737]
 [35.81538391]].
[2019-04-09 15:19:58,403] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6244141e-08 1.2456893e-06 1.5061901e-05 1.8436729e-08 6.6921967e-01
 1.9437447e-03 3.2845396e-01 1.3683954e-05 2.9732657e-04 4.7290054e-05
 8.1142198e-06], sum to 1.0000
[2019-04-09 15:19:58,410] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4328
[2019-04-09 15:19:58,432] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 40.0, 0.0, 0.0, 19.0, 20.87942785328932, -0.7418103203345353, 0.0, 1.0, 45.0, 33.37900494207163], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2514000.0000, 
sim time next is 2515200.0000, 
raw observation next is [-1.7, 42.0, 0.0, 0.0, 19.0, 20.83862365208054, -0.7544361972477178, 0.0, 1.0, 35.0, 26.012273197207], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.42, 0.0, 0.0, 0.08333333333333333, 0.23655197100671158, 0.24852126758409407, 0.0, 1.0, 0.4, 0.26012273197207003], 
reward next is 0.7399, 
noisyNet noise sample is [array([0.87954587], dtype=float32), 0.1913575]. 
=============================================
[2019-04-09 15:19:59,214] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1757495e-08 2.3944312e-06 2.9063209e-05 8.8250243e-09 7.8949207e-01
 8.4162055e-04 2.0943712e-01 1.1814950e-05 5.1167375e-05 1.3254781e-04
 2.1181645e-06], sum to 1.0000
[2019-04-09 15:19:59,214] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5403
[2019-04-09 15:19:59,227] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.8904344e-08 5.9534464e-06 1.8889409e-04 6.0150406e-08 2.1599707e-01
 1.1532486e-02 7.7083051e-01 1.3250367e-04 7.6936343e-04 5.2262651e-04
 2.0417416e-05], sum to 1.0000
[2019-04-09 15:19:59,227] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2734
[2019-04-09 15:19:59,249] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.7, 39.33333333333333, 0.0, 0.0, 19.0, 21.32043179505457, -0.6519544470040688, 0.0, 1.0, 35.0, 26.01613173161533], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2508000.0000, 
sim time next is 2509200.0000, 
raw observation next is [-1.7, 40.0, 0.0, 0.0, 19.0, 21.19458740542364, -0.68900818258766, 0.0, 1.0, 35.0, 23.359300190213144], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.4, 0.0, 0.0, 0.08333333333333333, 0.26621561711863667, 0.27033060580411333, 0.0, 1.0, 0.4, 0.23359300190213145], 
reward next is 0.7664, 
noisyNet noise sample is [array([0.33019373], dtype=float32), -1.2751702]. 
=============================================
[2019-04-09 15:19:59,257] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 30.0, 0.0, 0.0, 19.0, 20.89586611703013, -0.6644489699897326, 0.0, 1.0, 45.0, 34.76968170806789], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2487600.0000, 
sim time next is 2488800.0000, 
raw observation next is [-0.2333333333333333, 29.66666666666667, 0.0, 0.0, 19.0, 20.98745616767767, -0.6552793120236222, 0.0, 1.0, 45.0, 33.565578921034415], 
processed observation next is [0.0, 0.8260869565217391, 0.456140350877193, 0.2966666666666667, 0.0, 0.0, 0.08333333333333333, 0.2489546806398059, 0.2815735626587926, 0.0, 1.0, 0.6, 0.33565578921034417], 
reward next is 0.6643, 
noisyNet noise sample is [array([0.90075976], dtype=float32), -0.033145797]. 
=============================================
[2019-04-09 15:19:59,885] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8082734e-07 1.1436343e-05 1.5867944e-04 1.1950340e-07 6.5192819e-01
 6.9198799e-03 3.3951759e-01 8.7437686e-05 5.9534452e-04 6.3651503e-04
 1.4437680e-04], sum to 1.0000
[2019-04-09 15:19:59,885] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0850
[2019-04-09 15:19:59,931] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 43.0, 68.5, 721.0, 19.0, 19.80046544637661, -0.8622533316314719, 0.0, 1.0, 45.0, 36.321272258255064], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2455200.0000, 
sim time next is 2456400.0000, 
raw observation next is [-4.5, 40.66666666666667, 73.5, 758.3333333333333, 19.0, 20.01055176166404, -0.8400536398820247, 0.0, 1.0, 35.0, 29.034505925857736], 
processed observation next is [0.0, 0.43478260869565216, 0.3379501385041552, 0.40666666666666673, 0.245, 0.8379373848987108, 0.08333333333333333, 0.16754598013866998, 0.21998212003932507, 0.0, 1.0, 0.4, 0.29034505925857734], 
reward next is 0.7097, 
noisyNet noise sample is [array([1.6971544], dtype=float32), 0.3768018]. 
=============================================
[2019-04-09 15:20:00,391] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5005937e-09 4.0619702e-07 8.2651686e-06 3.4057506e-09 4.7700590e-01
 1.9372544e-03 5.2042562e-01 5.7106431e-06 5.6727987e-04 4.5247711e-05
 4.2847287e-06], sum to 1.0000
[2019-04-09 15:20:00,393] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4158
[2019-04-09 15:20:00,433] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 52.0, 175.5, 0.0, 19.0, 22.1017154041806, -0.3571327782910304, 0.0, 1.0, 45.0, 32.58790855997426], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2383200.0000, 
sim time next is 2384400.0000, 
raw observation next is [0.0, 50.33333333333334, 165.1666666666667, 0.0, 19.0, 22.12342939318113, -0.3658544496966898, 0.0, 1.0, 35.0, 25.490543361363095], 
processed observation next is [0.0, 0.6086956521739131, 0.46260387811634357, 0.5033333333333334, 0.5505555555555557, 0.0, 0.08333333333333333, 0.3436191160984275, 0.37804851676777007, 0.0, 1.0, 0.4, 0.25490543361363094], 
reward next is 0.7451, 
noisyNet noise sample is [array([0.29823098], dtype=float32), -0.07605312]. 
=============================================
[2019-04-09 15:20:00,685] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.0351181e-10 7.5583341e-07 2.4568877e-05 7.3855860e-10 3.5066876e-01
 6.6665951e-03 6.4237177e-01 1.6253065e-06 2.2785449e-04 3.7302525e-05
 7.7489761e-07], sum to 1.0000
[2019-04-09 15:20:00,685] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5451
[2019-04-09 15:20:00,799] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 55.33333333333333, 65.16666666666666, 20.5, 22.5, 22.18140070362362, -0.4913207358261171, 1.0, 1.0, 45.0, 41.51976951481575], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2536800.0000, 
sim time next is 2538000.0000, 
raw observation next is [-2.8, 56.0, 93.5, 25.5, 22.5, 22.52725693102177, -0.4206954304241246, 1.0, 1.0, 45.0, 41.21043689778144], 
processed observation next is [1.0, 0.391304347826087, 0.38504155124653744, 0.56, 0.31166666666666665, 0.0281767955801105, 0.375, 0.37727141091848093, 0.35976818985862513, 1.0, 1.0, 0.6, 0.41210436897781444], 
reward next is 0.5879, 
noisyNet noise sample is [array([-0.5353227], dtype=float32), 2.111774]. 
=============================================
[2019-04-09 15:20:00,805] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[44.21164 ]
 [43.51539 ]
 [41.739418]
 [40.284172]
 [39.83277 ]], R is [[46.04130173]
 [46.16569138]
 [46.29859543]
 [46.13870621]
 [45.81256485]].
[2019-04-09 15:20:00,974] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1211484e-09 8.6353032e-07 6.4738932e-05 1.3472865e-09 2.2878149e-01
 3.7544908e-03 7.6641190e-01 1.0486817e-05 9.5881498e-04 1.3871963e-05
 3.3299095e-06], sum to 1.0000
[2019-04-09 15:20:00,978] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9172
[2019-04-09 15:20:01,134] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 54.0, 13.5, 2.999999999999999, 22.5, 21.26355683730807, -0.6939099037317535, 1.0, 1.0, 45.0, 44.42427973804654], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2533200.0000, 
sim time next is 2534400.0000, 
raw observation next is [-2.8, 54.0, 28.5, 9.0, 22.5, 21.33988871363155, -0.6735782800899696, 1.0, 1.0, 45.0, 42.13417968718787], 
processed observation next is [1.0, 0.34782608695652173, 0.38504155124653744, 0.54, 0.095, 0.009944751381215469, 0.375, 0.2783240594692957, 0.2754739066366768, 1.0, 1.0, 0.6, 0.42134179687187867], 
reward next is 0.3891, 
noisyNet noise sample is [array([-0.65604585], dtype=float32), 1.5849164]. 
=============================================
[2019-04-09 15:20:01,167] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.1790680e-10 9.2824246e-07 2.9722037e-06 1.1632607e-09 7.6053488e-01
 5.6752316e-03 2.3351133e-01 1.6321294e-05 2.4599227e-04 1.1790053e-05
 5.9865863e-07], sum to 1.0000
[2019-04-09 15:20:01,167] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5987
[2019-04-09 15:20:01,239] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 79.0, 0.0, 0.0, 19.0, 21.70278690361014, -0.4674767360539887, 0.0, 1.0, 45.0, 37.91128357179099], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2617200.0000, 
sim time next is 2618400.0000, 
raw observation next is [-7.300000000000001, 79.0, 0.0, 0.0, 22.5, 21.67829712572037, -0.4772444424218613, 1.0, 1.0, 35.0, 39.11863815061198], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.79, 0.0, 0.0, 0.375, 0.30652476047669747, 0.3409185191927129, 1.0, 1.0, 0.4, 0.3911863815061198], 
reward next is 0.6088, 
noisyNet noise sample is [array([0.8892174], dtype=float32), -0.7166993]. 
=============================================
[2019-04-09 15:20:03,057] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.2566740e-12 2.3577011e-08 4.4450550e-07 1.2901379e-12 3.6002824e-01
 5.7618407e-04 6.3924861e-01 9.7623953e-08 1.4477705e-04 1.5323064e-06
 1.8115433e-08], sum to 1.0000
[2019-04-09 15:20:03,058] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7030
[2019-04-09 15:20:03,196] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 39.0, 225.0, 46.5, 22.5, 24.34944810611539, -0.0476296160621285, 1.0, 1.0, 35.0, 21.03723436507434], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2548800.0000, 
sim time next is 2550000.0000, 
raw observation next is [1.466666666666667, 36.0, 220.3333333333333, 30.16666666666666, 22.5, 24.43496185409496, 0.00366882225222868, 1.0, 1.0, 45.0, 39.637528762910534], 
processed observation next is [1.0, 0.5217391304347826, 0.5032317636195753, 0.36, 0.7344444444444442, 0.033333333333333326, 0.375, 0.5362468211745801, 0.5012229407507429, 1.0, 1.0, 0.6, 0.39637528762910534], 
reward next is 0.6036, 
noisyNet noise sample is [array([0.22311206], dtype=float32), -0.13121124]. 
=============================================
[2019-04-09 15:20:03,200] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[57.38231 ]
 [57.013943]
 [56.42447 ]
 [55.837196]
 [54.892082]], R is [[58.13158798]
 [58.33990097]
 [58.51878357]
 [58.65447998]
 [58.68470001]].
[2019-04-09 15:20:03,247] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.02790159e-14 1.53556091e-11 5.85962425e-07 1.31421981e-14
 6.36512935e-02 5.48604003e-05 9.36293125e-01 1.22334221e-09
 1.16923665e-07 6.06640249e-09 2.85085566e-10], sum to 1.0000
[2019-04-09 15:20:03,247] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9791
[2019-04-09 15:20:03,330] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5325294e-11 6.5140682e-08 1.7543986e-06 1.7993775e-11 2.1352273e-01
 3.0226803e-03 7.8340745e-01 6.1546723e-07 4.1219602e-05 2.9653754e-06
 4.5847648e-07], sum to 1.0000
[2019-04-09 15:20:03,332] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9331
[2019-04-09 15:20:03,366] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.1, 29.0, 77.33333333333333, 193.5, 22.5, 25.99992196829278, 0.4317630914317582, 1.0, 1.0, 45.0, 34.26644948947861], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2564400.0000, 
sim time next is 2565600.0000, 
raw observation next is [2.9, 29.0, 59.5, 135.8333333333333, 22.5, 25.92856392394035, 0.431136786030403, 1.0, 1.0, 45.0, 33.97161680611582], 
processed observation next is [1.0, 0.6956521739130435, 0.5429362880886427, 0.29, 0.19833333333333333, 0.1500920810313075, 0.375, 0.6607136603283624, 0.6437122620101343, 1.0, 1.0, 0.6, 0.3397161680611582], 
reward next is 0.6603, 
noisyNet noise sample is [array([-0.8099942], dtype=float32), 0.77415943]. 
=============================================
[2019-04-09 15:20:03,490] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 49.0, 134.5, 39.0, 22.5, 23.12797165434039, -0.3316197735154885, 1.0, 1.0, 35.0, 29.648620061098555], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2541600.0000, 
sim time next is 2542800.0000, 
raw observation next is [-1.0, 48.33333333333334, 133.5, 43.0, 22.5, 23.29570954011492, -0.2830202752701891, 1.0, 1.0, 45.0, 40.74126979801802], 
processed observation next is [1.0, 0.43478260869565216, 0.4349030470914128, 0.48333333333333345, 0.445, 0.04751381215469613, 0.375, 0.44130912834291003, 0.4056599082432703, 1.0, 1.0, 0.6, 0.40741269798018015], 
reward next is 0.5926, 
noisyNet noise sample is [array([-0.15326698], dtype=float32), 0.11254933]. 
=============================================
[2019-04-09 15:20:03,899] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6616863e-10 3.8338683e-08 9.7203711e-06 8.2372209e-10 3.5404310e-01
 4.5261863e-03 6.4136660e-01 2.3809176e-07 4.2687279e-05 1.0838760e-05
 6.0505948e-07], sum to 1.0000
[2019-04-09 15:20:03,899] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7993
[2019-04-09 15:20:04,050] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 79.0, 42.0, 4.0, 22.5, 22.28211550677419, -0.3634522061711678, 1.0, 1.0, 35.0, 34.801104240082736], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2620800.0000, 
sim time next is 2622000.0000, 
raw observation next is [-7.100000000000001, 77.66666666666667, 65.33333333333334, 1.333333333333333, 22.5, 22.33673006348085, -0.2998435543321193, 1.0, 1.0, 45.0, 47.966466790525914], 
processed observation next is [1.0, 0.34782608695652173, 0.26592797783933514, 0.7766666666666667, 0.21777777777777782, 0.00147329650092081, 0.375, 0.3613941719567375, 0.4000521485559602, 1.0, 1.0, 0.6, 0.47966466790525913], 
reward next is 0.5203, 
noisyNet noise sample is [array([2.7488003], dtype=float32), -1.4349278]. 
=============================================
[2019-04-09 15:20:04,083] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[44.300655]
 [43.560623]
 [43.098785]
 [42.5067  ]
 [42.609013]], R is [[46.06621933]
 [46.25754547]
 [46.22298813]
 [46.49605942]
 [46.73567963]].
[2019-04-09 15:20:04,530] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.4910649e-11 9.8864213e-08 2.4420347e-06 1.9538940e-10 7.0287895e-01
 9.6365093e-04 2.9610631e-01 7.0085139e-07 3.8322083e-05 9.1913025e-06
 3.2992943e-07], sum to 1.0000
[2019-04-09 15:20:04,531] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5097
[2019-04-09 15:20:04,702] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 49.0, 134.5, 39.0, 22.5, 23.18979891448289, -0.2948531556169645, 1.0, 1.0, 45.0, 42.35823762004116], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2541600.0000, 
sim time next is 2542800.0000, 
raw observation next is [-1.0, 48.33333333333334, 133.5, 43.0, 22.5, 23.47225073154699, -0.2618140544713909, 1.0, 1.0, 35.0, 29.122546385296335], 
processed observation next is [1.0, 0.43478260869565216, 0.4349030470914128, 0.48333333333333345, 0.445, 0.04751381215469613, 0.375, 0.4560208942955824, 0.41272864850953633, 1.0, 1.0, 0.4, 0.29122546385296333], 
reward next is 0.7088, 
noisyNet noise sample is [array([0.37044516], dtype=float32), 0.03061793]. 
=============================================
[2019-04-09 15:20:05,710] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.0695303e-12 4.2025943e-09 2.1035437e-07 1.6851610e-12 8.3871782e-01
 3.1184722e-04 1.6095574e-01 1.4096368e-07 1.2122256e-05 2.1047288e-06
 8.9321098e-08], sum to 1.0000
[2019-04-09 15:20:05,711] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3286
[2019-04-09 15:20:05,765] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.066666666666667, 48.0, 0.0, 0.0, 19.0, 24.84713275125468, 0.2398098151574515, 0.0, 1.0, 35.0, 20.994558315939024], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2578800.0000, 
sim time next is 2580000.0000, 
raw observation next is [-2.433333333333333, 52.0, 0.0, 0.0, 19.0, 24.56651157046841, 0.1954029420431116, 0.0, 1.0, 35.0, 18.135013294820546], 
processed observation next is [1.0, 0.8695652173913043, 0.3951985226223454, 0.52, 0.0, 0.0, 0.08333333333333333, 0.5472092975390342, 0.5651343140143705, 0.0, 1.0, 0.4, 0.18135013294820546], 
reward next is 0.8186, 
noisyNet noise sample is [array([-2.626271], dtype=float32), -0.07540808]. 
=============================================
[2019-04-09 15:20:05,771] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[53.43756 ]
 [54.817245]
 [55.81896 ]
 [57.05143 ]
 [57.669662]], R is [[52.84967804]
 [53.11123657]
 [53.33167267]
 [53.53336716]
 [53.60447693]].
[2019-04-09 15:20:05,926] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6561722e-12 2.3734050e-09 1.2141078e-06 1.0167953e-13 2.8922865e-01
 1.5288232e-04 7.1060628e-01 3.8164796e-08 9.8898254e-06 1.0733394e-06
 6.4739233e-08], sum to 1.0000
[2019-04-09 15:20:05,927] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1255
[2019-04-09 15:20:06,035] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.166666666666667, 49.33333333333334, 231.5, 157.6666666666667, 22.5, 25.59129292925745, 0.2386850011375485, 1.0, 1.0, 45.0, 40.590006103244164], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2637600.0000, 
sim time next is 2638800.0000, 
raw observation next is [-0.6, 47.0, 204.5, 179.0, 22.5, 24.67632059861877, 0.2352878813641326, 1.0, 1.0, 35.0, 30.97652424455459], 
processed observation next is [1.0, 0.5652173913043478, 0.44598337950138506, 0.47, 0.6816666666666666, 0.19779005524861878, 0.375, 0.5563600498848974, 0.5784292937880442, 1.0, 1.0, 0.4, 0.3097652424455459], 
reward next is 0.6902, 
noisyNet noise sample is [array([-1.3065636], dtype=float32), -0.034296263]. 
=============================================
[2019-04-09 15:20:07,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2650168e-10 2.6132213e-07 6.6210914e-06 4.4404527e-10 2.4810705e-01
 3.1194771e-03 7.4857455e-01 4.1126818e-06 1.7178514e-04 1.5197512e-05
 1.0213149e-06], sum to 1.0000
[2019-04-09 15:20:07,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6767
[2019-04-09 15:20:07,275] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.9, 78.33333333333334, 0.0, 0.0, 19.0, 21.92465934979649, -0.4000953587827945, 0.0, 1.0, 45.0, 37.33644035245662], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2614800.0000, 
sim time next is 2616000.0000, 
raw observation next is [-7.1, 78.66666666666667, 0.0, 0.0, 19.0, 21.9616842084064, -0.412389413283179, 0.0, 1.0, 45.0, 37.5303341647762], 
processed observation next is [1.0, 0.2608695652173913, 0.2659279778393352, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.3301403507005333, 0.3625368622389404, 0.0, 1.0, 0.6, 0.375303341647762], 
reward next is 0.6247, 
noisyNet noise sample is [array([-0.09479871], dtype=float32), 1.3849536]. 
=============================================
[2019-04-09 15:20:07,283] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[45.422035]
 [45.285576]
 [45.217014]
 [45.226124]
 [45.273605]], R is [[45.7976265 ]
 [45.96628571]
 [46.13647461]
 [46.30833435]
 [46.48121643]].
[2019-04-09 15:20:08,076] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0873946e-14 7.0590978e-11 7.8848942e-08 6.3235952e-15 1.0303084e-01
 6.7555891e-05 8.9690048e-01 8.1036378e-09 7.9796314e-07 1.2588507e-07
 3.2781633e-10], sum to 1.0000
[2019-04-09 15:20:08,076] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6333
[2019-04-09 15:20:08,189] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 54.0, 0.0, 0.0, 22.5, 26.70009569151042, 0.5294226997495131, 1.0, 1.0, 45.0, 38.301806362162466], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2656800.0000, 
sim time next is 2658000.0000, 
raw observation next is [-0.8, 56.00000000000001, 0.0, 0.0, 22.5, 25.41569071614261, 0.4592434865224429, 1.0, 1.0, 45.0, 38.6229292362887], 
processed observation next is [1.0, 0.782608695652174, 0.4404432132963989, 0.56, 0.0, 0.0, 0.375, 0.6179742263452175, 0.6530811621741476, 1.0, 1.0, 0.6, 0.386229292362887], 
reward next is 0.6138, 
noisyNet noise sample is [array([-0.6943416], dtype=float32), 0.2327383]. 
=============================================
[2019-04-09 15:20:08,193] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[61.112568]
 [61.49075 ]
 [61.6766  ]
 [61.50757 ]
 [61.782787]], R is [[61.10968781]
 [61.11557388]
 [61.11726379]
 [61.1234436 ]
 [61.22706223]].
[2019-04-09 15:20:08,299] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6091684e-10 2.0342425e-07 1.5657019e-05 1.1655915e-10 6.3103336e-01
 1.0322622e-03 3.6785322e-01 2.8408103e-06 5.8083577e-05 4.0186633e-06
 3.5646750e-07], sum to 1.0000
[2019-04-09 15:20:08,299] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7512
[2019-04-09 15:20:08,424] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.300000000000001, 79.0, 0.0, 0.0, 22.5, 21.85531890501931, -0.4319568676183367, 1.0, 1.0, 35.0, 39.5124514196532], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2618400.0000, 
sim time next is 2619600.0000, 
raw observation next is [-7.3, 79.0, 18.66666666666666, 6.666666666666667, 22.5, 22.03842701696146, -0.3649197456279977, 0.0, 1.0, 45.0, 50.33263223529468], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.79, 0.0622222222222222, 0.007366482504604052, 0.375, 0.33653558474678835, 0.3783600847906674, 0.0, 1.0, 0.6, 0.5033263223529468], 
reward next is 0.4967, 
noisyNet noise sample is [array([-0.5769903], dtype=float32), -0.504289]. 
=============================================
[2019-04-09 15:20:09,716] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7165837e-11 6.2988001e-08 2.6849928e-06 4.7225427e-12 2.7934173e-01
 1.3418500e-03 7.1927810e-01 8.7385109e-07 3.3427947e-05 1.2728908e-06
 1.4075185e-07], sum to 1.0000
[2019-04-09 15:20:09,716] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6356
[2019-04-09 15:20:09,759] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.366666666666666, 69.0, 0.0, 0.0, 19.0, 24.51215586808677, 0.2041850785081474, 0.0, 1.0, 45.0, 35.831479021174886], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2673600.0000, 
sim time next is 2674800.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 19.0, 24.35897939108203, 0.1827092502951725, 0.0, 1.0, 45.0, 36.58463737849038], 
processed observation next is [1.0, 1.0, 0.32409972299168976, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5299149492568359, 0.5609030834317242, 0.0, 1.0, 0.6, 0.36584637378490376], 
reward next is 0.6342, 
noisyNet noise sample is [array([-0.33708584], dtype=float32), 0.26676476]. 
=============================================
[2019-04-09 15:20:10,421] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5846490e-13 1.1791689e-09 2.4906387e-07 3.6765849e-13 2.1793349e-02
 3.7478399e-04 9.7783089e-01 4.4940933e-08 4.3000983e-07 1.2758991e-07
 1.3023785e-09], sum to 1.0000
[2019-04-09 15:20:10,421] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9920
[2019-04-09 15:20:10,475] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.1086226e-11 1.3790206e-07 6.2495183e-06 9.8866075e-11 5.7296470e-02
 6.9766672e-04 9.4198900e-01 3.8815335e-07 6.4679525e-06 3.3740673e-06
 1.1278523e-07], sum to 1.0000
[2019-04-09 15:20:10,475] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4542
[2019-04-09 15:20:10,512] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 79.0, 42.0, 4.0, 22.5, 21.9473535718321, -0.4106528782069712, 1.0, 1.0, 45.0, 37.73713403777383], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2620800.0000, 
sim time next is 2622000.0000, 
raw observation next is [-7.100000000000001, 77.66666666666667, 65.33333333333334, 1.333333333333333, 22.5, 21.97446320800927, -0.3740780853009352, 1.0, 1.0, 45.0, 37.87380268912945], 
processed observation next is [1.0, 0.34782608695652173, 0.26592797783933514, 0.7766666666666667, 0.21777777777777782, 0.00147329650092081, 0.375, 0.3312052673341057, 0.3753073048996883, 1.0, 1.0, 0.6, 0.3787380268912945], 
reward next is 0.6213, 
noisyNet noise sample is [array([1.116975], dtype=float32), -1.0000548]. 
=============================================
[2019-04-09 15:20:10,550] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[46.51009 ]
 [45.322258]
 [45.388798]
 [44.52951 ]
 [44.482475]], R is [[48.02040482]
 [48.16283035]
 [48.3028717 ]
 [48.44242859]
 [48.58171082]].
[2019-04-09 15:20:10,557] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 29.0, 106.6666666666667, 319.5, 22.5, 26.04778059361403, 0.4750339972478682, 1.0, 1.0, 45.0, 32.651547193260996], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2562000.0000, 
sim time next is 2563200.0000, 
raw observation next is [3.3, 29.0, 92.0, 256.5, 22.5, 26.62504134903985, 0.5387376303670676, 1.0, 1.0, 45.0, 32.45697948637689], 
processed observation next is [1.0, 0.6956521739130435, 0.554016620498615, 0.29, 0.30666666666666664, 0.28342541436464086, 0.375, 0.7187534457533209, 0.6795792101223559, 1.0, 1.0, 0.6, 0.32456979486376886], 
reward next is 0.6754, 
noisyNet noise sample is [array([0.68305075], dtype=float32), -0.45414782]. 
=============================================
[2019-04-09 15:20:10,955] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.3244020e-14 1.3407850e-09 6.0382621e-07 3.3337189e-13 4.5825130e-01
 1.5003708e-03 5.4023862e-01 1.0679302e-08 8.3531741e-06 8.0313185e-07
 6.2958936e-09], sum to 1.0000
[2019-04-09 15:20:10,955] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4905
[2019-04-09 15:20:11,116] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4033846e-11 3.1679981e-08 5.8821507e-07 7.2728851e-11 1.5360168e-01
 1.8446849e-04 8.4619582e-01 2.7609397e-07 1.3858972e-05 3.2265928e-06
 6.5091811e-08], sum to 1.0000
[2019-04-09 15:20:11,116] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7664
[2019-04-09 15:20:11,138] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 58.0, 109.6666666666667, 789.8333333333334, 22.5, 25.3388682396746, 0.2395683887431732, 1.0, 1.0, 45.0, 34.76702322346435], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2726400.0000, 
sim time next is 2727600.0000, 
raw observation next is [-5.199999999999999, 57.0, 107.8333333333333, 778.8333333333334, 22.5, 24.86698265004001, 0.3266059755001166, 1.0, 1.0, 45.0, 34.665907486513404], 
processed observation next is [1.0, 0.5652173913043478, 0.31855955678670367, 0.57, 0.35944444444444434, 0.8605893186003684, 0.375, 0.572248554170001, 0.6088686585000388, 1.0, 1.0, 0.6, 0.34665907486513403], 
reward next is 0.6533, 
noisyNet noise sample is [array([0.25231984], dtype=float32), -1.2607033]. 
=============================================
[2019-04-09 15:20:11,199] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 64.0, 0.0, 0.0, 19.0, 23.60683169832964, -0.02901570044270704, 0.0, 1.0, 45.0, 33.82055705226182], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2593200.0000, 
sim time next is 2594400.0000, 
raw observation next is [-4.833333333333333, 66.0, 0.0, 0.0, 19.0, 23.57905192671241, -0.03494380212750733, 0.0, 1.0, 45.0, 33.904054591784615], 
processed observation next is [1.0, 0.0, 0.3287165281625116, 0.66, 0.0, 0.0, 0.08333333333333333, 0.4649209938927008, 0.4883520659574976, 0.0, 1.0, 0.6, 0.33904054591784616], 
reward next is 0.6610, 
noisyNet noise sample is [array([0.46886486], dtype=float32), 1.6530412]. 
=============================================
[2019-04-09 15:20:11,939] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7896500e-12 1.2567976e-09 4.4599756e-06 1.3211848e-11 1.7206576e-01
 1.9464783e-04 8.2770872e-01 2.8264631e-07 2.5194262e-05 8.1649188e-07
 4.6769969e-08], sum to 1.0000
[2019-04-09 15:20:11,940] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7983
[2019-04-09 15:20:11,971] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.733333333333333, 69.0, 0.0, 0.0, 19.0, 24.69668846101212, 0.2036046572176498, 0.0, 1.0, 45.0, 36.037754189138084], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2672400.0000, 
sim time next is 2673600.0000, 
raw observation next is [-4.366666666666666, 69.0, 0.0, 0.0, 19.0, 24.37433570275566, 0.1577214436153976, 0.0, 1.0, 45.0, 35.378898515950155], 
processed observation next is [1.0, 0.9565217391304348, 0.34164358264081257, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5311946418963048, 0.5525738145384659, 0.0, 1.0, 0.6, 0.3537889851595015], 
reward next is 0.6462, 
noisyNet noise sample is [array([-1.5097995], dtype=float32), 0.6567252]. 
=============================================
[2019-04-09 15:20:12,704] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.27227104e-09 1.75577966e-07 2.63136499e-05 1.44082146e-09
 2.91391999e-01 2.30089994e-03 7.06157625e-01 5.86728265e-06
 1.07202184e-04 8.89016519e-06 9.76163278e-07], sum to 1.0000
[2019-04-09 15:20:12,704] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0140
[2019-04-09 15:20:12,727] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.6, 85.66666666666667, 0.0, 0.0, 19.0, 22.35143364504373, -0.2998375468294288, 0.0, 1.0, 35.0, 26.022975391008675], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2690400.0000, 
sim time next is 2691600.0000, 
raw observation next is [-14.3, 88.33333333333334, 0.0, 0.0, 19.0, 22.0862440284283, -0.3582211026467197, 0.0, 1.0, 35.0, 23.7197788761203], 
processed observation next is [1.0, 0.13043478260869565, 0.06648199445983377, 0.8833333333333334, 0.0, 0.0, 0.08333333333333333, 0.34052033570235835, 0.38059296578442675, 0.0, 1.0, 0.4, 0.237197788761203], 
reward next is 0.7628, 
noisyNet noise sample is [array([1.3467816], dtype=float32), 0.8681357]. 
=============================================
[2019-04-09 15:20:12,759] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4358113e-09 1.8120153e-07 2.6060519e-05 1.4984775e-09 2.9074141e-01
 2.3157408e-03 7.0678836e-01 6.1729852e-06 1.1191683e-04 9.2425826e-06
 1.0315709e-06], sum to 1.0000
[2019-04-09 15:20:12,759] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9384
[2019-04-09 15:20:12,796] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.2818906e-13 9.6467456e-09 5.8625142e-07 5.3042802e-13 2.7304444e-02
 1.9958685e-05 9.7267115e-01 1.9225001e-07 1.2940289e-06 2.2634717e-06
 3.6268633e-09], sum to 1.0000
[2019-04-09 15:20:12,796] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7866
[2019-04-09 15:20:12,807] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 91.0, 0.0, 0.0, 19.0, 21.87728495762154, -0.3790135140296314, 0.0, 1.0, 45.0, 38.10571287174577], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2692800.0000, 
sim time next is 2694000.0000, 
raw observation next is [-15.0, 88.33333333333334, 0.0, 0.0, 19.0, 21.78684886524896, -0.3843651888510092, 0.0, 1.0, 45.0, 38.688538473158694], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.8833333333333334, 0.0, 0.0, 0.08333333333333333, 0.31557073877074665, 0.371878270382997, 0.0, 1.0, 0.6, 0.38688538473158696], 
reward next is 0.6131, 
noisyNet noise sample is [array([1.3467816], dtype=float32), 0.8681357]. 
=============================================
[2019-04-09 15:20:12,827] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 66.33333333333334, 0.0, 0.0, 19.0, 23.46370344873627, 0.01433447843492192, 0.0, 1.0, 45.0, 35.07421852928573], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2668800.0000, 
sim time next is 2670000.0000, 
raw observation next is [-2.466666666666666, 67.66666666666667, 0.0, 0.0, 19.0, 23.3854368318487, 0.0005545644625921387, 0.0, 1.0, 45.0, 35.216557652728184], 
processed observation next is [1.0, 0.9130434782608695, 0.39427516158818104, 0.6766666666666667, 0.0, 0.0, 0.08333333333333333, 0.4487864026540584, 0.500184854820864, 0.0, 1.0, 0.6, 0.35216557652728187], 
reward next is 0.6478, 
noisyNet noise sample is [array([0.49151054], dtype=float32), 0.56104785]. 
=============================================
[2019-04-09 15:20:12,831] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[54.71188 ]
 [55.69256 ]
 [56.81294 ]
 [56.651287]
 [57.80322 ]], R is [[55.06725693]
 [55.16584396]
 [55.26460266]
 [55.36391449]
 [55.46461868]].
[2019-04-09 15:20:12,832] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[41.448986]
 [41.30671 ]
 [41.512104]
 [41.75652 ]
 [42.50831 ]], R is [[41.85326004]
 [42.05366898]
 [42.39593506]
 [42.71174622]
 [42.99376678]].
[2019-04-09 15:20:12,961] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3367415e-12 5.0991888e-10 1.0490982e-07 5.5241396e-13 4.4281983e-01
 5.4650212e-04 5.5663109e-01 3.7483385e-08 1.6725874e-06 6.8940824e-07
 1.4668399e-09], sum to 1.0000
[2019-04-09 15:20:12,962] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3732
[2019-04-09 15:20:13,096] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 63.0, 0.0, 0.0, 22.5, 25.20183530951179, 0.3928239814596823, 0.0, 1.0, 45.0, 40.25867920252304], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2664000.0000, 
sim time next is 2665200.0000, 
raw observation next is [-1.2, 63.66666666666667, 0.0, 0.0, 19.0, 25.2512730405027, 0.3813898981715368, 0.0, 1.0, 35.0, 28.088412529022023], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.6366666666666667, 0.0, 0.0, 0.08333333333333333, 0.6042727533752249, 0.6271299660571789, 0.0, 1.0, 0.4, 0.2808841252902202], 
reward next is 0.7191, 
noisyNet noise sample is [array([2.0119855], dtype=float32), -0.31257597]. 
=============================================
[2019-04-09 15:20:13,159] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.32857348e-14 8.38857428e-10 1.41388159e-06 1.54468777e-13
 8.78093421e-01 3.09896976e-04 1.21583834e-01 9.24465198e-08
 5.80484675e-06 5.59124192e-06 4.77451287e-08], sum to 1.0000
[2019-04-09 15:20:13,160] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3161
[2019-04-09 15:20:13,297] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 47.0, 185.5, 168.0, 22.5, 25.63507180173431, 0.4650437849281246, 1.0, 1.0, 45.0, 38.55228334671394], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2646000.0000, 
sim time next is 2647200.0000, 
raw observation next is [0.5, 48.0, 165.1666666666667, 193.3333333333333, 22.5, 26.47726085897231, 0.5365402142813677, 1.0, 1.0, 35.0, 28.668985987192194], 
processed observation next is [1.0, 0.6521739130434783, 0.4764542936288089, 0.48, 0.5505555555555557, 0.21362799263351745, 0.375, 0.7064384049143593, 0.6788467380937893, 1.0, 1.0, 0.4, 0.2866898598719219], 
reward next is 0.7133, 
noisyNet noise sample is [array([0.09857506], dtype=float32), -0.24310417]. 
=============================================
[2019-04-09 15:20:13,301] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.2597887e-12 1.5166584e-09 2.3038579e-07 2.0413627e-12 4.6869400e-01
 7.5147598e-04 5.3054959e-01 8.4379103e-08 3.2488722e-06 1.3839345e-06
 3.3768122e-09], sum to 1.0000
[2019-04-09 15:20:13,301] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8328
[2019-04-09 15:20:13,358] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 65.0, 0.0, 0.0, 19.0, 24.90234072340871, 0.3003232479308284, 0.0, 1.0, 35.0, 22.69899041170202], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2667600.0000, 
sim time next is 2668800.0000, 
raw observation next is [-1.833333333333333, 66.33333333333334, 0.0, 0.0, 19.0, 24.74721415277821, 0.2991895039045475, 0.0, 1.0, 45.0, 38.80702364803781], 
processed observation next is [1.0, 0.9130434782608695, 0.41181902123730385, 0.6633333333333334, 0.0, 0.0, 0.08333333333333333, 0.5622678460648508, 0.5997298346348492, 0.0, 1.0, 0.6, 0.38807023648037814], 
reward next is 0.6119, 
noisyNet noise sample is [array([2.0119855], dtype=float32), -0.31257597]. 
=============================================
[2019-04-09 15:20:13,401] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.7755415e-14 9.5770958e-10 2.8565057e-07 8.7652883e-13 1.8498820e-01
 7.3843745e-05 8.1491554e-01 6.3955419e-08 2.1383979e-05 6.0340818e-07
 8.6206446e-09], sum to 1.0000
[2019-04-09 15:20:13,401] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6917
[2019-04-09 15:20:13,534] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 22.5, 25.50905807871221, 0.4215322265531413, 1.0, 1.0, 45.0, 56.01071717633836], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2746800.0000, 
sim time next is 2748000.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 22.5, 25.41520210438249, 0.4068167418089357, 0.0, 1.0, 45.0, 57.75941897483085], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.375, 0.6179335086985409, 0.6356055806029786, 0.0, 1.0, 0.6, 0.5775941897483086], 
reward next is 0.4224, 
noisyNet noise sample is [array([-0.22488725], dtype=float32), -2.504337]. 
=============================================
[2019-04-09 15:20:13,551] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[61.107384]
 [60.80728 ]
 [61.07063 ]
 [60.968353]
 [60.89954 ]], R is [[60.36917114]
 [60.20537186]
 [60.16481781]
 [60.02554703]
 [59.87834167]].
[2019-04-09 15:20:14,147] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7916622e-10 1.7219432e-07 1.5783562e-05 4.9526216e-10 6.9452125e-01
 1.1201593e-02 2.9378277e-01 8.3876894e-06 4.4117137e-04 2.7314967e-05
 1.4961494e-06], sum to 1.0000
[2019-04-09 15:20:14,147] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9630
[2019-04-09 15:20:14,192] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 81.33333333333334, 0.0, 0.0, 19.0, 22.32817991315824, -0.3388662463177455, 0.0, 1.0, 35.0, 27.854910119369364], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2611200.0000, 
sim time next is 2612400.0000, 
raw observation next is [-6.533333333333333, 79.66666666666667, 0.0, 0.0, 19.0, 22.13758834255807, -0.3629448595254433, 0.0, 1.0, 45.0, 36.39000990011669], 
processed observation next is [1.0, 0.21739130434782608, 0.2816251154201293, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.34479902854650596, 0.3790183801581855, 0.0, 1.0, 0.6, 0.3639000990011669], 
reward next is 0.6361, 
noisyNet noise sample is [array([-0.3792057], dtype=float32), -3.766718]. 
=============================================
[2019-04-09 15:20:14,212] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6504718e-10 1.6570280e-07 1.5228601e-05 4.6970838e-10 6.9114327e-01
 1.1104605e-02 2.9725474e-01 8.2396000e-06 4.4479355e-04 2.7497126e-05
 1.4856740e-06], sum to 1.0000
[2019-04-09 15:20:14,216] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2029
[2019-04-09 15:20:14,240] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.533333333333333, 79.66666666666667, 0.0, 0.0, 19.0, 22.13758834255807, -0.3629448595254433, 0.0, 1.0, 45.0, 36.39000990011669], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2612400.0000, 
sim time next is 2613600.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 19.0, 22.01216013818707, -0.396604495443406, 0.0, 1.0, 35.0, 28.63796181772978], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.3343466781822559, 0.3677985015188647, 0.0, 1.0, 0.4, 0.2863796181772978], 
reward next is 0.7136, 
noisyNet noise sample is [array([-0.3792057], dtype=float32), -3.766718]. 
=============================================
[2019-04-09 15:20:14,819] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.8595887e-11 1.5404382e-08 1.0038278e-06 1.7816219e-10 1.6383212e-02
 8.1007392e-04 9.8279035e-01 3.6514405e-07 1.1472807e-05 3.3939696e-06
 5.1822429e-08], sum to 1.0000
[2019-04-09 15:20:14,820] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1033
[2019-04-09 15:20:14,957] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 19.0, 23.18951890186011, -0.06682403784205466, 0.0, 1.0, 45.0, 36.60456525170346], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2685600.0000, 
sim time next is 2686800.0000, 
raw observation next is [-11.63333333333333, 78.33333333333334, 0.0, 0.0, 19.0, 23.18198351936245, -0.08456508041679629, 0.0, 1.0, 45.0, 36.5810484227234], 
processed observation next is [1.0, 0.08695652173913043, 0.14035087719298256, 0.7833333333333334, 0.0, 0.0, 0.08333333333333333, 0.4318319599468709, 0.4718116398610679, 0.0, 1.0, 0.6, 0.365810484227234], 
reward next is 0.6342, 
noisyNet noise sample is [array([0.21867462], dtype=float32), -0.41218355]. 
=============================================
[2019-04-09 15:20:15,811] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.79085840e-11 5.23982457e-09 1.52545135e-06 7.74508409e-12
 5.65090358e-01 2.48397631e-03 4.32405233e-01 3.16600477e-07
 1.07831065e-05 7.59411614e-06 1.90849605e-07], sum to 1.0000
[2019-04-09 15:20:15,811] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9039
[2019-04-09 15:20:15,950] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 65.0, 122.5, 183.0, 22.5, 24.32146520159174, 0.01114122784972791, 1.0, 1.0, 35.0, 32.11043977622025], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2628000.0000, 
sim time next is 2629200.0000, 
raw observation next is [-4.633333333333334, 64.0, 142.1666666666667, 244.3333333333333, 22.5, 24.43038568411066, 0.06148449924159779, 1.0, 1.0, 45.0, 42.69110112616245], 
processed observation next is [1.0, 0.43478260869565216, 0.3342566943674977, 0.64, 0.473888888888889, 0.26998158379373843, 0.375, 0.5358654736758884, 0.5204948330805326, 1.0, 1.0, 0.6, 0.4269110112616245], 
reward next is 0.5731, 
noisyNet noise sample is [array([1.0951302], dtype=float32), 1.259879]. 
=============================================
[2019-04-09 15:20:16,240] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5551661e-11 3.8101957e-09 1.3037334e-05 1.9847750e-11 3.5404375e-01
 1.7415311e-03 6.4410520e-01 1.8033570e-07 9.3580726e-05 2.5119634e-06
 1.0781593e-07], sum to 1.0000
[2019-04-09 15:20:16,240] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3247
[2019-04-09 15:20:16,308] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 24.6785772329465, 0.2108509809752261, 0.0, 1.0, 45.0, 38.86070489206769], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2758800.0000, 
sim time next is 2760000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 24.36998287503113, 0.1549652912343229, 0.0, 1.0, 35.0, 29.371328207736497], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.5308319062525942, 0.5516550970781077, 0.0, 1.0, 0.4, 0.29371328207736497], 
reward next is 0.7063, 
noisyNet noise sample is [array([-0.47748545], dtype=float32), -0.2894112]. 
=============================================
[2019-04-09 15:20:16,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[53.371506]
 [54.093643]
 [54.38774 ]
 [55.031326]
 [56.5802  ]], R is [[53.01125336]
 [53.09253311]
 [53.17043304]
 [53.23762894]
 [53.28672028]].
[2019-04-09 15:20:19,940] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4617056e-10 1.6236592e-07 1.2576400e-05 5.7271388e-10 5.8499116e-01
 2.2984517e-03 4.1262358e-01 9.8560522e-06 2.7080519e-05 3.6834877e-05
 2.5410185e-07], sum to 1.0000
[2019-04-09 15:20:19,940] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3208
[2019-04-09 15:20:20,000] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 22.97276453584394, -0.1550940089172008, 0.0, 1.0, 45.0, 35.918570390392446], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2773200.0000, 
sim time next is 2774400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 22.90854283692375, -0.1644082669221806, 0.0, 1.0, 35.0, 26.385362127259533], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.40904523641031254, 0.4451972443592731, 0.0, 1.0, 0.4, 0.2638536212725953], 
reward next is 0.7361, 
noisyNet noise sample is [array([-0.04527059], dtype=float32), 1.1904889]. 
=============================================
[2019-04-09 15:20:20,062] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1374124e-11 2.1129702e-08 1.6612470e-06 3.8072180e-12 7.2312677e-01
 1.1069975e-03 2.7571952e-01 1.6947571e-06 3.9767285e-05 3.4592931e-06
 1.5146055e-07], sum to 1.0000
[2019-04-09 15:20:20,062] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0118
[2019-04-09 15:20:20,083] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.666666666666667, 71.0, 0.0, 0.0, 19.0, 23.73003823300081, 0.04867668385851116, 0.0, 1.0, 35.0, 28.036294733560872], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2679600.0000, 
sim time next is 2680800.0000, 
raw observation next is [-8.333333333333334, 70.0, 0.0, 0.0, 19.0, 23.61535437169494, 0.006770061721347281, 0.0, 1.0, 35.0, 25.00891835338729], 
processed observation next is [1.0, 0.0, 0.23176361957525393, 0.7, 0.0, 0.0, 0.08333333333333333, 0.4679461976412451, 0.5022566872404491, 0.0, 1.0, 0.4, 0.2500891835338729], 
reward next is 0.7499, 
noisyNet noise sample is [array([1.0917852], dtype=float32), 0.9594775]. 
=============================================
[2019-04-09 15:20:21,115] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7209744e-14 2.8453942e-11 1.4958832e-08 7.7600576e-15 4.9791720e-02
 1.2828472e-04 9.5007831e-01 7.8568990e-10 1.5627971e-06 9.9324780e-08
 6.2333014e-11], sum to 1.0000
[2019-04-09 15:20:21,115] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4390
[2019-04-09 15:20:21,207] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666666, 26.0, 114.1666666666667, 0.0, 22.5, 26.87430878853322, 0.6211941317318858, 1.0, 1.0, 45.0, 50.52406494042195], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2817600.0000, 
sim time next is 2818800.0000, 
raw observation next is [7.0, 24.0, 106.5, 0.0, 22.5, 27.02681565188829, 0.6516230981156284, 1.0, 1.0, 45.0, 50.43575299760096], 
processed observation next is [1.0, 0.6521739130434783, 0.6565096952908588, 0.24, 0.355, 0.0, 0.375, 0.7522346376573573, 0.7172076993718761, 1.0, 1.0, 0.6, 0.5043575299760096], 
reward next is 0.4956, 
noisyNet noise sample is [array([0.87272424], dtype=float32), -0.6830163]. 
=============================================
[2019-04-09 15:20:21,256] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.41812082e-13 7.18809512e-10 6.05693117e-07 1.40378160e-13
 7.00152874e-01 2.15024884e-05 2.99819857e-01 6.54848833e-08
 4.96447819e-06 1.15642656e-07 2.76315215e-09], sum to 1.0000
[2019-04-09 15:20:21,256] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5770
[2019-04-09 15:20:21,426] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 19.0, 25.54993429654709, 0.4659254776346808, 0.0, 1.0, 35.0, 35.53256783768522], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2838000.0000, 
sim time next is 2839200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 19.0, 25.51807407086935, 0.4298727260574934, 0.0, 1.0, 35.0, 29.490032911333877], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.08333333333333333, 0.6265061725724458, 0.643290908685831, 0.0, 1.0, 0.4, 0.29490032911333874], 
reward next is 0.7051, 
noisyNet noise sample is [array([0.4141326], dtype=float32), 1.3545119]. 
=============================================
[2019-04-09 15:20:21,797] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4792170e-13 1.1249389e-09 9.9989904e-07 1.4371464e-12 4.5355529e-01
 1.0168136e-03 5.4541492e-01 6.9889630e-08 9.1073633e-07 1.1069812e-05
 6.9978032e-09], sum to 1.0000
[2019-04-09 15:20:21,798] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8104
[2019-04-09 15:20:21,894] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 54.0, 0.0, 0.0, 22.5, 26.07242667114082, 0.5454753208904654, 1.0, 1.0, 35.0, 41.899378117556694], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2743200.0000, 
sim time next is 2744400.0000, 
raw observation next is [-4.333333333333334, 55.66666666666667, 0.0, 0.0, 22.5, 26.21376756045581, 0.5170136108035459, 1.0, 1.0, 45.0, 53.801517456327915], 
processed observation next is [1.0, 0.782608695652174, 0.3425669436749769, 0.5566666666666668, 0.0, 0.0, 0.375, 0.6844806300379842, 0.6723378702678486, 1.0, 1.0, 0.6, 0.5380151745632792], 
reward next is 0.4620, 
noisyNet noise sample is [array([0.7497929], dtype=float32), 0.34133768]. 
=============================================
[2019-04-09 15:20:23,024] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8072065e-13 1.0183155e-09 4.9690549e-08 3.6303707e-13 2.5349665e-01
 7.2917819e-04 7.4575835e-01 2.8915769e-08 3.1045001e-06 1.2572267e-05
 2.1359972e-09], sum to 1.0000
[2019-04-09 15:20:23,025] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4716
[2019-04-09 15:20:23,152] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.76766021e-13 4.88727531e-11 6.70933957e-08 1.35626875e-13
 1.17705554e-01 1.43758647e-04 8.82147908e-01 3.64169250e-09
 2.97068453e-07 2.45809929e-06 4.58860150e-09], sum to 1.0000
[2019-04-09 15:20:23,152] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9247
[2019-04-09 15:20:23,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3829947e-12 8.0659062e-09 6.5904322e-07 4.0963374e-12 1.0135143e-02
 7.8622444e-04 9.8904026e-01 6.5937193e-09 3.6201207e-05 1.4371741e-06
 1.0415784e-08], sum to 1.0000
[2019-04-09 15:20:23,159] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1113
[2019-04-09 15:20:23,199] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 60.66666666666666, 0.0, 0.0, 19.0, 24.25926189557216, 0.2216531200984766, 0.0, 1.0, 45.0, 43.08371529567741], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2756400.0000, 
sim time next is 2757600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 24.30501937900926, 0.2200208584619423, 0.0, 1.0, 45.0, 40.58799414452775], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.525418281584105, 0.5733402861539808, 0.0, 1.0, 0.6, 0.4058799414452775], 
reward next is 0.5941, 
noisyNet noise sample is [array([-0.33879468], dtype=float32), 1.0596265]. 
=============================================
[2019-04-09 15:20:23,210] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.8816213e-11 5.4665268e-09 2.4606669e-07 8.5546579e-12 4.3030862e-02
 3.3883529e-04 9.5662445e-01 3.5810379e-07 4.6726104e-06 6.4825673e-07
 1.4789605e-08], sum to 1.0000
[2019-04-09 15:20:23,210] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7015
[2019-04-09 15:20:23,229] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 52.66666666666667, 0.0, 0.0, 22.5, 25.05021858971782, 0.2954173149265426, 1.0, 1.0, 35.0, 37.40758575067332], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2742000.0000, 
sim time next is 2743200.0000, 
raw observation next is [-4.0, 54.0, 0.0, 0.0, 22.5, 25.00893944963364, 0.3786327938863932, 1.0, 1.0, 45.0, 62.96991837226101], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.54, 0.0, 0.0, 0.375, 0.58407828746947, 0.6262109312954643, 1.0, 1.0, 0.6, 0.6296991837226101], 
reward next is 0.3703, 
noisyNet noise sample is [array([-1.8521878], dtype=float32), 0.79627895]. 
=============================================
[2019-04-09 15:20:23,270] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 19.0, 23.64220005666433, -0.01076327118944262, 0.0, 1.0, 45.0, 43.39721601527716], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2876400.0000, 
sim time next is 2877600.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 22.5, 23.66600695090528, -0.01158411825270621, 1.0, 1.0, 45.0, 41.7243500101635], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.0, 0.0, 0.375, 0.4721672459087734, 0.49613862724909796, 1.0, 1.0, 0.6, 0.41724350010163497], 
reward next is 0.5828, 
noisyNet noise sample is [array([-0.33376208], dtype=float32), -0.855532]. 
=============================================
[2019-04-09 15:20:23,344] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 22.5, 25.42727783450697, 0.4337132457718966, 0.0, 1.0, 45.0, 55.83682844931927], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2750400.0000, 
sim time next is 2751600.0000, 
raw observation next is [-5.333333333333334, 60.66666666666667, 0.0, 0.0, 19.0, 25.37933837109696, 0.4184684054818303, 0.0, 1.0, 45.0, 49.9159631085452], 
processed observation next is [1.0, 0.8695652173913043, 0.31486611265004616, 0.6066666666666667, 0.0, 0.0, 0.08333333333333333, 0.61494486425808, 0.6394894684939434, 0.0, 1.0, 0.6, 0.49915963108545197], 
reward next is 0.5008, 
noisyNet noise sample is [array([0.3325624], dtype=float32), -1.0672836]. 
=============================================
[2019-04-09 15:20:23,555] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1086371e-13 1.3932358e-11 1.1875608e-08 1.1512621e-13 7.7799715e-02
 7.7327226e-05 9.2212218e-01 6.8576362e-09 7.6288921e-07 3.3910634e-08
 5.4581600e-10], sum to 1.0000
[2019-04-09 15:20:23,555] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2649
[2019-04-09 15:20:23,708] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 111.0, 793.5, 22.5, 25.20771945655091, 0.3050234067105264, 1.0, 1.0, 45.0, 34.76069609371285], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2725200.0000, 
sim time next is 2726400.0000, 
raw observation next is [-5.6, 58.0, 109.6666666666667, 789.8333333333334, 22.5, 25.34441361839472, 0.3559813680914763, 1.0, 1.0, 45.0, 34.28388406324372], 
processed observation next is [1.0, 0.5652173913043478, 0.30747922437673136, 0.58, 0.3655555555555557, 0.872744014732965, 0.375, 0.6120344681995601, 0.6186604560304921, 1.0, 1.0, 0.6, 0.3428388406324372], 
reward next is 0.6572, 
noisyNet noise sample is [array([1.480476], dtype=float32), 1.7450541]. 
=============================================
[2019-04-09 15:20:23,944] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3759651e-13 6.4328037e-10 8.7672601e-07 1.7868651e-12 1.2227850e-02
 7.2864837e-05 9.8769170e-01 2.7958889e-08 6.5728809e-06 1.3698524e-07
 2.1638678e-09], sum to 1.0000
[2019-04-09 15:20:23,944] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1091
[2019-04-09 15:20:23,974] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 23.85792037234485, 0.07181680549799278, 0.0, 1.0, 45.0, 36.74895656106236], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2763600.0000, 
sim time next is 2764800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 23.78107387797414, 0.05042588710568696, 0.0, 1.0, 45.0, 36.217796255142346], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.48175615649784503, 0.516808629035229, 0.0, 1.0, 0.6, 0.3621779625514235], 
reward next is 0.6378, 
noisyNet noise sample is [array([-1.9917278], dtype=float32), -0.2986491]. 
=============================================
[2019-04-09 15:20:24,240] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9554161e-11 4.8752028e-08 1.1538692e-06 7.1545222e-12 2.8776723e-01
 1.0140694e-04 7.1212614e-01 6.3472277e-07 1.7838730e-06 1.4979072e-06
 6.4279320e-08], sum to 1.0000
[2019-04-09 15:20:24,240] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9683
[2019-04-09 15:20:24,322] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 74.33333333333334, 0.0, 0.0, 19.0, 24.77438650902497, 0.2399031080262238, 0.0, 1.0, 35.0, 28.093000087673417], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2856000.0000, 
sim time next is 2857200.0000, 
raw observation next is [1.0, 76.66666666666667, 0.0, 0.0, 19.0, 24.59648478866925, 0.2184978640007949, 0.0, 1.0, 45.0, 46.36229244167961], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7666666666666667, 0.0, 0.0, 0.08333333333333333, 0.5497070657224375, 0.5728326213335982, 0.0, 1.0, 0.6, 0.4636229244167961], 
reward next is 0.5364, 
noisyNet noise sample is [array([-0.24826866], dtype=float32), -0.13265245]. 
=============================================
[2019-04-09 15:20:24,711] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8071024e-13 1.2303732e-10 2.4317474e-08 5.6112389e-14 2.3559209e-02
 1.4804481e-04 9.7629255e-01 1.3820330e-09 1.6739116e-07 4.6107662e-08
 2.7141080e-09], sum to 1.0000
[2019-04-09 15:20:24,712] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5220
[2019-04-09 15:20:24,845] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666666, 26.0, 114.1666666666667, 0.0, 22.5, 26.77857104508174, 0.5956858943624194, 1.0, 1.0, 45.0, 50.41922599230859], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2817600.0000, 
sim time next is 2818800.0000, 
raw observation next is [7.0, 24.0, 106.5, 0.0, 22.5, 26.93475891728027, 0.6274097843691014, 1.0, 1.0, 45.0, 50.525859509054385], 
processed observation next is [1.0, 0.6521739130434783, 0.6565096952908588, 0.24, 0.355, 0.0, 0.375, 0.7445632431066892, 0.7091365947897005, 1.0, 1.0, 0.6, 0.5052585950905438], 
reward next is 0.4947, 
noisyNet noise sample is [array([0.5212735], dtype=float32), 1.2853898]. 
=============================================
[2019-04-09 15:20:26,142] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.3706610e-11 1.5595528e-08 2.3991392e-06 4.2841349e-11 4.6596062e-01
 2.4130748e-04 5.3378445e-01 1.5602856e-07 7.1971126e-06 3.7849734e-06
 4.5898176e-08], sum to 1.0000
[2019-04-09 15:20:26,142] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9292
[2019-04-09 15:20:26,224] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 19.0, 23.09804818781418, -0.03678965155770834, 0.0, 1.0, 35.0, 24.190894752416614], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2937600.0000, 
sim time next is 2938800.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 22.93278192172982, -0.0500525600195971, 0.0, 1.0, 45.0, 36.011252585377065], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.4110651601441517, 0.48331581332680096, 0.0, 1.0, 0.6, 0.3601125258537706], 
reward next is 0.6399, 
noisyNet noise sample is [array([0.08358388], dtype=float32), -0.42919677]. 
=============================================
[2019-04-09 15:20:26,686] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.3624960e-12 1.7158490e-09 7.5833486e-07 5.1098752e-12 1.2799722e-01
 1.3799095e-04 8.7185740e-01 2.0083777e-07 5.7462184e-06 6.4779005e-07
 1.2088388e-08], sum to 1.0000
[2019-04-09 15:20:26,686] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3656
[2019-04-09 15:20:26,741] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 68.66666666666667, 0.0, 0.0, 19.0, 25.0620374530012, 0.3247749585385595, 0.0, 1.0, 35.0, 26.251748948777568], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2850000.0000, 
sim time next is 2851200.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 19.0, 24.8789455574641, 0.2760310456875049, 0.0, 1.0, 45.0, 43.67271876146175], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.72, 0.0, 0.0, 0.08333333333333333, 0.5732454631220083, 0.5920103485625017, 0.0, 1.0, 0.6, 0.43672718761461754], 
reward next is 0.5633, 
noisyNet noise sample is [array([-0.67200094], dtype=float32), 1.1964993]. 
=============================================
[2019-04-09 15:20:26,753] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5381891e-13 3.6757566e-09 1.8218559e-07 6.6116996e-13 2.0653278e-01
 1.0779928e-03 7.9236835e-01 2.0791362e-08 1.9737003e-05 8.1807286e-07
 2.1600181e-08], sum to 1.0000
[2019-04-09 15:20:26,754] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0248
[2019-04-09 15:20:26,888] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 51.66666666666666, 165.8333333333333, 550.5, 22.5, 25.68817869294455, 0.3474918814358736, 1.0, 1.0, 35.0, 30.366546419860406], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2803200.0000, 
sim time next is 2804400.0000, 
raw observation next is [-1.0, 50.0, 149.5, 635.5, 22.5, 25.7670002012009, 0.3738089890072314, 1.0, 1.0, 35.0, 24.187813431436716], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.5, 0.49833333333333335, 0.7022099447513812, 0.375, 0.6472500167667418, 0.6246029963357438, 1.0, 1.0, 0.4, 0.24187813431436717], 
reward next is 0.7581, 
noisyNet noise sample is [array([-0.22781646], dtype=float32), -0.75386906]. 
=============================================
[2019-04-09 15:20:29,520] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.3724673e-15 2.2851774e-11 3.6771286e-08 7.7371997e-16 6.5540443e-03
 1.5018024e-05 9.9343067e-01 3.8790798e-10 1.9560761e-07 7.9413134e-09
 3.1438516e-09], sum to 1.0000
[2019-04-09 15:20:29,572] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3028
[2019-04-09 15:20:29,656] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 85.83333333333334, 0.0, 22.5, 25.32895183873921, 0.4063146830933653, 1.0, 1.0, 45.0, 31.519522689402304], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2906400.0000, 
sim time next is 2907600.0000, 
raw observation next is [2.0, 100.0, 82.66666666666667, 8.999999999999998, 22.5, 25.27930780505775, 0.4180892024032303, 1.0, 1.0, 45.0, 31.137908852192435], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.27555555555555555, 0.009944751381215467, 0.375, 0.6066089837548126, 0.6393630674677434, 1.0, 1.0, 0.6, 0.31137908852192436], 
reward next is 0.6886, 
noisyNet noise sample is [array([-0.6476428], dtype=float32), -2.0512106]. 
=============================================
[2019-04-09 15:20:30,010] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3308807e-12 1.5257939e-09 2.1749095e-07 5.3065547e-12 5.1161766e-01
 4.5385092e-04 4.8791996e-01 2.4159223e-07 4.0741329e-06 4.0143441e-06
 7.5888993e-09], sum to 1.0000
[2019-04-09 15:20:30,010] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0951
[2019-04-09 15:20:30,063] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.110223024625157e-16, 48.0, 133.1666666666667, 720.5, 22.5, 24.90965850615292, 0.1725442796046325, 1.0, 1.0, 35.0, 26.98687057542374], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2805600.0000, 
sim time next is 2806800.0000, 
raw observation next is [0.9999999999999999, 46.0, 133.8333333333333, 750.0, 22.5, 24.89462151192191, 0.1824464961867686, 1.0, 1.0, 35.0, 36.152338225664124], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.46, 0.44611111111111096, 0.8287292817679558, 0.375, 0.574551792660159, 0.5608154987289229, 1.0, 1.0, 0.4, 0.36152338225664127], 
reward next is 0.6385, 
noisyNet noise sample is [array([0.2114677], dtype=float32), 2.5558007]. 
=============================================
[2019-04-09 15:20:30,778] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.34490861e-13 1.07534884e-10 5.94062385e-07 8.26856030e-15
 2.10269332e-01 4.08410910e-04 7.89316237e-01 1.41218424e-08
 3.86849388e-06 1.56784995e-06 2.25998376e-09], sum to 1.0000
[2019-04-09 15:20:30,780] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3682
[2019-04-09 15:20:30,839] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 100.0, 63.5, 0.0, 22.5, 25.13367351390309, 0.2425541409924317, 1.0, 1.0, 45.0, 41.405587268724915], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2887200.0000, 
sim time next is 2888400.0000, 
raw observation next is [0.3333333333333333, 97.66666666666667, 73.16666666666666, 0.0, 22.5, 25.230841456001, 0.2683353979120492, 1.0, 1.0, 45.0, 39.862238310333], 
processed observation next is [1.0, 0.43478260869565216, 0.4718374884579871, 0.9766666666666667, 0.24388888888888885, 0.0, 0.375, 0.6025701213334166, 0.5894451326373497, 1.0, 1.0, 0.6, 0.39862238310333], 
reward next is 0.6014, 
noisyNet noise sample is [array([0.5015679], dtype=float32), 1.2240818]. 
=============================================
[2019-04-09 15:20:31,434] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5488112e-09 4.1753910e-07 1.1302649e-05 1.0982077e-09 2.7174792e-01
 2.0661831e-03 7.2585076e-01 1.5506552e-06 2.7134767e-04 4.9089442e-05
 1.4376724e-06], sum to 1.0000
[2019-04-09 15:20:31,435] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5861
[2019-04-09 15:20:31,481] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 65.0, 0.0, 0.0, 19.0, 22.32090866993919, -0.2858831288618333, 0.0, 1.0, 45.0, 31.706583367514668], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3015600.0000, 
sim time next is 3016800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 22.28445413175766, -0.2957491232678133, 0.0, 1.0, 45.0, 31.612698374410655], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.3570378443131383, 0.40141695891072887, 0.0, 1.0, 0.6, 0.3161269837441065], 
reward next is 0.6839, 
noisyNet noise sample is [array([-0.36976382], dtype=float32), 1.0824897]. 
=============================================
[2019-04-09 15:20:32,354] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9422427e-11 4.4898236e-08 2.7728734e-07 7.7941931e-11 1.7170565e-01
 3.4310552e-04 8.2777125e-01 3.7183506e-07 1.7753831e-04 1.6427925e-06
 7.6162813e-08], sum to 1.0000
[2019-04-09 15:20:32,354] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8769
[2019-04-09 15:20:32,385] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 81.33333333333334, 0.0, 0.0, 19.0, 23.11829752624667, -0.1233573203167529, 0.0, 1.0, 45.0, 46.38172982215637], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2859600.0000, 
sim time next is 2860800.0000, 
raw observation next is [1.0, 83.66666666666666, 0.0, 0.0, 19.0, 23.10285998253454, -0.1236998219799921, 0.0, 1.0, 45.0, 46.84522479277123], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.8366666666666666, 0.0, 0.0, 0.08333333333333333, 0.4252383318778783, 0.4587667260066693, 0.0, 1.0, 0.6, 0.46845224792771234], 
reward next is 0.5315, 
noisyNet noise sample is [array([0.14600734], dtype=float32), 2.4039383]. 
=============================================
[2019-04-09 15:20:32,426] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2438180e-13 1.4267848e-10 1.5570320e-07 4.7813131e-13 7.4752814e-01
 1.1062044e-04 2.5235006e-01 1.8186778e-08 1.0916013e-05 6.4589123e-08
 9.1737995e-09], sum to 1.0000
[2019-04-09 15:20:32,429] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4455
[2019-04-09 15:20:32,435] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.10673217e-09 7.24483425e-06 1.42296602e-04 1.59074585e-08
 4.25064802e-01 1.29269045e-02 5.61473906e-01 1.29322734e-05
 2.34877865e-04 1.35150825e-04 1.88033846e-06], sum to 1.0000
[2019-04-09 15:20:32,442] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1820
[2019-04-09 15:20:32,491] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333333, 97.66666666666667, 73.16666666666666, 0.0, 22.5, 25.04378005830061, 0.2278877504696475, 1.0, 1.0, 35.0, 27.555627583196546], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2888400.0000, 
sim time next is 2889600.0000, 
raw observation next is [0.6666666666666666, 95.33333333333334, 79.5, 0.0, 22.5, 25.04740941774364, 0.225050277662082, 1.0, 1.0, 35.0, 24.054139226815536], 
processed observation next is [1.0, 0.43478260869565216, 0.4810710987996307, 0.9533333333333335, 0.265, 0.0, 0.375, 0.5872841181453033, 0.575016759220694, 1.0, 1.0, 0.4, 0.24054139226815535], 
reward next is 0.7595, 
noisyNet noise sample is [array([0.1025009], dtype=float32), 1.8006532]. 
=============================================
[2019-04-09 15:20:32,499] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 19.0, 22.4121852027152, -0.1969282472293075, 0.0, 1.0, 45.0, 35.78408032851771], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2953200.0000, 
sim time next is 2954400.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 19.0, 22.3478764693652, -0.2229634821820989, 0.0, 1.0, 35.0, 27.95681977376069], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.84, 0.0, 0.0, 0.08333333333333333, 0.3623230391137667, 0.42567883927263367, 0.0, 1.0, 0.4, 0.2795681977376069], 
reward next is 0.7204, 
noisyNet noise sample is [array([0.39044464], dtype=float32), 0.36436653]. 
=============================================
[2019-04-09 15:20:32,808] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1958629e-08 1.3016313e-06 8.9123232e-06 2.5047628e-09 1.6181324e-01
 1.5057730e-03 8.3637077e-01 4.3518617e-06 1.0207925e-04 1.9264898e-04
 9.8341900e-07], sum to 1.0000
[2019-04-09 15:20:32,808] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9761
[2019-04-09 15:20:32,852] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 19.0, 22.64720415609142, -0.1433936274721277, 0.0, 1.0, 45.0, 35.18095348374668], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2948400.0000, 
sim time next is 2949600.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 19.0, 22.55643869934001, -0.1537727226495841, 0.0, 1.0, 45.0, 35.88808741056241], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.08333333333333333, 0.3797032249450008, 0.44874242578347195, 0.0, 1.0, 0.6, 0.35888087410562414], 
reward next is 0.6411, 
noisyNet noise sample is [array([-0.75236315], dtype=float32), -0.24789292]. 
=============================================
[2019-04-09 15:20:33,169] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4242071e-10 3.2996761e-07 2.3090361e-06 9.7537023e-10 1.2949187e-01
 9.9223608e-04 8.6942285e-01 2.7668088e-06 5.1200932e-05 3.5008929e-05
 1.4349827e-06], sum to 1.0000
[2019-04-09 15:20:33,169] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0620
[2019-04-09 15:20:33,203] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 56.66666666666667, 14.33333333333333, 161.5, 19.0, 23.00826928157584, -0.1024048053931499, 0.0, 1.0, 45.0, 31.072460116020856], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3000000.0000, 
sim time next is 3001200.0000, 
raw observation next is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 19.0, 22.96844066938073, -0.125218995764795, 0.0, 1.0, 35.0, 24.182319840849203], 
processed observation next is [0.0, 0.7391304347826086, 0.4164358264081256, 0.5833333333333335, 0.0, 0.0, 0.08333333333333333, 0.4140367224483941, 0.4582603347450684, 0.0, 1.0, 0.4, 0.24182319840849203], 
reward next is 0.7582, 
noisyNet noise sample is [array([-0.9050145], dtype=float32), 0.29591125]. 
=============================================
[2019-04-09 15:20:33,527] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.0282803e-09 2.3658723e-07 4.7373138e-05 6.0795369e-09 1.5124309e-01
 1.3907233e-03 8.4725398e-01 4.2539932e-06 3.6083260e-05 2.1873231e-05
 2.4617220e-06], sum to 1.0000
[2019-04-09 15:20:33,528] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0834
[2019-04-09 15:20:33,580] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 67.0, 191.0, 67.33333333333331, 19.0, 21.45994970851524, -0.410052675262528, 0.0, 1.0, 35.0, 30.30423720043953], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2976000.0000, 
sim time next is 2977200.0000, 
raw observation next is [-3.0, 65.0, 217.0, 154.0, 19.0, 21.47214242262282, -0.4041416866565402, 0.0, 1.0, 35.0, 27.381935005320173], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.7233333333333334, 0.17016574585635358, 0.08333333333333333, 0.289345201885235, 0.36528610444781995, 0.0, 1.0, 0.4, 0.27381935005320174], 
reward next is 0.7262, 
noisyNet noise sample is [array([-0.90509427], dtype=float32), -0.9436828]. 
=============================================
[2019-04-09 15:20:34,028] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2778477e-10 5.3053522e-08 6.7995188e-06 1.5878095e-10 6.0606923e-02
 1.0627046e-03 9.3829566e-01 3.7625051e-07 2.4420289e-05 2.6861844e-06
 2.7477137e-07], sum to 1.0000
[2019-04-09 15:20:34,053] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9422
[2019-04-09 15:20:34,073] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7333333333333334, 85.33333333333334, 0.0, 0.0, 19.0, 22.73199460159025, -0.2443766563951136, 0.0, 1.0, 45.0, 31.6204499816318], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3090000.0000, 
sim time next is 3091200.0000, 
raw observation next is [-0.8666666666666667, 88.66666666666666, 0.0, 0.0, 19.0, 22.64825329289743, -0.2548989691020454, 0.0, 1.0, 45.0, 32.74048466741012], 
processed observation next is [0.0, 0.782608695652174, 0.4385964912280702, 0.8866666666666666, 0.0, 0.0, 0.08333333333333333, 0.3873544410747858, 0.41503367696598487, 0.0, 1.0, 0.6, 0.32740484667410125], 
reward next is 0.6726, 
noisyNet noise sample is [array([-1.1453753], dtype=float32), -0.78031594]. 
=============================================
[2019-04-09 15:20:34,095] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2670733e-14 4.9055027e-10 3.3821816e-08 8.2070100e-14 4.9194884e-02
 5.0749234e-04 9.5029449e-01 2.0992983e-09 2.9875473e-06 3.5145721e-08
 7.9796765e-09], sum to 1.0000
[2019-04-09 15:20:34,096] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5445
[2019-04-09 15:20:34,133] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 87.33333333333334, 0.0, 0.0, 22.5, 24.89135394731129, 0.3565347566805825, 0.0, 1.0, 45.0, 33.498441804124994], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2920800.0000, 
sim time next is 2922000.0000, 
raw observation next is [-1.0, 82.66666666666667, 0.0, 0.0, 22.5, 24.73448100818795, 0.3361541787066206, 1.0, 1.0, 45.0, 34.48421767397541], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.8266666666666667, 0.0, 0.0, 0.375, 0.5612067506823291, 0.6120513929022069, 1.0, 1.0, 0.6, 0.3448421767397541], 
reward next is 0.6552, 
noisyNet noise sample is [array([0.700841], dtype=float32), 0.5874347]. 
=============================================
[2019-04-09 15:20:34,149] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[61.615635]
 [62.350372]
 [62.84219 ]
 [63.043243]
 [63.18319 ]], R is [[61.44153976]
 [61.49214172]
 [61.61562347]
 [61.6697464 ]
 [61.72444916]].
[2019-04-09 15:20:34,516] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4730482e-09 2.9235184e-06 2.0508907e-05 1.5546949e-09 5.3071612e-01
 1.5542919e-03 4.6757969e-01 2.1118994e-06 1.1803164e-04 5.6030171e-06
 8.2154469e-07], sum to 1.0000
[2019-04-09 15:20:34,516] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3830
[2019-04-09 15:20:34,524] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.2338503e-12 2.0115665e-09 3.1730322e-06 2.9362245e-11 3.4662667e-01
 3.0281057e-03 6.5032339e-01 1.5879162e-07 1.7226923e-05 1.2169410e-06
 8.9645681e-08], sum to 1.0000
[2019-04-09 15:20:34,524] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4751
[2019-04-09 15:20:34,549] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 55.0, 31.0, 286.5, 19.0, 22.94617016591657, -0.1047839409571814, 0.0, 1.0, 35.0, 23.956327695940104], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 2998800.0000, 
sim time next is 3000000.0000, 
raw observation next is [-1.333333333333333, 56.66666666666667, 14.33333333333333, 161.5, 19.0, 22.92476315198385, -0.1386515274899274, 0.0, 1.0, 35.0, 20.704459942869462], 
processed observation next is [0.0, 0.7391304347826086, 0.42566943674976926, 0.5666666666666668, 0.047777777777777766, 0.17845303867403314, 0.08333333333333333, 0.4103969293319875, 0.4537828241700242, 0.0, 1.0, 0.4, 0.20704459942869463], 
reward next is 0.7930, 
noisyNet noise sample is [array([-2.3823807], dtype=float32), 0.8251128]. 
=============================================
[2019-04-09 15:20:34,581] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[43.460148]
 [44.17839 ]
 [44.142883]
 [44.377075]
 [44.783325]], R is [[43.62253571]
 [43.94674683]
 [44.17975616]
 [44.51878738]
 [44.83056641]].
[2019-04-09 15:20:34,584] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666666, 100.0, 85.66666666666667, 434.5, 22.5, 23.21714674300415, -0.1320474107290766, 1.0, 1.0, 45.0, 35.14423557176541], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3141600.0000, 
sim time next is 3142800.0000, 
raw observation next is [7.0, 100.0, 91.0, 519.5, 22.5, 23.65202956852706, -0.04838897445229681, 1.0, 1.0, 45.0, 33.69152281448957], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.30333333333333334, 0.5740331491712707, 0.375, 0.4710024640439216, 0.48387034184923444, 1.0, 1.0, 0.6, 0.3369152281448957], 
reward next is 0.6631, 
noisyNet noise sample is [array([0.96511495], dtype=float32), -0.15357818]. 
=============================================
[2019-04-09 15:20:34,601] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.5184342e-13 1.1312260e-09 2.1496144e-06 1.3895288e-11 3.3216706e-01
 2.5263694e-03 6.6529179e-01 9.3221672e-08 1.1690288e-05 8.2927193e-07
 5.2038899e-08], sum to 1.0000
[2019-04-09 15:20:34,606] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8355
[2019-04-09 15:20:34,635] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8583703e-11 1.1041786e-09 6.1971991e-07 1.1868094e-12 1.6789386e-01
 4.9878217e-05 8.3205205e-01 1.0998865e-07 3.2421153e-06 1.9417993e-07
 2.6245285e-08], sum to 1.0000
[2019-04-09 15:20:34,639] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9266
[2019-04-09 15:20:34,655] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 91.0, 519.5, 22.5, 23.65202956852706, -0.04838897445229681, 1.0, 1.0, 45.0, 33.69152281448957], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3142800.0000, 
sim time next is 3144000.0000, 
raw observation next is [7.0, 100.0, 96.33333333333333, 604.5, 22.5, 23.99516753430213, 0.03055022063255424, 1.0, 1.0, 45.0, 32.36365563724071], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.32111111111111107, 0.6679558011049723, 0.375, 0.4995972945251775, 0.5101834068775181, 1.0, 1.0, 0.6, 0.3236365563724071], 
reward next is 0.6764, 
noisyNet noise sample is [array([0.96511495], dtype=float32), -0.15357818]. 
=============================================
[2019-04-09 15:20:34,663] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[55.103268]
 [53.56274 ]
 [52.687283]
 [51.001717]
 [49.700676]], R is [[56.81810379]
 [56.91300964]
 [56.99243927]
 [57.04384232]
 [57.05366898]].
[2019-04-09 15:20:34,665] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 19.0, 23.45005655854104, 0.0642925752987149, 0.0, 1.0, 45.0, 35.50959055683283], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2935200.0000, 
sim time next is 2936400.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 23.36344746073254, 0.05479114926163989, 0.0, 1.0, 45.0, 35.23346871649259], 
processed observation next is [1.0, 1.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.44695395506104507, 0.5182637164205467, 0.0, 1.0, 0.6, 0.3523346871649259], 
reward next is 0.6477, 
noisyNet noise sample is [array([0.46061626], dtype=float32), 0.27118856]. 
=============================================
[2019-04-09 15:20:34,814] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.8675876e-10 8.0259234e-08 3.6128063e-06 9.8968875e-11 1.9421147e-01
 4.8996659e-04 8.0527556e-01 5.1436916e-07 1.3207460e-05 5.0111125e-06
 5.3427402e-07], sum to 1.0000
[2019-04-09 15:20:34,822] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8096
[2019-04-09 15:20:34,864] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 31.0, 286.5, 19.0, 23.19290022844424, -0.04817868858844684, 0.0, 1.0, 45.0, 30.42894294772684], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2998800.0000, 
sim time next is 3000000.0000, 
raw observation next is [-1.333333333333333, 56.66666666666667, 14.33333333333333, 161.5, 19.0, 23.19747870028921, -0.06610898133296624, 0.0, 1.0, 45.0, 30.302649434102108], 
processed observation next is [0.0, 0.7391304347826086, 0.42566943674976926, 0.5666666666666668, 0.047777777777777766, 0.17845303867403314, 0.08333333333333333, 0.43312322502410083, 0.47796367288901126, 0.0, 1.0, 0.6, 0.3030264943410211], 
reward next is 0.6970, 
noisyNet noise sample is [array([0.84751517], dtype=float32), -0.27066818]. 
=============================================
[2019-04-09 15:20:34,872] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[44.448036]
 [44.77871 ]
 [44.61684 ]
 [45.24517 ]
 [45.445457]], R is [[44.1699028 ]
 [44.42391205]
 [44.6699791 ]
 [44.97521591]
 [45.22316742]].
[2019-04-09 15:20:35,302] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4080319e-09 3.0417564e-07 1.2458784e-05 3.5023791e-09 4.3208629e-02
 3.3089102e-04 9.5635897e-01 4.8154989e-06 6.7984744e-05 1.5317128e-05
 6.9386607e-07], sum to 1.0000
[2019-04-09 15:20:35,307] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3982
[2019-04-09 15:20:35,332] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 73.0, 95.66666666666666, 62.83333333333333, 19.0, 21.39542173113775, -0.3984061372696503, 0.0, 1.0, 45.0, 47.174583700675726], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2968800.0000, 
sim time next is 2970000.0000, 
raw observation next is [-4.0, 71.0, 119.0, 90.5, 19.0, 21.51693232631369, -0.3774935874290109, 0.0, 1.0, 45.0, 44.7611220862079], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.39666666666666667, 0.1, 0.08333333333333333, 0.29307769385947413, 0.3741688041903297, 0.0, 1.0, 0.6, 0.447611220862079], 
reward next is 0.5524, 
noisyNet noise sample is [array([0.03697083], dtype=float32), -0.08065296]. 
=============================================
[2019-04-09 15:20:35,355] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[38.941196]
 [38.20863 ]
 [37.91734 ]
 [37.996906]
 [37.766506]], R is [[39.23968887]
 [39.37554932]
 [39.49947739]
 [39.71422577]
 [39.80835724]].
[2019-04-09 15:20:35,408] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.5480415e-10 1.2573804e-07 7.2520247e-06 5.1001109e-10 3.3926040e-01
 1.3284475e-03 6.5902269e-01 6.5778404e-06 3.4262086e-04 2.6447186e-05
 5.4909306e-06], sum to 1.0000
[2019-04-09 15:20:35,408] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6798
[2019-04-09 15:20:35,435] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 63.33333333333334, 121.8333333333333, 781.8333333333334, 19.0, 22.32308722528509, -0.1825016551286286, 0.0, 1.0, 45.0, 32.91905886406407], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2985600.0000, 
sim time next is 2986800.0000, 
raw observation next is [-2.333333333333333, 61.66666666666667, 108.5, 791.8333333333334, 19.0, 22.41062097680062, -0.1658777118721575, 0.0, 1.0, 45.0, 32.0630440263052], 
processed observation next is [0.0, 0.5652173913043478, 0.3979686057248385, 0.6166666666666667, 0.3616666666666667, 0.8749539594843463, 0.08333333333333333, 0.36755174806671825, 0.4447074293759475, 0.0, 1.0, 0.6, 0.320630440263052], 
reward next is 0.6794, 
noisyNet noise sample is [array([1.214931], dtype=float32), 0.7557671]. 
=============================================
[2019-04-09 15:20:35,627] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3190963e-10 2.8726288e-07 3.6412800e-06 5.4174715e-10 7.8922980e-02
 6.3419843e-04 9.2034745e-01 2.1189003e-06 7.5571894e-05 1.3109665e-05
 7.7465836e-07], sum to 1.0000
[2019-04-09 15:20:35,627] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9994
[2019-04-09 15:20:35,654] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 243.5, 351.8333333333333, 19.0, 22.01456080692129, -0.2613609060853792, 0.0, 1.0, 45.0, 36.403560076835106], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2979600.0000, 
sim time next is 2980800.0000, 
raw observation next is [-3.0, 65.0, 218.5, 487.5, 19.0, 22.12704626101338, -0.2365862752502628, 0.0, 1.0, 45.0, 35.576640626489734], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.7283333333333334, 0.5386740331491713, 0.08333333333333333, 0.343920521751115, 0.42113790824991243, 0.0, 1.0, 0.6, 0.35576640626489736], 
reward next is 0.6442, 
noisyNet noise sample is [array([0.17687526], dtype=float32), 0.93100405]. 
=============================================
[2019-04-09 15:20:35,911] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.5964494e-08 6.6568685e-07 8.0945174e-05 4.5950621e-08 3.0645514e-01
 2.0079464e-03 6.9084448e-01 5.9892918e-06 4.7333224e-04 1.2402216e-04
 7.3851593e-06], sum to 1.0000
[2019-04-09 15:20:35,911] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5401
[2019-04-09 15:20:35,939] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 19.0, 21.55685616598318, -0.4727380791510455, 0.0, 1.0, 35.0, 25.323833536171406], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3028800.0000, 
sim time next is 3030000.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 19.0, 21.4914570665694, -0.4861565658958307, 0.0, 1.0, 45.0, 32.314858287154834], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.08333333333333333, 0.29095475554744993, 0.33794781136805646, 0.0, 1.0, 0.6, 0.32314858287154835], 
reward next is 0.6769, 
noisyNet noise sample is [array([-0.24180175], dtype=float32), -0.74274176]. 
=============================================
[2019-04-09 15:20:35,949] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[36.156273]
 [36.822968]
 [37.185654]
 [37.97084 ]
 [39.336536]], R is [[36.16835785]
 [36.55343628]
 [36.85938644]
 [37.16985321]
 [37.55542755]].
[2019-04-09 15:20:36,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.05168135e-09 5.93267850e-06 6.34767957e-06 3.94606703e-09
 1.15960635e-01 2.26308592e-03 8.81424785e-01 5.88283956e-06
 2.46876327e-04 8.57697160e-05 7.01236672e-07], sum to 1.0000
[2019-04-09 15:20:36,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4097
[2019-04-09 15:20:36,542] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 69.0, 0.0, 0.0, 19.0, 21.84684265211547, -0.4183518546789164, 0.0, 1.0, 35.0, 24.811366088007844], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3026400.0000, 
sim time next is 3027600.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 19.0, 21.74549862382015, -0.4347035946459891, 0.0, 1.0, 45.0, 31.574818599631875], 
processed observation next is [0.0, 0.043478260869565216, 0.32409972299168976, 0.71, 0.0, 0.0, 0.08333333333333333, 0.31212488531834587, 0.3550988017846703, 0.0, 1.0, 0.6, 0.31574818599631876], 
reward next is 0.6843, 
noisyNet noise sample is [array([-0.26974413], dtype=float32), -1.50235]. 
=============================================
[2019-04-09 15:20:37,085] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.68478781e-09 1.71413069e-06 1.84699547e-05 2.82423884e-09
 5.18438637e-01 1.09796645e-03 4.80265886e-01 1.21919775e-05
 1.51778149e-04 1.23701393e-05 9.18454418e-07], sum to 1.0000
[2019-04-09 15:20:37,085] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7535
[2019-04-09 15:20:37,108] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 50.0, 111.5, 811.5, 19.0, 21.62738071133101, -0.4069975005638857, 0.0, 1.0, 35.0, 22.01747950354548], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3070800.0000, 
sim time next is 3072000.0000, 
raw observation next is [-1.666666666666667, 47.33333333333334, 109.8333333333333, 807.8333333333334, 19.0, 21.71255400676451, -0.3841201970367793, 0.0, 1.0, 45.0, 28.295538582639608], 
processed observation next is [0.0, 0.5652173913043478, 0.4164358264081256, 0.47333333333333344, 0.366111111111111, 0.892633517495396, 0.08333333333333333, 0.30937950056370916, 0.37195993432107355, 0.0, 1.0, 0.6, 0.28295538582639607], 
reward next is 0.7170, 
noisyNet noise sample is [array([2.8347037], dtype=float32), 0.48768604]. 
=============================================
[2019-04-09 15:20:37,177] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[41.059742]
 [41.20486 ]
 [40.480194]
 [40.52196 ]
 [40.119854]], R is [[41.64622116]
 [42.00958252]
 [42.30728531]
 [42.6621933 ]
 [42.9426384 ]].
[2019-04-09 15:20:37,585] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3649062e-13 8.1547530e-10 1.4598437e-07 1.3959207e-13 4.0331334e-01
 7.2261559e-05 5.9660810e-01 2.6510865e-08 4.6174346e-06 1.5086332e-06
 3.0867189e-09], sum to 1.0000
[2019-04-09 15:20:37,589] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6558
[2019-04-09 15:20:37,591] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.9282085e-10 2.6196804e-07 1.9434292e-05 4.0789896e-10 1.9048297e-01
 1.4762232e-03 8.0794799e-01 4.1814433e-06 3.2751002e-05 3.4606735e-05
 1.5982758e-06], sum to 1.0000
[2019-04-09 15:20:37,592] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7197
[2019-04-09 15:20:37,643] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 82.0, 0.0, 0.0, 19.0, 22.63367246319028, -0.2557789479179457, 0.0, 1.0, 45.0, 31.771860894446526], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3088800.0000, 
sim time next is 3090000.0000, 
raw observation next is [-0.7333333333333334, 85.33333333333334, 0.0, 0.0, 19.0, 22.5870184493144, -0.2659944198433448, 0.0, 1.0, 45.0, 32.27015675712884], 
processed observation next is [0.0, 0.782608695652174, 0.44228993536472766, 0.8533333333333334, 0.0, 0.0, 0.08333333333333333, 0.3822515374428666, 0.41133519338555175, 0.0, 1.0, 0.6, 0.3227015675712884], 
reward next is 0.6773, 
noisyNet noise sample is [array([0.62973166], dtype=float32), -2.2545714]. 
=============================================
[2019-04-09 15:20:37,648] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.8605567e-10 4.6273490e-07 4.3030918e-06 8.2971514e-09 9.5873736e-02
 9.6404541e-04 9.0283865e-01 2.3525090e-06 2.4741798e-04 6.8130787e-05
 8.5575027e-07], sum to 1.0000
[2019-04-09 15:20:37,649] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[41.39123 ]
 [41.34013 ]
 [41.586796]
 [41.19646 ]
 [41.138836]], R is [[41.96674347]
 [42.22935867]
 [42.49066925]
 [42.76263809]
 [43.12514114]].
[2019-04-09 15:20:37,649] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6582
[2019-04-09 15:20:37,670] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 100.0, 0.0, 0.0, 19.0, 25.9305976568001, 0.6921980370698196, 0.0, 1.0, 35.0, 18.932074738966683], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3188400.0000, 
sim time next is 3189600.0000, 
raw observation next is [2.0, 100.0, 0.0, 0.0, 19.0, 25.88120018497015, 0.6938711452862445, 0.0, 1.0, 45.0, 27.42806562665193], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6567666820808459, 0.7312903817620815, 0.0, 1.0, 0.6, 0.27428065626651926], 
reward next is 0.7257, 
noisyNet noise sample is [array([-1.4065822], dtype=float32), 1.1446623]. 
=============================================
[2019-04-09 15:20:37,683] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 21.60895038130585, -0.3650827826722127, 0.0, 1.0, 45.0, 48.16011066493419], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 2960400.0000, 
sim time next is 2961600.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 21.59017784427913, -0.3769237597198221, 0.0, 1.0, 45.0, 50.96485070268986], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.29918148702326075, 0.3743587467600593, 0.0, 1.0, 0.6, 0.5096485070268986], 
reward next is 0.4904, 
noisyNet noise sample is [array([-0.80022675], dtype=float32), 0.09937048]. 
=============================================
[2019-04-09 15:20:37,763] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.2167209e-09 1.7448186e-06 3.7581791e-05 8.2172363e-10 2.7667046e-01
 4.5780679e-03 7.1802598e-01 1.0416767e-05 6.4319675e-04 2.8453762e-05
 4.0098521e-06], sum to 1.0000
[2019-04-09 15:20:37,764] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4058
[2019-04-09 15:20:37,792] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4, 78.66666666666667, 0.0, 0.0, 19.0, 22.77396510371414, -0.2426203653499885, 0.0, 1.0, 45.0, 30.67770608878907], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3087600.0000, 
sim time next is 3088800.0000, 
raw observation next is [-0.6, 82.0, 0.0, 0.0, 19.0, 22.68918215984377, -0.2591700634371035, 0.0, 1.0, 45.0, 31.440957280323644], 
processed observation next is [0.0, 0.782608695652174, 0.44598337950138506, 0.82, 0.0, 0.0, 0.08333333333333333, 0.3907651799869809, 0.41360997885429884, 0.0, 1.0, 0.6, 0.31440957280323645], 
reward next is 0.6856, 
noisyNet noise sample is [array([1.3083768], dtype=float32), 0.48017552]. 
=============================================
[2019-04-09 15:20:37,867] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2778767e-10 9.3938290e-08 5.5939850e-06 6.0126859e-10 2.6512703e-01
 2.9973959e-04 7.3447210e-01 5.2483489e-07 9.1034264e-05 3.6961819e-06
 2.0376865e-07], sum to 1.0000
[2019-04-09 15:20:37,867] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7742
[2019-04-09 15:20:37,903] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 19.0, 22.22929290189471, -0.363154423807404, 0.0, 1.0, 45.0, 33.34745583914077], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3108000.0000, 
sim time next is 3109200.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 19.0, 22.19940070386271, -0.3693420174500446, 0.0, 1.0, 45.0, 33.227641495783104], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.08333333333333333, 0.34995005865522594, 0.3768859941833185, 0.0, 1.0, 0.6, 0.33227641495783106], 
reward next is 0.6677, 
noisyNet noise sample is [array([2.2691147], dtype=float32), -1.7091721]. 
=============================================
[2019-04-09 15:20:38,187] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5732452e-10 2.2253182e-07 1.9762881e-06 2.7242930e-10 2.7240521e-01
 3.9207909e-04 7.2715104e-01 1.1124120e-06 4.2270985e-05 5.9313556e-06
 1.2230143e-07], sum to 1.0000
[2019-04-09 15:20:38,187] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8606
[2019-04-09 15:20:38,225] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 19.0, 22.50724101832069, -0.2927497076513093, 0.0, 1.0, 45.0, 33.272696447613114], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3096000.0000, 
sim time next is 3097200.0000, 
raw observation next is [-1.0, 94.66666666666667, 0.0, 0.0, 19.0, 22.45975200406374, -0.3004056165830347, 0.0, 1.0, 45.0, 33.37005989672977], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9466666666666668, 0.0, 0.0, 0.08333333333333333, 0.3716460003386451, 0.39986479447232176, 0.0, 1.0, 0.6, 0.3337005989672977], 
reward next is 0.6663, 
noisyNet noise sample is [array([0.6437002], dtype=float32), -1.401991]. 
=============================================
[2019-04-09 15:20:38,495] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.68537936e-09 4.65222939e-07 8.61192621e-06 2.29003949e-09
 1.05218746e-01 6.24702836e-04 8.93445015e-01 2.65495146e-06
 6.60117948e-04 3.47700661e-05 4.81224788e-06], sum to 1.0000
[2019-04-09 15:20:38,497] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2539
[2019-04-09 15:20:38,526] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 91.0, 497.0, 19.0, 21.04300755016519, -0.5750174021012182, 0.0, 1.0, 45.0, 32.44145818524479], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3056400.0000, 
sim time next is 3057600.0000, 
raw observation next is [-5.333333333333333, 57.33333333333334, 96.33333333333333, 589.0000000000001, 19.0, 21.06215733579731, -0.5545930861005374, 0.0, 1.0, 45.0, 31.82809066349772], 
processed observation next is [0.0, 0.391304347826087, 0.3148661126500462, 0.5733333333333335, 0.32111111111111107, 0.650828729281768, 0.08333333333333333, 0.2551797779831091, 0.3151356379664875, 0.0, 1.0, 0.6, 0.3182809066349772], 
reward next is 0.6817, 
noisyNet noise sample is [array([1.1040865], dtype=float32), -0.54441845]. 
=============================================
[2019-04-09 15:20:39,365] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1365726e-09 4.8382492e-07 2.2579805e-05 9.9303010e-09 5.7357639e-01
 1.6405521e-03 4.2441905e-01 5.4996281e-06 3.1662031e-04 1.5813912e-05
 3.0363719e-06], sum to 1.0000
[2019-04-09 15:20:39,366] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9845
[2019-04-09 15:20:39,403] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 68.33333333333333, 14.66666666666666, 118.1666666666667, 19.0, 20.25422781295999, -0.8017634697711234, 0.0, 1.0, 35.0, 24.45313883001746], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3051600.0000, 
sim time next is 3052800.0000, 
raw observation next is [-6.0, 64.0, 42.0, 214.5, 19.0, 20.14451485404927, -0.7796061265958952, 0.0, 1.0, 45.0, 35.08328696326953], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.14, 0.23701657458563535, 0.08333333333333333, 0.17870957117077246, 0.24013129113470158, 0.0, 1.0, 0.6, 0.3508328696326953], 
reward next is 0.6492, 
noisyNet noise sample is [array([0.20297241], dtype=float32), 0.7731649]. 
=============================================
[2019-04-09 15:20:39,888] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1922133e-13 2.4582295e-11 4.3691188e-09 9.7401360e-15 1.2789682e-02
 8.2403116e-05 9.8712742e-01 2.4680509e-09 4.1586983e-07 1.9142075e-08
 1.5500733e-10], sum to 1.0000
[2019-04-09 15:20:39,890] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1458
[2019-04-09 15:20:39,930] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 106.5, 729.5, 22.5, 24.91919986011023, 0.2459017908560892, 1.0, 1.0, 45.0, 28.33622924488192], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3147600.0000, 
sim time next is 3148800.0000, 
raw observation next is [7.0, 100.0, 109.0, 755.8333333333334, 22.5, 25.17448424327088, 0.307298732072347, 1.0, 1.0, 45.0, 27.406870030534982], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.36333333333333334, 0.8351749539594844, 0.375, 0.5978736869392399, 0.6024329106907823, 1.0, 1.0, 0.6, 0.2740687003053498], 
reward next is 0.7259, 
noisyNet noise sample is [array([0.4383592], dtype=float32), -0.5898619]. 
=============================================
[2019-04-09 15:20:40,172] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.3905089e-09 1.1743194e-06 1.1698422e-05 1.5014747e-08 1.5367539e-01
 3.0885118e-03 8.4302264e-01 5.6756144e-06 1.2172692e-04 7.0751572e-05
 2.5026238e-06], sum to 1.0000
[2019-04-09 15:20:40,181] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3724
[2019-04-09 15:20:40,219] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 21.30782544163679, -0.5388512496475759, 0.0, 1.0, 45.0, 32.81802199123784], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3034800.0000, 
sim time next is 3036000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 21.25684158902278, -0.5458961161869419, 0.0, 1.0, 45.0, 33.793083428343934], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.2714034657518984, 0.31803462793768605, 0.0, 1.0, 0.6, 0.33793083428343934], 
reward next is 0.6621, 
noisyNet noise sample is [array([0.7714938], dtype=float32), -1.266547]. 
=============================================
[2019-04-09 15:20:40,234] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[36.052246]
 [36.147007]
 [37.07599 ]
 [37.578835]
 [38.08217 ]], R is [[36.39737701]
 [36.7052269 ]
 [37.08166885]
 [37.38556671]
 [37.68873596]].
[2019-04-09 15:20:40,674] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.8390335e-09 2.3560217e-06 4.0074854e-05 6.1732059e-09 4.0540579e-01
 8.1409514e-03 5.8604598e-01 1.0639478e-05 2.7399135e-04 7.2899820e-05
 7.3395349e-06], sum to 1.0000
[2019-04-09 15:20:40,677] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4818
[2019-04-09 15:20:40,728] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 21.3325839228737, -0.5487160281017008, 0.0, 1.0, 45.0, 33.02744575991174], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3039600.0000, 
sim time next is 3040800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 21.22303194403778, -0.5641606907094786, 0.0, 1.0, 45.0, 33.85924295780605], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.26858599533648153, 0.3119464364301738, 0.0, 1.0, 0.6, 0.3385924295780605], 
reward next is 0.6614, 
noisyNet noise sample is [array([1.4183065], dtype=float32), -0.28519866]. 
=============================================
[2019-04-09 15:20:40,816] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-09 15:20:40,828] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:20:40,828] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:20:40,829] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:20:40,829] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:20:40,829] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:20:40,830] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:20:40,832] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run12
[2019-04-09 15:20:40,850] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run12
[2019-04-09 15:20:40,860] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run12
[2019-04-09 15:21:57,977] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:NoisyNet noise sample: [array([0.05812688], dtype=float32), 0.083544664]
[2019-04-09 15:21:57,978] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation this: [10.83333333333333, 57.0, 0.0, 0.0, 19.0, 26.59604370453828, 0.6804218616240503, 0.0, 1.0, 45.0, 23.847094841974425]
[2019-04-09 15:21:57,978] A3C_EVAL-Part4-Light-Pit-Test-v5 DEBUG:Observation forecast: []
[2019-04-09 15:21:57,978] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Softmax [9.1901356e-12 7.8029121e-09 7.7461334e-07 9.4705840e-12 1.6336365e-01
 2.7293450e-04 8.3635324e-01 1.5746788e-07 8.1087992e-06 1.1595235e-06
 3.5258957e-08], sampled 0.05604581912474749
[2019-04-09 15:22:15,101] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2939.6536 126395.0630 904.8403
[2019-04-09 15:22:15,125] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:15,125] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:15,125] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:15,125] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:15,125] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:15,125] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:15,125] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:15,125] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:15,125] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:15,125] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:15,125] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:15,125] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:15,278] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:15,278] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:15,278] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:15,278] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:15,278] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:15,278] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:15,278] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:15,278] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:15,278] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:15,278] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:15,278] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:15,278] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:32,506] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2902.3039 133682.3094 612.3442
[2019-04-09 15:22:32,527] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:32,527] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:32,527] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:32,527] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:32,527] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:32,527] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:32,527] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:32,527] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:32,527] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:32,527] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:32,527] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:32,527] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:32,644] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:32,644] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:32,644] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:32,644] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:32,644] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:32,644] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:32,644] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:32,644] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:32,644] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:32,644] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:32,644] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:32,644] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:35,612] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2878.8522 135624.5683 371.8091
[2019-04-09 15:22:35,633] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:35,633] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:35,633] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:35,633] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:35,633] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:35,633] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:35,633] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:35,633] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:35,633] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:35,633] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:35,633] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:35,633] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:22:35,741] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:35,741] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:35,741] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:35,741] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:35,741] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:35,741] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:35,741] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:35,741] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:35,741] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:35,741] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:35,741] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:35,741] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:22:36,636] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 110000, evaluation results [110000.0, 2902.303855851498, 133682.30940486622, 612.3442077464526, 2939.653603033105, 126395.06300706163, 904.8403355214236, 2878.852239983126, 135624.5682831214, 371.80905400820893]
[2019-04-09 15:22:36,892] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9559411e-08 6.6395279e-07 5.0445877e-05 8.8247343e-10 4.8001491e-02
 9.4123109e-04 9.5093709e-01 9.6938129e-06 4.6519035e-05 1.2173733e-05
 7.5058261e-07], sum to 1.0000
[2019-04-09 15:22:36,892] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2712
[2019-04-09 15:22:36,934] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 72.66666666666667, 0.0, 0.0, 19.0, 20.78204920703749, -0.6671045008909332, 0.0, 1.0, 45.0, 34.569819725321494], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3050400.0000, 
sim time next is 3051600.0000, 
raw observation next is [-6.0, 68.33333333333333, 14.66666666666666, 118.1666666666667, 19.0, 20.73093382309361, -0.6667966494553891, 0.0, 1.0, 45.0, 34.62888537154862], 
processed observation next is [0.0, 0.30434782608695654, 0.296398891966759, 0.6833333333333332, 0.04888888888888887, 0.13057090239410685, 0.08333333333333333, 0.22757781859113427, 0.277734450181537, 0.0, 1.0, 0.6, 0.3462888537154862], 
reward next is 0.6537, 
noisyNet noise sample is [array([0.5854024], dtype=float32), -1.87603]. 
=============================================
[2019-04-09 15:22:36,951] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5724115e-15 4.3083466e-12 8.7889973e-11 1.8469310e-16 1.2643420e-02
 8.8081342e-06 9.8734772e-01 8.2978145e-11 1.7554234e-08 7.9891374e-09
 2.0092063e-10], sum to 1.0000
[2019-04-09 15:22:36,952] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4774
[2019-04-09 15:22:36,986] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333333, 100.0, 0.0, 0.0, 22.5, 25.31788763890533, 0.7428599966834922, 1.0, 1.0, 45.0, 25.158682353010164], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3176400.0000, 
sim time next is 3177600.0000, 
raw observation next is [4.666666666666666, 100.0, 0.0, 0.0, 22.5, 26.24383194383163, 0.8060544243637473, 0.0, 1.0, 45.0, 25.07350444298255], 
processed observation next is [1.0, 0.782608695652174, 0.5918744228993538, 1.0, 0.0, 0.0, 0.375, 0.6869859953193025, 0.7686848081212491, 0.0, 1.0, 0.6, 0.2507350444298255], 
reward next is 0.7493, 
noisyNet noise sample is [array([0.14491469], dtype=float32), -0.44706562]. 
=============================================
[2019-04-09 15:22:37,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9873252e-17 1.5581487e-12 2.4031512e-09 1.2973035e-16 9.3744740e-02
 5.1398871e-05 9.0620339e-01 1.2486177e-10 4.5750295e-07 5.7273555e-09
 3.5657217e-11], sum to 1.0000
[2019-04-09 15:22:37,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6146
[2019-04-09 15:22:37,508] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.3768677e-13 6.9355971e-10 5.3130282e-08 3.2722970e-14 3.6067698e-02
 2.0078974e-04 9.6373069e-01 4.1675801e-08 7.6952114e-07 8.6220169e-09
 1.9381766e-10], sum to 1.0000
[2019-04-09 15:22:37,511] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4797
[2019-04-09 15:22:37,523] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 112.5, 814.5, 22.5, 25.61473215994131, 0.5715142977409023, 1.0, 1.0, 45.0, 24.852396841720072], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3157200.0000, 
sim time next is 3158400.0000, 
raw observation next is [7.0, 100.0, 112.1666666666667, 808.8333333333334, 22.5, 25.72292912135688, 0.6049833510183188, 1.0, 1.0, 45.0, 24.17675988754424], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.373888888888889, 0.8937384898710866, 0.375, 0.6435774267797401, 0.7016611170061062, 1.0, 1.0, 0.6, 0.24176759887544239], 
reward next is 0.7582, 
noisyNet noise sample is [array([-0.44552773], dtype=float32), -0.51499134]. 
=============================================
[2019-04-09 15:22:37,530] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 97.66666666666667, 0.0, 0.0, 19.0, 25.79933740387265, 0.6049727738147784, 0.0, 1.0, 45.0, 28.00802601776525], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3190800.0000, 
sim time next is 3192000.0000, 
raw observation next is [2.0, 95.33333333333334, 0.0, 0.0, 19.0, 25.51411021281498, 0.5674988490497341, 0.0, 1.0, 45.0, 27.919742335063383], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.6261758510679151, 0.689166283016578, 0.0, 1.0, 0.6, 0.2791974233506338], 
reward next is 0.7208, 
noisyNet noise sample is [array([1.3698268], dtype=float32), 0.37838754]. 
=============================================
[2019-04-09 15:22:37,537] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[63.30436 ]
 [64.24031 ]
 [63.858204]
 [64.91458 ]
 [66.51195 ]], R is [[62.54589081]
 [62.64035416]
 [62.74121857]
 [62.84317398]
 [62.94621277]].
[2019-04-09 15:22:38,499] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.6418702e-12 1.2542944e-09 4.8656243e-08 7.5038248e-12 5.3315863e-02
 4.7339904e-04 9.4620693e-01 1.0103455e-07 3.4033844e-06 3.3595069e-07
 3.0686756e-08], sum to 1.0000
[2019-04-09 15:22:38,499] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9012
[2019-04-09 15:22:38,527] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 94.66666666666666, 0.0, 0.0, 19.0, 23.7986745370925, 0.1562612745475539, 0.0, 1.0, 45.0, 33.04888507001587], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3220800.0000, 
sim time next is 3222000.0000, 
raw observation next is [-3.0, 92.0, 0.0, 0.0, 19.0, 23.67318626503164, 0.1340590700629138, 0.0, 1.0, 45.0, 33.25511381076542], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.0, 0.0, 0.08333333333333333, 0.4727655220859699, 0.544686356687638, 0.0, 1.0, 0.6, 0.33255113810765424], 
reward next is 0.6674, 
noisyNet noise sample is [array([-1.2466127], dtype=float32), 1.42518]. 
=============================================
[2019-04-09 15:22:38,532] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[53.641132]
 [53.73939 ]
 [53.60093 ]
 [53.674572]
 [53.751675]], R is [[53.62413788]
 [53.75740814]
 [53.89203262]
 [54.02771759]
 [54.1642189 ]].
[2019-04-09 15:22:38,839] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.6642738e-15 3.8019497e-11 2.3661739e-08 3.5380200e-14 1.2237498e-01
 4.0615345e-05 8.7758362e-01 2.8805464e-10 7.0788224e-07 2.5192499e-08
 4.5988086e-10], sum to 1.0000
[2019-04-09 15:22:38,840] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6680
[2019-04-09 15:22:38,873] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 22.5, 25.91294690644369, 0.6061626805358677, 1.0, 1.0, 45.0, 30.999477153995315], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3264000.0000, 
sim time next is 3265200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 22.5, 25.78658896015319, 0.5874688187704324, 1.0, 1.0, 45.0, 31.118188691518935], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.65, 0.0, 0.0, 0.375, 0.6488824133460991, 0.6958229395901441, 1.0, 1.0, 0.6, 0.31118188691518933], 
reward next is 0.6888, 
noisyNet noise sample is [array([-0.6339325], dtype=float32), -1.2629231]. 
=============================================
[2019-04-09 15:22:39,166] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.2000960e-12 4.3554036e-09 1.7577848e-07 1.8704205e-11 6.0628408e-01
 7.4290595e-04 3.9295921e-01 2.0352543e-07 1.2588419e-05 7.6454518e-07
 4.4756554e-08], sum to 1.0000
[2019-04-09 15:22:39,166] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3091
[2019-04-09 15:22:39,186] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 19.0, 24.56861907711, 0.3626602369380157, 0.0, 1.0, 35.0, 17.213294856713297], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3200400.0000, 
sim time next is 3201600.0000, 
raw observation next is [0.6666666666666667, 100.0, 0.0, 0.0, 19.0, 24.50288787114119, 0.3405839299194262, 0.0, 1.0, 35.0, 17.9012793309561], 
processed observation next is [1.0, 0.043478260869565216, 0.4810710987996307, 1.0, 0.0, 0.0, 0.08333333333333333, 0.5419073225950992, 0.6135279766398087, 0.0, 1.0, 0.4, 0.179012793309561], 
reward next is 0.8210, 
noisyNet noise sample is [array([0.6359938], dtype=float32), 0.7559996]. 
=============================================
[2019-04-09 15:22:40,221] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.2700717e-12 1.8414659e-09 7.2799929e-07 1.7341691e-12 1.5240870e-01
 8.0809521e-04 8.4677887e-01 9.7622376e-08 3.2055780e-06 3.3664065e-07
 2.4730634e-08], sum to 1.0000
[2019-04-09 15:22:40,221] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5724
[2019-04-09 15:22:40,270] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 24.78093949265639, 0.3941425154533702, 0.0, 1.0, 45.0, 31.398671595374203], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3208800.0000, 
sim time next is 3210000.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 24.89475535654702, 0.3849270777672195, 0.0, 1.0, 45.0, 31.121271591768817], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.5745629463789182, 0.6283090259224066, 0.0, 1.0, 0.6, 0.3112127159176882], 
reward next is 0.6888, 
noisyNet noise sample is [array([0.761343], dtype=float32), 0.70896703]. 
=============================================
[2019-04-09 15:22:40,277] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[54.062386]
 [54.03968 ]
 [54.37444 ]
 [55.09608 ]
 [55.85741 ]], R is [[54.13266754]
 [54.27735519]
 [54.42086792]
 [54.63710403]
 [54.78620529]].
[2019-04-09 15:22:40,666] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.92519331e-11 7.33610683e-09 1.46463583e-06 4.57588203e-11
 2.28902876e-01 3.38294369e-04 7.70743906e-01 1.03442574e-06
 8.86078578e-06 3.47732066e-06 1.05989955e-07], sum to 1.0000
[2019-04-09 15:22:40,666] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4964
[2019-04-09 15:22:40,802] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 78.66666666666667, 0.0, 0.0, 22.5, 21.82949424689194, -0.3094937627547952, 1.0, 1.0, 45.0, 48.437419066041116], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3309600.0000, 
sim time next is 3310800.0000, 
raw observation next is [-11.0, 81.33333333333334, 16.0, 144.3333333333333, 22.5, 22.3064347138481, -0.2402534703537518, 1.0, 1.0, 45.0, 47.126849157886326], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.8133333333333335, 0.05333333333333334, 0.15948434622467766, 0.375, 0.3588695594873415, 0.41991550988208276, 1.0, 1.0, 0.6, 0.47126849157886325], 
reward next is 0.5287, 
noisyNet noise sample is [array([0.17460483], dtype=float32), 0.8362744]. 
=============================================
[2019-04-09 15:22:40,988] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6736882e-15 1.9296936e-11 4.2633648e-08 3.2426615e-15 7.0107150e-01
 1.1594826e-04 2.9880762e-01 1.7447983e-09 4.9262480e-06 3.3231441e-08
 8.0561093e-11], sum to 1.0000
[2019-04-09 15:22:40,988] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9241
[2019-04-09 15:22:41,030] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 16.33333333333333, 179.8333333333333, 22.5, 27.0818725558809, 0.8529046015438707, 1.0, 1.0, 35.0, 18.851672855018865], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3172800.0000, 
sim time next is 3174000.0000, 
raw observation next is [6.0, 100.0, 0.0, 0.0, 22.5, 26.90045123946273, 0.9104584204328806, 1.0, 1.0, 45.0, 23.37606204491697], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.0, 0.0, 0.375, 0.7417042699552274, 0.8034861401442935, 1.0, 1.0, 0.6, 0.2337606204491697], 
reward next is 0.7662, 
noisyNet noise sample is [array([0.24284583], dtype=float32), -2.0742671]. 
=============================================
[2019-04-09 15:22:41,038] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[72.22084 ]
 [72.76727 ]
 [72.89011 ]
 [73.24602 ]
 [73.188644]], R is [[71.87362671]
 [71.96637726]
 [71.98854828]
 [72.11320496]
 [72.22596741]].
[2019-04-09 15:22:41,050] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4441300e-14 2.6584591e-13 6.7989764e-10 3.3443796e-16 2.2283502e-02
 1.7474187e-05 9.7769862e-01 4.3996573e-10 4.2538090e-07 2.3519604e-09
 6.6535201e-11], sum to 1.0000
[2019-04-09 15:22:41,050] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9628
[2019-04-09 15:22:41,092] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 0.0, 0.0, 22.5, 26.90045123946273, 0.9104584204328806, 1.0, 1.0, 45.0, 23.37606204491697], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3174000.0000, 
sim time next is 3175200.0000, 
raw observation next is [6.0, 100.0, 0.0, 0.0, 22.5, 27.05471989470603, 0.9084798039189979, 1.0, 1.0, 45.0, 25.203798279090783], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 1.0, 0.0, 0.0, 0.375, 0.7545599912255024, 0.8028266013063327, 1.0, 1.0, 0.6, 0.25203798279090783], 
reward next is 0.7480, 
noisyNet noise sample is [array([-0.6496714], dtype=float32), -0.3170734]. 
=============================================
[2019-04-09 15:22:41,616] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.9926177e-10 5.0829975e-08 1.9752906e-06 3.8483487e-11 1.1998086e-01
 4.3080709e-04 8.7949342e-01 1.8365646e-06 8.8181470e-05 2.6746870e-06
 2.6682321e-07], sum to 1.0000
[2019-04-09 15:22:41,624] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1016
[2019-04-09 15:22:41,698] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.633333333333333, 76.33333333333333, 0.0, 0.0, 19.0, 22.73851625869005, -0.138952253463694, 0.0, 1.0, 45.0, 36.499727070148225], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3300000.0000, 
sim time next is 3301200.0000, 
raw observation next is [-10.0, 76.0, 0.0, 0.0, 19.0, 22.6490647890648, -0.1611526085594642, 0.0, 1.0, 35.0, 28.469683285272147], 
processed observation next is [1.0, 0.21739130434782608, 0.18559556786703602, 0.76, 0.0, 0.0, 0.08333333333333333, 0.3874220657553999, 0.44628246381351194, 0.0, 1.0, 0.4, 0.28469683285272146], 
reward next is 0.7153, 
noisyNet noise sample is [array([1.5110298], dtype=float32), -0.13354366]. 
=============================================
[2019-04-09 15:22:42,046] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0943079e-14 1.9010754e-10 8.5054175e-08 2.1687659e-14 4.5077157e-01
 9.0276859e-05 5.4913461e-01 1.4612859e-09 6.1117476e-07 2.9417856e-06
 1.2784762e-09], sum to 1.0000
[2019-04-09 15:22:42,046] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5819
[2019-04-09 15:22:42,048] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9286214e-15 4.4535525e-12 5.7714822e-10 7.9862904e-16 1.6387381e-03
 1.6484057e-06 9.9835962e-01 7.1541628e-10 2.5367257e-08 1.0214516e-09
 1.0245119e-11], sum to 1.0000
[2019-04-09 15:22:42,049] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5649
[2019-04-09 15:22:42,082] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 77.66666666666667, 0.0, 0.0, 19.0, 25.19447231437277, 0.4781452953393255, 0.0, 1.0, 45.0, 31.704776567769983], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3271200.0000, 
sim time next is 3272400.0000, 
raw observation next is [-5.0, 81.0, 0.0, 0.0, 19.0, 25.12126846255631, 0.4644418050059049, 0.0, 1.0, 45.0, 31.866669636019655], 
processed observation next is [1.0, 0.9130434782608695, 0.32409972299168976, 0.81, 0.0, 0.0, 0.08333333333333333, 0.593439038546359, 0.6548139350019683, 0.0, 1.0, 0.6, 0.31866669636019657], 
reward next is 0.6813, 
noisyNet noise sample is [array([-0.2179804], dtype=float32), 0.15119575]. 
=============================================
[2019-04-09 15:22:42,183] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 113.5, 769.0, 22.5, 25.50485940628086, 0.2560120492194478, 1.0, 1.0, 45.0, 38.33885910368207], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3322800.0000, 
sim time next is 3324000.0000, 
raw observation next is [-6.666666666666667, 60.66666666666667, 115.1666666666667, 788.3333333333333, 22.5, 25.45804279185291, 0.422755989597853, 1.0, 1.0, 45.0, 36.97608525556673], 
processed observation next is [1.0, 0.4782608695652174, 0.27793167128347185, 0.6066666666666667, 0.383888888888889, 0.871086556169429, 0.375, 0.6215035659877426, 0.6409186631992844, 1.0, 1.0, 0.6, 0.3697608525556673], 
reward next is 0.6302, 
noisyNet noise sample is [array([0.19611065], dtype=float32), 0.34143656]. 
=============================================
[2019-04-09 15:22:42,195] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[64.27126 ]
 [63.46513 ]
 [62.043293]
 [60.64683 ]
 [59.229004]], R is [[64.96497345]
 [64.93193054]
 [64.89794922]
 [64.94918823]
 [64.88790131]].
[2019-04-09 15:22:42,204] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4584808e-16 1.3856721e-12 1.0051627e-09 1.9932877e-15 1.7214082e-02
 1.4997579e-07 9.8278576e-01 2.0231500e-10 1.2745290e-08 3.1542638e-10
 2.0767012e-10], sum to 1.0000
[2019-04-09 15:22:42,205] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5820
[2019-04-09 15:22:42,278] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 34.0, 307.5, 22.5, 26.84043160434092, 0.776495419475284, 1.0, 1.0, 45.0, 29.685516427193697], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3258000.0000, 
sim time next is 3259200.0000, 
raw observation next is [-4.0, 73.0, 17.33333333333333, 171.8333333333333, 22.5, 26.6437866435675, 0.3976531147618425, 1.0, 1.0, 45.0, 29.749374809239235], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.73, 0.05777777777777776, 0.18987108655616938, 0.375, 0.7203155536306248, 0.6325510382539475, 1.0, 1.0, 0.6, 0.2974937480923924], 
reward next is 0.7025, 
noisyNet noise sample is [array([0.6702954], dtype=float32), 0.22377646]. 
=============================================
[2019-04-09 15:22:43,374] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0239643e-12 2.9104743e-09 2.9101497e-07 1.3651535e-12 3.4666845e-01
 9.9992775e-04 6.5231752e-01 9.3599075e-09 8.6302589e-06 5.1954507e-06
 2.5936718e-08], sum to 1.0000
[2019-04-09 15:22:43,378] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4074
[2019-04-09 15:22:43,440] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 84.0, 0.0, 0.0, 19.0, 23.85255183509112, 0.1582723738653196, 0.0, 1.0, 45.0, 34.446631012037656], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3283200.0000, 
sim time next is 3284400.0000, 
raw observation next is [-7.0, 79.33333333333334, 0.0, 0.0, 19.0, 23.75074564721786, 0.1312554025215339, 0.0, 1.0, 45.0, 34.819925607280695], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.7933333333333334, 0.0, 0.0, 0.08333333333333333, 0.4792288039348218, 0.5437518008405112, 0.0, 1.0, 0.6, 0.34819925607280694], 
reward next is 0.6518, 
noisyNet noise sample is [array([0.25373092], dtype=float32), -1.2320329]. 
=============================================
[2019-04-09 15:22:43,539] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.38819248e-12 8.84836826e-09 6.32611716e-07 6.98824592e-12
 3.13444048e-01 1.26876018e-03 6.85259640e-01 2.33402844e-08
 1.67049639e-05 1.01113965e-05 5.45066925e-08], sum to 1.0000
[2019-04-09 15:22:43,539] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1220
[2019-04-09 15:22:43,560] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.4499370e-15 8.9112146e-12 4.5257451e-08 3.7852128e-15 1.2230999e-02
 8.8680645e-06 9.8775953e-01 2.4020352e-10 7.3761720e-07 2.5780218e-09
 9.6535543e-11], sum to 1.0000
[2019-04-09 15:22:43,561] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5921
[2019-04-09 15:22:43,606] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 70.0, 0.0, 0.0, 19.0, 23.58183204084685, 0.087358829601205, 0.0, 1.0, 45.0, 35.021572242352164], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3286800.0000, 
sim time next is 3288000.0000, 
raw observation next is [-7.333333333333334, 72.33333333333334, 0.0, 0.0, 19.0, 23.53016994233605, 0.06214387516931872, 0.0, 1.0, 35.0, 27.555187406139325], 
processed observation next is [1.0, 0.043478260869565216, 0.2594644506001847, 0.7233333333333334, 0.0, 0.0, 0.08333333333333333, 0.460847495194671, 0.5207146250564395, 0.0, 1.0, 0.4, 0.27555187406139325], 
reward next is 0.7244, 
noisyNet noise sample is [array([0.25373092], dtype=float32), -1.2320329]. 
=============================================
[2019-04-09 15:22:43,622] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[51.81751 ]
 [52.47616 ]
 [54.04275 ]
 [55.263424]
 [56.155296]], R is [[50.97997284]
 [51.11995697]
 [51.33420563]
 [51.47266769]
 [51.6134758 ]].
[2019-04-09 15:22:43,628] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.6, 84.0, 109.6666666666667, 761.8333333333334, 22.5, 25.94991171937088, 0.6208597202695866, 1.0, 1.0, 45.0, 30.00715680317595], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3235200.0000, 
sim time next is 3236400.0000, 
raw observation next is [-2.4, 80.0, 111.0, 781.5, 22.5, 26.03422708725036, 0.6375350523897513, 1.0, 1.0, 45.0, 30.132549965997796], 
processed observation next is [1.0, 0.4782608695652174, 0.39612188365650974, 0.8, 0.37, 0.86353591160221, 0.375, 0.6695189239375301, 0.7125116841299172, 1.0, 1.0, 0.6, 0.30132549965997796], 
reward next is 0.6987, 
noisyNet noise sample is [array([-0.2506854], dtype=float32), 0.5455592]. 
=============================================
[2019-04-09 15:22:43,843] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0790548e-11 9.7945332e-08 3.1598106e-06 5.5830587e-11 3.7401643e-01
 1.0434793e-03 6.2491006e-01 8.6891561e-07 2.2358354e-05 2.9273567e-06
 6.9235176e-07], sum to 1.0000
[2019-04-09 15:22:43,855] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1756
[2019-04-09 15:22:43,898] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.0, 76.0, 0.0, 0.0, 19.0, 22.65523431777105, -0.1541991342652039, 0.0, 1.0, 45.0, 36.11947128206542], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3301200.0000, 
sim time next is 3302400.0000, 
raw observation next is [-10.33333333333333, 76.0, 0.0, 0.0, 19.0, 22.61791184043891, -0.1861964222131034, 0.0, 1.0, 35.0, 28.21125962183494], 
processed observation next is [1.0, 0.21739130434782608, 0.17636195752539252, 0.76, 0.0, 0.0, 0.08333333333333333, 0.3848259867032426, 0.43793452592896553, 0.0, 1.0, 0.4, 0.2821125962183494], 
reward next is 0.7179, 
noisyNet noise sample is [array([0.49864668], dtype=float32), 1.5811995]. 
=============================================
[2019-04-09 15:22:44,017] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.19319871e-15 1.13794196e-10 3.83755738e-09 9.06555647e-16
 6.36217967e-02 1.33661233e-05 9.36364532e-01 5.75626213e-10
 2.95273850e-07 1.38813327e-09 3.17413568e-10], sum to 1.0000
[2019-04-09 15:22:44,017] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6962
[2019-04-09 15:22:44,106] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 58.0, 93.5, 739.5, 22.5, 27.10371187205417, 0.8122519010521771, 1.0, 1.0, 45.0, 33.54450534809021], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3423600.0000, 
sim time next is 3424800.0000, 
raw observation next is [2.666666666666667, 61.0, 87.16666666666666, 715.8333333333334, 22.5, 27.30601720188933, 0.6698746821949114, 1.0, 1.0, 45.0, 32.8839072501047], 
processed observation next is [1.0, 0.6521739130434783, 0.5364727608494922, 0.61, 0.2905555555555555, 0.7909760589318601, 0.375, 0.7755014334907774, 0.7232915607316371, 1.0, 1.0, 0.6, 0.328839072501047], 
reward next is 0.6712, 
noisyNet noise sample is [array([0.7274753], dtype=float32), -0.16492705]. 
=============================================
[2019-04-09 15:22:44,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.8451622e-16 5.1205255e-12 6.8787998e-08 3.6387976e-15 2.0717260e-01
 2.0182337e-05 7.9280561e-01 6.5157200e-09 1.4608162e-06 2.6165212e-08
 3.2500305e-10], sum to 1.0000
[2019-04-09 15:22:44,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0073
[2019-04-09 15:22:44,156] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.22073541e-15 1.32177591e-12 3.94713107e-09 3.13673453e-16
 3.16539854e-02 1.45246815e-06 9.68344569e-01 4.62702682e-10
 2.96114209e-08 1.59658997e-09 1.15871646e-10], sum to 1.0000
[2019-04-09 15:22:44,156] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8960
[2019-04-09 15:22:44,216] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 100.0, 106.0, 790.5, 22.5, 26.55852951092946, 0.7604160150342877, 1.0, 1.0, 45.0, 29.336825226280318], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3247200.0000, 
sim time next is 3248400.0000, 
raw observation next is [-3.333333333333333, 90.33333333333334, 102.6666666666667, 776.1666666666667, 22.5, 26.72124259453341, 0.7883122085532706, 1.0, 1.0, 35.0, 22.908839461458196], 
processed observation next is [1.0, 0.6086956521739131, 0.37026777469990774, 0.9033333333333334, 0.3422222222222223, 0.8576427255985268, 0.375, 0.7267702162111176, 0.7627707361844235, 1.0, 1.0, 0.4, 0.22908839461458197], 
reward next is 0.7709, 
noisyNet noise sample is [array([-0.4971547], dtype=float32), -1.7898762]. 
=============================================
[2019-04-09 15:22:44,291] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 47.33333333333334, 64.5, 529.8333333333333, 22.5, 27.88203615968881, 0.9195002168043991, 1.0, 1.0, 45.0, 32.53567978264512], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3342000.0000, 
sim time next is 3343200.0000, 
raw observation next is [-2.0, 48.66666666666666, 51.83333333333334, 439.6666666666667, 22.5, 26.94325550034389, 0.8094582749420204, 1.0, 1.0, 45.0, 34.62968825729661], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.4866666666666666, 0.1727777777777778, 0.48581952117863725, 0.375, 0.7452712916953242, 0.7698194249806735, 1.0, 1.0, 0.6, 0.3462968825729661], 
reward next is 0.6537, 
noisyNet noise sample is [array([-2.9809496], dtype=float32), 1.0496755]. 
=============================================
[2019-04-09 15:22:44,949] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7512128e-14 9.6091225e-11 9.3535675e-08 4.2953034e-14 2.4247874e-02
 2.0667641e-05 9.7573107e-01 5.6242011e-09 1.9410768e-07 3.4397448e-08
 1.6028880e-09], sum to 1.0000
[2019-04-09 15:22:44,949] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7879
[2019-04-09 15:22:45,079] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 113.5, 769.0, 22.5, 25.43538508665264, 0.2419042277523935, 1.0, 1.0, 45.0, 37.47322633460351], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3322800.0000, 
sim time next is 3324000.0000, 
raw observation next is [-6.666666666666667, 60.66666666666667, 115.1666666666667, 788.3333333333333, 22.5, 25.40586921167358, 0.4130980648119142, 1.0, 1.0, 45.0, 36.809944660254104], 
processed observation next is [1.0, 0.4782608695652174, 0.27793167128347185, 0.6066666666666667, 0.383888888888889, 0.871086556169429, 0.375, 0.6171557676394649, 0.6376993549373048, 1.0, 1.0, 0.6, 0.368099446602541], 
reward next is 0.6319, 
noisyNet noise sample is [array([0.1491665], dtype=float32), 1.3909992]. 
=============================================
[2019-04-09 15:22:45,083] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[64.58653 ]
 [63.27395 ]
 [61.853146]
 [59.691883]
 [57.98395 ]], R is [[65.79296875]
 [65.76030731]
 [65.71849823]
 [65.67160797]
 [65.61685944]].
[2019-04-09 15:22:45,397] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0901817e-14 2.2439610e-10 3.0214250e-07 1.0666674e-13 1.7971906e-01
 1.2813476e-04 8.2015175e-01 7.8384481e-08 6.6136943e-07 1.0332208e-07
 5.8419829e-09], sum to 1.0000
[2019-04-09 15:22:45,398] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6801
[2019-04-09 15:22:45,451] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 22.5, 25.29832742584719, 0.4109883462706724, 1.0, 1.0, 45.0, 29.964182569528013], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3350400.0000, 
sim time next is 3351600.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 22.5, 25.10473080476363, 0.3943193812632251, 1.0, 1.0, 45.0, 29.981818837419134], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.375, 0.5920609003969691, 0.6314397937544084, 1.0, 1.0, 0.6, 0.29981818837419133], 
reward next is 0.7002, 
noisyNet noise sample is [array([0.49656937], dtype=float32), -0.26676893]. 
=============================================
[2019-04-09 15:22:47,264] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6166581e-15 2.1913545e-12 5.3424220e-08 8.0813598e-16 4.1000973e-03
 4.4437261e-06 9.9589539e-01 1.0181624e-10 6.2618241e-08 2.8472420e-09
 5.9265093e-11], sum to 1.0000
[2019-04-09 15:22:47,269] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7124
[2019-04-09 15:22:47,338] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 64.83333333333333, 544.8333333333333, 22.5, 26.70387436354868, 0.7081872179340976, 1.0, 1.0, 45.0, 31.684183565540927], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3428400.0000, 
sim time next is 3429600.0000, 
raw observation next is [2.0, 67.0, 52.83333333333334, 447.6666666666667, 22.5, 26.60163596487374, 0.6908307035528708, 1.0, 1.0, 45.0, 30.8063871965685], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.17611111111111113, 0.4946593001841621, 0.375, 0.7168029970728117, 0.7302769011842902, 1.0, 1.0, 0.6, 0.308063871965685], 
reward next is 0.6919, 
noisyNet noise sample is [array([0.82212216], dtype=float32), 0.6301252]. 
=============================================
[2019-04-09 15:22:47,570] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2667516e-14 1.2437842e-10 4.9819363e-08 3.7451043e-14 8.6484626e-03
 7.3279534e-06 9.9134314e-01 1.2137126e-09 1.0087681e-06 1.8969770e-08
 3.0204967e-09], sum to 1.0000
[2019-04-09 15:22:47,573] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5313
[2019-04-09 15:22:47,632] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 25.73312898476387, 0.6014330152406758, 0.0, 1.0, 45.0, 31.204695572476865], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3531600.0000, 
sim time next is 3532800.0000, 
raw observation next is [-0.3333333333333333, 74.0, 0.0, 0.0, 19.0, 25.77675787563579, 0.5992010856634361, 0.0, 1.0, 45.0, 31.31660731725619], 
processed observation next is [1.0, 0.9130434782608695, 0.4533702677747, 0.74, 0.0, 0.0, 0.08333333333333333, 0.6480631563029826, 0.6997336952211454, 0.0, 1.0, 0.6, 0.3131660731725619], 
reward next is 0.6868, 
noisyNet noise sample is [array([2.1889396], dtype=float32), 2.0640657]. 
=============================================
[2019-04-09 15:22:48,165] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9276329e-15 4.2023970e-10 1.0625330e-09 2.5795872e-14 1.3135031e-01
 1.9468807e-05 8.6863017e-01 1.1922228e-09 8.1642618e-08 9.0809662e-09
 7.7243362e-10], sum to 1.0000
[2019-04-09 15:22:48,166] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2412
[2019-04-09 15:22:48,297] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 22.5, 26.63953112660812, 0.7037730801299001, 1.0, 1.0, 45.0, 35.45955383436199], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3354000.0000, 
sim time next is 3355200.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 22.5, 26.57392013150375, 0.6943232579911803, 1.0, 1.0, 45.0, 35.667640977615406], 
processed observation next is [1.0, 0.8695652173913043, 0.3795013850415513, 0.55, 0.0, 0.0, 0.375, 0.7144933442919793, 0.73144108599706, 1.0, 1.0, 0.6, 0.35667640977615406], 
reward next is 0.6433, 
noisyNet noise sample is [array([0.3854746], dtype=float32), -0.31059963]. 
=============================================
[2019-04-09 15:22:48,326] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0955270e-12 2.9986499e-10 5.3393819e-07 4.3738347e-13 1.1569900e-01
 6.6907407e-05 8.8423216e-01 1.5396804e-09 1.2589746e-06 7.1225372e-08
 1.8391534e-09], sum to 1.0000
[2019-04-09 15:22:48,326] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5690
[2019-04-09 15:22:48,404] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 50.0, 110.0, 776.0, 22.5, 24.59620449824655, 0.3379741552933233, 1.0, 1.0, 45.0, 28.692501218511094], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3333600.0000, 
sim time next is 3334800.0000, 
raw observation next is [-3.666666666666667, 50.0, 107.3333333333333, 760.0, 22.5, 25.30536893015186, 0.4238052486394157, 1.0, 1.0, 45.0, 28.219596695094204], 
processed observation next is [1.0, 0.6086956521739131, 0.3610341643582641, 0.5, 0.3577777777777777, 0.8397790055248618, 0.375, 0.6087807441793217, 0.6412684162131386, 1.0, 1.0, 0.6, 0.282195966950942], 
reward next is 0.7178, 
noisyNet noise sample is [array([-1.0543011], dtype=float32), 0.12456897]. 
=============================================
[2019-04-09 15:22:49,595] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.0872687e-12 1.1111563e-08 1.9077324e-06 3.7676355e-12 4.3043110e-01
 1.0969912e-03 5.6845605e-01 1.4291082e-07 3.9183160e-06 9.8513683e-06
 1.9878637e-08], sum to 1.0000
[2019-04-09 15:22:49,596] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2860
[2019-04-09 15:22:49,625] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 64.0, 0.0, 0.0, 19.0, 25.3175957441155, 0.4724171337000883, 0.0, 1.0, 35.0, 25.898881938339102], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3540000.0000, 
sim time next is 3541200.0000, 
raw observation next is [-1.666666666666667, 62.0, 0.0, 0.0, 19.0, 25.15722925392244, 0.4268094307235637, 0.0, 1.0, 35.0, 23.2062997663262], 
processed observation next is [1.0, 1.0, 0.4164358264081256, 0.62, 0.0, 0.0, 0.08333333333333333, 0.5964357711602034, 0.6422698102411879, 0.0, 1.0, 0.4, 0.232062997663262], 
reward next is 0.7679, 
noisyNet noise sample is [array([0.09773511], dtype=float32), 1.3011969]. 
=============================================
[2019-04-09 15:22:50,164] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.7780521e-14 4.5680609e-11 4.0673893e-08 3.9970748e-15 1.8180298e-02
 4.1887481e-05 9.8177356e-01 7.9464546e-10 4.1605863e-06 1.5117280e-08
 1.0441393e-09], sum to 1.0000
[2019-04-09 15:22:50,167] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1544
[2019-04-09 15:22:50,192] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 19.0, 25.80366928031513, 0.5945562050057979, 0.0, 1.0, 45.0, 31.66151148748593], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3445200.0000, 
sim time next is 3446400.0000, 
raw observation next is [1.0, 81.33333333333334, 0.0, 0.0, 19.0, 25.73990177539287, 0.5827538373789509, 0.0, 1.0, 45.0, 31.598625570699575], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.8133333333333335, 0.0, 0.0, 0.08333333333333333, 0.6449918146160725, 0.694251279126317, 0.0, 1.0, 0.6, 0.31598625570699573], 
reward next is 0.6840, 
noisyNet noise sample is [array([0.88655734], dtype=float32), 0.736986]. 
=============================================
[2019-04-09 15:22:50,818] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6296666e-13 5.0054689e-09 7.0005649e-07 6.3604775e-13 1.1282711e-01
 1.6154071e-03 8.8551587e-01 6.8505734e-08 4.0449369e-05 3.0662696e-07
 1.2473156e-08], sum to 1.0000
[2019-04-09 15:22:50,819] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9145
[2019-04-09 15:22:50,841] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 83.66666666666666, 0.0, 0.0, 19.0, 25.16340088081655, 0.4403727771889421, 0.0, 1.0, 35.0, 22.199996588674495], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3447600.0000, 
sim time next is 3448800.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.0, 25.10761515063031, 0.4453394075141415, 0.0, 1.0, 45.0, 32.0085509440713], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 0.08333333333333333, 0.5923012625525258, 0.6484464691713805, 0.0, 1.0, 0.6, 0.320085509440713], 
reward next is 0.6799, 
noisyNet noise sample is [array([0.8473998], dtype=float32), 1.085026]. 
=============================================
[2019-04-09 15:22:51,762] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2547455e-11 1.5371127e-08 2.7940519e-07 2.6946577e-11 6.0355622e-01
 1.5087810e-03 3.9479113e-01 1.1793851e-06 1.4057425e-04 1.7621605e-06
 5.1178240e-08], sum to 1.0000
[2019-04-09 15:22:51,762] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0422
[2019-04-09 15:22:51,793] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 70.33333333333334, 0.0, 0.0, 19.0, 24.25475495461895, 0.1880468589530775, 0.0, 1.0, 45.0, 32.715244595432274], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3468000.0000, 
sim time next is 3469200.0000, 
raw observation next is [1.0, 68.66666666666667, 0.0, 0.0, 19.0, 24.29107092321888, 0.1839405952987595, 0.0, 1.0, 35.0, 25.636718446402245], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.6866666666666668, 0.0, 0.0, 0.08333333333333333, 0.5242559102682399, 0.5613135317662531, 0.0, 1.0, 0.4, 0.25636718446402246], 
reward next is 0.7436, 
noisyNet noise sample is [array([-0.4580733], dtype=float32), 0.14081313]. 
=============================================
[2019-04-09 15:22:52,085] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.76996190e-16 2.58272848e-11 3.89190502e-09 8.52772707e-16
 3.91450264e-02 9.92719379e-06 9.60844874e-01 4.68363792e-10
 1.37389733e-07 7.49543627e-09 1.10599176e-10], sum to 1.0000
[2019-04-09 15:22:52,086] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1675
[2019-04-09 15:22:52,147] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 10.0, 100.8333333333333, 22.5, 26.87463596779489, 0.7975065814325749, 1.0, 1.0, 45.0, 32.54080239332759], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3433200.0000, 
sim time next is 3434400.0000, 
raw observation next is [2.0, 67.0, 0.0, 0.0, 22.5, 26.93274532909124, 0.7770590875159669, 1.0, 1.0, 45.0, 32.591186533029735], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.67, 0.0, 0.0, 0.375, 0.7443954440909367, 0.7590196958386556, 1.0, 1.0, 0.6, 0.32591186533029737], 
reward next is 0.6741, 
noisyNet noise sample is [array([-0.7332521], dtype=float32), -0.2501965]. 
=============================================
[2019-04-09 15:22:52,563] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.3484140e-12 1.5093539e-08 2.1376090e-06 2.5925607e-11 1.1551972e-01
 8.1873448e-05 8.8439065e-01 4.4675471e-07 3.4088503e-06 1.7309599e-06
 2.5061182e-08], sum to 1.0000
[2019-04-09 15:22:52,564] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2116
[2019-04-09 15:22:52,586] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3684001e-16 5.0824961e-12 5.7038363e-10 8.6168228e-16 7.5904294e-03
 4.7622609e-05 9.9236166e-01 4.3112913e-10 2.2803469e-07 4.3539008e-09
 2.5502747e-11], sum to 1.0000
[2019-04-09 15:22:52,586] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7304
[2019-04-09 15:22:52,598] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 67.33333333333333, 0.0, 0.0, 19.0, 24.51205646531447, 0.2864515991979107, 0.0, 1.0, 45.0, 32.72915757792293], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3548400.0000, 
sim time next is 3549600.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 19.0, 24.41702652735183, 0.2730155720610305, 0.0, 1.0, 45.0, 32.57169358601628], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5347522106126524, 0.5910051906870102, 0.0, 1.0, 0.6, 0.3257169358601628], 
reward next is 0.6743, 
noisyNet noise sample is [array([-1.7298652], dtype=float32), -0.16539463]. 
=============================================
[2019-04-09 15:22:52,693] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 83.66666666666667, 660.0, 22.5, 27.82469077583072, 0.9902747056411977, 1.0, 1.0, 45.0, 23.251137313001422], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3512400.0000, 
sim time next is 3513600.0000, 
raw observation next is [3.0, 49.0, 75.0, 606.0, 22.5, 27.23896601941662, 0.9315876743491206, 1.0, 1.0, 45.0, 29.7407406784141], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.25, 0.6696132596685083, 0.375, 0.7699138349513849, 0.8105292247830401, 1.0, 1.0, 0.6, 0.297407406784141], 
reward next is 0.7026, 
noisyNet noise sample is [array([-0.33224806], dtype=float32), -1.4479171]. 
=============================================
[2019-04-09 15:22:52,703] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3588478e-14 7.1588881e-11 7.1583495e-09 5.5791478e-14 1.8841378e-01
 1.1450629e-05 8.1157458e-01 1.5750652e-09 8.4395090e-08 9.2704376e-08
 5.1687956e-09], sum to 1.0000
[2019-04-09 15:22:52,704] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4436
[2019-04-09 15:22:52,770] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666666, 60.66666666666667, 110.0, 776.6666666666667, 22.5, 26.22796292708357, 0.6242835413113799, 1.0, 1.0, 45.0, 30.66265366308219], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3494400.0000, 
sim time next is 3495600.0000, 
raw observation next is [1.0, 61.0, 112.0, 790.0, 22.5, 25.56348228197816, 0.5529139521355398, 1.0, 1.0, 45.0, 30.927841971380893], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.61, 0.37333333333333335, 0.8729281767955801, 0.375, 0.6302901901648467, 0.6843046507118467, 1.0, 1.0, 0.6, 0.3092784197138089], 
reward next is 0.6907, 
noisyNet noise sample is [array([-0.44807088], dtype=float32), 0.21374427]. 
=============================================
[2019-04-09 15:22:52,999] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1229440e-10 9.1758857e-08 2.0313037e-06 2.6690999e-10 1.6052793e-01
 1.0635635e-03 8.3838856e-01 2.3329735e-06 6.8996942e-06 8.3807226e-06
 1.6772486e-07], sum to 1.0000
[2019-04-09 15:22:53,002] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5845
[2019-04-09 15:22:53,045] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 24.0, 113.5, 789.5, 19.0, 25.35708866054511, 0.4134375283777607, 0.0, 1.0, 45.0, 23.1884861492122], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3668400.0000, 
sim time next is 3669600.0000, 
raw observation next is [9.333333333333334, 31.0, 115.1666666666667, 807.1666666666666, 19.0, 25.46787929618927, 0.4358724062653967, 0.0, 1.0, 45.0, 22.773802358860287], 
processed observation next is [0.0, 0.4782608695652174, 0.7211449676823639, 0.31, 0.383888888888889, 0.8918968692449355, 0.08333333333333333, 0.6223232746824392, 0.6452908020884656, 0.0, 1.0, 0.6, 0.22773802358860287], 
reward next is 0.7723, 
noisyNet noise sample is [array([-0.68422794], dtype=float32), -0.37930366]. 
=============================================
[2019-04-09 15:22:53,455] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.7763924e-14 5.4696241e-11 1.7303751e-08 3.9426006e-14 1.3516684e-02
 2.9233061e-06 9.8648006e-01 2.3515567e-09 2.8476751e-07 1.8794442e-08
 3.6684253e-09], sum to 1.0000
[2019-04-09 15:22:53,460] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1632
[2019-04-09 15:22:53,495] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 26.40792598039748, 0.7464257948147782, 0.0, 1.0, 45.0, 30.831285129416358], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3530400.0000, 
sim time next is 3531600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.34873300158682, 0.7368815038626438, 0.0, 1.0, 45.0, 30.95553053842204], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.6957277501322349, 0.7456271679542147, 0.0, 1.0, 0.6, 0.3095553053842204], 
reward next is 0.6904, 
noisyNet noise sample is [array([1.8754781], dtype=float32), -1.3473024]. 
=============================================
[2019-04-09 15:22:53,623] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4571281e-13 1.2374977e-10 2.4473918e-08 6.9170201e-14 8.3597302e-02
 1.1450782e-04 9.1628718e-01 2.6759932e-09 8.9483854e-07 1.7193076e-07
 1.2652799e-09], sum to 1.0000
[2019-04-09 15:22:53,623] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6138
[2019-04-09 15:22:53,674] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 19.0, 25.74265985994281, 0.5866618766650947, 0.0, 1.0, 45.0, 31.480589678363103], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3448800.0000, 
sim time next is 3450000.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.0, 25.6832523055511, 0.5084299420447193, 0.0, 1.0, 45.0, 32.083921325827845], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6402710254625917, 0.6694766473482398, 0.0, 1.0, 0.6, 0.32083921325827847], 
reward next is 0.6792, 
noisyNet noise sample is [array([0.2293284], dtype=float32), 1.5993623]. 
=============================================
[2019-04-09 15:22:53,693] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[62.825687]
 [63.120422]
 [63.49563 ]
 [64.81263 ]
 [65.090706]], R is [[62.37654114]
 [62.43797302]
 [62.4989624 ]
 [62.55935669]
 [62.61869431]].
[2019-04-09 15:22:53,714] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.0190049e-10 8.4570064e-07 3.3629254e-05 7.4394363e-10 6.3400313e-02
 1.5367104e-03 9.3497503e-01 3.4367065e-06 1.6488342e-05 3.2630545e-05
 9.1639509e-07], sum to 1.0000
[2019-04-09 15:22:53,716] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0110
[2019-04-09 15:22:53,741] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 65.0, 105.5, 717.0, 19.0, 23.00115803452005, -0.02677856253171981, 0.0, 1.0, 35.0, 23.246726363038736], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3578400.0000, 
sim time next is 3579600.0000, 
raw observation next is [-4.666666666666667, 61.33333333333334, 109.1666666666667, 744.3333333333334, 19.0, 22.96593242433477, -0.0349465086050874, 0.0, 1.0, 35.0, 20.5971601763095], 
processed observation next is [0.0, 0.43478260869565216, 0.3333333333333333, 0.6133333333333334, 0.363888888888889, 0.8224677716390424, 0.08333333333333333, 0.41382770202789754, 0.48835116379830423, 0.0, 1.0, 0.4, 0.205971601763095], 
reward next is 0.7940, 
noisyNet noise sample is [array([-0.6457589], dtype=float32), -0.5192564]. 
=============================================
[2019-04-09 15:22:53,990] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.8567599e-11 8.9453567e-07 5.2128789e-06 2.6494182e-10 2.9750016e-01
 2.3432290e-03 6.9996643e-01 9.5104122e-07 1.5990942e-04 2.3032173e-05
 9.8236676e-08], sum to 1.0000
[2019-04-09 15:22:53,990] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2218
[2019-04-09 15:22:54,056] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 25.0838185478883, 0.4120823564451544, 0.0, 1.0, 35.0, 25.72205549695062], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3544800.0000, 
sim time next is 3546000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 24.94201412694017, 0.3727318948343792, 0.0, 1.0, 35.0, 22.93374752033114], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.578501177245014, 0.6242439649447931, 0.0, 1.0, 0.4, 0.22933747520331138], 
reward next is 0.7707, 
noisyNet noise sample is [array([-0.9982361], dtype=float32), 0.090169914]. 
=============================================
[2019-04-09 15:22:54,074] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[52.57576 ]
 [55.23684 ]
 [57.108   ]
 [59.046158]
 [59.361935]], R is [[50.7832489 ]
 [51.01819611]
 [51.17853928]
 [51.33802414]
 [51.49703598]].
[2019-04-09 15:22:54,338] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9984946e-11 4.0306727e-09 3.2223525e-07 3.1630927e-11 3.3666575e-01
 6.3036241e-05 6.6325414e-01 1.7001621e-07 1.5342250e-05 1.1668791e-06
 8.4948660e-08], sum to 1.0000
[2019-04-09 15:22:54,339] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5039
[2019-04-09 15:22:54,375] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 24.78433949195185, 0.3581157965547983, 0.0, 1.0, 35.0, 25.27462735498846], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3546000.0000, 
sim time next is 3547200.0000, 
raw observation next is [-2.333333333333333, 63.66666666666667, 0.0, 0.0, 19.0, 24.67754472348801, 0.3409524663981277, 0.0, 1.0, 45.0, 32.93429101784821], 
processed observation next is [0.0, 0.043478260869565216, 0.3979686057248385, 0.6366666666666667, 0.0, 0.0, 0.08333333333333333, 0.5564620602906674, 0.6136508221327093, 0.0, 1.0, 0.6, 0.3293429101784821], 
reward next is 0.6707, 
noisyNet noise sample is [array([-0.42387304], dtype=float32), -0.80385095]. 
=============================================
[2019-04-09 15:22:54,430] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.23676902e-11 3.91467800e-08 1.19540701e-07 1.20457585e-11
 1.04020663e-01 3.03956651e-04 8.95649672e-01 3.53295462e-07
 2.27617293e-05 2.34726758e-06 6.42540954e-08], sum to 1.0000
[2019-04-09 15:22:54,430] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8563
[2019-04-09 15:22:54,463] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 69.66666666666667, 0.0, 0.0, 19.0, 25.27023242194728, 0.3679289502375018, 0.0, 1.0, 45.0, 30.356323957998924], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3709200.0000, 
sim time next is 3710400.0000, 
raw observation next is [-2.0, 67.33333333333333, 0.0, 0.0, 19.0, 25.14935083723113, 0.3444108345752303, 0.0, 1.0, 45.0, 30.64114593612913], 
processed observation next is [0.0, 0.9565217391304348, 0.40720221606648205, 0.6733333333333333, 0.0, 0.0, 0.08333333333333333, 0.5957792364359275, 0.6148036115250768, 0.0, 1.0, 0.6, 0.3064114593612913], 
reward next is 0.6936, 
noisyNet noise sample is [array([-0.09184857], dtype=float32), -1.1057094]. 
=============================================
[2019-04-09 15:22:54,531] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.9465240e-11 3.9428723e-08 1.2166145e-06 5.7326470e-11 6.5803856e-01
 1.3262081e-03 3.4054452e-01 3.1624108e-07 8.1304810e-05 6.6492116e-06
 1.1233778e-06], sum to 1.0000
[2019-04-09 15:22:54,544] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4587
[2019-04-09 15:22:54,572] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 47.33333333333334, 114.6666666666667, 813.8333333333334, 19.0, 23.93730857755748, 0.1669217704587986, 0.0, 1.0, 35.0, 19.181429464954434], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3590400.0000, 
sim time next is 3591600.0000, 
raw observation next is [-1.333333333333333, 44.66666666666667, 112.0, 808.0, 19.0, 23.93478538115938, 0.1643177069133322, 0.0, 1.0, 35.0, 17.232138800631787], 
processed observation next is [0.0, 0.5652173913043478, 0.42566943674976926, 0.4466666666666667, 0.37333333333333335, 0.8928176795580111, 0.08333333333333333, 0.49456544842994826, 0.5547725689711107, 0.0, 1.0, 0.4, 0.17232138800631788], 
reward next is 0.8277, 
noisyNet noise sample is [array([-0.9990133], dtype=float32), -0.46352234]. 
=============================================
[2019-04-09 15:22:54,583] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5631300e-10 1.3331869e-07 3.4927534e-06 1.2966266e-09 7.1129121e-02
 1.3529895e-04 9.2871398e-01 1.9152751e-06 1.3024242e-05 2.5670579e-06
 3.2881533e-07], sum to 1.0000
[2019-04-09 15:22:54,583] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4725
[2019-04-09 15:22:54,617] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333333, 70.0, 0.0, 0.0, 19.0, 23.00046128963219, -0.0756002635336319, 0.0, 1.0, 45.0, 34.0551575468303], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3568800.0000, 
sim time next is 3570000.0000, 
raw observation next is [-6.666666666666666, 70.0, 17.16666666666666, 171.6666666666667, 19.0, 22.91080654074243, -0.07753669657906413, 0.0, 1.0, 45.0, 34.15228365716148], 
processed observation next is [0.0, 0.30434782608695654, 0.2779316712834719, 0.7, 0.0572222222222222, 0.18968692449355437, 0.08333333333333333, 0.4092338783952026, 0.4741544344736453, 0.0, 1.0, 0.6, 0.3415228365716148], 
reward next is 0.6585, 
noisyNet noise sample is [array([-1.8131121], dtype=float32), 0.46321148]. 
=============================================
[2019-04-09 15:22:54,627] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[42.01445 ]
 [42.165215]
 [42.092224]
 [42.25327 ]
 [42.38483 ]], R is [[42.27205658]
 [42.50878143]
 [42.74549103]
 [42.9818306 ]
 [43.21730042]].
[2019-04-09 15:22:54,652] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.2349470e-12 6.0384799e-08 1.4030978e-06 3.7246737e-11 4.5820686e-01
 1.6439930e-03 5.4013884e-01 1.9984525e-07 8.2194547e-06 3.4562549e-07
 8.5379675e-08], sum to 1.0000
[2019-04-09 15:22:54,652] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0740
[2019-04-09 15:22:54,679] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.666666666666666, 48.0, 89.83333333333333, 716.8333333333333, 19.0, 26.26781121856604, 0.6626691053659924, 0.0, 1.0, 45.0, 24.944447769385853], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3684000.0000, 
sim time next is 3685200.0000, 
raw observation next is [5.333333333333333, 49.0, 83.16666666666666, 674.5, 19.0, 26.32734498714505, 0.6665531965543455, 0.0, 1.0, 35.0, 19.34158863581306], 
processed observation next is [0.0, 0.6521739130434783, 0.6103416435826409, 0.49, 0.2772222222222222, 0.7453038674033149, 0.08333333333333333, 0.6939454155954209, 0.7221843988514486, 0.0, 1.0, 0.4, 0.19341588635813062], 
reward next is 0.8066, 
noisyNet noise sample is [array([-1.0565876], dtype=float32), -0.6219723]. 
=============================================
[2019-04-09 15:22:54,682] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0958558e-11 6.7316790e-08 1.5531369e-06 4.2996117e-11 4.5934540e-01
 1.7677974e-03 5.3887534e-01 2.2331763e-07 9.0910062e-06 3.7527036e-07
 9.5237667e-08], sum to 1.0000
[2019-04-09 15:22:54,683] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3569
[2019-04-09 15:22:54,712] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.333333333333333, 49.0, 83.16666666666666, 674.5, 19.0, 26.32734498714505, 0.6665531965543455, 0.0, 1.0, 35.0, 19.34158863581306], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3685200.0000, 
sim time next is 3686400.0000, 
raw observation next is [5.0, 50.0, 75.5, 613.5, 19.0, 26.3444471603027, 0.6620690028133576, 0.0, 1.0, 35.0, 17.32400548612531], 
processed observation next is [0.0, 0.6956521739130435, 0.6011080332409973, 0.5, 0.25166666666666665, 0.6779005524861879, 0.08333333333333333, 0.6953705966918916, 0.7206896676044524, 0.0, 1.0, 0.4, 0.17324005486125307], 
reward next is 0.8268, 
noisyNet noise sample is [array([-1.0565876], dtype=float32), -0.6219723]. 
=============================================
[2019-04-09 15:22:54,733] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2055462e-16 1.1243909e-11 2.6249121e-09 9.3047575e-16 6.8956469e-03
 1.1255829e-05 9.9309301e-01 3.9824150e-10 6.5734802e-08 1.1248923e-08
 1.2228618e-10], sum to 1.0000
[2019-04-09 15:22:54,734] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2450
[2019-04-09 15:22:54,752] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2026108e-09 2.2980984e-07 2.8800034e-06 2.6223651e-10 2.2006798e-01
 6.6119502e-04 7.7923834e-01 2.5946629e-06 2.0348320e-05 4.8628272e-06
 1.6620767e-06], sum to 1.0000
[2019-04-09 15:22:54,754] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3275
[2019-04-09 15:22:54,769] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 59.66666666666667, 114.0, 803.3333333333333, 22.5, 25.73375170835063, 0.6093108927436961, 1.0, 1.0, 45.0, 29.790157015069195], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3496800.0000, 
sim time next is 3498000.0000, 
raw observation next is [1.666666666666667, 58.33333333333334, 115.1666666666667, 812.1666666666666, 22.5, 26.28909740434532, 0.6770609052593866, 1.0, 1.0, 45.0, 29.361133976486325], 
processed observation next is [1.0, 0.4782608695652174, 0.5087719298245615, 0.5833333333333335, 0.383888888888889, 0.8974217311233885, 0.375, 0.6907581170287767, 0.7256869684197955, 1.0, 1.0, 0.6, 0.29361133976486326], 
reward next is 0.7064, 
noisyNet noise sample is [array([-0.58616704], dtype=float32), -0.6955714]. 
=============================================
[2019-04-09 15:22:54,787] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[66.58365 ]
 [65.98447 ]
 [64.95709 ]
 [63.96988 ]
 [62.599888]], R is [[67.18746948]
 [67.21768951]
 [67.2371521 ]
 [67.25901031]
 [67.27142334]].
[2019-04-09 15:22:54,795] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.333333333333334, 66.66666666666667, 0.0, 0.0, 19.0, 23.66029427216248, 0.09006249448489985, 0.0, 1.0, 45.0, 33.76201665356126], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3561600.0000, 
sim time next is 3562800.0000, 
raw observation next is [-5.666666666666667, 68.33333333333333, 0.0, 0.0, 19.0, 23.63302291875696, 0.07888063986629235, 0.0, 1.0, 45.0, 33.68192112953821], 
processed observation next is [0.0, 0.21739130434782608, 0.30563250230840255, 0.6833333333333332, 0.0, 0.0, 0.08333333333333333, 0.46941857656308006, 0.5262935466220974, 0.0, 1.0, 0.6, 0.3368192112953821], 
reward next is 0.6632, 
noisyNet noise sample is [array([-0.844633], dtype=float32), 1.024922]. 
=============================================
[2019-04-09 15:22:55,129] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9034836e-14 3.6859571e-10 2.8132275e-08 2.3942710e-13 5.9244137e-02
 7.8740768e-06 9.4074750e-01 8.9327106e-09 4.0743043e-07 5.3291064e-08
 2.2902797e-09], sum to 1.0000
[2019-04-09 15:22:55,130] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1112
[2019-04-09 15:22:55,159] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 19.0, 25.765066445175, 0.6023045892582609, 0.0, 1.0, 45.0, 31.59259500268407], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3448800.0000, 
sim time next is 3450000.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.0, 25.71567436678881, 0.525272646984955, 0.0, 1.0, 45.0, 32.17168407940286], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6429728638990676, 0.6750908823283184, 0.0, 1.0, 0.6, 0.32171684079402857], 
reward next is 0.6783, 
noisyNet noise sample is [array([-0.59646386], dtype=float32), -0.12674275]. 
=============================================
[2019-04-09 15:22:55,193] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[60.85377 ]
 [61.0377  ]
 [61.651096]
 [62.840004]
 [63.309082]], R is [[60.50350952]
 [60.58255005]
 [60.66048813]
 [60.73416901]
 [60.81322098]].
[2019-04-09 15:22:55,419] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.03301055e-14 1.10067641e-11 6.78269796e-09 1.67489955e-15
 6.88923569e-03 2.99412874e-04 9.92810905e-01 6.21037444e-09
 2.99254850e-07 1.07429798e-07 2.21193189e-10], sum to 1.0000
[2019-04-09 15:22:55,419] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5455
[2019-04-09 15:22:55,446] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 27.11683833846568, 0.8694144401460454, 1.0, 1.0, 45.0, 29.539066207225666], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3524400.0000, 
sim time next is 3525600.0000, 
raw observation next is [1.333333333333333, 58.66666666666667, 0.0, 0.0, 22.5, 27.00765507990024, 0.8500881365714991, 0.0, 1.0, 45.0, 29.673328916215684], 
processed observation next is [1.0, 0.8260869565217391, 0.4995383194829178, 0.5866666666666667, 0.0, 0.0, 0.375, 0.7506379233250199, 0.7833627121904997, 0.0, 1.0, 0.6, 0.29673328916215685], 
reward next is 0.7033, 
noisyNet noise sample is [array([0.8554014], dtype=float32), -0.6127111]. 
=============================================
[2019-04-09 15:22:55,627] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.86548606e-12 2.56369170e-09 1.71104901e-07 4.59880875e-12
 2.33992618e-02 1.89049664e-04 9.76392508e-01 8.93358774e-08
 1.80932147e-05 8.27648023e-07 1.06034905e-08], sum to 1.0000
[2019-04-09 15:22:55,627] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5991
[2019-04-09 15:22:55,674] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 25.04286146612731, 0.3233550089565185, 0.0, 1.0, 45.0, 31.087211452730745], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3711600.0000, 
sim time next is 3712800.0000, 
raw observation next is [-3.0, 67.0, 0.0, 0.0, 19.0, 24.92299238878238, 0.2989092342037164, 0.0, 1.0, 45.0, 31.402491767688435], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.67, 0.0, 0.0, 0.08333333333333333, 0.5769160323985316, 0.5996364114012388, 0.0, 1.0, 0.6, 0.31402491767688434], 
reward next is 0.6860, 
noisyNet noise sample is [array([-1.7344749], dtype=float32), -1.8622501]. 
=============================================
[2019-04-09 15:22:55,754] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.0179157e-16 1.0431207e-11 1.0591626e-09 7.1597955e-17 6.9624878e-04
 2.0121615e-06 9.9930167e-01 2.4353281e-10 9.9432773e-09 1.6356312e-11
 1.4133896e-11], sum to 1.0000
[2019-04-09 15:22:55,755] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9052
[2019-04-09 15:22:55,775] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0350543e-13 3.8426048e-10 1.5009288e-08 4.7100239e-14 3.2997116e-02
 6.4305818e-06 9.6699375e-01 5.5543006e-09 2.6371381e-06 4.9969120e-08
 2.5108982e-09], sum to 1.0000
[2019-04-09 15:22:55,776] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8181
[2019-04-09 15:22:55,799] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666666, 76.0, 0.0, 0.0, 19.0, 26.21585138629874, 0.7101827256503602, 0.0, 1.0, 45.0, 31.303273878658466], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3534000.0000, 
sim time next is 3535200.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 19.0, 26.16582679934608, 0.6924057509033852, 0.0, 1.0, 45.0, 31.54338333729065], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6804855666121732, 0.7308019169677951, 0.0, 1.0, 0.6, 0.3154338333729065], 
reward next is 0.6846, 
noisyNet noise sample is [array([0.7065716], dtype=float32), 0.8967131]. 
=============================================
[2019-04-09 15:22:55,803] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 99.66666666666666, 765.3333333333334, 22.5, 26.89099907286396, 0.8583824058064499, 1.0, 1.0, 45.0, 28.37969102901235], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3508800.0000, 
sim time next is 3510000.0000, 
raw observation next is [3.0, 49.0, 95.0, 734.0, 22.5, 27.01995022154115, 0.9015047080539079, 1.0, 1.0, 45.0, 22.87149190939315], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.31666666666666665, 0.8110497237569061, 0.375, 0.7516625184617626, 0.8005015693513027, 1.0, 1.0, 0.6, 0.2287149190939315], 
reward next is 0.7713, 
noisyNet noise sample is [array([-0.09980796], dtype=float32), 0.26606357]. 
=============================================
[2019-04-09 15:22:55,837] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[68.07683 ]
 [67.924545]
 [67.68935 ]
 [67.567215]
 [67.38616 ]], R is [[68.2023468 ]
 [68.23652649]
 [68.29341888]
 [68.35617065]
 [68.41796875]].
[2019-04-09 15:22:56,009] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.72886330e-10 3.35464364e-07 8.90765023e-06 4.56205129e-10
 3.21965069e-01 1.07145584e-04 6.77903056e-01 1.70193334e-06
 8.44893748e-06 4.99394719e-06 3.89231587e-07], sum to 1.0000
[2019-04-09 15:22:56,009] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5558
[2019-04-09 15:22:56,070] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.666666666666666, 68.33333333333334, 98.0, 634.1666666666667, 19.0, 22.958742472815, -0.03132770907904376, 0.0, 1.0, 45.0, 32.62211292716396], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3576000.0000, 
sim time next is 3577200.0000, 
raw observation next is [-5.333333333333333, 66.66666666666666, 101.8333333333333, 689.6666666666666, 19.0, 23.03362209054901, -0.01372447199294866, 0.0, 1.0, 45.0, 32.55653300053673], 
processed observation next is [0.0, 0.391304347826087, 0.3148661126500462, 0.6666666666666665, 0.3394444444444443, 0.7620626151012891, 0.08333333333333333, 0.41946850754575077, 0.4954251760023505, 0.0, 1.0, 0.6, 0.3255653300053673], 
reward next is 0.6744, 
noisyNet noise sample is [array([1.9818385], dtype=float32), 1.0090669]. 
=============================================
[2019-04-09 15:22:56,177] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9650084e-12 1.1502992e-09 9.9739879e-08 1.8121173e-12 5.5919968e-02
 1.0038326e-03 9.4307238e-01 3.1902136e-08 3.3863469e-06 3.4561725e-07
 2.9306204e-09], sum to 1.0000
[2019-04-09 15:22:56,178] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5692
[2019-04-09 15:22:56,193] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.0376159e-10 1.9721598e-07 4.9685200e-06 1.1732501e-10 4.1822445e-01
 6.5385411e-04 5.8109307e-01 1.3118095e-06 9.4077650e-06 1.2232243e-05
 4.6391153e-07], sum to 1.0000
[2019-04-09 15:22:56,194] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5478
[2019-04-09 15:22:56,206] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 25.08797555278105, 0.4217755299192688, 0.0, 1.0, 45.0, 32.91679496300273], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3544800.0000, 
sim time next is 3546000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 24.99171017367357, 0.4007392240026997, 0.0, 1.0, 45.0, 32.83763449323776], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.5826425144727976, 0.6335797413342332, 0.0, 1.0, 0.6, 0.32837634493237755], 
reward next is 0.6716, 
noisyNet noise sample is [array([-0.34713697], dtype=float32), 0.045395758]. 
=============================================
[2019-04-09 15:22:56,211] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[53.5153  ]
 [55.833878]
 [57.72448 ]
 [59.67603 ]
 [59.99702 ]], R is [[51.86022186]
 [52.01245499]
 [52.16283035]
 [52.31243896]
 [52.46166992]].
[2019-04-09 15:22:56,227] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 24.54494112318668, 0.2147809150878538, 0.0, 1.0, 35.0, 21.71927959713104], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3609600.0000, 
sim time next is 3610800.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 24.49925165987336, 0.2098661360059758, 0.0, 1.0, 45.0, 27.645282857964474], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.5416043049894466, 0.5699553786686585, 0.0, 1.0, 0.6, 0.27645282857964476], 
reward next is 0.7235, 
noisyNet noise sample is [array([-1.3974402], dtype=float32), -0.50770515]. 
=============================================
[2019-04-09 15:22:56,311] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0368478e-11 9.0328722e-09 2.0980528e-07 2.1877621e-12 4.2366948e-02
 2.6329726e-04 9.5736659e-01 5.2448968e-08 2.6692046e-06 1.5948942e-07
 4.7959158e-08], sum to 1.0000
[2019-04-09 15:22:56,316] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1450
[2019-04-09 15:22:56,355] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 24.16209860299091, 0.1100121024111459, 0.0, 1.0, 45.0, 33.592328288544635], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3729600.0000, 
sim time next is 3730800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 24.17497605236677, 0.1086028686592233, 0.0, 1.0, 45.0, 33.62293971733817], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.5145813376972308, 0.5362009562197411, 0.0, 1.0, 0.6, 0.3362293971733817], 
reward next is 0.6638, 
noisyNet noise sample is [array([0.35515568], dtype=float32), 0.4182933]. 
=============================================
[2019-04-09 15:22:56,528] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6982939e-13 5.7549476e-10 5.9591319e-09 8.9236256e-14 2.6283314e-02
 5.2837855e-05 9.7366363e-01 8.7004706e-09 2.4071812e-07 7.1861388e-09
 1.4143250e-09], sum to 1.0000
[2019-04-09 15:22:56,528] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5913
[2019-04-09 15:22:56,557] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 74.0, 0.0, 0.0, 19.0, 26.06559218599606, 0.6634793026786227, 0.0, 1.0, 45.0, 31.20571726539552], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3532800.0000, 
sim time next is 3534000.0000, 
raw observation next is [-0.6666666666666666, 76.0, 0.0, 0.0, 19.0, 25.97638881641394, 0.6496376193675392, 0.0, 1.0, 45.0, 31.422124426437335], 
processed observation next is [1.0, 0.9130434782608695, 0.44413665743305636, 0.76, 0.0, 0.0, 0.08333333333333333, 0.6646990680344951, 0.716545873122513, 0.0, 1.0, 0.6, 0.31422124426437337], 
reward next is 0.6858, 
noisyNet noise sample is [array([-0.28274387], dtype=float32), -2.0130692]. 
=============================================
[2019-04-09 15:22:56,587] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[61.294704]
 [62.47965 ]
 [62.86325 ]
 [63.17807 ]
 [64.66509 ]], R is [[60.89542007]
 [60.9744072 ]
 [61.05392838]
 [61.13396835]
 [61.2151947 ]].
[2019-04-09 15:22:56,653] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4449112e-09 4.1195113e-07 6.9465264e-06 1.0252426e-09 7.2214343e-02
 7.9977559e-04 9.2686462e-01 1.1198682e-05 9.6561926e-05 4.7726985e-06
 1.3988483e-06], sum to 1.0000
[2019-04-09 15:22:56,653] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1638
[2019-04-09 15:22:56,683] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.666666666666668, 28.66666666666667, 0.0, 0.0, 19.0, 24.09496144923409, 0.04929509736055782, 0.0, 1.0, 45.0, 29.02134104639019], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3655200.0000, 
sim time next is 3656400.0000, 
raw observation next is [8.333333333333334, 30.33333333333334, 18.16666666666666, 168.0, 19.0, 24.10240652956957, 0.0692808327716155, 0.0, 1.0, 45.0, 28.288730721825942], 
processed observation next is [0.0, 0.30434782608695654, 0.6934441366574331, 0.3033333333333334, 0.060555555555555536, 0.1856353591160221, 0.08333333333333333, 0.5085338774641309, 0.5230936109238719, 0.0, 1.0, 0.6, 0.28288730721825944], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.06805123], dtype=float32), 0.10637751]. 
=============================================
[2019-04-09 15:22:57,023] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.7224336e-10 1.7882020e-07 9.1691290e-06 1.5508234e-09 7.0759349e-02
 7.8408467e-04 9.2834574e-01 5.9696794e-07 8.6751199e-05 1.3485178e-05
 7.2298661e-07], sum to 1.0000
[2019-04-09 15:22:57,023] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1972
[2019-04-09 15:22:57,052] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 69.0, 0.0, 0.0, 19.0, 24.33166355642108, 0.2233939837839177, 0.0, 1.0, 45.0, 32.98836328376913], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3555600.0000, 
sim time next is 3556800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 24.22034828243696, 0.2006888354780345, 0.0, 1.0, 45.0, 32.99706251900232], 
processed observation next is [0.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5183623568697465, 0.5668962784926782, 0.0, 1.0, 0.6, 0.3299706251900232], 
reward next is 0.6700, 
noisyNet noise sample is [array([0.24060535], dtype=float32), 0.7109676]. 
=============================================
[2019-04-09 15:22:57,556] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2392624e-15 6.4997799e-11 1.6493111e-08 1.6100584e-14 2.0462599e-02
 2.3074688e-06 9.7953218e-01 1.6438362e-09 2.8734473e-06 2.5353923e-08
 2.4576136e-10], sum to 1.0000
[2019-04-09 15:22:57,556] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7615
[2019-04-09 15:22:57,585] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 67.0, 0.0, 0.0, 19.0, 25.96276403354886, 0.6329354909043295, 0.0, 1.0, 45.0, 32.0977682930699], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3788400.0000, 
sim time next is 3789600.0000, 
raw observation next is [-2.666666666666667, 69.0, 0.0, 0.0, 19.0, 25.84952916018071, 0.6129529054360551, 0.0, 1.0, 45.0, 32.23866261024791], 
processed observation next is [1.0, 0.8695652173913043, 0.38873499538319484, 0.69, 0.0, 0.0, 0.08333333333333333, 0.654127430015059, 0.7043176351453516, 0.0, 1.0, 0.6, 0.3223866261024791], 
reward next is 0.6776, 
noisyNet noise sample is [array([-0.39546138], dtype=float32), 0.6055871]. 
=============================================
[2019-04-09 15:22:57,608] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0201196e-09 1.3763461e-06 6.9564380e-06 2.5739300e-09 2.7284330e-01
 1.2771485e-03 7.2579622e-01 7.5871189e-06 3.4505723e-05 3.2466203e-05
 4.7907866e-07], sum to 1.0000
[2019-04-09 15:22:57,609] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5986
[2019-04-09 15:22:57,635] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.7929676e-11 4.4178645e-08 4.9084415e-06 2.8815966e-10 1.4839630e-02
 2.4306291e-04 9.8486078e-01 1.2309843e-06 4.6309007e-05 3.8231360e-06
 1.8830069e-07], sum to 1.0000
[2019-04-09 15:22:57,635] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1947
[2019-04-09 15:22:57,647] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.2, 27.0, 0.0, 0.0, 19.0, 24.40848735711885, 0.1251010409263874, 0.0, 1.0, 45.0, 28.746867076501466], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3639600.0000, 
sim time next is 3640800.0000, 
raw observation next is [8.133333333333333, 27.66666666666667, 0.0, 0.0, 19.0, 24.48187135541223, 0.137799604022284, 0.0, 1.0, 45.0, 28.440835054460184], 
processed observation next is [0.0, 0.13043478260869565, 0.687903970452447, 0.2766666666666667, 0.0, 0.0, 0.08333333333333333, 0.5401559462843526, 0.5459332013407613, 0.0, 1.0, 0.6, 0.28440835054460184], 
reward next is 0.7156, 
noisyNet noise sample is [array([0.6919624], dtype=float32), -0.53444254]. 
=============================================
[2019-04-09 15:22:57,667] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 19.0, 23.95513370269842, 0.08500672609580424, 0.0, 1.0, 45.0, 29.767379405570388], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3628800.0000, 
sim time next is 3630000.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 19.0, 24.11183424746892, 0.1014348670270941, 0.0, 1.0, 45.0, 29.46278081482793], 
processed observation next is [0.0, 0.0, 0.7119113573407203, 0.25, 0.0, 0.0, 0.08333333333333333, 0.50931952062241, 0.5338116223423647, 0.0, 1.0, 0.6, 0.2946278081482793], 
reward next is 0.7054, 
noisyNet noise sample is [array([-0.5143426], dtype=float32), -0.15321101]. 
=============================================
[2019-04-09 15:22:57,675] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[44.616127]
 [45.34127 ]
 [45.228447]
 [45.18112 ]
 [45.03394 ]], R is [[43.99444199]
 [44.25682449]
 [44.51442719]
 [44.76945114]
 [45.02436829]].
[2019-04-09 15:22:57,764] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.3998364e-09 5.1764357e-07 1.7138698e-05 2.3147073e-09 5.6676799e-01
 1.1784378e-03 4.3186694e-01 2.2096470e-05 8.5342806e-05 5.7825073e-05
 3.6768176e-06], sum to 1.0000
[2019-04-09 15:22:57,770] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0884
[2019-04-09 15:22:57,789] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.333333333333334, 28.33333333333334, 0.0, 0.0, 19.0, 24.3980889313485, 0.1027223013774768, 0.0, 1.0, 35.0, 19.766515686651353], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3644400.0000, 
sim time next is 3645600.0000, 
raw observation next is [8.666666666666668, 27.66666666666667, 0.0, 0.0, 19.0, 24.31128096999861, 0.08028231618551847, 0.0, 1.0, 35.0, 17.934022049799452], 
processed observation next is [0.0, 0.17391304347826086, 0.7026777469990768, 0.2766666666666667, 0.0, 0.0, 0.08333333333333333, 0.5259400808332174, 0.5267607720618395, 0.0, 1.0, 0.4, 0.17934022049799453], 
reward next is 0.8207, 
noisyNet noise sample is [array([-0.8286361], dtype=float32), -0.9644857]. 
=============================================
[2019-04-09 15:22:57,984] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.0894441e-18 7.5932553e-14 1.2083358e-10 1.6395291e-16 2.9614484e-03
 7.5615688e-08 9.9703848e-01 1.1267821e-11 9.3283636e-10 7.7571660e-10
 2.9673152e-12], sum to 1.0000
[2019-04-09 15:22:57,987] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4790
[2019-04-09 15:22:58,010] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.0, 83.16666666666666, 682.3333333333333, 22.5, 27.20978977893753, 0.8627737079137229, 1.0, 1.0, 45.0, 27.14896497050782], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3771600.0000, 
sim time next is 3772800.0000, 
raw observation next is [0.0, 60.0, 75.5, 625.0, 22.5, 27.50583002295592, 0.9107454183618456, 1.0, 1.0, 45.0, 26.47536502943569], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6, 0.25166666666666665, 0.6906077348066298, 0.375, 0.7921525019129932, 0.8035818061206151, 1.0, 1.0, 0.6, 0.2647536502943569], 
reward next is 0.7352, 
noisyNet noise sample is [array([-0.30259812], dtype=float32), -1.7312288]. 
=============================================
[2019-04-09 15:22:58,062] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8083746e-09 3.6519253e-07 2.1722974e-05 1.2320659e-09 1.4040504e-01
 3.1901486e-03 8.5631216e-01 1.0755771e-06 5.8241963e-05 1.0900084e-05
 3.2739331e-07], sum to 1.0000
[2019-04-09 15:22:58,062] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8377
[2019-04-09 15:22:58,085] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 19.0, 24.17329670150935, 0.08007388481658219, 0.0, 1.0, 45.0, 29.443445789403995], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3636000.0000, 
sim time next is 3637200.0000, 
raw observation next is [8.733333333333334, 25.66666666666667, 0.0, 0.0, 19.0, 24.20828128794585, 0.08196102718568297, 0.0, 1.0, 45.0, 29.22654712016042], 
processed observation next is [0.0, 0.08695652173913043, 0.7045244690674055, 0.2566666666666667, 0.0, 0.0, 0.08333333333333333, 0.5173567739954876, 0.5273203423952276, 0.0, 1.0, 0.6, 0.2922654712016042], 
reward next is 0.7077, 
noisyNet noise sample is [array([0.30084467], dtype=float32), 0.9028297]. 
=============================================
[2019-04-09 15:22:58,281] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4085593e-09 1.3055205e-06 1.9857522e-05 1.4813745e-08 3.9966351e-01
 8.2348601e-04 5.9919089e-01 7.4993309e-06 2.4135073e-04 5.1029598e-05
 1.0509478e-06], sum to 1.0000
[2019-04-09 15:22:58,286] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0047
[2019-04-09 15:22:58,344] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.333333333333334, 28.33333333333334, 0.0, 0.0, 19.0, 24.22844877240777, 0.07897682911526975, 0.0, 1.0, 45.0, 29.00258931035783], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3644400.0000, 
sim time next is 3645600.0000, 
raw observation next is [8.666666666666668, 27.66666666666667, 0.0, 0.0, 19.0, 24.22452971016549, 0.06979001149723747, 0.0, 1.0, 35.0, 22.31727929866733], 
processed observation next is [0.0, 0.17391304347826086, 0.7026777469990768, 0.2766666666666667, 0.0, 0.0, 0.08333333333333333, 0.5187108091804576, 0.5232633371657458, 0.0, 1.0, 0.4, 0.2231727929866733], 
reward next is 0.7768, 
noisyNet noise sample is [array([-1.3095824], dtype=float32), 0.31615838]. 
=============================================
[2019-04-09 15:22:58,347] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3303300e-09 1.4724097e-06 2.2188508e-05 1.7357948e-08 4.0039164e-01
 9.0470765e-04 5.9835154e-01 8.3883024e-06 2.6380672e-04 5.5054632e-05
 1.2028213e-06], sum to 1.0000
[2019-04-09 15:22:58,348] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1447
[2019-04-09 15:22:58,387] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.666666666666668, 27.66666666666667, 0.0, 0.0, 19.0, 24.22452971016549, 0.06979001149723747, 0.0, 1.0, 35.0, 22.31727929866733], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3645600.0000, 
sim time next is 3646800.0000, 
raw observation next is [9.0, 27.0, 0.0, 0.0, 19.0, 24.21160459190216, 0.06016016987930259, 0.0, 1.0, 35.0, 19.845337218154327], 
processed observation next is [0.0, 0.21739130434782608, 0.7119113573407203, 0.27, 0.0, 0.0, 0.08333333333333333, 0.5176337159918466, 0.5200533899597676, 0.0, 1.0, 0.4, 0.19845337218154327], 
reward next is 0.8015, 
noisyNet noise sample is [array([-1.3095824], dtype=float32), 0.31615838]. 
=============================================
[2019-04-09 15:22:58,608] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.0188247e-12 4.0187684e-09 1.7756328e-06 2.0090402e-11 2.4637266e-01
 1.1596694e-03 7.5245941e-01 2.5219737e-07 3.8177845e-06 2.3139853e-06
 9.2641628e-08], sum to 1.0000
[2019-04-09 15:22:58,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1653
[2019-04-09 15:22:58,634] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.333333333333333, 54.66666666666667, 114.6666666666667, 817.1666666666666, 19.0, 23.91592974841354, 0.1896876041100243, 0.0, 1.0, 45.0, 29.445163850221345], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3584400.0000, 
sim time next is 3585600.0000, 
raw observation next is [-3.0, 55.0, 116.0, 819.5, 19.0, 24.03657205601573, 0.1929556540455787, 0.0, 1.0, 35.0, 23.08031834417359], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.55, 0.38666666666666666, 0.905524861878453, 0.08333333333333333, 0.503047671334644, 0.5643185513485263, 0.0, 1.0, 0.4, 0.23080318344173592], 
reward next is 0.7692, 
noisyNet noise sample is [array([-0.1800735], dtype=float32), 0.06807191]. 
=============================================
[2019-04-09 15:22:59,197] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2628204e-11 2.0096492e-08 2.1272342e-07 3.9896669e-11 7.9222366e-02
 7.2672461e-05 9.2069918e-01 1.4279682e-07 1.7065533e-06 3.6099907e-06
 1.0571970e-07], sum to 1.0000
[2019-04-09 15:22:59,198] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7179
[2019-04-09 15:22:59,213] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666666, 38.0, 116.1666666666667, 818.1666666666666, 19.0, 25.54015519969714, 0.4532842355691176, 0.0, 1.0, 45.0, 22.8976920612726], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3670800.0000, 
sim time next is 3672000.0000, 
raw observation next is [4.0, 45.0, 116.5, 822.5, 19.0, 25.52086805916307, 0.4529782219199303, 0.0, 1.0, 45.0, 22.97938674313977], 
processed observation next is [0.0, 0.5217391304347826, 0.5734072022160666, 0.45, 0.3883333333333333, 0.9088397790055248, 0.08333333333333333, 0.6267390049302559, 0.6509927406399768, 0.0, 1.0, 0.6, 0.2297938674313977], 
reward next is 0.7702, 
noisyNet noise sample is [array([-0.01159379], dtype=float32), -2.1029267]. 
=============================================
[2019-04-09 15:22:59,221] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[49.195866]
 [48.557693]
 [47.787453]
 [46.67503 ]
 [45.71887 ]], R is [[50.00381088]
 [50.27479553]
 [50.54340363]
 [50.80495834]
 [51.06035233]].
[2019-04-09 15:22:59,235] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.4173705e-12 1.0600759e-08 7.9139375e-07 8.7250530e-13 5.5811848e-02
 1.2537495e-04 9.4405878e-01 3.0706525e-07 6.3135502e-07 2.3051318e-06
 6.1887602e-09], sum to 1.0000
[2019-04-09 15:22:59,236] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1681
[2019-04-09 15:22:59,245] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0145025e-11 4.9112510e-09 6.6450752e-07 1.7766537e-11 2.8389255e-02
 2.8724925e-04 9.7131300e-01 1.0907612e-07 9.3961544e-06 1.7132955e-07
 6.5997824e-08], sum to 1.0000
[2019-04-09 15:22:59,249] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3544
[2019-04-09 15:22:59,261] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 24.38756529421202, 0.2509484276094584, 0.0, 1.0, 45.0, 35.294097185792836], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3805200.0000, 
sim time next is 3806400.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 24.37001574339619, 0.2484449992440414, 0.0, 1.0, 45.0, 35.511982932553], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.5308346452830159, 0.5828149997480138, 0.0, 1.0, 0.6, 0.35511982932553005], 
reward next is 0.6449, 
noisyNet noise sample is [array([-0.70071775], dtype=float32), 0.4969505]. 
=============================================
[2019-04-09 15:22:59,295] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 39.0, 38.5, 328.5, 19.0, 25.13482390320942, 0.3943287735042929, 0.0, 1.0, 45.0, 26.5035243304401], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3603600.0000, 
sim time next is 3604800.0000, 
raw observation next is [-0.3333333333333333, 40.0, 22.16666666666666, 204.1666666666667, 19.0, 25.10977721345791, 0.3759288781061988, 0.0, 1.0, 45.0, 26.441799132134143], 
processed observation next is [0.0, 0.7391304347826086, 0.4533702677747, 0.4, 0.07388888888888887, 0.22559852670349914, 0.08333333333333333, 0.5924814344548258, 0.6253096260353996, 0.0, 1.0, 0.6, 0.2644179913213414], 
reward next is 0.7356, 
noisyNet noise sample is [array([-0.27350083], dtype=float32), 0.65256494]. 
=============================================
[2019-04-09 15:22:59,642] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0696869e-11 4.3939528e-09 1.5784800e-06 5.4573953e-11 4.3565866e-01
 3.0262481e-05 5.6427664e-01 2.5952721e-07 2.9622521e-05 2.9759497e-06
 2.0921552e-08], sum to 1.0000
[2019-04-09 15:22:59,642] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2179
[2019-04-09 15:22:59,663] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 24.71960558422326, 0.253627285086576, 0.0, 1.0, 35.0, 24.70145932060744], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3715200.0000, 
sim time next is 3716400.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 19.0, 24.60352252775006, 0.2173822720117526, 0.0, 1.0, 35.0, 22.129759945597275], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5502935439791715, 0.5724607573372509, 0.0, 1.0, 0.4, 0.22129759945597274], 
reward next is 0.7787, 
noisyNet noise sample is [array([0.02899157], dtype=float32), -0.0110879615]. 
=============================================
[2019-04-09 15:22:59,800] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.52658419e-10 2.08731308e-07 2.36376650e-06 1.21015239e-10
 7.09031343e-01 5.44423820e-04 2.90404797e-01 3.30556617e-07
 1.45823315e-05 1.72693910e-06 1.69193498e-07], sum to 1.0000
[2019-04-09 15:22:59,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8656
[2019-04-09 15:22:59,838] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 24.45453909475044, 0.2037057915385746, 0.0, 1.0, 45.0, 28.16393179524778], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3614400.0000, 
sim time next is 3615600.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 24.40715748378754, 0.1930508979837306, 0.0, 1.0, 45.0, 28.195255884942526], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.5339297903156283, 0.5643502993279103, 0.0, 1.0, 0.6, 0.28195255884942527], 
reward next is 0.7180, 
noisyNet noise sample is [array([2.0511222], dtype=float32), 2.2796414]. 
=============================================
[2019-04-09 15:23:00,935] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5552745e-10 3.8456289e-08 2.9656783e-06 1.1579698e-10 1.2149365e-02
 1.3898350e-03 9.8643076e-01 7.1720700e-07 2.0360418e-05 5.8546361e-06
 8.5261092e-08], sum to 1.0000
[2019-04-09 15:23:00,936] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2925
[2019-04-09 15:23:00,952] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.0, 30.0, 74.83333333333334, 356.0000000000001, 19.0, 24.2987064644286, 0.1388955579968894, 0.0, 1.0, 45.0, 27.234770787376753], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3658800.0000, 
sim time next is 3660000.0000, 
raw observation next is [10.0, 28.0, 91.0, 446.3333333333334, 19.0, 24.38734812306479, 0.1711665752197798, 0.0, 1.0, 45.0, 26.662044147729464], 
processed observation next is [0.0, 0.34782608695652173, 0.739612188365651, 0.28, 0.30333333333333334, 0.49318600368324134, 0.08333333333333333, 0.5322790102553991, 0.55705552507326, 0.0, 1.0, 0.6, 0.26662044147729463], 
reward next is 0.7334, 
noisyNet noise sample is [array([0.6611111], dtype=float32), 0.8396173]. 
=============================================
[2019-04-09 15:23:00,957] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[43.77491 ]
 [43.12874 ]
 [42.859325]
 [42.656506]
 [42.395443]], R is [[44.63447571]
 [44.91578674]
 [45.18994904]
 [45.45608521]
 [45.72487259]].
[2019-04-09 15:23:01,365] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7857785e-10 1.3414553e-08 1.2507829e-06 1.5841524e-11 4.9118432e-01
 1.0785823e-03 5.0769287e-01 1.5580763e-07 4.0481507e-05 1.7104102e-06
 5.9252534e-07], sum to 1.0000
[2019-04-09 15:23:01,372] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7666
[2019-04-09 15:23:01,372] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5796747e-12 1.9428924e-08 1.4068777e-07 2.0015174e-12 1.3066643e-01
 4.2963246e-04 8.6888564e-01 1.2657512e-07 1.7607117e-05 3.9453667e-07
 1.6519396e-08], sum to 1.0000
[2019-04-09 15:23:01,375] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2779
[2019-04-09 15:23:01,397] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 47.0, 95.5, 740.5, 19.0, 26.23580976915119, 0.6474471364003928, 0.0, 1.0, 45.0, 24.511550067426626], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3682800.0000, 
sim time next is 3684000.0000, 
raw observation next is [5.666666666666666, 48.0, 89.83333333333333, 716.8333333333333, 19.0, 26.33442531165688, 0.6724392156164004, 0.0, 1.0, 45.0, 24.615097190788564], 
processed observation next is [0.0, 0.6521739130434783, 0.6195752539242845, 0.48, 0.2994444444444444, 0.7920810313075506, 0.08333333333333333, 0.6945354426380733, 0.7241464052054668, 0.0, 1.0, 0.6, 0.24615097190788565], 
reward next is 0.7538, 
noisyNet noise sample is [array([-0.15268761], dtype=float32), -0.13078941]. 
=============================================
[2019-04-09 15:23:01,404] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[55.851345]
 [55.608864]
 [55.35556 ]
 [55.044716]
 [54.763298]], R is [[56.23031235]
 [56.42289352]
 [56.61495209]
 [56.80524826]
 [56.99347687]].
[2019-04-09 15:23:01,409] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.33333333333333, 26.66666666666667, 109.3333333333333, 746.3333333333334, 19.0, 25.21820012579046, 0.3837287864617697, 0.0, 1.0, 45.0, 23.774468124130884], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3666000.0000, 
sim time next is 3667200.0000, 
raw observation next is [11.66666666666667, 25.33333333333334, 111.8333333333333, 771.8333333333334, 19.0, 25.35598077304652, 0.421327039042057, 0.0, 1.0, 45.0, 23.39819766136452], 
processed observation next is [0.0, 0.43478260869565216, 0.785780240073869, 0.2533333333333334, 0.37277777777777765, 0.8528545119705341, 0.08333333333333333, 0.6129983977538768, 0.6404423463473523, 0.0, 1.0, 0.6, 0.2339819766136452], 
reward next is 0.7660, 
noisyNet noise sample is [array([0.32714286], dtype=float32), -0.6480425]. 
=============================================
[2019-04-09 15:23:01,479] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1770202e-13 8.4889359e-11 6.5668615e-08 9.7866144e-15 1.9139415e-02
 1.3820525e-05 9.8083979e-01 1.4264817e-08 6.8703330e-06 3.7604515e-08
 2.2471680e-09], sum to 1.0000
[2019-04-09 15:23:01,481] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9657
[2019-04-09 15:23:01,521] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 73.0, 0.0, 0.0, 19.0, 24.9913571397648, 0.3910733253822975, 0.0, 1.0, 45.0, 33.10899619827033], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3796800.0000, 
sim time next is 3798000.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 19.0, 24.83715405047775, 0.361079109940371, 0.0, 1.0, 45.0, 33.48481628037494], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5697628375398125, 0.620359703313457, 0.0, 1.0, 0.6, 0.3348481628037494], 
reward next is 0.6652, 
noisyNet noise sample is [array([1.813754], dtype=float32), -0.17512001]. 
=============================================
[2019-04-09 15:23:01,529] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[57.35467 ]
 [58.141815]
 [59.299488]
 [59.73644 ]
 [60.49556 ]], R is [[57.46633148]
 [57.56058121]
 [57.65180969]
 [57.74836731]
 [57.84385681]].
[2019-04-09 15:23:01,962] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.8628528e-16 6.6069925e-13 1.9698689e-10 3.4823655e-16 1.0550750e-03
 3.7022471e-06 9.9894124e-01 4.0881003e-11 3.7013834e-09 1.9576844e-10
 4.9366865e-11], sum to 1.0000
[2019-04-09 15:23:01,962] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4809
[2019-04-09 15:23:02,001] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 51.00000000000001, 0.0, 0.0, 22.5, 26.52768752734237, 0.7193740173427582, 1.0, 1.0, 45.0, 27.76897737294373], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3871200.0000, 
sim time next is 3872400.0000, 
raw observation next is [1.0, 51.0, 0.0, 0.0, 22.5, 26.40954640694609, 0.6972821151953567, 0.0, 1.0, 45.0, 28.0812719348401], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.51, 0.0, 0.0, 0.375, 0.7007955339121743, 0.7324273717317856, 0.0, 1.0, 0.6, 0.280812719348401], 
reward next is 0.7192, 
noisyNet noise sample is [array([-0.23491465], dtype=float32), -0.24540001]. 
=============================================
[2019-04-09 15:23:02,127] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.0020795e-13 6.9918393e-10 1.0479562e-07 2.6751448e-12 5.3123143e-02
 8.3412902e-05 9.4679260e-01 1.1277694e-08 4.9911159e-07 3.0846238e-07
 1.4323958e-09], sum to 1.0000
[2019-04-09 15:23:02,128] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1190
[2019-04-09 15:23:02,158] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 59.0, 23.16666666666666, 224.5, 19.0, 26.39352068527494, 0.6400478572579039, 0.0, 1.0, 45.0, 25.605413444988105], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3691200.0000, 
sim time next is 3692400.0000, 
raw observation next is [4.0, 59.0, 12.5, 137.5, 19.0, 26.33248175590896, 0.6192326200654424, 0.0, 1.0, 45.0, 25.949985902179606], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.041666666666666664, 0.15193370165745856, 0.08333333333333333, 0.6943734796590801, 0.7064108733551474, 0.0, 1.0, 0.6, 0.25949985902179606], 
reward next is 0.7405, 
noisyNet noise sample is [array([-0.3306879], dtype=float32), 0.84665453]. 
=============================================
[2019-04-09 15:23:02,693] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.5386908e-13 6.9548172e-09 3.1996569e-07 3.4565319e-12 3.3469182e-03
 9.2041128e-06 9.9664235e-01 7.5482205e-08 9.5179831e-07 7.1055730e-08
 3.3968983e-09], sum to 1.0000
[2019-04-09 15:23:02,694] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9927
[2019-04-09 15:23:02,719] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 24.86349865689468, 0.2728317239273245, 0.0, 1.0, 45.0, 31.59465886939781], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3718800.0000, 
sim time next is 3720000.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 19.0, 24.83323696854236, 0.266780685377587, 0.0, 1.0, 45.0, 31.862983877463886], 
processed observation next is [1.0, 0.043478260869565216, 0.3795013850415513, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5694364140451965, 0.5889268951258623, 0.0, 1.0, 0.6, 0.31862983877463885], 
reward next is 0.6814, 
noisyNet noise sample is [array([0.04072213], dtype=float32), -0.5300159]. 
=============================================
[2019-04-09 15:23:02,758] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[50.78496 ]
 [50.32146 ]
 [50.579067]
 [51.341404]
 [51.244816]], R is [[50.99889755]
 [51.172966  ]
 [51.34271622]
 [51.51367188]
 [51.68349838]].
[2019-04-09 15:23:02,776] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7216818e-11 4.3361124e-09 8.0398075e-07 3.1360017e-12 2.4154784e-01
 1.2677509e-04 7.5831223e-01 2.9238686e-07 8.3216291e-06 3.6088315e-06
 6.6459862e-08], sum to 1.0000
[2019-04-09 15:23:02,777] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1199
[2019-04-09 15:23:02,817] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 77.0, 48.0, 298.0, 22.5, 23.0398415728566, -0.06576734751150555, 1.0, 1.0, 45.0, 35.18620647291421], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3830400.0000, 
sim time next is 3831600.0000, 
raw observation next is [-4.666666666666667, 75.0, 76.66666666666667, 397.3333333333333, 22.5, 23.0897467415291, -0.00177119110136686, 1.0, 1.0, 35.0, 28.056906936557063], 
processed observation next is [1.0, 0.34782608695652173, 0.3333333333333333, 0.75, 0.2555555555555556, 0.43904235727440144, 0.375, 0.42414556179409174, 0.499409602966211, 1.0, 1.0, 0.4, 0.28056906936557063], 
reward next is 0.7194, 
noisyNet noise sample is [array([0.8079118], dtype=float32), 1.5853057]. 
=============================================
[2019-04-09 15:23:02,960] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.1125259e-11 1.0707355e-09 5.8337457e-07 6.9464759e-12 2.3178026e-01
 1.0670695e-04 7.6810879e-01 4.2844295e-07 2.8932409e-06 3.5571642e-07
 2.5828831e-08], sum to 1.0000
[2019-04-09 15:23:02,960] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0312
[2019-04-09 15:23:02,994] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 73.0, 19.0, 184.8333333333333, 22.5, 23.6870170840802, 0.02552178057093215, 1.0, 1.0, 45.0, 35.05109930874981], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3742800.0000, 
sim time next is 3744000.0000, 
raw observation next is [-4.0, 71.0, 47.0, 282.5, 22.5, 23.79667462534229, 0.02630094761701088, 0.0, 1.0, 35.0, 26.76385235276484], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.71, 0.15666666666666668, 0.31215469613259667, 0.375, 0.4830562187785243, 0.5087669825390037, 0.0, 1.0, 0.4, 0.2676385235276484], 
reward next is 0.7324, 
noisyNet noise sample is [array([0.70621675], dtype=float32), -1.6759051]. 
=============================================
[2019-04-09 15:23:03,000] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[53.977974]
 [53.274666]
 [52.389122]
 [51.910984]
 [52.33757 ]], R is [[54.6412468 ]
 [54.74432373]
 [54.84443665]
 [54.95157623]
 [55.12330627]].
[2019-04-09 15:23:03,047] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1499977e-11 3.1094554e-08 2.9039616e-06 5.7343599e-11 4.8910308e-01
 6.4385287e-04 5.1023728e-01 6.1963021e-07 9.1433831e-06 2.9386242e-06
 1.2972112e-07], sum to 1.0000
[2019-04-09 15:23:03,048] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8224
[2019-04-09 15:23:03,071] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 24.85455350939682, 0.2868216929994271, 0.0, 1.0, 35.0, 24.629517704101488], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3715200.0000, 
sim time next is 3716400.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 19.0, 24.74872685972611, 0.2684719867650961, 0.0, 1.0, 45.0, 31.29864292814725], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5623939049771757, 0.5894906622550321, 0.0, 1.0, 0.6, 0.31298642928147247], 
reward next is 0.6870, 
noisyNet noise sample is [array([1.6451145], dtype=float32), 0.6435837]. 
=============================================
[2019-04-09 15:23:03,523] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.85622437e-11 3.25818412e-08 1.39321662e-06 7.74223567e-12
 3.32042463e-02 1.04536055e-04 9.66638327e-01 1.90831813e-07
 5.04314921e-05 7.37634878e-07 7.42432178e-08], sum to 1.0000
[2019-04-09 15:23:03,523] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8104
[2019-04-09 15:23:03,563] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 24.32232560335181, 0.1507672424108011, 0.0, 1.0, 45.0, 33.29762539580066], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3728400.0000, 
sim time next is 3729600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 24.28638159793716, 0.1422246157142802, 0.0, 1.0, 45.0, 33.36736105927962], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.5238651331614301, 0.5474082052380934, 0.0, 1.0, 0.6, 0.33367361059279615], 
reward next is 0.6663, 
noisyNet noise sample is [array([-1.7224863], dtype=float32), -1.4552474]. 
=============================================
[2019-04-09 15:23:03,570] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6649427e-11 3.0708588e-08 1.3138955e-06 7.2049398e-12 3.2573361e-02
 9.9577788e-05 9.6727711e-01 1.8148094e-07 4.7756992e-05 7.1925598e-07
 6.9711845e-08], sum to 1.0000
[2019-04-09 15:23:03,572] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4530
[2019-04-09 15:23:03,591] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 24.28638159793716, 0.1422246157142802, 0.0, 1.0, 45.0, 33.36736105927962], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3729600.0000, 
sim time next is 3730800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 24.30075523771107, 0.1407239749123832, 0.0, 1.0, 45.0, 33.42668858701811], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.5250629364759224, 0.5469079916374611, 0.0, 1.0, 0.6, 0.3342668858701811], 
reward next is 0.6657, 
noisyNet noise sample is [array([-1.7224863], dtype=float32), -1.4552474]. 
=============================================
[2019-04-09 15:23:03,774] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2084348e-13 3.0463059e-11 2.7953185e-08 7.7827129e-14 2.7898155e-02
 1.0448382e-04 9.7199708e-01 3.3694367e-09 1.3825556e-07 3.9503405e-08
 1.4891589e-09], sum to 1.0000
[2019-04-09 15:23:03,775] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4227
[2019-04-09 15:23:03,811] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 73.0, 0.0, 0.0, 19.0, 25.11071538949269, 0.4171373027056245, 0.0, 1.0, 45.0, 32.89866690153282], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3796800.0000, 
sim time next is 3798000.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 19.0, 24.95037972465242, 0.3863593742611295, 0.0, 1.0, 45.0, 33.29756267020706], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.08333333333333333, 0.5791983103877018, 0.6287864580870431, 0.0, 1.0, 0.6, 0.3329756267020706], 
reward next is 0.6670, 
noisyNet noise sample is [array([-0.6272405], dtype=float32), -1.3378707]. 
=============================================
[2019-04-09 15:23:03,828] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[61.065323]
 [61.712936]
 [62.98484 ]
 [63.692806]
 [64.12879 ]], R is [[60.7835083 ]
 [60.84668732]
 [60.90748215]
 [60.97424316]
 [61.04085922]].
[2019-04-09 15:23:04,103] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.2815233e-15 3.4781750e-11 9.8570982e-09 6.8210791e-14 5.0071046e-02
 5.9787503e-06 9.4992286e-01 8.5496668e-09 8.1173468e-08 1.1870308e-08
 1.0205338e-09], sum to 1.0000
[2019-04-09 15:23:04,105] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5405
[2019-04-09 15:23:04,143] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 105.5, 727.5, 22.5, 25.38548617554829, 0.3817596400230756, 1.0, 1.0, 45.0, 31.057848805662495], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3837600.0000, 
sim time next is 3838800.0000, 
raw observation next is [-1.666666666666667, 60.00000000000001, 108.5, 759.1666666666667, 22.5, 25.61070183150144, 0.4360999641785457, 1.0, 1.0, 45.0, 30.326769749351172], 
processed observation next is [1.0, 0.43478260869565216, 0.4164358264081256, 0.6000000000000001, 0.3616666666666667, 0.8388581952117865, 0.375, 0.63422515262512, 0.6453666547261819, 1.0, 1.0, 0.6, 0.30326769749351173], 
reward next is 0.6967, 
noisyNet noise sample is [array([1.3382467], dtype=float32), -0.21408938]. 
=============================================
[2019-04-09 15:23:04,206] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.7631970e-11 1.7055829e-08 2.8081945e-07 3.1534543e-12 5.9176791e-01
 6.4450898e-04 4.0755886e-01 2.3747758e-07 2.5747482e-05 2.4517572e-06
 1.3717585e-08], sum to 1.0000
[2019-04-09 15:23:04,206] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6382
[2019-04-09 15:23:04,243] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 23.696070476353, 0.04065316399259484, 0.0, 1.0, 35.0, 20.536000872125285], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3814800.0000, 
sim time next is 3816000.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 23.49841447144728, 0.03975376952100552, 0.0, 1.0, 45.0, 37.02001272858227], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.45820120595394, 0.5132512565070019, 0.0, 1.0, 0.6, 0.37020012728582274], 
reward next is 0.6298, 
noisyNet noise sample is [array([2.158867], dtype=float32), -1.5950041]. 
=============================================
[2019-04-09 15:23:04,251] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[50.642315]
 [51.196026]
 [51.55067 ]
 [52.362747]
 [53.349045]], R is [[50.86161423]
 [51.14763641]
 [51.41274643]
 [51.65404892]
 [51.85206985]].
[2019-04-09 15:23:04,432] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7241021e-15 4.4772083e-13 2.1508697e-09 6.0784747e-16 4.3923068e-03
 5.7009829e-06 9.9560189e-01 1.0051222e-09 7.4685296e-08 4.7587880e-09
 1.9232405e-10], sum to 1.0000
[2019-04-09 15:23:04,432] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1824
[2019-04-09 15:23:04,465] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 22.5, 26.76968574523877, 0.7813372348949222, 1.0, 1.0, 45.0, 28.96638173560773], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3781200.0000, 
sim time next is 3782400.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 22.5, 26.85063783290158, 0.7790644078308743, 1.0, 1.0, 45.0, 30.28561785566005], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.375, 0.7375531527417983, 0.7596881359436248, 1.0, 1.0, 0.6, 0.3028561785566005], 
reward next is 0.6971, 
noisyNet noise sample is [array([-0.21392669], dtype=float32), -0.12912261]. 
=============================================
[2019-04-09 15:23:04,961] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.7183147e-17 2.6246925e-11 4.4140266e-10 2.4083360e-16 2.6543310e-01
 1.2245551e-05 7.3455453e-01 9.7653408e-10 1.4574272e-07 1.9523541e-09
 2.9052143e-11], sum to 1.0000
[2019-04-09 15:23:04,961] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6328
[2019-04-09 15:23:05,019] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.0, 83.16666666666666, 682.3333333333333, 22.5, 25.52989861588289, 0.7260278492719698, 1.0, 1.0, 45.0, 29.709287307844345], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3771600.0000, 
sim time next is 3772800.0000, 
raw observation next is [0.0, 60.0, 75.5, 625.0, 22.5, 27.20181356524498, 0.8706619632830955, 1.0, 1.0, 45.0, 27.10273228679878], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6, 0.25166666666666665, 0.6906077348066298, 0.375, 0.7668177971037483, 0.7902206544276985, 1.0, 1.0, 0.6, 0.2710273228679878], 
reward next is 0.7290, 
noisyNet noise sample is [array([1.130237], dtype=float32), 0.67356515]. 
=============================================
[2019-04-09 15:23:06,187] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.3994247e-11 6.2524430e-09 2.3146799e-07 6.4389340e-12 5.4080706e-02
 1.2363747e-04 9.4578791e-01 2.0370328e-07 6.5555460e-06 6.2672666e-07
 1.3881491e-07], sum to 1.0000
[2019-04-09 15:23:06,187] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4549
[2019-04-09 15:23:06,241] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 19.0, 23.56263321976063, -0.001499522673970498, 0.0, 1.0, 35.0, 27.125642483264556], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3912000.0000, 
sim time next is 3913200.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 23.35581997724399, -0.03295463458482576, 0.0, 1.0, 45.0, 34.77162454899739], 
processed observation next is [1.0, 0.30434782608695654, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.4463183314369991, 0.4890151218050581, 0.0, 1.0, 0.6, 0.3477162454899739], 
reward next is 0.6523, 
noisyNet noise sample is [array([-0.29896575], dtype=float32), -0.9618867]. 
=============================================
[2019-04-09 15:23:06,830] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1033095e-12 9.1934407e-09 4.6187797e-08 9.1818567e-13 1.3057804e-01
 1.4784854e-04 8.6926961e-01 2.0323416e-08 4.0824198e-06 3.7255336e-07
 1.1483793e-08], sum to 1.0000
[2019-04-09 15:23:06,830] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7885
[2019-04-09 15:23:06,872] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 23.83296611663546, 0.0940935716973031, 0.0, 1.0, 45.0, 35.655105722169196], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3816000.0000, 
sim time next is 3817200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 23.76876704936268, 0.07441764530370656, 0.0, 1.0, 45.0, 35.83223838741725], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.48073058744688996, 0.5248058817679022, 0.0, 1.0, 0.6, 0.3583223838741725], 
reward next is 0.6417, 
noisyNet noise sample is [array([1.455447], dtype=float32), -0.44364446]. 
=============================================
[2019-04-09 15:23:07,096] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7746717e-15 8.4383455e-13 3.9567896e-10 2.1932504e-15 2.2243600e-02
 1.9607183e-05 9.7773665e-01 6.4732464e-10 8.3469949e-08 9.1666594e-09
 1.0188893e-10], sum to 1.0000
[2019-04-09 15:23:07,099] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3499
[2019-04-09 15:23:07,132] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 46.33333333333334, 118.5, 833.1666666666667, 22.5, 25.77784362334157, 0.5302146409880301, 1.0, 1.0, 45.0, 28.14880298277589], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3933600.0000, 
sim time next is 3934800.0000, 
raw observation next is [-6.0, 45.0, 117.5, 829.5, 22.5, 25.90677223736051, 0.5663131608144633, 1.0, 1.0, 45.0, 28.28568463511489], 
processed observation next is [1.0, 0.5652173913043478, 0.296398891966759, 0.45, 0.39166666666666666, 0.9165745856353591, 0.375, 0.6588976864467092, 0.688771053604821, 1.0, 1.0, 0.6, 0.2828568463511489], 
reward next is 0.7171, 
noisyNet noise sample is [array([-0.10677662], dtype=float32), -0.47495964]. 
=============================================
[2019-04-09 15:23:07,615] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.9470163e-12 5.3468545e-09 4.1657555e-07 2.3542487e-11 2.7932141e-02
 3.2057034e-04 9.7173899e-01 1.1565989e-07 7.2410558e-06 4.6970462e-07
 2.9781663e-08], sum to 1.0000
[2019-04-09 15:23:07,615] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6801
[2019-04-09 15:23:07,643] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 67.0, 0.0, 0.0, 19.0, 24.64999840717098, 0.2467309671863899, 0.0, 1.0, 45.0, 31.348602899266705], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3900000.0000, 
sim time next is 3901200.0000, 
raw observation next is [-2.666666666666667, 69.0, 0.0, 0.0, 19.0, 24.58294640637288, 0.2277425245796147, 0.0, 1.0, 45.0, 31.650270318240473], 
processed observation next is [1.0, 0.13043478260869565, 0.38873499538319484, 0.69, 0.0, 0.0, 0.08333333333333333, 0.54857886719774, 0.5759141748598716, 0.0, 1.0, 0.6, 0.31650270318240475], 
reward next is 0.6835, 
noisyNet noise sample is [array([0.24141021], dtype=float32), 0.42744523]. 
=============================================
[2019-04-09 15:23:07,832] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-09 15:23:07,833] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:23:07,833] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:23:07,841] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:23:07,843] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:23:07,843] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:23:07,846] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:23:07,846] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run13
[2019-04-09 15:23:07,849] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run13
[2019-04-09 15:23:07,884] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run13
[2019-04-09 15:24:41,844] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation: average rewards by now are 2919.2300 128972.2748 916.1402
[2019-04-09 15:24:41,874] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:41,874] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:41,874] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:41,874] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:41,874] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:41,874] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:41,874] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:41,874] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:41,874] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:41,874] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:41,874] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:41,874] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:41,874] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:42,020] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:42,020] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:42,020] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:42,020] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:42,020] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:42,020] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:42,020] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:42,020] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:42,020] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:42,020] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:42,020] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:42,020] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:42,020] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:57,881] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation: average rewards by now are 2879.4150 136042.5300 622.4949
[2019-04-09 15:24:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:24:58,005] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:58,005] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:58,005] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:58,005] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:58,005] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:58,005] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:58,005] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:58,005] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:58,005] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:58,005] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:58,005] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:58,005] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:24:58,005] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:00,640] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation: average rewards by now are 2854.7577 138454.1795 392.4986
[2019-04-09 15:25:00,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:00,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:00,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:00,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:00,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:00,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:00,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:00,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:00,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:00,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:00,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:00,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:00,661] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-09 15:25:00,769] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:00,769] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:00,769] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:00,769] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:00,769] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:00,769] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:00,769] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:00,769] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:00,769] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:00,769] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:00,769] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:00,769] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:00,769] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-09 15:25:01,663] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 120000, evaluation results [120000.0, 2879.4150417356336, 136042.53004546618, 622.4948852202003, 2919.23002165996, 128972.27483610318, 916.1402188420643, 2854.75773821496, 138454.1794732223, 392.49861818212054]
[2019-04-09 15:25:01,898] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.05349094e-14 6.42060016e-11 4.86566432e-09 1.67402245e-15
 4.03550193e-02 1.18308366e-04 9.59526002e-01 1.70400649e-09
 2.30958776e-07 4.81780944e-07 1.96446956e-10], sum to 1.0000
[2019-04-09 15:25:01,908] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2087
[2019-04-09 15:25:01,957] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 22.5, 25.64111897511139, 0.5212471439203183, 1.0, 1.0, 45.0, 30.248925578747503], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3956400.0000, 
sim time next is 3957600.0000, 
raw observation next is [-6.333333333333333, 42.33333333333334, 0.0, 0.0, 22.5, 25.53507476860802, 0.497231300101987, 1.0, 1.0, 45.0, 30.67723786251694], 
processed observation next is [1.0, 0.8260869565217391, 0.28716528162511545, 0.42333333333333345, 0.0, 0.0, 0.375, 0.6279228973840016, 0.6657437667006624, 1.0, 1.0, 0.6, 0.3067723786251694], 
reward next is 0.6932, 
noisyNet noise sample is [array([-0.268782], dtype=float32), 1.0382981]. 
=============================================
[2019-04-09 15:25:02,539] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0607274e-13 1.7405936e-10 7.7530844e-09 1.5931759e-13 9.5137447e-02
 4.7479116e-05 9.0480739e-01 3.8930061e-08 7.4743625e-06 2.8381249e-08
 1.0953801e-09], sum to 1.0000
[2019-04-09 15:25:02,539] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4771
[2019-04-09 15:25:02,610] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 58.33333333333334, 0.0, 0.0, 19.0, 25.53548252951325, 0.4997784481241455, 0.0, 1.0, 45.0, 29.483198290905285], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3883200.0000, 
sim time next is 3884400.0000, 
raw observation next is [-1.0, 60.0, 0.0, 0.0, 19.0, 25.37230994683633, 0.4768873038828854, 0.0, 1.0, 45.0, 29.7048015098904], 
processed observation next is [1.0, 1.0, 0.4349030470914128, 0.6, 0.0, 0.0, 0.08333333333333333, 0.614359162236361, 0.6589624346276285, 0.0, 1.0, 0.6, 0.297048015098904], 
reward next is 0.7030, 
noisyNet noise sample is [array([0.704304], dtype=float32), -0.439351]. 
=============================================
[2019-04-09 15:25:03,440] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0813702e-12 4.5870747e-09 2.3402304e-06 5.8886979e-12 7.4324203e-01
 8.0564590e-05 2.5665656e-01 1.4896030e-07 1.7659657e-05 6.7130492e-07
 1.9232941e-08], sum to 1.0000
[2019-04-09 15:25:03,442] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1879
[2019-04-09 15:25:03,454] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1983951e-14 1.1004540e-09 1.3938025e-07 9.8261568e-13 2.3815690e-03
 9.2237433e-06 9.9760854e-01 1.3578979e-08 3.6932005e-07 4.7120107e-08
 2.1218522e-09], sum to 1.0000
[2019-04-09 15:25:03,455] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8765
[2019-04-09 15:25:03,493] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.333333333333334, 65.0, 0.0, 0.0, 19.0, 23.9989899992503, 0.1110617019237251, 0.0, 1.0, 45.0, 33.21851094008332], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3908400.0000, 
sim time next is 3909600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 23.80971139867884, 0.07593892090349937, 0.0, 1.0, 35.0, 26.299603936785807], 
processed observation next is [1.0, 0.2608695652173913, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.4841426165565699, 0.5253129736344998, 0.0, 1.0, 0.4, 0.2629960393678581], 
reward next is 0.7370, 
noisyNet noise sample is [array([0.44324413], dtype=float32), 0.8277847]. 
=============================================
[2019-04-09 15:25:03,498] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 63.33333333333333, 0.0, 0.0, 19.0, 25.09191990235585, 0.3944989752925097, 0.0, 1.0, 45.0, 30.013424237795228], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3886800.0000, 
sim time next is 3888000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 24.97617187966861, 0.370482312392914, 0.0, 1.0, 45.0, 30.125753885248905], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.5813476566390507, 0.6234941041309713, 0.0, 1.0, 0.6, 0.30125753885248907], 
reward next is 0.6987, 
noisyNet noise sample is [array([0.04150868], dtype=float32), -0.31240508]. 
=============================================
[2019-04-09 15:25:03,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7721324e-12 5.7834280e-09 2.7786518e-06 7.9793446e-12 7.3238778e-01
 8.9032132e-05 2.6749957e-01 1.7770442e-07 1.9839457e-05 7.7709728e-07
 2.2948322e-08], sum to 1.0000
[2019-04-09 15:25:03,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0362
[2019-04-09 15:25:03,533] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[60.444656]
 [60.470592]
 [60.76532 ]
 [60.96989 ]
 [61.58397 ]], R is [[59.59555435]
 [59.69946671]
 [59.80365753]
 [59.90789413]
 [60.01330566]].
[2019-04-09 15:25:03,541] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 23.80971139867884, 0.07593892090349937, 0.0, 1.0, 35.0, 26.299603936785807], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3909600.0000, 
sim time next is 3910800.0000, 
raw observation next is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 19.0, 23.7352214801047, 0.04788741803287999, 0.0, 1.0, 35.0, 23.65332075194418], 
processed observation next is [1.0, 0.2608695652173913, 0.28716528162511545, 0.6066666666666667, 0.0, 0.0, 0.08333333333333333, 0.4779351233420582, 0.5159624726776267, 0.0, 1.0, 0.4, 0.2365332075194418], 
reward next is 0.7635, 
noisyNet noise sample is [array([0.44324413], dtype=float32), 0.8277847]. 
=============================================
[2019-04-09 15:25:03,694] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4423957e-12 5.0405985e-10 1.9251318e-07 7.9801557e-13 1.0458640e-02
 9.7381948e-05 9.8944283e-01 4.4897867e-08 8.2611052e-07 5.2320896e-08
 1.4881870e-09], sum to 1.0000
[2019-04-09 15:25:03,698] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9354
[2019-04-09 15:25:03,749] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 24.75778679585478, 0.3114444110906932, 0.0, 1.0, 45.0, 30.602618196697815], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3892800.0000, 
sim time next is 3894000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 24.68168428327402, 0.2869073173415032, 0.0, 1.0, 45.0, 30.692191114571557], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.5568070236061683, 0.5956357724471677, 0.0, 1.0, 0.6, 0.30692191114571554], 
reward next is 0.6931, 
noisyNet noise sample is [array([1.9504354], dtype=float32), 0.101696156]. 
=============================================
[2019-04-09 15:25:03,774] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[56.63846 ]
 [57.021915]
 [57.62532 ]
 [58.776993]
 [60.02949 ]], R is [[56.07558441]
 [56.20880127]
 [56.34149933]
 [56.47429276]
 [56.60712433]].
[2019-04-09 15:25:03,841] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.91541362e-15 3.85806213e-11 1.10823235e-08 4.83655162e-13
 1.75375283e-01 2.00129522e-04 8.24423552e-01 2.87912894e-08
 7.76048921e-07 1.96939141e-07 1.70298831e-09], sum to 1.0000
[2019-04-09 15:25:03,842] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6718
[2019-04-09 15:25:03,891] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 29.0, 0.0, 0.0, 19.0, 26.56284863776016, 0.672879387588415, 0.0, 1.0, 45.0, 29.247810082251767], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4050000.0000, 
sim time next is 4051200.0000, 
raw observation next is [-4.333333333333334, 29.66666666666667, 0.0, 0.0, 19.0, 26.43284760421993, 0.6493393384891414, 0.0, 1.0, 45.0, 29.272844071769818], 
processed observation next is [1.0, 0.9130434782608695, 0.3425669436749769, 0.2966666666666667, 0.0, 0.0, 0.08333333333333333, 0.7027373003516608, 0.7164464461630472, 0.0, 1.0, 0.6, 0.29272844071769816], 
reward next is 0.7073, 
noisyNet noise sample is [array([-0.13416187], dtype=float32), 0.90153754]. 
=============================================
[2019-04-09 15:25:04,177] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.75078961e-12 6.39123199e-09 8.00705280e-08 1.26651597e-11
 1.41853988e-02 1.96027413e-05 9.85793650e-01 4.08050944e-08
 1.07877395e-06 1.10107166e-07 1.64067568e-08], sum to 1.0000
[2019-04-09 15:25:04,178] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1492
[2019-04-09 15:25:04,209] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 75.0, 0.0, 0.0, 19.0, 24.35043442058435, 0.1692163393826863, 0.0, 1.0, 45.0, 32.581484777583356], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3904800.0000, 
sim time next is 3906000.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 24.20403371421321, 0.1472509282540582, 0.0, 1.0, 45.0, 32.876574901063336], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.5170028095177676, 0.5490836427513527, 0.0, 1.0, 0.6, 0.3287657490106334], 
reward next is 0.6712, 
noisyNet noise sample is [array([0.05054167], dtype=float32), -2.6867576]. 
=============================================
[2019-04-09 15:25:04,217] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[51.245476]
 [51.135178]
 [50.79948 ]
 [50.71888 ]
 [50.681   ]], R is [[51.52898026]
 [51.68787766]
 [51.84831619]
 [52.00981903]
 [52.17304993]].
[2019-04-09 15:25:04,787] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0250731e-12 8.1773799e-10 5.3214609e-07 3.4594805e-12 1.6683990e-02
 6.8673777e-05 9.8324043e-01 3.8381074e-08 6.2135764e-06 7.7971613e-08
 1.7127762e-08], sum to 1.0000
[2019-04-09 15:25:04,787] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1621
[2019-04-09 15:25:04,896] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.666666666666668, 51.66666666666666, 0.0, 0.0, 19.0, 24.08175984007052, 0.1607676144929093, 0.0, 1.0, 45.0, 33.87438887561976], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3969600.0000, 
sim time next is 3970800.0000, 
raw observation next is [-9.0, 53.0, 0.0, 0.0, 19.0, 23.8923236311553, 0.1230123555027081, 0.0, 1.0, 45.0, 34.40318526071481], 
processed observation next is [1.0, 1.0, 0.21329639889196678, 0.53, 0.0, 0.0, 0.08333333333333333, 0.49102696926294165, 0.5410041185009027, 0.0, 1.0, 0.6, 0.3440318526071481], 
reward next is 0.6560, 
noisyNet noise sample is [array([0.99049515], dtype=float32), 2.4033794]. 
=============================================
[2019-04-09 15:25:04,903] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1806030e-12 3.1010178e-10 1.0342012e-07 5.6517234e-12 1.4591811e-03
 2.0487501e-05 9.9851972e-01 1.3201038e-08 3.3613804e-07 1.0188862e-07
 7.6431483e-09], sum to 1.0000
[2019-04-09 15:25:04,903] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6228
[2019-04-09 15:25:04,953] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 71.0, 0.0, 0.0, 19.0, 24.17067233602991, 0.1353747693742475, 0.0, 1.0, 45.0, 33.19397596902708], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3907200.0000, 
sim time next is 3908400.0000, 
raw observation next is [-5.333333333333334, 65.0, 0.0, 0.0, 19.0, 23.98788220938989, 0.1003663415611584, 0.0, 1.0, 45.0, 33.549187615643206], 
processed observation next is [1.0, 0.21739130434782608, 0.31486611265004616, 0.65, 0.0, 0.0, 0.08333333333333333, 0.49899018411582424, 0.5334554471870528, 0.0, 1.0, 0.6, 0.33549187615643206], 
reward next is 0.6645, 
noisyNet noise sample is [array([-0.50227547], dtype=float32), 1.8025552]. 
=============================================
[2019-04-09 15:25:05,009] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.3596641e-12 1.2872232e-08 1.2434460e-07 1.8055342e-12 5.9236223e-03
 5.1927047e-05 9.9402350e-01 1.1134741e-08 7.7845709e-07 5.1913499e-08
 3.5065228e-09], sum to 1.0000
[2019-04-09 15:25:05,009] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2560
[2019-04-09 15:25:05,051] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.33333333333333, 58.0, 0.0, 0.0, 19.0, 23.3098334203634, -0.003322519418874306, 0.0, 1.0, 45.0, 35.5484599259469], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3975600.0000, 
sim time next is 3976800.0000, 
raw observation next is [-10.66666666666667, 58.0, 0.0, 0.0, 19.0, 23.2535803769179, -0.02190226000194924, 0.0, 1.0, 45.0, 35.63861806329969], 
processed observation next is [1.0, 0.0, 0.16712834718374878, 0.58, 0.0, 0.0, 0.08333333333333333, 0.43779836474315825, 0.49269924666601694, 0.0, 1.0, 0.6, 0.3563861806329969], 
reward next is 0.6436, 
noisyNet noise sample is [array([0.1611958], dtype=float32), -0.5183339]. 
=============================================
[2019-04-09 15:25:05,135] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.5761315e-10 3.9493290e-08 4.7181084e-06 2.7038291e-10 1.1809809e-01
 1.7993344e-03 8.8003886e-01 8.0889231e-07 5.4967317e-05 3.0702545e-06
 6.4058227e-08], sum to 1.0000
[2019-04-09 15:25:05,135] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3901
[2019-04-09 15:25:05,167] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4856939e-12 7.8481506e-09 2.2689471e-07 1.3341224e-11 1.2222014e-02
 3.8561840e-05 9.8773032e-01 1.8129994e-08 8.5771262e-06 2.6346115e-07
 1.6921105e-08], sum to 1.0000
[2019-04-09 15:25:05,174] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1570
[2019-04-09 15:25:05,178] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.0, 65.0, 0.0, 0.0, 19.0, 21.93274944828448, -0.3642087318178582, 0.0, 1.0, 45.0, 36.486519164294506], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3994800.0000, 
sim time next is 3996000.0000, 
raw observation next is [-13.0, 63.0, 0.0, 0.0, 19.0, 21.82270327145535, -0.3806713249804978, 0.0, 1.0, 45.0, 36.39544830729892], 
processed observation next is [1.0, 0.2608695652173913, 0.10249307479224376, 0.63, 0.0, 0.0, 0.08333333333333333, 0.31855860595461244, 0.37310955833983406, 0.0, 1.0, 0.6, 0.3639544830729892], 
reward next is 0.6360, 
noisyNet noise sample is [array([0.73942393], dtype=float32), 0.56954515]. 
=============================================
[2019-04-09 15:25:05,187] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8734776e-11 7.2466171e-09 7.5200785e-07 5.5765104e-11 3.2124616e-02
 2.4603971e-04 9.6762073e-01 6.2067903e-07 6.2281092e-06 9.4677819e-07
 9.0565230e-08], sum to 1.0000
[2019-04-09 15:25:05,188] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4711
[2019-04-09 15:25:05,191] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[47.017925]
 [47.065784]
 [47.114956]
 [47.106426]
 [47.11443 ]], R is [[47.1365242 ]
 [47.30029678]
 [47.46216202]
 [47.62272644]
 [47.78253937]].
[2019-04-09 15:25:05,210] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.33333333333333, 59.66666666666667, 0.0, 0.0, 19.0, 23.19197384373101, -0.05971867985373824, 0.0, 1.0, 45.0, 35.72095447747025], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3979200.0000, 
sim time next is 3980400.0000, 
raw observation next is [-11.66666666666667, 61.33333333333334, 0.0, 0.0, 19.0, 23.05963421750202, -0.09667877821243026, 0.0, 1.0, 45.0, 35.828280104063936], 
processed observation next is [1.0, 0.043478260869565216, 0.13942751615881802, 0.6133333333333334, 0.0, 0.0, 0.08333333333333333, 0.4216361847918349, 0.4677737405958566, 0.0, 1.0, 0.6, 0.35828280104063936], 
reward next is 0.6417, 
noisyNet noise sample is [array([-1.2165337], dtype=float32), 0.74216485]. 
=============================================
[2019-04-09 15:25:05,221] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 41.0, 0.0, 0.0, 19.0, 23.73859904620902, -0.03023169620081982, 0.0, 1.0, 35.0, 25.309179381404682], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4086000.0000, 
sim time next is 4087200.0000, 
raw observation next is [-4.666666666666667, 38.66666666666667, 0.0, 0.0, 22.5, 23.60453710405653, -0.04756336665317423, 0.0, 1.0, 45.0, 32.03447829466245], 
processed observation next is [1.0, 0.30434782608695654, 0.3333333333333333, 0.3866666666666667, 0.0, 0.0, 0.375, 0.46704475867137746, 0.4841455444489419, 0.0, 1.0, 0.6, 0.3203447829466245], 
reward next is 0.6797, 
noisyNet noise sample is [array([0.56663966], dtype=float32), -2.159997]. 
=============================================
[2019-04-09 15:25:05,726] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7629054e-14 1.0062261e-10 2.9651135e-08 5.3939299e-14 8.7830508e-03
 9.3097769e-06 9.9120742e-01 1.7132168e-09 1.7175947e-07 3.9633811e-08
 1.0343075e-09], sum to 1.0000
[2019-04-09 15:25:05,726] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7760
[2019-04-09 15:25:05,764] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.333333333333333, 50.33333333333333, 102.1666666666667, 705.8333333333334, 22.5, 25.04445038132935, 0.341417042124335, 1.0, 1.0, 45.0, 32.00771263228288], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3922800.0000, 
sim time next is 3924000.0000, 
raw observation next is [-7.0, 49.0, 106.5, 733.5, 22.5, 25.40092709866909, 0.395662622751558, 1.0, 1.0, 45.0, 31.35187946203288], 
processed observation next is [1.0, 0.43478260869565216, 0.2686980609418283, 0.49, 0.355, 0.8104972375690608, 0.375, 0.6167439248890908, 0.631887540917186, 1.0, 1.0, 0.6, 0.3135187946203288], 
reward next is 0.6865, 
noisyNet noise sample is [array([-0.028772], dtype=float32), -0.572515]. 
=============================================
[2019-04-09 15:25:05,769] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[59.983818]
 [58.765648]
 [57.009357]
 [55.84893 ]
 [55.085354]], R is [[61.08827209]
 [61.1573143 ]
 [61.21724319]
 [61.2720108 ]
 [61.32305145]].
[2019-04-09 15:25:05,832] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.4397995e-15 1.4325253e-10 6.1137378e-10 1.5410404e-14 1.6489018e-01
 1.2381950e-04 8.3497715e-01 4.3555728e-09 8.5645843e-06 2.6870450e-07
 6.4789835e-10], sum to 1.0000
[2019-04-09 15:25:05,832] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6151
[2019-04-09 15:25:05,988] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 29.0, 116.0, 835.5, 22.5, 25.91609833274108, 0.5047524827468463, 1.0, 1.0, 35.0, 25.3235412452785], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4021200.0000, 
sim time next is 4022400.0000, 
raw observation next is [-3.666666666666667, 28.0, 114.6666666666667, 831.8333333333334, 22.5, 26.25452868915742, 0.4276556291307766, 1.0, 1.0, 45.0, 33.64065376030197], 
processed observation next is [1.0, 0.5652173913043478, 0.3610341643582641, 0.28, 0.38222222222222235, 0.9191528545119706, 0.375, 0.6878773907631185, 0.6425518763769255, 1.0, 1.0, 0.6, 0.33640653760301975], 
reward next is 0.6636, 
noisyNet noise sample is [array([-0.8172922], dtype=float32), -0.62950516]. 
=============================================
[2019-04-09 15:25:06,656] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9697826e-15 7.3863849e-12 1.3568259e-08 9.4682419e-15 3.7029114e-02
 2.1293414e-04 9.6275806e-01 7.2525402e-10 4.9278981e-08 6.6693628e-09
 1.3001848e-10], sum to 1.0000
[2019-04-09 15:25:06,656] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8400
[2019-04-09 15:25:06,709] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 36.66666666666667, 90.83333333333334, 734.6666666666667, 22.5, 26.89240307511358, 0.5709619470713063, 1.0, 1.0, 45.0, 27.98644750795671], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3943200.0000, 
sim time next is 3944400.0000, 
raw observation next is [-4.0, 35.33333333333333, 84.33333333333334, 692.6666666666667, 22.5, 25.14345948727604, 0.6136505551981635, 1.0, 1.0, 45.0, 28.072110357934477], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.3533333333333333, 0.28111111111111114, 0.765377532228361, 0.375, 0.5952882906063367, 0.7045501850660546, 1.0, 1.0, 0.6, 0.28072110357934477], 
reward next is 0.7193, 
noisyNet noise sample is [array([-1.0197407], dtype=float32), -1.3046769]. 
=============================================
[2019-04-09 15:25:06,752] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2898282e-15 6.5293227e-12 1.1231173e-09 4.6588549e-15 2.5920898e-03
 1.2840820e-05 9.9739504e-01 3.7068439e-09 1.4514195e-08 3.6684127e-09
 1.0477887e-10], sum to 1.0000
[2019-04-09 15:25:06,752] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5482
[2019-04-09 15:25:06,787] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4148195e-15 8.4253758e-12 1.5296736e-08 1.0810213e-14 3.7750404e-02
 2.1212500e-04 9.6203738e-01 7.8969420e-10 5.5059189e-08 7.1956019e-09
 1.4163352e-10], sum to 1.0000
[2019-04-09 15:25:06,787] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7820
[2019-04-09 15:25:06,855] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 35.0, 93.83333333333334, 652.0, 22.5, 27.70802046635757, 0.9270453709041946, 1.0, 1.0, 45.0, 20.93599439341489], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4117200.0000, 
sim time next is 4118400.0000, 
raw observation next is [4.0, 35.0, 93.5, 566.0, 22.5, 27.23104759622851, 0.8753411065378858, 1.0, 1.0, 45.0, 23.964632513546654], 
processed observation next is [1.0, 0.6956521739130435, 0.5734072022160666, 0.35, 0.31166666666666665, 0.625414364640884, 0.375, 0.7692539663523759, 0.7917803688459619, 1.0, 1.0, 0.6, 0.23964632513546655], 
reward next is 0.7604, 
noisyNet noise sample is [array([-0.5783831], dtype=float32), -0.5125547]. 
=============================================
[2019-04-09 15:25:06,856] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 35.33333333333334, 69.66666666666666, 567.3333333333334, 22.5, 27.03865147720822, 0.7893397949697222, 1.0, 1.0, 45.0, 27.6249199134257], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3946800.0000, 
sim time next is 3948000.0000, 
raw observation next is [-4.666666666666666, 36.66666666666666, 58.16666666666666, 474.8333333333334, 22.5, 27.05908439328099, 0.7643622531300012, 1.0, 1.0, 45.0, 28.437662245605676], 
processed observation next is [1.0, 0.6956521739130435, 0.33333333333333337, 0.3666666666666666, 0.19388888888888886, 0.5246777163904237, 0.375, 0.7549236994400825, 0.7547874177100004, 1.0, 1.0, 0.6, 0.28437662245605677], 
reward next is 0.7156, 
noisyNet noise sample is [array([-1.0197407], dtype=float32), -1.3046769]. 
=============================================
[2019-04-09 15:25:06,893] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[67.1486  ]
 [67.184654]
 [67.32326 ]
 [67.42475 ]
 [67.424736]], R is [[67.1515274 ]
 [67.20375824]
 [67.25576019]
 [67.3024826 ]
 [67.34959412]].
[2019-04-09 15:25:07,950] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2820493e-10 2.9256901e-08 4.4113290e-06 3.6778808e-10 2.2556003e-02
 1.5239033e-03 9.7580564e-01 4.1196372e-06 1.0044255e-04 5.2065170e-06
 1.5689098e-07], sum to 1.0000
[2019-04-09 15:25:07,953] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0178
[2019-04-09 15:25:07,994] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 19.0, 22.64110368152669, -0.1804227081846504, 0.0, 1.0, 45.0, 36.12245922508174], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 3985200.0000, 
sim time next is 3986400.0000, 
raw observation next is [-12.0, 63.00000000000001, 0.0, 0.0, 19.0, 22.68761081085897, -0.2165673197528238, 0.0, 1.0, 35.0, 29.51740652146063], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.6300000000000001, 0.0, 0.0, 0.08333333333333333, 0.39063423423824756, 0.4278108934157254, 0.0, 1.0, 0.4, 0.29517406521460626], 
reward next is 0.7048, 
noisyNet noise sample is [array([0.93664867], dtype=float32), 1.5388653]. 
=============================================
[2019-04-09 15:25:08,193] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1148904e-14 8.3357810e-10 2.0991595e-07 7.1000909e-13 2.8598535e-01
 2.2824766e-05 7.1399111e-01 3.4354763e-09 4.8811671e-07 6.6980974e-08
 7.2311717e-09], sum to 1.0000
[2019-04-09 15:25:08,195] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2995
[2019-04-09 15:25:08,350] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.6389749e-11 7.0271561e-08 8.8164217e-07 1.9079588e-10 1.4091769e-01
 1.5079632e-04 8.5891342e-01 2.8693898e-07 1.4175322e-05 2.6752452e-06
 3.4631306e-08], sum to 1.0000
[2019-04-09 15:25:08,350] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2153
[2019-04-09 15:25:08,371] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666667, 38.0, 118.8333333333333, 820.1666666666666, 22.5, 25.46467123890702, 0.3877393371258871, 1.0, 1.0, 45.0, 36.07932212507704], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4016400.0000, 
sim time next is 4017600.0000, 
raw observation next is [-6.0, 37.0, 118.5, 828.5, 22.5, 25.81252656888195, 0.4596448373085398, 1.0, 1.0, 45.0, 34.77046238399148], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.37, 0.395, 0.9154696132596685, 0.375, 0.6510438807401625, 0.6532149457695132, 1.0, 1.0, 0.6, 0.3477046238399148], 
reward next is 0.6523, 
noisyNet noise sample is [array([-0.34724092], dtype=float32), 1.3463145]. 
=============================================
[2019-04-09 15:25:08,391] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.0, 67.0, 0.0, 0.0, 19.0, 21.94160474067242, -0.3601616493304187, 0.0, 1.0, 45.0, 37.07540649552098], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3993600.0000, 
sim time next is 3994800.0000, 
raw observation next is [-13.0, 65.0, 0.0, 0.0, 19.0, 21.84224008488664, -0.388880713615523, 0.0, 1.0, 45.0, 36.69561145975985], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.65, 0.0, 0.0, 0.08333333333333333, 0.3201866737405534, 0.37037309546149233, 0.0, 1.0, 0.6, 0.3669561145975985], 
reward next is 0.6330, 
noisyNet noise sample is [array([1.4163369], dtype=float32), -0.37465817]. 
=============================================
[2019-04-09 15:25:08,896] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.0065535e-11 9.6928090e-08 5.0961376e-06 2.3445079e-10 1.7273176e-01
 4.0566866e-04 8.2672948e-01 1.6069955e-06 1.0902425e-04 1.6866445e-05
 3.1801991e-07], sum to 1.0000
[2019-04-09 15:25:08,896] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6764
[2019-04-09 15:25:08,931] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 19.0, 22.88603256847353, -0.1205960729017848, 0.0, 1.0, 45.0, 35.997631173522805], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3981600.0000, 
sim time next is 3982800.0000, 
raw observation next is [-12.0, 63.00000000000001, 0.0, 0.0, 19.0, 22.79827864152476, -0.1435073214535966, 0.0, 1.0, 45.0, 36.0579592572404], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.6300000000000001, 0.0, 0.0, 0.08333333333333333, 0.39985655346039667, 0.4521642261821344, 0.0, 1.0, 0.6, 0.36057959257240396], 
reward next is 0.6394, 
noisyNet noise sample is [array([-0.7106888], dtype=float32), -0.0007838546]. 
=============================================
[2019-04-09 15:25:09,032] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.4592528e-12 6.2270393e-09 1.1228072e-07 3.4840956e-13 3.3821756e-01
 3.8421861e-04 6.6137731e-01 1.0667446e-08 2.0727255e-05 1.0891321e-07
 5.1815565e-08], sum to 1.0000
[2019-04-09 15:25:09,032] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7151
[2019-04-09 15:25:09,077] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 37.0, 0.0, 0.0, 19.0, 25.26180327901366, 0.3758044630353746, 0.0, 1.0, 45.0, 30.870430742133557], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4060800.0000, 
sim time next is 4062000.0000, 
raw observation next is [-6.0, 38.33333333333334, 0.0, 0.0, 19.0, 25.13721422328405, 0.3480184812409726, 0.0, 1.0, 45.0, 31.132796239360417], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.3833333333333334, 0.0, 0.0, 0.08333333333333333, 0.5947678519403375, 0.6160061604136575, 0.0, 1.0, 0.6, 0.31132796239360416], 
reward next is 0.6887, 
noisyNet noise sample is [array([1.1490246], dtype=float32), 1.8267035]. 
=============================================
[2019-04-09 15:25:09,078] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5935432e-14 7.7530316e-13 1.2048528e-09 1.0991351e-14 4.6876256e-04
 1.6668398e-06 9.9952960e-01 9.5985420e-10 1.1537034e-08 3.4882128e-09
 3.6003991e-11], sum to 1.0000
[2019-04-09 15:25:09,078] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3192
[2019-04-09 15:25:09,095] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[56.30273 ]
 [57.3311  ]
 [57.47536 ]
 [57.943226]
 [58.59375 ]], R is [[55.17824936]
 [55.31776428]
 [55.45862961]
 [55.60123444]
 [55.74599075]].
[2019-04-09 15:25:09,114] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 19.0, 24.88270615223293, 0.3811037541874854, 0.0, 1.0, 45.0, 32.269619046024076], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3963600.0000, 
sim time next is 3964800.0000, 
raw observation next is [-7.333333333333334, 46.33333333333334, 0.0, 0.0, 19.0, 24.77759552631442, 0.3588363400881103, 0.0, 1.0, 45.0, 32.58172311991773], 
processed observation next is [1.0, 0.9130434782608695, 0.2594644506001847, 0.46333333333333343, 0.0, 0.0, 0.08333333333333333, 0.5647996271928685, 0.6196121133627034, 0.0, 1.0, 0.6, 0.3258172311991773], 
reward next is 0.6742, 
noisyNet noise sample is [array([1.2323384], dtype=float32), -1.1324006]. 
=============================================
[2019-04-09 15:25:09,348] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1888485e-13 9.3490806e-09 4.1539306e-07 2.0321342e-12 3.7996814e-01
 5.7534728e-04 6.1939514e-01 3.5651081e-07 5.9501803e-05 9.5344114e-07
 6.2209949e-08], sum to 1.0000
[2019-04-09 15:25:09,348] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4729
[2019-04-09 15:25:09,394] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.333333333333334, 50.33333333333334, 0.0, 0.0, 19.0, 24.46833999111993, 0.230263010116024, 0.0, 1.0, 45.0, 33.87127778695587], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 3968400.0000, 
sim time next is 3969600.0000, 
raw observation next is [-8.666666666666668, 51.66666666666666, 0.0, 0.0, 19.0, 24.15886753059517, 0.1819603587248581, 0.0, 1.0, 45.0, 33.8261074310833], 
processed observation next is [1.0, 0.9565217391304348, 0.22253000923361033, 0.5166666666666666, 0.0, 0.0, 0.08333333333333333, 0.5132389608829309, 0.560653452908286, 0.0, 1.0, 0.6, 0.338261074310833], 
reward next is 0.6617, 
noisyNet noise sample is [array([-0.37683952], dtype=float32), -0.22487904]. 
=============================================
[2019-04-09 15:25:09,419] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8812713e-14 4.2556438e-11 7.0976249e-08 4.2167105e-13 6.2835537e-02
 4.7061501e-05 9.3711686e-01 6.2806644e-09 3.0329198e-07 1.0282839e-07
 5.8993138e-10], sum to 1.0000
[2019-04-09 15:25:09,419] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6586
[2019-04-09 15:25:09,488] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 30.33333333333334, 0.0, 0.0, 19.0, 26.78117809656558, 0.6994856749959718, 0.0, 1.0, 35.0, 24.88366813402929], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4047600.0000, 
sim time next is 4048800.0000, 
raw observation next is [-4.0, 29.66666666666666, 0.0, 0.0, 19.0, 26.60028966433635, 0.6750998590866993, 0.0, 1.0, 45.0, 29.850009931258683], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.29666666666666663, 0.0, 0.0, 0.08333333333333333, 0.7166908053613626, 0.7250332863622332, 0.0, 1.0, 0.6, 0.2985000993125868], 
reward next is 0.7015, 
noisyNet noise sample is [array([1.0276153], dtype=float32), 0.2712509]. 
=============================================
[2019-04-09 15:25:10,249] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6506281e-16 1.7976291e-11 5.1563493e-08 8.6720360e-15 6.4595910e-03
 3.6859240e-06 9.9353677e-01 6.6187589e-09 1.2737762e-08 1.6757305e-08
 2.0419103e-10], sum to 1.0000
[2019-04-09 15:25:10,252] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0311
[2019-04-09 15:25:10,346] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 24.0, 105.6666666666667, 800.0, 22.5, 26.0290668879803, 0.6624219821179081, 1.0, 1.0, 45.0, 30.79870546675391], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4026000.0000, 
sim time next is 4027200.0000, 
raw observation next is [-2.333333333333333, 22.0, 101.5, 780.3333333333334, 22.5, 27.11489789624775, 0.7791096518681239, 1.0, 1.0, 45.0, 30.121271331006298], 
processed observation next is [1.0, 0.6086956521739131, 0.3979686057248385, 0.22, 0.3383333333333333, 0.8622467771639043, 0.375, 0.7595748246873125, 0.7597032172893746, 1.0, 1.0, 0.6, 0.301212713310063], 
reward next is 0.6988, 
noisyNet noise sample is [array([0.6921339], dtype=float32), 1.108728]. 
=============================================
[2019-04-09 15:25:10,579] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9380550e-15 1.2102444e-12 2.4706578e-10 2.6649177e-15 3.3137865e-02
 1.5477906e-06 9.6686065e-01 1.4077545e-10 2.3781871e-08 1.8344313e-09
 2.6469294e-11], sum to 1.0000
[2019-04-09 15:25:10,580] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6029
[2019-04-09 15:25:10,686] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 25.33333333333333, 16.66666666666667, 160.8333333333333, 22.5, 27.55373732026905, 0.8746410649063416, 1.0, 1.0, 45.0, 31.112803017251984], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4038000.0000, 
sim time next is 4039200.0000, 
raw observation next is [-3.0, 26.0, 0.0, 0.0, 22.5, 27.56064554353422, 0.8726582751434897, 1.0, 1.0, 45.0, 31.086218339223535], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.26, 0.0, 0.0, 0.375, 0.7967204619611851, 0.7908860917144965, 1.0, 1.0, 0.6, 0.31086218339223537], 
reward next is 0.6891, 
noisyNet noise sample is [array([-0.35031614], dtype=float32), -0.4680106]. 
=============================================
[2019-04-09 15:25:10,855] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5300561e-15 8.5014903e-12 8.3460389e-10 1.2015823e-16 2.6844928e-02
 1.6646733e-05 9.7313833e-01 5.9067923e-10 1.0560897e-08 3.1791680e-09
 1.8961620e-11], sum to 1.0000
[2019-04-09 15:25:10,860] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6553
[2019-04-09 15:25:10,931] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.0096330e-10 1.1415184e-06 2.2218859e-05 7.7520190e-10 1.9845305e-01
 7.8019570e-04 8.0061287e-01 2.1562560e-06 1.1659495e-04 1.1113451e-05
 6.4062453e-07], sum to 1.0000
[2019-04-09 15:25:10,931] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0134
[2019-04-09 15:25:10,947] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 22.66666666666667, 70.66666666666667, 575.3333333333334, 22.5, 27.9116381286972, 0.9172554403925922, 1.0, 1.0, 45.0, 26.6700099403064], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4033200.0000, 
sim time next is 4034400.0000, 
raw observation next is [-1.666666666666667, 23.33333333333334, 59.16666666666667, 488.8333333333334, 22.5, 28.07476755096076, 0.9504195003238692, 1.0, 1.0, 45.0, 26.515234701042015], 
processed observation next is [1.0, 0.6956521739130435, 0.4164358264081256, 0.2333333333333334, 0.19722222222222224, 0.5401473296500922, 0.375, 0.8395639625800634, 0.8168065001079564, 1.0, 1.0, 0.6, 0.2651523470104202], 
reward next is 0.7348, 
noisyNet noise sample is [array([1.1894351], dtype=float32), 2.300493]. 
=============================================
[2019-04-09 15:25:11,051] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666667, 51.00000000000001, 76.66666666666667, 406.6666666666667, 19.0, 23.49184785848241, 0.03112829031154698, 0.0, 1.0, 35.0, 24.903451862728346], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4177200.0000, 
sim time next is 4178400.0000, 
raw observation next is [-4.333333333333334, 48.0, 94.66666666666667, 516.6666666666666, 19.0, 23.46336882104737, 0.02531831541559558, 0.0, 1.0, 35.0, 21.720733262900744], 
processed observation next is [0.0, 0.34782608695652173, 0.3425669436749769, 0.48, 0.3155555555555556, 0.570902394106814, 0.08333333333333333, 0.45528073508728095, 0.5084394384718652, 0.0, 1.0, 0.4, 0.21720733262900743], 
reward next is 0.7828, 
noisyNet noise sample is [array([1.2422456], dtype=float32), 0.023800345]. 
=============================================
[2019-04-09 15:25:12,316] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.0048245e-17 1.8943785e-12 1.2298846e-08 5.0978456e-16 4.5648549e-02
 2.7023591e-06 9.5434862e-01 2.9495603e-10 5.5352963e-08 1.2781119e-08
 3.2812641e-11], sum to 1.0000
[2019-04-09 15:25:12,329] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0871
[2019-04-09 15:25:12,434] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 28.0, 114.6666666666667, 831.8333333333334, 22.5, 26.41910415542243, 0.4536031485488285, 1.0, 1.0, 45.0, 32.9706231793227], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4022400.0000, 
sim time next is 4023600.0000, 
raw observation next is [-3.333333333333333, 27.0, 112.3333333333333, 824.0, 22.5, 26.44202524568193, 0.6454304333502376, 1.0, 1.0, 45.0, 31.935939354746516], 
processed observation next is [1.0, 0.5652173913043478, 0.37026777469990774, 0.27, 0.37444444444444436, 0.9104972375690608, 0.375, 0.7035021038068274, 0.7151434777834126, 1.0, 1.0, 0.6, 0.3193593935474652], 
reward next is 0.6806, 
noisyNet noise sample is [array([-1.446455], dtype=float32), -0.14551257]. 
=============================================
[2019-04-09 15:25:12,460] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1586333e-11 2.0052395e-08 8.3096523e-07 2.0693355e-11 9.4429227e-03
 1.9191607e-04 9.9036050e-01 9.5608996e-08 2.1051039e-06 1.4615695e-06
 6.1740401e-08], sum to 1.0000
[2019-04-09 15:25:12,461] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3434
[2019-04-09 15:25:12,488] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 38.66666666666667, 0.0, 0.0, 19.0, 24.29951885857257, 0.1140409483726778, 0.0, 1.0, 45.0, 32.54020137532711], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4076400.0000, 
sim time next is 4077600.0000, 
raw observation next is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 19.0, 24.20242468052454, 0.08787434710132376, 0.0, 1.0, 45.0, 32.55155704601709], 
processed observation next is [1.0, 0.17391304347826086, 0.3425669436749769, 0.3633333333333333, 0.0, 0.0, 0.08333333333333333, 0.516868723377045, 0.5292914490337746, 0.0, 1.0, 0.6, 0.3255155704601709], 
reward next is 0.6745, 
noisyNet noise sample is [array([-0.717749], dtype=float32), -0.21333551]. 
=============================================
[2019-04-09 15:25:12,903] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.6878364e-14 6.3478528e-11 1.3596074e-08 3.3554109e-14 5.8706212e-03
 2.1684373e-05 9.9410713e-01 1.3462611e-08 5.8892880e-07 6.8048704e-09
 1.3262931e-10], sum to 1.0000
[2019-04-09 15:25:12,903] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9092
[2019-04-09 15:25:12,991] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 20.0, 96.5, 753.0, 22.5, 26.12508672311282, 0.7091481868298245, 1.0, 1.0, 45.0, 29.893395905338053], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4028400.0000, 
sim time next is 4029600.0000, 
raw observation next is [-1.666666666666667, 20.66666666666667, 91.5, 725.6666666666667, 22.5, 27.38830545869211, 0.8315688316896587, 1.0, 1.0, 45.0, 29.482544978240227], 
processed observation next is [1.0, 0.6521739130434783, 0.4164358264081256, 0.20666666666666672, 0.305, 0.801841620626151, 0.375, 0.7823587882243425, 0.7771896105632196, 1.0, 1.0, 0.6, 0.29482544978240227], 
reward next is 0.7052, 
noisyNet noise sample is [array([0.4179747], dtype=float32), 1.6231831]. 
=============================================
[2019-04-09 15:25:13,463] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.2543922e-11 1.9232447e-08 1.0044780e-06 3.0682006e-11 4.5034572e-02
 2.5385174e-05 9.5493430e-01 6.5143738e-08 3.5239034e-06 1.0038757e-06
 3.0987881e-08], sum to 1.0000
[2019-04-09 15:25:13,465] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9838
[2019-04-09 15:25:13,509] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2276833e-11 1.7942137e-08 2.4597134e-07 4.4666749e-11 7.6487564e-02
 4.9975613e-05 9.2345905e-01 1.5260449e-07 2.0367943e-06 9.4692723e-07
 4.5062136e-09], sum to 1.0000
[2019-04-09 15:25:13,510] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5442
[2019-04-09 15:25:13,513] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 19.0, 24.40221175745946, 0.1587853256331102, 0.0, 1.0, 45.0, 31.44664708339549], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4234800.0000, 
sim time next is 4236000.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 19.0, 24.3775003261741, 0.1509385371542968, 0.0, 1.0, 45.0, 31.65050419497653], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.08333333333333333, 0.5314583605145083, 0.5503128457180989, 0.0, 1.0, 0.6, 0.3165050419497653], 
reward next is 0.6835, 
noisyNet noise sample is [array([-0.9847138], dtype=float32), 2.4663033]. 
=============================================
[2019-04-09 15:25:13,518] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[47.085052]
 [48.1805  ]
 [48.833492]
 [48.828945]
 [48.810547]], R is [[46.29642487]
 [46.51899719]
 [46.74150085]
 [46.96393585]
 [47.18618774]].
[2019-04-09 15:25:13,545] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 19.0, 25.03461767294527, 0.3570898343694545, 0.0, 1.0, 45.0, 29.49633180794772], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4152000.0000, 
sim time next is 4153200.0000, 
raw observation next is [-1.666666666666667, 43.66666666666666, 0.0, 0.0, 19.0, 24.95850575753281, 0.3376890534529874, 0.0, 1.0, 45.0, 29.72204107695177], 
processed observation next is [0.0, 0.043478260869565216, 0.4164358264081256, 0.4366666666666666, 0.0, 0.0, 0.08333333333333333, 0.5798754797944007, 0.6125630178176625, 0.0, 1.0, 0.6, 0.29722041076951766], 
reward next is 0.7028, 
noisyNet noise sample is [array([1.3740047], dtype=float32), 1.148596]. 
=============================================
[2019-04-09 15:25:14,568] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.5385069e-10 1.2525373e-07 7.6919578e-06 5.3689037e-10 1.1986869e-01
 1.7984778e-04 8.7981874e-01 2.4221481e-06 1.0292991e-04 1.9317764e-05
 2.9701283e-07], sum to 1.0000
[2019-04-09 15:25:14,571] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7009
[2019-04-09 15:25:14,612] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 51.00000000000001, 76.66666666666667, 406.6666666666667, 19.0, 23.51377342064116, 0.03360072605258098, 0.0, 1.0, 35.0, 24.92860878880171], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4177200.0000, 
sim time next is 4178400.0000, 
raw observation next is [-4.333333333333334, 48.0, 94.66666666666667, 516.6666666666666, 19.0, 23.49099748168121, 0.04535770987030781, 0.0, 1.0, 45.0, 31.025936692400236], 
processed observation next is [0.0, 0.34782608695652173, 0.3425669436749769, 0.48, 0.3155555555555556, 0.570902394106814, 0.08333333333333333, 0.45758312347343405, 0.515119236623436, 0.0, 1.0, 0.6, 0.31025936692400236], 
reward next is 0.6897, 
noisyNet noise sample is [array([1.0983688], dtype=float32), 1.566332]. 
=============================================
[2019-04-09 15:25:14,656] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9492167e-10 7.7628123e-08 5.1673373e-06 3.0091638e-10 1.1319392e-01
 1.4665874e-04 8.8656443e-01 1.6333089e-06 7.3540134e-05 1.4333142e-05
 1.9056974e-07], sum to 1.0000
[2019-04-09 15:25:14,657] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3101
[2019-04-09 15:25:14,688] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 45.0, 100.0, 574.0, 19.0, 23.52021380624221, 0.05174763270205258, 0.0, 1.0, 35.0, 23.708822935628092], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4179600.0000, 
sim time next is 4180800.0000, 
raw observation next is [-3.333333333333333, 41.66666666666667, 105.3333333333333, 631.3333333333333, 19.0, 23.52794107299189, 0.08217508733825106, 0.0, 1.0, 45.0, 30.33026093230583], 
processed observation next is [0.0, 0.391304347826087, 0.37026777469990774, 0.41666666666666674, 0.351111111111111, 0.6976058931860036, 0.08333333333333333, 0.4606617560826575, 0.527391695779417, 0.0, 1.0, 0.6, 0.3033026093230583], 
reward next is 0.6967, 
noisyNet noise sample is [array([1.0983688], dtype=float32), 1.566332]. 
=============================================
[2019-04-09 15:25:15,095] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7997647e-15 1.5574755e-13 4.4237777e-10 1.0825884e-15 6.9240602e-03
 6.7047847e-07 9.9307525e-01 1.2454839e-11 1.1719599e-08 2.1926164e-10
 8.2514507e-12], sum to 1.0000
[2019-04-09 15:25:15,095] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2613
[2019-04-09 15:25:15,126] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 30.33333333333333, 114.3333333333333, 824.0, 22.5, 26.85304155880186, 0.7554398860624677, 1.0, 1.0, 45.0, 24.391104767519757], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4110000.0000, 
sim time next is 4111200.0000, 
raw observation next is [3.0, 31.0, 111.0, 812.0, 22.5, 27.25818981403779, 0.818632703413856, 1.0, 1.0, 45.0, 21.259882367732388], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.31, 0.37, 0.8972375690607735, 0.375, 0.7715158178364826, 0.7728775678046187, 1.0, 1.0, 0.6, 0.21259882367732388], 
reward next is 0.7874, 
noisyNet noise sample is [array([1.2188381], dtype=float32), 0.94164294]. 
=============================================
[2019-04-09 15:25:15,240] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.3288910e-12 5.0773123e-09 3.3801527e-07 1.0470403e-11 2.5175594e-02
 7.4228423e-04 9.7407794e-01 1.9707231e-07 3.4435261e-06 3.2429475e-07
 4.8562496e-09], sum to 1.0000
[2019-04-09 15:25:15,241] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3735
[2019-04-09 15:25:15,280] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 40.0, 0.0, 0.0, 19.0, 25.24992161272956, 0.4257403671366732, 0.0, 1.0, 45.0, 29.157838987569846], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4149600.0000, 
sim time next is 4150800.0000, 
raw observation next is [-1.0, 39.0, 0.0, 0.0, 19.0, 25.17117316840944, 0.403318663091304, 0.0, 1.0, 45.0, 29.3092055737496], 
processed observation next is [0.0, 0.043478260869565216, 0.4349030470914128, 0.39, 0.0, 0.0, 0.08333333333333333, 0.5975977640341199, 0.634439554363768, 0.0, 1.0, 0.6, 0.293092055737496], 
reward next is 0.7069, 
noisyNet noise sample is [array([-0.33522242], dtype=float32), -0.29516515]. 
=============================================
[2019-04-09 15:25:15,572] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2930282e-13 1.9425031e-10 1.1135078e-08 6.6883958e-14 9.3249865e-02
 2.0112542e-05 9.0672892e-01 5.0276534e-09 1.0734472e-06 2.8095588e-08
 1.6435333e-09], sum to 1.0000
[2019-04-09 15:25:15,572] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4060
[2019-04-09 15:25:15,598] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.0399742e-13 7.7241163e-10 3.1979518e-08 4.2202556e-13 1.7089432e-02
 2.4740910e-04 9.8266071e-01 1.5468208e-08 2.2558286e-06 1.5917492e-07
 4.3805364e-09], sum to 1.0000
[2019-04-09 15:25:15,598] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6201
[2019-04-09 15:25:15,606] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 38.0, 98.0, 574.0, 22.5, 24.94400686442128, 0.2427334593081434, 1.0, 1.0, 45.0, 28.935830698495085], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4093200.0000, 
sim time next is 4094400.0000, 
raw observation next is [-2.666666666666667, 37.0, 102.0, 644.0, 22.5, 25.11200734696169, 0.3008081769885326, 1.0, 1.0, 45.0, 28.66594433302541], 
processed observation next is [1.0, 0.391304347826087, 0.38873499538319484, 0.37, 0.34, 0.7116022099447514, 0.375, 0.5926672789134741, 0.6002693923295109, 1.0, 1.0, 0.6, 0.2866594433302541], 
reward next is 0.7133, 
noisyNet noise sample is [array([-0.044086], dtype=float32), 1.4943674]. 
=============================================
[2019-04-09 15:25:15,634] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 56.0, 93.0, 605.5, 19.0, 26.08383740359674, 0.6125159848500054, 0.0, 1.0, 45.0, 25.70244671737145], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4291200.0000, 
sim time next is 4292400.0000, 
raw observation next is [6.866666666666667, 57.33333333333334, 77.66666666666666, 572.5, 19.0, 26.1769512957367, 0.625215381392774, 0.0, 1.0, 45.0, 25.448553541884067], 
processed observation next is [0.0, 0.6956521739130435, 0.6528162511542014, 0.5733333333333335, 0.25888888888888884, 0.6325966850828729, 0.08333333333333333, 0.6814126079780584, 0.7084051271309247, 0.0, 1.0, 0.6, 0.25448553541884067], 
reward next is 0.7455, 
noisyNet noise sample is [array([0.08733149], dtype=float32), -1.825687]. 
=============================================
[2019-04-09 15:25:15,712] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.8200442e-13 1.4092627e-09 1.6827705e-07 3.0166264e-12 1.4015703e-02
 8.7264409e-05 9.8588723e-01 6.1139765e-08 9.4857278e-06 1.4780149e-07
 1.7017323e-08], sum to 1.0000
[2019-04-09 15:25:15,712] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0013
[2019-04-09 15:25:15,751] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.933333333333333, 70.33333333333333, 0.0, 0.0, 19.0, 25.15741686255834, 0.3085436686551506, 0.0, 1.0, 45.0, 30.75316208162444], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4333200.0000, 
sim time next is 4334400.0000, 
raw observation next is [3.9, 70.0, 0.0, 0.0, 19.0, 25.02543020236541, 0.3082276399879321, 0.0, 1.0, 35.0, 25.23202458705952], 
processed observation next is [1.0, 0.17391304347826086, 0.5706371191135734, 0.7, 0.0, 0.0, 0.08333333333333333, 0.5854525168637842, 0.602742546662644, 0.0, 1.0, 0.4, 0.2523202458705952], 
reward next is 0.7477, 
noisyNet noise sample is [array([0.08246425], dtype=float32), -1.7726783]. 
=============================================
[2019-04-09 15:25:15,795] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3688140e-09 1.8007844e-07 5.5522714e-06 2.2301461e-10 9.6349411e-02
 6.5780571e-04 9.0293342e-01 1.5926894e-06 4.6042569e-05 5.7257885e-06
 1.7575803e-07], sum to 1.0000
[2019-04-09 15:25:15,796] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6383
[2019-04-09 15:25:15,830] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 51.00000000000001, 76.66666666666667, 406.6666666666667, 19.0, 23.50799983009379, 0.04192740529370442, 0.0, 1.0, 45.0, 31.99611195295839], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4177200.0000, 
sim time next is 4178400.0000, 
raw observation next is [-4.333333333333334, 48.0, 94.66666666666667, 516.6666666666666, 19.0, 23.52303859436959, 0.05417033007154359, 0.0, 1.0, 45.0, 31.322242758408997], 
processed observation next is [0.0, 0.34782608695652173, 0.3425669436749769, 0.48, 0.3155555555555556, 0.570902394106814, 0.08333333333333333, 0.46025321619746595, 0.5180567766905145, 0.0, 1.0, 0.6, 0.31322242758408997], 
reward next is 0.6868, 
noisyNet noise sample is [array([0.68281394], dtype=float32), -1.1018022]. 
=============================================
[2019-04-09 15:25:16,346] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3853084e-10 8.2427682e-09 4.9099287e-07 1.1048308e-10 1.2168451e-01
 1.5890111e-04 8.7814820e-01 1.6051747e-07 6.5826130e-06 9.4888253e-07
 1.5653852e-07], sum to 1.0000
[2019-04-09 15:25:16,362] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9487
[2019-04-09 15:25:16,370] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8586395e-12 3.4385600e-10 1.4445338e-07 6.5072181e-14 1.7909992e-01
 3.6929847e-05 8.2085198e-01 6.2891083e-08 1.0951091e-05 9.8134237e-08
 4.1447472e-09], sum to 1.0000
[2019-04-09 15:25:16,371] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2047
[2019-04-09 15:25:16,393] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 19.0, 24.68602793223928, 0.2176773110617151, 0.0, 1.0, 35.0, 23.597842414658537], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4227600.0000, 
sim time next is 4228800.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 19.0, 24.61405478760479, 0.2082344719264228, 0.0, 1.0, 45.0, 30.110010134097354], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.47, 0.0, 0.0, 0.08333333333333333, 0.5511712323003991, 0.5694114906421409, 0.0, 1.0, 0.6, 0.30110010134097354], 
reward next is 0.6989, 
noisyNet noise sample is [array([0.87410825], dtype=float32), -0.79030323]. 
=============================================
[2019-04-09 15:25:16,394] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 25.45455229334633, 0.4704456602006665, 0.0, 1.0, 45.0, 28.704021085875304], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4147200.0000, 
sim time next is 4148400.0000, 
raw observation next is [-1.0, 41.0, 0.0, 0.0, 19.0, 25.34272522459886, 0.4464918062435011, 0.0, 1.0, 45.0, 28.962513417472202], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.41, 0.0, 0.0, 0.08333333333333333, 0.6118937687165715, 0.648830602081167, 0.0, 1.0, 0.6, 0.289625134174722], 
reward next is 0.7104, 
noisyNet noise sample is [array([-1.7952594], dtype=float32), -0.30553806]. 
=============================================
[2019-04-09 15:25:16,538] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0666885e-10 4.1371841e-08 2.3323564e-06 1.3197042e-10 3.7720150e-01
 1.5940163e-03 6.2117016e-01 2.9437206e-07 2.9287427e-05 2.2890843e-06
 1.3439715e-07], sum to 1.0000
[2019-04-09 15:25:16,539] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8126
[2019-04-09 15:25:16,570] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 47.66666666666667, 0.0, 0.0, 19.0, 24.33239522294452, 0.1505563430666538, 0.0, 1.0, 45.0, 31.312659804566227], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4232400.0000, 
sim time next is 4233600.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 19.0, 24.29845007131011, 0.1484221630439751, 0.0, 1.0, 45.0, 32.0826678248632], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.08333333333333333, 0.5248708392758425, 0.5494740543479917, 0.0, 1.0, 0.6, 0.320826678248632], 
reward next is 0.6792, 
noisyNet noise sample is [array([-1.8906853], dtype=float32), 0.1394937]. 
=============================================
[2019-04-09 15:25:16,582] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2593166e-14 2.4335644e-12 4.0543551e-09 3.3767701e-15 3.5035731e-03
 4.0564679e-05 9.9645555e-01 1.6581847e-10 3.1058235e-07 4.0429731e-09
 1.2821354e-10], sum to 1.0000
[2019-04-09 15:25:16,583] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8119
[2019-04-09 15:25:16,615] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7758818e-15 1.8067377e-12 1.2518966e-08 1.0874367e-15 4.3410952e-03
 1.4113553e-05 9.9564457e-01 7.6421154e-09 2.1930387e-07 3.2535070e-09
 5.9164569e-11], sum to 1.0000
[2019-04-09 15:25:16,617] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8182
[2019-04-09 15:25:16,620] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 33.0, 115.1666666666667, 776.8333333333334, 22.5, 26.13964899539739, 0.5177421672623355, 1.0, 1.0, 45.0, 25.799920305217533], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4099200.0000, 
sim time next is 4100400.0000, 
raw observation next is [-1.0, 32.0, 117.5, 792.5, 22.5, 26.27755994573115, 0.5334722184232227, 1.0, 1.0, 45.0, 25.689200254065888], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.32, 0.39166666666666666, 0.8756906077348067, 0.375, 0.6897966621442624, 0.6778240728077409, 1.0, 1.0, 0.6, 0.25689200254065886], 
reward next is 0.7431, 
noisyNet noise sample is [array([1.2495956], dtype=float32), 0.115382865]. 
=============================================
[2019-04-09 15:25:16,667] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 35.0, 0.0, 0.0, 22.5, 27.41373722572288, 0.879580851285538, 1.0, 1.0, 45.0, 21.57359270486006], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4126800.0000, 
sim time next is 4128000.0000, 
raw observation next is [3.0, 36.0, 0.0, 0.0, 22.5, 27.33472272057929, 0.8499144777610494, 1.0, 1.0, 45.0, 25.55799453105478], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.36, 0.0, 0.0, 0.375, 0.7778935600482741, 0.7833048259203498, 1.0, 1.0, 0.6, 0.25557994531054784], 
reward next is 0.7444, 
noisyNet noise sample is [array([0.68502057], dtype=float32), -0.3168414]. 
=============================================
[2019-04-09 15:25:16,675] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[70.11965 ]
 [70.43185 ]
 [71.05577 ]
 [71.798294]
 [71.69135 ]], R is [[69.58055115]
 [69.66900635]
 [69.76237488]
 [69.82131958]
 [69.89235687]].
[2019-04-09 15:25:16,899] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5235103e-09 6.6308610e-07 4.3144628e-05 1.5655675e-09 1.2115153e-01
 7.7444402e-04 8.7792641e-01 2.2212330e-06 8.5680113e-05 1.5453772e-05
 5.3446064e-07], sum to 1.0000
[2019-04-09 15:25:16,899] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7311
[2019-04-09 15:25:16,915] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.0141037e-12 4.8321136e-10 6.2042432e-08 1.2312437e-12 1.3967620e-01
 8.0964220e-04 8.5951257e-01 9.9730904e-08 7.0240662e-07 6.3471794e-07
 1.1344416e-08], sum to 1.0000
[2019-04-09 15:25:16,935] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3874
[2019-04-09 15:25:16,939] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 24.20797098190539, 0.1108188163187798, 0.0, 1.0, 45.0, 32.19237465502604], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4240800.0000, 
sim time next is 4242000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 24.19667176781165, 0.1053259381181334, 0.0, 1.0, 45.0, 32.1398958304699], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.5163893139843042, 0.5351086460393778, 0.0, 1.0, 0.6, 0.32139895830469895], 
reward next is 0.6786, 
noisyNet noise sample is [array([1.0466307], dtype=float32), 0.3748689]. 
=============================================
[2019-04-09 15:25:16,955] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[43.937473]
 [44.582798]
 [44.815704]
 [45.082893]
 [45.729755]], R is [[43.77119064]
 [44.01155853]
 [44.24930573]
 [44.48516083]
 [44.719944  ]].
[2019-04-09 15:25:16,958] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 38.0, 83.33333333333333, 548.0, 19.0, 25.62086759235894, 0.5041704438452544, 0.0, 1.0, 45.0, 25.78294680180492], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4206000.0000, 
sim time next is 4207200.0000, 
raw observation next is [2.333333333333333, 39.0, 60.66666666666667, 485.5000000000001, 19.0, 25.76806454445245, 0.5114977669185543, 0.0, 1.0, 45.0, 25.97634296224606], 
processed observation next is [0.0, 0.6956521739130435, 0.5272391505078486, 0.39, 0.20222222222222225, 0.5364640883977901, 0.08333333333333333, 0.6473387120377042, 0.6704992556395181, 0.0, 1.0, 0.6, 0.25976342962246063], 
reward next is 0.7402, 
noisyNet noise sample is [array([-0.39624238], dtype=float32), -1.2967118]. 
=============================================
[2019-04-09 15:25:16,960] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.4928970e-15 8.3628389e-11 1.0571407e-08 3.0774844e-15 1.5305316e-03
 4.1831508e-06 9.9846518e-01 1.8771982e-09 1.4507319e-07 3.4670757e-09
 3.7653956e-11], sum to 1.0000
[2019-04-09 15:25:16,964] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3468
[2019-04-09 15:25:16,985] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 38.66666666666666, 0.0, 0.0, 19.0, 26.3706499715787, 0.7027578009334418, 0.0, 1.0, 45.0, 26.123390650127845], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4138800.0000, 
sim time next is 4140000.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 19.0, 26.34223119055577, 0.6904389617837071, 0.0, 1.0, 45.0, 26.34431905332967], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.4, 0.0, 0.0, 0.08333333333333333, 0.695185932546314, 0.7301463205945691, 0.0, 1.0, 0.6, 0.2634431905332967], 
reward next is 0.7366, 
noisyNet noise sample is [array([0.127679], dtype=float32), -0.012142442]. 
=============================================
[2019-04-09 15:25:16,992] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[60.79711 ]
 [61.339924]
 [62.556374]
 [63.390533]
 [63.79447 ]], R is [[60.41308594]
 [60.54772186]
 [60.6825676 ]
 [60.81697083]
 [60.94851303]].
[2019-04-09 15:25:17,433] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.3479809e-10 2.4512575e-08 9.1912432e-07 1.0284322e-10 2.6611680e-02
 9.1135618e-05 9.7327423e-01 2.4650114e-06 1.7172280e-05 2.2284839e-06
 8.2274937e-08], sum to 1.0000
[2019-04-09 15:25:17,438] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2550
[2019-04-09 15:25:17,464] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 18.33333333333333, 8.833333333333332, 19.0, 23.84877192711991, 0.008306965965824642, 0.0, 1.0, 45.0, 31.676576002682744], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4261200.0000, 
sim time next is 4262400.0000, 
raw observation next is [3.0, 49.0, 55.0, 26.5, 19.0, 23.83142608423488, 0.01291276148081059, 0.0, 1.0, 45.0, 31.579383953224795], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.49, 0.18333333333333332, 0.029281767955801105, 0.08333333333333333, 0.48595217368624005, 0.5043042538269369, 0.0, 1.0, 0.6, 0.31579383953224793], 
reward next is 0.6842, 
noisyNet noise sample is [array([2.203667], dtype=float32), 0.2149752]. 
=============================================
[2019-04-09 15:25:17,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2311351e-13 1.3529735e-09 2.7154632e-08 9.9184658e-14 6.6064574e-02
 1.0323687e-04 9.3383074e-01 5.8085354e-09 1.3835107e-06 1.2101751e-07
 2.8920333e-09], sum to 1.0000
[2019-04-09 15:25:17,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4250
[2019-04-09 15:25:17,619] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 42.66666666666667, 0.0, 0.0, 19.0, 25.67517636700661, 0.5215762826888231, 0.0, 1.0, 45.0, 28.466098901830065], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4144800.0000, 
sim time next is 4146000.0000, 
raw observation next is [-0.6666666666666666, 42.33333333333334, 0.0, 0.0, 19.0, 25.55122031788927, 0.4951504796283008, 0.0, 1.0, 45.0, 28.553031773912505], 
processed observation next is [1.0, 1.0, 0.44413665743305636, 0.42333333333333345, 0.0, 0.0, 0.08333333333333333, 0.6292683598241059, 0.6650501598761003, 0.0, 1.0, 0.6, 0.2855303177391251], 
reward next is 0.7145, 
noisyNet noise sample is [array([-1.3317477], dtype=float32), 0.26597825]. 
=============================================
[2019-04-09 15:25:17,627] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[58.012295]
 [58.247192]
 [58.43751 ]
 [59.01245 ]
 [59.45977 ]], R is [[58.18218613]
 [58.31570435]
 [58.45758057]
 [58.65996552]
 [58.80165482]].
[2019-04-09 15:25:17,772] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4478627e-11 4.3025441e-09 7.0442883e-07 2.8892153e-12 2.6166845e-02
 6.7387424e-05 9.7375613e-01 2.9162726e-08 7.8735911e-06 9.9075658e-07
 7.1895400e-08], sum to 1.0000
[2019-04-09 15:25:17,773] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8787
[2019-04-09 15:25:17,803] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 40.0, 46.0, 356.5, 19.0, 25.75570363061487, 0.4982546216592266, 0.0, 1.0, 45.0, 26.307905323565528], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4208400.0000, 
sim time next is 4209600.0000, 
raw observation next is [1.9, 40.33333333333334, 31.33333333333333, 227.5, 19.0, 25.72639313728741, 0.4751793571793949, 0.0, 1.0, 45.0, 26.15227608495878], 
processed observation next is [0.0, 0.7391304347826086, 0.515235457063712, 0.40333333333333343, 0.10444444444444442, 0.2513812154696133, 0.08333333333333333, 0.6438660947739508, 0.6583931190597984, 0.0, 1.0, 0.6, 0.2615227608495878], 
reward next is 0.7385, 
noisyNet noise sample is [array([-1.6877786], dtype=float32), 0.69141316]. 
=============================================
[2019-04-09 15:25:17,916] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3006397e-12 1.7975663e-09 1.6996918e-06 8.1673736e-13 4.6040881e-03
 4.9891728e-06 9.9538261e-01 2.8617857e-08 6.5913068e-06 9.3249675e-08
 7.3364239e-09], sum to 1.0000
[2019-04-09 15:25:17,920] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3454
[2019-04-09 15:25:17,922] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9753347e-14 8.6684958e-11 5.3654845e-09 8.6379726e-16 1.4761349e-02
 6.7987762e-06 9.8523140e-01 1.0542403e-08 4.7461771e-07 2.2475643e-08
 7.1747122e-11], sum to 1.0000
[2019-04-09 15:25:17,932] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6293
[2019-04-09 15:25:17,948] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 46.0, 0.0, 0.0, 19.0, 24.86474907819516, 0.332360144518139, 0.0, 1.0, 45.0, 30.422279047985384], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4154400.0000, 
sim time next is 4155600.0000, 
raw observation next is [-2.333333333333333, 47.33333333333334, 0.0, 0.0, 19.0, 24.76502581595396, 0.3093021266052255, 0.0, 1.0, 45.0, 30.466885544938464], 
processed observation next is [0.0, 0.08695652173913043, 0.3979686057248385, 0.47333333333333344, 0.0, 0.0, 0.08333333333333333, 0.5637521513294965, 0.6031007088684085, 0.0, 1.0, 0.6, 0.30466885544938466], 
reward next is 0.6953, 
noisyNet noise sample is [array([-0.804643], dtype=float32), -2.235312]. 
=============================================
[2019-04-09 15:25:17,976] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 28.0, 120.5, 828.5, 22.5, 26.12694998253076, 0.5866549460281781, 1.0, 1.0, 45.0, 24.567526839257006], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4104000.0000, 
sim time next is 4105200.0000, 
raw observation next is [1.666666666666667, 28.33333333333334, 120.1666666666667, 836.8333333333334, 22.5, 26.46315020338074, 0.6179555348447623, 1.0, 1.0, 45.0, 24.532883235424272], 
processed observation next is [1.0, 0.5217391304347826, 0.5087719298245615, 0.2833333333333334, 0.40055555555555566, 0.9246777163904236, 0.375, 0.7052625169483949, 0.7059851782815875, 1.0, 1.0, 0.6, 0.24532883235424272], 
reward next is 0.7547, 
noisyNet noise sample is [array([-1.397891], dtype=float32), -0.36542243]. 
=============================================
[2019-04-09 15:25:18,570] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.1421840e-11 4.1114141e-08 8.7011790e-07 8.8649102e-11 3.0049749e-02
 1.3461236e-04 9.6980506e-01 1.2103593e-06 7.8760522e-06 5.1040200e-07
 1.0237209e-07], sum to 1.0000
[2019-04-09 15:25:18,572] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5481
[2019-04-09 15:25:18,576] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5775744e-12 1.1458648e-09 2.6379034e-06 1.1454485e-11 2.5171220e-02
 2.9748360e-05 9.7479010e-01 7.3510620e-08 5.5882188e-06 5.3930540e-07
 1.1139905e-08], sum to 1.0000
[2019-04-09 15:25:18,579] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8975
[2019-04-09 15:25:18,601] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.7, 41.0, 0.0, 0.0, 19.0, 25.58013075047842, 0.4322616718246599, 0.0, 1.0, 45.0, 26.866989951313744], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4212000.0000, 
sim time next is 4213200.0000, 
raw observation next is [1.6, 41.33333333333334, 0.0, 0.0, 19.0, 25.49673985446849, 0.4112415280815189, 0.0, 1.0, 45.0, 27.361842366644574], 
processed observation next is [0.0, 0.782608695652174, 0.5069252077562327, 0.41333333333333344, 0.0, 0.0, 0.08333333333333333, 0.6247283212057075, 0.6370805093605063, 0.0, 1.0, 0.6, 0.27361842366644573], 
reward next is 0.7264, 
noisyNet noise sample is [array([1.9334549], dtype=float32), -1.2975206]. 
=============================================
[2019-04-09 15:25:18,618] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 35.0, 113.0, 755.0, 19.0, 23.82889872583064, 0.1454667514453972, 0.0, 1.0, 45.0, 28.395632359900723], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4184400.0000, 
sim time next is 4185600.0000, 
raw observation next is [-1.333333333333333, 35.0, 114.8333333333333, 782.0, 19.0, 24.03622558327923, 0.1738556819601352, 0.0, 1.0, 45.0, 27.41717909027808], 
processed observation next is [0.0, 0.43478260869565216, 0.42566943674976926, 0.35, 0.38277777777777766, 0.8640883977900552, 0.08333333333333333, 0.5030187986066025, 0.5579518939867117, 0.0, 1.0, 0.6, 0.27417179090278077], 
reward next is 0.7258, 
noisyNet noise sample is [array([-1.2533921], dtype=float32), 1.3255482]. 
=============================================
[2019-04-09 15:25:18,666] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0857412e-13 2.5563331e-11 4.2692758e-08 6.7226322e-14 2.0294134e-03
 2.0730980e-05 9.9794966e-01 2.4607685e-09 2.1873883e-07 1.6159127e-08
 7.8737339e-10], sum to 1.0000
[2019-04-09 15:25:18,666] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6988
[2019-04-09 15:25:18,697] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.1855222e-12 1.4543311e-09 3.0509784e-07 3.8911248e-12 8.5318498e-03
 8.7250221e-05 9.9137878e-01 5.9648251e-08 1.3355708e-06 2.5916935e-07
 8.1250171e-09], sum to 1.0000
[2019-04-09 15:25:18,703] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4642
[2019-04-09 15:25:18,705] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.933333333333334, 57.33333333333333, 108.3333333333333, 638.5, 19.0, 25.97691474002625, 0.5883332099182053, 0.0, 1.0, 45.0, 25.47732136076442], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4290000.0000, 
sim time next is 4291200.0000, 
raw observation next is [7.0, 56.0, 93.0, 605.5, 19.0, 26.08984566910039, 0.6100292644174345, 0.0, 1.0, 45.0, 25.406494238377384], 
processed observation next is [0.0, 0.6956521739130435, 0.6565096952908588, 0.56, 0.31, 0.669060773480663, 0.08333333333333333, 0.6741538057583659, 0.7033430881391448, 0.0, 1.0, 0.6, 0.25406494238377386], 
reward next is 0.7459, 
noisyNet noise sample is [array([-0.23340708], dtype=float32), 0.5858865]. 
=============================================
[2019-04-09 15:25:18,726] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 41.33333333333334, 0.0, 0.0, 19.0, 25.5084527832957, 0.4133314995656717, 0.0, 1.0, 45.0, 27.34242133444084], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4213200.0000, 
sim time next is 4214400.0000, 
raw observation next is [1.5, 41.66666666666667, 0.0, 0.0, 19.0, 25.41524525488893, 0.3916558869092482, 0.0, 1.0, 45.0, 27.778996673359778], 
processed observation next is [0.0, 0.782608695652174, 0.5041551246537397, 0.41666666666666674, 0.0, 0.0, 0.08333333333333333, 0.6179371045740775, 0.6305519623030827, 0.0, 1.0, 0.6, 0.2777899667335978], 
reward next is 0.7222, 
noisyNet noise sample is [array([0.8453304], dtype=float32), -1.1216186]. 
=============================================
[2019-04-09 15:25:19,231] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.6412627e-15 1.8608523e-11 1.3712740e-08 6.4505057e-15 2.5140878e-03
 1.9467341e-05 9.9746633e-01 1.2502147e-09 1.4573942e-07 1.6144178e-09
 4.1442624e-10], sum to 1.0000
[2019-04-09 15:25:19,231] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0102
[2019-04-09 15:25:19,252] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 37.33333333333334, 0.0, 0.0, 19.0, 26.51860123977042, 0.735569720577547, 0.0, 1.0, 45.0, 25.855739420374384], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4137600.0000, 
sim time next is 4138800.0000, 
raw observation next is [1.0, 38.66666666666666, 0.0, 0.0, 19.0, 26.45469377865105, 0.7247651222119855, 0.0, 1.0, 45.0, 26.022818818341037], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.38666666666666655, 0.0, 0.0, 0.08333333333333333, 0.7045578148875874, 0.7415883740706618, 0.0, 1.0, 0.6, 0.26022818818341037], 
reward next is 0.7398, 
noisyNet noise sample is [array([0.0592903], dtype=float32), -0.76421446]. 
=============================================
[2019-04-09 15:25:19,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.7976576e-11 6.3343826e-08 3.9342842e-07 2.9558654e-11 9.3202613e-02
 8.3241786e-05 9.0669394e-01 4.0311278e-08 1.8447940e-05 9.8578585e-07
 2.7466805e-07], sum to 1.0000
[2019-04-09 15:25:19,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1515
[2019-04-09 15:25:19,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.6769201e-11 4.2026219e-08 8.8519937e-06 1.5720368e-10 9.5832765e-02
 4.7968727e-04 9.0365893e-01 6.8080658e-07 1.7628197e-05 1.2448603e-06
 1.4657837e-07], sum to 1.0000
[2019-04-09 15:25:19,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7674
[2019-04-09 15:25:19,876] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.666666666666667, 53.66666666666667, 185.6666666666667, 209.8333333333333, 19.0, 24.17184132368206, 0.1169581702019544, 0.0, 1.0, 45.0, 30.292731998607003], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4268400.0000, 
sim time next is 4269600.0000, 
raw observation next is [4.0, 54.0, 193.0, 367.5, 19.0, 24.30567748441722, 0.1600141046497295, 0.0, 1.0, 45.0, 30.14495681677697], 
processed observation next is [0.0, 0.43478260869565216, 0.5734072022160666, 0.54, 0.6433333333333333, 0.40607734806629836, 0.08333333333333333, 0.5254731237014351, 0.5533380348832432, 0.0, 1.0, 0.6, 0.3014495681677697], 
reward next is 0.6986, 
noisyNet noise sample is [array([0.12143864], dtype=float32), 0.8240757]. 
=============================================
[2019-04-09 15:25:19,885] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 47.33333333333334, 0.0, 0.0, 19.0, 24.80254555392274, 0.314718501555285, 0.0, 1.0, 45.0, 30.324388244837778], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4155600.0000, 
sim time next is 4156800.0000, 
raw observation next is [-2.666666666666667, 48.66666666666666, 0.0, 0.0, 19.0, 24.69561585728331, 0.2906310354270982, 0.0, 1.0, 45.0, 30.60709019492697], 
processed observation next is [0.0, 0.08695652173913043, 0.38873499538319484, 0.4866666666666666, 0.0, 0.0, 0.08333333333333333, 0.5579679881069426, 0.5968770118090327, 0.0, 1.0, 0.6, 0.30607090194926967], 
reward next is 0.6939, 
noisyNet noise sample is [array([-0.00875137], dtype=float32), 0.22358648]. 
=============================================
[2019-04-09 15:25:19,902] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.32435150e-17 7.73576420e-14 1.09153957e-10 5.02377729e-16
 1.22890554e-01 8.28863540e-06 8.77101183e-01 1.39627797e-11
 1.08466276e-08 5.16713872e-09 4.26928284e-11], sum to 1.0000
[2019-04-09 15:25:19,904] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1301
[2019-04-09 15:25:19,952] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0095316e-12 7.3091759e-09 2.7624361e-07 9.5669785e-11 4.4135023e-02
 5.4839038e-04 9.5530599e-01 1.0837873e-06 7.2961634e-06 1.8535467e-06
 6.6640261e-08], sum to 1.0000
[2019-04-09 15:25:19,954] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 88.0, 108.6666666666667, 0.0, 22.5, 27.97183469188929, 1.015989401098821, 1.0, 1.0, 45.0, 27.503296824471395], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4450800.0000, 
sim time next is 4452000.0000, 
raw observation next is [0.3333333333333334, 90.0, 117.6666666666667, 0.9999999999999998, 22.5, 27.8527772806141, 0.8497420338700694, 1.0, 1.0, 45.0, 31.304487734619528], 
processed observation next is [1.0, 0.5217391304347826, 0.4718374884579871, 0.9, 0.3922222222222223, 0.0011049723756906074, 0.375, 0.8210647733845082, 0.7832473446233564, 1.0, 1.0, 0.6, 0.3130448773461953], 
reward next is 0.6870, 
noisyNet noise sample is [array([0.44687012], dtype=float32), -1.4462197]. 
=============================================
[2019-04-09 15:25:19,956] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2517
[2019-04-09 15:25:19,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.6956  ]
 [76.56435 ]
 [76.464325]
 [75.95196 ]
 [75.4883  ]], R is [[76.84532928]
 [76.80184174]
 [76.76496887]
 [76.72657013]
 [76.70281982]].
[2019-04-09 15:25:19,983] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 41.66666666666667, 0.0, 0.0, 19.0, 25.40339121008601, 0.3889542564281394, 0.0, 1.0, 45.0, 27.787480133526778], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4214400.0000, 
sim time next is 4215600.0000, 
raw observation next is [1.4, 42.0, 0.0, 0.0, 19.0, 25.32910833401508, 0.3751786783635768, 0.0, 1.0, 45.0, 28.14771154580108], 
processed observation next is [0.0, 0.8260869565217391, 0.5013850415512465, 0.42, 0.0, 0.0, 0.08333333333333333, 0.6107590278345899, 0.6250595594545256, 0.0, 1.0, 0.6, 0.2814771154580108], 
reward next is 0.7185, 
noisyNet noise sample is [array([-1.1059504], dtype=float32), 0.8537338]. 
=============================================
[2019-04-09 15:25:19,986] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2604913e-09 7.7064705e-08 1.6150183e-05 4.2225196e-10 1.7433436e-01
 2.0889419e-03 8.2349098e-01 7.6787619e-06 5.2441446e-05 8.6117061e-06
 7.2970533e-07], sum to 1.0000
[2019-04-09 15:25:19,987] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4708
[2019-04-09 15:25:20,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.38501589e-11 2.05068607e-09 7.83757116e-07 8.69447830e-12
 9.16237310e-02 2.71555880e-04 9.08093750e-01 1.15902445e-07
 6.60918386e-06 3.40940142e-06 1.55241047e-08], sum to 1.0000
[2019-04-09 15:25:20,005] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9140
[2019-04-09 15:25:20,022] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.0, 24.50861727987301, 0.2546354676895399, 0.0, 1.0, 45.0, 31.03530955695145], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4159200.0000, 
sim time next is 4160400.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 19.0, 24.49087470429885, 0.2367242545283982, 0.0, 1.0, 45.0, 31.15793039919206], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.08333333333333333, 0.5409062253582375, 0.5789080848427994, 0.0, 1.0, 0.6, 0.31157930399192063], 
reward next is 0.6884, 
noisyNet noise sample is [array([0.2961106], dtype=float32), -0.31143776]. 
=============================================
[2019-04-09 15:25:20,036] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 36.0, 198.0, 698.6666666666666, 19.0, 24.89876572781386, 0.3705424186605983, 0.0, 1.0, 45.0, 24.535705182222593], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4195200.0000, 
sim time next is 4196400.0000, 
raw observation next is [2.0, 38.0, 209.5, 572.3333333333334, 19.0, 24.97756095376513, 0.3890348056171285, 0.0, 1.0, 45.0, 24.562047289165395], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.38, 0.6983333333333334, 0.6324125230202579, 0.08333333333333333, 0.5814634128137609, 0.6296782685390429, 0.0, 1.0, 0.6, 0.24562047289165395], 
reward next is 0.7544, 
noisyNet noise sample is [array([0.00219811], dtype=float32), -0.38787517]. 
=============================================
[2019-04-09 15:25:20,597] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7078759e-11 7.1476447e-10 2.0943105e-07 9.8735933e-12 1.2248233e-03
 2.6374924e-05 9.9874640e-01 3.1140907e-08 1.9944039e-06 1.8247121e-07
 1.8149922e-08], sum to 1.0000
[2019-04-09 15:25:20,598] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2620
[2019-04-09 15:25:20,624] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 19.0, 24.42588394136925, 0.1689721556528121, 0.0, 1.0, 45.0, 31.28968577459021], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4233600.0000, 
sim time next is 4234800.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 19.0, 24.39891544674879, 0.1600392796892624, 0.0, 1.0, 45.0, 31.49406025018531], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.08333333333333333, 0.5332429538957326, 0.5533464265630875, 0.0, 1.0, 0.6, 0.3149406025018531], 
reward next is 0.6851, 
noisyNet noise sample is [array([0.22196595], dtype=float32), -1.029742]. 
=============================================
[2019-04-09 15:25:20,647] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3166867e-17 2.0665601e-13 4.2029266e-10 8.0232986e-17 1.4887000e-02
 1.4240541e-06 9.8511153e-01 4.9929436e-11 4.0777870e-08 7.4715165e-11
 1.4022179e-12], sum to 1.0000
[2019-04-09 15:25:20,647] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3693
[2019-04-09 15:25:20,719] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 78.0, 45.0, 9.166666666666664, 22.5, 26.88102816137348, 0.8656031497040672, 1.0, 1.0, 45.0, 34.18721768095253], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4466400.0000, 
sim time next is 4467600.0000, 
raw observation next is [0.0, 78.0, 37.0, 27.5, 22.5, 26.9806556604304, 0.8797095628002763, 1.0, 1.0, 45.0, 31.534275942084786], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.78, 0.12333333333333334, 0.03038674033149171, 0.375, 0.7483879717025334, 0.7932365209334254, 1.0, 1.0, 0.6, 0.3153427594208479], 
reward next is 0.6847, 
noisyNet noise sample is [array([-0.22051322], dtype=float32), -1.2543797]. 
=============================================
[2019-04-09 15:25:20,740] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2440452e-12 7.9431608e-09 3.2997750e-07 5.6066614e-12 2.7863839e-01
 3.3911548e-04 7.2101569e-01 5.8664366e-08 6.1821270e-06 1.9104665e-07
 8.7182999e-09], sum to 1.0000
[2019-04-09 15:25:20,740] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6003
[2019-04-09 15:25:20,765] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.666666666666667, 54.0, 134.8333333333333, 785.6666666666666, 19.0, 24.76662859507893, 0.2973779736716399, 0.0, 1.0, 45.0, 26.932004013648054], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4274400.0000, 
sim time next is 4275600.0000, 
raw observation next is [6.333333333333333, 53.0, 120.8333333333333, 826.1666666666666, 19.0, 24.89005470225285, 0.324853351936968, 0.0, 1.0, 45.0, 26.31202844222767], 
processed observation next is [0.0, 0.4782608695652174, 0.6380424746075716, 0.53, 0.4027777777777777, 0.9128913443830571, 0.08333333333333333, 0.5741712251877376, 0.6082844506456561, 0.0, 1.0, 0.6, 0.2631202844222767], 
reward next is 0.7369, 
noisyNet noise sample is [array([-0.30626535], dtype=float32), 1.2620754]. 
=============================================
[2019-04-09 15:25:22,631] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1038351e-15 1.3228163e-11 1.5382822e-08 4.9638169e-16 2.0182515e-02
 5.3540289e-06 9.7981197e-01 4.9230803e-10 9.0202228e-08 1.4881870e-08
 4.8972693e-10], sum to 1.0000
[2019-04-09 15:25:22,631] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7666
[2019-04-09 15:25:22,691] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.86666666666667, 39.33333333333334, 113.6666666666667, 762.8333333333333, 22.5, 27.44504715365829, 0.8333665578212104, 1.0, 1.0, 45.0, 21.637866824121673], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4357200.0000, 
sim time next is 4358400.0000, 
raw observation next is [11.73333333333333, 36.66666666666666, 115.8333333333333, 788.0, 22.5, 27.64047410716834, 0.8853224412586534, 1.0, 1.0, 45.0, 19.980660233263343], 
processed observation next is [1.0, 0.43478260869565216, 0.7876269621421976, 0.3666666666666666, 0.386111111111111, 0.8707182320441988, 0.375, 0.8033728422640284, 0.7951074804195511, 1.0, 1.0, 0.6, 0.19980660233263342], 
reward next is 0.8002, 
noisyNet noise sample is [array([-0.79172343], dtype=float32), 0.43785352]. 
=============================================
[2019-04-09 15:25:22,900] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.5304792e-10 3.5568839e-08 3.5756884e-06 1.0905238e-09 3.5541445e-01
 5.7459343e-04 6.4398474e-01 3.3911131e-06 1.6348253e-05 2.7548056e-06
 9.8869528e-08], sum to 1.0000
[2019-04-09 15:25:22,905] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1503
[2019-04-09 15:25:22,951] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 24.18743747318706, 0.09830799587121153, 0.0, 1.0, 45.0, 32.01437594936048], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4245600.0000, 
sim time next is 4246800.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 24.16211925912904, 0.08215085625803181, 0.0, 1.0, 35.0, 25.025629115314125], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.5135099382607532, 0.5273836187526773, 0.0, 1.0, 0.4, 0.25025629115314124], 
reward next is 0.7497, 
noisyNet noise sample is [array([-0.5926786], dtype=float32), -0.6857163]. 
=============================================
[2019-04-09 15:25:23,465] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0972239e-15 1.8435240e-12 4.6445376e-09 1.3119283e-15 6.1622924e-01
 1.4616639e-05 3.8375613e-01 8.0181327e-11 5.4484367e-10 2.9091061e-09
 5.4855554e-11], sum to 1.0000
[2019-04-09 15:25:23,465] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5613
[2019-04-09 15:25:23,497] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.766666666666666, 47.0, 108.3333333333333, 694.1666666666667, 22.5, 26.8332663807485, 0.7064759001719683, 1.0, 1.0, 35.0, 20.654236016380125], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4354800.0000, 
sim time next is 4356000.0000, 
raw observation next is [10.0, 42.0, 111.0, 728.5, 22.5, 27.15861166728789, 0.7624807067455869, 1.0, 1.0, 35.0, 17.618380608486618], 
processed observation next is [1.0, 0.43478260869565216, 0.739612188365651, 0.42, 0.37, 0.8049723756906078, 0.375, 0.7632176389406574, 0.7541602355818623, 1.0, 1.0, 0.4, 0.17618380608486617], 
reward next is 0.8238, 
noisyNet noise sample is [array([-0.6778061], dtype=float32), -0.044159744]. 
=============================================
[2019-04-09 15:25:23,512] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[70.738495]
 [69.149124]
 [67.8139  ]
 [66.355965]
 [64.50817 ]], R is [[72.25804138]
 [72.32891846]
 [72.32840729]
 [72.32781219]
 [72.37338257]].
[2019-04-09 15:25:23,632] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0431060e-14 2.6562372e-10 1.0257666e-08 1.1088509e-13 3.9036125e-03
 2.8544855e-05 9.9606740e-01 3.7080292e-09 3.4732739e-07 6.9123097e-08
 2.5772906e-09], sum to 1.0000
[2019-04-09 15:25:23,633] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1932
[2019-04-09 15:25:23,663] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.166666666666667, 71.33333333333333, 0.0, 0.0, 19.0, 25.39810640765725, 0.3613686381390049, 0.0, 1.0, 45.0, 30.742406890493562], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4329600.0000, 
sim time next is 4330800.0000, 
raw observation next is [4.0, 71.0, 0.0, 0.0, 19.0, 25.31233226022709, 0.3419982239155451, 0.0, 1.0, 45.0, 30.779692219735715], 
processed observation next is [1.0, 0.13043478260869565, 0.5734072022160666, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6093610216855909, 0.6139994079718484, 0.0, 1.0, 0.6, 0.30779692219735716], 
reward next is 0.6922, 
noisyNet noise sample is [array([0.965375], dtype=float32), 0.15262133]. 
=============================================
[2019-04-09 15:25:24,073] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8961205e-13 2.2635785e-10 2.3682883e-08 4.6543981e-13 3.7928047e-03
 5.7075315e-05 9.9614727e-01 8.0506899e-09 2.7903127e-06 3.6472450e-08
 2.5495537e-09], sum to 1.0000
[2019-04-09 15:25:24,074] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4234
[2019-04-09 15:25:24,099] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.166666666666667, 72.33333333333334, 0.0, 0.0, 19.0, 25.06669668715379, 0.2845723370537853, 0.0, 1.0, 45.0, 30.91531881604992], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4342800.0000, 
sim time next is 4344000.0000, 
raw observation next is [3.033333333333333, 73.66666666666667, 0.0, 0.0, 19.0, 24.96772978636976, 0.2693123502895435, 0.0, 1.0, 45.0, 30.90862305203997], 
processed observation next is [1.0, 0.2608695652173913, 0.5466297322253002, 0.7366666666666667, 0.0, 0.0, 0.08333333333333333, 0.5806441488641466, 0.5897707834298478, 0.0, 1.0, 0.6, 0.3090862305203997], 
reward next is 0.6909, 
noisyNet noise sample is [array([-0.17607239], dtype=float32), -0.37411982]. 
=============================================
[2019-04-09 15:25:24,106] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[58.132774]
 [57.938477]
 [57.983288]
 [58.04249 ]
 [57.86943 ]], R is [[58.25068665]
 [58.35902405]
 [58.4654808 ]
 [58.57033157]
 [58.67393112]].
[2019-04-09 15:25:24,445] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.82873106e-16 1.09051635e-12 2.05161643e-09 7.49506209e-16
 1.26288354e-03 1.87307467e-06 9.98735130e-01 1.22918092e-10
 6.73869351e-08 3.71444292e-10 1.10549495e-11], sum to 1.0000
[2019-04-09 15:25:24,446] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6898
[2019-04-09 15:25:24,481] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.533333333333333, 52.00000000000001, 104.5, 646.0, 22.5, 26.19429271696137, 0.6002237536844778, 1.0, 1.0, 45.0, 27.33642685658304], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4353600.0000, 
sim time next is 4354800.0000, 
raw observation next is [8.766666666666666, 47.0, 108.3333333333333, 694.1666666666667, 22.5, 26.74769971587781, 0.6913926950794806, 1.0, 1.0, 45.0, 24.44236274115643], 
processed observation next is [1.0, 0.391304347826087, 0.7054478301015698, 0.47, 0.361111111111111, 0.767034990791897, 0.375, 0.7289749763231509, 0.7304642316931602, 1.0, 1.0, 0.6, 0.2444236274115643], 
reward next is 0.7556, 
noisyNet noise sample is [array([0.5733236], dtype=float32), 0.42045757]. 
=============================================
[2019-04-09 15:25:24,490] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1718111e-14 1.3387152e-10 3.2613954e-08 2.2158233e-14 3.1937388e-01
 2.1092335e-05 6.8060482e-01 4.2499293e-09 2.5274645e-07 1.6479440e-08
 6.0067240e-10], sum to 1.0000
[2019-04-09 15:25:24,491] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9324
[2019-04-09 15:25:24,524] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 58.33333333333334, 0.0, 0.0, 19.0, 25.98277588105393, 0.6389710536021159, 0.0, 1.0, 45.0, 27.479259099864997], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4569600.0000, 
sim time next is 4570800.0000, 
raw observation next is [1.333333333333333, 59.66666666666667, 0.0, 0.0, 19.0, 25.93944207218308, 0.6284439037326073, 0.0, 1.0, 45.0, 27.125352459415666], 
processed observation next is [1.0, 0.9130434782608695, 0.4995383194829178, 0.5966666666666667, 0.0, 0.0, 0.08333333333333333, 0.6616201726819234, 0.7094813012442024, 0.0, 1.0, 0.6, 0.27125352459415664], 
reward next is 0.7287, 
noisyNet noise sample is [array([0.40256447], dtype=float32), 1.4583745]. 
=============================================
[2019-04-09 15:25:24,660] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.3885744e-15 1.8360669e-11 1.9403602e-08 4.0793623e-15 3.7937663e-03
 1.0236284e-05 9.9619603e-01 1.6883265e-09 5.0803923e-08 5.0105822e-09
 2.2439407e-10], sum to 1.0000
[2019-04-09 15:25:24,661] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7441
[2019-04-09 15:25:24,682] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.9, 62.66666666666667, 0.0, 0.0, 19.0, 27.3941479958084, 0.9745826270349615, 0.0, 1.0, 45.0, 26.92008172145755], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4405200.0000, 
sim time next is 4406400.0000, 
raw observation next is [7.6, 63.0, 0.0, 0.0, 19.0, 27.33237074361475, 0.9612176454378253, 0.0, 1.0, 45.0, 27.034993803205943], 
processed observation next is [1.0, 0.0, 0.6731301939058172, 0.63, 0.0, 0.0, 0.08333333333333333, 0.7776975619678957, 0.8204058818126084, 0.0, 1.0, 0.6, 0.2703499380320594], 
reward next is 0.7297, 
noisyNet noise sample is [array([-0.09205122], dtype=float32), -0.6589754]. 
=============================================
[2019-04-09 15:25:24,910] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3854994e-12 1.0408048e-09 5.6448926e-07 5.1991250e-12 5.7657938e-02
 2.6092361e-04 9.4207746e-01 1.8733227e-08 2.9153302e-06 1.1281539e-07
 5.1114255e-09], sum to 1.0000
[2019-04-09 15:25:24,910] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4460
[2019-04-09 15:25:24,938] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.4, 73.0, 0.0, 0.0, 19.0, 25.9027152641429, 0.514416155771588, 0.0, 1.0, 45.0, 28.54669615269904], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4305600.0000, 
sim time next is 4306800.0000, 
raw observation next is [5.300000000000001, 73.0, 0.0, 0.0, 19.0, 25.87971207753949, 0.5085456260653811, 0.0, 1.0, 45.0, 28.656833278021566], 
processed observation next is [0.0, 0.8695652173913043, 0.6094182825484765, 0.73, 0.0, 0.0, 0.08333333333333333, 0.6566426731282909, 0.6695152086884604, 0.0, 1.0, 0.6, 0.28656833278021565], 
reward next is 0.7134, 
noisyNet noise sample is [array([-2.2166607], dtype=float32), -0.5781573]. 
=============================================
[2019-04-09 15:25:25,053] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2726905e-13 1.8479096e-09 3.5420550e-07 1.9747591e-12 6.3094214e-02
 2.0949248e-05 9.3688303e-01 4.2742062e-08 1.2098316e-06 2.6925801e-07
 1.7449940e-08], sum to 1.0000
[2019-04-09 15:25:25,055] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0353
[2019-04-09 15:25:25,084] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.34493025e-17 9.87083570e-13 1.25387548e-10 2.07543040e-17
 2.57862750e-02 1.39941108e-07 9.74213660e-01 1.37039390e-11
 1.63500182e-08 3.92695987e-10 1.87798843e-12], sum to 1.0000
[2019-04-09 15:25:25,093] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4166
[2019-04-09 15:25:25,119] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.1, 73.0, 0.0, 0.0, 19.0, 25.78566476797291, 0.4865256626079133, 0.0, 1.0, 45.0, 28.862296122971294], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4309200.0000, 
sim time next is 4310400.0000, 
raw observation next is [5.0, 74.0, 0.0, 0.0, 19.0, 25.726172993623, 0.4679902561664029, 0.0, 1.0, 35.0, 22.645555774567597], 
processed observation next is [0.0, 0.9130434782608695, 0.6011080332409973, 0.74, 0.0, 0.0, 0.08333333333333333, 0.6438477494685833, 0.6559967520554676, 0.0, 1.0, 0.4, 0.22645555774567597], 
reward next is 0.7735, 
noisyNet noise sample is [array([-0.91003734], dtype=float32), -2.0864062]. 
=============================================
[2019-04-09 15:25:25,127] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 135.3333333333333, 0.0, 22.5, 27.99582496388075, 1.023294430995996, 1.0, 1.0, 45.0, 26.893640738825553], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4448400.0000, 
sim time next is 4449600.0000, 
raw observation next is [1.0, 86.0, 122.0, 0.0, 22.5, 28.00271123025881, 1.026766005284922, 1.0, 1.0, 45.0, 26.71419893091217], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.4066666666666667, 0.0, 0.375, 0.8335592691882342, 0.8422553350949741, 1.0, 1.0, 0.6, 0.2671419893091217], 
reward next is 0.7329, 
noisyNet noise sample is [array([-0.23638642], dtype=float32), -0.040358577]. 
=============================================
[2019-04-09 15:25:25,502] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6793862e-13 3.3054033e-09 3.9570801e-07 4.6372385e-12 3.9777508e-01
 7.6186028e-05 6.0213482e-01 1.0745729e-07 1.3096891e-05 1.8988450e-07
 3.1295244e-08], sum to 1.0000
[2019-04-09 15:25:25,504] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0199
[2019-04-09 15:25:25,527] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.033333333333333, 73.66666666666667, 0.0, 0.0, 19.0, 25.06457721158434, 0.2929869506315125, 0.0, 1.0, 45.0, 30.80312805042047], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4344000.0000, 
sim time next is 4345200.0000, 
raw observation next is [2.9, 75.0, 0.0, 0.0, 19.0, 25.02013389085554, 0.2830404165324727, 0.0, 1.0, 35.0, 23.9956411958761], 
processed observation next is [1.0, 0.30434782608695654, 0.5429362880886427, 0.75, 0.0, 0.0, 0.08333333333333333, 0.585011157571295, 0.5943468055108242, 0.0, 1.0, 0.4, 0.239956411958761], 
reward next is 0.7600, 
noisyNet noise sample is [array([-0.82421005], dtype=float32), 0.8514608]. 
=============================================
[2019-04-09 15:25:25,809] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1590703e-16 1.8826720e-12 1.4072024e-09 2.7100650e-16 1.4445803e-03
 1.9525924e-06 9.9855345e-01 2.2411815e-11 5.4074434e-08 6.5246891e-11
 8.6926612e-12], sum to 1.0000
[2019-04-09 15:25:25,818] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0377
[2019-04-09 15:25:25,878] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 25.9107866346101, 0.6721864665112621, 0.0, 1.0, 45.0, 32.61540038772185], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4485600.0000, 
sim time next is 4486800.0000, 
raw observation next is [-0.09999999999999999, 72.0, 0.0, 0.0, 19.0, 25.844664150301, 0.5953388544099377, 0.0, 1.0, 45.0, 33.40322007341979], 
processed observation next is [1.0, 0.9565217391304348, 0.4598337950138504, 0.72, 0.0, 0.0, 0.08333333333333333, 0.6537220125250833, 0.6984462848033126, 0.0, 1.0, 0.6, 0.33403220073419787], 
reward next is 0.6660, 
noisyNet noise sample is [array([0.552847], dtype=float32), -1.8933026]. 
=============================================
[2019-04-09 15:25:26,298] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2422264e-13 1.0381290e-09 9.1023324e-08 8.4251304e-14 2.6208559e-02
 9.1670903e-05 9.7369927e-01 1.6140747e-08 3.6422213e-07 7.1061734e-09
 7.8653334e-10], sum to 1.0000
[2019-04-09 15:25:26,300] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9744
[2019-04-09 15:25:26,338] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 19.0, 25.22344940857174, 0.4739651204431688, 0.0, 1.0, 45.0, 33.83905687894631], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4496400.0000, 
sim time next is 4497600.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 19.0, 25.2928867625669, 0.4614153636389518, 0.0, 1.0, 45.0, 33.78639895925446], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.08333333333333333, 0.6077405635472418, 0.653805121212984, 0.0, 1.0, 0.6, 0.3378639895925446], 
reward next is 0.6621, 
noisyNet noise sample is [array([0.84932303], dtype=float32), -0.7352803]. 
=============================================
[2019-04-09 15:25:26,499] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8029463e-15 2.7468035e-11 5.2351967e-10 1.3992666e-14 8.5816517e-02
 2.2366921e-06 9.1418099e-01 1.4426391e-09 1.7008253e-07 3.7497014e-09
 5.7216315e-11], sum to 1.0000
[2019-04-09 15:25:26,499] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9914
[2019-04-09 15:25:26,524] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.9, 62.66666666666667, 0.0, 0.0, 19.0, 27.35725522425899, 0.964942924404995, 0.0, 1.0, 45.0, 26.96559364645628], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4405200.0000, 
sim time next is 4406400.0000, 
raw observation next is [7.6, 63.0, 0.0, 0.0, 19.0, 27.29454822595969, 0.9515301780186914, 0.0, 1.0, 45.0, 27.07731371295797], 
processed observation next is [1.0, 0.0, 0.6731301939058172, 0.63, 0.0, 0.0, 0.08333333333333333, 0.7745456854966409, 0.8171767260062305, 0.0, 1.0, 0.6, 0.2707731371295797], 
reward next is 0.7292, 
noisyNet noise sample is [array([0.87391686], dtype=float32), -0.56325245]. 
=============================================
[2019-04-09 15:25:26,726] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.67818905e-12 9.35813649e-10 1.03195376e-07 8.19979609e-13
 3.49590555e-02 2.83018508e-05 9.65012193e-01 2.72839831e-08
 2.40172227e-07 9.42230116e-08 1.43905909e-09], sum to 1.0000
[2019-04-09 15:25:26,728] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8425
[2019-04-09 15:25:26,729] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2107747e-16 9.5717470e-12 6.7452932e-10 5.9315340e-16 7.9939581e-02
 1.6299633e-05 9.2004412e-01 7.0419304e-10 2.0813228e-08 8.7780654e-09
 1.7277083e-10], sum to 1.0000
[2019-04-09 15:25:26,730] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9515
[2019-04-09 15:25:26,758] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 73.0, 20.5, 28.49999999999999, 22.5, 24.0966999466029, 0.1572167051713109, 1.0, 1.0, 35.0, 22.43350166889048], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4606800.0000, 
sim time next is 4608000.0000, 
raw observation next is [-2.0, 71.0, 61.5, 85.5, 22.5, 24.19144936665005, 0.184846906616789, 1.0, 1.0, 45.0, 28.234078043593648], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.205, 0.09447513812154697, 0.375, 0.5159541138875042, 0.5616156355389297, 1.0, 1.0, 0.6, 0.28234078043593647], 
reward next is 0.7177, 
noisyNet noise sample is [array([0.3173402], dtype=float32), -2.4015853]. 
=============================================
[2019-04-09 15:25:26,774] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[57.41879 ]
 [56.721897]
 [56.50026 ]
 [56.45226 ]
 [56.398575]], R is [[58.6075325 ]
 [58.79712296]
 [58.92202759]
 [59.04663467]
 [59.17054367]].
[2019-04-09 15:25:26,797] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 78.0, 45.0, 9.166666666666664, 22.5, 26.91828265617313, 0.8710641349788211, 1.0, 1.0, 45.0, 34.25335179811179], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4466400.0000, 
sim time next is 4467600.0000, 
raw observation next is [0.0, 78.0, 37.0, 27.5, 22.5, 26.97745972918517, 0.8830165291076333, 1.0, 1.0, 45.0, 31.168001126433875], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.78, 0.12333333333333334, 0.03038674033149171, 0.375, 0.7481216440987642, 0.7943388430358778, 1.0, 1.0, 0.6, 0.31168001126433875], 
reward next is 0.6883, 
noisyNet noise sample is [array([2.1623347], dtype=float32), -1.948703]. 
=============================================
[2019-04-09 15:25:26,855] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6647287e-17 1.6961984e-13 4.4722284e-10 4.7475786e-17 7.2637326e-03
 3.9397182e-06 9.9273235e-01 4.7109525e-11 1.8142805e-08 1.3742288e-09
 3.4715837e-12], sum to 1.0000
[2019-04-09 15:25:26,855] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7106
[2019-04-09 15:25:26,892] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.9228506e-17 4.3542336e-12 4.0362491e-09 2.8619388e-16 1.0664637e-01
 1.3582221e-04 8.9321762e-01 3.1970888e-11 9.7878008e-08 1.2669870e-09
 2.1222560e-10], sum to 1.0000
[2019-04-09 15:25:26,900] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 74.0, 20.83333333333334, 45.83333333333334, 22.5, 27.30425990705671, 0.8999049845174006, 1.0, 1.0, 45.0, 33.97215132652542], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4470000.0000, 
sim time next is 4471200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 27.16467452283611, 0.9091349854242853, 1.0, 1.0, 45.0, 28.408988410390215], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.763722876903009, 0.8030449951414284, 1.0, 1.0, 0.6, 0.2840898841039021], 
reward next is 0.7159, 
noisyNet noise sample is [array([-0.15568854], dtype=float32), 0.867546]. 
=============================================
[2019-04-09 15:25:26,901] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4258
[2019-04-09 15:25:26,932] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 180.3333333333333, 5.0, 22.5, 25.46632832800999, 0.8160096786486962, 1.0, 1.0, 45.0, 30.39710180705104], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4454400.0000, 
sim time next is 4455600.0000, 
raw observation next is [0.0, 92.0, 177.5, 5.0, 22.5, 27.21581457574566, 0.9308912244909017, 1.0, 1.0, 45.0, 30.160573448686634], 
processed observation next is [1.0, 0.5652173913043478, 0.46260387811634357, 0.92, 0.5916666666666667, 0.0055248618784530384, 0.375, 0.7679845479788051, 0.8102970748303006, 1.0, 1.0, 0.6, 0.30160573448686634], 
reward next is 0.6984, 
noisyNet noise sample is [array([0.14784995], dtype=float32), 0.73820454]. 
=============================================
[2019-04-09 15:25:27,162] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5607021e-15 5.3706883e-12 2.7891878e-10 3.6131098e-16 4.8151114e-03
 1.9991817e-06 9.9518281e-01 1.0080582e-09 1.2447413e-08 8.2251042e-09
 9.5813383e-12], sum to 1.0000
[2019-04-09 15:25:27,162] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1212
[2019-04-09 15:25:27,192] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 26.07480827369208, 0.7070987710643362, 1.0, 1.0, 45.0, 33.39207494396216], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4477200.0000, 
sim time next is 4478400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 26.06697195364463, 0.7030924213321367, 1.0, 1.0, 45.0, 33.36035345606825], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.6722476628037191, 0.7343641404440455, 1.0, 1.0, 0.6, 0.33360353456068254], 
reward next is 0.6664, 
noisyNet noise sample is [array([0.12071206], dtype=float32), 0.02607526]. 
=============================================
[2019-04-09 15:25:27,286] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3725601e-16 9.4762522e-13 1.6288548e-09 1.0095906e-16 1.4353842e-03
 7.4419711e-07 9.9856395e-01 2.6326781e-11 4.3947658e-08 1.4816662e-10
 1.0995770e-12], sum to 1.0000
[2019-04-09 15:25:27,286] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3447
[2019-04-09 15:25:27,332] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 26.08320999861344, 0.7053515605016051, 1.0, 1.0, 45.0, 33.35873494941356], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4478400.0000, 
sim time next is 4479600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.04699864556449, 0.693990468460405, 0.0, 1.0, 45.0, 33.286939411926056], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.6705832204637074, 0.7313301561534683, 0.0, 1.0, 0.6, 0.3328693941192606], 
reward next is 0.6671, 
noisyNet noise sample is [array([-1.1615319], dtype=float32), -0.33045387]. 
=============================================
[2019-04-09 15:25:27,362] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.5591600e-13 5.3296578e-10 4.2213163e-08 1.3732780e-13 2.6679365e-03
 4.5938807e-05 9.9728370e-01 2.6860686e-08 2.2249062e-06 1.6516864e-07
 8.8663175e-09], sum to 1.0000
[2019-04-09 15:25:27,363] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7860
[2019-04-09 15:25:27,406] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 73.0, 0.0, 0.0, 19.0, 24.86605557698253, 0.3663869347580906, 0.0, 1.0, 45.0, 33.756851957267365], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4502400.0000, 
sim time next is 4503600.0000, 
raw observation next is [-1.0, 73.0, 0.0, 0.0, 19.0, 24.88606817364612, 0.3674908617297297, 0.0, 1.0, 45.0, 33.650883751425425], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.73, 0.0, 0.0, 0.08333333333333333, 0.57383901447051, 0.6224969539099099, 0.0, 1.0, 0.6, 0.33650883751425426], 
reward next is 0.6635, 
noisyNet noise sample is [array([-0.33399478], dtype=float32), 1.3618679]. 
=============================================
[2019-04-09 15:25:27,443] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.03595275e-15 1.24034897e-11 1.59145497e-08 2.39936418e-15
 6.50546746e-03 1.39151989e-05 9.93480682e-01 1.82636836e-10
 1.39552725e-08 8.76428596e-10 5.19994013e-11], sum to 1.0000
[2019-04-09 15:25:27,443] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5437
[2019-04-09 15:25:27,482] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.3, 84.0, 142.5, 131.5, 22.5, 27.30326079144583, 0.8773911290155808, 1.0, 1.0, 45.0, 29.885797742766176], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4438800.0000, 
sim time next is 4440000.0000, 
raw observation next is [1.2, 84.66666666666667, 157.5, 64.5, 22.5, 27.30769161531653, 0.9069901744863317, 1.0, 1.0, 45.0, 29.037675429397854], 
processed observation next is [1.0, 0.391304347826087, 0.4958448753462604, 0.8466666666666667, 0.525, 0.0712707182320442, 0.375, 0.775640967943044, 0.8023300581621106, 1.0, 1.0, 0.6, 0.2903767542939785], 
reward next is 0.7096, 
noisyNet noise sample is [array([0.8400781], dtype=float32), -0.23260239]. 
=============================================
[2019-04-09 15:25:27,490] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.92493 ]
 [70.39039 ]
 [69.221275]
 [67.07308 ]
 [65.78217 ]], R is [[73.83631134]
 [73.79908752]
 [73.76715851]
 [73.72167969]
 [73.67671967]].
[2019-04-09 15:25:27,510] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0279024e-12 5.7195897e-09 1.0006618e-07 4.2107379e-13 1.8126726e-01
 8.0565522e-05 8.1864852e-01 3.1882273e-08 3.3547747e-06 1.6473126e-07
 8.6436485e-09], sum to 1.0000
[2019-04-09 15:25:27,518] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8389
[2019-04-09 15:25:27,521] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.90067874e-16 9.94877791e-13 3.21680980e-11 2.30200146e-17
 2.07102485e-03 5.34027208e-07 9.97928500e-01 1.01918106e-10
 3.18809845e-09 5.15416487e-10 6.70331672e-12], sum to 1.0000
[2019-04-09 15:25:27,522] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1708
[2019-04-09 15:25:27,557] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1248944e-15 3.4094668e-11 2.4354212e-09 1.9721076e-15 5.9265583e-03
 7.6719707e-06 9.9406576e-01 1.6915487e-09 3.3701230e-08 1.6203735e-09
 2.7318949e-09], sum to 1.0000
[2019-04-09 15:25:27,557] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1419
[2019-04-09 15:25:27,567] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 51.0, 119.5, 0.0, 22.5, 26.56876985853967, 0.6421418185716735, 1.0, 1.0, 45.0, 28.79866033680924], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4534800.0000, 
sim time next is 4536000.0000, 
raw observation next is [2.0, 48.0, 122.5, 0.0, 22.5, 26.58221950823685, 0.6639922722066447, 1.0, 1.0, 45.0, 28.767638050013005], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.48, 0.4083333333333333, 0.0, 0.375, 0.7151849590197376, 0.7213307574022149, 1.0, 1.0, 0.6, 0.28767638050013006], 
reward next is 0.7123, 
noisyNet noise sample is [array([0.248282], dtype=float32), 2.5641303]. 
=============================================
[2019-04-09 15:25:27,575] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 19.0, 25.07568961848289, 0.3992121269857502, 0.0, 1.0, 45.0, 33.7956911023734], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4500000.0000, 
sim time next is 4501200.0000, 
raw observation next is [-0.7333333333333334, 73.0, 0.0, 0.0, 19.0, 25.02256133257968, 0.3849192142808084, 0.0, 1.0, 45.0, 33.75770418450589], 
processed observation next is [1.0, 0.08695652173913043, 0.44228993536472766, 0.73, 0.0, 0.0, 0.08333333333333333, 0.58521344438164, 0.6283064047602694, 0.0, 1.0, 0.6, 0.3375770418450589], 
reward next is 0.6624, 
noisyNet noise sample is [array([-1.1692262], dtype=float32), -0.85630184]. 
=============================================
[2019-04-09 15:25:27,576] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[68.702126]
 [68.55772 ]
 [68.18385 ]
 [67.50207 ]
 [66.702446]], R is [[68.75093079]
 [68.7754364 ]
 [68.79949188]
 [68.8221283 ]
 [68.8425293 ]].
[2019-04-09 15:25:27,584] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.533333333333333, 82.66666666666667, 127.5, 198.5, 22.5, 26.91817160492447, 0.8423815049614637, 1.0, 1.0, 45.0, 29.45780005042332], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4437600.0000, 
sim time next is 4438800.0000, 
raw observation next is [1.3, 84.0, 142.5, 131.5, 22.5, 27.27978274091217, 0.8721085895602215, 1.0, 1.0, 45.0, 29.94215406075115], 
processed observation next is [1.0, 0.391304347826087, 0.49861495844875353, 0.84, 0.475, 0.1453038674033149, 0.375, 0.7733152284093476, 0.7907028631867404, 1.0, 1.0, 0.6, 0.2994215406075115], 
reward next is 0.7006, 
noisyNet noise sample is [array([-1.128733], dtype=float32), 0.8002801]. 
=============================================
[2019-04-09 15:25:27,904] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.4036769e-16 1.3313289e-12 6.4355266e-10 1.9169891e-16 1.3702435e-02
 8.6626742e-06 9.8628891e-01 1.6592107e-10 8.2175200e-09 9.7208686e-10
 2.0657630e-11], sum to 1.0000
[2019-04-09 15:25:27,907] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9310
[2019-04-09 15:25:27,982] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 255.5, 80.5, 22.5, 26.86781318422166, 0.7530686194134653, 1.0, 1.0, 45.0, 27.58588568309105], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4543200.0000, 
sim time next is 4544400.0000, 
raw observation next is [3.0, 47.66666666666667, 261.1666666666666, 102.1666666666667, 22.5, 26.18626098793292, 0.6881655399220312, 1.0, 1.0, 45.0, 28.142238634717067], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.47666666666666674, 0.8705555555555552, 0.11289134438305713, 0.375, 0.6821884156610766, 0.7293885133073438, 1.0, 1.0, 0.6, 0.28142238634717065], 
reward next is 0.7186, 
noisyNet noise sample is [array([-0.46650815], dtype=float32), -1.1370642]. 
=============================================
[2019-04-09 15:25:28,211] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.37055284e-13 1.32340215e-11 5.39617018e-10 1.84497356e-13
 3.66272201e-04 4.30012733e-06 9.99629021e-01 4.09309653e-09
 2.66874565e-07 1.06533456e-07 7.46969597e-10], sum to 1.0000
[2019-04-09 15:25:28,212] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2704
[2019-04-09 15:25:28,213] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.0601745e-14 3.4959405e-10 8.7405144e-10 2.9227578e-13 1.2986774e-03
 3.4276695e-06 9.9869776e-01 1.0789765e-08 8.6689973e-08 3.7172349e-08
 6.0180189e-10], sum to 1.0000
[2019-04-09 15:25:28,213] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6518
[2019-04-09 15:25:28,235] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 72.33333333333333, 18.5, 11.0, 22.5, 24.4443954819196, 0.2239160300266338, 1.0, 1.0, 45.0, 32.3652059167169], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4520400.0000, 
sim time next is 4521600.0000, 
raw observation next is [-0.8, 73.0, 55.5, 33.0, 22.5, 24.41399365990871, 0.2303727158451984, 1.0, 1.0, 45.0, 32.18415292086594], 
processed observation next is [1.0, 0.34782608695652173, 0.4404432132963989, 0.73, 0.185, 0.036464088397790057, 0.375, 0.5344994716590591, 0.5767909052817328, 1.0, 1.0, 0.6, 0.3218415292086594], 
reward next is 0.6782, 
noisyNet noise sample is [array([-0.28595105], dtype=float32), 1.6609906]. 
=============================================
[2019-04-09 15:25:28,252] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 19.0, 26.07650882044547, 0.6205007709603229, 0.0, 1.0, 45.0, 27.09285367125831], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4672800.0000, 
sim time next is 4674000.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 19.0, 26.04404387555148, 0.6214309380717757, 0.0, 1.0, 45.0, 26.991705175769027], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.08333333333333333, 0.67033698962929, 0.7071436460239253, 0.0, 1.0, 0.6, 0.26991705175769026], 
reward next is 0.7301, 
noisyNet noise sample is [array([2.5437944], dtype=float32), 0.34182552]. 
=============================================
[2019-04-09 15:25:28,294] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[57.25948 ]
 [57.890305]
 [58.588432]
 [59.001244]
 [59.626213]], R is [[56.74781799]
 [56.90941238]
 [57.06867218]
 [57.22594833]
 [57.38127899]].
[2019-04-09 15:25:29,117] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.4398600e-14 5.2197469e-11 4.1199772e-08 1.9546209e-12 6.2174581e-02
 1.5930951e-05 9.3780911e-01 4.1952366e-09 3.0874881e-07 7.1239796e-08
 1.0465646e-08], sum to 1.0000
[2019-04-09 15:25:29,125] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6348
[2019-04-09 15:25:29,160] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 58.66666666666666, 0.0, 0.0, 19.0, 26.14555803799907, 0.6373583220851505, 0.0, 1.0, 45.0, 26.93641899391008], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4671600.0000, 
sim time next is 4672800.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 19.0, 26.04895737019251, 0.61780733016244, 0.0, 1.0, 45.0, 27.440745286602386], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.08333333333333333, 0.6707464475160426, 0.7059357767208133, 0.0, 1.0, 0.6, 0.27440745286602386], 
reward next is 0.7256, 
noisyNet noise sample is [array([-0.4339495], dtype=float32), 1.1293628]. 
=============================================
[2019-04-09 15:25:29,282] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3294930e-12 5.4320042e-09 3.5103278e-07 1.3731250e-12 3.7286749e-01
 2.1093508e-04 6.2691087e-01 3.7797605e-08 9.3230055e-06 9.0496934e-07
 3.5807489e-08], sum to 1.0000
[2019-04-09 15:25:29,282] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2542
[2019-04-09 15:25:29,307] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8666666666666667, 72.33333333333333, 18.5, 11.0, 22.5, 24.46509210250696, 0.2414832968630267, 1.0, 1.0, 45.0, 32.348672983786656], 
current ob forecast is [], 
actual action is [1, 35.0], 
sim time this is 4520400.0000, 
sim time next is 4521600.0000, 
raw observation next is [-0.8, 73.0, 55.5, 33.0, 22.5, 24.42942744109563, 0.2378581262227858, 1.0, 1.0, 35.0, 25.126920940914587], 
processed observation next is [1.0, 0.34782608695652173, 0.4404432132963989, 0.73, 0.185, 0.036464088397790057, 0.375, 0.5357856200913025, 0.5792860420742619, 1.0, 1.0, 0.4, 0.2512692094091459], 
reward next is 0.7487, 
noisyNet noise sample is [array([0.60051364], dtype=float32), 0.2624759]. 
=============================================
[2019-04-09 15:25:29,581] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.14483338e-13 2.44867210e-10 5.75785783e-08 1.99531892e-13
 4.63936329e-01 2.06524910e-05 5.36042631e-01 1.46890144e-08
 3.12228650e-07 1.69905174e-08 1.62254263e-08], sum to 1.0000
[2019-04-09 15:25:29,581] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5725
[2019-04-09 15:25:29,611] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 61.5, 85.5, 22.5, 24.16674214166839, 0.1893742121237828, 1.0, 1.0, 45.0, 28.671647932864854], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4608000.0000, 
sim time next is 4609200.0000, 
raw observation next is [-2.0, 71.0, 102.5, 142.5, 22.5, 24.2696058744075, 0.2615888877665842, 1.0, 1.0, 45.0, 28.45204974464587], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.3416666666666667, 0.1574585635359116, 0.375, 0.522467156200625, 0.5871962959221947, 1.0, 1.0, 0.6, 0.2845204974464587], 
reward next is 0.7155, 
noisyNet noise sample is [array([-1.7843386], dtype=float32), 1.0003929]. 
=============================================
[2019-04-09 15:25:29,647] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.3964441e-13 3.0238888e-11 1.7030226e-08 8.3941765e-14 2.5129151e-02
 3.2340988e-05 9.7483701e-01 7.5869560e-09 1.4710083e-06 1.4714866e-08
 1.7173022e-09], sum to 1.0000
[2019-04-09 15:25:29,647] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8730
[2019-04-09 15:25:29,684] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 25.74394352446835, 0.5096134600276295, 0.0, 1.0, 45.0, 27.5129475604084], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4687200.0000, 
sim time next is 4688400.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 25.66156661562567, 0.4837888730759358, 0.0, 1.0, 45.0, 27.55810250794614], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6384638846354725, 0.6612629576919786, 0.0, 1.0, 0.6, 0.27558102507946136], 
reward next is 0.7244, 
noisyNet noise sample is [array([-0.86257416], dtype=float32), -0.00055247365]. 
=============================================
[2019-04-09 15:25:29,881] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4226649e-15 1.5392396e-11 7.4960976e-10 6.9819754e-14 6.4070988e-03
 1.4195234e-06 9.9359149e-01 1.2783412e-09 4.4104159e-08 1.1626432e-08
 5.2506936e-11], sum to 1.0000
[2019-04-09 15:25:29,881] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9779
[2019-04-09 15:25:29,918] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 19.0, 25.17072997375361, 0.4438606668509381, 0.0, 1.0, 45.0, 33.87291718501882], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4496400.0000, 
sim time next is 4497600.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 19.0, 25.24125454940116, 0.4318446389090315, 0.0, 1.0, 45.0, 33.81811272693304], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.08333333333333333, 0.6034378791167633, 0.6439482129696772, 0.0, 1.0, 0.6, 0.33818112726933036], 
reward next is 0.6618, 
noisyNet noise sample is [array([-1.3555394], dtype=float32), 0.35887522]. 
=============================================
[2019-04-09 15:25:30,068] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7024642e-16 8.7629053e-12 3.0897922e-09 3.5228979e-16 2.0563074e-03
 4.0109699e-05 9.9790359e-01 7.8354753e-12 2.0922108e-08 1.3027972e-09
 3.3718042e-11], sum to 1.0000
[2019-04-09 15:25:30,068] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5682
[2019-04-09 15:25:30,103] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 76.0, 29.0, 45.83333333333334, 22.5, 27.59358487650795, 0.9190397681528006, 1.0, 1.0, 45.0, 33.345251869733346], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4468800.0000, 
sim time next is 4470000.0000, 
raw observation next is [0.0, 74.0, 20.83333333333334, 45.83333333333334, 22.5, 27.31009029567879, 0.6144823103197128, 1.0, 1.0, 45.0, 34.23976485625968], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.74, 0.06944444444444446, 0.050644567219152864, 0.375, 0.7758408579732325, 0.7048274367732376, 1.0, 1.0, 0.6, 0.3423976485625968], 
reward next is 0.6576, 
noisyNet noise sample is [array([0.634128], dtype=float32), 1.4707618]. 
=============================================
[2019-04-09 15:25:30,106] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[76.364136]
 [76.57031 ]
 [76.88709 ]
 [77.40752 ]
 [77.789795]], R is [[75.85106659]
 [75.7591095 ]
 [75.67960358]
 [75.60232544]
 [75.51777649]].
[2019-04-09 15:25:30,321] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9420400e-16 5.2516424e-12 1.5964261e-09 4.5445852e-17 4.7219242e-03
 2.5034173e-05 9.9525297e-01 1.1399702e-09 1.3275961e-07 5.3046106e-10
 7.3115647e-10], sum to 1.0000
[2019-04-09 15:25:30,328] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9422
[2019-04-09 15:25:30,373] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 140.5, 3.0, 22.5, 26.9041720741081, 0.9213532595536743, 1.0, 1.0, 45.0, 32.319324061676056], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4456800.0000, 
sim time next is 4458000.0000, 
raw observation next is [0.0, 89.66666666666667, 103.5, 0.9999999999999998, 22.5, 27.19331848758217, 0.9417513948625307, 1.0, 1.0, 45.0, 28.490912448204966], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.8966666666666667, 0.345, 0.0011049723756906074, 0.375, 0.7661098739651809, 0.8139171316208436, 1.0, 1.0, 0.6, 0.28490912448204964], 
reward next is 0.7151, 
noisyNet noise sample is [array([-0.27438375], dtype=float32), -1.5061098]. 
=============================================
[2019-04-09 15:25:30,389] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.8534  ]
 [77.651955]
 [77.47142 ]
 [77.17289 ]
 [76.70463 ]], R is [[77.50537109]
 [77.40711975]
 [77.329422  ]
 [77.25569153]
 [77.16838074]].
[2019-04-09 15:25:30,402] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4686280e-17 1.5066408e-11 2.4251709e-10 3.4501280e-16 1.9400003e-03
 2.4876169e-06 9.9805731e-01 2.5920016e-10 6.6528607e-08 1.8962412e-09
 2.4977292e-10], sum to 1.0000
[2019-04-09 15:25:30,402] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3785
[2019-04-09 15:25:30,442] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 26.37007689311712, 0.703127725988264, 1.0, 1.0, 45.0, 28.066480619089525], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4563600.0000, 
sim time next is 4564800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 26.25471712739117, 0.6903121894773845, 1.0, 1.0, 45.0, 27.868752103929044], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.6878930939492642, 0.7301040631591281, 1.0, 1.0, 0.6, 0.27868752103929045], 
reward next is 0.7213, 
noisyNet noise sample is [array([0.21912499], dtype=float32), -0.21994491]. 
=============================================
[2019-04-09 15:25:30,460] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4580954e-17 3.6120938e-13 1.8470803e-10 2.1967674e-16 2.4760165e-03
 2.9998765e-07 9.9752361e-01 1.6408389e-10 3.2781444e-10 6.2495550e-11
 2.1034155e-12], sum to 1.0000
[2019-04-09 15:25:30,461] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6909
[2019-04-09 15:25:30,486] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 46.0, 171.5, 28.83333333333333, 22.5, 25.61746754462251, 0.6693622335316997, 1.0, 1.0, 45.0, 26.986622082025146], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4548000.0000, 
sim time next is 4549200.0000, 
raw observation next is [2.333333333333333, 47.0, 145.6666666666667, 21.33333333333333, 22.5, 26.97823849360484, 0.769684808607741, 1.0, 1.0, 45.0, 26.436447643505105], 
processed observation next is [1.0, 0.6521739130434783, 0.5272391505078486, 0.47, 0.48555555555555574, 0.02357274401473296, 0.375, 0.7481865411337368, 0.756561602869247, 1.0, 1.0, 0.6, 0.26436447643505107], 
reward next is 0.7356, 
noisyNet noise sample is [array([1.4246035], dtype=float32), -0.5831116]. 
=============================================
[2019-04-09 15:25:30,499] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1560128e-14 1.3585142e-11 8.3451077e-09 2.0908746e-14 3.6040230e-03
 3.2492042e-06 9.9639279e-01 3.3094011e-10 3.3524532e-08 6.0219318e-09
 2.1109063e-10], sum to 1.0000
[2019-04-09 15:25:30,499] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8990
[2019-04-09 15:25:30,541] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 129.8333333333333, 227.3333333333333, 22.5, 25.02191045562287, 0.3617085203586441, 1.0, 1.0, 45.0, 27.697222352246406], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4610400.0000, 
sim time next is 4611600.0000, 
raw observation next is [-2.0, 71.0, 143.5, 340.0, 22.5, 25.42571169428273, 0.4155020446967979, 1.0, 1.0, 45.0, 27.098259502164936], 
processed observation next is [1.0, 0.391304347826087, 0.40720221606648205, 0.71, 0.47833333333333333, 0.3756906077348066, 0.375, 0.618809307856894, 0.6385006815655992, 1.0, 1.0, 0.6, 0.27098259502164934], 
reward next is 0.7290, 
noisyNet noise sample is [array([-0.83161193], dtype=float32), -1.05688]. 
=============================================
[2019-04-09 15:25:31,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.85422898e-13 2.46531545e-10 1.22730475e-08 1.70850869e-13
 1.96811333e-02 1.01636251e-05 9.80308652e-01 2.81838641e-09
 4.40040644e-08 3.94375244e-09 2.13848633e-10], sum to 1.0000
[2019-04-09 15:25:31,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5211
[2019-04-09 15:25:31,087] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 19.0, 25.2978020796365, 0.4619824984205265, 0.0, 1.0, 45.0, 33.781642770040094], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4497600.0000, 
sim time next is 4498800.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 19.0, 25.186260313985, 0.4469918201562563, 0.0, 1.0, 45.0, 33.81130404775188], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.08333333333333333, 0.5988550261654165, 0.6489972733854188, 0.0, 1.0, 0.6, 0.33811304047751883], 
reward next is 0.6619, 
noisyNet noise sample is [array([-0.58954144], dtype=float32), -0.9590802]. 
=============================================
[2019-04-09 15:25:31,624] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5054199e-17 2.9824824e-13 3.6561209e-11 4.7843177e-17 2.5108646e-04
 2.8549794e-05 9.9972028e-01 8.3292401e-12 7.5815567e-09 1.0582768e-10
 2.6402034e-12], sum to 1.0000
[2019-04-09 15:25:31,626] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0955
[2019-04-09 15:25:31,701] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 50.0, 121.6666666666667, 837.3333333333334, 22.5, 26.73869897317185, 0.5214032957223049, 1.0, 1.0, 45.0, 22.712564780252954], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4621200.0000, 
sim time next is 4622400.0000, 
raw observation next is [3.0, 49.0, 121.0, 846.0, 22.5, 26.58596426533257, 0.7874520371438546, 1.0, 1.0, 45.0, 21.523685961304977], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.49, 0.4033333333333333, 0.9348066298342541, 0.375, 0.7154970221110476, 0.7624840123812849, 1.0, 1.0, 0.6, 0.21523685961304978], 
reward next is 0.7848, 
noisyNet noise sample is [array([-0.15160193], dtype=float32), -0.5826318]. 
=============================================
[2019-04-09 15:25:31,987] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2142322e-13 8.1195839e-10 3.0410163e-08 1.7157613e-13 2.5080593e-02
 1.5236846e-05 9.7490293e-01 8.7477979e-08 1.1179159e-06 5.1470693e-08
 1.0566333e-08], sum to 1.0000
[2019-04-09 15:25:31,987] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4915
[2019-04-09 15:25:32,027] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8, 66.33333333333333, 0.0, 0.0, 19.0, 25.18109537096249, 0.3705471924349573, 0.0, 1.0, 45.0, 27.58215636511582], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4588800.0000, 
sim time next is 4590000.0000, 
raw observation next is [-1.1, 67.0, 0.0, 0.0, 19.0, 25.02920672729731, 0.3602867225145452, 0.0, 1.0, 45.0, 27.4486109548696], 
processed observation next is [1.0, 0.13043478260869565, 0.4321329639889197, 0.67, 0.0, 0.0, 0.08333333333333333, 0.5857672272747759, 0.6200955741715151, 0.0, 1.0, 0.6, 0.274486109548696], 
reward next is 0.7255, 
noisyNet noise sample is [array([-0.02677602], dtype=float32), -0.16532438]. 
=============================================
[2019-04-09 15:25:32,039] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[56.79937 ]
 [57.433388]
 [57.931168]
 [58.73713 ]
 [59.520416]], R is [[56.41794586]
 [56.57794571]
 [56.73853683]
 [56.89775085]
 [57.05311584]].
[2019-04-09 15:25:32,690] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3483361e-14 8.8640657e-11 1.4877810e-08 7.2116521e-13 1.2484310e-02
 1.3241909e-05 9.8750222e-01 4.8480975e-09 1.2681004e-07 6.6968703e-08
 5.2510973e-10], sum to 1.0000
[2019-04-09 15:25:32,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2417
[2019-04-09 15:25:32,718] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 102.5, 142.5, 22.5, 24.31689518194385, 0.2633617708895801, 1.0, 1.0, 45.0, 28.29351255606628], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4609200.0000, 
sim time next is 4610400.0000, 
raw observation next is [-2.0, 71.0, 129.8333333333333, 227.3333333333333, 22.5, 25.0289084057098, 0.3576494954175205, 1.0, 1.0, 45.0, 27.695207419674844], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.4327777777777776, 0.2511970534069981, 0.375, 0.5857423671424833, 0.6192164984725068, 1.0, 1.0, 0.6, 0.27695207419674844], 
reward next is 0.7230, 
noisyNet noise sample is [array([0.503117], dtype=float32), 0.12436756]. 
=============================================
[2019-04-09 15:25:32,922] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6818657e-18 9.6522914e-14 1.5576569e-10 2.4676855e-18 1.1581273e-03
 8.1325089e-07 9.9884111e-01 2.9065886e-12 2.0315571e-09 3.2628242e-10
 2.2210678e-13], sum to 1.0000
[2019-04-09 15:25:32,923] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8656
[2019-04-09 15:25:32,945] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7466482e-16 3.4801027e-12 1.3936875e-10 2.3440508e-15 1.6600224e-03
 4.3152017e-06 9.9833578e-01 3.3336356e-10 3.5950054e-09 7.6800927e-10
 5.1855385e-11], sum to 1.0000
[2019-04-09 15:25:32,946] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7015
[2019-04-09 15:25:32,955] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.733333333333333, 44.0, 130.0, 144.0, 22.5, 28.47017104028195, 1.102920456344729, 1.0, 1.0, 45.0, 14.600149171194534], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4638000.0000, 
sim time next is 4639200.0000, 
raw observation next is [5.466666666666667, 45.0, 104.1666666666667, 146.6666666666667, 22.5, 28.5298739792511, 0.9643987231276295, 1.0, 1.0, 45.0, 20.338114497247116], 
processed observation next is [1.0, 0.6956521739130435, 0.6140350877192983, 0.45, 0.3472222222222223, 0.1620626151012892, 0.375, 0.877489498270925, 0.8214662410425432, 1.0, 1.0, 0.6, 0.20338114497247115], 
reward next is 0.7966, 
noisyNet noise sample is [array([-1.7251911], dtype=float32), -0.17032276]. 
=============================================
[2019-04-09 15:25:32,981] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.0, 146.5, 638.0, 22.5, 26.16906775997208, 0.5930290569671599, 1.0, 1.0, 45.0, 25.97610154557688], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4615200.0000, 
sim time next is 4616400.0000, 
raw observation next is [0.6666666666666666, 57.33333333333334, 134.8333333333333, 724.0, 22.5, 26.4218709998479, 0.6564477165122096, 1.0, 1.0, 45.0, 25.276263805335006], 
processed observation next is [1.0, 0.43478260869565216, 0.4810710987996307, 0.5733333333333335, 0.4494444444444443, 0.8, 0.375, 0.7018225833206584, 0.7188159055040698, 1.0, 1.0, 0.6, 0.2527626380533501], 
reward next is 0.7472, 
noisyNet noise sample is [array([1.4340482], dtype=float32), 2.6365356]. 
=============================================
[2019-04-09 15:25:33,025] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4952738e-12 2.0754229e-09 3.8047489e-07 1.5869333e-12 1.0086323e-01
 7.9515076e-04 8.9834058e-01 1.0017144e-08 5.6732881e-07 1.0946280e-07
 1.3839466e-09], sum to 1.0000
[2019-04-09 15:25:33,025] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9192
[2019-04-09 15:25:33,055] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 0.0, 0.0, 19.0, 25.94101182034759, 0.5816221670706997, 0.0, 1.0, 45.0, 27.03309981391605], 
current ob forecast is [], 
actual action is [1, 45.0], 
sim time this is 4680000.0000, 
sim time next is 4681200.0000, 
raw observation next is [-0.3333333333333333, 94.66666666666667, 0.0, 0.0, 19.0, 25.96980563778359, 0.5664864769905472, 0.0, 1.0, 45.0, 27.024586432167965], 
processed observation next is [1.0, 0.17391304347826086, 0.4533702677747, 0.9466666666666668, 0.0, 0.0, 0.08333333333333333, 0.6641504698152992, 0.6888288256635158, 0.0, 1.0, 0.6, 0.27024586432167963], 
reward next is 0.7298, 
noisyNet noise sample is [array([0.9232356], dtype=float32), 0.98895824]. 
=============================================
[2019-04-09 15:25:33,152] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-09 15:25:33,152] A3C_EVAL-Part4-Light-Pit-Train-v3 INFO:Evaluation job starts!
[2019-04-09 15:25:33,153] A3C_EVAL-Part4-Light-Pit-Test-v5 INFO:Evaluation job starts!
[2019-04-09 15:25:33,154] A3C_EVAL-Part4-Light-Pit-Test-v6 INFO:Evaluation job starts!
[2019-04-09 15:25:33,154] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:25:33,154] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:25:33,154] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-09 15:25:33,159] EPLUS_ENV_Part4-Light-Pit-Test-v5_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v5-res1/Eplus-env-sub_run14
[2019-04-09 15:25:33,175] EPLUS_ENV_Part4-Light-Pit-Test-v6_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Test-v6-res1/Eplus-env-sub_run14
[2019-04-09 15:25:33,190] EPLUS_ENV_Part4-Light-Pit-Train-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/79/Eplus-env-Part4-Light-Pit-Train-v3-res1/Eplus-env-sub_run14
