Using TensorFlow backend.
[2019-04-03 21:51:53,410] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v1', activation='linear', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-v1', eval_act_func='part4_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=100000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.0001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=5000000, metric_func='part4_v1', model_dir='None', model_param=[12, 1], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_v1', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=10, test_env=['Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'], test_mode='Multiple', train_act_func='part4_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=10.0, weight_initer='glorot_uniform', window_len=20)
[2019-04-03 21:51:53,410] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-03 21:51:53.442038: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-03 21:52:09,657] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-03 21:52:09,658] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-v1', 'Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'] ...
[2019-04-03 21:52:09,681] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation worker starts!
[2019-04-03 21:52:09,705] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation worker starts!
[2019-04-03 21:52:09,729] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation worker starts!
[2019-04-03 21:52:09,729] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:09,730] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-03 21:52:09,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:09,794] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run1
[2019-04-03 21:52:10,731] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:10,732] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-03 21:52:10,807] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:10,808] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run1
[2019-04-03 21:52:11,733] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:11,734] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-03 21:52:11,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:11,854] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run1
[2019-04-03 21:52:12,735] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:12,735] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-03 21:52:12,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:12,846] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run1
[2019-04-03 21:52:13,736] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:13,737] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-03 21:52:13,950] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:13,952] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run1
[2019-04-03 21:52:14,738] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:14,739] A3C_AGENT_WORKER-Thread-7 INFO:Local worker starts!
[2019-04-03 21:52:14,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:14,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run1
[2019-04-03 21:52:15,740] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:15,741] A3C_AGENT_WORKER-Thread-8 INFO:Local worker starts!
[2019-04-03 21:52:15,883] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:15,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run1
[2019-04-03 21:52:16,742] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:16,743] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-04-03 21:52:16,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:16,951] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run1
[2019-04-03 21:52:17,743] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:17,744] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-03 21:52:18,235] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-03 21:52:18,235] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 21:52:18,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:18,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run1
[2019-04-03 21:52:18,329] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 21:52:18,329] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:18,335] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run1
[2019-04-03 21:52:18,449] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 21:52:18,450] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:18,451] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run1
[2019-04-03 21:52:18,527] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:18,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run1
[2019-04-03 21:52:18,749] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:18,750] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-03 21:52:19,206] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:19,208] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run1
[2019-04-03 21:52:19,761] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:19,762] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-03 21:52:20,024] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:20,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run1
[2019-04-03 21:52:20,768] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:20,769] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-03 21:52:21,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:21,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run1
[2019-04-03 21:52:21,769] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:21,770] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-03 21:52:22,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:22,139] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run1
[2019-04-03 21:52:22,771] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:22,771] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-03 21:52:23,016] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:23,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run1
[2019-04-03 21:52:23,772] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:23,773] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-03 21:52:24,428] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:24,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run1
[2019-04-03 21:52:24,774] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:24,775] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-03 21:52:25,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:25,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run1
[2019-04-03 21:53:00,376] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:53:00,376] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.9, 82.66666666666667, 35.66666666666666, 0.0, 26.0, 25.52567725182879, 0.283370581003885, 1.0, 1.0, 0.0]
[2019-04-03 21:53:00,377] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 21:53:00,378] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0830365  0.29387978 0.01537636 0.09628686 0.16762201 0.24234644
 0.10145207], sampled 0.5689962540963055
[2019-04-03 21:53:10,482] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:53:10,482] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.08333333333333337, 66.0, 173.6666666666667, 244.6666666666667, 26.0, 25.05780457283657, 0.2248514800447684, 0.0, 1.0, 0.0]
[2019-04-03 21:53:10,483] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 21:53:10,483] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.06155102 0.27239046 0.02112751 0.089841   0.1746202  0.3249916
 0.05547817], sampled 0.9725627836976074
[2019-04-03 21:53:44,038] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:53:44,039] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-15.66666666666667, 83.0, 0.0, 0.0, 19.0, 20.89400155200677, -0.6694059872386536, 0.0, 1.0, 0.0]
[2019-04-03 21:53:44,039] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 21:53:44,039] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.04877106 0.31218323 0.03308284 0.0753342  0.16585654 0.29510838
 0.06966372], sampled 0.6346868945095491
[2019-04-03 21:54:03,411] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:54:03,411] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.270575772333334, 79.09768339666667, 0.0, 0.0, 23.5, 23.84781712121328, 0.0539626798029356, 0.0, 1.0, 0.0]
[2019-04-03 21:54:03,411] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 21:54:03,412] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.05300341 0.21001269 0.01992442 0.10791225 0.26954463 0.2783858
 0.06121683], sampled 0.7452829594091173
[2019-04-03 21:54:16,487] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7458.2465 217898598.6321 1188.0247
[2019-04-03 21:54:17,527] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:54:17,527] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.666666666666666, 49.33333333333333, 0.0, 0.0, 25.0, 23.8647869193974, 0.01859521617423642, 0.0, 1.0, 41114.60008810717]
[2019-04-03 21:54:17,527] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 21:54:17,527] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.06700297 0.24568288 0.02577572 0.07254801 0.26302525 0.26554278
 0.06042238], sampled 0.8849880706558982
[2019-04-03 21:54:39,504] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:54:39,504] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [12.0, 19.0, 89.33333333333333, 691.6666666666667, 26.0, 27.54196952147445, 1.008006539523706, 1.0, 1.0, 0.0]
[2019-04-03 21:54:39,504] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 21:54:39,505] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.11034062 0.20574337 0.01241562 0.08540776 0.29847065 0.15598153
 0.1316404 ], sampled 0.8990834532068653
[2019-04-03 21:54:39,937] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7310.9398 246771801.5642 981.7301
[2019-04-03 21:54:44,651] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7251.2884 254092743.4161 692.7758
[2019-04-03 21:54:45,688] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 7310.939790448689, 246771801.56421003, 981.7301101639936, 7458.246469990233, 217898598.63206992, 1188.0246807496706, 7251.288425681861, 254092743.41612852, 692.775760885787]
[2019-04-03 21:55:19,145] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00625428 0.76935977 0.0086679  0.05095941 0.06223894 0.0409744
 0.06154535], sum to 1.0000
[2019-04-03 21:55:19,145] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9272
[2019-04-03 21:55:19,452] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 64.0, 15.0, 0.0, 20.0, 20.69099181407145, -0.801427886498418, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 232800.0000, 
sim time next is 233400.0000, 
raw observation next is [-3.4, 64.5, 12.0, 0.0, 21.0, 20.92331086826726, -0.7925392908515988, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.645, 0.04, 0.0, 0.25, 0.2436092390222718, 0.2358202363828004, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7393152], dtype=float32), 0.76753074]. 
=============================================
[2019-04-03 21:55:23,852] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00100787 0.86115426 0.00636307 0.02206041 0.05286574 0.03124317
 0.02530549], sum to 1.0000
[2019-04-03 21:55:23,854] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9517
[2019-04-03 21:55:24,145] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.41666666666667, 67.5, 0.0, 0.0, 19.0, 18.40900456528238, -1.240994577072108, 0.0, 1.0, 99825.71037043059], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 276600.0000, 
sim time next is 277200.0000, 
raw observation next is [-10.6, 67.0, 0.0, 0.0, 20.0, 18.42360824089398, -1.229682422680743, 0.0, 1.0, 89514.52217197987], 
processed observation next is [1.0, 0.21739130434782608, 0.1689750692520776, 0.67, 0.0, 0.0, 0.16666666666666666, 0.03530068674116501, 0.090105859106419, 0.0, 1.0, 0.42625962939038037], 
reward next is 0.5737, 
noisyNet noise sample is [array([0.15462697], dtype=float32), -1.0105674]. 
=============================================
[2019-04-03 21:55:26,351] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7525: loss -0.0486
[2019-04-03 21:55:26,430] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7525: learning rate 0.0001
[2019-04-03 21:55:26,543] A3C_AGENT_WORKER-Thread-7 INFO:Local step 500, global step 7595: loss -0.0761
[2019-04-03 21:55:26,545] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 500, global step 7597: learning rate 0.0001
[2019-04-03 21:55:26,604] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 7614: loss -0.0225
[2019-04-03 21:55:26,605] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 7614: learning rate 0.0001
[2019-04-03 21:55:26,695] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7639: loss 0.0248
[2019-04-03 21:55:26,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7639: learning rate 0.0001
[2019-04-03 21:55:26,925] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7691: loss -0.0590
[2019-04-03 21:55:26,954] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7703: learning rate 0.0001
[2019-04-03 21:55:27,335] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 7794: loss 1.4576
[2019-04-03 21:55:27,338] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 7794: learning rate 0.0001
[2019-04-03 21:55:28,361] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 8014: loss -0.1754
[2019-04-03 21:55:28,372] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 8017: learning rate 0.0001
[2019-04-03 21:55:28,442] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 8034: loss -1.3104
[2019-04-03 21:55:28,448] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 8034: learning rate 0.0001
[2019-04-03 21:55:28,532] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8052: loss -1.6118
[2019-04-03 21:55:28,532] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8052: learning rate 0.0001
[2019-04-03 21:55:29,048] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8161: loss -0.1961
[2019-04-03 21:55:29,048] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8161: learning rate 0.0001
[2019-04-03 21:55:29,453] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8246: loss -0.2478
[2019-04-03 21:55:29,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8246: learning rate 0.0001
[2019-04-03 21:55:29,578] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8272: loss -0.5155
[2019-04-03 21:55:29,578] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8272: learning rate 0.0001
[2019-04-03 21:55:29,900] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8322: loss -0.8753
[2019-04-03 21:55:29,922] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8322: learning rate 0.0001
[2019-04-03 21:55:29,961] A3C_AGENT_WORKER-Thread-8 INFO:Local step 500, global step 8327: loss -0.8345
[2019-04-03 21:55:29,999] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 500, global step 8327: learning rate 0.0001
[2019-04-03 21:55:30,140] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8369: loss -0.3077
[2019-04-03 21:55:30,141] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8369: learning rate 0.0001
[2019-04-03 21:55:30,526] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8443: loss -0.7027
[2019-04-03 21:55:30,549] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8443: learning rate 0.0001
[2019-04-03 21:55:40,201] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00429267 0.41753274 0.00836241 0.07413334 0.25690442 0.02071238
 0.21806195], sum to 1.0000
[2019-04-03 21:55:40,201] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8486
[2019-04-03 21:55:40,513] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.25, 51.0, 58.0, 905.0, 19.0, 19.86002820188061, -1.032467737311114, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 390600.0000, 
sim time next is 391200.0000, 
raw observation next is [-12.06666666666667, 51.0, 57.5, 902.0, 21.0, 19.5615510965748, -1.014966291278485, 1.0, 1.0, 196266.4560289789], 
processed observation next is [1.0, 0.5217391304347826, 0.1283471837488457, 0.51, 0.19166666666666668, 0.9966850828729282, 0.25, 0.13012925804789996, 0.16167790290717168, 1.0, 1.0, 0.9346021715665661], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.9630872], dtype=float32), 0.6736285]. 
=============================================
[2019-04-03 21:56:00,549] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0820458e-05 8.1733674e-01 6.7538564e-04 7.1360110e-03 1.6530950e-01
 2.9803985e-03 6.5211761e-03], sum to 1.0000
[2019-04-03 21:56:00,553] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6769
[2019-04-03 21:56:00,741] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 18.91243351784637, -1.159328563371327, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 546000.0000, 
sim time next is 546600.0000, 
raw observation next is [0.5, 92.0, 12.0, 37.99999999999999, 19.0, 18.82024210118262, -1.16214591365019, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.04, 0.04198895027624309, 0.08333333333333333, 0.06835350843188515, 0.11261802878327003, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14412707], dtype=float32), -0.09109919]. 
=============================================
[2019-04-03 21:56:02,546] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4306358e-05 6.6884786e-01 2.6319822e-04 7.5253816e-03 3.1115088e-01
 3.0285148e-03 9.1499854e-03], sum to 1.0000
[2019-04-03 21:56:02,546] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5185
[2019-04-03 21:56:02,748] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1000, global step 15275: loss 3.4864
[2019-04-03 21:56:02,751] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1000, global step 15275: learning rate 0.0001
[2019-04-03 21:56:02,767] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6666666666666667, 82.33333333333334, 87.0, 136.0, 19.0, 18.81769035200371, -1.120249461971347, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 559200.0000, 
sim time next is 559800.0000, 
raw observation next is [-0.7, 82.0, 89.0, 135.0, 19.0, 18.81414046089411, -1.127379448795909, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.443213296398892, 0.82, 0.2966666666666667, 0.14917127071823205, 0.08333333333333333, 0.0678450384078424, 0.12420685040136366, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22962001], dtype=float32), -0.78329915]. 
=============================================
[2019-04-03 21:56:02,964] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15333: loss 3.0871
[2019-04-03 21:56:02,964] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15333: learning rate 0.0001
[2019-04-03 21:56:02,998] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15343: loss 5.8124
[2019-04-03 21:56:03,012] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15343: learning rate 0.0001
[2019-04-03 21:56:03,137] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 15390: loss 4.2925
[2019-04-03 21:56:03,139] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 15390: learning rate 0.0001
[2019-04-03 21:56:03,253] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 15425: loss 11.4023
[2019-04-03 21:56:03,258] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 15428: learning rate 0.0001
[2019-04-03 21:56:03,934] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15692: loss 2.9100
[2019-04-03 21:56:03,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15694: learning rate 0.0001
[2019-04-03 21:56:05,109] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16087: loss 4.4878
[2019-04-03 21:56:05,110] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16087: learning rate 0.0001
[2019-04-03 21:56:05,183] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.5340611e-06 8.8122320e-01 1.3833499e-04 4.2248075e-03 1.1155231e-01
 1.2184288e-03 1.6333155e-03], sum to 1.0000
[2019-04-03 21:56:05,185] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9791
[2019-04-03 21:56:05,294] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 75.0, 0.0, 0.0, 19.0, 18.72710477176958, -1.215629520374553, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 619200.0000, 
sim time next is 619800.0000, 
raw observation next is [-4.5, 73.83333333333333, 0.0, 0.0, 19.5, 18.63573941359064, -1.216592548510036, 0.0, 1.0, 199012.8802064955], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.7383333333333333, 0.0, 0.0, 0.125, 0.05297828446588652, 0.09446915049665468, 0.0, 1.0, 0.9476803819356928], 
reward next is 0.0523, 
noisyNet noise sample is [array([0.68455493], dtype=float32), 0.9882759]. 
=============================================
[2019-04-03 21:56:05,514] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 16217: loss 8.3516
[2019-04-03 21:56:05,515] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 16217: learning rate 0.0001
[2019-04-03 21:56:05,657] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 16264: loss 4.4430
[2019-04-03 21:56:05,659] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 16265: learning rate 0.0001
[2019-04-03 21:56:05,719] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16287: loss 2.5268
[2019-04-03 21:56:05,724] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16288: learning rate 0.0001
[2019-04-03 21:56:05,885] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 16357: loss 2.4789
[2019-04-03 21:56:05,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 16357: learning rate 0.0001
[2019-04-03 21:56:06,020] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16409: loss 2.3747
[2019-04-03 21:56:06,021] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16409: learning rate 0.0001
[2019-04-03 21:56:06,281] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16497: loss 1.9751
[2019-04-03 21:56:06,283] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16498: learning rate 0.0001
[2019-04-03 21:56:06,312] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1000, global step 16504: loss 20.9357
[2019-04-03 21:56:06,313] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1000, global step 16504: learning rate 0.0001
[2019-04-03 21:56:06,319] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9767971e-06 9.5299727e-01 3.8242539e-05 3.9324337e-03 4.2188514e-02
 2.7359542e-04 5.6683854e-04], sum to 1.0000
[2019-04-03 21:56:06,319] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16507: loss 4.7874
[2019-04-03 21:56:06,320] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0282
[2019-04-03 21:56:06,322] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16508: learning rate 0.0001
[2019-04-03 21:56:06,340] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.1, 75.0, 0.0, 0.0, 19.0, 18.68811901718497, -1.196601688398188, 0.0, 1.0, 18722.84417175294], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 616800.0000, 
sim time next is 617400.0000, 
raw observation next is [-4.2, 75.0, 0.0, 0.0, 19.0, 18.69081574205084, -1.206519878990285, 0.0, 1.0, 18720.55949081133], 
processed observation next is [0.0, 0.13043478260869565, 0.34626038781163443, 0.75, 0.0, 0.0, 0.08333333333333333, 0.05756797850423675, 0.09782670700323837, 0.0, 1.0, 0.08914552138481587], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.0907762], dtype=float32), 0.79831874]. 
=============================================
[2019-04-03 21:56:06,589] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16622: loss 4.3416
[2019-04-03 21:56:06,591] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16622: learning rate 0.0001
[2019-04-03 21:56:14,638] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1522765e-08 9.5091838e-01 5.3268805e-06 7.6433754e-04 4.8002619e-02
 6.4842294e-05 2.4450466e-04], sum to 1.0000
[2019-04-03 21:56:14,639] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7785
[2019-04-03 21:56:14,768] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 18.70065276683887, -1.273322321179217, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 715200.0000, 
sim time next is 715800.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 18.65473482112485, -1.283901917883737, 0.0, 1.0, 100209.0069285641], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.054561235093737594, 0.07203269403875434, 0.0, 1.0, 0.47718574727887664], 
reward next is 0.5228, 
noisyNet noise sample is [array([0.79952174], dtype=float32), -1.9420561]. 
=============================================
[2019-04-03 21:56:19,656] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1162074e-06 9.4929010e-01 4.0318129e-05 1.7123655e-03 4.3056749e-02
 3.0471722e-04 5.5946293e-03], sum to 1.0000
[2019-04-03 21:56:19,659] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8783
[2019-04-03 21:56:19,724] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.7, 75.0, 16.0, 0.0, 19.0, 19.50006860745345, -1.043770135449354, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 806400.0000, 
sim time next is 807000.0000, 
raw observation next is [-6.616666666666667, 75.0, 21.33333333333334, 0.0, 19.0, 19.87791619073923, -1.016566558352588, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2793167128347184, 0.75, 0.07111111111111112, 0.0, 0.08333333333333333, 0.15649301589493594, 0.16114448054913733, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2212375], dtype=float32), 0.8643654]. 
=============================================
[2019-04-03 21:56:19,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[42.11505 ]
 [42.494232]
 [42.47441 ]
 [42.42054 ]
 [43.118805]], R is [[41.08109665]
 [40.67028427]
 [40.26358032]
 [39.86094666]
 [39.46233749]].
[2019-04-03 21:56:21,400] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9044859e-05 7.2750324e-01 1.1740578e-03 2.0742562e-02 5.5911928e-02
 6.9116947e-04 1.9393796e-01], sum to 1.0000
[2019-04-03 21:56:21,400] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5180
[2019-04-03 21:56:21,438] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 73.66666666666667, 100.8333333333333, 0.0, 19.0, 19.54599849712263, -1.086847982716428, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 822000.0000, 
sim time next is 822600.0000, 
raw observation next is [-4.5, 75.0, 99.0, 0.0, 19.0, 19.61392985411389, -1.083682800447298, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.75, 0.33, 0.0, 0.08333333333333333, 0.13449415450949095, 0.13877239985090065, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.8877136], dtype=float32), -0.45074636]. 
=============================================
[2019-04-03 21:56:23,522] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1500, global step 22881: loss -1.9505
[2019-04-03 21:56:23,540] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1500, global step 22884: learning rate 0.0001
[2019-04-03 21:56:24,577] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23129: loss -1.3323
[2019-04-03 21:56:24,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23129: learning rate 0.0001
[2019-04-03 21:56:24,616] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 23139: loss 1.5207
[2019-04-03 21:56:24,616] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 23139: learning rate 0.0001
[2019-04-03 21:56:24,857] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23207: loss -0.0103
[2019-04-03 21:56:24,857] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23207: learning rate 0.0001
[2019-04-03 21:56:25,296] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 23312: loss 0.0997
[2019-04-03 21:56:25,297] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 23312: learning rate 0.0001
[2019-04-03 21:56:25,423] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23340: loss 4.7013
[2019-04-03 21:56:25,425] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23340: learning rate 0.0001
[2019-04-03 21:56:26,807] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 23773: loss -4.0702
[2019-04-03 21:56:26,807] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 23773: learning rate 0.0001
[2019-04-03 21:56:28,271] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 24154: loss 22.4454
[2019-04-03 21:56:28,273] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 24155: learning rate 0.0001
[2019-04-03 21:56:28,585] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1500, global step 24226: loss -0.8800
[2019-04-03 21:56:28,586] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1500, global step 24226: learning rate 0.0001
[2019-04-03 21:56:28,617] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 24235: loss -2.4682
[2019-04-03 21:56:28,618] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 24235: learning rate 0.0001
[2019-04-03 21:56:28,965] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24310: loss -2.1551
[2019-04-03 21:56:28,967] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24310: learning rate 0.0001
[2019-04-03 21:56:29,554] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24448: loss 0.1240
[2019-04-03 21:56:29,560] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24448: learning rate 0.0001
[2019-04-03 21:56:29,676] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 24482: loss 1.0822
[2019-04-03 21:56:29,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 24482: learning rate 0.0001
[2019-04-03 21:56:29,764] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24504: loss 5.7427
[2019-04-03 21:56:29,767] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24505: learning rate 0.0001
[2019-04-03 21:56:30,642] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24691: loss 21.6265
[2019-04-03 21:56:30,643] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24691: learning rate 0.0001
[2019-04-03 21:56:31,417] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24865: loss 13.1549
[2019-04-03 21:56:31,418] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24865: learning rate 0.0001
[2019-04-03 21:56:33,093] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8546149e-08 5.1268858e-01 1.6291683e-06 1.0242566e-03 3.9003545e-01
 5.6532783e-05 9.6193515e-02], sum to 1.0000
[2019-04-03 21:56:33,093] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4545
[2019-04-03 21:56:33,314] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.9, 93.0, 92.0, 0.0, 24.0, 24.54740116398906, 0.07261437563255889, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 915000.0000, 
sim time next is 915600.0000, 
raw observation next is [4.0, 93.0, 91.0, 0.0, 23.0, 24.38939755018687, 0.03174673438244368, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5734072022160666, 0.93, 0.30333333333333334, 0.0, 0.4166666666666667, 0.532449795848906, 0.5105822447941478, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6697641], dtype=float32), -0.32754818]. 
=============================================
[2019-04-03 21:56:39,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1286885e-08 1.6411494e-01 3.5227922e-07 5.8388095e-03 1.2547576e-01
 1.1846721e-06 7.0456898e-01], sum to 1.0000
[2019-04-03 21:56:39,400] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5291
[2019-04-03 21:56:39,445] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 106.5, 0.0, 26.0, 25.40919363179468, 0.372923875437582, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1000800.0000, 
sim time next is 1001400.0000, 
raw observation next is [14.4, 81.0, 102.3333333333333, 0.0, 26.0, 25.44876659299613, 0.3839928688641303, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.341111111111111, 0.0, 0.6666666666666666, 0.6207305494163441, 0.6279976229547101, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2590536], dtype=float32), 0.22969675]. 
=============================================
[2019-04-03 21:56:49,446] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2000, global step 30579: loss 5.4722
[2019-04-03 21:56:49,450] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2000, global step 30579: learning rate 0.0001
[2019-04-03 21:56:49,459] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 30586: loss 3.5049
[2019-04-03 21:56:49,460] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 30586: learning rate 0.0001
[2019-04-03 21:56:49,597] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5627838e-09 1.9803414e-02 1.4455500e-06 2.9362170e-03 2.0634598e-01
 6.2135581e-07 7.7091235e-01], sum to 1.0000
[2019-04-03 21:56:49,598] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4023
[2019-04-03 21:56:49,603] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.55, 55.0, 0.0, 0.0, 26.0, 26.3914371059545, 0.7110775490843357, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1103400.0000, 
sim time next is 1104000.0000, 
raw observation next is [15.36666666666667, 55.66666666666667, 0.0, 0.0, 26.0, 26.25262590384787, 0.6976246523411365, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8882733148661128, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.687718825320656, 0.7325415507803789, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4764101], dtype=float32), 0.5033084]. 
=============================================
[2019-04-03 21:56:49,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[88.383545]
 [88.49549 ]
 [88.59937 ]
 [88.65359 ]
 [88.677536]], R is [[88.4343338 ]
 [88.54998779]
 [88.66448975]
 [88.77784729]
 [88.89006805]].
[2019-04-03 21:56:49,998] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 30850: loss 8.0485
[2019-04-03 21:56:50,001] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 30850: learning rate 0.0001
[2019-04-03 21:56:50,139] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 30928: loss 6.2364
[2019-04-03 21:56:50,161] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 30928: learning rate 0.0001
[2019-04-03 21:56:50,281] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31000: loss 5.0423
[2019-04-03 21:56:50,283] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31000: learning rate 0.0001
[2019-04-03 21:56:51,129] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 31446: loss 2.4203
[2019-04-03 21:56:51,130] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 31447: learning rate 0.0001
[2019-04-03 21:56:51,797] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 31801: loss 1.8290
[2019-04-03 21:56:51,799] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 31801: learning rate 0.0001
[2019-04-03 21:56:52,487] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32155: loss 2.8476
[2019-04-03 21:56:52,489] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32157: learning rate 0.0001
[2019-04-03 21:56:53,077] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32432: loss 3.9590
[2019-04-03 21:56:53,079] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32435: learning rate 0.0001
[2019-04-03 21:56:53,301] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32530: loss 2.5936
[2019-04-03 21:56:53,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32531: learning rate 0.0001
[2019-04-03 21:56:53,317] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 32533: loss 4.0728
[2019-04-03 21:56:53,320] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 32534: learning rate 0.0001
[2019-04-03 21:56:53,480] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32602: loss 2.7973
[2019-04-03 21:56:53,480] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32602: learning rate 0.0001
[2019-04-03 21:56:53,567] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2000, global step 32631: loss 2.7727
[2019-04-03 21:56:53,572] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2000, global step 32631: learning rate 0.0001
[2019-04-03 21:56:54,517] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 33063: loss 3.1414
[2019-04-03 21:56:54,519] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 33063: learning rate 0.0001
[2019-04-03 21:56:54,684] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 33131: loss 1.9605
[2019-04-03 21:56:54,685] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 33131: learning rate 0.0001
[2019-04-03 21:56:55,195] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 33364: loss 2.8160
[2019-04-03 21:56:55,199] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 33364: learning rate 0.0001
[2019-04-03 21:57:08,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4930572e-11 2.4373764e-03 5.7923016e-10 5.5186375e-04 8.7489575e-02
 2.2274618e-09 9.0952122e-01], sum to 1.0000
[2019-04-03 21:57:08,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6425
[2019-04-03 21:57:09,175] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.22969543171212, 0.4850250624730421, 0.0, 1.0, 41135.88733779678], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1381200.0000, 
sim time next is 1381800.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.21286157783024, 0.4860583127794655, 0.0, 1.0, 40943.82426115229], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6010717981525199, 0.6620194375931552, 0.0, 1.0, 0.1949705917197728], 
reward next is 0.8050, 
noisyNet noise sample is [array([-0.08669123], dtype=float32), 0.99879086]. 
=============================================
[2019-04-03 21:57:12,071] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.10157941e-11 1.45775371e-03 5.14838705e-09 5.36871725e-04
 1.02434024e-01 1.11539418e-08 8.95571351e-01], sum to 1.0000
[2019-04-03 21:57:12,072] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9620
[2019-04-03 21:57:12,331] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 54.66666666666666, 0.0, 26.0, 26.03959329027296, 0.5081324453996278, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1437000.0000, 
sim time next is 1437600.0000, 
raw observation next is [1.1, 92.0, 50.33333333333333, 0.0, 26.0, 25.92381638853897, 0.5069379827750623, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.16777777777777778, 0.0, 0.6666666666666666, 0.6603180323782475, 0.6689793275916874, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60893387], dtype=float32), -0.43537787]. 
=============================================
[2019-04-03 21:57:16,544] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2500, global step 38746: loss 1.8536
[2019-04-03 21:57:16,545] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2500, global step 38746: learning rate 0.0001
[2019-04-03 21:57:18,406] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39132: loss 1.5985
[2019-04-03 21:57:18,406] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39132: learning rate 0.0001
[2019-04-03 21:57:18,787] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39199: loss 0.3547
[2019-04-03 21:57:18,788] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39199: learning rate 0.0001
[2019-04-03 21:57:19,074] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39246: loss 2.0863
[2019-04-03 21:57:19,075] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39246: learning rate 0.0001
[2019-04-03 21:57:20,417] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 39448: loss 1.5538
[2019-04-03 21:57:20,449] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 39448: learning rate 0.0001
[2019-04-03 21:57:20,518] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 39471: loss 1.4079
[2019-04-03 21:57:20,518] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 39471: learning rate 0.0001
[2019-04-03 21:57:21,840] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 39690: loss 0.9287
[2019-04-03 21:57:21,853] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 39690: learning rate 0.0001
[2019-04-03 21:57:22,864] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39940: loss 1.0449
[2019-04-03 21:57:22,865] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39941: learning rate 0.0001
[2019-04-03 21:57:24,669] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.9701416e-10 2.6003813e-02 2.7803262e-07 4.5351064e-04 6.3143969e-01
 4.7021950e-07 3.4210229e-01], sum to 1.0000
[2019-04-03 21:57:24,669] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4894
[2019-04-03 21:57:24,726] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.7, 82.5, 0.0, 0.0, 26.0, 25.52778030014558, 0.5569354950050492, 0.0, 1.0, 9374.982461393132], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1564200.0000, 
sim time next is 1564800.0000, 
raw observation next is [4.600000000000001, 83.66666666666666, 0.0, 0.0, 26.0, 25.68106735461973, 0.5399238924807606, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5900277008310251, 0.8366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6400889462183109, 0.6799746308269201, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20507033], dtype=float32), 0.4131347]. 
=============================================
[2019-04-03 21:57:25,207] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40545: loss 1.6702
[2019-04-03 21:57:25,208] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40545: learning rate 0.0001
[2019-04-03 21:57:25,562] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 40610: loss 2.5202
[2019-04-03 21:57:25,564] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 40610: learning rate 0.0001
[2019-04-03 21:57:25,709] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40650: loss 3.4142
[2019-04-03 21:57:25,709] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40650: learning rate 0.0001
[2019-04-03 21:57:25,814] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2500, global step 40686: loss 2.0377
[2019-04-03 21:57:25,815] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2500, global step 40686: learning rate 0.0001
[2019-04-03 21:57:25,891] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40706: loss 2.2380
[2019-04-03 21:57:25,891] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40706: learning rate 0.0001
[2019-04-03 21:57:26,043] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40748: loss 1.2117
[2019-04-03 21:57:26,045] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40748: learning rate 0.0001
[2019-04-03 21:57:26,800] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40985: loss 2.2545
[2019-04-03 21:57:26,801] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40985: learning rate 0.0001
[2019-04-03 21:57:27,269] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 41128: loss 2.5498
[2019-04-03 21:57:27,269] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 41128: learning rate 0.0001
[2019-04-03 21:57:30,351] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1650292e-11 8.8066532e-04 9.3986809e-09 1.7783456e-04 1.4331539e-01
 1.5283087e-08 8.5562605e-01], sum to 1.0000
[2019-04-03 21:57:30,352] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3570
[2019-04-03 21:57:30,366] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 74.0, 0.0, 0.0, 26.0, 25.42508175719121, 0.6829984091502088, 0.0, 1.0, 61556.08662674941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1544400.0000, 
sim time next is 1545000.0000, 
raw observation next is [7.516666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 25.8451037973143, 0.699457890700962, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6708217913204063, 0.7433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6537586497761918, 0.7331526302336541, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9040961], dtype=float32), 1.54019]. 
=============================================
[2019-04-03 21:57:30,369] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[112.38593]
 [112.75068]
 [111.74757]
 [111.18656]
 [110.75132]], R is [[112.50554657]
 [112.08737183]
 [111.13957214]
 [110.08332062]
 [109.04597473]].
[2019-04-03 21:57:46,131] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2436531e-10 1.6456170e-03 1.5045639e-08 3.0927688e-03 6.2786557e-02
 7.0893797e-08 9.3247497e-01], sum to 1.0000
[2019-04-03 21:57:46,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2929
[2019-04-03 21:57:46,288] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 101.1666666666667, 0.0, 26.0, 24.76443139319572, 0.4265619932987548, 1.0, 1.0, 196464.9794054528], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1689600.0000, 
sim time next is 1690200.0000, 
raw observation next is [1.1, 88.0, 100.0, 0.0, 26.0, 24.24950551790538, 0.4312265888611167, 1.0, 1.0, 197950.3739572187], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3333333333333333, 0.0, 0.6666666666666666, 0.520792126492115, 0.6437421962870389, 1.0, 1.0, 0.942620828367708], 
reward next is 0.0574, 
noisyNet noise sample is [array([0.78706545], dtype=float32), -0.4702003]. 
=============================================
[2019-04-03 21:57:53,141] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3000, global step 47013: loss -0.6050
[2019-04-03 21:57:53,142] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3000, global step 47013: learning rate 0.0001
[2019-04-03 21:57:54,061] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47171: loss -0.1181
[2019-04-03 21:57:54,062] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47171: learning rate 0.0001
[2019-04-03 21:57:54,738] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47308: loss -0.0675
[2019-04-03 21:57:54,741] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47310: learning rate 0.0001
[2019-04-03 21:57:54,952] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 47349: loss -2.0796
[2019-04-03 21:57:54,952] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 47349: learning rate 0.0001
[2019-04-03 21:57:55,054] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47377: loss -0.4382
[2019-04-03 21:57:55,056] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47377: learning rate 0.0001
[2019-04-03 21:57:55,884] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 47538: loss -0.5203
[2019-04-03 21:57:55,884] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 47538: learning rate 0.0001
[2019-04-03 21:57:55,901] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 47541: loss -0.0650
[2019-04-03 21:57:55,902] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 47541: learning rate 0.0001
[2019-04-03 21:57:57,562] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47849: loss -0.2955
[2019-04-03 21:57:57,570] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47849: learning rate 0.0001
[2019-04-03 21:57:58,000] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4287312e-08 8.1773233e-03 8.0122624e-07 9.6338853e-04 4.4283849e-01
 3.3580263e-06 5.4801661e-01], sum to 1.0000
[2019-04-03 21:57:58,001] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5947
[2019-04-03 21:57:58,094] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.933333333333333, 82.16666666666667, 0.0, 0.0, 26.0, 24.20075307242129, 0.1211343904426185, 0.0, 1.0, 46314.64427647562], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1821000.0000, 
sim time next is 1821600.0000, 
raw observation next is [-6.0, 83.0, 0.0, 0.0, 26.0, 24.1660017885738, 0.1134127446249533, 0.0, 1.0, 46388.49555660396], 
processed observation next is [0.0, 0.08695652173913043, 0.296398891966759, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5138334823811501, 0.5378042482083177, 0.0, 1.0, 0.22089759788859026], 
reward next is 0.7791, 
noisyNet noise sample is [array([-0.37086922], dtype=float32), 1.6427202]. 
=============================================
[2019-04-03 21:58:00,979] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 48467: loss -0.5544
[2019-04-03 21:58:01,015] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 48467: learning rate 0.0001
[2019-04-03 21:58:01,712] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3000, global step 48574: loss -0.1156
[2019-04-03 21:58:01,733] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3000, global step 48574: learning rate 0.0001
[2019-04-03 21:58:02,449] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48675: loss -0.5827
[2019-04-03 21:58:02,450] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48675: learning rate 0.0001
[2019-04-03 21:58:02,826] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48736: loss -0.2886
[2019-04-03 21:58:02,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48736: learning rate 0.0001
[2019-04-03 21:58:03,033] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48775: loss -0.1102
[2019-04-03 21:58:03,033] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48775: learning rate 0.0001
[2019-04-03 21:58:03,141] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 48793: loss -0.2821
[2019-04-03 21:58:03,144] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 48793: learning rate 0.0001
[2019-04-03 21:58:03,346] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48828: loss -0.5986
[2019-04-03 21:58:03,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48828: learning rate 0.0001
[2019-04-03 21:58:04,791] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 49076: loss -0.1697
[2019-04-03 21:58:04,846] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 49076: learning rate 0.0001
[2019-04-03 21:58:35,298] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3500, global step 55001: loss 1.2483
[2019-04-03 21:58:35,300] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3500, global step 55001: learning rate 0.0001
[2019-04-03 21:58:35,679] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55071: loss -4.1549
[2019-04-03 21:58:35,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55071: learning rate 0.0001
[2019-04-03 21:58:36,020] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 55133: loss 0.3980
[2019-04-03 21:58:36,020] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 55133: learning rate 0.0001
[2019-04-03 21:58:36,360] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55188: loss -3.4242
[2019-04-03 21:58:36,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55188: learning rate 0.0001
[2019-04-03 21:58:36,493] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55214: loss -1.3922
[2019-04-03 21:58:36,494] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55214: learning rate 0.0001
[2019-04-03 21:58:36,995] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 55291: loss 0.8982
[2019-04-03 21:58:36,995] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 55291: learning rate 0.0001
[2019-04-03 21:58:37,035] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 55298: loss 0.9758
[2019-04-03 21:58:37,035] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 55298: learning rate 0.0001
[2019-04-03 21:58:38,512] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55591: loss -2.7182
[2019-04-03 21:58:38,513] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55591: learning rate 0.0001
[2019-04-03 21:58:41,025] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3500, global step 56070: loss 0.5241
[2019-04-03 21:58:41,025] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3500, global step 56070: learning rate 0.0001
[2019-04-03 21:58:41,132] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 56096: loss 0.8087
[2019-04-03 21:58:41,133] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 56096: learning rate 0.0001
[2019-04-03 21:58:42,111] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56290: loss -1.3856
[2019-04-03 21:58:42,112] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56290: learning rate 0.0001
[2019-04-03 21:58:43,432] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56602: loss 0.5464
[2019-04-03 21:58:43,433] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56602: learning rate 0.0001
[2019-04-03 21:58:43,523] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56628: loss -1.5060
[2019-04-03 21:58:43,533] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56631: learning rate 0.0001
[2019-04-03 21:58:44,559] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56893: loss -1.1343
[2019-04-03 21:58:44,562] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56893: learning rate 0.0001
[2019-04-03 21:58:44,594] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56904: loss -3.2122
[2019-04-03 21:58:44,595] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56904: learning rate 0.0001
[2019-04-03 21:58:45,566] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 57163: loss -1.3093
[2019-04-03 21:58:45,567] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 57163: learning rate 0.0001
[2019-04-03 21:58:57,971] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9212962e-09 5.0755101e-04 3.3160140e-07 6.2778872e-04 2.6825506e-02
 4.2711821e-07 9.7203833e-01], sum to 1.0000
[2019-04-03 21:58:57,971] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0511
[2019-04-03 21:58:58,031] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 87.0, 0.0, 0.0, 26.0, 24.05837324889223, 0.06309067626810543, 0.0, 1.0, 43611.92552822136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2260800.0000, 
sim time next is 2261400.0000, 
raw observation next is [-8.483333333333334, 87.66666666666667, 0.0, 0.0, 26.0, 24.00770776592125, 0.05520447320968088, 0.0, 1.0, 43599.89011702919], 
processed observation next is [1.0, 0.17391304347826086, 0.2276084949215143, 0.8766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5006423138267708, 0.5184014910698936, 0.0, 1.0, 0.20761852436680564], 
reward next is 0.7924, 
noisyNet noise sample is [array([0.7769887], dtype=float32), 1.2111375]. 
=============================================
[2019-04-03 21:59:12,840] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 62970: loss 0.3131
[2019-04-03 21:59:12,841] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 62970: learning rate 0.0001
[2019-04-03 21:59:14,233] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 63239: loss -0.9426
[2019-04-03 21:59:14,235] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 63239: learning rate 0.0001
[2019-04-03 21:59:14,444] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 63273: loss -2.8108
[2019-04-03 21:59:14,445] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 63274: learning rate 0.0001
[2019-04-03 21:59:14,567] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4000, global step 63293: loss -2.6259
[2019-04-03 21:59:14,571] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4000, global step 63293: learning rate 0.0001
[2019-04-03 21:59:15,476] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63442: loss -1.9253
[2019-04-03 21:59:15,493] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63442: learning rate 0.0001
[2019-04-03 21:59:15,876] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 63521: loss -1.3385
[2019-04-03 21:59:15,880] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 63521: learning rate 0.0001
[2019-04-03 21:59:16,232] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63589: loss 0.2427
[2019-04-03 21:59:16,232] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63592: learning rate 0.0001
[2019-04-03 21:59:16,314] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 63619: loss 0.0877
[2019-04-03 21:59:16,314] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 63619: learning rate 0.0001
[2019-04-03 21:59:18,989] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 64244: loss 0.4229
[2019-04-03 21:59:18,993] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 64244: learning rate 0.0001
[2019-04-03 21:59:19,250] A3C_AGENT_WORKER-Thread-8 INFO:Local step 4000, global step 64302: loss 0.4144
[2019-04-03 21:59:19,265] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 4000, global step 64302: learning rate 0.0001
[2019-04-03 21:59:19,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1097884e-07 9.8127581e-04 6.1038776e-07 1.0959207e-03 9.6223161e-02
 1.5588922e-06 9.0169740e-01], sum to 1.0000
[2019-04-03 21:59:19,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3495
[2019-04-03 21:59:19,831] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.42792306477737, -0.1019867423006762, 0.0, 1.0, 44419.96303543804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2436000.0000, 
sim time next is 2436600.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.39197589197315, -0.1098567909943452, 0.0, 1.0, 44407.77557752245], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.44933132433109585, 0.4633810696685516, 0.0, 1.0, 0.21146559798820214], 
reward next is 0.7885, 
noisyNet noise sample is [array([-1.3445967], dtype=float32), -1.5296756]. 
=============================================
[2019-04-03 21:59:19,931] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64461: loss 0.4260
[2019-04-03 21:59:19,945] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64461: learning rate 0.0001
[2019-04-03 21:59:20,016] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4800961e-09 9.2865397e-05 8.5624734e-09 1.0415121e-04 9.0756407e-03
 3.0719654e-08 9.9072731e-01], sum to 1.0000
[2019-04-03 21:59:20,033] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8377
[2019-04-03 21:59:20,097] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 47.00000000000001, 110.6666666666667, 227.3333333333334, 26.0, 24.94902872094423, 0.3080088270480636, 0.0, 1.0, 28171.87178833127], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2387400.0000, 
sim time next is 2388000.0000, 
raw observation next is [0.0, 47.0, 98.33333333333333, 284.1666666666667, 26.0, 24.96495857966568, 0.3129836073187096, 0.0, 1.0, 20454.0355888984], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.3277777777777778, 0.31399631675874773, 0.6666666666666666, 0.58041321497214, 0.6043278691062365, 0.0, 1.0, 0.09740016947094476], 
reward next is 0.9026, 
noisyNet noise sample is [array([-0.83316994], dtype=float32), -0.48699385]. 
=============================================
[2019-04-03 21:59:20,156] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.57127 ]
 [80.71586 ]
 [80.856445]
 [81.06554 ]
 [81.29151 ]], R is [[80.57287598]
 [80.63299561]
 [80.64391327]
 [80.63078308]
 [80.64102173]].
[2019-04-03 21:59:20,836] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64639: loss 0.4709
[2019-04-03 21:59:20,836] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64639: learning rate 0.0001
[2019-04-03 21:59:22,011] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 64890: loss 0.5322
[2019-04-03 21:59:22,037] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 64890: learning rate 0.0001
[2019-04-03 21:59:22,498] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 65021: loss 0.5795
[2019-04-03 21:59:22,528] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 65025: learning rate 0.0001
[2019-04-03 21:59:22,852] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 65126: loss 0.5602
[2019-04-03 21:59:22,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 65126: learning rate 0.0001
[2019-04-03 21:59:24,368] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 65473: loss 0.6715
[2019-04-03 21:59:24,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 65473: learning rate 0.0001
[2019-04-03 21:59:24,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1900217e-09 7.9967402e-05 2.6403375e-08 2.9287091e-04 7.2049196e-03
 2.7910509e-08 9.9242216e-01], sum to 1.0000
[2019-04-03 21:59:24,644] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5329
[2019-04-03 21:59:24,849] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8999999999999999, 34.0, 0.0, 0.0, 26.0, 25.20039013585588, 0.2508426502497689, 0.0, 1.0, 40298.6105217578], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2500200.0000, 
sim time next is 2500800.0000, 
raw observation next is [-0.8, 34.33333333333334, 0.0, 0.0, 26.0, 25.18185990891742, 0.2475446971031683, 0.0, 1.0, 40253.90198284896], 
processed observation next is [0.0, 0.9565217391304348, 0.4404432132963989, 0.34333333333333343, 0.0, 0.0, 0.6666666666666666, 0.5984883257431184, 0.5825148990343895, 0.0, 1.0, 0.191685247537376], 
reward next is 0.8083, 
noisyNet noise sample is [array([1.4581487], dtype=float32), -0.41651046]. 
=============================================
[2019-04-03 21:59:45,575] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 70690: loss 0.6299
[2019-04-03 21:59:45,580] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 70691: learning rate 0.0001
[2019-04-03 21:59:46,880] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4500, global step 70986: loss 0.4593
[2019-04-03 21:59:46,881] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4500, global step 70986: learning rate 0.0001
[2019-04-03 21:59:46,989] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 71034: loss 0.4606
[2019-04-03 21:59:46,990] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 71034: learning rate 0.0001
[2019-04-03 21:59:47,334] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4500, global step 71120: loss 0.4854
[2019-04-03 21:59:47,388] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4500, global step 71120: learning rate 0.0001
[2019-04-03 21:59:48,404] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 71344: loss -1.4661
[2019-04-03 21:59:48,424] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 71344: learning rate 0.0001
[2019-04-03 21:59:48,594] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71389: loss 0.2943
[2019-04-03 21:59:48,610] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71391: learning rate 0.0001
[2019-04-03 21:59:48,720] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 71432: loss 0.3574
[2019-04-03 21:59:48,721] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 71432: learning rate 0.0001
[2019-04-03 21:59:49,200] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 71570: loss 0.2900
[2019-04-03 21:59:49,205] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 71571: learning rate 0.0001
[2019-04-03 21:59:50,573] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 71908: loss 0.2915
[2019-04-03 21:59:50,573] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 71908: learning rate 0.0001
[2019-04-03 21:59:51,738] A3C_AGENT_WORKER-Thread-8 INFO:Local step 4500, global step 72174: loss 0.3123
[2019-04-03 21:59:51,739] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 4500, global step 72174: learning rate 0.0001
[2019-04-03 21:59:51,872] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72207: loss 0.2930
[2019-04-03 21:59:51,899] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72207: learning rate 0.0001
[2019-04-03 21:59:52,760] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 72433: loss 0.2370
[2019-04-03 21:59:52,763] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 72433: learning rate 0.0001
[2019-04-03 21:59:53,979] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72734: loss 0.2156
[2019-04-03 21:59:53,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72734: learning rate 0.0001
[2019-04-03 21:59:54,044] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72757: loss 0.2292
[2019-04-03 21:59:54,048] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72757: learning rate 0.0001
[2019-04-03 21:59:54,373] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 72840: loss 0.2216
[2019-04-03 21:59:54,374] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 72840: learning rate 0.0001
[2019-04-03 21:59:56,834] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 73424: loss 0.1629
[2019-04-03 21:59:56,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 73424: learning rate 0.0001
[2019-04-03 22:00:05,855] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.2469597e-10 2.2056247e-06 7.3020612e-09 1.1218390e-04 3.7935909e-03
 2.1548682e-10 9.9609202e-01], sum to 1.0000
[2019-04-03 22:00:05,855] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3843
[2019-04-03 22:00:06,095] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 24.79273919836839, 0.3348117528267845, 0.0, 1.0, 183205.3147665771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2838600.0000, 
sim time next is 2839200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 24.84034566075241, 0.3610959420495708, 0.0, 1.0, 105649.935703486], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.5700288050627007, 0.6203653140165236, 0.0, 1.0, 0.5030949319213619], 
reward next is 0.4969, 
noisyNet noise sample is [array([0.20314005], dtype=float32), -1.1430234]. 
=============================================
[2019-04-03 22:00:13,459] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8527436e-12 6.9299915e-07 1.3731704e-11 5.5178070e-06 7.5193508e-05
 1.8631336e-11 9.9991858e-01], sum to 1.0000
[2019-04-03 22:00:13,459] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0782
[2019-04-03 22:00:13,478] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 164.0, 0.0, 26.0, 25.35667432947472, 0.3363077041993843, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2899800.0000, 
sim time next is 2900400.0000, 
raw observation next is [2.0, 100.0, 151.6666666666667, 0.0, 26.0, 25.39731256301352, 0.3385448379912159, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.5055555555555558, 0.0, 0.6666666666666666, 0.6164427135844601, 0.6128482793304053, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3792747], dtype=float32), 0.26680544]. 
=============================================
[2019-04-03 22:00:15,078] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7398662e-10 5.8447067e-06 1.4012098e-09 4.2196945e-05 1.7858453e-03
 1.0774004e-09 9.9816614e-01], sum to 1.0000
[2019-04-03 22:00:15,078] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2339
[2019-04-03 22:00:15,229] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.00000000000001, 0.0, 0.0, 26.0, 24.85789737026417, 0.3229278375925482, 0.0, 1.0, 43289.10546502016], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2941800.0000, 
sim time next is 2942400.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.82694007722237, 0.3150732121356645, 0.0, 1.0, 43291.54801895672], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5689116731018643, 0.6050244040452215, 0.0, 1.0, 0.20615022866169866], 
reward next is 0.7938, 
noisyNet noise sample is [array([0.2388832], dtype=float32), -1.8051586]. 
=============================================
[2019-04-03 22:00:16,213] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5000, global step 78671: loss 0.1472
[2019-04-03 22:00:16,213] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5000, global step 78671: learning rate 0.0001
[2019-04-03 22:00:16,744] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 78790: loss 0.1049
[2019-04-03 22:00:16,745] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 78790: learning rate 0.0001
[2019-04-03 22:00:16,890] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.4423829e-09 1.1362171e-03 2.6746326e-07 1.7686271e-03 2.2663511e-02
 2.7522464e-07 9.7443116e-01], sum to 1.0000
[2019-04-03 22:00:16,914] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0715
[2019-04-03 22:00:17,092] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.52220861900584, 0.2395859630912371, 0.0, 1.0, 42679.32363770415], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2950800.0000, 
sim time next is 2951400.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.48357815075277, 0.231484457791979, 0.0, 1.0, 42737.27427850183], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5402981792293975, 0.5771614859306596, 0.0, 1.0, 0.20351082989762775], 
reward next is 0.7965, 
noisyNet noise sample is [array([0.08637522], dtype=float32), -0.17345613]. 
=============================================
[2019-04-03 22:00:17,572] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5000, global step 78924: loss 0.0974
[2019-04-03 22:00:17,572] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5000, global step 78924: learning rate 0.0001
[2019-04-03 22:00:19,231] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.6125823e-08 1.0406420e-03 1.0655333e-07 2.0112409e-03 7.6880753e-02
 3.8203929e-08 9.2006713e-01], sum to 1.0000
[2019-04-03 22:00:19,237] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3060
[2019-04-03 22:00:19,282] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 79283: loss 0.0741
[2019-04-03 22:00:19,293] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 79284: learning rate 0.0001
[2019-04-03 22:00:19,447] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.14748836443009, 0.3184201692423906, 0.0, 1.0, 38639.58576301083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3018000.0000, 
sim time next is 3018600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.12005199160612, 0.3099281683767692, 0.0, 1.0, 38536.26499683742], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5933376659671765, 0.6033093894589231, 0.0, 1.0, 0.1835060237944639], 
reward next is 0.8165, 
noisyNet noise sample is [array([0.9002551], dtype=float32), -0.111127965]. 
=============================================
[2019-04-03 22:00:21,019] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 79729: loss 0.0765
[2019-04-03 22:00:21,020] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 79729: learning rate 0.0001
[2019-04-03 22:00:21,273] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 79763: loss 0.0786
[2019-04-03 22:00:21,274] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 79763: learning rate 0.0001
[2019-04-03 22:00:22,102] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79949: loss 0.0818
[2019-04-03 22:00:22,103] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79949: learning rate 0.0001
[2019-04-03 22:00:22,882] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 80071: loss 0.0828
[2019-04-03 22:00:22,884] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 80071: learning rate 0.0001
[2019-04-03 22:00:23,380] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 80163: loss 0.0843
[2019-04-03 22:00:23,382] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 80163: learning rate 0.0001
[2019-04-03 22:00:23,623] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80186: loss 0.0685
[2019-04-03 22:00:23,624] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80186: learning rate 0.0001
[2019-04-03 22:00:23,863] A3C_AGENT_WORKER-Thread-8 INFO:Local step 5000, global step 80219: loss 0.1013
[2019-04-03 22:00:23,864] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 5000, global step 80219: learning rate 0.0001
[2019-04-03 22:00:25,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.9258216e-09 1.4452504e-04 3.9127961e-08 3.8594371e-04 1.1415353e-02
 1.5761644e-08 9.8805410e-01], sum to 1.0000
[2019-04-03 22:00:25,284] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9400
[2019-04-03 22:00:25,345] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 55.83333333333334, 74.0, 602.6666666666667, 26.0, 25.13807049132855, 0.4095937275217976, 0.0, 1.0, 18706.97687464843], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2994600.0000, 
sim time next is 2995200.0000, 
raw observation next is [-1.0, 55.0, 69.5, 570.5, 26.0, 25.1485810504084, 0.4077719122581202, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.55, 0.23166666666666666, 0.6303867403314917, 0.6666666666666666, 0.5957150875340332, 0.6359239707527068, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40561968], dtype=float32), -0.32710668]. 
=============================================
[2019-04-03 22:00:25,915] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80710: loss -1.2914
[2019-04-03 22:00:25,917] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80710: learning rate 0.0001
[2019-04-03 22:00:25,942] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 80710: loss 0.1531
[2019-04-03 22:00:26,014] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 80710: learning rate 0.0001
[2019-04-03 22:00:26,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3982670e-08 8.7446102e-04 6.7486191e-08 1.9572228e-03 8.8313604e-03
 1.6967127e-07 9.8833662e-01], sum to 1.0000
[2019-04-03 22:00:26,224] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0116
[2019-04-03 22:00:26,237] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 72.33333333333334, 0.0, 0.0, 26.0, 23.73353029670082, -0.01534898816244457, 0.0, 1.0, 40256.92130183658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3046800.0000, 
sim time next is 3047400.0000, 
raw observation next is [-6.0, 73.5, 0.0, 0.0, 26.0, 23.70715570182383, -0.01997213569679513, 0.0, 1.0, 40308.97534106412], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.735, 0.0, 0.0, 0.6666666666666666, 0.47559630848531914, 0.4933426214344016, 0.0, 1.0, 0.19194750162411486], 
reward next is 0.8081, 
noisyNet noise sample is [array([-0.32873872], dtype=float32), 0.18028028]. 
=============================================
[2019-04-03 22:00:26,445] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 80822: loss 0.1538
[2019-04-03 22:00:26,466] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 80822: learning rate 0.0001
[2019-04-03 22:00:27,578] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 81104: loss 0.1979
[2019-04-03 22:00:27,579] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 81104: learning rate 0.0001
[2019-04-03 22:00:28,740] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 81371: loss 0.2775
[2019-04-03 22:00:28,743] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 81371: learning rate 0.0001
[2019-04-03 22:00:29,559] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.6030361e-10 2.9563362e-05 2.3261710e-09 2.6002823e-04 8.2427950e-04
 7.0247164e-09 9.9888617e-01], sum to 1.0000
[2019-04-03 22:00:29,559] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4955
[2019-04-03 22:00:29,616] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 54.0, 102.5, 697.0, 26.0, 25.24264193947732, 0.3175440634906181, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3060000.0000, 
sim time next is 3060600.0000, 
raw observation next is [-4.0, 54.0, 103.6666666666667, 717.6666666666667, 26.0, 25.20023536099408, 0.3178828279636817, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.34555555555555567, 0.7930018416206263, 0.6666666666666666, 0.6000196134161732, 0.6059609426545606, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1767713], dtype=float32), -1.5228125]. 
=============================================
[2019-04-03 22:00:33,155] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.04773294e-10 3.14449753e-05 6.59276411e-10 1.18182506e-04
 2.34107720e-03 8.87383611e-10 9.97509360e-01], sum to 1.0000
[2019-04-03 22:00:33,155] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2928
[2019-04-03 22:00:33,198] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 100.0, 0.0, 0.0, 26.0, 25.36157446763756, 0.3415331958092201, 0.0, 1.0, 48787.61044959391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3105000.0000, 
sim time next is 3105600.0000, 
raw observation next is [-0.3333333333333334, 100.0, 0.0, 0.0, 26.0, 25.39681327088925, 0.346671930546594, 0.0, 1.0, 30949.25314093123], 
processed observation next is [0.0, 0.9565217391304348, 0.4533702677747, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6164011059074376, 0.615557310182198, 0.0, 1.0, 0.14737739590919632], 
reward next is 0.8526, 
noisyNet noise sample is [array([1.8301619], dtype=float32), -0.81209934]. 
=============================================
[2019-04-03 22:00:40,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.3688444e-13 1.8527103e-07 4.1436216e-12 2.4917383e-06 4.8107468e-05
 6.4400100e-12 9.9994922e-01], sum to 1.0000
[2019-04-03 22:00:40,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8060
[2019-04-03 22:00:40,300] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.833333333333334, 100.0, 88.33333333333334, 477.0, 26.0, 26.07389162331545, 0.5029232584740516, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3142200.0000, 
sim time next is 3142800.0000, 
raw observation next is [7.0, 100.0, 91.0, 519.5, 26.0, 26.21322687214066, 0.5176531774067985, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.30333333333333334, 0.5740331491712707, 0.6666666666666666, 0.6844355726783883, 0.6725510591355995, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.25388846], dtype=float32), -0.57077295]. 
=============================================
[2019-04-03 22:00:42,961] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.2295282e-11 4.0455980e-05 1.6745036e-09 1.8937375e-05 1.2448516e-04
 3.8385792e-10 9.9981624e-01], sum to 1.0000
[2019-04-03 22:00:42,962] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4428
[2019-04-03 22:00:42,980] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.499773263622, 0.5488533285574565, 0.0, 1.0, 36777.34090674205], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3208800.0000, 
sim time next is 3209400.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.54497960224998, 0.5478956903448484, 0.0, 1.0, 18745.08055169804], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6287483001874984, 0.6826318967816162, 0.0, 1.0, 0.08926228834141924], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.3980169], dtype=float32), -1.460611]. 
=============================================
[2019-04-03 22:00:43,919] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5500, global step 86091: loss 0.0171
[2019-04-03 22:00:43,919] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5500, global step 86091: learning rate 0.0001
[2019-04-03 22:00:44,674] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5500, global step 86360: loss 0.0037
[2019-04-03 22:00:44,676] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5500, global step 86360: learning rate 0.0001
[2019-04-03 22:00:45,234] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 86534: loss 0.0053
[2019-04-03 22:00:45,265] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 86534: learning rate 0.0001
[2019-04-03 22:00:47,466] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 87215: loss 0.0063
[2019-04-03 22:00:47,467] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 87215: learning rate 0.0001
[2019-04-03 22:00:48,113] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 87443: loss 0.0104
[2019-04-03 22:00:48,113] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 87443: learning rate 0.0001
[2019-04-03 22:00:48,558] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 87587: loss 0.0013
[2019-04-03 22:00:48,558] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 87587: learning rate 0.0001
[2019-04-03 22:00:48,871] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87699: loss 0.0034
[2019-04-03 22:00:48,872] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87699: learning rate 0.0001
[2019-04-03 22:00:49,887] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 88009: loss 0.0088
[2019-04-03 22:00:49,888] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 88009: learning rate 0.0001
[2019-04-03 22:00:50,002] A3C_AGENT_WORKER-Thread-8 INFO:Local step 5500, global step 88047: loss 0.0046
[2019-04-03 22:00:50,003] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 5500, global step 88047: learning rate 0.0001
[2019-04-03 22:00:50,820] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 88309: loss 0.0095
[2019-04-03 22:00:50,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 88309: learning rate 0.0001
[2019-04-03 22:00:50,862] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 88323: loss 0.0047
[2019-04-03 22:00:50,867] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 88323: learning rate 0.0001
[2019-04-03 22:00:51,416] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 88511: loss 0.0147
[2019-04-03 22:00:51,417] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 88511: learning rate 0.0001
[2019-04-03 22:00:52,002] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88716: loss 0.0055
[2019-04-03 22:00:52,004] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88716: learning rate 0.0001
[2019-04-03 22:00:52,635] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88907: loss 0.0071
[2019-04-03 22:00:52,636] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88907: learning rate 0.0001
[2019-04-03 22:00:53,079] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 89053: loss 0.0101
[2019-04-03 22:00:53,081] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 89053: learning rate 0.0001
[2019-04-03 22:00:54,695] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 89551: loss 0.0396
[2019-04-03 22:00:54,710] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 89551: learning rate 0.0001
[2019-04-03 22:00:55,136] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9412743e-10 2.3331080e-05 7.9797752e-10 2.4133142e-04 2.2769012e-04
 2.7293431e-10 9.9950767e-01], sum to 1.0000
[2019-04-03 22:00:55,137] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7490
[2019-04-03 22:00:55,191] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 48.0, 87.0, 674.0, 26.0, 26.70627341513327, 0.6046629561827696, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3339000.0000, 
sim time next is 3339600.0000, 
raw observation next is [-2.333333333333333, 47.33333333333333, 82.5, 645.1666666666667, 26.0, 26.0538704899812, 0.6325342196124738, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3979686057248385, 0.4733333333333333, 0.275, 0.7128913443830571, 0.6666666666666666, 0.6711558741651, 0.7108447398708245, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29045907], dtype=float32), -0.04359956]. 
=============================================
[2019-04-03 22:00:55,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8776766e-10 3.9303002e-05 2.9974240e-10 1.3115176e-04 2.8797926e-04
 5.3266358e-10 9.9954152e-01], sum to 1.0000
[2019-04-03 22:00:55,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2384
[2019-04-03 22:00:55,309] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.666666666666667, 68.0, 109.8333333333333, 719.1666666666667, 26.0, 26.31540119310965, 0.5701598932122728, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3320400.0000, 
sim time next is 3321000.0000, 
raw observation next is [-7.5, 67.0, 111.0, 740.0, 26.0, 26.32762022032994, 0.5766294495061444, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2548476454293629, 0.67, 0.37, 0.8176795580110497, 0.6666666666666666, 0.6939683516941617, 0.6922098165020482, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.365777], dtype=float32), 0.35418746]. 
=============================================
[2019-04-03 22:00:55,318] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[86.02207]
 [86.11234]
 [86.29052]
 [86.29841]
 [86.19036]], R is [[85.98202515]
 [86.12220764]
 [86.26098633]
 [86.39837646]
 [86.53439331]].
[2019-04-03 22:00:56,669] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3414891e-11 1.0960712e-05 2.3420171e-10 5.2555674e-06 4.5028815e-04
 1.5688471e-11 9.9953353e-01], sum to 1.0000
[2019-04-03 22:00:56,690] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2568
[2019-04-03 22:00:56,808] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 49.33333333333334, 43.66666666666667, 378.3333333333334, 26.0, 26.53499404322155, 0.6501699207528522, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3343800.0000, 
sim time next is 3344400.0000, 
raw observation next is [-2.0, 50.0, 35.5, 317.0, 26.0, 26.27075852308992, 0.6276357247293252, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.40720221606648205, 0.5, 0.11833333333333333, 0.35027624309392263, 0.6666666666666666, 0.68922987692416, 0.7092119082431084, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6972879], dtype=float32), 0.20095523]. 
=============================================
[2019-04-03 22:01:00,509] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4677504e-11 2.9749036e-04 1.3443696e-09 7.2182076e-05 9.4274880e-04
 1.3989021e-09 9.9868757e-01], sum to 1.0000
[2019-04-03 22:01:00,517] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7074
[2019-04-03 22:01:00,579] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.110223024625157e-16, 56.00000000000001, 97.0, 618.6666666666667, 26.0, 26.26090584973278, 0.5426733471029316, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3403200.0000, 
sim time next is 3403800.0000, 
raw observation next is [0.5, 54.0, 99.0, 658.0, 26.0, 26.34304856520561, 0.5625553505450777, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.54, 0.33, 0.7270718232044199, 0.6666666666666666, 0.6952540471004675, 0.6875184501816926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86667836], dtype=float32), 0.31237835]. 
=============================================
[2019-04-03 22:01:06,123] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6000, global step 94287: loss 0.0233
[2019-04-03 22:01:06,127] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6000, global step 94289: learning rate 0.0001
[2019-04-03 22:01:06,224] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6000, global step 94342: loss 0.0169
[2019-04-03 22:01:06,226] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6000, global step 94342: learning rate 0.0001
[2019-04-03 22:01:07,009] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 94705: loss 0.0023
[2019-04-03 22:01:07,010] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 94705: learning rate 0.0001
[2019-04-03 22:01:07,583] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 94967: loss 0.0000
[2019-04-03 22:01:07,599] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 94977: learning rate 0.0001
[2019-04-03 22:01:08,861] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 95588: loss 0.0072
[2019-04-03 22:01:08,863] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 95588: learning rate 0.0001
[2019-04-03 22:01:08,873] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2438455e-08 3.5934611e-03 1.2022623e-07 5.2659732e-04 7.7376748e-03
 6.7269490e-08 9.8814207e-01], sum to 1.0000
[2019-04-03 22:01:08,876] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9207
[2019-04-03 22:01:08,887] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.666666666666668, 25.66666666666667, 0.0, 0.0, 26.0, 25.5511002593219, 0.3612247262164186, 0.0, 1.0, 18745.17978307895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3651600.0000, 
sim time next is 3652200.0000, 
raw observation next is [9.5, 26.0, 0.0, 0.0, 26.0, 25.53540918883996, 0.3563181339503146, 0.0, 1.0, 20387.70307928355], 
processed observation next is [0.0, 0.2608695652173913, 0.7257617728531857, 0.26, 0.0, 0.0, 0.6666666666666666, 0.6279507657366633, 0.6187727113167715, 0.0, 1.0, 0.09708430037754072], 
reward next is 0.9029, 
noisyNet noise sample is [array([1.6403562], dtype=float32), -0.5730477]. 
=============================================
[2019-04-03 22:01:08,980] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95653: loss -0.0007
[2019-04-03 22:01:08,986] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95655: learning rate 0.0001
[2019-04-03 22:01:09,281] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 95784: loss 0.0018
[2019-04-03 22:01:09,284] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 95786: learning rate 0.0001
[2019-04-03 22:01:10,011] A3C_AGENT_WORKER-Thread-8 INFO:Local step 6000, global step 96098: loss 0.0073
[2019-04-03 22:01:10,012] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 6000, global step 96098: learning rate 0.0001
[2019-04-03 22:01:10,309] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6979711e-09 2.5627185e-03 1.5141948e-08 4.2146738e-04 1.2833985e-03
 2.0954818e-08 9.9573237e-01], sum to 1.0000
[2019-04-03 22:01:10,309] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9977
[2019-04-03 22:01:10,340] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.50561489477938, 0.4318808285389823, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3615600.0000, 
sim time next is 3616200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.54917882427235, 0.4252203654692376, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6290982353560292, 0.6417401218230792, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23884666], dtype=float32), -0.9642894]. 
=============================================
[2019-04-03 22:01:10,870] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 96491: loss -0.7093
[2019-04-03 22:01:10,871] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 96491: learning rate 0.0001
[2019-04-03 22:01:11,012] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 96565: loss 0.0259
[2019-04-03 22:01:11,014] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 96566: learning rate 0.0001
[2019-04-03 22:01:11,104] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96611: loss -1.2331
[2019-04-03 22:01:11,106] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96611: learning rate 0.0001
[2019-04-03 22:01:11,503] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 96812: loss 0.0057
[2019-04-03 22:01:11,505] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 96815: learning rate 0.0001
[2019-04-03 22:01:12,183] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 97157: loss 0.0097
[2019-04-03 22:01:12,183] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 97157: learning rate 0.0001
[2019-04-03 22:01:12,279] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 97218: loss 0.0111
[2019-04-03 22:01:12,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 97218: learning rate 0.0001
[2019-04-03 22:01:12,496] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 97330: loss 0.0248
[2019-04-03 22:01:12,508] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 97331: learning rate 0.0001
[2019-04-03 22:01:13,911] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 98070: loss 0.1109
[2019-04-03 22:01:13,911] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 98070: learning rate 0.0001
[2019-04-03 22:01:16,293] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8002975e-10 3.4041066e-05 7.9522083e-10 2.9427445e-04 5.3825235e-04
 5.1370652e-10 9.9913341e-01], sum to 1.0000
[2019-04-03 22:01:16,294] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5917
[2019-04-03 22:01:16,301] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 44.33333333333334, 105.5, 783.0, 26.0, 25.38568942612213, 0.4744536898532328, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3680400.0000, 
sim time next is 3681000.0000, 
raw observation next is [6.0, 45.0, 104.0, 776.0, 26.0, 25.43216353503763, 0.4840871601265515, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.45, 0.3466666666666667, 0.8574585635359117, 0.6666666666666666, 0.6193469612531359, 0.6613623867088505, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18401125], dtype=float32), 1.0430648]. 
=============================================
[2019-04-03 22:01:16,305] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[89.687386]
 [89.699776]
 [89.71997 ]
 [89.76353 ]
 [89.85416 ]], R is [[89.75753021]
 [89.85995483]
 [89.96135712]
 [90.06174469]
 [90.16112518]].
[2019-04-03 22:01:16,787] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.2869516e-11 4.0982431e-04 7.0677858e-10 1.7478567e-05 2.4451513e-04
 7.8387852e-10 9.9932814e-01], sum to 1.0000
[2019-04-03 22:01:16,789] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5166
[2019-04-03 22:01:16,810] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 67.0, 0.0, 0.0, 26.0, 25.38746207620942, 0.3899052865941433, 0.0, 1.0, 43644.20503397665], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3712800.0000, 
sim time next is 3713400.0000, 
raw observation next is [-3.0, 68.0, 0.0, 0.0, 26.0, 25.3769960242748, 0.3879512731957052, 0.0, 1.0, 47192.68964014237], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6147496686895666, 0.6293170910652351, 0.0, 1.0, 0.2247270935244875], 
reward next is 0.7753, 
noisyNet noise sample is [array([-1.8257245], dtype=float32), 1.573065]. 
=============================================
[2019-04-03 22:01:17,528] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-03 22:01:17,530] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:01:17,531] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:01:17,531] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:01:17,532] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:01:17,533] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:01:17,535] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:01:17,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run2
[2019-04-03 22:01:17,540] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run2
[2019-04-03 22:01:17,567] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run2
[2019-04-03 22:01:58,200] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.20674337], dtype=float32), 0.17511941]
[2019-04-03 22:01:58,201] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.775557561562891e-17, 68.5, 0.0, 0.0, 26.0, 25.05114603633005, 0.301761482894924, 0.0, 1.0, 37969.0278484925]
[2019-04-03 22:01:58,201] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:01:58,201] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.2546558e-10 7.0016901e-04 2.8563387e-09 3.1397492e-04 1.1751204e-03
 4.6212869e-09 9.9781078e-01], sampled 0.49261759790201287
[2019-04-03 22:02:53,222] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.20674337], dtype=float32), 0.17511941]
[2019-04-03 22:02:53,222] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.879747248, 50.0070141, 86.48930103, 673.3572466, 26.0, 25.17941956260518, 0.3540679933804154, 0.0, 1.0, 0.0]
[2019-04-03 22:02:53,222] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:02:53,222] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.3915908e-09 2.4126505e-04 2.9069038e-09 3.2093859e-04 1.6932940e-03
 1.8290791e-09 9.9774444e-01], sampled 0.8826710195353874
[2019-04-03 22:03:11,108] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-03 22:03:37,236] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7902 263384057.0069 1551.8724
[2019-04-03 22:03:40,081] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6342 275806815.2143 1233.1762
[2019-04-03 22:03:41,127] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 100000, evaluation results [100000.0, 7241.790204729313, 263384057.00685066, 1551.8724049905088, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.634213265249, 275806815.21429336, 1233.1762374343539]
[2019-04-03 22:03:47,559] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6500, global step 102095: loss 0.1155
[2019-04-03 22:03:47,561] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6500, global step 102095: learning rate 0.0001
[2019-04-03 22:03:47,911] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6500, global step 102217: loss 0.0793
[2019-04-03 22:03:47,912] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6500, global step 102218: learning rate 0.0001
[2019-04-03 22:03:48,544] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 102401: loss 0.1037
[2019-04-03 22:03:48,546] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 102401: learning rate 0.0001
[2019-04-03 22:03:48,701] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.4202542e-10 1.7517102e-04 2.2977225e-08 3.0719908e-04 9.9841668e-04
 2.2938430e-08 9.9851912e-01], sum to 1.0000
[2019-04-03 22:03:48,703] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0292
[2019-04-03 22:03:48,716] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.55308843031354, 0.3987438724202033, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3898200.0000, 
sim time next is 3898800.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.39956451122486, 0.3781834128939551, 0.0, 1.0, 89275.63470299296], 
processed observation next is [1.0, 0.13043478260869565, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6166303759354049, 0.6260611376313183, 0.0, 1.0, 0.4251220700142522], 
reward next is 0.5749, 
noisyNet noise sample is [array([1.6277987], dtype=float32), -0.43645126]. 
=============================================
[2019-04-03 22:03:48,804] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 102492: loss 0.0946
[2019-04-03 22:03:48,836] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 102493: learning rate 0.0001
[2019-04-03 22:03:52,456] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 103593: loss 0.1304
[2019-04-03 22:03:52,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 103593: learning rate 0.0001
[2019-04-03 22:03:52,673] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 103662: loss 0.1374
[2019-04-03 22:03:52,674] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 103662: learning rate 0.0001
[2019-04-03 22:03:53,003] A3C_AGENT_WORKER-Thread-8 INFO:Local step 6500, global step 103780: loss 0.1153
[2019-04-03 22:03:53,003] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 6500, global step 103780: learning rate 0.0001
[2019-04-03 22:03:53,279] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103874: loss 0.1180
[2019-04-03 22:03:53,280] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103875: learning rate 0.0001
[2019-04-03 22:03:54,033] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 104135: loss 0.1316
[2019-04-03 22:03:54,033] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 104135: learning rate 0.0001
[2019-04-03 22:03:54,514] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104286: loss 0.1430
[2019-04-03 22:03:54,516] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104286: learning rate 0.0001
[2019-04-03 22:03:54,580] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 104313: loss 0.1125
[2019-04-03 22:03:54,582] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 104313: learning rate 0.0001
[2019-04-03 22:03:55,078] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 104476: loss 1.0642
[2019-04-03 22:03:55,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 104476: learning rate 0.0001
[2019-04-03 22:03:55,813] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104738: loss 0.1099
[2019-04-03 22:03:55,813] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104738: learning rate 0.0001
[2019-04-03 22:03:57,014] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 105093: loss 0.0808
[2019-04-03 22:03:57,027] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 105093: learning rate 0.0001
[2019-04-03 22:03:57,147] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 105136: loss 0.7678
[2019-04-03 22:03:57,169] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 105142: learning rate 0.0001
[2019-04-03 22:03:59,517] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 105804: loss 0.0589
[2019-04-03 22:03:59,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 105804: learning rate 0.0001
[2019-04-03 22:04:06,942] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9366924e-10 2.5573456e-05 9.9094999e-10 2.5664651e-04 2.6888883e-04
 2.1187342e-10 9.9944884e-01], sum to 1.0000
[2019-04-03 22:04:06,942] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8869
[2019-04-03 22:04:06,981] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 31.66666666666666, 117.3333333333333, 839.1666666666667, 26.0, 26.22484852527413, 0.5598719934177322, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4020000.0000, 
sim time next is 4020600.0000, 
raw observation next is [-4.333333333333333, 30.33333333333333, 116.6666666666667, 837.3333333333334, 26.0, 25.91252103218011, 0.5290256711334403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.342566943674977, 0.3033333333333333, 0.388888888888889, 0.9252302025782689, 0.6666666666666666, 0.6593767526816757, 0.6763418903778134, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8399045], dtype=float32), 0.45005718]. 
=============================================
[2019-04-03 22:04:07,813] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7116437e-09 5.5707437e-03 5.9275848e-08 1.1650717e-03 5.3067007e-03
 6.4026004e-08 9.8795742e-01], sum to 1.0000
[2019-04-03 22:04:07,823] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0345
[2019-04-03 22:04:07,853] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 38.0, 0.0, 0.0, 26.0, 25.04249686603852, 0.284858269173319, 0.0, 1.0, 40731.10329138637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4071600.0000, 
sim time next is 4072200.0000, 
raw observation next is [-5.0, 38.5, 0.0, 0.0, 26.0, 25.04526327714862, 0.2812463308696852, 0.0, 1.0, 40678.52027234907], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.385, 0.0, 0.0, 0.6666666666666666, 0.5871052730957184, 0.5937487769565618, 0.0, 1.0, 0.19370723939213844], 
reward next is 0.8063, 
noisyNet noise sample is [array([-0.75175244], dtype=float32), 0.3079668]. 
=============================================
[2019-04-03 22:04:08,288] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.4066623e-10 1.4641625e-04 1.5358366e-09 2.4953912e-04 7.8900834e-04
 3.4719507e-09 9.9881506e-01], sum to 1.0000
[2019-04-03 22:04:08,288] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1360
[2019-04-03 22:04:08,320] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 40.16666666666666, 0.0, 0.0, 26.0, 25.35960760673904, 0.4191466188464352, 0.0, 1.0, 51480.76333918254], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4151400.0000, 
sim time next is 4152000.0000, 
raw observation next is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 26.0, 25.33291582289689, 0.4149989718932308, 0.0, 1.0, 44756.25450635418], 
processed observation next is [0.0, 0.043478260869565216, 0.42566943674976926, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.6110763185747409, 0.638332990631077, 0.0, 1.0, 0.21312502145882942], 
reward next is 0.7869, 
noisyNet noise sample is [array([1.744721], dtype=float32), 0.14843857]. 
=============================================
[2019-04-03 22:04:08,338] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[81.99666]
 [82.10729]
 [82.11836]
 [82.45531]
 [82.97778]], R is [[81.60313416]
 [81.54195404]
 [81.51805115]
 [81.59931946]
 [81.65745544]].
[2019-04-03 22:04:11,934] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7000, global step 109846: loss 0.1259
[2019-04-03 22:04:11,936] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7000, global step 109846: learning rate 0.0001
[2019-04-03 22:04:13,054] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 110218: loss 0.1450
[2019-04-03 22:04:13,056] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 110218: learning rate 0.0001
[2019-04-03 22:04:13,137] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7000, global step 110239: loss 0.1641
[2019-04-03 22:04:13,138] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7000, global step 110239: learning rate 0.0001
[2019-04-03 22:04:14,414] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 110679: loss 0.1094
[2019-04-03 22:04:14,418] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 110679: learning rate 0.0001
[2019-04-03 22:04:18,179] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111887: loss 0.1662
[2019-04-03 22:04:18,193] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111893: learning rate 0.0001
[2019-04-03 22:04:18,582] A3C_AGENT_WORKER-Thread-8 INFO:Local step 7000, global step 112000: loss 0.1990
[2019-04-03 22:04:18,591] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 7000, global step 112000: learning rate 0.0001
[2019-04-03 22:04:18,644] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 112022: loss 0.1748
[2019-04-03 22:04:18,645] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 112022: learning rate 0.0001
[2019-04-03 22:04:18,820] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 112081: loss 0.2079
[2019-04-03 22:04:18,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 112082: learning rate 0.0001
[2019-04-03 22:04:19,298] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 112264: loss 0.2649
[2019-04-03 22:04:19,299] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 112264: learning rate 0.0001
[2019-04-03 22:04:19,566] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112353: loss 0.2727
[2019-04-03 22:04:19,567] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112353: learning rate 0.0001
[2019-04-03 22:04:19,776] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 112433: loss 0.2669
[2019-04-03 22:04:19,777] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 112433: learning rate 0.0001
[2019-04-03 22:04:19,810] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 112445: loss 0.2001
[2019-04-03 22:04:19,812] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 112445: learning rate 0.0001
[2019-04-03 22:04:21,486] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 113024: loss 0.2432
[2019-04-03 22:04:21,490] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 113025: learning rate 0.0001
[2019-04-03 22:04:21,861] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 113186: loss 0.2516
[2019-04-03 22:04:21,862] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 113186: learning rate 0.0001
[2019-04-03 22:04:22,233] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.12926815e-08 3.43498634e-03 5.01928810e-08 2.48486921e-03
 1.42822694e-02 5.45717391e-08 9.79797721e-01], sum to 1.0000
[2019-04-03 22:04:22,235] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6357
[2019-04-03 22:04:22,428] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.35876651397377, 0.3201582309266646, 0.0, 1.0, 69163.77139398515], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4249800.0000, 
sim time next is 4250400.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.32355858071357, 0.3204419322102182, 0.0, 1.0, 53152.20337123], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6102965483927975, 0.606813977403406, 0.0, 1.0, 0.2531057303391905], 
reward next is 0.7469, 
noisyNet noise sample is [array([-0.9119312], dtype=float32), 0.8026278]. 
=============================================
[2019-04-03 22:04:22,708] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 113507: loss 0.1858
[2019-04-03 22:04:22,710] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 113508: learning rate 0.0001
[2019-04-03 22:04:25,487] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 114440: loss 0.3353
[2019-04-03 22:04:25,491] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 114441: learning rate 0.0001
[2019-04-03 22:04:26,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2752954e-11 1.9732408e-06 9.0625445e-11 7.5936339e-05 1.2186060e-04
 3.7685608e-11 9.9980026e-01], sum to 1.0000
[2019-04-03 22:04:26,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0386
[2019-04-03 22:04:26,016] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.0, 36.0, 49.66666666666666, 0.0, 26.0, 27.90900468853622, 1.052158556302801, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4378800.0000, 
sim time next is 4379400.0000, 
raw observation next is [13.0, 36.5, 39.0, 0.0, 26.0, 28.25499075099443, 1.07830264428852, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.365, 0.13, 0.0, 0.6666666666666666, 0.8545825625828692, 0.8594342147628401, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2289296], dtype=float32), 0.110360585]. 
=============================================
[2019-04-03 22:04:28,658] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4599861e-11 1.5284761e-05 1.3952485e-10 3.4056080e-05 6.3932472e-05
 1.7643381e-10 9.9988675e-01], sum to 1.0000
[2019-04-03 22:04:28,658] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8687
[2019-04-03 22:04:28,693] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.466666666666667, 75.66666666666667, 0.0, 0.0, 26.0, 25.51303020706951, 0.4002701503412173, 0.0, 1.0, 58520.7963269426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4318800.0000, 
sim time next is 4319400.0000, 
raw observation next is [4.483333333333333, 75.83333333333333, 0.0, 0.0, 26.0, 25.47313929112457, 0.4013011820039227, 0.0, 1.0, 66222.36309274769], 
processed observation next is [0.0, 1.0, 0.5867959372114497, 0.7583333333333333, 0.0, 0.0, 0.6666666666666666, 0.6227616075937142, 0.6337670606679743, 0.0, 1.0, 0.3153445861559414], 
reward next is 0.6847, 
noisyNet noise sample is [array([-1.0706701], dtype=float32), -0.39474976]. 
=============================================
[2019-04-03 22:04:28,996] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4667111e-12 2.3637608e-06 1.4671547e-10 1.8348317e-05 6.0498740e-05
 3.3329006e-11 9.9991870e-01], sum to 1.0000
[2019-04-03 22:04:28,999] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6259
[2019-04-03 22:04:29,015] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.65, 61.83333333333334, 0.0, 0.0, 26.0, 25.94296269554719, 0.6199476950015744, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4402200.0000, 
sim time next is 4402800.0000, 
raw observation next is [8.5, 62.0, 0.0, 0.0, 26.0, 25.85675504078742, 0.6024882910131673, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.698060941828255, 0.62, 0.0, 0.0, 0.6666666666666666, 0.654729586732285, 0.7008294303377225, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33980876], dtype=float32), -1.8605609]. 
=============================================
[2019-04-03 22:04:30,579] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.7162170e-12 1.8593720e-06 7.3806707e-12 3.3928627e-05 2.9349203e-05
 4.8528048e-12 9.9993491e-01], sum to 1.0000
[2019-04-03 22:04:30,579] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0301
[2019-04-03 22:04:30,608] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 26.0, 26.65916462575019, 0.8709173970385912, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4366200.0000, 
sim time next is 4366800.0000, 
raw observation next is [14.6, 29.0, 116.5, 847.5, 26.0, 27.12831822885573, 0.920304773554478, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8670360110803325, 0.29, 0.3883333333333333, 0.93646408839779, 0.6666666666666666, 0.7606931857379774, 0.8067682578514926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.347751], dtype=float32), -0.89278024]. 
=============================================
[2019-04-03 22:04:35,163] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7500, global step 117880: loss 0.7430
[2019-04-03 22:04:35,163] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7500, global step 117880: learning rate 0.0001
[2019-04-03 22:04:35,251] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 117913: loss 0.7142
[2019-04-03 22:04:35,252] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 117913: learning rate 0.0001
[2019-04-03 22:04:35,765] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7500, global step 118099: loss 0.7954
[2019-04-03 22:04:35,765] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7500, global step 118099: learning rate 0.0001
[2019-04-03 22:04:36,547] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.0449891e-11 1.9521673e-05 5.1944099e-10 4.0224080e-05 1.0303159e-04
 2.2718724e-10 9.9983728e-01], sum to 1.0000
[2019-04-03 22:04:36,549] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6512
[2019-04-03 22:04:36,566] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.38996922372194, 0.4341119730864333, 0.0, 1.0, 112798.12922161], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4498200.0000, 
sim time next is 4498800.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.25928496614999, 0.4387687134486429, 0.0, 1.0, 87011.05573950936], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6049404138458326, 0.6462562378162143, 0.0, 1.0, 0.41433836066433033], 
reward next is 0.5857, 
noisyNet noise sample is [array([-1.7069921], dtype=float32), 0.00088367204]. 
=============================================
[2019-04-03 22:04:36,635] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 118422: loss 0.7178
[2019-04-03 22:04:36,660] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 118423: learning rate 0.0001
[2019-04-03 22:04:40,038] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2964802e-10 1.4234200e-04 1.4169276e-09 1.0932533e-04 4.0608441e-04
 2.1504991e-10 9.9934214e-01], sum to 1.0000
[2019-04-03 22:04:40,041] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7050
[2019-04-03 22:04:40,057] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.883333333333333, 80.66666666666667, 80.00000000000001, 154.6666666666667, 26.0, 25.48956545495795, 0.493740319267407, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4435800.0000, 
sim time next is 4436400.0000, 
raw observation next is [1.766666666666667, 81.33333333333334, 100.0, 193.3333333333333, 26.0, 25.49180058434925, 0.5191045215273543, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5115420129270545, 0.8133333333333335, 0.3333333333333333, 0.21362799263351745, 0.6666666666666666, 0.6243167153624375, 0.6730348405091181, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.79792976], dtype=float32), -1.6092347]. 
=============================================
[2019-04-03 22:04:41,049] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119789: loss 1.0064
[2019-04-03 22:04:41,050] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119789: learning rate 0.0001
[2019-04-03 22:04:41,642] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119944: loss 0.9961
[2019-04-03 22:04:41,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119944: learning rate 0.0001
[2019-04-03 22:04:41,704] A3C_AGENT_WORKER-Thread-8 INFO:Local step 7500, global step 119962: loss 8.5760
[2019-04-03 22:04:41,705] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 7500, global step 119962: learning rate 0.0001
[2019-04-03 22:04:42,061] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 120062: loss 0.9910
[2019-04-03 22:04:42,062] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 120062: learning rate 0.0001
[2019-04-03 22:04:43,048] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 120409: loss 1.0008
[2019-04-03 22:04:43,050] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 120410: learning rate 0.0001
[2019-04-03 22:04:43,373] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120530: loss 0.9442
[2019-04-03 22:04:43,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120530: learning rate 0.0001
[2019-04-03 22:04:43,418] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 120544: loss 0.9398
[2019-04-03 22:04:43,420] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 120545: learning rate 0.0001
[2019-04-03 22:04:43,667] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 120634: loss 0.8934
[2019-04-03 22:04:43,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 120634: learning rate 0.0001
[2019-04-03 22:04:44,783] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4672872e-09 3.9327452e-03 2.8045601e-08 3.6439912e-03 1.8874423e-03
 1.6733607e-08 9.9053580e-01], sum to 1.0000
[2019-04-03 22:04:44,783] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9855
[2019-04-03 22:04:44,834] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 71.0, 0.0, 0.0, 26.0, 25.29154848797237, 0.4083542448904403, 0.0, 1.0, 41284.71107346577], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4512000.0000, 
sim time next is 4512600.0000, 
raw observation next is [-0.9, 71.0, 0.0, 0.0, 26.0, 25.2772795845034, 0.4033232025024243, 0.0, 1.0, 41206.78219669916], 
processed observation next is [1.0, 0.21739130434782608, 0.43767313019390586, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6064399653752833, 0.6344410675008081, 0.0, 1.0, 0.1962227723652341], 
reward next is 0.8038, 
noisyNet noise sample is [array([0.81364566], dtype=float32), -1.0166075]. 
=============================================
[2019-04-03 22:04:45,215] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 121043: loss 0.8204
[2019-04-03 22:04:45,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 121043: learning rate 0.0001
[2019-04-03 22:04:45,729] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 121182: loss 0.8287
[2019-04-03 22:04:45,731] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 121182: learning rate 0.0001
[2019-04-03 22:04:45,954] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 121246: loss 0.8204
[2019-04-03 22:04:45,954] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 121246: learning rate 0.0001
[2019-04-03 22:04:48,876] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 122153: loss 0.7976
[2019-04-03 22:04:48,912] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 122155: learning rate 0.0001
[2019-04-03 22:04:55,652] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.8002431e-10 2.9072718e-05 4.7799826e-09 9.6699636e-04 9.9533144e-04
 2.6706892e-10 9.9800867e-01], sum to 1.0000
[2019-04-03 22:04:55,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1422
[2019-04-03 22:04:55,699] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.49910751142904, 0.548817811653853, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4652400.0000, 
sim time next is 4653000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.44844059390521, 0.5692014818800463, 0.0, 1.0, 197390.8288262744], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6207033828254342, 0.6897338272933488, 0.0, 1.0, 0.9399563277441638], 
reward next is 0.0600, 
noisyNet noise sample is [array([0.86564523], dtype=float32), 1.3851882]. 
=============================================
[2019-04-03 22:04:55,724] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[80.21022 ]
 [79.52601 ]
 [79.85331 ]
 [79.97648 ]
 [80.228836]], R is [[80.27658844]
 [79.5394516 ]
 [79.62099457]
 [79.62262726]
 [79.82640076]].
[2019-04-03 22:04:58,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4485207e-10 6.8831821e-03 1.9123521e-08 7.0328201e-04 1.6943052e-04
 2.5301217e-09 9.9224418e-01], sum to 1.0000
[2019-04-03 22:04:58,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9050
[2019-04-03 22:04:58,547] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.58308593968467, 0.418833612933996, 0.0, 1.0, 23949.38070098848], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4683600.0000, 
sim time next is 4684200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.42786908590456, 0.4097087157165821, 0.0, 1.0, 108328.3381161755], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6189890904920468, 0.6365695719055274, 0.0, 1.0, 0.5158492291246453], 
reward next is 0.4842, 
noisyNet noise sample is [array([0.45363247], dtype=float32), 0.7493146]. 
=============================================
[2019-04-03 22:05:00,966] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8000, global step 126010: loss 0.0310
[2019-04-03 22:05:00,988] A3C_AGENT_WORKER-Thread-7 INFO:Local step 8000, global step 126018: loss 0.0362
[2019-04-03 22:05:00,989] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8000, global step 126010: learning rate 0.0001
[2019-04-03 22:05:00,991] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 8000, global step 126018: learning rate 0.0001
[2019-04-03 22:05:01,692] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 126181: loss 0.0356
[2019-04-03 22:05:01,693] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 126181: learning rate 0.0001
[2019-04-03 22:05:04,781] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 126703: loss 0.0220
[2019-04-03 22:05:04,785] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 126703: learning rate 0.0001
[2019-04-03 22:05:13,782] A3C_AGENT_WORKER-Thread-8 INFO:Local step 8000, global step 127838: loss 0.0016
[2019-04-03 22:05:13,783] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 8000, global step 127838: learning rate 0.0001
[2019-04-03 22:05:14,147] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127884: loss 0.0016
[2019-04-03 22:05:14,174] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127884: learning rate 0.0001
[2019-04-03 22:05:14,975] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128006: loss 0.0018
[2019-04-03 22:05:14,981] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128006: learning rate 0.0001
[2019-04-03 22:05:15,241] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 128048: loss 0.0015
[2019-04-03 22:05:15,242] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 128048: learning rate 0.0001
[2019-04-03 22:05:17,001] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 128366: loss 0.0017
[2019-04-03 22:05:17,004] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 128366: learning rate 0.0001
[2019-04-03 22:05:17,832] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 128557: loss 0.0014
[2019-04-03 22:05:17,832] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 128557: learning rate 0.0001
[2019-04-03 22:05:18,378] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 128668: loss 0.0019
[2019-04-03 22:05:18,381] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 128668: learning rate 0.0001
[2019-04-03 22:05:18,866] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128771: loss 0.0019
[2019-04-03 22:05:18,869] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128771: learning rate 0.0001
[2019-04-03 22:05:20,573] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 129106: loss 0.0022
[2019-04-03 22:05:20,573] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 129106: learning rate 0.0001
[2019-04-03 22:05:21,441] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 129265: loss 0.0007
[2019-04-03 22:05:21,456] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 129265: learning rate 0.0001
[2019-04-03 22:05:22,009] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 129369: loss 0.0019
[2019-04-03 22:05:22,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 129369: learning rate 0.0001
[2019-04-03 22:05:24,728] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 129913: loss 0.0048
[2019-04-03 22:05:24,732] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 129913: learning rate 0.0001
[2019-04-03 22:05:40,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.5514653e-09 1.2119223e-03 1.5983098e-08 2.0022348e-03 1.1396634e-02
 5.6792331e-09 9.8538929e-01], sum to 1.0000
[2019-04-03 22:05:40,101] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2503
[2019-04-03 22:05:40,116] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.02468451019073, 0.2279469764963744, 0.0, 1.0, 38655.10529833408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4948200.0000, 
sim time next is 4948800.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.03052050303135, 0.2197574854590133, 0.0, 1.0, 38671.57250173028], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5858767085859459, 0.5732524951530045, 0.0, 1.0, 0.18415034524633467], 
reward next is 0.8158, 
noisyNet noise sample is [array([-1.8411827], dtype=float32), 0.34612533]. 
=============================================
[2019-04-03 22:05:40,271] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0397172e-09 2.7967992e-05 2.3586066e-09 5.9789536e-04 2.3094367e-03
 1.0432288e-10 9.9706465e-01], sum to 1.0000
[2019-04-03 22:05:40,271] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6243
[2019-04-03 22:05:40,483] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.333333333333333, 24.66666666666667, 122.8333333333333, 861.6666666666667, 26.0, 27.12834273913565, 0.726567398272214, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4969200.0000, 
sim time next is 4969800.0000, 
raw observation next is [6.5, 24.5, 123.0, 865.0, 26.0, 27.08696216024669, 0.7268544882331619, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6426592797783934, 0.245, 0.41, 0.9558011049723757, 0.6666666666666666, 0.7572468466872241, 0.7422848294110539, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9113987], dtype=float32), 0.2679757]. 
=============================================
[2019-04-03 22:05:41,132] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6553606e-09 3.1270392e-04 2.1879469e-09 7.8092478e-02 1.1984573e-03
 6.0120531e-10 9.2039633e-01], sum to 1.0000
[2019-04-03 22:05:41,132] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1559
[2019-04-03 22:05:41,138] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 19.0, 86.0, 665.0, 26.0, 28.56673825462448, 1.135077813530631, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5068800.0000, 
sim time next is 5069400.0000, 
raw observation next is [12.0, 18.66666666666667, 82.66666666666666, 638.3333333333334, 26.0, 28.78468074891978, 1.165242524789347, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.1866666666666667, 0.2755555555555555, 0.705340699815838, 0.6666666666666666, 0.898723395743315, 0.8884141749297824, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.471105], dtype=float32), -0.1498639]. 
=============================================
[2019-04-03 22:05:41,307] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0859120e-10 7.4193895e-06 7.4614231e-10 4.2888485e-03 1.1677060e-03
 9.1089393e-11 9.9453604e-01], sum to 1.0000
[2019-04-03 22:05:41,308] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9308
[2019-04-03 22:05:41,320] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.666666666666667, 25.83333333333334, 46.66666666666666, 416.3333333333333, 26.0, 27.30312593321145, 0.8571267513156414, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4986600.0000, 
sim time next is 4987200.0000, 
raw observation next is [7.333333333333334, 25.66666666666667, 40.33333333333333, 360.1666666666666, 26.0, 27.5515456986638, 0.8367428712373237, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6657433056325024, 0.2566666666666667, 0.13444444444444442, 0.3979742173112338, 0.6666666666666666, 0.7959621415553167, 0.7789142904124412, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2085891], dtype=float32), 1.0592998]. 
=============================================
[2019-04-03 22:05:42,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:42,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:42,643] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run2
[2019-04-03 22:05:46,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:46,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:46,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run2
[2019-04-03 22:05:46,126] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:46,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:46,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run2
[2019-04-03 22:05:47,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:47,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:47,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run2
[2019-04-03 22:05:53,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:53,850] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:53,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run2
[2019-04-03 22:05:54,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:54,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:54,731] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run2
[2019-04-03 22:05:55,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:55,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:55,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run2
[2019-04-03 22:05:56,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:56,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:56,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run2
[2019-04-03 22:05:58,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:58,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:58,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run2
[2019-04-03 22:05:59,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:59,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:59,362] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run2
[2019-04-03 22:05:59,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:59,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:59,487] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run2
[2019-04-03 22:06:00,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:00,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:00,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run2
[2019-04-03 22:06:00,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:00,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:00,435] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run2
[2019-04-03 22:06:00,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:00,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:00,971] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run2
[2019-04-03 22:06:01,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:01,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:01,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run2
[2019-04-03 22:06:02,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:02,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:02,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run2
[2019-04-03 22:06:05,463] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5330771e-10 1.5012801e-04 1.7585376e-09 2.6068179e-04 5.6558521e-04
 2.1189894e-09 9.9902356e-01], sum to 1.0000
[2019-04-03 22:06:05,463] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0986
[2019-04-03 22:06:05,508] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.19215370389179, -0.5799949085836608, 0.0, 1.0, 40400.39719985658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 23400.0000, 
sim time next is 24000.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 26.0, 21.21316093290682, -0.574606449456398, 0.0, 1.0, 40382.21349976964], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.2677634110755684, 0.3084645168478673, 0.0, 1.0, 0.1922962547608078], 
reward next is 0.8077, 
noisyNet noise sample is [array([-1.5213186], dtype=float32), -1.113887]. 
=============================================
[2019-04-03 22:06:05,512] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[90.36793 ]
 [90.31767 ]
 [90.246666]
 [90.17342 ]
 [90.089   ]], R is [[90.33408356]
 [90.23835754]
 [90.14347839]
 [90.04940796]
 [89.95612335]].
[2019-04-03 22:06:18,743] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.2451041e-10 1.2153180e-05 1.0944258e-08 5.1477872e-04 1.1985715e-03
 9.2019847e-10 9.9827445e-01], sum to 1.0000
[2019-04-03 22:06:18,744] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3839
[2019-04-03 22:06:18,766] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.8743729121492, 0.04050269585148047, 0.0, 1.0, 44645.25830883632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 172200.0000, 
sim time next is 172800.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.8308937355709, 0.03032683161558022, 0.0, 1.0, 44590.278034094], 
processed observation next is [1.0, 0.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.485907811297575, 0.5101089438718601, 0.0, 1.0, 0.21233465730520953], 
reward next is 0.7877, 
noisyNet noise sample is [array([-0.80784154], dtype=float32), 0.5587367]. 
=============================================
[2019-04-03 22:06:29,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5361504e-10 2.7127002e-04 1.9783608e-08 1.7695590e-03 1.1195957e-03
 1.7674067e-09 9.9683952e-01], sum to 1.0000
[2019-04-03 22:06:29,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6840
[2019-04-03 22:06:29,833] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.733333333333333, 74.83333333333334, 0.0, 0.0, 26.0, 23.78729652788273, 0.01255251800237771, 0.0, 1.0, 43983.43983548123], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 103800.0000, 
sim time next is 104400.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 23.72958116474457, -0.001227074442508682, 0.0, 1.0, 44091.23331460002], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.4774650970620475, 0.49959097518583045, 0.0, 1.0, 0.2099582538790477], 
reward next is 0.7900, 
noisyNet noise sample is [array([-1.2254412], dtype=float32), -2.0170748]. 
=============================================
[2019-04-03 22:06:43,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2071216e-10 3.6748081e-06 1.1772985e-09 6.0590351e-04 9.9813893e-05
 8.3840358e-11 9.9929059e-01], sum to 1.0000
[2019-04-03 22:06:43,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4856
[2019-04-03 22:06:43,659] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 42.0, 74.0, 525.6666666666667, 26.0, 25.72602777588968, 0.4947124064919063, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 313800.0000, 
sim time next is 314400.0000, 
raw observation next is [-9.5, 42.0, 72.0, 501.3333333333333, 26.0, 26.07768774578741, 0.5181739161530378, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.24, 0.5539594843462247, 0.6666666666666666, 0.6731406454822843, 0.6727246387176793, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4906169], dtype=float32), 1.9070853]. 
=============================================
[2019-04-03 22:06:43,832] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7750471e-09 1.9052160e-05 4.5355524e-09 1.1055197e-03 5.1053305e-04
 3.2395450e-10 9.9836487e-01], sum to 1.0000
[2019-04-03 22:06:43,832] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7656
[2019-04-03 22:06:43,921] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 65.0, 132.3333333333333, 0.0, 26.0, 25.12181138877006, 0.2229636088120984, 1.0, 1.0, 70751.3904542931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 218400.0000, 
sim time next is 219000.0000, 
raw observation next is [-4.583333333333333, 65.0, 135.6666666666667, 0.0, 26.0, 25.15001017059532, 0.2290969070260579, 1.0, 1.0, 37347.39367718322], 
processed observation next is [1.0, 0.5217391304347826, 0.3356417359187443, 0.65, 0.45222222222222236, 0.0, 0.6666666666666666, 0.5958341808829433, 0.5763656356753527, 1.0, 1.0, 0.1778447317961106], 
reward next is 0.8222, 
noisyNet noise sample is [array([-0.9226406], dtype=float32), -0.22709227]. 
=============================================
[2019-04-03 22:06:43,927] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.61644]
 [76.57017]
 [76.73722]
 [76.92519]
 [76.95478]], R is [[76.54111481]
 [76.43878937]
 [76.46391296]
 [76.57187653]
 [76.55177307]].
[2019-04-03 22:06:53,033] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4598413e-10 3.8969167e-05 2.6722415e-09 7.3896757e-05 1.0578865e-04
 6.2174477e-10 9.9978131e-01], sum to 1.0000
[2019-04-03 22:06:53,033] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3094
[2019-04-03 22:06:53,253] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.8, 70.0, 15.0, 205.5, 26.0, 24.41514897886819, 0.1185951097904449, 1.0, 1.0, 93587.46375274497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 288000.0000, 
sim time next is 288600.0000, 
raw observation next is [-12.71666666666667, 69.5, 20.0, 265.6666666666667, 26.0, 24.66466479957528, 0.1783145451922401, 1.0, 1.0, 86633.082336185], 
processed observation next is [1.0, 0.34782608695652173, 0.1103416435826407, 0.695, 0.06666666666666667, 0.29355432780847146, 0.6666666666666666, 0.55538873329794, 0.5594381817307467, 1.0, 1.0, 0.4125384873151667], 
reward next is 0.5875, 
noisyNet noise sample is [array([0.9848882], dtype=float32), -0.09135237]. 
=============================================
[2019-04-03 22:06:57,380] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.8768005e-09 1.8826621e-03 2.8250420e-07 3.8080614e-03 2.4974982e-03
 1.7452905e-08 9.9181139e-01], sum to 1.0000
[2019-04-03 22:06:57,380] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9191
[2019-04-03 22:06:57,397] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 51.0, 0.0, 0.0, 26.0, 23.02708203501658, -0.210894639354243, 0.0, 1.0, 46193.30568350979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 445200.0000, 
sim time next is 445800.0000, 
raw observation next is [-11.1, 51.5, 0.0, 0.0, 26.0, 22.97454222382104, -0.224726834044513, 0.0, 1.0, 46294.59259444171], 
processed observation next is [1.0, 0.13043478260869565, 0.1551246537396122, 0.515, 0.0, 0.0, 0.6666666666666666, 0.41454518531841994, 0.42509105531849567, 0.0, 1.0, 0.2204504409259129], 
reward next is 0.7795, 
noisyNet noise sample is [array([-0.38780233], dtype=float32), 0.94630355]. 
=============================================
[2019-04-03 22:07:03,838] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5506220e-11 8.7364951e-06 2.5989627e-10 7.3818475e-05 1.9193118e-05
 4.6236327e-11 9.9989820e-01], sum to 1.0000
[2019-04-03 22:07:03,840] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9375
[2019-04-03 22:07:03,857] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.1, 79.5, 0.0, 0.0, 26.0, 24.43409989114882, 0.1530192062437922, 0.0, 1.0, 47550.24547310801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 336600.0000, 
sim time next is 337200.0000, 
raw observation next is [-13.2, 80.33333333333333, 0.0, 0.0, 26.0, 24.34969245520572, 0.1392303310622896, 0.0, 1.0, 47560.43384245523], 
processed observation next is [1.0, 0.9130434782608695, 0.09695290858725764, 0.8033333333333332, 0.0, 0.0, 0.6666666666666666, 0.52914103793381, 0.5464101103540965, 0.0, 1.0, 0.22647825639264396], 
reward next is 0.7735, 
noisyNet noise sample is [array([-0.46240026], dtype=float32), -0.94263816]. 
=============================================
[2019-04-03 22:07:04,108] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0728021e-10 4.4452272e-06 1.5715631e-09 3.0435031e-05 1.3158753e-04
 1.4343095e-10 9.9983346e-01], sum to 1.0000
[2019-04-03 22:07:04,108] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8320
[2019-04-03 22:07:04,218] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.9, 59.00000000000001, 0.0, 0.0, 26.0, 25.70411482377166, 0.3761615058152095, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 325200.0000, 
sim time next is 325800.0000, 
raw observation next is [-12.0, 60.0, 0.0, 0.0, 26.0, 25.54318090564127, 0.3521450703563758, 1.0, 1.0, 104861.268217479], 
processed observation next is [1.0, 0.782608695652174, 0.13019390581717452, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6285984088034393, 0.617381690118792, 1.0, 1.0, 0.4993393724641857], 
reward next is 0.5007, 
noisyNet noise sample is [array([-0.56465966], dtype=float32), 1.003047]. 
=============================================
[2019-04-03 22:07:05,755] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2215101e-10 1.4209466e-04 3.9327066e-09 1.7488179e-04 3.7966695e-04
 4.1330586e-10 9.9930334e-01], sum to 1.0000
[2019-04-03 22:07:05,755] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8153
[2019-04-03 22:07:06,036] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.9, 68.0, 0.0, 0.0, 26.0, 23.72635098491163, -0.009128637589630348, 0.0, 1.0, 47184.94092371457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 343800.0000, 
sim time next is 344400.0000, 
raw observation next is [-13.9, 67.33333333333334, 0.0, 0.0, 26.0, 23.65919300768117, -0.01447291577866559, 0.0, 1.0, 47224.31904413381], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.4715994173067643, 0.4951756947404448, 0.0, 1.0, 0.2248777097339705], 
reward next is 0.7751, 
noisyNet noise sample is [array([-0.69134027], dtype=float32), -0.7748571]. 
=============================================
[2019-04-03 22:07:12,772] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3210317e-09 7.6671444e-02 3.1323790e-08 1.8446088e-03 1.3728170e-02
 9.6346104e-09 9.0775573e-01], sum to 1.0000
[2019-04-03 22:07:12,772] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8816
[2019-04-03 22:07:12,894] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 87.66666666666666, 121.6666666666667, 115.6666666666667, 26.0, 24.83131676880189, 0.2522061832559656, 0.0, 1.0, 63679.6801270498], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 553800.0000, 
sim time next is 554400.0000, 
raw observation next is [-0.6, 87.0, 110.5, 122.0, 26.0, 24.80459707971662, 0.2552938985192064, 0.0, 1.0, 70276.23744532927], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.87, 0.36833333333333335, 0.13480662983425415, 0.6666666666666666, 0.5670497566430516, 0.5850979661730688, 0.0, 1.0, 0.3346487497396632], 
reward next is 0.6654, 
noisyNet noise sample is [array([0.8770781], dtype=float32), -1.2753023]. 
=============================================
[2019-04-03 22:07:15,421] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0048126e-09 2.9971442e-04 3.6816910e-08 1.8336367e-03 1.5894551e-03
 3.6311765e-09 9.9627715e-01], sum to 1.0000
[2019-04-03 22:07:15,421] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6736
[2019-04-03 22:07:15,477] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.266666666666667, 37.66666666666667, 19.16666666666667, 0.0, 26.0, 24.83988940285458, 0.1410473673525513, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 462000.0000, 
sim time next is 462600.0000, 
raw observation next is [-7.0, 36.5, 23.0, 0.0, 26.0, 25.15169825162273, 0.1718135281529992, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2686980609418283, 0.365, 0.07666666666666666, 0.0, 0.6666666666666666, 0.5959748543018941, 0.5572711760509997, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02923788], dtype=float32), 0.7328665]. 
=============================================
[2019-04-03 22:07:24,181] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7170657e-12 5.3039275e-05 3.4456499e-10 1.0921927e-05 1.7837925e-04
 8.4206028e-12 9.9975759e-01], sum to 1.0000
[2019-04-03 22:07:24,181] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1936
[2019-04-03 22:07:24,241] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.55, 96.5, 0.0, 0.0, 26.0, 24.83403332451908, 0.2344556379286678, 0.0, 1.0, 40012.19174868377], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 516600.0000, 
sim time next is 517200.0000, 
raw observation next is [3.633333333333333, 96.66666666666666, 0.0, 0.0, 26.0, 24.84276599367649, 0.2405629281880052, 0.0, 1.0, 39923.88670582083], 
processed observation next is [1.0, 1.0, 0.5632502308402586, 0.9666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5702304994730408, 0.5801876427293351, 0.0, 1.0, 0.1901137462181944], 
reward next is 0.8099, 
noisyNet noise sample is [array([-1.6833549], dtype=float32), 0.7412499]. 
=============================================
[2019-04-03 22:07:31,214] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8139580e-10 3.6143744e-04 7.7194706e-10 2.9115263e-05 5.0529261e-04
 3.7325931e-10 9.9910408e-01], sum to 1.0000
[2019-04-03 22:07:31,234] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8068
[2019-04-03 22:07:31,290] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 26.0, 24.96144397079855, 0.3156118443923779, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 562800.0000, 
sim time next is 563400.0000, 
raw observation next is [-1.0, 80.5, 130.0, 396.0, 26.0, 24.95658355604128, 0.3144380456576992, 0.0, 1.0, 18731.33440420001], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.805, 0.43333333333333335, 0.4375690607734807, 0.6666666666666666, 0.5797152963367734, 0.6048126818858998, 0.0, 1.0, 0.08919683049619052], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.0448757], dtype=float32), 0.25044677]. 
=============================================
[2019-04-03 22:07:48,958] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6260524e-09 1.3792336e-02 2.1688930e-08 1.7857691e-03 1.3383118e-02
 8.0606402e-09 9.7103882e-01], sum to 1.0000
[2019-04-03 22:07:48,961] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6085
[2019-04-03 22:07:48,974] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3, 72.0, 0.0, 0.0, 26.0, 24.66851019148494, 0.172662614480049, 0.0, 1.0, 38923.86877160888], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 883800.0000, 
sim time next is 884400.0000, 
raw observation next is [-0.2, 72.0, 0.0, 0.0, 26.0, 24.6131477106589, 0.1619039558291192, 0.0, 1.0, 38908.32262860522], 
processed observation next is [1.0, 0.21739130434782608, 0.4570637119113574, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5510956425549084, 0.553967985276373, 0.0, 1.0, 0.18527772680288201], 
reward next is 0.8147, 
noisyNet noise sample is [array([-0.28650647], dtype=float32), -0.12798937]. 
=============================================
[2019-04-03 22:07:51,263] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2569001e-10 2.2838417e-04 3.4314336e-09 4.8827223e-04 5.7832412e-03
 6.1294164e-10 9.9350005e-01], sum to 1.0000
[2019-04-03 22:07:51,263] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6651
[2019-04-03 22:07:51,311] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.04999999999999999, 53.5, 131.0, 449.0, 26.0, 25.74934088126603, 0.3610844601765453, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 736200.0000, 
sim time next is 736800.0000, 
raw observation next is [0.1333333333333333, 52.33333333333333, 124.0, 503.0, 26.0, 25.6592661048353, 0.3576568126056918, 1.0, 1.0, 35331.13486205231], 
processed observation next is [1.0, 0.5217391304347826, 0.46629732225300097, 0.5233333333333333, 0.41333333333333333, 0.5558011049723757, 0.6666666666666666, 0.6382721754029417, 0.6192189375352306, 1.0, 1.0, 0.16824349934310626], 
reward next is 0.8318, 
noisyNet noise sample is [array([-0.01450555], dtype=float32), -1.4044968]. 
=============================================
[2019-04-03 22:07:53,045] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6463684e-11 1.5703797e-04 3.1154657e-10 2.6572042e-04 1.1717338e-03
 1.6021596e-11 9.9840552e-01], sum to 1.0000
[2019-04-03 22:07:53,045] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0151
[2019-04-03 22:07:53,162] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.916666666666667, 74.33333333333333, 78.33333333333334, 0.0, 26.0, 25.70373395535353, 0.2910413259584538, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 814200.0000, 
sim time next is 814800.0000, 
raw observation next is [-5.633333333333334, 73.66666666666667, 82.66666666666666, 0.0, 26.0, 25.65312982208356, 0.2894040307400862, 1.0, 1.0, 45500.16678389372], 
processed observation next is [1.0, 0.43478260869565216, 0.30655586334256696, 0.7366666666666667, 0.2755555555555555, 0.0, 0.6666666666666666, 0.6377608185069633, 0.5964680102466954, 1.0, 1.0, 0.2166674608756844], 
reward next is 0.7833, 
noisyNet noise sample is [array([0.3523495], dtype=float32), -0.39171782]. 
=============================================
[2019-04-03 22:07:57,254] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2694794e-10 7.2037987e-04 3.9673163e-09 5.0585024e-04 1.8735114e-03
 5.6175242e-09 9.9690032e-01], sum to 1.0000
[2019-04-03 22:07:57,255] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7092
