Using TensorFlow backend.
[2019-04-03 21:51:53,410] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v1', activation='linear', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-v1', eval_act_func='part4_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=100000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.0001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=5000000, metric_func='part4_v1', model_dir='None', model_param=[12, 1], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_v1', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=10, test_env=['Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'], test_mode='Multiple', train_act_func='part4_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=10.0, weight_initer='glorot_uniform', window_len=20)
[2019-04-03 21:51:53,410] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-03 21:51:53.442038: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-03 21:52:09,657] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-03 21:52:09,658] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-v1', 'Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'] ...
[2019-04-03 21:52:09,681] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation worker starts!
[2019-04-03 21:52:09,705] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation worker starts!
[2019-04-03 21:52:09,729] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation worker starts!
[2019-04-03 21:52:09,729] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:09,730] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-03 21:52:09,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:09,794] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run1
[2019-04-03 21:52:10,731] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:10,732] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-03 21:52:10,807] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:10,808] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run1
[2019-04-03 21:52:11,733] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:11,734] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-03 21:52:11,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:11,854] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run1
[2019-04-03 21:52:12,735] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:12,735] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-03 21:52:12,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:12,846] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run1
[2019-04-03 21:52:13,736] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:13,737] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-03 21:52:13,950] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:13,952] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run1
[2019-04-03 21:52:14,738] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:14,739] A3C_AGENT_WORKER-Thread-7 INFO:Local worker starts!
[2019-04-03 21:52:14,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:14,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run1
[2019-04-03 21:52:15,740] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:15,741] A3C_AGENT_WORKER-Thread-8 INFO:Local worker starts!
[2019-04-03 21:52:15,883] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:15,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run1
[2019-04-03 21:52:16,742] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:16,743] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-04-03 21:52:16,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:16,951] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run1
[2019-04-03 21:52:17,743] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:17,744] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-03 21:52:18,235] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-03 21:52:18,235] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 21:52:18,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:18,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run1
[2019-04-03 21:52:18,329] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 21:52:18,329] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:18,335] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run1
[2019-04-03 21:52:18,449] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 21:52:18,450] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:18,451] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run1
[2019-04-03 21:52:18,527] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:18,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run1
[2019-04-03 21:52:18,749] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:18,750] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-03 21:52:19,206] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:19,208] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run1
[2019-04-03 21:52:19,761] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:19,762] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-03 21:52:20,024] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:20,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run1
[2019-04-03 21:52:20,768] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:20,769] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-03 21:52:21,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:21,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run1
[2019-04-03 21:52:21,769] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:21,770] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-03 21:52:22,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:22,139] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run1
[2019-04-03 21:52:22,771] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:22,771] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-03 21:52:23,016] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:23,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run1
[2019-04-03 21:52:23,772] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:23,773] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-03 21:52:24,428] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:24,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run1
[2019-04-03 21:52:24,774] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-03 21:52:24,775] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-03 21:52:25,075] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 21:52:25,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run1
[2019-04-03 21:53:00,376] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:53:00,376] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.9, 82.66666666666667, 35.66666666666666, 0.0, 26.0, 25.52567725182879, 0.283370581003885, 1.0, 1.0, 0.0]
[2019-04-03 21:53:00,377] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 21:53:00,378] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0830365  0.29387978 0.01537636 0.09628686 0.16762201 0.24234644
 0.10145207], sampled 0.5689962540963055
[2019-04-03 21:53:10,482] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:53:10,482] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.08333333333333337, 66.0, 173.6666666666667, 244.6666666666667, 26.0, 25.05780457283657, 0.2248514800447684, 0.0, 1.0, 0.0]
[2019-04-03 21:53:10,483] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 21:53:10,483] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.06155102 0.27239046 0.02112751 0.089841   0.1746202  0.3249916
 0.05547817], sampled 0.9725627836976074
[2019-04-03 21:53:44,038] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:53:44,039] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-15.66666666666667, 83.0, 0.0, 0.0, 19.0, 20.89400155200677, -0.6694059872386536, 0.0, 1.0, 0.0]
[2019-04-03 21:53:44,039] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 21:53:44,039] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.04877106 0.31218323 0.03308284 0.0753342  0.16585654 0.29510838
 0.06966372], sampled 0.6346868945095491
[2019-04-03 21:54:03,411] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:54:03,411] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.270575772333334, 79.09768339666667, 0.0, 0.0, 23.5, 23.84781712121328, 0.0539626798029356, 0.0, 1.0, 0.0]
[2019-04-03 21:54:03,411] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 21:54:03,412] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.05300341 0.21001269 0.01992442 0.10791225 0.26954463 0.2783858
 0.06121683], sampled 0.7452829594091173
[2019-04-03 21:54:16,487] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7458.2465 217898598.6321 1188.0247
[2019-04-03 21:54:17,527] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:54:17,527] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.666666666666666, 49.33333333333333, 0.0, 0.0, 25.0, 23.8647869193974, 0.01859521617423642, 0.0, 1.0, 41114.60008810717]
[2019-04-03 21:54:17,527] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 21:54:17,527] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.06700297 0.24568288 0.02577572 0.07254801 0.26302525 0.26554278
 0.06042238], sampled 0.8849880706558982
[2019-04-03 21:54:39,504] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-03 21:54:39,504] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [12.0, 19.0, 89.33333333333333, 691.6666666666667, 26.0, 27.54196952147445, 1.008006539523706, 1.0, 1.0, 0.0]
[2019-04-03 21:54:39,504] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 21:54:39,505] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.11034062 0.20574337 0.01241562 0.08540776 0.29847065 0.15598153
 0.1316404 ], sampled 0.8990834532068653
[2019-04-03 21:54:39,937] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7310.9398 246771801.5642 981.7301
[2019-04-03 21:54:44,651] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7251.2884 254092743.4161 692.7758
[2019-04-03 21:54:45,688] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 7310.939790448689, 246771801.56421003, 981.7301101639936, 7458.246469990233, 217898598.63206992, 1188.0246807496706, 7251.288425681861, 254092743.41612852, 692.775760885787]
[2019-04-03 21:55:19,145] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00625428 0.76935977 0.0086679  0.05095941 0.06223894 0.0409744
 0.06154535], sum to 1.0000
[2019-04-03 21:55:19,145] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9272
[2019-04-03 21:55:19,452] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 64.0, 15.0, 0.0, 20.0, 20.69099181407145, -0.801427886498418, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 232800.0000, 
sim time next is 233400.0000, 
raw observation next is [-3.4, 64.5, 12.0, 0.0, 21.0, 20.92331086826726, -0.7925392908515988, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.645, 0.04, 0.0, 0.25, 0.2436092390222718, 0.2358202363828004, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7393152], dtype=float32), 0.76753074]. 
=============================================
[2019-04-03 21:55:23,852] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00100787 0.86115426 0.00636307 0.02206041 0.05286574 0.03124317
 0.02530549], sum to 1.0000
[2019-04-03 21:55:23,854] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9517
[2019-04-03 21:55:24,145] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-10.41666666666667, 67.5, 0.0, 0.0, 19.0, 18.40900456528238, -1.240994577072108, 0.0, 1.0, 99825.71037043059], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 276600.0000, 
sim time next is 277200.0000, 
raw observation next is [-10.6, 67.0, 0.0, 0.0, 20.0, 18.42360824089398, -1.229682422680743, 0.0, 1.0, 89514.52217197987], 
processed observation next is [1.0, 0.21739130434782608, 0.1689750692520776, 0.67, 0.0, 0.0, 0.16666666666666666, 0.03530068674116501, 0.090105859106419, 0.0, 1.0, 0.42625962939038037], 
reward next is 0.5737, 
noisyNet noise sample is [array([0.15462697], dtype=float32), -1.0105674]. 
=============================================
[2019-04-03 21:55:26,351] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7525: loss -0.0486
[2019-04-03 21:55:26,430] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7525: learning rate 0.0001
[2019-04-03 21:55:26,543] A3C_AGENT_WORKER-Thread-7 INFO:Local step 500, global step 7595: loss -0.0761
[2019-04-03 21:55:26,545] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 500, global step 7597: learning rate 0.0001
[2019-04-03 21:55:26,604] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 7614: loss -0.0225
[2019-04-03 21:55:26,605] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 7614: learning rate 0.0001
[2019-04-03 21:55:26,695] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7639: loss 0.0248
[2019-04-03 21:55:26,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7639: learning rate 0.0001
[2019-04-03 21:55:26,925] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7691: loss -0.0590
[2019-04-03 21:55:26,954] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7703: learning rate 0.0001
[2019-04-03 21:55:27,335] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 7794: loss 1.4576
[2019-04-03 21:55:27,338] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 7794: learning rate 0.0001
[2019-04-03 21:55:28,361] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 8014: loss -0.1754
[2019-04-03 21:55:28,372] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 8017: learning rate 0.0001
[2019-04-03 21:55:28,442] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 8034: loss -1.3104
[2019-04-03 21:55:28,448] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 8034: learning rate 0.0001
[2019-04-03 21:55:28,532] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8052: loss -1.6118
[2019-04-03 21:55:28,532] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8052: learning rate 0.0001
[2019-04-03 21:55:29,048] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8161: loss -0.1961
[2019-04-03 21:55:29,048] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8161: learning rate 0.0001
[2019-04-03 21:55:29,453] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8246: loss -0.2478
[2019-04-03 21:55:29,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8246: learning rate 0.0001
[2019-04-03 21:55:29,578] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8272: loss -0.5155
[2019-04-03 21:55:29,578] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8272: learning rate 0.0001
[2019-04-03 21:55:29,900] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8322: loss -0.8753
[2019-04-03 21:55:29,922] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8322: learning rate 0.0001
[2019-04-03 21:55:29,961] A3C_AGENT_WORKER-Thread-8 INFO:Local step 500, global step 8327: loss -0.8345
[2019-04-03 21:55:29,999] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 500, global step 8327: learning rate 0.0001
[2019-04-03 21:55:30,140] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8369: loss -0.3077
[2019-04-03 21:55:30,141] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8369: learning rate 0.0001
[2019-04-03 21:55:30,526] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8443: loss -0.7027
[2019-04-03 21:55:30,549] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8443: learning rate 0.0001
[2019-04-03 21:55:40,201] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00429267 0.41753274 0.00836241 0.07413334 0.25690442 0.02071238
 0.21806195], sum to 1.0000
[2019-04-03 21:55:40,201] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8486
[2019-04-03 21:55:40,513] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.25, 51.0, 58.0, 905.0, 19.0, 19.86002820188061, -1.032467737311114, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 390600.0000, 
sim time next is 391200.0000, 
raw observation next is [-12.06666666666667, 51.0, 57.5, 902.0, 21.0, 19.5615510965748, -1.014966291278485, 1.0, 1.0, 196266.4560289789], 
processed observation next is [1.0, 0.5217391304347826, 0.1283471837488457, 0.51, 0.19166666666666668, 0.9966850828729282, 0.25, 0.13012925804789996, 0.16167790290717168, 1.0, 1.0, 0.9346021715665661], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.9630872], dtype=float32), 0.6736285]. 
=============================================
[2019-04-03 21:56:00,549] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0820458e-05 8.1733674e-01 6.7538564e-04 7.1360110e-03 1.6530950e-01
 2.9803985e-03 6.5211761e-03], sum to 1.0000
[2019-04-03 21:56:00,553] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6769
[2019-04-03 21:56:00,741] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 18.91243351784637, -1.159328563371327, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 546000.0000, 
sim time next is 546600.0000, 
raw observation next is [0.5, 92.0, 12.0, 37.99999999999999, 19.0, 18.82024210118262, -1.16214591365019, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.04, 0.04198895027624309, 0.08333333333333333, 0.06835350843188515, 0.11261802878327003, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14412707], dtype=float32), -0.09109919]. 
=============================================
[2019-04-03 21:56:02,546] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4306358e-05 6.6884786e-01 2.6319822e-04 7.5253816e-03 3.1115088e-01
 3.0285148e-03 9.1499854e-03], sum to 1.0000
[2019-04-03 21:56:02,546] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5185
[2019-04-03 21:56:02,748] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1000, global step 15275: loss 3.4864
[2019-04-03 21:56:02,751] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1000, global step 15275: learning rate 0.0001
[2019-04-03 21:56:02,767] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6666666666666667, 82.33333333333334, 87.0, 136.0, 19.0, 18.81769035200371, -1.120249461971347, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 559200.0000, 
sim time next is 559800.0000, 
raw observation next is [-0.7, 82.0, 89.0, 135.0, 19.0, 18.81414046089411, -1.127379448795909, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.443213296398892, 0.82, 0.2966666666666667, 0.14917127071823205, 0.08333333333333333, 0.0678450384078424, 0.12420685040136366, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22962001], dtype=float32), -0.78329915]. 
=============================================
[2019-04-03 21:56:02,964] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15333: loss 3.0871
[2019-04-03 21:56:02,964] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15333: learning rate 0.0001
[2019-04-03 21:56:02,998] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15343: loss 5.8124
[2019-04-03 21:56:03,012] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15343: learning rate 0.0001
[2019-04-03 21:56:03,137] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 15390: loss 4.2925
[2019-04-03 21:56:03,139] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 15390: learning rate 0.0001
[2019-04-03 21:56:03,253] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 15425: loss 11.4023
[2019-04-03 21:56:03,258] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 15428: learning rate 0.0001
[2019-04-03 21:56:03,934] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15692: loss 2.9100
[2019-04-03 21:56:03,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15694: learning rate 0.0001
[2019-04-03 21:56:05,109] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16087: loss 4.4878
[2019-04-03 21:56:05,110] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16087: learning rate 0.0001
[2019-04-03 21:56:05,183] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.5340611e-06 8.8122320e-01 1.3833499e-04 4.2248075e-03 1.1155231e-01
 1.2184288e-03 1.6333155e-03], sum to 1.0000
[2019-04-03 21:56:05,185] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9791
[2019-04-03 21:56:05,294] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 75.0, 0.0, 0.0, 19.0, 18.72710477176958, -1.215629520374553, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 619200.0000, 
sim time next is 619800.0000, 
raw observation next is [-4.5, 73.83333333333333, 0.0, 0.0, 19.5, 18.63573941359064, -1.216592548510036, 0.0, 1.0, 199012.8802064955], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.7383333333333333, 0.0, 0.0, 0.125, 0.05297828446588652, 0.09446915049665468, 0.0, 1.0, 0.9476803819356928], 
reward next is 0.0523, 
noisyNet noise sample is [array([0.68455493], dtype=float32), 0.9882759]. 
=============================================
[2019-04-03 21:56:05,514] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 16217: loss 8.3516
[2019-04-03 21:56:05,515] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 16217: learning rate 0.0001
[2019-04-03 21:56:05,657] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 16264: loss 4.4430
[2019-04-03 21:56:05,659] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 16265: learning rate 0.0001
[2019-04-03 21:56:05,719] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16287: loss 2.5268
[2019-04-03 21:56:05,724] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16288: learning rate 0.0001
[2019-04-03 21:56:05,885] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 16357: loss 2.4789
[2019-04-03 21:56:05,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 16357: learning rate 0.0001
[2019-04-03 21:56:06,020] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16409: loss 2.3747
[2019-04-03 21:56:06,021] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16409: learning rate 0.0001
[2019-04-03 21:56:06,281] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16497: loss 1.9751
[2019-04-03 21:56:06,283] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16498: learning rate 0.0001
[2019-04-03 21:56:06,312] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1000, global step 16504: loss 20.9357
[2019-04-03 21:56:06,313] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1000, global step 16504: learning rate 0.0001
[2019-04-03 21:56:06,319] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9767971e-06 9.5299727e-01 3.8242539e-05 3.9324337e-03 4.2188514e-02
 2.7359542e-04 5.6683854e-04], sum to 1.0000
[2019-04-03 21:56:06,319] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16507: loss 4.7874
[2019-04-03 21:56:06,320] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0282
[2019-04-03 21:56:06,322] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16508: learning rate 0.0001
[2019-04-03 21:56:06,340] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.1, 75.0, 0.0, 0.0, 19.0, 18.68811901718497, -1.196601688398188, 0.0, 1.0, 18722.84417175294], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 616800.0000, 
sim time next is 617400.0000, 
raw observation next is [-4.2, 75.0, 0.0, 0.0, 19.0, 18.69081574205084, -1.206519878990285, 0.0, 1.0, 18720.55949081133], 
processed observation next is [0.0, 0.13043478260869565, 0.34626038781163443, 0.75, 0.0, 0.0, 0.08333333333333333, 0.05756797850423675, 0.09782670700323837, 0.0, 1.0, 0.08914552138481587], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.0907762], dtype=float32), 0.79831874]. 
=============================================
[2019-04-03 21:56:06,589] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16622: loss 4.3416
[2019-04-03 21:56:06,591] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16622: learning rate 0.0001
[2019-04-03 21:56:14,638] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1522765e-08 9.5091838e-01 5.3268805e-06 7.6433754e-04 4.8002619e-02
 6.4842294e-05 2.4450466e-04], sum to 1.0000
[2019-04-03 21:56:14,639] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7785
[2019-04-03 21:56:14,768] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 18.70065276683887, -1.273322321179217, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 715200.0000, 
sim time next is 715800.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 18.65473482112485, -1.283901917883737, 0.0, 1.0, 100209.0069285641], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.054561235093737594, 0.07203269403875434, 0.0, 1.0, 0.47718574727887664], 
reward next is 0.5228, 
noisyNet noise sample is [array([0.79952174], dtype=float32), -1.9420561]. 
=============================================
[2019-04-03 21:56:19,656] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1162074e-06 9.4929010e-01 4.0318129e-05 1.7123655e-03 4.3056749e-02
 3.0471722e-04 5.5946293e-03], sum to 1.0000
[2019-04-03 21:56:19,659] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8783
[2019-04-03 21:56:19,724] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.7, 75.0, 16.0, 0.0, 19.0, 19.50006860745345, -1.043770135449354, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 806400.0000, 
sim time next is 807000.0000, 
raw observation next is [-6.616666666666667, 75.0, 21.33333333333334, 0.0, 19.0, 19.87791619073923, -1.016566558352588, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2793167128347184, 0.75, 0.07111111111111112, 0.0, 0.08333333333333333, 0.15649301589493594, 0.16114448054913733, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2212375], dtype=float32), 0.8643654]. 
=============================================
[2019-04-03 21:56:19,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[42.11505 ]
 [42.494232]
 [42.47441 ]
 [42.42054 ]
 [43.118805]], R is [[41.08109665]
 [40.67028427]
 [40.26358032]
 [39.86094666]
 [39.46233749]].
[2019-04-03 21:56:21,400] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9044859e-05 7.2750324e-01 1.1740578e-03 2.0742562e-02 5.5911928e-02
 6.9116947e-04 1.9393796e-01], sum to 1.0000
[2019-04-03 21:56:21,400] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5180
[2019-04-03 21:56:21,438] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 73.66666666666667, 100.8333333333333, 0.0, 19.0, 19.54599849712263, -1.086847982716428, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 822000.0000, 
sim time next is 822600.0000, 
raw observation next is [-4.5, 75.0, 99.0, 0.0, 19.0, 19.61392985411389, -1.083682800447298, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.75, 0.33, 0.0, 0.08333333333333333, 0.13449415450949095, 0.13877239985090065, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.8877136], dtype=float32), -0.45074636]. 
=============================================
[2019-04-03 21:56:23,522] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1500, global step 22881: loss -1.9505
[2019-04-03 21:56:23,540] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1500, global step 22884: learning rate 0.0001
[2019-04-03 21:56:24,577] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23129: loss -1.3323
[2019-04-03 21:56:24,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23129: learning rate 0.0001
[2019-04-03 21:56:24,616] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 23139: loss 1.5207
[2019-04-03 21:56:24,616] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 23139: learning rate 0.0001
[2019-04-03 21:56:24,857] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23207: loss -0.0103
[2019-04-03 21:56:24,857] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23207: learning rate 0.0001
[2019-04-03 21:56:25,296] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 23312: loss 0.0997
[2019-04-03 21:56:25,297] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 23312: learning rate 0.0001
[2019-04-03 21:56:25,423] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23340: loss 4.7013
[2019-04-03 21:56:25,425] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23340: learning rate 0.0001
[2019-04-03 21:56:26,807] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 23773: loss -4.0702
[2019-04-03 21:56:26,807] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 23773: learning rate 0.0001
[2019-04-03 21:56:28,271] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 24154: loss 22.4454
[2019-04-03 21:56:28,273] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 24155: learning rate 0.0001
[2019-04-03 21:56:28,585] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1500, global step 24226: loss -0.8800
[2019-04-03 21:56:28,586] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1500, global step 24226: learning rate 0.0001
[2019-04-03 21:56:28,617] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 24235: loss -2.4682
[2019-04-03 21:56:28,618] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 24235: learning rate 0.0001
[2019-04-03 21:56:28,965] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24310: loss -2.1551
[2019-04-03 21:56:28,967] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24310: learning rate 0.0001
[2019-04-03 21:56:29,554] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24448: loss 0.1240
[2019-04-03 21:56:29,560] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24448: learning rate 0.0001
[2019-04-03 21:56:29,676] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 24482: loss 1.0822
[2019-04-03 21:56:29,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 24482: learning rate 0.0001
[2019-04-03 21:56:29,764] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24504: loss 5.7427
[2019-04-03 21:56:29,767] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24505: learning rate 0.0001
[2019-04-03 21:56:30,642] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24691: loss 21.6265
[2019-04-03 21:56:30,643] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24691: learning rate 0.0001
[2019-04-03 21:56:31,417] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24865: loss 13.1549
[2019-04-03 21:56:31,418] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24865: learning rate 0.0001
[2019-04-03 21:56:33,093] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8546149e-08 5.1268858e-01 1.6291683e-06 1.0242566e-03 3.9003545e-01
 5.6532783e-05 9.6193515e-02], sum to 1.0000
[2019-04-03 21:56:33,093] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4545
[2019-04-03 21:56:33,314] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.9, 93.0, 92.0, 0.0, 24.0, 24.54740116398906, 0.07261437563255889, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 915000.0000, 
sim time next is 915600.0000, 
raw observation next is [4.0, 93.0, 91.0, 0.0, 23.0, 24.38939755018687, 0.03174673438244368, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5734072022160666, 0.93, 0.30333333333333334, 0.0, 0.4166666666666667, 0.532449795848906, 0.5105822447941478, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6697641], dtype=float32), -0.32754818]. 
=============================================
[2019-04-03 21:56:39,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1286885e-08 1.6411494e-01 3.5227922e-07 5.8388095e-03 1.2547576e-01
 1.1846721e-06 7.0456898e-01], sum to 1.0000
[2019-04-03 21:56:39,400] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5291
[2019-04-03 21:56:39,445] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 106.5, 0.0, 26.0, 25.40919363179468, 0.372923875437582, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1000800.0000, 
sim time next is 1001400.0000, 
raw observation next is [14.4, 81.0, 102.3333333333333, 0.0, 26.0, 25.44876659299613, 0.3839928688641303, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.341111111111111, 0.0, 0.6666666666666666, 0.6207305494163441, 0.6279976229547101, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2590536], dtype=float32), 0.22969675]. 
=============================================
[2019-04-03 21:56:49,446] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2000, global step 30579: loss 5.4722
[2019-04-03 21:56:49,450] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2000, global step 30579: learning rate 0.0001
[2019-04-03 21:56:49,459] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 30586: loss 3.5049
[2019-04-03 21:56:49,460] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 30586: learning rate 0.0001
[2019-04-03 21:56:49,597] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5627838e-09 1.9803414e-02 1.4455500e-06 2.9362170e-03 2.0634598e-01
 6.2135581e-07 7.7091235e-01], sum to 1.0000
[2019-04-03 21:56:49,598] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4023
[2019-04-03 21:56:49,603] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.55, 55.0, 0.0, 0.0, 26.0, 26.3914371059545, 0.7110775490843357, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1103400.0000, 
sim time next is 1104000.0000, 
raw observation next is [15.36666666666667, 55.66666666666667, 0.0, 0.0, 26.0, 26.25262590384787, 0.6976246523411365, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8882733148661128, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.687718825320656, 0.7325415507803789, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4764101], dtype=float32), 0.5033084]. 
=============================================
[2019-04-03 21:56:49,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[88.383545]
 [88.49549 ]
 [88.59937 ]
 [88.65359 ]
 [88.677536]], R is [[88.4343338 ]
 [88.54998779]
 [88.66448975]
 [88.77784729]
 [88.89006805]].
[2019-04-03 21:56:49,998] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 30850: loss 8.0485
[2019-04-03 21:56:50,001] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 30850: learning rate 0.0001
[2019-04-03 21:56:50,139] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 30928: loss 6.2364
[2019-04-03 21:56:50,161] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 30928: learning rate 0.0001
[2019-04-03 21:56:50,281] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31000: loss 5.0423
[2019-04-03 21:56:50,283] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31000: learning rate 0.0001
[2019-04-03 21:56:51,129] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 31446: loss 2.4203
[2019-04-03 21:56:51,130] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 31447: learning rate 0.0001
[2019-04-03 21:56:51,797] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 31801: loss 1.8290
[2019-04-03 21:56:51,799] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 31801: learning rate 0.0001
[2019-04-03 21:56:52,487] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32155: loss 2.8476
[2019-04-03 21:56:52,489] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32157: learning rate 0.0001
[2019-04-03 21:56:53,077] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32432: loss 3.9590
[2019-04-03 21:56:53,079] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32435: learning rate 0.0001
[2019-04-03 21:56:53,301] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32530: loss 2.5936
[2019-04-03 21:56:53,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32531: learning rate 0.0001
[2019-04-03 21:56:53,317] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 32533: loss 4.0728
[2019-04-03 21:56:53,320] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 32534: learning rate 0.0001
[2019-04-03 21:56:53,480] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32602: loss 2.7973
[2019-04-03 21:56:53,480] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32602: learning rate 0.0001
[2019-04-03 21:56:53,567] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2000, global step 32631: loss 2.7727
[2019-04-03 21:56:53,572] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2000, global step 32631: learning rate 0.0001
[2019-04-03 21:56:54,517] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 33063: loss 3.1414
[2019-04-03 21:56:54,519] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 33063: learning rate 0.0001
[2019-04-03 21:56:54,684] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 33131: loss 1.9605
[2019-04-03 21:56:54,685] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 33131: learning rate 0.0001
[2019-04-03 21:56:55,195] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 33364: loss 2.8160
[2019-04-03 21:56:55,199] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 33364: learning rate 0.0001
[2019-04-03 21:57:08,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4930572e-11 2.4373764e-03 5.7923016e-10 5.5186375e-04 8.7489575e-02
 2.2274618e-09 9.0952122e-01], sum to 1.0000
[2019-04-03 21:57:08,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6425
[2019-04-03 21:57:09,175] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.22969543171212, 0.4850250624730421, 0.0, 1.0, 41135.88733779678], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1381200.0000, 
sim time next is 1381800.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.21286157783024, 0.4860583127794655, 0.0, 1.0, 40943.82426115229], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6010717981525199, 0.6620194375931552, 0.0, 1.0, 0.1949705917197728], 
reward next is 0.8050, 
noisyNet noise sample is [array([-0.08669123], dtype=float32), 0.99879086]. 
=============================================
[2019-04-03 21:57:12,071] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.10157941e-11 1.45775371e-03 5.14838705e-09 5.36871725e-04
 1.02434024e-01 1.11539418e-08 8.95571351e-01], sum to 1.0000
[2019-04-03 21:57:12,072] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9620
[2019-04-03 21:57:12,331] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 54.66666666666666, 0.0, 26.0, 26.03959329027296, 0.5081324453996278, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1437000.0000, 
sim time next is 1437600.0000, 
raw observation next is [1.1, 92.0, 50.33333333333333, 0.0, 26.0, 25.92381638853897, 0.5069379827750623, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.16777777777777778, 0.0, 0.6666666666666666, 0.6603180323782475, 0.6689793275916874, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60893387], dtype=float32), -0.43537787]. 
=============================================
[2019-04-03 21:57:16,544] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2500, global step 38746: loss 1.8536
[2019-04-03 21:57:16,545] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2500, global step 38746: learning rate 0.0001
[2019-04-03 21:57:18,406] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39132: loss 1.5985
[2019-04-03 21:57:18,406] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39132: learning rate 0.0001
[2019-04-03 21:57:18,787] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39199: loss 0.3547
[2019-04-03 21:57:18,788] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39199: learning rate 0.0001
[2019-04-03 21:57:19,074] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39246: loss 2.0863
[2019-04-03 21:57:19,075] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39246: learning rate 0.0001
[2019-04-03 21:57:20,417] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 39448: loss 1.5538
[2019-04-03 21:57:20,449] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 39448: learning rate 0.0001
[2019-04-03 21:57:20,518] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 39471: loss 1.4079
[2019-04-03 21:57:20,518] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 39471: learning rate 0.0001
[2019-04-03 21:57:21,840] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 39690: loss 0.9287
[2019-04-03 21:57:21,853] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 39690: learning rate 0.0001
[2019-04-03 21:57:22,864] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39940: loss 1.0449
[2019-04-03 21:57:22,865] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39941: learning rate 0.0001
[2019-04-03 21:57:24,669] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.9701416e-10 2.6003813e-02 2.7803262e-07 4.5351064e-04 6.3143969e-01
 4.7021950e-07 3.4210229e-01], sum to 1.0000
[2019-04-03 21:57:24,669] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4894
[2019-04-03 21:57:24,726] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.7, 82.5, 0.0, 0.0, 26.0, 25.52778030014558, 0.5569354950050492, 0.0, 1.0, 9374.982461393132], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1564200.0000, 
sim time next is 1564800.0000, 
raw observation next is [4.600000000000001, 83.66666666666666, 0.0, 0.0, 26.0, 25.68106735461973, 0.5399238924807606, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5900277008310251, 0.8366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6400889462183109, 0.6799746308269201, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20507033], dtype=float32), 0.4131347]. 
=============================================
[2019-04-03 21:57:25,207] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40545: loss 1.6702
[2019-04-03 21:57:25,208] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40545: learning rate 0.0001
[2019-04-03 21:57:25,562] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 40610: loss 2.5202
[2019-04-03 21:57:25,564] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 40610: learning rate 0.0001
[2019-04-03 21:57:25,709] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40650: loss 3.4142
[2019-04-03 21:57:25,709] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40650: learning rate 0.0001
[2019-04-03 21:57:25,814] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2500, global step 40686: loss 2.0377
[2019-04-03 21:57:25,815] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2500, global step 40686: learning rate 0.0001
[2019-04-03 21:57:25,891] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40706: loss 2.2380
[2019-04-03 21:57:25,891] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40706: learning rate 0.0001
[2019-04-03 21:57:26,043] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40748: loss 1.2117
[2019-04-03 21:57:26,045] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40748: learning rate 0.0001
[2019-04-03 21:57:26,800] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40985: loss 2.2545
[2019-04-03 21:57:26,801] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40985: learning rate 0.0001
[2019-04-03 21:57:27,269] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 41128: loss 2.5498
[2019-04-03 21:57:27,269] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 41128: learning rate 0.0001
[2019-04-03 21:57:30,351] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.1650292e-11 8.8066532e-04 9.3986809e-09 1.7783456e-04 1.4331539e-01
 1.5283087e-08 8.5562605e-01], sum to 1.0000
[2019-04-03 21:57:30,352] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3570
[2019-04-03 21:57:30,366] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 74.0, 0.0, 0.0, 26.0, 25.42508175719121, 0.6829984091502088, 0.0, 1.0, 61556.08662674941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1544400.0000, 
sim time next is 1545000.0000, 
raw observation next is [7.516666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 25.8451037973143, 0.699457890700962, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6708217913204063, 0.7433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6537586497761918, 0.7331526302336541, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9040961], dtype=float32), 1.54019]. 
=============================================
[2019-04-03 21:57:30,369] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[112.38593]
 [112.75068]
 [111.74757]
 [111.18656]
 [110.75132]], R is [[112.50554657]
 [112.08737183]
 [111.13957214]
 [110.08332062]
 [109.04597473]].
[2019-04-03 21:57:46,131] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2436531e-10 1.6456170e-03 1.5045639e-08 3.0927688e-03 6.2786557e-02
 7.0893797e-08 9.3247497e-01], sum to 1.0000
[2019-04-03 21:57:46,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2929
[2019-04-03 21:57:46,288] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 101.1666666666667, 0.0, 26.0, 24.76443139319572, 0.4265619932987548, 1.0, 1.0, 196464.9794054528], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1689600.0000, 
sim time next is 1690200.0000, 
raw observation next is [1.1, 88.0, 100.0, 0.0, 26.0, 24.24950551790538, 0.4312265888611167, 1.0, 1.0, 197950.3739572187], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3333333333333333, 0.0, 0.6666666666666666, 0.520792126492115, 0.6437421962870389, 1.0, 1.0, 0.942620828367708], 
reward next is 0.0574, 
noisyNet noise sample is [array([0.78706545], dtype=float32), -0.4702003]. 
=============================================
[2019-04-03 21:57:53,141] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3000, global step 47013: loss -0.6050
[2019-04-03 21:57:53,142] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3000, global step 47013: learning rate 0.0001
[2019-04-03 21:57:54,061] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47171: loss -0.1181
[2019-04-03 21:57:54,062] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47171: learning rate 0.0001
[2019-04-03 21:57:54,738] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47308: loss -0.0675
[2019-04-03 21:57:54,741] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47310: learning rate 0.0001
[2019-04-03 21:57:54,952] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 47349: loss -2.0796
[2019-04-03 21:57:54,952] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 47349: learning rate 0.0001
[2019-04-03 21:57:55,054] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47377: loss -0.4382
[2019-04-03 21:57:55,056] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47377: learning rate 0.0001
[2019-04-03 21:57:55,884] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 47538: loss -0.5203
[2019-04-03 21:57:55,884] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 47538: learning rate 0.0001
[2019-04-03 21:57:55,901] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 47541: loss -0.0650
[2019-04-03 21:57:55,902] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 47541: learning rate 0.0001
[2019-04-03 21:57:57,562] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47849: loss -0.2955
[2019-04-03 21:57:57,570] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47849: learning rate 0.0001
[2019-04-03 21:57:58,000] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4287312e-08 8.1773233e-03 8.0122624e-07 9.6338853e-04 4.4283849e-01
 3.3580263e-06 5.4801661e-01], sum to 1.0000
[2019-04-03 21:57:58,001] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5947
[2019-04-03 21:57:58,094] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.933333333333333, 82.16666666666667, 0.0, 0.0, 26.0, 24.20075307242129, 0.1211343904426185, 0.0, 1.0, 46314.64427647562], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1821000.0000, 
sim time next is 1821600.0000, 
raw observation next is [-6.0, 83.0, 0.0, 0.0, 26.0, 24.1660017885738, 0.1134127446249533, 0.0, 1.0, 46388.49555660396], 
processed observation next is [0.0, 0.08695652173913043, 0.296398891966759, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5138334823811501, 0.5378042482083177, 0.0, 1.0, 0.22089759788859026], 
reward next is 0.7791, 
noisyNet noise sample is [array([-0.37086922], dtype=float32), 1.6427202]. 
=============================================
[2019-04-03 21:58:00,979] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 48467: loss -0.5544
[2019-04-03 21:58:01,015] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 48467: learning rate 0.0001
[2019-04-03 21:58:01,712] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3000, global step 48574: loss -0.1156
[2019-04-03 21:58:01,733] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3000, global step 48574: learning rate 0.0001
[2019-04-03 21:58:02,449] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48675: loss -0.5827
[2019-04-03 21:58:02,450] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48675: learning rate 0.0001
[2019-04-03 21:58:02,826] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48736: loss -0.2886
[2019-04-03 21:58:02,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48736: learning rate 0.0001
[2019-04-03 21:58:03,033] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48775: loss -0.1102
[2019-04-03 21:58:03,033] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48775: learning rate 0.0001
[2019-04-03 21:58:03,141] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 48793: loss -0.2821
[2019-04-03 21:58:03,144] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 48793: learning rate 0.0001
[2019-04-03 21:58:03,346] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48828: loss -0.5986
[2019-04-03 21:58:03,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48828: learning rate 0.0001
[2019-04-03 21:58:04,791] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 49076: loss -0.1697
[2019-04-03 21:58:04,846] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 49076: learning rate 0.0001
[2019-04-03 21:58:35,298] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3500, global step 55001: loss 1.2483
[2019-04-03 21:58:35,300] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3500, global step 55001: learning rate 0.0001
[2019-04-03 21:58:35,679] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55071: loss -4.1549
[2019-04-03 21:58:35,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55071: learning rate 0.0001
[2019-04-03 21:58:36,020] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 55133: loss 0.3980
[2019-04-03 21:58:36,020] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 55133: learning rate 0.0001
[2019-04-03 21:58:36,360] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55188: loss -3.4242
[2019-04-03 21:58:36,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55188: learning rate 0.0001
[2019-04-03 21:58:36,493] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55214: loss -1.3922
[2019-04-03 21:58:36,494] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55214: learning rate 0.0001
[2019-04-03 21:58:36,995] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 55291: loss 0.8982
[2019-04-03 21:58:36,995] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 55291: learning rate 0.0001
[2019-04-03 21:58:37,035] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 55298: loss 0.9758
[2019-04-03 21:58:37,035] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 55298: learning rate 0.0001
[2019-04-03 21:58:38,512] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55591: loss -2.7182
[2019-04-03 21:58:38,513] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55591: learning rate 0.0001
[2019-04-03 21:58:41,025] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3500, global step 56070: loss 0.5241
[2019-04-03 21:58:41,025] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3500, global step 56070: learning rate 0.0001
[2019-04-03 21:58:41,132] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 56096: loss 0.8087
[2019-04-03 21:58:41,133] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 56096: learning rate 0.0001
[2019-04-03 21:58:42,111] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56290: loss -1.3856
[2019-04-03 21:58:42,112] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56290: learning rate 0.0001
[2019-04-03 21:58:43,432] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56602: loss 0.5464
[2019-04-03 21:58:43,433] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56602: learning rate 0.0001
[2019-04-03 21:58:43,523] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56628: loss -1.5060
[2019-04-03 21:58:43,533] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56631: learning rate 0.0001
[2019-04-03 21:58:44,559] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56893: loss -1.1343
[2019-04-03 21:58:44,562] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56893: learning rate 0.0001
[2019-04-03 21:58:44,594] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56904: loss -3.2122
[2019-04-03 21:58:44,595] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56904: learning rate 0.0001
[2019-04-03 21:58:45,566] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 57163: loss -1.3093
[2019-04-03 21:58:45,567] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 57163: learning rate 0.0001
[2019-04-03 21:58:57,971] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9212962e-09 5.0755101e-04 3.3160140e-07 6.2778872e-04 2.6825506e-02
 4.2711821e-07 9.7203833e-01], sum to 1.0000
[2019-04-03 21:58:57,971] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0511
[2019-04-03 21:58:58,031] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 87.0, 0.0, 0.0, 26.0, 24.05837324889223, 0.06309067626810543, 0.0, 1.0, 43611.92552822136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2260800.0000, 
sim time next is 2261400.0000, 
raw observation next is [-8.483333333333334, 87.66666666666667, 0.0, 0.0, 26.0, 24.00770776592125, 0.05520447320968088, 0.0, 1.0, 43599.89011702919], 
processed observation next is [1.0, 0.17391304347826086, 0.2276084949215143, 0.8766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5006423138267708, 0.5184014910698936, 0.0, 1.0, 0.20761852436680564], 
reward next is 0.7924, 
noisyNet noise sample is [array([0.7769887], dtype=float32), 1.2111375]. 
=============================================
[2019-04-03 21:59:12,840] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 62970: loss 0.3131
[2019-04-03 21:59:12,841] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 62970: learning rate 0.0001
[2019-04-03 21:59:14,233] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 63239: loss -0.9426
[2019-04-03 21:59:14,235] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 63239: learning rate 0.0001
[2019-04-03 21:59:14,444] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 63273: loss -2.8108
[2019-04-03 21:59:14,445] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 63274: learning rate 0.0001
[2019-04-03 21:59:14,567] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4000, global step 63293: loss -2.6259
[2019-04-03 21:59:14,571] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4000, global step 63293: learning rate 0.0001
[2019-04-03 21:59:15,476] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63442: loss -1.9253
[2019-04-03 21:59:15,493] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63442: learning rate 0.0001
[2019-04-03 21:59:15,876] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 63521: loss -1.3385
[2019-04-03 21:59:15,880] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 63521: learning rate 0.0001
[2019-04-03 21:59:16,232] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63589: loss 0.2427
[2019-04-03 21:59:16,232] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63592: learning rate 0.0001
[2019-04-03 21:59:16,314] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 63619: loss 0.0877
[2019-04-03 21:59:16,314] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 63619: learning rate 0.0001
[2019-04-03 21:59:18,989] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 64244: loss 0.4229
[2019-04-03 21:59:18,993] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 64244: learning rate 0.0001
[2019-04-03 21:59:19,250] A3C_AGENT_WORKER-Thread-8 INFO:Local step 4000, global step 64302: loss 0.4144
[2019-04-03 21:59:19,265] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 4000, global step 64302: learning rate 0.0001
[2019-04-03 21:59:19,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1097884e-07 9.8127581e-04 6.1038776e-07 1.0959207e-03 9.6223161e-02
 1.5588922e-06 9.0169740e-01], sum to 1.0000
[2019-04-03 21:59:19,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3495
[2019-04-03 21:59:19,831] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.42792306477737, -0.1019867423006762, 0.0, 1.0, 44419.96303543804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2436000.0000, 
sim time next is 2436600.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 26.0, 23.39197589197315, -0.1098567909943452, 0.0, 1.0, 44407.77557752245], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.6666666666666666, 0.44933132433109585, 0.4633810696685516, 0.0, 1.0, 0.21146559798820214], 
reward next is 0.7885, 
noisyNet noise sample is [array([-1.3445967], dtype=float32), -1.5296756]. 
=============================================
[2019-04-03 21:59:19,931] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64461: loss 0.4260
[2019-04-03 21:59:19,945] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64461: learning rate 0.0001
[2019-04-03 21:59:20,016] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4800961e-09 9.2865397e-05 8.5624734e-09 1.0415121e-04 9.0756407e-03
 3.0719654e-08 9.9072731e-01], sum to 1.0000
[2019-04-03 21:59:20,033] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8377
[2019-04-03 21:59:20,097] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 47.00000000000001, 110.6666666666667, 227.3333333333334, 26.0, 24.94902872094423, 0.3080088270480636, 0.0, 1.0, 28171.87178833127], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2387400.0000, 
sim time next is 2388000.0000, 
raw observation next is [0.0, 47.0, 98.33333333333333, 284.1666666666667, 26.0, 24.96495857966568, 0.3129836073187096, 0.0, 1.0, 20454.0355888984], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.3277777777777778, 0.31399631675874773, 0.6666666666666666, 0.58041321497214, 0.6043278691062365, 0.0, 1.0, 0.09740016947094476], 
reward next is 0.9026, 
noisyNet noise sample is [array([-0.83316994], dtype=float32), -0.48699385]. 
=============================================
[2019-04-03 21:59:20,156] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.57127 ]
 [80.71586 ]
 [80.856445]
 [81.06554 ]
 [81.29151 ]], R is [[80.57287598]
 [80.63299561]
 [80.64391327]
 [80.63078308]
 [80.64102173]].
[2019-04-03 21:59:20,836] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64639: loss 0.4709
[2019-04-03 21:59:20,836] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64639: learning rate 0.0001
[2019-04-03 21:59:22,011] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 64890: loss 0.5322
[2019-04-03 21:59:22,037] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 64890: learning rate 0.0001
[2019-04-03 21:59:22,498] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 65021: loss 0.5795
[2019-04-03 21:59:22,528] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 65025: learning rate 0.0001
[2019-04-03 21:59:22,852] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 65126: loss 0.5602
[2019-04-03 21:59:22,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 65126: learning rate 0.0001
[2019-04-03 21:59:24,368] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 65473: loss 0.6715
[2019-04-03 21:59:24,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 65473: learning rate 0.0001
[2019-04-03 21:59:24,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1900217e-09 7.9967402e-05 2.6403375e-08 2.9287091e-04 7.2049196e-03
 2.7910509e-08 9.9242216e-01], sum to 1.0000
[2019-04-03 21:59:24,644] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5329
[2019-04-03 21:59:24,849] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8999999999999999, 34.0, 0.0, 0.0, 26.0, 25.20039013585588, 0.2508426502497689, 0.0, 1.0, 40298.6105217578], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2500200.0000, 
sim time next is 2500800.0000, 
raw observation next is [-0.8, 34.33333333333334, 0.0, 0.0, 26.0, 25.18185990891742, 0.2475446971031683, 0.0, 1.0, 40253.90198284896], 
processed observation next is [0.0, 0.9565217391304348, 0.4404432132963989, 0.34333333333333343, 0.0, 0.0, 0.6666666666666666, 0.5984883257431184, 0.5825148990343895, 0.0, 1.0, 0.191685247537376], 
reward next is 0.8083, 
noisyNet noise sample is [array([1.4581487], dtype=float32), -0.41651046]. 
=============================================
[2019-04-03 21:59:45,575] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 70690: loss 0.6299
[2019-04-03 21:59:45,580] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 70691: learning rate 0.0001
[2019-04-03 21:59:46,880] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4500, global step 70986: loss 0.4593
[2019-04-03 21:59:46,881] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4500, global step 70986: learning rate 0.0001
[2019-04-03 21:59:46,989] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 71034: loss 0.4606
[2019-04-03 21:59:46,990] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 71034: learning rate 0.0001
[2019-04-03 21:59:47,334] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4500, global step 71120: loss 0.4854
[2019-04-03 21:59:47,388] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4500, global step 71120: learning rate 0.0001
[2019-04-03 21:59:48,404] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 71344: loss -1.4661
[2019-04-03 21:59:48,424] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 71344: learning rate 0.0001
[2019-04-03 21:59:48,594] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71389: loss 0.2943
[2019-04-03 21:59:48,610] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71391: learning rate 0.0001
[2019-04-03 21:59:48,720] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 71432: loss 0.3574
[2019-04-03 21:59:48,721] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 71432: learning rate 0.0001
[2019-04-03 21:59:49,200] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 71570: loss 0.2900
[2019-04-03 21:59:49,205] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 71571: learning rate 0.0001
[2019-04-03 21:59:50,573] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 71908: loss 0.2915
[2019-04-03 21:59:50,573] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 71908: learning rate 0.0001
[2019-04-03 21:59:51,738] A3C_AGENT_WORKER-Thread-8 INFO:Local step 4500, global step 72174: loss 0.3123
[2019-04-03 21:59:51,739] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 4500, global step 72174: learning rate 0.0001
[2019-04-03 21:59:51,872] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72207: loss 0.2930
[2019-04-03 21:59:51,899] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72207: learning rate 0.0001
[2019-04-03 21:59:52,760] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 72433: loss 0.2370
[2019-04-03 21:59:52,763] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 72433: learning rate 0.0001
[2019-04-03 21:59:53,979] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72734: loss 0.2156
[2019-04-03 21:59:53,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72734: learning rate 0.0001
[2019-04-03 21:59:54,044] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72757: loss 0.2292
[2019-04-03 21:59:54,048] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72757: learning rate 0.0001
[2019-04-03 21:59:54,373] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 72840: loss 0.2216
[2019-04-03 21:59:54,374] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 72840: learning rate 0.0001
[2019-04-03 21:59:56,834] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 73424: loss 0.1629
[2019-04-03 21:59:56,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 73424: learning rate 0.0001
[2019-04-03 22:00:05,855] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.2469597e-10 2.2056247e-06 7.3020612e-09 1.1218390e-04 3.7935909e-03
 2.1548682e-10 9.9609202e-01], sum to 1.0000
[2019-04-03 22:00:05,855] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3843
[2019-04-03 22:00:06,095] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 24.79273919836839, 0.3348117528267845, 0.0, 1.0, 183205.3147665771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2838600.0000, 
sim time next is 2839200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 24.84034566075241, 0.3610959420495708, 0.0, 1.0, 105649.935703486], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.5700288050627007, 0.6203653140165236, 0.0, 1.0, 0.5030949319213619], 
reward next is 0.4969, 
noisyNet noise sample is [array([0.20314005], dtype=float32), -1.1430234]. 
=============================================
[2019-04-03 22:00:13,459] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8527436e-12 6.9299915e-07 1.3731704e-11 5.5178070e-06 7.5193508e-05
 1.8631336e-11 9.9991858e-01], sum to 1.0000
[2019-04-03 22:00:13,459] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0782
[2019-04-03 22:00:13,478] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 164.0, 0.0, 26.0, 25.35667432947472, 0.3363077041993843, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2899800.0000, 
sim time next is 2900400.0000, 
raw observation next is [2.0, 100.0, 151.6666666666667, 0.0, 26.0, 25.39731256301352, 0.3385448379912159, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.5055555555555558, 0.0, 0.6666666666666666, 0.6164427135844601, 0.6128482793304053, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3792747], dtype=float32), 0.26680544]. 
=============================================
[2019-04-03 22:00:15,078] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7398662e-10 5.8447067e-06 1.4012098e-09 4.2196945e-05 1.7858453e-03
 1.0774004e-09 9.9816614e-01], sum to 1.0000
[2019-04-03 22:00:15,078] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2339
[2019-04-03 22:00:15,229] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.00000000000001, 0.0, 0.0, 26.0, 24.85789737026417, 0.3229278375925482, 0.0, 1.0, 43289.10546502016], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2941800.0000, 
sim time next is 2942400.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.82694007722237, 0.3150732121356645, 0.0, 1.0, 43291.54801895672], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5689116731018643, 0.6050244040452215, 0.0, 1.0, 0.20615022866169866], 
reward next is 0.7938, 
noisyNet noise sample is [array([0.2388832], dtype=float32), -1.8051586]. 
=============================================
[2019-04-03 22:00:16,213] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5000, global step 78671: loss 0.1472
[2019-04-03 22:00:16,213] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5000, global step 78671: learning rate 0.0001
[2019-04-03 22:00:16,744] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 78790: loss 0.1049
[2019-04-03 22:00:16,745] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 78790: learning rate 0.0001
[2019-04-03 22:00:16,890] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.4423829e-09 1.1362171e-03 2.6746326e-07 1.7686271e-03 2.2663511e-02
 2.7522464e-07 9.7443116e-01], sum to 1.0000
[2019-04-03 22:00:16,914] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0715
[2019-04-03 22:00:17,092] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.52220861900584, 0.2395859630912371, 0.0, 1.0, 42679.32363770415], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2950800.0000, 
sim time next is 2951400.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.48357815075277, 0.231484457791979, 0.0, 1.0, 42737.27427850183], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5402981792293975, 0.5771614859306596, 0.0, 1.0, 0.20351082989762775], 
reward next is 0.7965, 
noisyNet noise sample is [array([0.08637522], dtype=float32), -0.17345613]. 
=============================================
[2019-04-03 22:00:17,572] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5000, global step 78924: loss 0.0974
[2019-04-03 22:00:17,572] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5000, global step 78924: learning rate 0.0001
[2019-04-03 22:00:19,231] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.6125823e-08 1.0406420e-03 1.0655333e-07 2.0112409e-03 7.6880753e-02
 3.8203929e-08 9.2006713e-01], sum to 1.0000
[2019-04-03 22:00:19,237] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3060
[2019-04-03 22:00:19,282] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 79283: loss 0.0741
[2019-04-03 22:00:19,293] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 79284: learning rate 0.0001
[2019-04-03 22:00:19,447] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.14748836443009, 0.3184201692423906, 0.0, 1.0, 38639.58576301083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3018000.0000, 
sim time next is 3018600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.12005199160612, 0.3099281683767692, 0.0, 1.0, 38536.26499683742], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5933376659671765, 0.6033093894589231, 0.0, 1.0, 0.1835060237944639], 
reward next is 0.8165, 
noisyNet noise sample is [array([0.9002551], dtype=float32), -0.111127965]. 
=============================================
[2019-04-03 22:00:21,019] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 79729: loss 0.0765
[2019-04-03 22:00:21,020] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 79729: learning rate 0.0001
[2019-04-03 22:00:21,273] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 79763: loss 0.0786
[2019-04-03 22:00:21,274] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 79763: learning rate 0.0001
[2019-04-03 22:00:22,102] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79949: loss 0.0818
[2019-04-03 22:00:22,103] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79949: learning rate 0.0001
[2019-04-03 22:00:22,882] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 80071: loss 0.0828
[2019-04-03 22:00:22,884] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 80071: learning rate 0.0001
[2019-04-03 22:00:23,380] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 80163: loss 0.0843
[2019-04-03 22:00:23,382] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 80163: learning rate 0.0001
[2019-04-03 22:00:23,623] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80186: loss 0.0685
[2019-04-03 22:00:23,624] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80186: learning rate 0.0001
[2019-04-03 22:00:23,863] A3C_AGENT_WORKER-Thread-8 INFO:Local step 5000, global step 80219: loss 0.1013
[2019-04-03 22:00:23,864] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 5000, global step 80219: learning rate 0.0001
[2019-04-03 22:00:25,283] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.9258216e-09 1.4452504e-04 3.9127961e-08 3.8594371e-04 1.1415353e-02
 1.5761644e-08 9.8805410e-01], sum to 1.0000
[2019-04-03 22:00:25,284] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9400
[2019-04-03 22:00:25,345] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 55.83333333333334, 74.0, 602.6666666666667, 26.0, 25.13807049132855, 0.4095937275217976, 0.0, 1.0, 18706.97687464843], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2994600.0000, 
sim time next is 2995200.0000, 
raw observation next is [-1.0, 55.0, 69.5, 570.5, 26.0, 25.1485810504084, 0.4077719122581202, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.55, 0.23166666666666666, 0.6303867403314917, 0.6666666666666666, 0.5957150875340332, 0.6359239707527068, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40561968], dtype=float32), -0.32710668]. 
=============================================
[2019-04-03 22:00:25,915] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80710: loss -1.2914
[2019-04-03 22:00:25,917] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80710: learning rate 0.0001
[2019-04-03 22:00:25,942] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 80710: loss 0.1531
[2019-04-03 22:00:26,014] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 80710: learning rate 0.0001
[2019-04-03 22:00:26,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3982670e-08 8.7446102e-04 6.7486191e-08 1.9572228e-03 8.8313604e-03
 1.6967127e-07 9.8833662e-01], sum to 1.0000
[2019-04-03 22:00:26,224] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0116
[2019-04-03 22:00:26,237] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 72.33333333333334, 0.0, 0.0, 26.0, 23.73353029670082, -0.01534898816244457, 0.0, 1.0, 40256.92130183658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3046800.0000, 
sim time next is 3047400.0000, 
raw observation next is [-6.0, 73.5, 0.0, 0.0, 26.0, 23.70715570182383, -0.01997213569679513, 0.0, 1.0, 40308.97534106412], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.735, 0.0, 0.0, 0.6666666666666666, 0.47559630848531914, 0.4933426214344016, 0.0, 1.0, 0.19194750162411486], 
reward next is 0.8081, 
noisyNet noise sample is [array([-0.32873872], dtype=float32), 0.18028028]. 
=============================================
[2019-04-03 22:00:26,445] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 80822: loss 0.1538
[2019-04-03 22:00:26,466] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 80822: learning rate 0.0001
[2019-04-03 22:00:27,578] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 81104: loss 0.1979
[2019-04-03 22:00:27,579] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 81104: learning rate 0.0001
[2019-04-03 22:00:28,740] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 81371: loss 0.2775
[2019-04-03 22:00:28,743] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 81371: learning rate 0.0001
[2019-04-03 22:00:29,559] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.6030361e-10 2.9563362e-05 2.3261710e-09 2.6002823e-04 8.2427950e-04
 7.0247164e-09 9.9888617e-01], sum to 1.0000
[2019-04-03 22:00:29,559] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4955
[2019-04-03 22:00:29,616] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 54.0, 102.5, 697.0, 26.0, 25.24264193947732, 0.3175440634906181, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3060000.0000, 
sim time next is 3060600.0000, 
raw observation next is [-4.0, 54.0, 103.6666666666667, 717.6666666666667, 26.0, 25.20023536099408, 0.3178828279636817, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.34555555555555567, 0.7930018416206263, 0.6666666666666666, 0.6000196134161732, 0.6059609426545606, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1767713], dtype=float32), -1.5228125]. 
=============================================
[2019-04-03 22:00:33,155] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.04773294e-10 3.14449753e-05 6.59276411e-10 1.18182506e-04
 2.34107720e-03 8.87383611e-10 9.97509360e-01], sum to 1.0000
[2019-04-03 22:00:33,155] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2928
[2019-04-03 22:00:33,198] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 100.0, 0.0, 0.0, 26.0, 25.36157446763756, 0.3415331958092201, 0.0, 1.0, 48787.61044959391], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3105000.0000, 
sim time next is 3105600.0000, 
raw observation next is [-0.3333333333333334, 100.0, 0.0, 0.0, 26.0, 25.39681327088925, 0.346671930546594, 0.0, 1.0, 30949.25314093123], 
processed observation next is [0.0, 0.9565217391304348, 0.4533702677747, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6164011059074376, 0.615557310182198, 0.0, 1.0, 0.14737739590919632], 
reward next is 0.8526, 
noisyNet noise sample is [array([1.8301619], dtype=float32), -0.81209934]. 
=============================================
[2019-04-03 22:00:40,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.3688444e-13 1.8527103e-07 4.1436216e-12 2.4917383e-06 4.8107468e-05
 6.4400100e-12 9.9994922e-01], sum to 1.0000
[2019-04-03 22:00:40,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8060
[2019-04-03 22:00:40,300] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.833333333333334, 100.0, 88.33333333333334, 477.0, 26.0, 26.07389162331545, 0.5029232584740516, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3142200.0000, 
sim time next is 3142800.0000, 
raw observation next is [7.0, 100.0, 91.0, 519.5, 26.0, 26.21322687214066, 0.5176531774067985, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.30333333333333334, 0.5740331491712707, 0.6666666666666666, 0.6844355726783883, 0.6725510591355995, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.25388846], dtype=float32), -0.57077295]. 
=============================================
[2019-04-03 22:00:42,961] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.2295282e-11 4.0455980e-05 1.6745036e-09 1.8937375e-05 1.2448516e-04
 3.8385792e-10 9.9981624e-01], sum to 1.0000
[2019-04-03 22:00:42,962] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4428
[2019-04-03 22:00:42,980] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.499773263622, 0.5488533285574565, 0.0, 1.0, 36777.34090674205], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3208800.0000, 
sim time next is 3209400.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.54497960224998, 0.5478956903448484, 0.0, 1.0, 18745.08055169804], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6287483001874984, 0.6826318967816162, 0.0, 1.0, 0.08926228834141924], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.3980169], dtype=float32), -1.460611]. 
=============================================
[2019-04-03 22:00:43,919] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5500, global step 86091: loss 0.0171
[2019-04-03 22:00:43,919] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5500, global step 86091: learning rate 0.0001
[2019-04-03 22:00:44,674] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5500, global step 86360: loss 0.0037
[2019-04-03 22:00:44,676] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5500, global step 86360: learning rate 0.0001
[2019-04-03 22:00:45,234] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 86534: loss 0.0053
[2019-04-03 22:00:45,265] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 86534: learning rate 0.0001
[2019-04-03 22:00:47,466] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 87215: loss 0.0063
[2019-04-03 22:00:47,467] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 87215: learning rate 0.0001
[2019-04-03 22:00:48,113] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 87443: loss 0.0104
[2019-04-03 22:00:48,113] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 87443: learning rate 0.0001
[2019-04-03 22:00:48,558] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 87587: loss 0.0013
[2019-04-03 22:00:48,558] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 87587: learning rate 0.0001
[2019-04-03 22:00:48,871] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87699: loss 0.0034
[2019-04-03 22:00:48,872] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87699: learning rate 0.0001
[2019-04-03 22:00:49,887] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 88009: loss 0.0088
[2019-04-03 22:00:49,888] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 88009: learning rate 0.0001
[2019-04-03 22:00:50,002] A3C_AGENT_WORKER-Thread-8 INFO:Local step 5500, global step 88047: loss 0.0046
[2019-04-03 22:00:50,003] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 5500, global step 88047: learning rate 0.0001
[2019-04-03 22:00:50,820] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 88309: loss 0.0095
[2019-04-03 22:00:50,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 88309: learning rate 0.0001
[2019-04-03 22:00:50,862] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 88323: loss 0.0047
[2019-04-03 22:00:50,867] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 88323: learning rate 0.0001
[2019-04-03 22:00:51,416] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 88511: loss 0.0147
[2019-04-03 22:00:51,417] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 88511: learning rate 0.0001
[2019-04-03 22:00:52,002] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88716: loss 0.0055
[2019-04-03 22:00:52,004] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88716: learning rate 0.0001
[2019-04-03 22:00:52,635] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88907: loss 0.0071
[2019-04-03 22:00:52,636] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88907: learning rate 0.0001
[2019-04-03 22:00:53,079] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 89053: loss 0.0101
[2019-04-03 22:00:53,081] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 89053: learning rate 0.0001
[2019-04-03 22:00:54,695] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 89551: loss 0.0396
[2019-04-03 22:00:54,710] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 89551: learning rate 0.0001
[2019-04-03 22:00:55,136] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9412743e-10 2.3331080e-05 7.9797752e-10 2.4133142e-04 2.2769012e-04
 2.7293431e-10 9.9950767e-01], sum to 1.0000
[2019-04-03 22:00:55,137] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7490
[2019-04-03 22:00:55,191] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 48.0, 87.0, 674.0, 26.0, 26.70627341513327, 0.6046629561827696, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3339000.0000, 
sim time next is 3339600.0000, 
raw observation next is [-2.333333333333333, 47.33333333333333, 82.5, 645.1666666666667, 26.0, 26.0538704899812, 0.6325342196124738, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3979686057248385, 0.4733333333333333, 0.275, 0.7128913443830571, 0.6666666666666666, 0.6711558741651, 0.7108447398708245, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29045907], dtype=float32), -0.04359956]. 
=============================================
[2019-04-03 22:00:55,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8776766e-10 3.9303002e-05 2.9974240e-10 1.3115176e-04 2.8797926e-04
 5.3266358e-10 9.9954152e-01], sum to 1.0000
[2019-04-03 22:00:55,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2384
[2019-04-03 22:00:55,309] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.666666666666667, 68.0, 109.8333333333333, 719.1666666666667, 26.0, 26.31540119310965, 0.5701598932122728, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3320400.0000, 
sim time next is 3321000.0000, 
raw observation next is [-7.5, 67.0, 111.0, 740.0, 26.0, 26.32762022032994, 0.5766294495061444, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2548476454293629, 0.67, 0.37, 0.8176795580110497, 0.6666666666666666, 0.6939683516941617, 0.6922098165020482, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.365777], dtype=float32), 0.35418746]. 
=============================================
[2019-04-03 22:00:55,318] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[86.02207]
 [86.11234]
 [86.29052]
 [86.29841]
 [86.19036]], R is [[85.98202515]
 [86.12220764]
 [86.26098633]
 [86.39837646]
 [86.53439331]].
[2019-04-03 22:00:56,669] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3414891e-11 1.0960712e-05 2.3420171e-10 5.2555674e-06 4.5028815e-04
 1.5688471e-11 9.9953353e-01], sum to 1.0000
[2019-04-03 22:00:56,690] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2568
[2019-04-03 22:00:56,808] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 49.33333333333334, 43.66666666666667, 378.3333333333334, 26.0, 26.53499404322155, 0.6501699207528522, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3343800.0000, 
sim time next is 3344400.0000, 
raw observation next is [-2.0, 50.0, 35.5, 317.0, 26.0, 26.27075852308992, 0.6276357247293252, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.40720221606648205, 0.5, 0.11833333333333333, 0.35027624309392263, 0.6666666666666666, 0.68922987692416, 0.7092119082431084, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6972879], dtype=float32), 0.20095523]. 
=============================================
[2019-04-03 22:01:00,509] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4677504e-11 2.9749036e-04 1.3443696e-09 7.2182076e-05 9.4274880e-04
 1.3989021e-09 9.9868757e-01], sum to 1.0000
[2019-04-03 22:01:00,517] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7074
[2019-04-03 22:01:00,579] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.110223024625157e-16, 56.00000000000001, 97.0, 618.6666666666667, 26.0, 26.26090584973278, 0.5426733471029316, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3403200.0000, 
sim time next is 3403800.0000, 
raw observation next is [0.5, 54.0, 99.0, 658.0, 26.0, 26.34304856520561, 0.5625553505450777, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.54, 0.33, 0.7270718232044199, 0.6666666666666666, 0.6952540471004675, 0.6875184501816926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86667836], dtype=float32), 0.31237835]. 
=============================================
[2019-04-03 22:01:06,123] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6000, global step 94287: loss 0.0233
[2019-04-03 22:01:06,127] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6000, global step 94289: learning rate 0.0001
[2019-04-03 22:01:06,224] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6000, global step 94342: loss 0.0169
[2019-04-03 22:01:06,226] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6000, global step 94342: learning rate 0.0001
[2019-04-03 22:01:07,009] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 94705: loss 0.0023
[2019-04-03 22:01:07,010] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 94705: learning rate 0.0001
[2019-04-03 22:01:07,583] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 94967: loss 0.0000
[2019-04-03 22:01:07,599] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 94977: learning rate 0.0001
[2019-04-03 22:01:08,861] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 95588: loss 0.0072
[2019-04-03 22:01:08,863] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 95588: learning rate 0.0001
[2019-04-03 22:01:08,873] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2438455e-08 3.5934611e-03 1.2022623e-07 5.2659732e-04 7.7376748e-03
 6.7269490e-08 9.8814207e-01], sum to 1.0000
[2019-04-03 22:01:08,876] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9207
[2019-04-03 22:01:08,887] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.666666666666668, 25.66666666666667, 0.0, 0.0, 26.0, 25.5511002593219, 0.3612247262164186, 0.0, 1.0, 18745.17978307895], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3651600.0000, 
sim time next is 3652200.0000, 
raw observation next is [9.5, 26.0, 0.0, 0.0, 26.0, 25.53540918883996, 0.3563181339503146, 0.0, 1.0, 20387.70307928355], 
processed observation next is [0.0, 0.2608695652173913, 0.7257617728531857, 0.26, 0.0, 0.0, 0.6666666666666666, 0.6279507657366633, 0.6187727113167715, 0.0, 1.0, 0.09708430037754072], 
reward next is 0.9029, 
noisyNet noise sample is [array([1.6403562], dtype=float32), -0.5730477]. 
=============================================
[2019-04-03 22:01:08,980] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95653: loss -0.0007
[2019-04-03 22:01:08,986] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95655: learning rate 0.0001
[2019-04-03 22:01:09,281] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 95784: loss 0.0018
[2019-04-03 22:01:09,284] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 95786: learning rate 0.0001
[2019-04-03 22:01:10,011] A3C_AGENT_WORKER-Thread-8 INFO:Local step 6000, global step 96098: loss 0.0073
[2019-04-03 22:01:10,012] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 6000, global step 96098: learning rate 0.0001
[2019-04-03 22:01:10,309] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6979711e-09 2.5627185e-03 1.5141948e-08 4.2146738e-04 1.2833985e-03
 2.0954818e-08 9.9573237e-01], sum to 1.0000
[2019-04-03 22:01:10,309] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9977
[2019-04-03 22:01:10,340] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.50561489477938, 0.4318808285389823, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3615600.0000, 
sim time next is 3616200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.54917882427235, 0.4252203654692376, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6290982353560292, 0.6417401218230792, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23884666], dtype=float32), -0.9642894]. 
=============================================
[2019-04-03 22:01:10,870] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 96491: loss -0.7093
[2019-04-03 22:01:10,871] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 96491: learning rate 0.0001
[2019-04-03 22:01:11,012] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 96565: loss 0.0259
[2019-04-03 22:01:11,014] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 96566: learning rate 0.0001
[2019-04-03 22:01:11,104] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96611: loss -1.2331
[2019-04-03 22:01:11,106] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96611: learning rate 0.0001
[2019-04-03 22:01:11,503] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 96812: loss 0.0057
[2019-04-03 22:01:11,505] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 96815: learning rate 0.0001
[2019-04-03 22:01:12,183] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 97157: loss 0.0097
[2019-04-03 22:01:12,183] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 97157: learning rate 0.0001
[2019-04-03 22:01:12,279] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 97218: loss 0.0111
[2019-04-03 22:01:12,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 97218: learning rate 0.0001
[2019-04-03 22:01:12,496] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 97330: loss 0.0248
[2019-04-03 22:01:12,508] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 97331: learning rate 0.0001
[2019-04-03 22:01:13,911] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 98070: loss 0.1109
[2019-04-03 22:01:13,911] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 98070: learning rate 0.0001
[2019-04-03 22:01:16,293] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8002975e-10 3.4041066e-05 7.9522083e-10 2.9427445e-04 5.3825235e-04
 5.1370652e-10 9.9913341e-01], sum to 1.0000
[2019-04-03 22:01:16,294] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5917
[2019-04-03 22:01:16,301] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 44.33333333333334, 105.5, 783.0, 26.0, 25.38568942612213, 0.4744536898532328, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3680400.0000, 
sim time next is 3681000.0000, 
raw observation next is [6.0, 45.0, 104.0, 776.0, 26.0, 25.43216353503763, 0.4840871601265515, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.45, 0.3466666666666667, 0.8574585635359117, 0.6666666666666666, 0.6193469612531359, 0.6613623867088505, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18401125], dtype=float32), 1.0430648]. 
=============================================
[2019-04-03 22:01:16,305] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[89.687386]
 [89.699776]
 [89.71997 ]
 [89.76353 ]
 [89.85416 ]], R is [[89.75753021]
 [89.85995483]
 [89.96135712]
 [90.06174469]
 [90.16112518]].
[2019-04-03 22:01:16,787] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.2869516e-11 4.0982431e-04 7.0677858e-10 1.7478567e-05 2.4451513e-04
 7.8387852e-10 9.9932814e-01], sum to 1.0000
[2019-04-03 22:01:16,789] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5166
[2019-04-03 22:01:16,810] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 67.0, 0.0, 0.0, 26.0, 25.38746207620942, 0.3899052865941433, 0.0, 1.0, 43644.20503397665], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3712800.0000, 
sim time next is 3713400.0000, 
raw observation next is [-3.0, 68.0, 0.0, 0.0, 26.0, 25.3769960242748, 0.3879512731957052, 0.0, 1.0, 47192.68964014237], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6147496686895666, 0.6293170910652351, 0.0, 1.0, 0.2247270935244875], 
reward next is 0.7753, 
noisyNet noise sample is [array([-1.8257245], dtype=float32), 1.573065]. 
=============================================
[2019-04-03 22:01:17,528] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-03 22:01:17,530] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:01:17,531] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:01:17,531] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:01:17,532] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:01:17,533] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:01:17,535] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:01:17,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run2
[2019-04-03 22:01:17,540] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run2
[2019-04-03 22:01:17,567] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run2
[2019-04-03 22:01:58,200] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.20674337], dtype=float32), 0.17511941]
[2019-04-03 22:01:58,201] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.775557561562891e-17, 68.5, 0.0, 0.0, 26.0, 25.05114603633005, 0.301761482894924, 0.0, 1.0, 37969.0278484925]
[2019-04-03 22:01:58,201] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:01:58,201] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.2546558e-10 7.0016901e-04 2.8563387e-09 3.1397492e-04 1.1751204e-03
 4.6212869e-09 9.9781078e-01], sampled 0.49261759790201287
[2019-04-03 22:02:53,222] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.20674337], dtype=float32), 0.17511941]
[2019-04-03 22:02:53,222] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.879747248, 50.0070141, 86.48930103, 673.3572466, 26.0, 25.17941956260518, 0.3540679933804154, 0.0, 1.0, 0.0]
[2019-04-03 22:02:53,222] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-03 22:02:53,222] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.3915908e-09 2.4126505e-04 2.9069038e-09 3.2093859e-04 1.6932940e-03
 1.8290791e-09 9.9774444e-01], sampled 0.8826710195353874
[2019-04-03 22:03:11,108] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-03 22:03:37,236] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7902 263384057.0069 1551.8724
[2019-04-03 22:03:40,081] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6342 275806815.2143 1233.1762
[2019-04-03 22:03:41,127] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 100000, evaluation results [100000.0, 7241.790204729313, 263384057.00685066, 1551.8724049905088, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.634213265249, 275806815.21429336, 1233.1762374343539]
[2019-04-03 22:03:47,559] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6500, global step 102095: loss 0.1155
[2019-04-03 22:03:47,561] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6500, global step 102095: learning rate 0.0001
[2019-04-03 22:03:47,911] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6500, global step 102217: loss 0.0793
[2019-04-03 22:03:47,912] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6500, global step 102218: learning rate 0.0001
[2019-04-03 22:03:48,544] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 102401: loss 0.1037
[2019-04-03 22:03:48,546] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 102401: learning rate 0.0001
[2019-04-03 22:03:48,701] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.4202542e-10 1.7517102e-04 2.2977225e-08 3.0719908e-04 9.9841668e-04
 2.2938430e-08 9.9851912e-01], sum to 1.0000
[2019-04-03 22:03:48,703] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0292
[2019-04-03 22:03:48,716] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.55308843031354, 0.3987438724202033, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3898200.0000, 
sim time next is 3898800.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.39956451122486, 0.3781834128939551, 0.0, 1.0, 89275.63470299296], 
processed observation next is [1.0, 0.13043478260869565, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6166303759354049, 0.6260611376313183, 0.0, 1.0, 0.4251220700142522], 
reward next is 0.5749, 
noisyNet noise sample is [array([1.6277987], dtype=float32), -0.43645126]. 
=============================================
[2019-04-03 22:03:48,804] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 102492: loss 0.0946
[2019-04-03 22:03:48,836] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 102493: learning rate 0.0001
[2019-04-03 22:03:52,456] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 103593: loss 0.1304
[2019-04-03 22:03:52,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 103593: learning rate 0.0001
[2019-04-03 22:03:52,673] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 103662: loss 0.1374
[2019-04-03 22:03:52,674] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 103662: learning rate 0.0001
[2019-04-03 22:03:53,003] A3C_AGENT_WORKER-Thread-8 INFO:Local step 6500, global step 103780: loss 0.1153
[2019-04-03 22:03:53,003] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 6500, global step 103780: learning rate 0.0001
[2019-04-03 22:03:53,279] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103874: loss 0.1180
[2019-04-03 22:03:53,280] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103875: learning rate 0.0001
[2019-04-03 22:03:54,033] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 104135: loss 0.1316
[2019-04-03 22:03:54,033] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 104135: learning rate 0.0001
[2019-04-03 22:03:54,514] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104286: loss 0.1430
[2019-04-03 22:03:54,516] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104286: learning rate 0.0001
[2019-04-03 22:03:54,580] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 104313: loss 0.1125
[2019-04-03 22:03:54,582] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 104313: learning rate 0.0001
[2019-04-03 22:03:55,078] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 104476: loss 1.0642
[2019-04-03 22:03:55,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 104476: learning rate 0.0001
[2019-04-03 22:03:55,813] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104738: loss 0.1099
[2019-04-03 22:03:55,813] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104738: learning rate 0.0001
[2019-04-03 22:03:57,014] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 105093: loss 0.0808
[2019-04-03 22:03:57,027] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 105093: learning rate 0.0001
[2019-04-03 22:03:57,147] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 105136: loss 0.7678
[2019-04-03 22:03:57,169] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 105142: learning rate 0.0001
[2019-04-03 22:03:59,517] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 105804: loss 0.0589
[2019-04-03 22:03:59,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 105804: learning rate 0.0001
[2019-04-03 22:04:06,942] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9366924e-10 2.5573456e-05 9.9094999e-10 2.5664651e-04 2.6888883e-04
 2.1187342e-10 9.9944884e-01], sum to 1.0000
[2019-04-03 22:04:06,942] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8869
[2019-04-03 22:04:06,981] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 31.66666666666666, 117.3333333333333, 839.1666666666667, 26.0, 26.22484852527413, 0.5598719934177322, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4020000.0000, 
sim time next is 4020600.0000, 
raw observation next is [-4.333333333333333, 30.33333333333333, 116.6666666666667, 837.3333333333334, 26.0, 25.91252103218011, 0.5290256711334403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.342566943674977, 0.3033333333333333, 0.388888888888889, 0.9252302025782689, 0.6666666666666666, 0.6593767526816757, 0.6763418903778134, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8399045], dtype=float32), 0.45005718]. 
=============================================
[2019-04-03 22:04:07,813] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7116437e-09 5.5707437e-03 5.9275848e-08 1.1650717e-03 5.3067007e-03
 6.4026004e-08 9.8795742e-01], sum to 1.0000
[2019-04-03 22:04:07,823] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0345
[2019-04-03 22:04:07,853] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 38.0, 0.0, 0.0, 26.0, 25.04249686603852, 0.284858269173319, 0.0, 1.0, 40731.10329138637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4071600.0000, 
sim time next is 4072200.0000, 
raw observation next is [-5.0, 38.5, 0.0, 0.0, 26.0, 25.04526327714862, 0.2812463308696852, 0.0, 1.0, 40678.52027234907], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.385, 0.0, 0.0, 0.6666666666666666, 0.5871052730957184, 0.5937487769565618, 0.0, 1.0, 0.19370723939213844], 
reward next is 0.8063, 
noisyNet noise sample is [array([-0.75175244], dtype=float32), 0.3079668]. 
=============================================
[2019-04-03 22:04:08,288] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.4066623e-10 1.4641625e-04 1.5358366e-09 2.4953912e-04 7.8900834e-04
 3.4719507e-09 9.9881506e-01], sum to 1.0000
[2019-04-03 22:04:08,288] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1360
[2019-04-03 22:04:08,320] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 40.16666666666666, 0.0, 0.0, 26.0, 25.35960760673904, 0.4191466188464352, 0.0, 1.0, 51480.76333918254], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4151400.0000, 
sim time next is 4152000.0000, 
raw observation next is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 26.0, 25.33291582289689, 0.4149989718932308, 0.0, 1.0, 44756.25450635418], 
processed observation next is [0.0, 0.043478260869565216, 0.42566943674976926, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.6110763185747409, 0.638332990631077, 0.0, 1.0, 0.21312502145882942], 
reward next is 0.7869, 
noisyNet noise sample is [array([1.744721], dtype=float32), 0.14843857]. 
=============================================
[2019-04-03 22:04:08,338] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[81.99666]
 [82.10729]
 [82.11836]
 [82.45531]
 [82.97778]], R is [[81.60313416]
 [81.54195404]
 [81.51805115]
 [81.59931946]
 [81.65745544]].
[2019-04-03 22:04:11,934] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7000, global step 109846: loss 0.1259
[2019-04-03 22:04:11,936] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7000, global step 109846: learning rate 0.0001
[2019-04-03 22:04:13,054] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 110218: loss 0.1450
[2019-04-03 22:04:13,056] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 110218: learning rate 0.0001
[2019-04-03 22:04:13,137] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7000, global step 110239: loss 0.1641
[2019-04-03 22:04:13,138] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7000, global step 110239: learning rate 0.0001
[2019-04-03 22:04:14,414] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 110679: loss 0.1094
[2019-04-03 22:04:14,418] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 110679: learning rate 0.0001
[2019-04-03 22:04:18,179] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111887: loss 0.1662
[2019-04-03 22:04:18,193] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111893: learning rate 0.0001
[2019-04-03 22:04:18,582] A3C_AGENT_WORKER-Thread-8 INFO:Local step 7000, global step 112000: loss 0.1990
[2019-04-03 22:04:18,591] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 7000, global step 112000: learning rate 0.0001
[2019-04-03 22:04:18,644] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 112022: loss 0.1748
[2019-04-03 22:04:18,645] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 112022: learning rate 0.0001
[2019-04-03 22:04:18,820] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 112081: loss 0.2079
[2019-04-03 22:04:18,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 112082: learning rate 0.0001
[2019-04-03 22:04:19,298] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 112264: loss 0.2649
[2019-04-03 22:04:19,299] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 112264: learning rate 0.0001
[2019-04-03 22:04:19,566] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112353: loss 0.2727
[2019-04-03 22:04:19,567] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112353: learning rate 0.0001
[2019-04-03 22:04:19,776] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 112433: loss 0.2669
[2019-04-03 22:04:19,777] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 112433: learning rate 0.0001
[2019-04-03 22:04:19,810] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 112445: loss 0.2001
[2019-04-03 22:04:19,812] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 112445: learning rate 0.0001
[2019-04-03 22:04:21,486] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 113024: loss 0.2432
[2019-04-03 22:04:21,490] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 113025: learning rate 0.0001
[2019-04-03 22:04:21,861] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 113186: loss 0.2516
[2019-04-03 22:04:21,862] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 113186: learning rate 0.0001
[2019-04-03 22:04:22,233] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.12926815e-08 3.43498634e-03 5.01928810e-08 2.48486921e-03
 1.42822694e-02 5.45717391e-08 9.79797721e-01], sum to 1.0000
[2019-04-03 22:04:22,235] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6357
[2019-04-03 22:04:22,428] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.35876651397377, 0.3201582309266646, 0.0, 1.0, 69163.77139398515], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4249800.0000, 
sim time next is 4250400.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.32355858071357, 0.3204419322102182, 0.0, 1.0, 53152.20337123], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6102965483927975, 0.606813977403406, 0.0, 1.0, 0.2531057303391905], 
reward next is 0.7469, 
noisyNet noise sample is [array([-0.9119312], dtype=float32), 0.8026278]. 
=============================================
[2019-04-03 22:04:22,708] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 113507: loss 0.1858
[2019-04-03 22:04:22,710] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 113508: learning rate 0.0001
[2019-04-03 22:04:25,487] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 114440: loss 0.3353
[2019-04-03 22:04:25,491] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 114441: learning rate 0.0001
[2019-04-03 22:04:26,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2752954e-11 1.9732408e-06 9.0625445e-11 7.5936339e-05 1.2186060e-04
 3.7685608e-11 9.9980026e-01], sum to 1.0000
[2019-04-03 22:04:26,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0386
[2019-04-03 22:04:26,016] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.0, 36.0, 49.66666666666666, 0.0, 26.0, 27.90900468853622, 1.052158556302801, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4378800.0000, 
sim time next is 4379400.0000, 
raw observation next is [13.0, 36.5, 39.0, 0.0, 26.0, 28.25499075099443, 1.07830264428852, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.365, 0.13, 0.0, 0.6666666666666666, 0.8545825625828692, 0.8594342147628401, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2289296], dtype=float32), 0.110360585]. 
=============================================
[2019-04-03 22:04:28,658] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4599861e-11 1.5284761e-05 1.3952485e-10 3.4056080e-05 6.3932472e-05
 1.7643381e-10 9.9988675e-01], sum to 1.0000
[2019-04-03 22:04:28,658] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8687
[2019-04-03 22:04:28,693] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.466666666666667, 75.66666666666667, 0.0, 0.0, 26.0, 25.51303020706951, 0.4002701503412173, 0.0, 1.0, 58520.7963269426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4318800.0000, 
sim time next is 4319400.0000, 
raw observation next is [4.483333333333333, 75.83333333333333, 0.0, 0.0, 26.0, 25.47313929112457, 0.4013011820039227, 0.0, 1.0, 66222.36309274769], 
processed observation next is [0.0, 1.0, 0.5867959372114497, 0.7583333333333333, 0.0, 0.0, 0.6666666666666666, 0.6227616075937142, 0.6337670606679743, 0.0, 1.0, 0.3153445861559414], 
reward next is 0.6847, 
noisyNet noise sample is [array([-1.0706701], dtype=float32), -0.39474976]. 
=============================================
[2019-04-03 22:04:28,996] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4667111e-12 2.3637608e-06 1.4671547e-10 1.8348317e-05 6.0498740e-05
 3.3329006e-11 9.9991870e-01], sum to 1.0000
[2019-04-03 22:04:28,999] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6259
[2019-04-03 22:04:29,015] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.65, 61.83333333333334, 0.0, 0.0, 26.0, 25.94296269554719, 0.6199476950015744, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4402200.0000, 
sim time next is 4402800.0000, 
raw observation next is [8.5, 62.0, 0.0, 0.0, 26.0, 25.85675504078742, 0.6024882910131673, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.698060941828255, 0.62, 0.0, 0.0, 0.6666666666666666, 0.654729586732285, 0.7008294303377225, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33980876], dtype=float32), -1.8605609]. 
=============================================
[2019-04-03 22:04:30,579] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.7162170e-12 1.8593720e-06 7.3806707e-12 3.3928627e-05 2.9349203e-05
 4.8528048e-12 9.9993491e-01], sum to 1.0000
[2019-04-03 22:04:30,579] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0301
[2019-04-03 22:04:30,608] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 26.0, 26.65916462575019, 0.8709173970385912, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4366200.0000, 
sim time next is 4366800.0000, 
raw observation next is [14.6, 29.0, 116.5, 847.5, 26.0, 27.12831822885573, 0.920304773554478, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8670360110803325, 0.29, 0.3883333333333333, 0.93646408839779, 0.6666666666666666, 0.7606931857379774, 0.8067682578514926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.347751], dtype=float32), -0.89278024]. 
=============================================
[2019-04-03 22:04:35,163] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7500, global step 117880: loss 0.7430
[2019-04-03 22:04:35,163] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7500, global step 117880: learning rate 0.0001
[2019-04-03 22:04:35,251] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 117913: loss 0.7142
[2019-04-03 22:04:35,252] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 117913: learning rate 0.0001
[2019-04-03 22:04:35,765] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7500, global step 118099: loss 0.7954
[2019-04-03 22:04:35,765] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7500, global step 118099: learning rate 0.0001
[2019-04-03 22:04:36,547] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.0449891e-11 1.9521673e-05 5.1944099e-10 4.0224080e-05 1.0303159e-04
 2.2718724e-10 9.9983728e-01], sum to 1.0000
[2019-04-03 22:04:36,549] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6512
[2019-04-03 22:04:36,566] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.38996922372194, 0.4341119730864333, 0.0, 1.0, 112798.12922161], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4498200.0000, 
sim time next is 4498800.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.25928496614999, 0.4387687134486429, 0.0, 1.0, 87011.05573950936], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6049404138458326, 0.6462562378162143, 0.0, 1.0, 0.41433836066433033], 
reward next is 0.5857, 
noisyNet noise sample is [array([-1.7069921], dtype=float32), 0.00088367204]. 
=============================================
[2019-04-03 22:04:36,635] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 118422: loss 0.7178
[2019-04-03 22:04:36,660] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 118423: learning rate 0.0001
[2019-04-03 22:04:40,038] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2964802e-10 1.4234200e-04 1.4169276e-09 1.0932533e-04 4.0608441e-04
 2.1504991e-10 9.9934214e-01], sum to 1.0000
[2019-04-03 22:04:40,041] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7050
[2019-04-03 22:04:40,057] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.883333333333333, 80.66666666666667, 80.00000000000001, 154.6666666666667, 26.0, 25.48956545495795, 0.493740319267407, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4435800.0000, 
sim time next is 4436400.0000, 
raw observation next is [1.766666666666667, 81.33333333333334, 100.0, 193.3333333333333, 26.0, 25.49180058434925, 0.5191045215273543, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5115420129270545, 0.8133333333333335, 0.3333333333333333, 0.21362799263351745, 0.6666666666666666, 0.6243167153624375, 0.6730348405091181, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.79792976], dtype=float32), -1.6092347]. 
=============================================
[2019-04-03 22:04:41,049] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119789: loss 1.0064
[2019-04-03 22:04:41,050] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119789: learning rate 0.0001
[2019-04-03 22:04:41,642] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119944: loss 0.9961
[2019-04-03 22:04:41,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119944: learning rate 0.0001
[2019-04-03 22:04:41,704] A3C_AGENT_WORKER-Thread-8 INFO:Local step 7500, global step 119962: loss 8.5760
[2019-04-03 22:04:41,705] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 7500, global step 119962: learning rate 0.0001
[2019-04-03 22:04:42,061] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 120062: loss 0.9910
[2019-04-03 22:04:42,062] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 120062: learning rate 0.0001
[2019-04-03 22:04:43,048] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 120409: loss 1.0008
[2019-04-03 22:04:43,050] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 120410: learning rate 0.0001
[2019-04-03 22:04:43,373] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120530: loss 0.9442
[2019-04-03 22:04:43,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120530: learning rate 0.0001
[2019-04-03 22:04:43,418] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 120544: loss 0.9398
[2019-04-03 22:04:43,420] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 120545: learning rate 0.0001
[2019-04-03 22:04:43,667] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 120634: loss 0.8934
[2019-04-03 22:04:43,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 120634: learning rate 0.0001
[2019-04-03 22:04:44,783] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4672872e-09 3.9327452e-03 2.8045601e-08 3.6439912e-03 1.8874423e-03
 1.6733607e-08 9.9053580e-01], sum to 1.0000
[2019-04-03 22:04:44,783] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9855
[2019-04-03 22:04:44,834] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 71.0, 0.0, 0.0, 26.0, 25.29154848797237, 0.4083542448904403, 0.0, 1.0, 41284.71107346577], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4512000.0000, 
sim time next is 4512600.0000, 
raw observation next is [-0.9, 71.0, 0.0, 0.0, 26.0, 25.2772795845034, 0.4033232025024243, 0.0, 1.0, 41206.78219669916], 
processed observation next is [1.0, 0.21739130434782608, 0.43767313019390586, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6064399653752833, 0.6344410675008081, 0.0, 1.0, 0.1962227723652341], 
reward next is 0.8038, 
noisyNet noise sample is [array([0.81364566], dtype=float32), -1.0166075]. 
=============================================
[2019-04-03 22:04:45,215] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 121043: loss 0.8204
[2019-04-03 22:04:45,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 121043: learning rate 0.0001
[2019-04-03 22:04:45,729] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 121182: loss 0.8287
[2019-04-03 22:04:45,731] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 121182: learning rate 0.0001
[2019-04-03 22:04:45,954] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 121246: loss 0.8204
[2019-04-03 22:04:45,954] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 121246: learning rate 0.0001
[2019-04-03 22:04:48,876] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 122153: loss 0.7976
[2019-04-03 22:04:48,912] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 122155: learning rate 0.0001
[2019-04-03 22:04:55,652] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.8002431e-10 2.9072718e-05 4.7799826e-09 9.6699636e-04 9.9533144e-04
 2.6706892e-10 9.9800867e-01], sum to 1.0000
[2019-04-03 22:04:55,665] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1422
[2019-04-03 22:04:55,699] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.49910751142904, 0.548817811653853, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4652400.0000, 
sim time next is 4653000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.44844059390521, 0.5692014818800463, 0.0, 1.0, 197390.8288262744], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6207033828254342, 0.6897338272933488, 0.0, 1.0, 0.9399563277441638], 
reward next is 0.0600, 
noisyNet noise sample is [array([0.86564523], dtype=float32), 1.3851882]. 
=============================================
[2019-04-03 22:04:55,724] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[80.21022 ]
 [79.52601 ]
 [79.85331 ]
 [79.97648 ]
 [80.228836]], R is [[80.27658844]
 [79.5394516 ]
 [79.62099457]
 [79.62262726]
 [79.82640076]].
[2019-04-03 22:04:58,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4485207e-10 6.8831821e-03 1.9123521e-08 7.0328201e-04 1.6943052e-04
 2.5301217e-09 9.9224418e-01], sum to 1.0000
[2019-04-03 22:04:58,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9050
[2019-04-03 22:04:58,547] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.58308593968467, 0.418833612933996, 0.0, 1.0, 23949.38070098848], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4683600.0000, 
sim time next is 4684200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.42786908590456, 0.4097087157165821, 0.0, 1.0, 108328.3381161755], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6189890904920468, 0.6365695719055274, 0.0, 1.0, 0.5158492291246453], 
reward next is 0.4842, 
noisyNet noise sample is [array([0.45363247], dtype=float32), 0.7493146]. 
=============================================
[2019-04-03 22:05:00,966] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8000, global step 126010: loss 0.0310
[2019-04-03 22:05:00,988] A3C_AGENT_WORKER-Thread-7 INFO:Local step 8000, global step 126018: loss 0.0362
[2019-04-03 22:05:00,989] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8000, global step 126010: learning rate 0.0001
[2019-04-03 22:05:00,991] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 8000, global step 126018: learning rate 0.0001
[2019-04-03 22:05:01,692] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 126181: loss 0.0356
[2019-04-03 22:05:01,693] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 126181: learning rate 0.0001
[2019-04-03 22:05:04,781] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 126703: loss 0.0220
[2019-04-03 22:05:04,785] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 126703: learning rate 0.0001
[2019-04-03 22:05:13,782] A3C_AGENT_WORKER-Thread-8 INFO:Local step 8000, global step 127838: loss 0.0016
[2019-04-03 22:05:13,783] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 8000, global step 127838: learning rate 0.0001
[2019-04-03 22:05:14,147] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127884: loss 0.0016
[2019-04-03 22:05:14,174] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127884: learning rate 0.0001
[2019-04-03 22:05:14,975] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128006: loss 0.0018
[2019-04-03 22:05:14,981] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128006: learning rate 0.0001
[2019-04-03 22:05:15,241] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 128048: loss 0.0015
[2019-04-03 22:05:15,242] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 128048: learning rate 0.0001
[2019-04-03 22:05:17,001] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 128366: loss 0.0017
[2019-04-03 22:05:17,004] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 128366: learning rate 0.0001
[2019-04-03 22:05:17,832] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 128557: loss 0.0014
[2019-04-03 22:05:17,832] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 128557: learning rate 0.0001
[2019-04-03 22:05:18,378] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 128668: loss 0.0019
[2019-04-03 22:05:18,381] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 128668: learning rate 0.0001
[2019-04-03 22:05:18,866] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 128771: loss 0.0019
[2019-04-03 22:05:18,869] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 128771: learning rate 0.0001
[2019-04-03 22:05:20,573] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 129106: loss 0.0022
[2019-04-03 22:05:20,573] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 129106: learning rate 0.0001
[2019-04-03 22:05:21,441] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 129265: loss 0.0007
[2019-04-03 22:05:21,456] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 129265: learning rate 0.0001
[2019-04-03 22:05:22,009] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 129369: loss 0.0019
[2019-04-03 22:05:22,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 129369: learning rate 0.0001
[2019-04-03 22:05:24,728] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 129913: loss 0.0048
[2019-04-03 22:05:24,732] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 129913: learning rate 0.0001
[2019-04-03 22:05:40,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.5514653e-09 1.2119223e-03 1.5983098e-08 2.0022348e-03 1.1396634e-02
 5.6792331e-09 9.8538929e-01], sum to 1.0000
[2019-04-03 22:05:40,101] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2503
[2019-04-03 22:05:40,116] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.02468451019073, 0.2279469764963744, 0.0, 1.0, 38655.10529833408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4948200.0000, 
sim time next is 4948800.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.03052050303135, 0.2197574854590133, 0.0, 1.0, 38671.57250173028], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5858767085859459, 0.5732524951530045, 0.0, 1.0, 0.18415034524633467], 
reward next is 0.8158, 
noisyNet noise sample is [array([-1.8411827], dtype=float32), 0.34612533]. 
=============================================
[2019-04-03 22:05:40,271] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0397172e-09 2.7967992e-05 2.3586066e-09 5.9789536e-04 2.3094367e-03
 1.0432288e-10 9.9706465e-01], sum to 1.0000
[2019-04-03 22:05:40,271] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6243
[2019-04-03 22:05:40,483] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.333333333333333, 24.66666666666667, 122.8333333333333, 861.6666666666667, 26.0, 27.12834273913565, 0.726567398272214, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4969200.0000, 
sim time next is 4969800.0000, 
raw observation next is [6.5, 24.5, 123.0, 865.0, 26.0, 27.08696216024669, 0.7268544882331619, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6426592797783934, 0.245, 0.41, 0.9558011049723757, 0.6666666666666666, 0.7572468466872241, 0.7422848294110539, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9113987], dtype=float32), 0.2679757]. 
=============================================
[2019-04-03 22:05:41,132] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6553606e-09 3.1270392e-04 2.1879469e-09 7.8092478e-02 1.1984573e-03
 6.0120531e-10 9.2039633e-01], sum to 1.0000
[2019-04-03 22:05:41,132] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1559
[2019-04-03 22:05:41,138] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 19.0, 86.0, 665.0, 26.0, 28.56673825462448, 1.135077813530631, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5068800.0000, 
sim time next is 5069400.0000, 
raw observation next is [12.0, 18.66666666666667, 82.66666666666666, 638.3333333333334, 26.0, 28.78468074891978, 1.165242524789347, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.1866666666666667, 0.2755555555555555, 0.705340699815838, 0.6666666666666666, 0.898723395743315, 0.8884141749297824, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.471105], dtype=float32), -0.1498639]. 
=============================================
[2019-04-03 22:05:41,307] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0859120e-10 7.4193895e-06 7.4614231e-10 4.2888485e-03 1.1677060e-03
 9.1089393e-11 9.9453604e-01], sum to 1.0000
[2019-04-03 22:05:41,308] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9308
[2019-04-03 22:05:41,320] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.666666666666667, 25.83333333333334, 46.66666666666666, 416.3333333333333, 26.0, 27.30312593321145, 0.8571267513156414, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4986600.0000, 
sim time next is 4987200.0000, 
raw observation next is [7.333333333333334, 25.66666666666667, 40.33333333333333, 360.1666666666666, 26.0, 27.5515456986638, 0.8367428712373237, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6657433056325024, 0.2566666666666667, 0.13444444444444442, 0.3979742173112338, 0.6666666666666666, 0.7959621415553167, 0.7789142904124412, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2085891], dtype=float32), 1.0592998]. 
=============================================
[2019-04-03 22:05:42,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:42,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:42,643] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run2
[2019-04-03 22:05:46,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:46,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:46,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run2
[2019-04-03 22:05:46,126] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:46,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:46,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run2
[2019-04-03 22:05:47,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:47,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:47,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run2
[2019-04-03 22:05:53,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:53,850] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:53,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run2
[2019-04-03 22:05:54,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:54,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:54,731] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run2
[2019-04-03 22:05:55,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:55,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:55,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run2
[2019-04-03 22:05:56,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:56,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:56,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run2
[2019-04-03 22:05:58,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:58,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:58,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run2
[2019-04-03 22:05:59,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:59,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:59,362] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run2
[2019-04-03 22:05:59,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:05:59,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:05:59,487] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run2
[2019-04-03 22:06:00,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:00,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:00,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run2
[2019-04-03 22:06:00,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:00,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:00,435] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run2
[2019-04-03 22:06:00,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:00,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:00,971] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run2
[2019-04-03 22:06:01,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:01,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:01,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run2
[2019-04-03 22:06:02,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:06:02,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:06:02,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run2
[2019-04-03 22:06:05,463] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5330771e-10 1.5012801e-04 1.7585376e-09 2.6068179e-04 5.6558521e-04
 2.1189894e-09 9.9902356e-01], sum to 1.0000
[2019-04-03 22:06:05,463] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0986
[2019-04-03 22:06:05,508] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.19215370389179, -0.5799949085836608, 0.0, 1.0, 40400.39719985658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 23400.0000, 
sim time next is 24000.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 26.0, 21.21316093290682, -0.574606449456398, 0.0, 1.0, 40382.21349976964], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.2677634110755684, 0.3084645168478673, 0.0, 1.0, 0.1922962547608078], 
reward next is 0.8077, 
noisyNet noise sample is [array([-1.5213186], dtype=float32), -1.113887]. 
=============================================
[2019-04-03 22:06:05,512] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[90.36793 ]
 [90.31767 ]
 [90.246666]
 [90.17342 ]
 [90.089   ]], R is [[90.33408356]
 [90.23835754]
 [90.14347839]
 [90.04940796]
 [89.95612335]].
[2019-04-03 22:06:18,743] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.2451041e-10 1.2153180e-05 1.0944258e-08 5.1477872e-04 1.1985715e-03
 9.2019847e-10 9.9827445e-01], sum to 1.0000
[2019-04-03 22:06:18,744] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3839
[2019-04-03 22:06:18,766] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.8743729121492, 0.04050269585148047, 0.0, 1.0, 44645.25830883632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 172200.0000, 
sim time next is 172800.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.8308937355709, 0.03032683161558022, 0.0, 1.0, 44590.278034094], 
processed observation next is [1.0, 0.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.485907811297575, 0.5101089438718601, 0.0, 1.0, 0.21233465730520953], 
reward next is 0.7877, 
noisyNet noise sample is [array([-0.80784154], dtype=float32), 0.5587367]. 
=============================================
[2019-04-03 22:06:29,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5361504e-10 2.7127002e-04 1.9783608e-08 1.7695590e-03 1.1195957e-03
 1.7674067e-09 9.9683952e-01], sum to 1.0000
[2019-04-03 22:06:29,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6840
[2019-04-03 22:06:29,833] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.733333333333333, 74.83333333333334, 0.0, 0.0, 26.0, 23.78729652788273, 0.01255251800237771, 0.0, 1.0, 43983.43983548123], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 103800.0000, 
sim time next is 104400.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 23.72958116474457, -0.001227074442508682, 0.0, 1.0, 44091.23331460002], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.4774650970620475, 0.49959097518583045, 0.0, 1.0, 0.2099582538790477], 
reward next is 0.7900, 
noisyNet noise sample is [array([-1.2254412], dtype=float32), -2.0170748]. 
=============================================
[2019-04-03 22:06:43,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2071216e-10 3.6748081e-06 1.1772985e-09 6.0590351e-04 9.9813893e-05
 8.3840358e-11 9.9929059e-01], sum to 1.0000
[2019-04-03 22:06:43,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4856
[2019-04-03 22:06:43,659] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 42.0, 74.0, 525.6666666666667, 26.0, 25.72602777588968, 0.4947124064919063, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 313800.0000, 
sim time next is 314400.0000, 
raw observation next is [-9.5, 42.0, 72.0, 501.3333333333333, 26.0, 26.07768774578741, 0.5181739161530378, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.24, 0.5539594843462247, 0.6666666666666666, 0.6731406454822843, 0.6727246387176793, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4906169], dtype=float32), 1.9070853]. 
=============================================
[2019-04-03 22:06:43,832] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7750471e-09 1.9052160e-05 4.5355524e-09 1.1055197e-03 5.1053305e-04
 3.2395450e-10 9.9836487e-01], sum to 1.0000
[2019-04-03 22:06:43,832] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7656
[2019-04-03 22:06:43,921] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 65.0, 132.3333333333333, 0.0, 26.0, 25.12181138877006, 0.2229636088120984, 1.0, 1.0, 70751.3904542931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 218400.0000, 
sim time next is 219000.0000, 
raw observation next is [-4.583333333333333, 65.0, 135.6666666666667, 0.0, 26.0, 25.15001017059532, 0.2290969070260579, 1.0, 1.0, 37347.39367718322], 
processed observation next is [1.0, 0.5217391304347826, 0.3356417359187443, 0.65, 0.45222222222222236, 0.0, 0.6666666666666666, 0.5958341808829433, 0.5763656356753527, 1.0, 1.0, 0.1778447317961106], 
reward next is 0.8222, 
noisyNet noise sample is [array([-0.9226406], dtype=float32), -0.22709227]. 
=============================================
[2019-04-03 22:06:43,927] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.61644]
 [76.57017]
 [76.73722]
 [76.92519]
 [76.95478]], R is [[76.54111481]
 [76.43878937]
 [76.46391296]
 [76.57187653]
 [76.55177307]].
[2019-04-03 22:06:53,033] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4598413e-10 3.8969167e-05 2.6722415e-09 7.3896757e-05 1.0578865e-04
 6.2174477e-10 9.9978131e-01], sum to 1.0000
[2019-04-03 22:06:53,033] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3094
[2019-04-03 22:06:53,253] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.8, 70.0, 15.0, 205.5, 26.0, 24.41514897886819, 0.1185951097904449, 1.0, 1.0, 93587.46375274497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 288000.0000, 
sim time next is 288600.0000, 
raw observation next is [-12.71666666666667, 69.5, 20.0, 265.6666666666667, 26.0, 24.66466479957528, 0.1783145451922401, 1.0, 1.0, 86633.082336185], 
processed observation next is [1.0, 0.34782608695652173, 0.1103416435826407, 0.695, 0.06666666666666667, 0.29355432780847146, 0.6666666666666666, 0.55538873329794, 0.5594381817307467, 1.0, 1.0, 0.4125384873151667], 
reward next is 0.5875, 
noisyNet noise sample is [array([0.9848882], dtype=float32), -0.09135237]. 
=============================================
[2019-04-03 22:06:57,380] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.8768005e-09 1.8826621e-03 2.8250420e-07 3.8080614e-03 2.4974982e-03
 1.7452905e-08 9.9181139e-01], sum to 1.0000
[2019-04-03 22:06:57,380] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9191
[2019-04-03 22:06:57,397] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 51.0, 0.0, 0.0, 26.0, 23.02708203501658, -0.210894639354243, 0.0, 1.0, 46193.30568350979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 445200.0000, 
sim time next is 445800.0000, 
raw observation next is [-11.1, 51.5, 0.0, 0.0, 26.0, 22.97454222382104, -0.224726834044513, 0.0, 1.0, 46294.59259444171], 
processed observation next is [1.0, 0.13043478260869565, 0.1551246537396122, 0.515, 0.0, 0.0, 0.6666666666666666, 0.41454518531841994, 0.42509105531849567, 0.0, 1.0, 0.2204504409259129], 
reward next is 0.7795, 
noisyNet noise sample is [array([-0.38780233], dtype=float32), 0.94630355]. 
=============================================
[2019-04-03 22:07:03,838] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5506220e-11 8.7364951e-06 2.5989627e-10 7.3818475e-05 1.9193118e-05
 4.6236327e-11 9.9989820e-01], sum to 1.0000
[2019-04-03 22:07:03,840] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9375
[2019-04-03 22:07:03,857] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.1, 79.5, 0.0, 0.0, 26.0, 24.43409989114882, 0.1530192062437922, 0.0, 1.0, 47550.24547310801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 336600.0000, 
sim time next is 337200.0000, 
raw observation next is [-13.2, 80.33333333333333, 0.0, 0.0, 26.0, 24.34969245520572, 0.1392303310622896, 0.0, 1.0, 47560.43384245523], 
processed observation next is [1.0, 0.9130434782608695, 0.09695290858725764, 0.8033333333333332, 0.0, 0.0, 0.6666666666666666, 0.52914103793381, 0.5464101103540965, 0.0, 1.0, 0.22647825639264396], 
reward next is 0.7735, 
noisyNet noise sample is [array([-0.46240026], dtype=float32), -0.94263816]. 
=============================================
[2019-04-03 22:07:04,108] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0728021e-10 4.4452272e-06 1.5715631e-09 3.0435031e-05 1.3158753e-04
 1.4343095e-10 9.9983346e-01], sum to 1.0000
[2019-04-03 22:07:04,108] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8320
[2019-04-03 22:07:04,218] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.9, 59.00000000000001, 0.0, 0.0, 26.0, 25.70411482377166, 0.3761615058152095, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 325200.0000, 
sim time next is 325800.0000, 
raw observation next is [-12.0, 60.0, 0.0, 0.0, 26.0, 25.54318090564127, 0.3521450703563758, 1.0, 1.0, 104861.268217479], 
processed observation next is [1.0, 0.782608695652174, 0.13019390581717452, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6285984088034393, 0.617381690118792, 1.0, 1.0, 0.4993393724641857], 
reward next is 0.5007, 
noisyNet noise sample is [array([-0.56465966], dtype=float32), 1.003047]. 
=============================================
[2019-04-03 22:07:05,755] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2215101e-10 1.4209466e-04 3.9327066e-09 1.7488179e-04 3.7966695e-04
 4.1330586e-10 9.9930334e-01], sum to 1.0000
[2019-04-03 22:07:05,755] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8153
[2019-04-03 22:07:06,036] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.9, 68.0, 0.0, 0.0, 26.0, 23.72635098491163, -0.009128637589630348, 0.0, 1.0, 47184.94092371457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 343800.0000, 
sim time next is 344400.0000, 
raw observation next is [-13.9, 67.33333333333334, 0.0, 0.0, 26.0, 23.65919300768117, -0.01447291577866559, 0.0, 1.0, 47224.31904413381], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.4715994173067643, 0.4951756947404448, 0.0, 1.0, 0.2248777097339705], 
reward next is 0.7751, 
noisyNet noise sample is [array([-0.69134027], dtype=float32), -0.7748571]. 
=============================================
[2019-04-03 22:07:12,772] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3210317e-09 7.6671444e-02 3.1323790e-08 1.8446088e-03 1.3728170e-02
 9.6346104e-09 9.0775573e-01], sum to 1.0000
[2019-04-03 22:07:12,772] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8816
[2019-04-03 22:07:12,894] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 87.66666666666666, 121.6666666666667, 115.6666666666667, 26.0, 24.83131676880189, 0.2522061832559656, 0.0, 1.0, 63679.6801270498], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 553800.0000, 
sim time next is 554400.0000, 
raw observation next is [-0.6, 87.0, 110.5, 122.0, 26.0, 24.80459707971662, 0.2552938985192064, 0.0, 1.0, 70276.23744532927], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.87, 0.36833333333333335, 0.13480662983425415, 0.6666666666666666, 0.5670497566430516, 0.5850979661730688, 0.0, 1.0, 0.3346487497396632], 
reward next is 0.6654, 
noisyNet noise sample is [array([0.8770781], dtype=float32), -1.2753023]. 
=============================================
[2019-04-03 22:07:15,421] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0048126e-09 2.9971442e-04 3.6816910e-08 1.8336367e-03 1.5894551e-03
 3.6311765e-09 9.9627715e-01], sum to 1.0000
[2019-04-03 22:07:15,421] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6736
[2019-04-03 22:07:15,477] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.266666666666667, 37.66666666666667, 19.16666666666667, 0.0, 26.0, 24.83988940285458, 0.1410473673525513, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 462000.0000, 
sim time next is 462600.0000, 
raw observation next is [-7.0, 36.5, 23.0, 0.0, 26.0, 25.15169825162273, 0.1718135281529992, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2686980609418283, 0.365, 0.07666666666666666, 0.0, 0.6666666666666666, 0.5959748543018941, 0.5572711760509997, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02923788], dtype=float32), 0.7328665]. 
=============================================
[2019-04-03 22:07:24,181] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7170657e-12 5.3039275e-05 3.4456499e-10 1.0921927e-05 1.7837925e-04
 8.4206028e-12 9.9975759e-01], sum to 1.0000
[2019-04-03 22:07:24,181] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1936
[2019-04-03 22:07:24,241] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.55, 96.5, 0.0, 0.0, 26.0, 24.83403332451908, 0.2344556379286678, 0.0, 1.0, 40012.19174868377], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 516600.0000, 
sim time next is 517200.0000, 
raw observation next is [3.633333333333333, 96.66666666666666, 0.0, 0.0, 26.0, 24.84276599367649, 0.2405629281880052, 0.0, 1.0, 39923.88670582083], 
processed observation next is [1.0, 1.0, 0.5632502308402586, 0.9666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5702304994730408, 0.5801876427293351, 0.0, 1.0, 0.1901137462181944], 
reward next is 0.8099, 
noisyNet noise sample is [array([-1.6833549], dtype=float32), 0.7412499]. 
=============================================
[2019-04-03 22:07:31,214] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8139580e-10 3.6143744e-04 7.7194706e-10 2.9115263e-05 5.0529261e-04
 3.7325931e-10 9.9910408e-01], sum to 1.0000
[2019-04-03 22:07:31,234] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8068
[2019-04-03 22:07:31,290] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 26.0, 24.96144397079855, 0.3156118443923779, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 562800.0000, 
sim time next is 563400.0000, 
raw observation next is [-1.0, 80.5, 130.0, 396.0, 26.0, 24.95658355604128, 0.3144380456576992, 0.0, 1.0, 18731.33440420001], 
processed observation next is [0.0, 0.5217391304347826, 0.4349030470914128, 0.805, 0.43333333333333335, 0.4375690607734807, 0.6666666666666666, 0.5797152963367734, 0.6048126818858998, 0.0, 1.0, 0.08919683049619052], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.0448757], dtype=float32), 0.25044677]. 
=============================================
[2019-04-03 22:07:48,958] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6260524e-09 1.3792336e-02 2.1688930e-08 1.7857691e-03 1.3383118e-02
 8.0606402e-09 9.7103882e-01], sum to 1.0000
[2019-04-03 22:07:48,961] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6085
[2019-04-03 22:07:48,974] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3, 72.0, 0.0, 0.0, 26.0, 24.66851019148494, 0.172662614480049, 0.0, 1.0, 38923.86877160888], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 883800.0000, 
sim time next is 884400.0000, 
raw observation next is [-0.2, 72.0, 0.0, 0.0, 26.0, 24.6131477106589, 0.1619039558291192, 0.0, 1.0, 38908.32262860522], 
processed observation next is [1.0, 0.21739130434782608, 0.4570637119113574, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5510956425549084, 0.553967985276373, 0.0, 1.0, 0.18527772680288201], 
reward next is 0.8147, 
noisyNet noise sample is [array([-0.28650647], dtype=float32), -0.12798937]. 
=============================================
[2019-04-03 22:07:51,263] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2569001e-10 2.2838417e-04 3.4314336e-09 4.8827223e-04 5.7832412e-03
 6.1294164e-10 9.9350005e-01], sum to 1.0000
[2019-04-03 22:07:51,263] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6651
[2019-04-03 22:07:51,311] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.04999999999999999, 53.5, 131.0, 449.0, 26.0, 25.74934088126603, 0.3610844601765453, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 736200.0000, 
sim time next is 736800.0000, 
raw observation next is [0.1333333333333333, 52.33333333333333, 124.0, 503.0, 26.0, 25.6592661048353, 0.3576568126056918, 1.0, 1.0, 35331.13486205231], 
processed observation next is [1.0, 0.5217391304347826, 0.46629732225300097, 0.5233333333333333, 0.41333333333333333, 0.5558011049723757, 0.6666666666666666, 0.6382721754029417, 0.6192189375352306, 1.0, 1.0, 0.16824349934310626], 
reward next is 0.8318, 
noisyNet noise sample is [array([-0.01450555], dtype=float32), -1.4044968]. 
=============================================
[2019-04-03 22:07:53,045] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6463684e-11 1.5703797e-04 3.1154657e-10 2.6572042e-04 1.1717338e-03
 1.6021596e-11 9.9840552e-01], sum to 1.0000
[2019-04-03 22:07:53,045] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0151
[2019-04-03 22:07:53,162] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.916666666666667, 74.33333333333333, 78.33333333333334, 0.0, 26.0, 25.70373395535353, 0.2910413259584538, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 814200.0000, 
sim time next is 814800.0000, 
raw observation next is [-5.633333333333334, 73.66666666666667, 82.66666666666666, 0.0, 26.0, 25.65312982208356, 0.2894040307400862, 1.0, 1.0, 45500.16678389372], 
processed observation next is [1.0, 0.43478260869565216, 0.30655586334256696, 0.7366666666666667, 0.2755555555555555, 0.0, 0.6666666666666666, 0.6377608185069633, 0.5964680102466954, 1.0, 1.0, 0.2166674608756844], 
reward next is 0.7833, 
noisyNet noise sample is [array([0.3523495], dtype=float32), -0.39171782]. 
=============================================
[2019-04-03 22:07:57,254] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2694794e-10 7.2037987e-04 3.9673163e-09 5.0585024e-04 1.8735114e-03
 5.6175242e-09 9.9690032e-01], sum to 1.0000
[2019-04-03 22:07:57,255] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7092
[2019-04-03 22:07:57,308] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 73.66666666666666, 10.66666666666666, 0.0, 26.0, 24.79801380084196, 0.2403767352183723, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 805800.0000, 
sim time next is 806400.0000, 
raw observation next is [-6.7, 75.0, 16.0, 0.0, 26.0, 25.22101978421441, 0.2608986173580957, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2770083102493075, 0.75, 0.05333333333333334, 0.0, 0.6666666666666666, 0.6017516486845341, 0.586966205786032, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36057198], dtype=float32), -0.24072042]. 
=============================================
[2019-04-03 22:07:59,981] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.6857190e-10 7.3983811e-04 1.0421787e-08 3.4907574e-04 3.5938729e-02
 1.4118271e-09 9.6297234e-01], sum to 1.0000
[2019-04-03 22:07:59,983] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2725
[2019-04-03 22:08:00,005] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.5601401555592, 0.1637301616529894, 0.0, 1.0, 38441.52376961637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 889200.0000, 
sim time next is 889800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.58770374426504, 0.1614066843115627, 0.0, 1.0, 38423.39655862674], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5489753120220865, 0.5538022281038543, 0.0, 1.0, 0.1829685550410797], 
reward next is 0.8170, 
noisyNet noise sample is [array([-0.06094564], dtype=float32), -0.3049925]. 
=============================================
[2019-04-03 22:08:07,146] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0276211e-13 1.0351043e-05 5.6752784e-12 1.2568531e-05 1.9316249e-05
 2.6869636e-13 9.9995780e-01], sum to 1.0000
[2019-04-03 22:08:07,147] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6107
[2019-04-03 22:08:07,164] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.11666666666667, 81.83333333333333, 110.6666666666667, 0.0, 26.0, 26.54769370372154, 0.6813221899892926, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1000200.0000, 
sim time next is 1000800.0000, 
raw observation next is [14.4, 81.0, 106.5, 0.0, 26.0, 26.61576579237198, 0.6912607833356632, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.355, 0.0, 0.6666666666666666, 0.7179804826976651, 0.7304202611118877, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44699907], dtype=float32), -0.768994]. 
=============================================
[2019-04-03 22:08:09,281] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.2893216e-13 6.2314110e-05 1.5811314e-11 4.3246993e-05 1.3352429e-04
 3.4528417e-12 9.9976093e-01], sum to 1.0000
[2019-04-03 22:08:09,285] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7610
[2019-04-03 22:08:09,291] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 25.77504638112531, 0.5307345863828744, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1017000.0000, 
sim time next is 1017600.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.65847252482949, 0.5225062654167406, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6382060437357909, 0.6741687551389135, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24942584], dtype=float32), 0.65495354]. 
=============================================
[2019-04-03 22:08:22,719] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.8070841e-08 3.5397816e-02 2.8095852e-07 4.0232902e-03 9.7975694e-02
 1.2348441e-07 8.6260271e-01], sum to 1.0000
[2019-04-03 22:08:22,726] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1730
[2019-04-03 22:08:22,732] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.48267869540337, 0.1451954390579796, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1234800.0000, 
sim time next is 1235400.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.46230294286677, 0.1450785873792985, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.45519191190556424, 0.5483595291264328, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39010862], dtype=float32), 2.083147]. 
=============================================
[2019-04-03 22:08:34,436] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5646727e-11 2.5149199e-04 1.2228070e-09 3.4284731e-05 1.7106438e-04
 3.0760822e-10 9.9954319e-01], sum to 1.0000
[2019-04-03 22:08:34,438] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1339
[2019-04-03 22:08:34,477] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.31727606373381, 0.4543638821029813, 0.0, 1.0, 47911.65532473931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1483800.0000, 
sim time next is 1484400.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.34890072115504, 0.454246484321513, 0.0, 1.0, 40834.03964725122], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6124083934295866, 0.6514154947738376, 0.0, 1.0, 0.19444780784405344], 
reward next is 0.8056, 
noisyNet noise sample is [array([2.4556408], dtype=float32), 0.3833095]. 
=============================================
[2019-04-03 22:08:37,776] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6605523e-11 1.7344725e-04 7.2636996e-10 3.5670837e-05 1.5374344e-04
 2.6975347e-10 9.9963713e-01], sum to 1.0000
[2019-04-03 22:08:37,777] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0537
[2019-04-03 22:08:37,796] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 99.16666666666667, 0.0, 0.0, 26.0, 25.29032960453704, 0.4539178843128271, 0.0, 1.0, 56888.67930540042], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1396200.0000, 
sim time next is 1396800.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.21901016447978, 0.4562260120897386, 0.0, 1.0, 45532.73539683093], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.601584180373315, 0.6520753373632462, 0.0, 1.0, 0.21682254950871874], 
reward next is 0.7832, 
noisyNet noise sample is [array([0.42081288], dtype=float32), -0.89833236]. 
=============================================
[2019-04-03 22:08:45,985] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.8111040e-11 9.1188422e-06 1.5515687e-10 9.6100939e-06 1.4376554e-04
 2.1202879e-12 9.9983752e-01], sum to 1.0000
[2019-04-03 22:08:45,985] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5919
[2019-04-03 22:08:46,030] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.31666666666667, 61.83333333333334, 0.0, 0.0, 26.0, 26.56742471213217, 0.7409464464812379, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1620600.0000, 
sim time next is 1621200.0000, 
raw observation next is [10.13333333333333, 62.66666666666667, 0.0, 0.0, 26.0, 26.5120138662639, 0.7318231002005954, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7433056325023084, 0.6266666666666667, 0.0, 0.0, 0.6666666666666666, 0.7093344888553249, 0.7439410334001985, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11452939], dtype=float32), -1.4828331]. 
=============================================
[2019-04-03 22:08:56,197] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6341477e-12 3.1918036e-05 4.3400017e-10 2.6256953e-06 1.5467659e-04
 7.6828830e-12 9.9981076e-01], sum to 1.0000
[2019-04-03 22:08:56,246] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1913
[2019-04-03 22:08:56,269] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.600000000000001, 83.66666666666666, 0.0, 0.0, 26.0, 25.68868479370917, 0.5449829582043788, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1564800.0000, 
sim time next is 1565400.0000, 
raw observation next is [4.5, 84.83333333333334, 0.0, 0.0, 26.0, 25.70167638545807, 0.5338723515199878, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5872576177285319, 0.8483333333333334, 0.0, 0.0, 0.6666666666666666, 0.6418063654548393, 0.6779574505066627, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0331435], dtype=float32), -1.5146003]. 
=============================================
[2019-04-03 22:09:09,491] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.01414585e-10 1.39726239e-04 1.24751653e-09 1.41717916e-04
 1.03903224e-03 2.80664852e-10 9.98679459e-01], sum to 1.0000
[2019-04-03 22:09:09,498] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8763
[2019-04-03 22:09:09,510] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.083333333333334, 92.83333333333334, 0.0, 0.0, 26.0, 25.61221475803555, 0.5405273200838544, 0.0, 1.0, 18734.67538624313], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1666200.0000, 
sim time next is 1666800.0000, 
raw observation next is [5.0, 92.0, 0.0, 0.0, 26.0, 25.58662480637297, 0.53767898909442, 0.0, 1.0, 28064.02042309564], 
processed observation next is [1.0, 0.30434782608695654, 0.6011080332409973, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6322187338644142, 0.6792263296981401, 0.0, 1.0, 0.13363819249093162], 
reward next is 0.8664, 
noisyNet noise sample is [array([0.00930906], dtype=float32), 1.1818485]. 
=============================================
[2019-04-03 22:09:16,605] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3346695e-11 1.4081757e-05 7.8410306e-11 5.4589291e-05 5.3767033e-04
 4.8652991e-12 9.9939370e-01], sum to 1.0000
[2019-04-03 22:09:16,606] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7213
[2019-04-03 22:09:16,664] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.366666666666667, 75.33333333333333, 0.0, 0.0, 26.0, 25.30068056117317, 0.6029087173239025, 0.0, 1.0, 197762.5127839971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1629600.0000, 
sim time next is 1630200.0000, 
raw observation next is [7.283333333333333, 75.66666666666667, 0.0, 0.0, 26.0, 25.32524699418589, 0.6539297109981442, 0.0, 1.0, 193118.1176617036], 
processed observation next is [1.0, 0.8695652173913043, 0.6643582640812559, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6104372495154907, 0.7179765703327147, 0.0, 1.0, 0.9196100841033505], 
reward next is 0.0804, 
noisyNet noise sample is [array([-0.8488714], dtype=float32), -0.056451086]. 
=============================================
[2019-04-03 22:09:32,743] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.6212030e-10 7.3471334e-04 7.9149691e-09 1.5589759e-04 3.7086196e-04
 3.0184968e-09 9.9873859e-01], sum to 1.0000
[2019-04-03 22:09:32,743] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4162
[2019-04-03 22:09:32,809] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 83.0, 122.5, 0.0, 26.0, 24.94535073758443, 0.3452374583605304, 0.0, 1.0, 39823.89444697162], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1771200.0000, 
sim time next is 1771800.0000, 
raw observation next is [-2.383333333333333, 83.0, 123.6666666666667, 0.0, 26.0, 24.9419784605958, 0.3456335143114311, 0.0, 1.0, 45468.65082247229], 
processed observation next is [0.0, 0.5217391304347826, 0.3965835641735919, 0.83, 0.4122222222222223, 0.0, 0.6666666666666666, 0.5784982050496499, 0.6152111714371437, 0.0, 1.0, 0.21651738486891567], 
reward next is 0.7835, 
noisyNet noise sample is [array([0.9549694], dtype=float32), -2.0753472]. 
=============================================
[2019-04-03 22:09:46,207] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.0271122e-11 3.0695808e-06 3.3328384e-09 2.7032502e-05 4.0329123e-05
 1.0756774e-11 9.9992955e-01], sum to 1.0000
[2019-04-03 22:09:46,207] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3982
[2019-04-03 22:09:46,237] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 62.0, 112.0, 0.0, 26.0, 25.7690023944222, 0.3420464870424442, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1953000.0000, 
sim time next is 1953600.0000, 
raw observation next is [-3.0, 62.0, 105.6666666666667, 0.0, 26.0, 25.75708213815803, 0.3361530915581085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.62, 0.3522222222222223, 0.0, 0.6666666666666666, 0.6464235115131691, 0.6120510305193695, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.96884584], dtype=float32), -0.95613474]. 
=============================================
[2019-04-03 22:09:59,648] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.17802616e-11 1.29075925e-05 2.69568812e-10 1.54677819e-05
 1.03355615e-05 2.23549772e-11 9.99961257e-01], sum to 1.0000
[2019-04-03 22:09:59,648] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1508
[2019-04-03 22:09:59,716] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.34865462303422, 0.150896312037989, 0.0, 1.0, 42007.69989682148], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1990800.0000, 
sim time next is 1991400.0000, 
raw observation next is [-6.100000000000001, 86.33333333333333, 0.0, 0.0, 26.0, 24.31441310417484, 0.1558491147971126, 0.0, 1.0, 41916.4632623335], 
processed observation next is [1.0, 0.043478260869565216, 0.2936288088642659, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.52620109201457, 0.5519497049323708, 0.0, 1.0, 0.1996022060111119], 
reward next is 0.8004, 
noisyNet noise sample is [array([-0.4608193], dtype=float32), -2.437209]. 
=============================================
[2019-04-03 22:10:29,616] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5584245e-11 2.8685911e-06 2.6481517e-10 9.3080453e-06 9.1104623e-04
 1.1283042e-11 9.9907684e-01], sum to 1.0000
[2019-04-03 22:10:29,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1404
[2019-04-03 22:10:29,641] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 51.0, 241.5, 71.5, 26.0, 25.22074536790879, 0.3205782056164819, 1.0, 1.0, 18696.98945420844], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2293200.0000, 
sim time next is 2293800.0000, 
raw observation next is [-1.516666666666667, 50.0, 234.6666666666667, 70.66666666666667, 26.0, 25.24706516168987, 0.3251476260027613, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4205909510618652, 0.5, 0.7822222222222224, 0.07808471454880295, 0.6666666666666666, 0.6039220968074893, 0.6083825420009205, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9762222], dtype=float32), -1.3482808]. 
=============================================
[2019-04-03 22:10:29,713] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5396964e-11 5.2414002e-08 1.4323055e-10 1.9856639e-06 4.4301716e-05
 4.2517287e-12 9.9995363e-01], sum to 1.0000
[2019-04-03 22:10:29,713] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5585
[2019-04-03 22:10:29,743] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 64.0, 0.0, 0.0, 26.0, 24.88373265973871, 0.2947731546875648, 0.0, 1.0, 38538.34105702936], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2332200.0000, 
sim time next is 2332800.0000, 
raw observation next is [-2.3, 65.0, 0.0, 0.0, 26.0, 24.90319579017838, 0.3006188416844375, 0.0, 1.0, 38512.7067778012], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5752663158481983, 0.6002062805614792, 0.0, 1.0, 0.18339384179905333], 
reward next is 0.8166, 
noisyNet noise sample is [array([-0.8217913], dtype=float32), -0.028985534]. 
=============================================
[2019-04-03 22:10:33,324] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-03 22:10:33,325] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:10:33,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:10:33,327] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:10:33,328] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:10:33,328] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:10:33,328] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:10:33,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run3
[2019-04-03 22:10:33,349] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run3
[2019-04-03 22:10:33,349] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run3
[2019-04-03 22:12:54,573] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-03 22:13:23,981] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-03 22:13:28,064] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-03 22:13:29,099] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 200000, evaluation results [200000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-03 22:13:46,183] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2396412e-09 6.5683707e-06 1.1848023e-09 8.6252039e-06 3.7949707e-04
 2.3298921e-10 9.9960524e-01], sum to 1.0000
[2019-04-03 22:13:46,184] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1473
[2019-04-03 22:13:46,229] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.383333333333333, 27.0, 81.0, 800.0, 26.0, 24.96796717476861, 0.2800231656216398, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2470200.0000, 
sim time next is 2470800.0000, 
raw observation next is [2.566666666666667, 27.0, 79.5, 792.0, 26.0, 24.97471083677251, 0.2791678425697103, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.5337026777469991, 0.27, 0.265, 0.8751381215469614, 0.6666666666666666, 0.5812259030643757, 0.5930559475232368, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15200438], dtype=float32), -1.0708517]. 
=============================================
[2019-04-03 22:13:50,428] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9222525e-11 7.7595837e-07 1.0817486e-10 9.4629348e-07 2.0888213e-04
 2.3875669e-11 9.9978942e-01], sum to 1.0000
[2019-04-03 22:13:50,430] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8935
[2019-04-03 22:13:50,479] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.583333333333333, 63.0, 0.0, 0.0, 26.0, 24.76994152239203, 0.2476062593194627, 0.0, 1.0, 41906.75236377244], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2592600.0000, 
sim time next is 2593200.0000, 
raw observation next is [-4.666666666666667, 64.0, 0.0, 0.0, 26.0, 24.72750565800479, 0.2384745028455935, 0.0, 1.0, 41913.22258540933], 
processed observation next is [1.0, 0.0, 0.3333333333333333, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5606254715003992, 0.5794915009485312, 0.0, 1.0, 0.19958677421623489], 
reward next is 0.8004, 
noisyNet noise sample is [array([-1.3352516], dtype=float32), -0.5823446]. 
=============================================
[2019-04-03 22:13:52,916] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0949031e-11 5.4816314e-07 5.9969453e-11 4.6948895e-07 1.0458755e-05
 6.1401158e-12 9.9998844e-01], sum to 1.0000
[2019-04-03 22:13:52,916] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2740
[2019-04-03 22:13:52,938] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.733333333333333, 28.66666666666667, 178.8333333333333, 405.3333333333334, 26.0, 25.30308624850745, 0.3148524659078742, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2553600.0000, 
sim time next is 2554200.0000, 
raw observation next is [3.0, 28.0, 171.0, 482.0, 26.0, 25.4183425133729, 0.3428141598073027, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.28, 0.57, 0.532596685082873, 0.6666666666666666, 0.6181952094477415, 0.6142713866024342, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1448046], dtype=float32), 0.25904402]. 
=============================================
[2019-04-03 22:13:56,017] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.7770720e-12 2.2820118e-06 7.3946023e-11 5.3544289e-07 1.1815287e-05
 1.7664913e-11 9.9998534e-01], sum to 1.0000
[2019-04-03 22:13:56,019] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2143
[2019-04-03 22:13:56,053] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.88613282435814, 0.2471861578937908, 0.0, 1.0, 41686.05936843724], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2597400.0000, 
sim time next is 2598000.0000, 
raw observation next is [-5.0, 72.0, 0.0, 0.0, 26.0, 24.88405404741618, 0.2457707775980502, 0.0, 1.0, 41670.95272255494], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5736711706180149, 0.5819235925326834, 0.0, 1.0, 0.19843310820264257], 
reward next is 0.8016, 
noisyNet noise sample is [array([1.9287618], dtype=float32), -0.83737725]. 
=============================================
[2019-04-03 22:13:56,063] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[84.52456 ]
 [84.42936 ]
 [84.34782 ]
 [84.229836]
 [84.20193 ]], R is [[84.53393555]
 [84.49009705]
 [84.44650269]
 [84.40311432]
 [84.35998535]].
[2019-04-03 22:13:59,118] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2799661e-11 1.3289131e-06 2.9652408e-10 1.8874230e-06 6.9661182e-05
 9.4894839e-12 9.9992716e-01], sum to 1.0000
[2019-04-03 22:13:59,118] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4735
[2019-04-03 22:13:59,267] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.300000000000001, 79.0, 0.0, 0.0, 26.0, 23.82084104630318, 0.06666278058467502, 1.0, 1.0, 202354.9928688726], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2618400.0000, 
sim time next is 2619000.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.08338539231539, 0.1731902778785061, 1.0, 1.0, 203354.029469123], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5069487826929491, 0.5577300926261687, 1.0, 1.0, 0.968352521281538], 
reward next is 0.0316, 
noisyNet noise sample is [array([0.22241317], dtype=float32), -1.1417222]. 
=============================================
[2019-04-03 22:13:59,302] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.13573 ]
 [83.23333 ]
 [83.38809 ]
 [83.539566]
 [83.71463 ]], R is [[86.17852783]
 [85.35314941]
 [85.2858963 ]
 [85.21962738]
 [85.1543808 ]].
[2019-04-03 22:14:00,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1850264e-10 4.8918915e-05 1.2226747e-08 9.5314999e-06 2.2096794e-04
 9.7146358e-10 9.9972051e-01], sum to 1.0000
[2019-04-03 22:14:00,628] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1831
[2019-04-03 22:14:00,698] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.16666666666667, 83.0, 0.0, 0.0, 26.0, 23.27262899895617, -0.06865591368462533, 0.0, 1.0, 43825.15433495284], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2697000.0000, 
sim time next is 2697600.0000, 
raw observation next is [-15.33333333333334, 83.0, 0.0, 0.0, 26.0, 23.23865185876365, -0.07443361442917086, 0.0, 1.0, 43693.0803482123], 
processed observation next is [1.0, 0.21739130434782608, 0.03785780240073851, 0.83, 0.0, 0.0, 0.6666666666666666, 0.43655432156363744, 0.4751887951902764, 0.0, 1.0, 0.20806228737243954], 
reward next is 0.7919, 
noisyNet noise sample is [array([1.3525944], dtype=float32), 0.2220403]. 
=============================================
[2019-04-03 22:14:10,000] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.27513902e-10 1.36574417e-05 1.09083875e-09 1.02720905e-05
 1.04529216e-04 3.35346056e-11 9.99871492e-01], sum to 1.0000
[2019-04-03 22:14:10,031] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9519
[2019-04-03 22:14:10,058] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.3, 26.5, 71.0, 76.0, 26.0, 24.80755228773688, 0.3045079592914465, 1.0, 1.0, 177989.6508815089], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2824200.0000, 
sim time next is 2824800.0000, 
raw observation next is [6.2, 27.0, 60.00000000000001, 71.0, 26.0, 25.09709708912077, 0.3468957978111817, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6343490304709142, 0.27, 0.2, 0.07845303867403315, 0.6666666666666666, 0.5914247574267307, 0.6156319326037273, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.087851], dtype=float32), 0.29288056]. 
=============================================
[2019-04-03 22:15:01,071] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.03577711e-11 1.77140623e-06 6.79968248e-10 1.74567026e-06
 1.04132669e-05 1.35335285e-11 9.99986053e-01], sum to 1.0000
[2019-04-03 22:15:01,071] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2303
[2019-04-03 22:15:01,107] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 63.33333333333334, 0.0, 0.0, 26.0, 25.33060608371655, 0.5263182122372924, 0.0, 1.0, 56875.7600934307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3358200.0000, 
sim time next is 3358800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.51044493533991, 0.5364345489633927, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6258704112783257, 0.6788115163211309, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22608255], dtype=float32), -1.6763122]. 
=============================================
[2019-04-03 22:15:02,424] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.5864193e-12 2.2757275e-05 4.8758503e-10 5.5774740e-06 3.2187429e-05
 5.5660140e-11 9.9993944e-01], sum to 1.0000
[2019-04-03 22:15:02,464] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7486
[2019-04-03 22:15:02,490] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.29442375448688, 0.4756725187153049, 0.0, 1.0, 50771.5163309534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3279000.0000, 
sim time next is 3279600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.31585480705353, 0.4709978013030334, 0.0, 1.0, 45681.87279140203], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6096545672544608, 0.6569992671010111, 0.0, 1.0, 0.21753272757810488], 
reward next is 0.7825, 
noisyNet noise sample is [array([1.6091805], dtype=float32), -0.21281959]. 
=============================================
[2019-04-03 22:15:09,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2811680e-11 6.0526518e-06 5.0785121e-10 3.7704115e-06 4.1067735e-05
 3.3544181e-11 9.9994910e-01], sum to 1.0000
[2019-04-03 22:15:09,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8555
[2019-04-03 22:15:09,867] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 25.21176364014702, 0.4075222949873701, 0.0, 1.0, 45718.45656211058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3366000.0000, 
sim time next is 3366600.0000, 
raw observation next is [-5.166666666666667, 72.0, 0.0, 0.0, 26.0, 25.1749578230314, 0.3995833930690086, 0.0, 1.0, 42961.25213688086], 
processed observation next is [1.0, 1.0, 0.31948291782086796, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5979131519192832, 0.6331944643563362, 0.0, 1.0, 0.2045773911280041], 
reward next is 0.7954, 
noisyNet noise sample is [array([-0.38234884], dtype=float32), 0.41561812]. 
=============================================
[2019-04-03 22:15:11,752] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.7242241e-10 5.0736405e-04 1.3123542e-09 3.7848815e-05 9.4005291e-04
 3.3616812e-09 9.9851483e-01], sum to 1.0000
[2019-04-03 22:15:11,753] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7956
[2019-04-03 22:15:11,774] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 44.66666666666667, 0.0, 0.0, 26.0, 25.44548861641057, 0.3928209453043814, 0.0, 1.0, 56843.31282307065], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3619200.0000, 
sim time next is 3619800.0000, 
raw observation next is [-1.5, 46.0, 0.0, 0.0, 26.0, 25.417736359625, 0.3915067210706158, 0.0, 1.0, 59025.17434843891], 
processed observation next is [0.0, 0.9130434782608695, 0.4210526315789474, 0.46, 0.0, 0.0, 0.6666666666666666, 0.6181446966354166, 0.630502240356872, 0.0, 1.0, 0.28107225880209], 
reward next is 0.7189, 
noisyNet noise sample is [array([0.07605746], dtype=float32), 1.016255]. 
=============================================
[2019-04-03 22:15:12,440] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0851767e-11 2.0690927e-06 3.2695114e-11 1.0510400e-06 2.4114262e-05
 1.4808177e-12 9.9997282e-01], sum to 1.0000
[2019-04-03 22:15:12,440] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6301
[2019-04-03 22:15:12,482] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 48.5, 109.0, 764.0, 26.0, 26.54282976056674, 0.6210439564036273, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3407400.0000, 
sim time next is 3408000.0000, 
raw observation next is [2.666666666666667, 48.66666666666666, 110.0, 770.6666666666667, 26.0, 26.56218586876994, 0.6347510213395395, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5364727608494922, 0.4866666666666666, 0.36666666666666664, 0.8515653775322285, 0.6666666666666666, 0.7135154890641617, 0.7115836737798465, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10905172], dtype=float32), 0.1376062]. 
=============================================
[2019-04-03 22:15:12,488] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[87.17318 ]
 [87.0234  ]
 [86.88397 ]
 [86.76624 ]
 [86.721954]], R is [[87.52577972]
 [87.65052032]
 [87.77401733]
 [87.89627838]
 [88.01731873]].
[2019-04-03 22:15:37,177] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5403667e-10 5.3225493e-04 4.0228748e-10 1.1214983e-06 9.0467103e-05
 1.2700012e-10 9.9937624e-01], sum to 1.0000
[2019-04-03 22:15:37,205] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8191
[2019-04-03 22:15:37,229] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 51.66666666666667, 0.0, 0.0, 26.0, 25.4077235994349, 0.3842037290571373, 0.0, 1.0, 35312.26329905964], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3622200.0000, 
sim time next is 3622800.0000, 
raw observation next is [-2.333333333333333, 53.33333333333334, 0.0, 0.0, 26.0, 25.38910226501576, 0.3791751830462357, 0.0, 1.0, 46056.21112495223], 
processed observation next is [0.0, 0.9565217391304348, 0.3979686057248385, 0.5333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6157585220846468, 0.6263917276820786, 0.0, 1.0, 0.2193152910712011], 
reward next is 0.7807, 
noisyNet noise sample is [array([-0.47114316], dtype=float32), -1.3907156]. 
=============================================
[2019-04-03 22:15:37,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7964592e-12 6.1371366e-06 1.4971887e-11 1.2460647e-06 4.0511128e-05
 6.1642293e-12 9.9995208e-01], sum to 1.0000
[2019-04-03 22:15:37,903] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6500
[2019-04-03 22:15:37,929] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 77.0, 101.8333333333333, 690.6666666666666, 26.0, 26.15236911432388, 0.491105311195613, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3750000.0000, 
sim time next is 3750600.0000, 
raw observation next is [-3.166666666666667, 77.0, 103.6666666666667, 706.3333333333333, 26.0, 26.16898794957394, 0.5049300179528609, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3748845798707295, 0.77, 0.34555555555555567, 0.7804788213627992, 0.6666666666666666, 0.6807489957978282, 0.6683100059842869, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.87266845], dtype=float32), -0.24036764]. 
=============================================
[2019-04-03 22:15:51,660] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0679524e-11 2.1263044e-05 3.9482512e-10 9.6381973e-07 2.1555254e-05
 2.6956076e-10 9.9995625e-01], sum to 1.0000
[2019-04-03 22:15:51,664] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6959
[2019-04-03 22:15:51,754] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.66666666666667, 61.33333333333333, 62.00000000000001, 296.0000000000001, 26.0, 25.45738976692274, 0.3561941504222581, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4003800.0000, 
sim time next is 4004400.0000, 
raw observation next is [-12.33333333333333, 59.66666666666667, 77.5, 370.0, 26.0, 25.60059835135763, 0.4035815004608259, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.12096029547553101, 0.5966666666666667, 0.25833333333333336, 0.4088397790055249, 0.6666666666666666, 0.6333831959464691, 0.6345271668202753, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14814413], dtype=float32), 0.5774517]. 
=============================================
[2019-04-03 22:16:07,114] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3800365e-12 2.1724205e-07 2.9469076e-11 1.6348697e-07 2.5443248e-06
 2.2866503e-12 9.9999714e-01], sum to 1.0000
[2019-04-03 22:16:07,114] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0343
[2019-04-03 22:16:07,149] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 50.0, 0.0, 0.0, 26.0, 25.38014433594826, 0.5267333337052943, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3868800.0000, 
sim time next is 3869400.0000, 
raw observation next is [1.166666666666667, 50.5, 0.0, 0.0, 26.0, 25.48811187827305, 0.5275001090394112, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.49492151431209613, 0.505, 0.0, 0.0, 0.6666666666666666, 0.6240093231894207, 0.6758333696798037, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42386267], dtype=float32), -0.44554612]. 
=============================================
[2019-04-03 22:16:08,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.9813933e-12 1.2207253e-05 1.3741504e-10 1.3328705e-07 1.2330835e-05
 1.0551114e-11 9.9997532e-01], sum to 1.0000
[2019-04-03 22:16:08,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3141
[2019-04-03 22:16:08,550] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.31752638118938, 0.4043392763480445, 0.0, 1.0, 62346.04294055855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3894600.0000, 
sim time next is 3895200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.26133268746101, 0.4021228887712961, 0.0, 1.0, 49000.37991185666], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6051110572884175, 0.6340409629237653, 0.0, 1.0, 0.23333514243741266], 
reward next is 0.7667, 
noisyNet noise sample is [array([0.2171235], dtype=float32), 1.717481]. 
=============================================
[2019-04-03 22:16:48,640] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.4017232e-14 2.9443638e-07 1.5240111e-12 2.5014707e-08 1.9536293e-07
 1.7144977e-13 9.9999940e-01], sum to 1.0000
[2019-04-03 22:16:48,640] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1250
[2019-04-03 22:16:48,664] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 78.0, 49.0, 0.0, 26.0, 26.02787326175942, 0.5770719498697144, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4465800.0000, 
sim time next is 4466400.0000, 
raw observation next is [0.0, 78.0, 45.0, 9.166666666666664, 26.0, 26.10261001061986, 0.5828266706889906, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.15, 0.010128913443830568, 0.6666666666666666, 0.6752175008849882, 0.6942755568963301, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.76041], dtype=float32), 1.3841473]. 
=============================================
[2019-04-03 22:16:50,420] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6896935e-10 1.3685464e-04 4.7570559e-10 1.3149487e-06 2.8815144e-04
 4.2388806e-10 9.9957365e-01], sum to 1.0000
[2019-04-03 22:16:50,438] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9563
[2019-04-03 22:16:50,453] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 38.16666666666667, 129.3333333333333, 542.0, 26.0, 25.35895530066799, 0.4368489390159924, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4204200.0000, 
sim time next is 4204800.0000, 
raw observation next is [3.0, 37.0, 114.0, 544.0, 26.0, 25.35635377426713, 0.4402211406094272, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.37, 0.38, 0.6011049723756906, 0.6666666666666666, 0.6130294811889275, 0.6467403802031424, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39932257], dtype=float32), 0.071051486]. 
=============================================
[2019-04-03 22:16:51,670] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0289239e-12 1.7948379e-06 1.5271199e-11 9.9897250e-08 2.1292144e-06
 3.4927432e-13 9.9999595e-01], sum to 1.0000
[2019-04-03 22:16:51,695] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5672
[2019-04-03 22:16:51,719] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 90.83333333333334, 122.0, 2.0, 26.0, 25.96008306047805, 0.6075250294476696, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4457400.0000, 
sim time next is 4458000.0000, 
raw observation next is [0.0, 89.66666666666667, 103.5, 0.9999999999999998, 26.0, 26.153083770429, 0.6217663403487708, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.8966666666666667, 0.345, 0.0011049723756906074, 0.6666666666666666, 0.6794236475357499, 0.7072554467829235, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0196289], dtype=float32), -1.7575984]. 
=============================================
[2019-04-03 22:16:51,766] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[91.27533 ]
 [91.4674  ]
 [91.38682 ]
 [90.764336]
 [89.532974]], R is [[90.91486359]
 [91.00571442]
 [91.09565735]
 [90.86146545]
 [90.01824951]].
[2019-04-03 22:16:52,711] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5608450e-12 5.9093603e-05 3.9398210e-11 2.6693399e-07 4.9615407e-04
 2.0357894e-11 9.9944454e-01], sum to 1.0000
[2019-04-03 22:16:52,711] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0075
[2019-04-03 22:16:52,768] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.1, 68.33333333333334, 76.66666666666667, 409.1666666666667, 26.0, 25.50069213772108, 0.4611797574404797, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4350000.0000, 
sim time next is 4350600.0000, 
raw observation next is [4.65, 65.5, 92.0, 491.0, 26.0, 25.805236788614, 0.5054972833869602, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5914127423822716, 0.655, 0.30666666666666664, 0.5425414364640884, 0.6666666666666666, 0.6504363990511667, 0.66849909446232, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35450676], dtype=float32), -0.22929521]. 
=============================================
[2019-04-03 22:16:56,452] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7563798e-12 7.2189760e-05 5.1527518e-11 3.7807776e-08 1.8750017e-05
 5.5964790e-12 9.9990904e-01], sum to 1.0000
[2019-04-03 22:16:56,452] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5422
[2019-04-03 22:16:56,511] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.54385416942845, 0.4371174984356787, 0.0, 1.0, 31442.25444549326], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4497600.0000, 
sim time next is 4498200.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.39544580692588, 0.4304156998491899, 0.0, 1.0, 114201.3732516114], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6162871505771568, 0.64347189994973, 0.0, 1.0, 0.5438160631029114], 
reward next is 0.4562, 
noisyNet noise sample is [array([-0.72423077], dtype=float32), 1.3456929]. 
=============================================
[2019-04-03 22:17:00,080] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2807955e-11 1.3405538e-05 1.8081137e-10 5.5673303e-07 1.4441527e-05
 4.8058602e-11 9.9997163e-01], sum to 1.0000
[2019-04-03 22:17:00,080] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4070
[2019-04-03 22:17:00,118] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.033333333333333, 68.16666666666666, 0.0, 0.0, 26.0, 25.01834704004518, 0.2939069652202982, 0.0, 1.0, 52557.72783773782], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4301400.0000, 
sim time next is 4302000.0000, 
raw observation next is [6.0, 69.0, 0.0, 0.0, 26.0, 24.96770276796358, 0.2905106203998265, 0.0, 1.0, 56549.3236518721], 
processed observation next is [0.0, 0.8260869565217391, 0.6288088642659281, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5806418973302984, 0.5968368734666089, 0.0, 1.0, 0.26928249358034334], 
reward next is 0.7307, 
noisyNet noise sample is [array([-0.35153508], dtype=float32), 0.47514778]. 
=============================================
[2019-04-03 22:17:00,169] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[80.21581 ]
 [79.945114]
 [79.879135]
 [80.07745 ]
 [80.403366]], R is [[80.54854584]
 [80.49279022]
 [80.52422333]
 [80.71897888]
 [80.91178894]].
[2019-04-03 22:17:11,304] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.0684090e-12 1.5715533e-04 6.1776459e-11 2.5993452e-07 1.6989616e-05
 4.4454666e-12 9.9982554e-01], sum to 1.0000
[2019-04-03 22:17:11,306] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2620
[2019-04-03 22:17:11,335] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 130.5, 0.9999999999999998, 26.0, 26.37390250007046, 0.5680410835473847, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4700400.0000, 
sim time next is 4701000.0000, 
raw observation next is [0.0, 92.0, 146.0, 2.0, 26.0, 26.40040469175683, 0.5757208662328255, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.4866666666666667, 0.0022099447513812156, 0.6666666666666666, 0.7000337243130691, 0.6919069554109418, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17878945], dtype=float32), 0.70618844]. 
=============================================
[2019-04-03 22:17:11,339] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[92.265015]
 [92.252754]
 [92.32761 ]
 [92.190636]
 [92.32921 ]], R is [[92.40023804]
 [92.47623444]
 [92.55147552]
 [92.6259613 ]
 [92.6996994 ]].
[2019-04-03 22:17:15,304] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1460333e-11 5.9373388e-06 1.6622680e-10 1.4495552e-07 3.2311011e-05
 6.9496416e-11 9.9996161e-01], sum to 1.0000
[2019-04-03 22:17:15,305] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0307
[2019-04-03 22:17:15,331] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 73.5, 0.0, 0.0, 26.0, 25.17680853910562, 0.3751510952341134, 0.0, 1.0, 36188.79767894872], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4600200.0000, 
sim time next is 4600800.0000, 
raw observation next is [-2.6, 74.0, 0.0, 0.0, 26.0, 25.20454126345438, 0.3780697949527347, 0.0, 1.0, 36162.77286942682], 
processed observation next is [1.0, 0.2608695652173913, 0.3905817174515236, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6003784386211984, 0.6260232649842449, 0.0, 1.0, 0.1722036803306039], 
reward next is 0.8278, 
noisyNet noise sample is [array([-0.43983167], dtype=float32), -1.580375]. 
=============================================
[2019-04-03 22:17:24,686] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.8065970e-12 1.6033705e-06 4.4136539e-12 3.8051493e-08 3.8174639e-06
 6.5331518e-13 9.9999452e-01], sum to 1.0000
[2019-04-03 22:17:24,689] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1872
[2019-04-03 22:17:24,713] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 78.0, 0.0, 0.0, 26.0, 25.29692011224179, 0.438359369232137, 1.0, 1.0, 25752.18211764261], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4732200.0000, 
sim time next is 4732800.0000, 
raw observation next is [-0.6666666666666666, 78.0, 0.0, 0.0, 26.0, 25.17633733616131, 0.4193752662702163, 1.0, 1.0, 58456.17078819711], 
processed observation next is [1.0, 0.782608695652174, 0.44413665743305636, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5980281113467759, 0.6397917554234055, 1.0, 1.0, 0.27836271803903384], 
reward next is 0.7216, 
noisyNet noise sample is [array([-1.0402035], dtype=float32), 0.878695]. 
=============================================
[2019-04-03 22:17:24,718] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9218728e-12 2.5486203e-05 2.6739560e-11 6.6410529e-08 2.8189127e-06
 2.1152882e-12 9.9997163e-01], sum to 1.0000
[2019-04-03 22:17:24,722] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5514
[2019-04-03 22:17:24,737] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 80.5, 0.0, 0.0, 26.0, 25.34345750756383, 0.4501961521768711, 0.0, 1.0, 54805.34797477401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4746600.0000, 
sim time next is 4747200.0000, 
raw observation next is [-3.0, 79.33333333333333, 0.0, 0.0, 26.0, 25.27318331220595, 0.4428964932455903, 0.0, 1.0, 48813.28174194528], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.7933333333333333, 0.0, 0.0, 0.6666666666666666, 0.606098609350496, 0.6476321644151968, 0.0, 1.0, 0.23244419877116798], 
reward next is 0.7676, 
noisyNet noise sample is [array([0.95406145], dtype=float32), -0.9321044]. 
=============================================
[2019-04-03 22:17:29,714] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.8704469e-12 5.1833584e-05 3.3312311e-11 4.6279286e-08 4.0998912e-06
 8.8528490e-12 9.9994409e-01], sum to 1.0000
[2019-04-03 22:17:29,716] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2140
[2019-04-03 22:17:29,740] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 53.0, 0.0, 0.0, 26.0, 25.40866426579813, 0.4061592362609501, 0.0, 1.0, 46260.42998313937], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4829400.0000, 
sim time next is 4830000.0000, 
raw observation next is [-0.6666666666666666, 53.66666666666667, 0.0, 0.0, 26.0, 25.40624447841639, 0.4036897292734634, 0.0, 1.0, 41882.137371301], 
processed observation next is [0.0, 0.9130434782608695, 0.44413665743305636, 0.5366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6171870398680325, 0.6345632430911544, 0.0, 1.0, 0.19943874938714765], 
reward next is 0.8006, 
noisyNet noise sample is [array([-1.4872329], dtype=float32), 0.7755796]. 
=============================================
[2019-04-03 22:17:29,752] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[82.45324 ]
 [82.460884]
 [82.29566 ]
 [82.06877 ]
 [81.89451 ]], R is [[82.38026428]
 [82.33617401]
 [82.217659  ]
 [82.09796143]
 [82.03230286]].
[2019-04-03 22:17:31,850] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.65653333e-11 7.32696935e-05 3.87415156e-10 5.42687985e-07
 7.89816477e-05 1.27631915e-11 9.99847174e-01], sum to 1.0000
[2019-04-03 22:17:31,851] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6105
[2019-04-03 22:17:31,879] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 58.66666666666666, 0.0, 0.0, 26.0, 25.24584279355368, 0.34714025647812, 0.0, 1.0, 38023.33394124656], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5035200.0000, 
sim time next is 5035800.0000, 
raw observation next is [-2.666666666666667, 61.83333333333334, 0.0, 0.0, 26.0, 25.26128160551507, 0.3521453270437063, 0.0, 1.0, 37406.24316596101], 
processed observation next is [1.0, 0.2608695652173913, 0.38873499538319484, 0.6183333333333334, 0.0, 0.0, 0.6666666666666666, 0.6051068004595891, 0.6173817756812354, 0.0, 1.0, 0.1781249674569572], 
reward next is 0.8219, 
noisyNet noise sample is [array([-1.9529947], dtype=float32), -0.18336487]. 
=============================================
[2019-04-03 22:17:34,694] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1641504e-12 6.4275459e-06 1.4048866e-11 4.9773725e-08 2.0131994e-04
 1.6095027e-12 9.9979228e-01], sum to 1.0000
[2019-04-03 22:17:34,697] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1755
[2019-04-03 22:17:34,707] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 26.0, 28.02647262029113, 1.024502676804169, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5079600.0000, 
sim time next is 5080200.0000, 
raw observation next is [10.83333333333333, 17.33333333333334, 0.0, 0.0, 26.0, 27.95625401554602, 1.013285672637775, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.76269621421976, 0.1733333333333334, 0.0, 0.0, 0.6666666666666666, 0.8296878346288349, 0.8377618908792583, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0969547], dtype=float32), -0.73028105]. 
=============================================
[2019-04-03 22:17:36,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:36,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:36,374] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run3
[2019-04-03 22:17:36,484] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:36,484] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:36,534] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run3
[2019-04-03 22:17:37,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:37,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:37,083] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run3
[2019-04-03 22:17:39,332] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:39,334] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:39,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run3
[2019-04-03 22:17:44,645] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.5386532e-14 2.5258356e-07 3.9532158e-12 1.9780364e-08 2.3752144e-05
 9.7714664e-13 9.9997604e-01], sum to 1.0000
[2019-04-03 22:17:44,646] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7724
[2019-04-03 22:17:44,663] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.65, 19.16666666666667, 0.0, 0.0, 26.0, 26.7212303878141, 0.7264059261101649, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5091000.0000, 
sim time next is 5091600.0000, 
raw observation next is [8.6, 19.33333333333334, 0.0, 0.0, 26.0, 26.70433751311858, 0.6972942165878857, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.700831024930748, 0.19333333333333338, 0.0, 0.0, 0.6666666666666666, 0.7253614594265484, 0.7324314055292952, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0438787], dtype=float32), -1.1495651]. 
=============================================
[2019-04-03 22:17:45,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:45,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:45,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run3
[2019-04-03 22:17:45,872] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:45,872] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:45,882] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run3
[2019-04-03 22:17:45,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:45,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:45,963] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run3
[2019-04-03 22:17:45,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:45,976] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:45,988] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run3
[2019-04-03 22:17:47,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:47,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:47,283] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run3
[2019-04-03 22:17:47,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:47,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:47,543] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run3
[2019-04-03 22:17:48,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:48,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:48,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run3
[2019-04-03 22:17:52,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:52,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:52,463] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run3
[2019-04-03 22:17:53,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:53,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:53,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run3
[2019-04-03 22:17:54,302] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:54,302] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:54,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run3
[2019-04-03 22:17:54,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:54,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:54,367] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run3
[2019-04-03 22:17:56,775] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:17:56,775] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:17:56,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run3
[2019-04-03 22:18:03,310] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2064590e-12 2.9305226e-04 9.5367048e-11 2.9806870e-07 1.1787244e-05
 3.1410825e-11 9.9969482e-01], sum to 1.0000
[2019-04-03 22:18:03,313] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6749
[2019-04-03 22:18:03,335] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.4333333333333333, 95.66666666666666, 0.0, 0.0, 26.0, 24.51594559739407, 0.1924455531638918, 0.0, 1.0, 40206.38593472949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 80400.0000, 
sim time next is 81000.0000, 
raw observation next is [0.4, 95.5, 0.0, 0.0, 26.0, 24.49101794008482, 0.188008830655655, 0.0, 1.0, 40161.77912175525], 
processed observation next is [0.0, 0.9565217391304348, 0.4736842105263158, 0.955, 0.0, 0.0, 0.6666666666666666, 0.540918161673735, 0.5626696102185517, 0.0, 1.0, 0.19124656724645359], 
reward next is 0.8088, 
noisyNet noise sample is [array([-1.1497953], dtype=float32), 0.64588404]. 
=============================================
[2019-04-03 22:18:03,345] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.27911]
 [86.3653 ]
 [86.43992]
 [86.5545 ]
 [86.68189]], R is [[86.1290741 ]
 [86.07632446]
 [86.02384186]
 [85.97156525]
 [85.91944122]].
[2019-04-03 22:18:09,092] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.1761581e-12 7.5531614e-07 5.0907507e-11 5.6552335e-07 3.5826455e-05
 3.0537948e-12 9.9996281e-01], sum to 1.0000
[2019-04-03 22:18:09,093] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0789
[2019-04-03 22:18:09,156] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 61.0, 145.0, 231.9999999999999, 26.0, 25.92424416315448, 0.4446574751965781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 137400.0000, 
sim time next is 138000.0000, 
raw observation next is [-6.700000000000001, 61.0, 146.5, 169.0, 26.0, 25.90108012763567, 0.4337984057604154, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.48833333333333334, 0.1867403314917127, 0.6666666666666666, 0.6584233439696391, 0.6445994685868052, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6686051], dtype=float32), -0.3522012]. 
=============================================
[2019-04-03 22:18:09,159] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[77.86421 ]
 [78.50028 ]
 [79.04088 ]
 [79.37279 ]
 [79.692856]], R is [[77.31095886]
 [77.53784943]
 [77.76247406]
 [77.98484802]
 [78.20500183]].
[2019-04-03 22:18:12,601] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.3098959e-11 5.0379953e-04 5.7393329e-10 8.0012285e-07 7.1134850e-06
 7.1356171e-11 9.9948823e-01], sum to 1.0000
[2019-04-03 22:18:12,602] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1572
[2019-04-03 22:18:12,675] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.50629332475311, -0.03007512574710892, 1.0, 1.0, 158125.3409287521], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 200400.0000, 
sim time next is 201000.0000, 
raw observation next is [-8.9, 78.0, 11.33333333333333, 110.6666666666666, 26.0, 23.9355086424383, 0.06263352026122797, 1.0, 1.0, 109118.5890055595], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.03777777777777777, 0.12228360957642719, 0.6666666666666666, 0.4946257202031917, 0.520877840087076, 1.0, 1.0, 0.5196123285979023], 
reward next is 0.4804, 
noisyNet noise sample is [array([-0.14425816], dtype=float32), -0.44044006]. 
=============================================
[2019-04-03 22:18:12,686] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[73.56697 ]
 [71.10435 ]
 [70.42837 ]
 [67.826065]
 [67.93423 ]], R is [[75.26080322]
 [74.75521851]
 [74.03866577]
 [73.33473969]
 [73.38699341]].
[2019-04-03 22:18:17,919] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.67667381e-11 6.49645517e-05 2.09027434e-10 2.41953927e-07
 1.54181216e-05 1.15721556e-11 9.99919415e-01], sum to 1.0000
[2019-04-03 22:18:17,920] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4315
[2019-04-03 22:18:17,971] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.766666666666667, 63.0, 143.6666666666667, 0.0, 26.0, 25.30132369799749, 0.3540707847525419, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 222000.0000, 
sim time next is 222600.0000, 
raw observation next is [-3.583333333333333, 62.5, 138.3333333333333, 0.0, 26.0, 25.843585592808, 0.4023010156367655, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.36334256694367506, 0.625, 0.46111111111111097, 0.0, 0.6666666666666666, 0.6536321327340001, 0.6341003385455884, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2501827], dtype=float32), -0.43373743]. 
=============================================
[2019-04-03 22:18:19,769] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2792458e-09 8.6814769e-05 6.8977011e-09 8.1917515e-06 1.8304346e-04
 1.7570067e-09 9.9972194e-01], sum to 1.0000
[2019-04-03 22:18:19,770] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2292
[2019-04-03 22:18:19,809] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.91666666666667, 69.0, 0.0, 0.0, 26.0, 23.27685238191511, -0.1310316547095124, 0.0, 1.0, 48201.51666115654], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 352200.0000, 
sim time next is 352800.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 23.26581323464423, -0.1446673006140628, 0.0, 1.0, 48391.01285250529], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4388177695536859, 0.45177756646197903, 0.0, 1.0, 0.2304333945357395], 
reward next is 0.7696, 
noisyNet noise sample is [array([0.16731846], dtype=float32), 0.13735943]. 
=============================================
[2019-04-03 22:18:22,039] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7959365e-11 3.6030722e-06 2.8735891e-10 5.6060207e-07 1.1205686e-04
 2.5013923e-11 9.9988377e-01], sum to 1.0000
[2019-04-03 22:18:22,052] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2940
[2019-04-03 22:18:22,087] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.399999999999999, 79.5, 0.0, 0.0, 26.0, 24.19656561694077, 0.1000108826217753, 0.0, 1.0, 44356.52889196049], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 258600.0000, 
sim time next is 259200.0000, 
raw observation next is [-4.5, 79.0, 0.0, 0.0, 26.0, 24.15870167770939, 0.09203649788015399, 0.0, 1.0, 44362.86153365797], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.79, 0.0, 0.0, 0.6666666666666666, 0.513225139809116, 0.530678832626718, 0.0, 1.0, 0.21125172158884747], 
reward next is 0.7887, 
noisyNet noise sample is [array([-1.0102884], dtype=float32), -1.1121159]. 
=============================================
[2019-04-03 22:18:25,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.03690612e-09 4.76845016e-04 1.09902105e-08 9.62821196e-06
 5.55648352e-04 5.99785666e-10 9.98957872e-01], sum to 1.0000
[2019-04-03 22:18:25,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9028
[2019-04-03 22:18:25,987] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.4, 69.5, 0.0, 0.0, 26.0, 23.46750764233409, -0.07780798748475569, 0.0, 1.0, 46359.29128555916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 273000.0000, 
sim time next is 273600.0000, 
raw observation next is [-9.5, 70.0, 0.0, 0.0, 26.0, 23.41080842201293, -0.09330872821628405, 0.0, 1.0, 46524.46015133633], 
processed observation next is [1.0, 0.17391304347826086, 0.1994459833795014, 0.7, 0.0, 0.0, 0.6666666666666666, 0.4509007018344109, 0.46889709059457196, 0.0, 1.0, 0.22154504833969682], 
reward next is 0.7785, 
noisyNet noise sample is [array([-0.04988495], dtype=float32), -0.41331273]. 
=============================================
[2019-04-03 22:18:53,949] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7741348e-13 1.8363207e-06 3.8694438e-12 6.8893353e-09 4.0981608e-06
 1.4436451e-13 9.9999416e-01], sum to 1.0000
[2019-04-03 22:18:53,949] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0995
[2019-04-03 22:18:53,965] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.633333333333333, 96.66666666666666, 0.0, 0.0, 26.0, 24.84276599367649, 0.2405629281880052, 0.0, 1.0, 39923.88670582083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 517200.0000, 
sim time next is 517800.0000, 
raw observation next is [3.716666666666666, 96.83333333333334, 0.0, 0.0, 26.0, 24.88396069463973, 0.2412304247799812, 0.0, 1.0, 39846.87247087662], 
processed observation next is [1.0, 1.0, 0.5655586334256695, 0.9683333333333334, 0.0, 0.0, 0.6666666666666666, 0.5736633912199774, 0.580410141593327, 0.0, 1.0, 0.18974701176607914], 
reward next is 0.8103, 
noisyNet noise sample is [array([0.05948848], dtype=float32), -0.58393717]. 
=============================================
[2019-04-03 22:18:55,538] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1743290e-10 1.4080357e-02 6.9651340e-10 4.2255797e-06 2.1901843e-03
 9.7891362e-11 9.8372513e-01], sum to 1.0000
[2019-04-03 22:18:55,540] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2031
[2019-04-03 22:18:55,602] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 81.5, 127.0, 467.0, 26.0, 24.98980047118538, 0.3543195270409796, 0.0, 1.0, 34569.20983397232], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 570600.0000, 
sim time next is 571200.0000, 
raw observation next is [-1.2, 82.0, 122.5, 401.3333333333334, 26.0, 25.03085718300046, 0.3527494720919501, 0.0, 1.0, 18721.56247302251], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.82, 0.4083333333333333, 0.443462246777164, 0.6666666666666666, 0.5859047652500383, 0.6175831573639834, 0.0, 1.0, 0.08915029749058338], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.41878024], dtype=float32), -0.14530657]. 
=============================================
[2019-04-03 22:19:02,014] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.6828190e-11 4.8947707e-04 8.3510193e-10 1.3740349e-06 4.8715118e-04
 5.7988517e-11 9.9902189e-01], sum to 1.0000
[2019-04-03 22:19:02,017] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9981
[2019-04-03 22:19:02,078] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.95664519812016, 0.2058833686426854, 0.0, 1.0, 42392.8270665032], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 680400.0000, 
sim time next is 681000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.93353263979714, 0.1985390977415142, 0.0, 1.0, 42325.09253259456], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5777943866497616, 0.5661796992471714, 0.0, 1.0, 0.2015480596790217], 
reward next is 0.7985, 
noisyNet noise sample is [array([-0.75582176], dtype=float32), -0.86576194]. 
=============================================
[2019-04-03 22:19:02,087] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[77.2992 ]
 [77.35598]
 [77.44784]
 [77.4893 ]
 [77.5469 ]], R is [[77.24897003]
 [77.2746048 ]
 [77.29969025]
 [77.3243866 ]
 [77.34760284]].
[2019-04-03 22:19:10,635] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7065741e-12 2.1759239e-05 2.1120307e-11 8.4472418e-08 7.1325899e-06
 8.0582416e-13 9.9997103e-01], sum to 1.0000
[2019-04-03 22:19:10,635] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2733
[2019-04-03 22:19:10,701] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.45, 75.0, 32.0, 0.0, 26.0, 25.59155502017575, 0.31225846181947, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 808200.0000, 
sim time next is 808800.0000, 
raw observation next is [-6.366666666666667, 75.0, 36.83333333333333, 0.0, 26.0, 25.75384630888435, 0.3308070796805104, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.28624192059095105, 0.75, 0.12277777777777776, 0.0, 0.6666666666666666, 0.6461538590736957, 0.6102690265601701, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16119951], dtype=float32), -0.33461607]. 
=============================================
[2019-04-03 22:19:11,830] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9298712e-12 2.7845925e-04 1.7770457e-10 5.7638118e-07 1.4387348e-04
 1.8099005e-11 9.9957711e-01], sum to 1.0000
[2019-04-03 22:19:11,841] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0961
[2019-04-03 22:19:11,906] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.55, 63.5, 0.0, 0.0, 26.0, 24.89814836343407, 0.2185501404944946, 0.0, 1.0, 78031.08757712957], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 675000.0000, 
sim time next is 675600.0000, 
raw observation next is [-2.633333333333333, 64.0, 0.0, 0.0, 26.0, 24.92628056113482, 0.2215763537063617, 0.0, 1.0, 55345.10238962933], 
processed observation next is [0.0, 0.8260869565217391, 0.38965835641735924, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5771900467612351, 0.5738587845687873, 0.0, 1.0, 0.26354810661728256], 
reward next is 0.7365, 
noisyNet noise sample is [array([1.4165918], dtype=float32), -0.83619493]. 
=============================================
[2019-04-03 22:19:14,246] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.8755571e-13 1.1773956e-04 3.4725597e-11 2.5405794e-07 2.9091936e-05
 6.1383673e-13 9.9985290e-01], sum to 1.0000
[2019-04-03 22:19:14,249] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7703
[2019-04-03 22:19:14,285] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.18769880640989, 0.4001242837165144, 0.0, 1.0, 39583.81284539436], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 943800.0000, 
sim time next is 944400.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.19818455702357, 0.4018416767911137, 0.0, 1.0, 39412.90971112327], 
processed observation next is [1.0, 0.9565217391304348, 0.6011080332409973, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5998487130852975, 0.6339472255970379, 0.0, 1.0, 0.18768052243392033], 
reward next is 0.8123, 
noisyNet noise sample is [array([-0.39728627], dtype=float32), -0.26329386]. 
=============================================
[2019-04-03 22:19:15,565] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1326263e-12 9.4521429e-06 3.3468402e-11 1.6418134e-07 2.1740021e-05
 8.0266842e-13 9.9996865e-01], sum to 1.0000
[2019-04-03 22:19:15,565] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6341
[2019-04-03 22:19:15,596] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 60.5, 0.0, 0.0, 26.0, 24.99157940172093, 0.3063623675524723, 0.0, 1.0, 45527.65921248797], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 766200.0000, 
sim time next is 766800.0000, 
raw observation next is [-5.6, 61.0, 0.0, 0.0, 26.0, 24.96173175495727, 0.2976190865613015, 0.0, 1.0, 44597.48449791086], 
processed observation next is [1.0, 0.9130434782608695, 0.30747922437673136, 0.61, 0.0, 0.0, 0.6666666666666666, 0.5801443129131059, 0.5992063621871005, 0.0, 1.0, 0.21236897379957553], 
reward next is 0.7876, 
noisyNet noise sample is [array([-0.16850673], dtype=float32), 0.79128474]. 
=============================================
[2019-04-03 22:19:24,846] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5323853e-12 1.9007482e-04 4.6997323e-10 1.2813847e-06 6.4701313e-04
 1.6417654e-11 9.9916160e-01], sum to 1.0000
[2019-04-03 22:19:24,851] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9660
[2019-04-03 22:19:24,869] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7, 72.66666666666666, 0.0, 0.0, 26.0, 24.54759913151238, 0.1655153577887809, 0.0, 1.0, 39041.2890409379], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 881400.0000, 
sim time next is 882000.0000, 
raw observation next is [-0.6, 72.0, 0.0, 0.0, 26.0, 24.54184069899271, 0.1742938231775472, 0.0, 1.0, 39027.9729748359], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5451533915827259, 0.5580979410591824, 0.0, 1.0, 0.18584749035636144], 
reward next is 0.8142, 
noisyNet noise sample is [array([0.15611671], dtype=float32), 1.3444319]. 
=============================================
[2019-04-03 22:19:24,877] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[82.22088 ]
 [82.28932 ]
 [82.340126]
 [82.39983 ]
 [82.43881 ]], R is [[82.14382935]
 [82.13648224]
 [82.12921906]
 [82.12207031]
 [82.11508942]].
[2019-04-03 22:19:25,621] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.2455151e-10 7.5765856e-04 9.6330055e-10 1.0978176e-06 4.1674520e-04
 1.1730802e-10 9.9882454e-01], sum to 1.0000
[2019-04-03 22:19:25,625] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7823
[2019-04-03 22:19:25,630] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.71666666666667, 63.33333333333333, 163.6666666666667, 0.0, 26.0, 25.13644670983552, 0.5076462185348048, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1165800.0000, 
sim time next is 1166400.0000, 
raw observation next is [18.8, 63.0, 165.5, 0.0, 26.0, 25.12379322682811, 0.5062648368558045, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.9833795013850417, 0.63, 0.5516666666666666, 0.0, 0.6666666666666666, 0.5936494355690091, 0.6687549456186015, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33479813], dtype=float32), 0.3620009]. 
=============================================
[2019-04-03 22:19:26,692] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5948466e-11 6.9106491e-06 5.8297617e-11 5.2535650e-08 4.3323421e-06
 2.0982628e-12 9.9998879e-01], sum to 1.0000
[2019-04-03 22:19:26,693] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1610
[2019-04-03 22:19:26,700] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.33003090288422, 0.3182658079234335, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1206000.0000, 
sim time next is 1206600.0000, 
raw observation next is [16.51666666666667, 75.5, 0.0, 0.0, 26.0, 24.30236734999059, 0.3139630349902843, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 1.0, 0.9201292705447832, 0.755, 0.0, 0.0, 0.6666666666666666, 0.5251972791658824, 0.6046543449967614, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.071127], dtype=float32), -0.7655808]. 
=============================================
[2019-04-03 22:19:28,347] A3C_AGENT_WORKER-Thread-8 INFO:Evaluating...
[2019-04-03 22:19:28,348] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:19:28,349] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:19:28,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:19:28,349] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:19:28,350] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:19:28,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run4
[2019-04-03 22:19:28,352] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:19:28,369] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run4
[2019-04-03 22:19:28,389] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run4
[2019-04-03 22:20:24,632] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2101992], dtype=float32), 0.23505734]
[2019-04-03 22:20:24,633] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-9.5, 91.0, 0.0, 0.0, 26.0, 23.11923711002736, -0.1933500885695978, 0.0, 1.0, 44508.93037771605]
[2019-04-03 22:20:24,633] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 22:20:24,634] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.1517708e-10 8.8548783e-04 2.4188593e-09 9.5385246e-07 4.0594189e-04
 5.3237953e-10 9.9870753e-01], sampled 0.16666430688330303
[2019-04-03 22:21:58,917] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.0115 240027589.7957 1605.3639
[2019-04-03 22:22:31,346] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.8660 263368142.7523 1551.5776
[2019-04-03 22:22:35,575] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7611 275780177.1969 1233.1879
[2019-04-03 22:22:36,610] A3C_AGENT_WORKER-Thread-8 INFO:Global step: 300000, evaluation results [300000.0, 7241.86598689388, 263368142.75228876, 1551.5775533335125, 7353.011477163436, 240027589.79567444, 1605.3639186669802, 7182.76106096703, 275780177.1969239, 1233.1878816783853]
[2019-04-03 22:22:44,113] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.2392724e-13 7.0675324e-06 4.2491431e-12 9.7625321e-09 5.0693503e-05
 5.7810858e-13 9.9994230e-01], sum to 1.0000
[2019-04-03 22:22:44,113] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2408
[2019-04-03 22:22:44,124] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.9, 68.5, 0.0, 0.0, 26.0, 25.73871952960575, 0.6671212951123594, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1121400.0000, 
sim time next is 1122000.0000, 
raw observation next is [11.8, 69.33333333333333, 0.0, 0.0, 26.0, 25.752300421283, 0.6637756120423003, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.7894736842105264, 0.6933333333333332, 0.0, 0.0, 0.6666666666666666, 0.6460250351069167, 0.7212585373474334, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.150308], dtype=float32), -0.4166789]. 
=============================================
[2019-04-03 22:22:44,127] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[92.4982 ]
 [92.58904]
 [92.78373]
 [92.7301 ]
 [92.53061]], R is [[92.37402344]
 [92.45028687]
 [92.52578735]
 [92.5113678 ]
 [92.37711334]].
[2019-04-03 22:22:46,111] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0637096e-13 1.0730042e-04 7.8801479e-11 3.4435459e-08 6.5900604e-05
 4.1185787e-12 9.9982673e-01], sum to 1.0000
[2019-04-03 22:22:46,132] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7810
[2019-04-03 22:22:46,155] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.55346706099766, 0.5326105432012667, 0.0, 1.0, 18744.19154646739], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1315800.0000, 
sim time next is 1316400.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.4819746980034, 0.5365402490896706, 0.0, 1.0, 55763.4973781662], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6234978915002832, 0.6788467496965569, 0.0, 1.0, 0.26554046370555334], 
reward next is 0.7345, 
noisyNet noise sample is [array([-0.16993219], dtype=float32), 0.5855329]. 
=============================================
[2019-04-03 22:22:58,244] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0246099e-12 1.5040878e-04 4.1566018e-11 1.0766071e-07 3.2609343e-05
 7.7555002e-12 9.9981683e-01], sum to 1.0000
[2019-04-03 22:22:58,244] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5662
[2019-04-03 22:22:58,257] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.91666666666667, 68.33333333333333, 98.66666666666666, 0.0, 26.0, 25.60797127042032, 0.5625153461188096, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1158600.0000, 
sim time next is 1159200.0000, 
raw observation next is [17.2, 67.0, 106.5, 0.0, 26.0, 25.53300036541425, 0.5501755069607853, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.9390581717451525, 0.67, 0.355, 0.0, 0.6666666666666666, 0.6277500304511875, 0.6833918356535951, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51473975], dtype=float32), 0.7833333]. 
=============================================
[2019-04-03 22:23:01,581] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0136958e-14 1.0969776e-06 4.4971416e-13 2.5296920e-10 7.3740154e-07
 1.7287856e-14 9.9999821e-01], sum to 1.0000
[2019-04-03 22:23:01,611] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5173
[2019-04-03 22:23:01,625] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.6, 68.0, 85.0, 701.0, 26.0, 26.1517758328057, 0.6601149014286668, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1517400.0000, 
sim time next is 1518000.0000, 
raw observation next is [9.066666666666666, 66.33333333333334, 83.33333333333334, 694.6666666666667, 26.0, 26.38185424324445, 0.6845105898288927, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.713758079409049, 0.6633333333333334, 0.2777777777777778, 0.7675874769797423, 0.6666666666666666, 0.6984878536037042, 0.7281701966096309, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2682572], dtype=float32), 1.5377331]. 
=============================================
[2019-04-03 22:23:01,639] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.32755 ]
 [90.89709 ]
 [91.47179 ]
 [92.05981 ]
 [92.703636]], R is [[89.84503937]
 [89.94658661]
 [90.04711914]
 [90.14665222]
 [90.24518585]].
[2019-04-03 22:23:03,494] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7530775e-13 8.7648132e-06 2.0175760e-12 7.9801517e-09 1.8638410e-07
 5.7335977e-13 9.9999094e-01], sum to 1.0000
[2019-04-03 22:23:03,496] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9183
[2019-04-03 22:23:03,598] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 24.95891743551235, 0.5260664603625148, 0.0, 1.0, 117113.6733525194], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1369800.0000, 
sim time next is 1370400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.06887289108332, 0.5492013249300111, 0.0, 1.0, 68007.79781464477], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.58907274092361, 0.6830671083100036, 0.0, 1.0, 0.3238466562602132], 
reward next is 0.6762, 
noisyNet noise sample is [array([1.37283], dtype=float32), 0.97824293]. 
=============================================
[2019-04-03 22:23:19,243] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.3350775e-13 1.2684404e-07 1.3754635e-12 3.3827563e-09 1.0507035e-06
 5.6867529e-14 9.9999881e-01], sum to 1.0000
[2019-04-03 22:23:19,244] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9999
[2019-04-03 22:23:19,255] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.6, 68.0, 85.0, 701.0, 26.0, 26.15106923323026, 0.6599642642310207, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1517400.0000, 
sim time next is 1518000.0000, 
raw observation next is [9.066666666666666, 66.33333333333334, 83.33333333333334, 694.6666666666667, 26.0, 26.38115001220662, 0.684358547827825, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.713758079409049, 0.6633333333333334, 0.2777777777777778, 0.7675874769797423, 0.6666666666666666, 0.6984291676838851, 0.7281195159426083, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9777969], dtype=float32), -0.44918865]. 
=============================================
[2019-04-03 22:23:19,318] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[90.60901 ]
 [91.12313 ]
 [91.65147 ]
 [92.199905]
 [92.81274 ]], R is [[90.18527985]
 [90.28342438]
 [90.38059235]
 [90.47678375]
 [90.57201385]].
[2019-04-03 22:23:20,503] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3241356e-13 1.1181303e-04 9.4053463e-12 1.1312144e-08 5.5364585e-06
 5.2224570e-12 9.9988270e-01], sum to 1.0000
[2019-04-03 22:23:20,503] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5739
[2019-04-03 22:23:20,582] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 32.0, 0.0, 26.0, 25.90555192366789, 0.5149093283123346, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1414800.0000, 
sim time next is 1415400.0000, 
raw observation next is [-0.5, 99.16666666666667, 36.66666666666667, 0.0, 26.0, 25.96004918929946, 0.5069802509039095, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.44875346260387816, 0.9916666666666667, 0.12222222222222223, 0.0, 0.6666666666666666, 0.6633374324416218, 0.6689934169679699, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.057442], dtype=float32), -1.6289486]. 
=============================================
[2019-04-03 22:23:23,583] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.8568222e-11 1.7591101e-03 6.0724896e-09 5.2629053e-07 4.5291163e-04
 3.9263118e-10 9.9778736e-01], sum to 1.0000
[2019-04-03 22:23:23,587] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9952
[2019-04-03 22:23:23,722] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 85.66666666666667, 115.3333333333333, 0.0, 26.0, 24.90857816775353, 0.3460756405329682, 0.0, 1.0, 42096.36577851309], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1768800.0000, 
sim time next is 1769400.0000, 
raw observation next is [-2.3, 85.0, 119.0, 0.0, 26.0, 24.92737251724211, 0.3481257258758854, 0.0, 1.0, 32689.12305131619], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.85, 0.39666666666666667, 0.0, 0.6666666666666666, 0.577281043103509, 0.6160419086252952, 0.0, 1.0, 0.1556624907205533], 
reward next is 0.8443, 
noisyNet noise sample is [array([0.8272556], dtype=float32), 1.5459219]. 
=============================================
[2019-04-03 22:23:29,425] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5779956e-13 3.2206779e-06 7.8830215e-12 1.0199925e-08 3.3468837e-06
 3.6171166e-13 9.9999344e-01], sum to 1.0000
[2019-04-03 22:23:29,458] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0710
[2019-04-03 22:23:29,481] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 74.0, 0.0, 0.0, 26.0, 25.45992233334081, 0.6886929334818411, 0.0, 1.0, 42384.33540258616], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1544400.0000, 
sim time next is 1545000.0000, 
raw observation next is [7.516666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 25.87185070231836, 0.7030376087667171, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6708217913204063, 0.7433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6559875585265299, 0.7343458695889057, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3018752], dtype=float32), 0.36164913]. 
=============================================
[2019-04-03 22:23:29,537] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[87.06454]
 [87.13143]
 [86.47834]
 [85.88192]
 [85.43183]], R is [[87.16272736]
 [87.08927155]
 [86.39094543]
 [85.58220673]
 [84.7898941 ]].
[2019-04-03 22:23:36,900] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.1680210e-11 3.2378375e-04 6.8372974e-10 4.6048865e-07 5.7111291e-05
 2.6747884e-10 9.9961865e-01], sum to 1.0000
[2019-04-03 22:23:36,900] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4691
[2019-04-03 22:23:36,989] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.2, 86.33333333333333, 75.66666666666666, 0.0, 26.0, 24.88781226523171, 0.3279426747770748, 0.0, 1.0, 70145.48939786661], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1763400.0000, 
sim time next is 1764000.0000, 
raw observation next is [-2.3, 87.0, 81.0, 0.0, 26.0, 24.88793870203348, 0.3334178595820632, 0.0, 1.0, 56703.60914417369], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.27, 0.0, 0.6666666666666666, 0.5739948918361234, 0.6111392865273544, 0.0, 1.0, 0.2700171864008271], 
reward next is 0.7300, 
noisyNet noise sample is [array([-0.557956], dtype=float32), -1.1798203]. 
=============================================
[2019-04-03 22:23:36,993] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[77.423325]
 [77.232216]
 [76.84319 ]
 [76.746216]
 [76.90408 ]], R is [[77.53709412]
 [77.42770386]
 [77.16374207]
 [77.15273285]
 [77.3812027 ]].
[2019-04-03 22:23:42,627] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4499592e-11 6.4069145e-06 4.1445496e-11 1.6424490e-07 9.5452233e-05
 5.5719838e-12 9.9989796e-01], sum to 1.0000
[2019-04-03 22:23:42,631] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1507
[2019-04-03 22:23:42,701] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.1, 66.66666666666667, 230.3333333333333, 8.0, 26.0, 25.6370799527529, 0.3284211426517152, 1.0, 1.0, 60061.81734862381], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1943400.0000, 
sim time next is 1944000.0000, 
raw observation next is [-5.0, 65.0, 229.5, 7.0, 26.0, 25.68891559616228, 0.344928611799167, 1.0, 1.0, 38800.64245352406], 
processed observation next is [1.0, 0.5217391304347826, 0.32409972299168976, 0.65, 0.765, 0.0077348066298342545, 0.6666666666666666, 0.6407429663468566, 0.6149762039330556, 1.0, 1.0, 0.18476496406440027], 
reward next is 0.8152, 
noisyNet noise sample is [array([-1.4719024], dtype=float32), -0.26996654]. 
=============================================
[2019-04-03 22:23:42,712] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[74.99292 ]
 [75.27241 ]
 [75.32473 ]
 [75.7309  ]
 [76.097145]], R is [[74.79077911]
 [74.75685883]
 [74.57781219]
 [74.83203125]
 [75.08370972]].
[2019-04-03 22:23:44,264] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.2769030e-12 7.1681548e-06 2.9472608e-10 2.5956891e-07 1.2413622e-04
 9.2153689e-12 9.9986851e-01], sum to 1.0000
[2019-04-03 22:23:44,272] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3369
[2019-04-03 22:23:44,328] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 62.0, 59.33333333333334, 0.0, 26.0, 25.60659880389239, 0.319918130445177, 1.0, 1.0, 27034.59980943718], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1957800.0000, 
sim time next is 1958400.0000, 
raw observation next is [-2.8, 62.0, 52.0, 0.0, 26.0, 25.55525722264509, 0.3159812957444894, 1.0, 1.0, 26354.14750232171], 
processed observation next is [1.0, 0.6956521739130435, 0.38504155124653744, 0.62, 0.17333333333333334, 0.0, 0.6666666666666666, 0.6296047685537575, 0.6053270985814965, 1.0, 1.0, 0.12549594048724624], 
reward next is 0.8745, 
noisyNet noise sample is [array([-0.62848985], dtype=float32), -1.1817126]. 
=============================================
[2019-04-03 22:24:18,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3613707e-12 3.2804972e-05 1.8157886e-10 3.2151096e-08 1.8160937e-06
 9.5225928e-12 9.9996543e-01], sum to 1.0000
[2019-04-03 22:24:18,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7838
[2019-04-03 22:24:18,203] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.19877649579291, 0.09549846359814428, 0.0, 1.0, 43475.66519349427], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2091600.0000, 
sim time next is 2092200.0000, 
raw observation next is [-6.283333333333333, 86.33333333333333, 0.0, 0.0, 26.0, 24.11543004374047, 0.1041188083931322, 0.0, 1.0, 43822.83791849922], 
processed observation next is [1.0, 0.21739130434782608, 0.288550323176362, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5096191703117059, 0.5347062694643774, 0.0, 1.0, 0.208680180564282], 
reward next is 0.7913, 
noisyNet noise sample is [array([-1.837195], dtype=float32), -0.30654222]. 
=============================================
[2019-04-03 22:24:18,577] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7526822e-11 5.6150868e-05 9.7530671e-11 3.7028360e-07 2.0322266e-05
 5.9801266e-12 9.9992323e-01], sum to 1.0000
[2019-04-03 22:24:18,577] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8935
[2019-04-03 22:24:18,730] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 69.5, 293.0, 101.0, 26.0, 25.79712855094739, 0.4347514694816823, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2115000.0000, 
sim time next is 2115600.0000, 
raw observation next is [-6.9, 67.66666666666667, 269.3333333333334, 106.5, 26.0, 25.88229925623976, 0.4410469741934289, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.27146814404432135, 0.6766666666666667, 0.8977777777777781, 0.11767955801104972, 0.6666666666666666, 0.6568582713533134, 0.6470156580644763, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.71097213], dtype=float32), -0.27611724]. 
=============================================
[2019-04-03 22:24:40,685] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.3252472e-13 1.0085462e-06 1.8097796e-11 1.0008952e-08 4.0227229e-07
 1.1187214e-12 9.9999857e-01], sum to 1.0000
[2019-04-03 22:24:40,685] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5561
[2019-04-03 22:24:40,705] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.616666666666667, 77.5, 0.0, 0.0, 26.0, 24.5123774053179, 0.2104150795076331, 0.0, 1.0, 44181.98174070633], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2245800.0000, 
sim time next is 2246400.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.46277649377204, 0.2002235510640354, 0.0, 1.0, 44246.38144604994], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5385647078143366, 0.5667411836880117, 0.0, 1.0, 0.21069705450499973], 
reward next is 0.7893, 
noisyNet noise sample is [array([-0.0327334], dtype=float32), -1.36089]. 
=============================================
[2019-04-03 22:24:51,636] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0731047e-12 7.6616798e-06 4.8383437e-12 3.5044120e-08 5.2422201e-06
 3.3192503e-13 9.9998701e-01], sum to 1.0000
[2019-04-03 22:24:51,653] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3182
[2019-04-03 22:24:51,681] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 29.0, 129.0, 325.6666666666667, 26.0, 25.76414092336771, 0.3880765941309394, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2560200.0000, 
sim time next is 2560800.0000, 
raw observation next is [3.3, 29.0, 121.5, 338.3333333333333, 26.0, 25.81803044047701, 0.3893948985966237, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.554016620498615, 0.29, 0.405, 0.3738489871086556, 0.6666666666666666, 0.6515025367064174, 0.6297982995322079, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8694967], dtype=float32), 1.464952]. 
=============================================
[2019-04-03 22:24:59,367] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1934785e-10 2.2144952e-05 8.5170226e-10 2.4474710e-07 3.3591201e-05
 9.3054633e-11 9.9994397e-01], sum to 1.0000
[2019-04-03 22:24:59,367] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7157
[2019-04-03 22:24:59,419] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.75, 42.5, 0.0, 0.0, 26.0, 24.89207615363606, 0.2125057653100292, 0.0, 1.0, 42991.61512595251], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2413800.0000, 
sim time next is 2414400.0000, 
raw observation next is [-4.833333333333333, 42.0, 0.0, 0.0, 26.0, 24.84119686952986, 0.2029527543216852, 0.0, 1.0, 43015.9288699974], 
processed observation next is [0.0, 0.9565217391304348, 0.3287165281625116, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5700997391274883, 0.5676509181072283, 0.0, 1.0, 0.20483775652379713], 
reward next is 0.7952, 
noisyNet noise sample is [array([-0.17796996], dtype=float32), 1.2745713]. 
=============================================
[2019-04-03 22:25:15,874] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.0339303e-13 2.4415237e-06 2.0651833e-12 1.5971901e-08 9.2870932e-06
 8.4295594e-14 9.9998820e-01], sum to 1.0000
[2019-04-03 22:25:15,874] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6516
[2019-04-03 22:25:15,911] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 50.0, 115.0, 165.0, 26.0, 25.96417183818371, 0.4595132589688102, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2649600.0000, 
sim time next is 2650200.0000, 
raw observation next is [0.5, 50.0, 101.6666666666667, 151.3333333333333, 26.0, 25.98012332495362, 0.4607374263144486, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4764542936288089, 0.5, 0.338888888888889, 0.16721915285451192, 0.6666666666666666, 0.6650102770794684, 0.6535791421048162, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43032306], dtype=float32), -0.29205716]. 
=============================================
[2019-04-03 22:25:25,726] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.1036100e-13 2.8145889e-06 2.7834308e-11 1.3127769e-08 8.0184072e-06
 1.2066234e-13 9.9998915e-01], sum to 1.0000
[2019-04-03 22:25:25,727] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2034
[2019-04-03 22:25:25,772] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0549925e-12 2.0693828e-06 1.6504284e-10 2.2759455e-08 6.9060320e-06
 3.6718705e-12 9.9999094e-01], sum to 1.0000
[2019-04-03 22:25:25,779] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 56.5, 0.0, 0.0, 26.0, 25.47485604794691, 0.4137520988388615, 1.0, 1.0, 52124.60234094736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2745000.0000, 
sim time next is 2745600.0000, 
raw observation next is [-4.666666666666666, 57.33333333333333, 0.0, 0.0, 26.0, 25.25423916166563, 0.4006010399475587, 1.0, 1.0, 52984.82599257762], 
processed observation next is [1.0, 0.782608695652174, 0.33333333333333337, 0.5733333333333333, 0.0, 0.0, 0.6666666666666666, 0.6045199301388026, 0.6335336799825195, 1.0, 1.0, 0.25230869520275057], 
reward next is 0.7477, 
noisyNet noise sample is [array([0.9879802], dtype=float32), 0.72265476]. 
=============================================
[2019-04-03 22:25:25,781] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2855
[2019-04-03 22:25:25,795] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 72.5, 0.0, 0.0, 26.0, 24.37663389689201, 0.1750374253472832, 0.0, 1.0, 44440.30726646776], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2683800.0000, 
sim time next is 2684400.0000, 
raw observation next is [-10.33333333333333, 73.66666666666667, 0.0, 0.0, 26.0, 24.28332620344689, 0.1594221239523938, 0.0, 1.0, 44468.22600786677], 
processed observation next is [1.0, 0.043478260869565216, 0.17636195752539252, 0.7366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5236105169539075, 0.5531407079841313, 0.0, 1.0, 0.21175345718031793], 
reward next is 0.7882, 
noisyNet noise sample is [array([1.3567177], dtype=float32), 0.06835887]. 
=============================================
[2019-04-03 22:25:37,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4320471e-12 5.7095099e-06 7.2029632e-11 6.5667940e-08 3.3184562e-05
 2.1908742e-12 9.9996114e-01], sum to 1.0000
[2019-04-03 22:25:37,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0168
[2019-04-03 22:25:37,408] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 53.0, 0.0, 0.0, 26.0, 25.33244246956481, 0.3596539664893707, 0.0, 1.0, 87985.96137907643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2845800.0000, 
sim time next is 2846400.0000, 
raw observation next is [2.0, 56.0, 0.0, 0.0, 26.0, 25.25811797043865, 0.3573963521718664, 0.0, 1.0, 64486.86042100425], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.56, 0.0, 0.0, 0.6666666666666666, 0.6048431642032209, 0.6191321173906221, 0.0, 1.0, 0.30708028771906787], 
reward next is 0.6929, 
noisyNet noise sample is [array([-0.09957619], dtype=float32), 0.13186911]. 
=============================================
[2019-04-03 22:25:38,502] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3868758e-11 8.1767776e-04 9.4039199e-10 2.0087886e-07 1.0021412e-04
 1.1479209e-10 9.9908197e-01], sum to 1.0000
[2019-04-03 22:25:38,502] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9956
[2019-04-03 22:25:38,541] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.09920079711095, 0.0222279845671012, 0.0, 1.0, 56215.19025214394], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2788200.0000, 
sim time next is 2788800.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 23.93957326101369, 0.006023015746727899, 0.0, 1.0, 60583.09291889046], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.49496443841780763, 0.5020076719155759, 0.0, 1.0, 0.28849091866138316], 
reward next is 0.7115, 
noisyNet noise sample is [array([-0.11711025], dtype=float32), 0.90443563]. 
=============================================
[2019-04-03 22:25:44,309] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.3933112e-12 2.0021474e-05 3.7950580e-11 5.3215238e-08 4.5450586e-05
 2.6164331e-12 9.9993455e-01], sum to 1.0000
[2019-04-03 22:25:44,309] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9985
[2019-04-03 22:25:44,328] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.88039048743001, 0.3311491436989361, 0.0, 1.0, 43297.71681390614], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2941200.0000, 
sim time next is 2941800.0000, 
raw observation next is [-2.0, 85.00000000000001, 0.0, 0.0, 26.0, 24.85783820094465, 0.3229187708264029, 0.0, 1.0, 43289.12569314575], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.8500000000000001, 0.0, 0.0, 0.6666666666666666, 0.5714865167453874, 0.6076395902754677, 0.0, 1.0, 0.2061386937768845], 
reward next is 0.7939, 
noisyNet noise sample is [array([-0.20503668], dtype=float32), 0.8170208]. 
=============================================
[2019-04-03 22:25:55,696] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.43980421e-12 1.20658275e-04 1.63056263e-10 1.10981304e-07
 3.85923689e-04 2.18738309e-11 9.99493241e-01], sum to 1.0000
[2019-04-03 22:25:55,729] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6704
[2019-04-03 22:25:55,746] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666666, 100.0, 0.0, 0.0, 26.0, 25.30081562123873, 0.3191802020628556, 0.0, 1.0, 39327.89554491204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3112800.0000, 
sim time next is 3113400.0000, 
raw observation next is [0.8333333333333334, 100.0, 0.0, 0.0, 26.0, 25.27188218654041, 0.3352986684554417, 0.0, 1.0, 39536.00093918037], 
processed observation next is [1.0, 0.0, 0.4856879039704525, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6059901822117008, 0.6117662228184806, 0.0, 1.0, 0.18826667113895415], 
reward next is 0.8117, 
noisyNet noise sample is [array([-0.42595017], dtype=float32), 0.20958604]. 
=============================================
[2019-04-03 22:26:03,922] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6247323e-11 1.3764678e-04 3.4599773e-10 1.3982620e-07 3.8712482e-05
 2.8100607e-11 9.9982351e-01], sum to 1.0000
[2019-04-03 22:26:03,923] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9794
[2019-04-03 22:26:03,936] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.7659527965481, 0.2240457100982016, 0.0, 1.0, 42771.54643831423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3391200.0000, 
sim time next is 3391800.0000, 
raw observation next is [-3.0, 60.83333333333333, 0.0, 0.0, 26.0, 24.72220044336211, 0.2178222198129289, 0.0, 1.0, 42843.21904267136], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6083333333333333, 0.0, 0.0, 0.6666666666666666, 0.5601833702801757, 0.5726074066043096, 0.0, 1.0, 0.2040153287746255], 
reward next is 0.7960, 
noisyNet noise sample is [array([-1.5106167], dtype=float32), 0.35131177]. 
=============================================
[2019-04-03 22:26:05,706] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.8555454e-13 1.7864336e-04 4.9441021e-11 3.0670524e-08 2.6201624e-05
 2.5027584e-12 9.9979514e-01], sum to 1.0000
[2019-04-03 22:26:05,706] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6501
[2019-04-03 22:26:05,722] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.1, 100.0, 0.0, 0.0, 26.0, 25.3610934262762, 0.2978959430392837, 0.0, 1.0, 99063.11249745169], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3121800.0000, 
sim time next is 3122400.0000, 
raw observation next is [2.2, 100.0, 0.0, 0.0, 26.0, 25.29859944431307, 0.2982146946018633, 0.0, 1.0, 77832.2631185539], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6082166203594225, 0.5994048982006212, 0.0, 1.0, 0.3706298243740662], 
reward next is 0.6294, 
noisyNet noise sample is [array([0.56525874], dtype=float32), 2.7349749]. 
=============================================
[2019-04-03 22:26:11,146] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1257751e-13 6.5136521e-07 6.0079518e-13 1.3768868e-09 1.6706019e-06
 8.2636915e-15 9.9999774e-01], sum to 1.0000
[2019-04-03 22:26:11,149] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2758
[2019-04-03 22:26:11,178] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 92.66666666666666, 718.3333333333333, 26.0, 26.81057575383308, 0.627011604644984, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3510600.0000, 
sim time next is 3511200.0000, 
raw observation next is [3.0, 49.0, 90.33333333333334, 702.6666666666666, 26.0, 26.24733309050979, 0.6691645804550288, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.30111111111111116, 0.776427255985267, 0.6666666666666666, 0.6872777575424825, 0.7230548601516763, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49884868], dtype=float32), -1.3078936]. 
=============================================
[2019-04-03 22:26:12,383] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6421656e-11 8.4313950e-05 2.6199547e-09 1.2313131e-06 6.3037820e-05
 7.2343471e-11 9.9985135e-01], sum to 1.0000
[2019-04-03 22:26:12,386] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3234
[2019-04-03 22:26:12,409] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.94824125736911, 0.3586935390864376, 0.0, 1.0, 43830.6488363183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3290400.0000, 
sim time next is 3291000.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.97960274913984, 0.349248875779577, 0.0, 1.0, 43816.29087330391], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.58163356242832, 0.6164162919265257, 0.0, 1.0, 0.20864900415859003], 
reward next is 0.7914, 
noisyNet noise sample is [array([0.27612013], dtype=float32), 0.89068395]. 
=============================================
[2019-04-03 22:26:12,415] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[70.357346]
 [70.55107 ]
 [70.6934  ]
 [70.92379 ]
 [71.125824]], R is [[70.28064728]
 [70.36912537]
 [70.45665741]
 [70.54341888]
 [70.62949371]].
[2019-04-03 22:26:14,870] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.5435041e-11 1.5644811e-04 2.0736206e-10 1.4133565e-07 5.9138845e-05
 4.4268571e-11 9.9978429e-01], sum to 1.0000
[2019-04-03 22:26:14,870] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3356
[2019-04-03 22:26:14,944] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.5, 70.0, 88.0, 425.0, 26.0, 24.5830002618378, 0.4092911304282303, 0.0, 1.0, 202112.1114429946], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3573000.0000, 
sim time next is 3573600.0000, 
raw observation next is [-6.333333333333334, 70.0, 90.0, 466.8333333333333, 26.0, 25.33973367715206, 0.4859278165059229, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.28716528162511545, 0.7, 0.3, 0.5158379373848987, 0.6666666666666666, 0.6116444730960051, 0.6619759388353076, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8403882], dtype=float32), 0.058535963]. 
=============================================
[2019-04-03 22:26:19,387] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.9037228e-12 1.6004797e-05 7.6375212e-11 3.9373557e-08 1.2359690e-04
 1.5386597e-11 9.9986029e-01], sum to 1.0000
[2019-04-03 22:26:19,391] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7188
[2019-04-03 22:26:19,400] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 48.33333333333334, 0.0, 0.0, 26.0, 25.31555918234606, 0.3705884339922544, 0.0, 1.0, 38480.7057384359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3626400.0000, 
sim time next is 3627000.0000, 
raw observation next is [3.0, 42.5, 0.0, 0.0, 26.0, 25.35789187000995, 0.3802126218013648, 0.0, 1.0, 38351.87457138798], 
processed observation next is [0.0, 1.0, 0.5457063711911359, 0.425, 0.0, 0.0, 0.6666666666666666, 0.6131576558341626, 0.626737540600455, 0.0, 1.0, 0.18262797414946658], 
reward next is 0.8174, 
noisyNet noise sample is [array([-0.9397595], dtype=float32), -0.053404126]. 
=============================================
[2019-04-03 22:26:19,428] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[76.64601]
 [76.56737]
 [76.48454]
 [76.44859]
 [76.46834]], R is [[76.82051849]
 [76.86907196]
 [76.91668701]
 [76.9619751 ]
 [77.00060272]].
[2019-04-03 22:26:20,160] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4977274e-10 1.5057922e-04 1.4873005e-09 7.9796411e-07 5.1802183e-03
 2.6484315e-10 9.9466830e-01], sum to 1.0000
[2019-04-03 22:26:20,163] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2076
[2019-04-03 22:26:20,186] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.48843056740702, 0.4314322263140539, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3615600.0000, 
sim time next is 3616200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.53639219362175, 0.4252793661662677, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6280326828018126, 0.6417597887220893, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.51831794], dtype=float32), -0.6517187]. 
=============================================
[2019-04-03 22:26:28,585] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.08081936e-11 8.34940420e-06 1.12561196e-10 9.86664190e-08
 7.92435458e-05 1.38939841e-11 9.99912262e-01], sum to 1.0000
[2019-04-03 22:26:28,588] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0153
[2019-04-03 22:26:28,614] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 25.33762305770028, 0.366918273532004, 0.0, 1.0, 38958.54952243254], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3625200.0000, 
sim time next is 3625800.0000, 
raw observation next is [-1.0, 54.16666666666666, 0.0, 0.0, 26.0, 25.31683977871457, 0.3673328777148704, 0.0, 1.0, 38573.39344987871], 
processed observation next is [0.0, 1.0, 0.4349030470914128, 0.5416666666666665, 0.0, 0.0, 0.6666666666666666, 0.6097366482262142, 0.6224442925716235, 0.0, 1.0, 0.1836828259518034], 
reward next is 0.8163, 
noisyNet noise sample is [array([-0.29093704], dtype=float32), 0.32474]. 
=============================================
[2019-04-03 22:26:31,176] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9800342e-11 1.7119208e-05 6.7398437e-10 1.5261813e-07 2.6977679e-04
 6.6085221e-11 9.9971288e-01], sum to 1.0000
[2019-04-03 22:26:31,176] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5943
[2019-04-03 22:26:31,194] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 25.26374498708966, 0.4227878648936662, 0.0, 1.0, 41341.73470651767], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3549000.0000, 
sim time next is 3549600.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.24262389058011, 0.425349921330435, 0.0, 1.0, 41139.67658168299], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6035519908816758, 0.641783307110145, 0.0, 1.0, 0.19590322181753803], 
reward next is 0.8041, 
noisyNet noise sample is [array([2.0225263], dtype=float32), -0.1113684]. 
=============================================
[2019-04-03 22:26:31,280] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.2881602e-11 2.0278131e-03 3.5523165e-10 1.1396928e-07 1.7815779e-04
 1.3981083e-10 9.9779391e-01], sum to 1.0000
[2019-04-03 22:26:31,288] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7431
[2019-04-03 22:26:31,356] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.166666666666667, 65.83333333333334, 103.6666666666667, 703.3333333333334, 26.0, 25.81619452879779, 0.5144092009047577, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3577800.0000, 
sim time next is 3578400.0000, 
raw observation next is [-5.0, 65.0, 105.5, 717.0, 26.0, 25.76153559785025, 0.5073627831216948, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.32409972299168976, 0.65, 0.3516666666666667, 0.7922651933701658, 0.6666666666666666, 0.6467946331541876, 0.6691209277072315, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3231826], dtype=float32), 1.0085715]. 
=============================================
[2019-04-03 22:26:32,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5648568e-11 2.0489706e-06 6.7477461e-11 3.7303693e-08 1.1126412e-05
 8.6504354e-12 9.9998677e-01], sum to 1.0000
[2019-04-03 22:26:32,828] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7354
[2019-04-03 22:26:32,848] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 50.83333333333334, 116.6666666666667, 819.3333333333334, 26.0, 25.19519511209842, 0.4504845800067681, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3588600.0000, 
sim time next is 3589200.0000, 
raw observation next is [-2.0, 50.0, 116.0, 817.5, 26.0, 25.19882337245548, 0.4516525351215623, 0.0, 1.0, 18703.67321859777], 
processed observation next is [0.0, 0.5652173913043478, 0.40720221606648205, 0.5, 0.38666666666666666, 0.9033149171270718, 0.6666666666666666, 0.5999019477046232, 0.6505508450405207, 0.0, 1.0, 0.08906511056475129], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.13295126], dtype=float32), -0.696508]. 
=============================================
[2019-04-03 22:26:34,292] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6680194e-12 2.1924255e-04 1.1725087e-11 2.9189115e-08 2.2730588e-05
 8.3473782e-13 9.9975795e-01], sum to 1.0000
[2019-04-03 22:26:34,294] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5426
[2019-04-03 22:26:34,310] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 42.33333333333334, 113.6666666666667, 819.8333333333334, 26.0, 25.29060484830892, 0.4541529935182582, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3676800.0000, 
sim time next is 3677400.0000, 
raw observation next is [5.5, 42.5, 113.0, 818.0, 26.0, 25.29173649857264, 0.4550744729564339, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6149584487534627, 0.425, 0.37666666666666665, 0.9038674033149171, 0.6666666666666666, 0.6076447082143867, 0.651691490985478, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4277883], dtype=float32), 0.8081384]. 
=============================================
[2019-04-03 22:26:38,011] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2752330e-13 1.3327724e-07 7.5085632e-12 7.0808968e-09 1.7401297e-06
 1.6698607e-13 9.9999809e-01], sum to 1.0000
[2019-04-03 22:26:38,019] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8878
[2019-04-03 22:26:38,047] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 35.33333333333334, 69.66666666666666, 567.3333333333334, 26.0, 26.91382889416571, 0.4397512478970909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3946800.0000, 
sim time next is 3947400.0000, 
raw observation next is [-4.5, 36.0, 66.0, 536.0, 26.0, 26.78934786911109, 0.7022364895340824, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.36, 0.22, 0.5922651933701657, 0.6666666666666666, 0.7324456557592575, 0.7340788298446941, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.77805525], dtype=float32), 0.023958502]. 
=============================================
[2019-04-03 22:26:40,559] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4038811e-12 8.6985938e-06 8.8413533e-11 2.4108516e-08 5.7045872e-07
 2.9033035e-12 9.9999070e-01], sum to 1.0000
[2019-04-03 22:26:40,564] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4225
[2019-04-03 22:26:40,588] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 77.0, 0.0, 0.0, 26.0, 25.45882093859746, 0.5101841039214978, 0.0, 1.0, 76940.27959612344], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3794400.0000, 
sim time next is 3795000.0000, 
raw observation next is [-3.0, 76.0, 0.0, 0.0, 26.0, 25.43699107017922, 0.4676720030737827, 0.0, 1.0, 68446.57649030015], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6197492558482685, 0.6558906676912609, 0.0, 1.0, 0.3259360785252388], 
reward next is 0.6741, 
noisyNet noise sample is [array([1.1943661], dtype=float32), -0.1564214]. 
=============================================
[2019-04-03 22:26:40,614] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.703384]
 [74.07066 ]
 [73.61978 ]
 [73.28395 ]
 [73.22763 ]], R is [[75.18173981]
 [75.06354523]
 [74.98971558]
 [75.06619263]
 [75.31552887]].
[2019-04-03 22:26:41,860] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.8941545e-13 2.3045939e-06 1.9892488e-12 1.2186435e-08 5.3504623e-06
 4.5102924e-13 9.9999237e-01], sum to 1.0000
[2019-04-03 22:26:41,871] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6477
[2019-04-03 22:26:41,904] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 60.0, 107.0, 743.3333333333334, 26.0, 26.46894131479482, 0.6026011789720924, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3838200.0000, 
sim time next is 3838800.0000, 
raw observation next is [-1.666666666666667, 60.00000000000001, 108.5, 759.1666666666667, 26.0, 26.51474528954822, 0.6138479954821133, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4164358264081256, 0.6000000000000001, 0.3616666666666667, 0.8388581952117865, 0.6666666666666666, 0.7095621074623516, 0.7046159984940378, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.407064], dtype=float32), 1.176272]. 
=============================================
[2019-04-03 22:26:48,741] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4012233e-12 1.4903216e-05 9.2177543e-11 1.5842005e-08 2.2288448e-05
 3.6322861e-12 9.9996281e-01], sum to 1.0000
[2019-04-03 22:26:48,742] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9277
[2019-04-03 22:26:48,767] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 71.0, 0.0, 0.0, 26.0, 25.21310398357436, 0.3412854306715429, 0.0, 1.0, 41096.08143402708], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3907200.0000, 
sim time next is 3907800.0000, 
raw observation next is [-5.0, 68.0, 0.0, 0.0, 26.0, 25.15070205857801, 0.3248468359466415, 0.0, 1.0, 41238.96076216097], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.68, 0.0, 0.0, 0.6666666666666666, 0.595891838214834, 0.6082822786488805, 0.0, 1.0, 0.19637600362933794], 
reward next is 0.8036, 
noisyNet noise sample is [array([0.6928429], dtype=float32), -0.6778521]. 
=============================================
[2019-04-03 22:26:51,087] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.8499248e-11 1.3458289e-05 1.0123268e-09 1.6897228e-07 4.4171778e-05
 1.7733903e-10 9.9994218e-01], sum to 1.0000
[2019-04-03 22:26:51,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8280
[2019-04-03 22:26:51,103] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.0, 66.0, 0.0, 0.0, 26.0, 23.70026795885194, 0.00801126805090649, 0.0, 1.0, 43787.43261170352], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3994200.0000, 
sim time next is 3994800.0000, 
raw observation next is [-13.0, 65.0, 0.0, 0.0, 26.0, 23.62336415586986, -0.007242373483682761, 0.0, 1.0, 43757.37639546395], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.65, 0.0, 0.0, 0.6666666666666666, 0.4686136796558218, 0.49758587550543903, 0.0, 1.0, 0.2083684590260188], 
reward next is 0.7916, 
noisyNet noise sample is [array([-0.5189956], dtype=float32), 2.5671637]. 
=============================================
[2019-04-03 22:26:52,196] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.5166833e-10 6.9487491e-04 3.3193881e-10 3.2003376e-07 2.7480963e-04
 8.3375071e-11 9.9902999e-01], sum to 1.0000
[2019-04-03 22:26:52,196] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6023
[2019-04-03 22:26:52,231] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 35.0, 113.0, 755.0, 26.0, 25.25486275987633, 0.3957936463478612, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4184400.0000, 
sim time next is 4185000.0000, 
raw observation next is [-1.5, 35.0, 114.0, 774.0, 26.0, 25.24754863239799, 0.3935908478723674, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.4210526315789474, 0.35, 0.38, 0.8552486187845304, 0.6666666666666666, 0.6039623860331659, 0.6311969492907892, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0666273], dtype=float32), -0.8598968]. 
=============================================
[2019-04-03 22:26:52,234] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[82.97965 ]
 [83.07584 ]
 [83.04169 ]
 [83.03438 ]
 [83.066895]], R is [[83.10829926]
 [83.27721405]
 [83.44444275]
 [83.61000061]
 [83.77390289]].
[2019-04-03 22:26:53,322] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4003908e-11 3.8933833e-04 9.4484948e-11 1.1945549e-07 5.8752488e-05
 2.6963464e-11 9.9955183e-01], sum to 1.0000
[2019-04-03 22:26:53,323] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6733
[2019-04-03 22:26:53,343] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 30.0, 118.5, 834.5, 26.0, 25.10590439556877, 0.3950250270600444, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4190400.0000, 
sim time next is 4191000.0000, 
raw observation next is [1.166666666666667, 30.66666666666666, 118.3333333333333, 838.6666666666667, 26.0, 25.14180591219151, 0.3977330248319351, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.49492151431209613, 0.3066666666666666, 0.3944444444444443, 0.9267034990791898, 0.6666666666666666, 0.595150492682626, 0.6325776749439783, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6168677], dtype=float32), 0.6120779]. 
=============================================
[2019-04-03 22:26:53,351] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[83.450836]
 [83.45191 ]
 [83.273796]
 [83.330826]
 [83.3352  ]], R is [[83.60797882]
 [83.77189636]
 [83.78522491]
 [83.8511734 ]
 [84.01266479]].
[2019-04-03 22:27:05,150] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7570962e-11 7.9343590e-05 3.1124789e-10 1.2981766e-07 9.3189963e-05
 3.5951575e-11 9.9982733e-01], sum to 1.0000
[2019-04-03 22:27:05,152] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2957
[2019-04-03 22:27:05,163] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 39.0, 0.0, 0.0, 26.0, 25.38941200165718, 0.4249194526689545, 0.0, 1.0, 43759.15916760438], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4150800.0000, 
sim time next is 4151400.0000, 
raw observation next is [-1.166666666666667, 40.16666666666666, 0.0, 0.0, 26.0, 25.35962916969284, 0.4191638104431289, 0.0, 1.0, 51457.82841894306], 
processed observation next is [0.0, 0.043478260869565216, 0.43028624192059095, 0.40166666666666656, 0.0, 0.0, 0.6666666666666666, 0.6133024308077367, 0.6397212701477096, 0.0, 1.0, 0.24503727818544316], 
reward next is 0.7550, 
noisyNet noise sample is [array([2.1392055], dtype=float32), 0.8083228]. 
=============================================
[2019-04-03 22:27:10,316] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.12996504e-13 1.05658974e-05 1.30417534e-11 2.31147030e-08
 2.65033777e-05 7.11326939e-13 9.99962926e-01], sum to 1.0000
[2019-04-03 22:27:10,316] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2472
[2019-04-03 22:27:10,330] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 47.0, 264.0, 113.0, 26.0, 26.24306380271662, 0.607249771004142, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4545000.0000, 
sim time next is 4545600.0000, 
raw observation next is [3.0, 46.33333333333334, 245.5, 96.16666666666667, 26.0, 26.33292570271053, 0.619845780214018, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.46333333333333343, 0.8183333333333334, 0.10626151012891345, 0.6666666666666666, 0.6944104752258774, 0.7066152600713393, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11612133], dtype=float32), 0.2704333]. 
=============================================
[2019-04-03 22:27:11,262] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.9152623e-14 5.5520777e-06 1.3496044e-11 2.8769986e-09 3.1582018e-05
 1.4247574e-14 9.9996281e-01], sum to 1.0000
[2019-04-03 22:27:11,262] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4371
[2019-04-03 22:27:11,270] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.46666666666667, 58.66666666666666, 0.0, 0.0, 26.0, 26.8387884990618, 0.8398759333643487, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4394400.0000, 
sim time next is 4395000.0000, 
raw observation next is [10.33333333333333, 58.83333333333334, 0.0, 0.0, 26.0, 26.81330639738505, 0.8296542632863207, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7488457987072946, 0.5883333333333334, 0.0, 0.0, 0.6666666666666666, 0.7344421997820874, 0.7765514210954403, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1913117], dtype=float32), 0.073568515]. 
=============================================
[2019-04-03 22:27:11,289] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[89.972046]
 [91.61651 ]
 [93.2076  ]
 [95.77726 ]
 [95.26294 ]], R is [[89.22570801]
 [89.33345032]
 [89.44011688]
 [89.54571533]
 [89.65026093]].
[2019-04-03 22:27:13,163] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3547101e-13 5.1083498e-06 7.8500791e-12 2.7889535e-09 6.0410713e-07
 5.0705140e-14 9.9999428e-01], sum to 1.0000
[2019-04-03 22:27:13,164] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5814
[2019-04-03 22:27:13,175] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 83.83333333333334, 75.66666666666666, 0.0, 26.0, 26.03885453954243, 0.5770653484822278, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4461000.0000, 
sim time next is 4461600.0000, 
raw observation next is [0.0, 82.66666666666667, 73.33333333333334, 0.0, 26.0, 26.1593564227452, 0.5841969893587414, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.8266666666666667, 0.24444444444444446, 0.0, 0.6666666666666666, 0.6799463685621001, 0.6947323297862472, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.289011], dtype=float32), 0.79614526]. 
=============================================
[2019-04-03 22:27:14,359] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.9523809e-12 1.3551065e-04 5.9445344e-11 2.1116309e-08 2.1323769e-05
 4.1195580e-12 9.9984312e-01], sum to 1.0000
[2019-04-03 22:27:14,360] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9988
[2019-04-03 22:27:14,370] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.199999999999999, 73.0, 0.0, 0.0, 26.0, 25.80800206419864, 0.4683724836443848, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4308000.0000, 
sim time next is 4308600.0000, 
raw observation next is [5.15, 73.0, 0.0, 0.0, 26.0, 25.8127497409524, 0.4599754450412316, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.605263157894737, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6510624784126998, 0.6533251483470772, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.92666], dtype=float32), 0.1982443]. 
=============================================
[2019-04-03 22:27:15,064] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7889063e-13 2.1084113e-06 7.5168856e-12 1.8199253e-09 6.1052610e-06
 2.1171383e-13 9.9999177e-01], sum to 1.0000
[2019-04-03 22:27:15,070] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9832
[2019-04-03 22:27:15,078] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.65, 61.83333333333334, 0.0, 0.0, 26.0, 25.88000330412989, 0.5991473585904927, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4402200.0000, 
sim time next is 4402800.0000, 
raw observation next is [8.5, 62.0, 0.0, 0.0, 26.0, 25.79410833696405, 0.5817917560375389, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.698060941828255, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6495090280803376, 0.6939305853458463, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3543772], dtype=float32), -0.4188246]. 
=============================================
[2019-04-03 22:27:20,433] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.0048417e-13 1.5863836e-07 5.8357108e-12 4.7174806e-09 1.4032970e-06
 9.8921664e-14 9.9999845e-01], sum to 1.0000
[2019-04-03 22:27:20,433] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1822
[2019-04-03 22:27:20,463] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 23.33333333333334, 23.33333333333334, 26.0, 25.46195803628043, 0.4883200331822358, 1.0, 1.0, 32983.4222461405], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4556400.0000, 
sim time next is 4557000.0000, 
raw observation next is [2.0, 52.0, 18.66666666666667, 18.66666666666667, 26.0, 25.38644441323368, 0.4965337276876782, 1.0, 1.0, 20889.50218544856], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.06222222222222224, 0.02062615101289135, 0.6666666666666666, 0.6155370344361399, 0.6655112425625594, 1.0, 1.0, 0.09947381993070743], 
reward next is 0.9005, 
noisyNet noise sample is [array([-2.1111288], dtype=float32), 0.57272977]. 
=============================================
[2019-04-03 22:27:20,480] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[79.00079 ]
 [79.02325 ]
 [79.38757 ]
 [79.810036]
 [80.15155 ]], R is [[79.22866058]
 [79.27931213]
 [79.48651886]
 [79.69165802]
 [79.89474487]].
[2019-04-03 22:27:25,829] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.88747759e-12 1.92480893e-05 1.22190216e-10 9.49909307e-09
 2.15108976e-05 1.28375045e-11 9.99959230e-01], sum to 1.0000
[2019-04-03 22:27:25,832] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7991
[2019-04-03 22:27:25,845] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.4, 73.0, 0.0, 0.0, 26.0, 25.15226614970445, 0.3719084486936102, 0.0, 1.0, 36217.98374276456], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4599600.0000, 
sim time next is 4600200.0000, 
raw observation next is [-2.5, 73.5, 0.0, 0.0, 26.0, 25.17680913442195, 0.3751512845094696, 0.0, 1.0, 36188.79822820047], 
processed observation next is [1.0, 0.21739130434782608, 0.39335180055401664, 0.735, 0.0, 0.0, 0.6666666666666666, 0.598067427868496, 0.6250504281698231, 0.0, 1.0, 0.1723276106104784], 
reward next is 0.8277, 
noisyNet noise sample is [array([-0.73358524], dtype=float32), -0.08095623]. 
=============================================
[2019-04-03 22:27:27,918] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.78280798e-13 2.75628054e-06 9.16794527e-13 1.91130622e-09
 2.43413774e-06 1.02369454e-13 9.99994874e-01], sum to 1.0000
[2019-04-03 22:27:27,920] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4723
[2019-04-03 22:27:27,938] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.95, 49.83333333333334, 200.3333333333333, 442.3333333333334, 26.0, 27.29536613160755, 0.8653869369967474, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4632600.0000, 
sim time next is 4633200.0000, 
raw observation next is [5.0, 50.0, 199.0, 364.0, 26.0, 27.39840751110675, 0.8812882459185506, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6011080332409973, 0.5, 0.6633333333333333, 0.4022099447513812, 0.6666666666666666, 0.7832006259255625, 0.7937627486395168, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42898637], dtype=float32), -0.34394258]. 
=============================================
[2019-04-03 22:27:33,853] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.76590252e-12 9.49564583e-06 1.10861815e-11 1.82541164e-08
 6.18348931e-05 3.32177320e-13 9.99928594e-01], sum to 1.0000
[2019-04-03 22:27:33,855] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0306
[2019-04-03 22:27:33,874] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 90.0, 212.1666666666667, 6.0, 26.0, 26.37231166196202, 0.5836777877907067, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4706400.0000, 
sim time next is 4707000.0000, 
raw observation next is [0.5, 89.0, 213.0, 6.0, 26.0, 26.36268267023568, 0.5748349739224716, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4764542936288089, 0.89, 0.71, 0.0066298342541436465, 0.6666666666666666, 0.6968902225196402, 0.6916116579741572, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29105633], dtype=float32), 1.6134764]. 
=============================================
[2019-04-03 22:27:33,877] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[91.830475]
 [92.10567 ]
 [92.34944 ]
 [92.54667 ]
 [92.63522 ]], R is [[91.43565369]
 [91.52130127]
 [91.60608673]
 [91.69002533]
 [91.77312469]].
[2019-04-03 22:27:34,667] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-03 22:27:34,667] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:27:34,667] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:27:34,667] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:27:34,669] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:27:34,669] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:27:34,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run5
[2019-04-03 22:27:34,686] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:27:34,695] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run5
[2019-04-03 22:27:34,713] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run5
[2019-04-03 22:29:29,566] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18832287], dtype=float32), 0.25546092]
[2019-04-03 22:29:29,567] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-5.0, 59.0, 0.0, 0.0, 26.0, 25.18841171285705, 0.3965468159810714, 1.0, 1.0, 58629.62185360191]
[2019-04-03 22:29:29,567] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-03 22:29:29,568] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.53025118e-11 6.94320115e-05 1.06358228e-10 1.00632626e-07
 1.22791520e-04 6.64054619e-12 9.99807775e-01], sampled 0.007695412520608458
[2019-04-03 22:30:18,100] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.8442 239852718.2087 1606.0366
[2019-04-03 22:30:48,527] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.3665 263473042.3734 1557.0848
[2019-04-03 22:30:53,713] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.9756 275735130.1179 1232.8990
[2019-04-03 22:30:54,751] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 400000, evaluation results [400000.0, 7241.366464888587, 263473042.37339455, 1557.0848052692845, 7353.8441990060055, 239852718.20873997, 1606.0365541615429, 7182.975570867066, 275735130.11792564, 1232.8989907060786]
[2019-04-03 22:30:56,755] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.6188869e-11 1.1368934e-03 1.7478550e-09 2.9834410e-07 2.4048315e-04
 5.1572563e-10 9.9862230e-01], sum to 1.0000
[2019-04-03 22:30:56,755] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6228
[2019-04-03 22:30:56,805] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.47121387225328, 0.2378643794269926, 0.0, 1.0, 40833.68626698024], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4765800.0000, 
sim time next is 4766400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.42487278074377, 0.2286244388440725, 0.0, 1.0, 40924.50934480098], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5354060650619807, 0.5762081462813575, 0.0, 1.0, 0.1948786159276237], 
reward next is 0.8051, 
noisyNet noise sample is [array([0.8738107], dtype=float32), 0.09564073]. 
=============================================
[2019-04-03 22:31:00,526] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3779038e-10 7.9153600e-04 7.6766987e-10 2.6247803e-06 2.4221172e-03
 8.4881255e-11 9.9678373e-01], sum to 1.0000
[2019-04-03 22:31:00,526] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3213
[2019-04-03 22:31:00,569] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 142.6666666666667, 387.0, 26.0, 25.11733888571968, 0.3712847393282085, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4895400.0000, 
sim time next is 4896000.0000, 
raw observation next is [3.0, 45.0, 132.5, 369.5, 26.0, 25.12200256961361, 0.3724107737053989, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.44166666666666665, 0.40828729281767956, 0.6666666666666666, 0.5935002141344675, 0.6241369245684663, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4626094], dtype=float32), 0.049345795]. 
=============================================
[2019-04-03 22:31:00,582] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[77.81625 ]
 [78.07494 ]
 [78.31697 ]
 [78.53516 ]
 [78.775116]], R is [[77.77020264]
 [77.99250031]
 [78.21257782]
 [78.43045044]
 [78.64614868]].
[2019-04-03 22:31:11,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:11,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:11,611] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run4
[2019-04-03 22:31:11,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:11,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:11,867] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run4
[2019-04-03 22:31:13,470] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:13,471] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:13,472] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run4
[2019-04-03 22:31:14,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:14,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:14,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run4
[2019-04-03 22:31:23,277] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5965358e-07 2.8630389e-02 1.3362442e-06 1.7481112e-04 2.5811533e-03
 3.3404194e-07 9.6861178e-01], sum to 1.0000
[2019-04-03 22:31:23,277] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8187
[2019-04-03 22:31:23,310] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.200000000000001, 96.0, 0.0, 0.0, 26.0, 20.3823294811375, -0.7711514690131991, 0.0, 1.0, 43268.82661251873], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 8400.0000, 
sim time next is 9000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 26.0, 20.45050668558164, -0.760783523060803, 0.0, 1.0, 43011.68768994651], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 0.6666666666666666, 0.20420889046513668, 0.24640549231306566, 0.0, 1.0, 0.20481756042831673], 
reward next is 0.7952, 
noisyNet noise sample is [array([0.9660485], dtype=float32), 1.254142]. 
=============================================
[2019-04-03 22:31:23,334] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[59.53053 ]
 [56.493534]
 [53.613018]
 [50.38683 ]
 [46.814453]], R is [[62.78957748]
 [62.95563889]
 [63.11878204]
 [63.27894974]
 [63.43599319]].
[2019-04-03 22:31:26,239] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:26,239] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:26,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run4
[2019-04-03 22:31:27,142] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.33159672e-12 2.45364390e-06 1.92051028e-11 1.32814808e-08
 1.17595555e-05 2.96686937e-13 9.99985695e-01], sum to 1.0000
[2019-04-03 22:31:27,142] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7007
[2019-04-03 22:31:27,243] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 26.0, 26.04836037175352, 0.5869964218774111, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4996800.0000, 
sim time next is 4997400.0000, 
raw observation next is [5.666666666666667, 24.0, 0.0, 0.0, 26.0, 25.98634234957513, 0.5709965400630156, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6195752539242845, 0.24, 0.0, 0.0, 0.6666666666666666, 0.6655285291312607, 0.6903321800210053, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27628642], dtype=float32), 0.076439045]. 
=============================================
[2019-04-03 22:31:31,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:31,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:31,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run4
[2019-04-03 22:31:32,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:32,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:32,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run4
[2019-04-03 22:31:32,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:32,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:32,976] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run4
[2019-04-03 22:31:33,249] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3244382e-13 1.3014444e-05 3.6326222e-12 2.4392028e-09 3.3033686e-07
 2.0177047e-13 9.9998665e-01], sum to 1.0000
[2019-04-03 22:31:33,249] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8551
[2019-04-03 22:31:33,288] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 45.0, 109.5, 670.5, 26.0, 26.53000295065297, 0.6247592569162822, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5044800.0000, 
sim time next is 5045400.0000, 
raw observation next is [2.0, 44.0, 112.0, 698.0, 26.0, 26.61348711462023, 0.6505322363207164, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.518005540166205, 0.44, 0.37333333333333335, 0.7712707182320442, 0.6666666666666666, 0.7177905928850192, 0.7168440787735721, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0817776], dtype=float32), -0.37783843]. 
=============================================
[2019-04-03 22:31:34,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:34,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:34,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run4
[2019-04-03 22:31:36,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:36,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:36,675] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run4
[2019-04-03 22:31:39,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:39,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:39,806] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run4
[2019-04-03 22:31:39,987] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:39,988] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:39,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run4
[2019-04-03 22:31:40,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:40,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:40,239] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run4
[2019-04-03 22:31:43,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:43,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:43,435] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run4
[2019-04-03 22:31:44,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:44,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:44,267] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run4
[2019-04-03 22:31:48,454] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.1757782e-11 3.3560407e-04 1.8460800e-10 3.2722522e-07 1.5793419e-04
 1.5457790e-11 9.9950612e-01], sum to 1.0000
[2019-04-03 22:31:48,454] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4537
[2019-04-03 22:31:48,550] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.033333333333333, 77.0, 71.50000000000001, 49.33333333333332, 26.0, 25.35996607478111, 0.2270609033574146, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 206400.0000, 
sim time next is 207000.0000, 
raw observation next is [-7.85, 76.5, 79.0, 0.0, 26.0, 25.40632516551008, 0.2209623480160316, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24515235457063714, 0.765, 0.2633333333333333, 0.0, 0.6666666666666666, 0.6171937637925066, 0.5736541160053439, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8980204], dtype=float32), -0.9102019]. 
=============================================
[2019-04-03 22:31:48,584] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[84.99301 ]
 [85.81856 ]
 [86.503006]
 [87.20105 ]
 [87.52939 ]], R is [[84.4076004 ]
 [84.56352234]
 [84.71788788]
 [84.87071228]
 [85.02200317]].
[2019-04-03 22:31:48,736] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:31:48,736] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:31:48,738] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run4
[2019-04-03 22:32:10,444] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.1482081e-10 5.8197307e-05 3.9551997e-09 5.8620475e-07 2.0071197e-05
 7.8437812e-10 9.9992120e-01], sum to 1.0000
[2019-04-03 22:32:10,449] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7762
[2019-04-03 22:32:10,484] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.2, 67.5, 0.0, 0.0, 26.0, 23.39061031175273, -0.08735181387662495, 0.0, 1.0, 47518.81597000534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 347400.0000, 
sim time next is 348000.0000, 
raw observation next is [-14.3, 68.0, 0.0, 0.0, 26.0, 23.32008352248707, -0.09518261907766423, 0.0, 1.0, 47583.25219810135], 
processed observation next is [1.0, 0.0, 0.06648199445983377, 0.68, 0.0, 0.0, 0.6666666666666666, 0.44334029354058924, 0.4682724603074453, 0.0, 1.0, 0.22658691522905405], 
reward next is 0.7734, 
noisyNet noise sample is [array([0.0352121], dtype=float32), 0.2515182]. 
=============================================
[2019-04-03 22:32:10,510] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[62.06089 ]
 [62.18702 ]
 [62.371025]
 [62.544594]
 [62.726482]], R is [[62.05063248]
 [62.20384598]
 [62.35585785]
 [62.50669479]
 [62.65637207]].
[2019-04-03 22:32:19,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.8876641e-11 6.1461462e-05 1.5329767e-09 4.7769625e-08 4.8580023e-05
 6.6672272e-11 9.9988985e-01], sum to 1.0000
[2019-04-03 22:32:19,347] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0205
[2019-04-03 22:32:19,402] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.08898998626671, -0.1589418719373603, 0.0, 1.0, 44279.72124114567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 187800.0000, 
sim time next is 188400.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 23.05167048102818, -0.1725415007231773, 0.0, 1.0, 44322.96510428788], 
processed observation next is [1.0, 0.17391304347826086, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4209725400856816, 0.4424861664256075, 0.0, 1.0, 0.21106173859184707], 
reward next is 0.7889, 
noisyNet noise sample is [array([-0.347222], dtype=float32), -0.62316686]. 
=============================================
[2019-04-03 22:32:24,794] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4163839e-12 9.9009349e-06 6.6718742e-11 2.3646248e-08 1.3801690e-05
 2.9297400e-12 9.9997628e-01], sum to 1.0000
[2019-04-03 22:32:24,794] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9103
[2019-04-03 22:32:24,808] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.966666666666667, 71.0, 0.0, 0.0, 26.0, 24.12413343229557, 0.08545579177673328, 0.0, 1.0, 44565.29230686714], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 261600.0000, 
sim time next is 262200.0000, 
raw observation next is [-6.333333333333334, 69.0, 0.0, 0.0, 26.0, 24.14993192803506, 0.0812965499261787, 0.0, 1.0, 44683.84938549842], 
processed observation next is [1.0, 0.0, 0.28716528162511545, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5124943273362549, 0.5270988499753929, 0.0, 1.0, 0.2127802351690401], 
reward next is 0.7872, 
noisyNet noise sample is [array([0.23995891], dtype=float32), 0.5673977]. 
=============================================
[2019-04-03 22:32:28,824] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6735500e-11 1.8146816e-06 9.5584769e-11 1.0066591e-07 6.5671379e-06
 1.2894551e-12 9.9999154e-01], sum to 1.0000
[2019-04-03 22:32:28,824] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3023
[2019-04-03 22:32:28,904] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.96666666666667, 51.66666666666667, 0.0, 0.0, 26.0, 25.69274636783045, 0.3921062531843091, 1.0, 1.0, 155980.3398587133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 321600.0000, 
sim time next is 322200.0000, 
raw observation next is [-11.15, 53.0, 0.0, 0.0, 26.0, 25.69567113570359, 0.4011654208686277, 1.0, 1.0, 106320.850756951], 
processed observation next is [1.0, 0.7391304347826086, 0.15373961218836565, 0.53, 0.0, 0.0, 0.6666666666666666, 0.6413059279752993, 0.6337218069562093, 1.0, 1.0, 0.5062897655092905], 
reward next is 0.4937, 
noisyNet noise sample is [array([0.41796267], dtype=float32), 0.96763265]. 
=============================================
[2019-04-03 22:32:29,068] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.5327128e-11 1.9149590e-04 1.5048561e-09 1.5614161e-07 4.5742829e-05
 4.4584690e-11 9.9976259e-01], sum to 1.0000
[2019-04-03 22:32:29,068] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5597
[2019-04-03 22:32:29,099] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.5, 69.0, 0.0, 0.0, 26.0, 23.26414369170375, -0.1081439439529799, 0.0, 1.0, 47675.46836704694], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 349200.0000, 
sim time next is 349800.0000, 
raw observation next is [-14.58333333333333, 69.0, 0.0, 0.0, 26.0, 23.26869452671967, -0.1139947866018163, 0.0, 1.0, 47726.69404553359], 
processed observation next is [1.0, 0.043478260869565216, 0.05863342566943682, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4390578772266392, 0.4620017377993946, 0.0, 1.0, 0.22726997164539806], 
reward next is 0.7727, 
noisyNet noise sample is [array([1.8016001], dtype=float32), 0.5817062]. 
=============================================
[2019-04-03 22:32:31,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.7487649e-11 1.4763454e-04 2.6483600e-09 8.7547761e-07 1.9134211e-04
 1.0047000e-09 9.9966013e-01], sum to 1.0000
[2019-04-03 22:32:31,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4896
[2019-04-03 22:32:31,696] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.8, 74.66666666666667, 0.0, 0.0, 26.0, 22.18240022868527, -0.3911389787929263, 0.0, 1.0, 48635.43810963229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 364800.0000, 
sim time next is 365400.0000, 
raw observation next is [-15.9, 75.5, 0.0, 0.0, 26.0, 22.09260767470404, -0.3987692275161987, 0.0, 1.0, 48616.22180920339], 
processed observation next is [1.0, 0.21739130434782608, 0.02216066481994457, 0.755, 0.0, 0.0, 0.6666666666666666, 0.3410506395586701, 0.3670769241612671, 0.0, 1.0, 0.23150581813906376], 
reward next is 0.7685, 
noisyNet noise sample is [array([-0.5174216], dtype=float32), 0.4925035]. 
=============================================
[2019-04-03 22:32:37,822] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9448263e-14 1.8758985e-06 1.2728176e-12 1.9971740e-09 5.1408682e-07
 1.3964734e-14 9.9999762e-01], sum to 1.0000
[2019-04-03 22:32:37,822] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5088
[2019-04-03 22:32:37,849] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.1, 94.66666666666667, 0.0, 0.0, 26.0, 24.84140936145365, 0.23286656900516, 0.0, 1.0, 40669.4721206519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 513600.0000, 
sim time next is 514200.0000, 
raw observation next is [3.2, 95.33333333333333, 0.0, 0.0, 26.0, 24.83883540037029, 0.2328424515335057, 0.0, 1.0, 40535.159348651], 
processed observation next is [1.0, 0.9565217391304348, 0.551246537396122, 0.9533333333333333, 0.0, 0.0, 0.6666666666666666, 0.5699029500308574, 0.5776141505111686, 0.0, 1.0, 0.19302456832690953], 
reward next is 0.8070, 
noisyNet noise sample is [array([-0.7146924], dtype=float32), 0.9374203]. 
=============================================
[2019-04-03 22:32:57,302] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3931113e-13 1.9296633e-04 3.7514294e-11 1.4366677e-08 2.5967076e-06
 1.4238160e-12 9.9980444e-01], sum to 1.0000
[2019-04-03 22:32:57,302] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1388
[2019-04-03 22:32:57,393] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 91.0, 89.0, 103.5, 26.0, 25.12127962601887, 0.2950435476437003, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 550800.0000, 
sim time next is 551400.0000, 
raw observation next is [-0.09999999999999999, 90.33333333333334, 107.3333333333333, 103.3333333333333, 26.0, 25.08837845845053, 0.2880457744596266, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.4598337950138504, 0.9033333333333334, 0.3577777777777777, 0.11418047882136276, 0.6666666666666666, 0.5906982048708777, 0.5960152581532089, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0056915], dtype=float32), 0.21049738]. 
=============================================
[2019-04-03 22:32:59,206] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.5614090e-12 1.3629187e-04 2.2700872e-10 2.3020075e-07 1.6378764e-04
 1.9639700e-11 9.9969971e-01], sum to 1.0000
[2019-04-03 22:32:59,207] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5646
[2019-04-03 22:32:59,286] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.93652700834915, 0.279929529823899, 0.0, 1.0, 55423.96114403652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 586800.0000, 
sim time next is 587400.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.94051795388465, 0.2840133886212677, 0.0, 1.0, 45983.22836862898], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5783764961570542, 0.5946711295404226, 0.0, 1.0, 0.2189677541363285], 
reward next is 0.7810, 
noisyNet noise sample is [array([-0.25240076], dtype=float32), 0.038788434]. 
=============================================
[2019-04-03 22:33:00,429] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8850056e-12 9.0483394e-05 9.7134731e-12 3.9449819e-08 2.0071000e-04
 2.0298041e-12 9.9970871e-01], sum to 1.0000
[2019-04-03 22:33:00,430] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8243
[2019-04-03 22:33:00,503] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 82.0, 122.5, 401.3333333333334, 26.0, 25.03089374951435, 0.352769816312589, 0.0, 1.0, 18721.56855304877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 571200.0000, 
sim time next is 571800.0000, 
raw observation next is [-1.2, 82.5, 118.0, 335.6666666666667, 26.0, 25.03036711223281, 0.3433812288695368, 0.0, 1.0, 18720.32766375913], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.825, 0.3933333333333333, 0.370902394106814, 0.6666666666666666, 0.5858639260194008, 0.6144604096231789, 0.0, 1.0, 0.08914441744647204], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.22749159], dtype=float32), 1.4131557]. 
=============================================
[2019-04-03 22:33:02,825] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.04756601e-15 1.27055555e-05 2.24635875e-13 1.23314503e-09
 9.55224550e-06 2.15691671e-14 9.99977708e-01], sum to 1.0000
[2019-04-03 22:33:02,825] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1076
[2019-04-03 22:33:02,843] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 58.5, 99.66666666666666, 669.0, 26.0, 25.86307608612963, 0.3895737347697715, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 733800.0000, 
sim time next is 734400.0000, 
raw observation next is [-0.6, 57.0, 107.5, 614.0, 26.0, 25.84242318358226, 0.3889886225688682, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.44598337950138506, 0.57, 0.35833333333333334, 0.6784530386740332, 0.6666666666666666, 0.6535352652985216, 0.6296628741896227, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6643582], dtype=float32), -1.6879895]. 
=============================================
[2019-04-03 22:33:05,000] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.61159484e-13 1.35324983e-04 5.74943467e-13 1.02913784e-08
 1.04198036e-04 7.65526761e-14 9.99760449e-01], sum to 1.0000
[2019-04-03 22:33:05,000] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5075
[2019-04-03 22:33:05,085] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2333333333333334, 54.66666666666667, 123.1666666666667, 504.0, 26.0, 25.79166270175036, 0.3739685885639761, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 735600.0000, 
sim time next is 736200.0000, 
raw observation next is [-0.04999999999999999, 53.5, 131.0, 449.0, 26.0, 25.74934045993063, 0.3610843215657347, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.461218836565097, 0.535, 0.43666666666666665, 0.49613259668508286, 0.6666666666666666, 0.6457783716608857, 0.6203614405219116, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.009751], dtype=float32), 0.19087431]. 
=============================================
[2019-04-03 22:33:54,837] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8372662e-13 2.0126190e-05 1.4681591e-12 4.8044861e-09 3.3475453e-06
 1.4810343e-13 9.9997652e-01], sum to 1.0000
[2019-04-03 22:33:54,861] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5167
[2019-04-03 22:33:54,871] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.2, 62.33333333333334, 0.0, 0.0, 26.0, 25.80543688720109, 0.7033146365648218, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1113000.0000, 
sim time next is 1113600.0000, 
raw observation next is [13.1, 62.66666666666667, 0.0, 0.0, 26.0, 25.8411887398038, 0.7008416150309588, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8254847645429363, 0.6266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6534323949836501, 0.7336138716769862, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1900343], dtype=float32), -0.81237274]. 
=============================================
[2019-04-03 22:34:01,137] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.6679267e-15 1.9902918e-05 5.2468609e-13 1.4160494e-09 1.1583473e-06
 1.2510494e-13 9.9997890e-01], sum to 1.0000
[2019-04-03 22:34:01,138] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0534
[2019-04-03 22:34:01,162] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.2, 83.0, 18.5, 58.66666666666666, 26.0, 25.90886324721222, 0.6510686003811065, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1066800.0000, 
sim time next is 1067400.0000, 
raw observation next is [12.2, 83.0, 22.0, 69.0, 26.0, 26.06846783951909, 0.6821834736301109, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.07333333333333333, 0.07624309392265194, 0.6666666666666666, 0.6723723199599242, 0.727394491210037, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8644181], dtype=float32), 0.78731096]. 
=============================================
[2019-04-03 22:34:06,635] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7419158e-13 6.3461099e-05 8.5136542e-12 8.4699687e-09 1.8680595e-06
 1.4958517e-12 9.9993467e-01], sum to 1.0000
[2019-04-03 22:34:06,637] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1351
[2019-04-03 22:34:06,663] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.6, 79.0, 0.0, 0.0, 26.0, 25.69791661686843, 0.6185968810850114, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1142400.0000, 
sim time next is 1143000.0000, 
raw observation next is [11.6, 80.0, 0.0, 0.0, 26.0, 25.69408047647835, 0.6138907990043675, 0.0, 1.0, 18721.72874102454], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6411733730398627, 0.7046302663347892, 0.0, 1.0, 0.089151089242974], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.96694285], dtype=float32), 1.2039814]. 
=============================================
[2019-04-03 22:34:06,708] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[91.46791 ]
 [91.53715 ]
 [91.51635 ]
 [91.493324]
 [91.43047 ]], R is [[91.52581024]
 [91.61054993]
 [91.60527802]
 [91.58770752]
 [91.53862   ]].
[2019-04-03 22:34:11,239] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.4076479e-14 6.6473735e-06 4.1424598e-12 6.1522959e-10 2.1830972e-07
 2.5854793e-13 9.9999309e-01], sum to 1.0000
[2019-04-03 22:34:11,244] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0736
[2019-04-03 22:34:11,262] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 92.66666666666667, 0.0, 0.0, 26.0, 25.43554916510456, 0.4889175833930757, 0.0, 1.0, 18764.61200768632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1477200.0000, 
sim time next is 1477800.0000, 
raw observation next is [2.2, 93.0, 0.0, 0.0, 26.0, 25.50531379848363, 0.4808908841697459, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6254428165403025, 0.6602969613899153, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09455795], dtype=float32), -2.1217892]. 
=============================================
[2019-04-03 22:34:14,508] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4318096e-14 2.2278505e-06 1.1529482e-11 1.3267910e-09 2.8014293e-08
 2.4122425e-13 9.9999774e-01], sum to 1.0000
[2019-04-03 22:34:14,514] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0183
[2019-04-03 22:34:14,531] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.48871233591588, 0.5215442137621845, 0.0, 1.0, 40754.9296333681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1317600.0000, 
sim time next is 1318200.0000, 
raw observation next is [1.516666666666667, 92.0, 0.0, 0.0, 26.0, 25.40570659932159, 0.5130729119506064, 0.0, 1.0, 81657.49092201325], 
processed observation next is [1.0, 0.2608695652173913, 0.5046168051708219, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6171422166101325, 0.6710243039835354, 0.0, 1.0, 0.3888451948667298], 
reward next is 0.6112, 
noisyNet noise sample is [array([-0.6622292], dtype=float32), 0.48821345]. 
=============================================
[2019-04-03 22:34:21,978] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.3464418e-14 2.6995442e-06 1.6040704e-12 1.3896558e-09 1.7386236e-08
 1.6229531e-13 9.9999726e-01], sum to 1.0000
[2019-04-03 22:34:21,981] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4740
[2019-04-03 22:34:22,010] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 51.33333333333334, 0.0, 26.0, 26.03560052235375, 0.5281914525603849, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1503600.0000, 
sim time next is 1504200.0000, 
raw observation next is [2.1, 100.0, 55.66666666666666, 0.0, 26.0, 26.0953151264114, 0.5320747108604459, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5207756232686982, 1.0, 0.18555555555555553, 0.0, 0.6666666666666666, 0.6746095938676167, 0.677358236953482, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35676876], dtype=float32), 0.97872764]. 
=============================================
[2019-04-03 22:34:27,650] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3893349e-14 5.1571697e-06 5.1836893e-13 1.7162675e-08 1.4951233e-08
 4.4420617e-14 9.9999487e-01], sum to 1.0000
[2019-04-03 22:34:27,651] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8167
[2019-04-03 22:34:27,666] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.800000000000001, 84.66666666666667, 0.0, 0.0, 26.0, 25.99131065986735, 0.6266943578465319, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1635600.0000, 
sim time next is 1636200.0000, 
raw observation next is [6.9, 84.0, 0.0, 0.0, 26.0, 25.82975373054571, 0.6095682961377173, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6537396121883658, 0.84, 0.0, 0.0, 0.6666666666666666, 0.652479477545476, 0.7031894320459058, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2205935], dtype=float32), -0.19704923]. 
=============================================
[2019-04-03 22:34:27,710] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5646623e-14 3.9678375e-07 6.3312955e-14 9.6892894e-10 6.8993430e-08
 2.0862462e-15 9.9999952e-01], sum to 1.0000
[2019-04-03 22:34:27,714] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8889
[2019-04-03 22:34:27,723] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.533333333333333, 64.66666666666666, 81.66666666666666, 688.3333333333333, 26.0, 26.49890315135366, 0.7083654259055728, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1518600.0000, 
sim time next is 1519200.0000, 
raw observation next is [10.0, 63.0, 80.0, 682.0, 26.0, 26.63110348157565, 0.7308035579246727, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.739612188365651, 0.63, 0.26666666666666666, 0.7535911602209945, 0.6666666666666666, 0.7192586234646375, 0.7436011859748909, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3845115], dtype=float32), 0.61790353]. 
=============================================
[2019-04-03 22:34:29,906] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3879129e-11 2.2360464e-04 4.6284143e-11 8.4257067e-08 2.2481465e-06
 1.6895350e-11 9.9977404e-01], sum to 1.0000
[2019-04-03 22:34:29,906] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9364
[2019-04-03 22:34:29,940] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 87.0, 108.0, 0.0, 26.0, 24.89827725105942, 0.3356723632431354, 0.0, 1.0, 63798.63110521323], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1767600.0000, 
sim time next is 1768200.0000, 
raw observation next is [-2.3, 86.33333333333333, 111.6666666666667, 0.0, 26.0, 24.89496275663548, 0.3414901862928701, 0.0, 1.0, 55568.31462205387], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.8633333333333333, 0.37222222222222234, 0.0, 0.6666666666666666, 0.5745802297196233, 0.6138300620976234, 0.0, 1.0, 0.2646110220097803], 
reward next is 0.7354, 
noisyNet noise sample is [array([-0.41996676], dtype=float32), -1.5306001]. 
=============================================
[2019-04-03 22:34:36,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6995962e-14 3.9563016e-05 2.0091962e-12 1.8061870e-10 9.0585644e-08
 2.2258862e-13 9.9996030e-01], sum to 1.0000
[2019-04-03 22:34:36,998] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5749
[2019-04-03 22:34:37,010] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.60330116244999, 0.5358891882591091, 0.0, 1.0, 106574.279429865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1652400.0000, 
sim time next is 1653000.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.48933404803802, 0.5397251719053842, 0.0, 1.0, 129423.0974060342], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6241111706698351, 0.679908390635128, 0.0, 1.0, 0.6163004638382581], 
reward next is 0.3837, 
noisyNet noise sample is [array([-0.01805815], dtype=float32), 0.092254415]. 
=============================================
[2019-04-03 22:34:37,028] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[87.17446 ]
 [86.70987 ]
 [86.75697 ]
 [86.83287 ]
 [86.923874]], R is [[87.23414612]
 [86.85430908]
 [86.98576355]
 [87.11590576]
 [87.24475098]].
[2019-04-03 22:35:10,645] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4016279e-12 2.7114950e-04 1.5504301e-10 6.3410702e-08 1.2942063e-05
 2.3138680e-11 9.9971575e-01], sum to 1.0000
[2019-04-03 22:35:10,645] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1829
[2019-04-03 22:35:10,668] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.199999999999999, 87.0, 0.0, 0.0, 26.0, 24.21321926624926, 0.0638995299894536, 0.0, 1.0, 41079.6130000487], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2007600.0000, 
sim time next is 2008200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.15917574545531, 0.05870301665495665, 0.0, 1.0, 41111.63103439956], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5132646454546093, 0.5195676722183189, 0.0, 1.0, 0.19576967159237885], 
reward next is 0.8042, 
noisyNet noise sample is [array([-1.7698662], dtype=float32), 0.6237149]. 
=============================================
[2019-04-03 22:35:16,208] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.4842498e-12 7.6032447e-05 1.8591849e-11 5.8727803e-08 3.0155229e-07
 1.3530302e-12 9.9992359e-01], sum to 1.0000
[2019-04-03 22:35:16,208] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7628
[2019-04-03 22:35:16,274] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.7333333333333335, 43.33333333333334, 135.1666666666667, 45.0, 26.0, 25.84582509551527, 0.4285893620705887, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2301600.0000, 
sim time next is 2302200.0000, 
raw observation next is [0.55, 43.5, 138.0, 42.0, 26.0, 25.19644174744904, 0.3747504426637489, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4778393351800555, 0.435, 0.46, 0.04640883977900553, 0.6666666666666666, 0.5997034789540866, 0.6249168142212497, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3051414], dtype=float32), 1.0885115]. 
=============================================
[2019-04-03 22:35:22,590] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3708370e-12 8.9843900e-07 3.7377983e-12 1.6622991e-08 5.5263240e-07
 1.9733577e-13 9.9999845e-01], sum to 1.0000
[2019-04-03 22:35:22,591] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1515
[2019-04-03 22:35:22,631] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.300000000000001, 70.0, 138.6666666666667, 0.0, 26.0, 25.53259224149004, 0.3382447845586626, 1.0, 1.0, 52090.45366816923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2200800.0000, 
sim time next is 2201400.0000, 
raw observation next is [-4.2, 69.5, 143.0, 0.0, 26.0, 25.54572033178955, 0.3521147726430722, 1.0, 1.0, 34939.25662634293], 
processed observation next is [1.0, 0.4782608695652174, 0.34626038781163443, 0.695, 0.4766666666666667, 0.0, 0.6666666666666666, 0.6288100276491292, 0.6173715908810241, 1.0, 1.0, 0.16637741250639493], 
reward next is 0.8336, 
noisyNet noise sample is [array([-1.5093772], dtype=float32), 0.93762183]. 
=============================================
[2019-04-03 22:35:27,823] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.9839269e-13 2.0697164e-04 4.0531771e-12 5.7583835e-09 1.3193837e-06
 6.9110879e-13 9.9979168e-01], sum to 1.0000
[2019-04-03 22:35:27,824] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5029
[2019-04-03 22:35:27,881] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 75.0, 21.5, 131.0, 26.0, 25.49604865484374, 0.3471602717850472, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2188800.0000, 
sim time next is 2189400.0000, 
raw observation next is [-5.600000000000001, 75.0, 28.0, 174.6666666666667, 26.0, 25.63152023884818, 0.3350694473764682, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3074792243767313, 0.75, 0.09333333333333334, 0.1930018416206262, 0.6666666666666666, 0.6359600199040149, 0.6116898157921561, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.656604], dtype=float32), 0.30748156]. 
=============================================
[2019-04-03 22:35:28,529] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.6172245e-13 2.9124880e-05 7.5794909e-12 3.8261008e-09 8.9849647e-07
 8.7812971e-13 9.9996996e-01], sum to 1.0000
[2019-04-03 22:35:28,532] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0661
[2019-04-03 22:35:28,553] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.900000000000002, 91.0, 0.0, 0.0, 26.0, 23.69355740815719, -0.00880450846326535, 0.0, 1.0, 43320.51480658642], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2265600.0000, 
sim time next is 2266200.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.68407757142769, -0.01663578602112774, 0.0, 1.0, 43271.28039951844], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.47367313095230745, 0.4944547379929574, 0.0, 1.0, 0.20605371618818302], 
reward next is 0.7939, 
noisyNet noise sample is [array([-1.6582556], dtype=float32), 0.29048985]. 
=============================================
[2019-04-03 22:35:37,112] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3657050e-10 4.0912730e-04 7.2655315e-10 3.6710304e-07 8.2446177e-06
 5.5153375e-11 9.9958223e-01], sum to 1.0000
[2019-04-03 22:35:37,112] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9772
[2019-04-03 22:35:37,230] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 25.33333333333334, 0.0, 26.0, 24.04225058023461, 0.06965190161459937, 0.0, 1.0, 41994.14846309182], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2362200.0000, 
sim time next is 2362800.0000, 
raw observation next is [-3.4, 69.0, 31.16666666666667, 0.0, 26.0, 24.06963236790867, 0.1323117145497074, 0.0, 1.0, 202421.3760289045], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.1038888888888889, 0.0, 0.6666666666666666, 0.5058026973257226, 0.5441039048499025, 0.0, 1.0, 0.9639113144233546], 
reward next is 0.0361, 
noisyNet noise sample is [array([0.1935141], dtype=float32), -0.24670961]. 
=============================================
[2019-04-03 22:35:37,320] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2017448e-10 2.0557013e-04 3.0396766e-10 1.3633843e-07 1.4657029e-05
 7.5566907e-11 9.9977964e-01], sum to 1.0000
[2019-04-03 22:35:37,333] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3894
[2019-04-03 22:35:37,375] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.09999999999999998, 52.33333333333334, 180.6666666666667, 0.0, 26.0, 24.93253304312198, 0.3010743752525865, 0.0, 1.0, 32085.05329501612], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2382600.0000, 
sim time next is 2383200.0000, 
raw observation next is [0.0, 52.0, 175.5, 0.0, 26.0, 24.94656897009354, 0.3057531343285658, 0.0, 1.0, 20024.93663413872], 
processed observation next is [0.0, 0.6086956521739131, 0.46260387811634357, 0.52, 0.585, 0.0, 0.6666666666666666, 0.5788807475077951, 0.6019177114428552, 0.0, 1.0, 0.09535684111494629], 
reward next is 0.9046, 
noisyNet noise sample is [array([-1.5268933], dtype=float32), -0.25562847]. 
=============================================
[2019-04-03 22:35:54,097] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.6481756e-14 6.3107514e-06 9.0609508e-13 2.1016284e-09 1.2916789e-07
 2.7320191e-14 9.9999356e-01], sum to 1.0000
[2019-04-03 22:35:54,097] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6529
[2019-04-03 22:35:54,176] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 58.0, 0.0, 0.0, 26.0, 25.00996230116721, 0.351014551214573, 1.0, 1.0, 124694.9800446237], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2659200.0000, 
sim time next is 2659800.0000, 
raw observation next is [-1.1, 59.0, 0.0, 0.0, 26.0, 24.99388697926278, 0.3760955333883615, 1.0, 1.0, 92153.02422370088], 
processed observation next is [1.0, 0.782608695652174, 0.4321329639889197, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5828239149385649, 0.6253651777961206, 1.0, 1.0, 0.43882392487476607], 
reward next is 0.5612, 
noisyNet noise sample is [array([-1.5353365], dtype=float32), 0.8415237]. 
=============================================
[2019-04-03 22:35:58,415] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4894540e-12 6.6461304e-05 4.4141146e-11 6.4198389e-08 3.0050580e-06
 5.6006462e-12 9.9993050e-01], sum to 1.0000
[2019-04-03 22:35:58,416] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1802
[2019-04-03 22:35:58,453] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.1, 82.16666666666667, 0.0, 0.0, 26.0, 24.35870408480511, 0.1147649730160362, 0.0, 1.0, 42884.56054959602], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2609400.0000, 
sim time next is 2610000.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 26.0, 24.30502292257525, 0.09987116631863302, 0.0, 1.0, 42997.12528376363], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5254185768812709, 0.5332903887728776, 0.0, 1.0, 0.20474821563696965], 
reward next is 0.7953, 
noisyNet noise sample is [array([-0.5644164], dtype=float32), -2.1161666]. 
=============================================
[2019-04-03 22:35:58,477] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.6012  ]
 [80.6699  ]
 [80.72192 ]
 [80.771286]
 [80.81901 ]], R is [[80.51057434]
 [80.50125122]
 [80.49246979]
 [80.48409271]
 [80.47618103]].
[2019-04-03 22:36:10,146] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.62359850e-14 1.30345434e-05 1.75805941e-12 4.37208536e-09
 2.17873847e-07 1.03915066e-13 9.99986768e-01], sum to 1.0000
[2019-04-03 22:36:10,146] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6485
[2019-04-03 22:36:10,214] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 24.55287046565662, 0.3454039319748728, 1.0, 1.0, 87895.31178526561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2833200.0000, 
sim time next is 2833800.0000, 
raw observation next is [2.833333333333333, 38.16666666666667, 0.0, 0.0, 26.0, 24.96591717107259, 0.3924754381005051, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.3816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5804930975893825, 0.6308251460335017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3202604], dtype=float32), 0.40243393]. 
=============================================
[2019-04-03 22:36:23,285] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.2762742e-14 3.2578112e-06 1.1937954e-12 2.3254993e-09 1.8864889e-06
 1.9267550e-14 9.9999487e-01], sum to 1.0000
[2019-04-03 22:36:23,287] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2541
[2019-04-03 22:36:23,329] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 81.5, 0.0, 0.0, 26.0, 25.00089616684421, 0.4417484461020406, 0.0, 1.0, 101817.6854686568], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2925000.0000, 
sim time next is 2925600.0000, 
raw observation next is [-1.0, 82.66666666666667, 0.0, 0.0, 26.0, 25.09565862037598, 0.4686543638795432, 0.0, 1.0, 64672.91056831117], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5913048850313315, 0.6562181212931811, 0.0, 1.0, 0.30796624080148177], 
reward next is 0.6920, 
noisyNet noise sample is [array([-1.1905777], dtype=float32), 0.20706016]. 
=============================================
[2019-04-03 22:36:29,217] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.0934148e-12 2.2335890e-04 9.2682195e-11 5.6957653e-08 2.6299682e-05
 4.4127969e-11 9.9975020e-01], sum to 1.0000
[2019-04-03 22:36:29,217] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0460
[2019-04-03 22:36:29,266] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 54.0, 106.0, 759.0, 26.0, 25.13109013462183, 0.3079333524865343, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3061800.0000, 
sim time next is 3062400.0000, 
raw observation next is [-4.0, 54.0, 106.8333333333333, 766.6666666666666, 26.0, 25.07488841737554, 0.3043622964538373, 0.0, 1.0, 35975.9901179529], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.356111111111111, 0.8471454880294659, 0.6666666666666666, 0.589574034781295, 0.6014540988179458, 0.0, 1.0, 0.17131423865691855], 
reward next is 0.8287, 
noisyNet noise sample is [array([-0.5589723], dtype=float32), -0.4050138]. 
=============================================
[2019-04-03 22:36:35,565] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2420440e-14 6.6879966e-07 4.8054085e-13 1.8181583e-09 1.3875909e-05
 9.6388072e-15 9.9998546e-01], sum to 1.0000
[2019-04-03 22:36:35,575] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3023
[2019-04-03 22:36:35,611] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 48.0, 87.0, 674.0, 26.0, 26.70659148872784, 0.6047252152531053, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3339000.0000, 
sim time next is 3339600.0000, 
raw observation next is [-2.333333333333333, 47.33333333333333, 82.5, 645.1666666666667, 26.0, 26.05415910521073, 0.6323712281546745, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3979686057248385, 0.4733333333333333, 0.275, 0.7128913443830571, 0.6666666666666666, 0.6711799254342274, 0.7107904093848916, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2738333], dtype=float32), 0.24560347]. 
=============================================
[2019-04-03 22:36:40,237] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4631335e-12 2.9688906e-07 2.1539821e-12 8.5974650e-09 5.4552147e-06
 1.2806189e-14 9.9999416e-01], sum to 1.0000
[2019-04-03 22:36:40,237] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4834
[2019-04-03 22:36:40,252] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 50.0, 99.66666666666666, 726.0, 26.0, 26.51813685043662, 0.6924886092527304, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3336600.0000, 
sim time next is 3337200.0000, 
raw observation next is [-3.0, 50.0, 96.5, 713.0, 26.0, 26.61746764298194, 0.7016439909026381, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3795013850415513, 0.5, 0.32166666666666666, 0.7878453038674034, 0.6666666666666666, 0.7181223035818283, 0.7338813303008793, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.659929], dtype=float32), -0.90635794]. 
=============================================
[2019-04-03 22:36:41,108] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.47768557e-12 2.17469551e-06 1.77066722e-10 1.02079376e-07
 1.30450380e-05 2.72702065e-12 9.99984741e-01], sum to 1.0000
[2019-04-03 22:36:41,109] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6128
[2019-04-03 22:36:41,145] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 68.0, 0.0, 0.0, 26.0, 25.91456400142292, 0.6149275142332985, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3267000.0000, 
sim time next is 3267600.0000, 
raw observation next is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.87489343979703, 0.5971728161651401, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6562411199830859, 0.69905760538838, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7929906], dtype=float32), -0.6236124]. 
=============================================
[2019-04-03 22:36:49,395] A3C_AGENT_WORKER-Thread-8 INFO:Evaluating...
[2019-04-03 22:36:49,395] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:36:49,396] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:36:49,396] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:36:49,397] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:36:49,397] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:36:49,398] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:36:49,400] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run6
[2019-04-03 22:36:49,417] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run6
[2019-04-03 22:36:49,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run6
[2019-04-03 22:37:07,617] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.21776989], dtype=float32), 0.29300985]
[2019-04-03 22:37:07,617] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-10.4, 77.0, 0.0, 0.0, 26.0, 23.18334881779897, -0.117928794915944, 0.0, 1.0, 46660.28432119713]
[2019-04-03 22:37:07,617] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:37:07,617] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.0069404e-10 7.8359470e-03 7.3712134e-09 1.3448755e-06 4.0097203e-04
 1.7299043e-09 9.9176168e-01], sampled 0.8383522961119007
[2019-04-03 22:39:24,128] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.21776989], dtype=float32), 0.29300985]
[2019-04-03 22:39:24,128] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.7, 58.66666666666667, 21.66666666666667, 190.8333333333333, 26.0, 25.55714859405311, 0.4738464658221581, 1.0, 1.0, 158674.0045309217]
[2019-04-03 22:39:24,128] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-03 22:39:24,129] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.2828206e-12 1.1571273e-04 1.2051839e-11 3.8966252e-08 1.4117442e-05
 7.8384489e-13 9.9987018e-01], sampled 0.4128831272641784
[2019-04-03 22:39:48,494] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4196 239941874.3313 1605.3476
[2019-04-03 22:40:17,571] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.8260 263376537.7352 1553.3210
[2019-04-03 22:40:23,441] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.7642 275779513.5059 1232.6471
[2019-04-03 22:40:24,480] A3C_AGENT_WORKER-Thread-8 INFO:Global step: 500000, evaluation results [500000.0, 7241.826010784863, 263376537.73517773, 1553.321032361531, 7353.419646041243, 239941874.33133844, 1605.347596910568, 7182.764221400699, 275779513.50585467, 1232.6470856617902]
[2019-04-03 22:40:28,129] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6561755e-13 5.8486527e-05 2.2690913e-11 3.1989725e-08 1.4401756e-05
 3.7612432e-13 9.9992704e-01], sum to 1.0000
[2019-04-03 22:40:28,129] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1583
[2019-04-03 22:40:28,168] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.32106990493048, 0.4706594536055131, 0.0, 1.0, 57479.2358235124], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3451800.0000, 
sim time next is 3452400.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.35505619429851, 0.4753809142007255, 0.0, 1.0, 46989.38642364014], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6129213495248758, 0.6584603047335752, 0.0, 1.0, 0.22375898296971497], 
reward next is 0.7762, 
noisyNet noise sample is [array([1.1836249], dtype=float32), 0.7341133]. 
=============================================
[2019-04-03 22:40:28,229] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.2027424e-12 2.9764476e-04 2.3883796e-11 6.5695424e-08 4.3583354e-05
 2.5018285e-13 9.9965870e-01], sum to 1.0000
[2019-04-03 22:40:28,230] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1077
[2019-04-03 22:40:28,258] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 37.5, 338.0, 26.0, 26.55110607554683, 0.6863384516391955, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3517200.0000, 
sim time next is 3517800.0000, 
raw observation next is [2.833333333333333, 49.5, 29.33333333333333, 275.6666666666666, 26.0, 26.36457891814576, 0.6631091317956136, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.541089566020314, 0.495, 0.09777777777777776, 0.30460405156537745, 0.6666666666666666, 0.6970482431788133, 0.7210363772652045, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.86845005], dtype=float32), -0.36541322]. 
=============================================
[2019-04-03 22:40:31,401] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.8897389e-14 1.7647668e-05 5.6999522e-13 1.8057210e-09 1.8139714e-07
 2.8706116e-14 9.9998212e-01], sum to 1.0000
[2019-04-03 22:40:31,401] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7205
[2019-04-03 22:40:31,417] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 50.5, 115.0, 806.0, 26.0, 26.01858734534509, 0.6259134841332806, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3504600.0000, 
sim time next is 3505200.0000, 
raw observation next is [2.666666666666667, 50.0, 112.8333333333333, 801.8333333333334, 26.0, 25.5153431143055, 0.594440269936327, 1.0, 1.0, 18680.41167023055], 
processed observation next is [1.0, 0.5652173913043478, 0.5364727608494922, 0.5, 0.376111111111111, 0.8860036832412523, 0.6666666666666666, 0.6262785928587918, 0.6981467566454423, 1.0, 1.0, 0.08895434128681215], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.6134383], dtype=float32), -0.9393594]. 
=============================================
[2019-04-03 22:40:33,348] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6702406e-13 4.3419431e-04 7.2775588e-12 7.4620559e-09 3.0893254e-06
 9.5373572e-14 9.9956268e-01], sum to 1.0000
[2019-04-03 22:40:33,348] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4567
[2019-04-03 22:40:33,362] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 49.5, 110.6666666666667, 797.6666666666666, 26.0, 25.98956553244324, 0.644412193158524, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3505800.0000, 
sim time next is 3506400.0000, 
raw observation next is [3.0, 49.0, 108.5, 793.5, 26.0, 26.31153157025155, 0.666646790561705, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.3616666666666667, 0.8767955801104972, 0.6666666666666666, 0.6926276308542958, 0.7222155968539017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8645469], dtype=float32), 0.79395854]. 
=============================================
[2019-04-03 22:40:42,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1258312e-11 1.7577341e-03 5.5788735e-11 5.1839578e-08 1.7227100e-04
 3.7150338e-11 9.9806994e-01], sum to 1.0000
[2019-04-03 22:40:42,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8905
[2019-04-03 22:40:42,481] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 44.66666666666667, 0.0, 0.0, 26.0, 25.44716478977913, 0.3926915560734632, 0.0, 1.0, 57170.96803896352], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3619200.0000, 
sim time next is 3619800.0000, 
raw observation next is [-1.5, 46.0, 0.0, 0.0, 26.0, 25.41822427316493, 0.3913601890567307, 0.0, 1.0, 59799.85087491576], 
processed observation next is [0.0, 0.9130434782608695, 0.4210526315789474, 0.46, 0.0, 0.0, 0.6666666666666666, 0.6181853560970776, 0.6304533963522435, 0.0, 1.0, 0.28476119464245603], 
reward next is 0.7152, 
noisyNet noise sample is [array([0.7273178], dtype=float32), -1.2484875]. 
=============================================
[2019-04-03 22:40:48,460] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.8732888e-12 8.9429937e-05 5.3603660e-11 1.4253621e-08 1.0819423e-04
 4.9860979e-12 9.9980241e-01], sum to 1.0000
[2019-04-03 22:40:48,461] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1569
[2019-04-03 22:40:48,478] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 24.96788922064519, 0.3203569358535758, 0.0, 1.0, 23917.22087498418], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3697200.0000, 
sim time next is 3697800.0000, 
raw observation next is [3.833333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 24.95485123965786, 0.3164376957352332, 0.0, 1.0, 32581.71863045832], 
processed observation next is [0.0, 0.8260869565217391, 0.5687903970452447, 0.5966666666666667, 0.0, 0.0, 0.6666666666666666, 0.579570936638155, 0.6054792319117445, 0.0, 1.0, 0.15515104109742056], 
reward next is 0.8448, 
noisyNet noise sample is [array([-0.29992014], dtype=float32), 1.0301689]. 
=============================================
[2019-04-03 22:41:02,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.0155409e-12 2.6556859e-06 1.3795811e-11 3.4100950e-08 5.9545499e-07
 4.7484445e-13 9.9999666e-01], sum to 1.0000
[2019-04-03 22:41:02,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4687
[2019-04-03 22:41:03,023] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 26.0, 25.20351168954283, 0.4436208110084355, 0.0, 1.0, 20877.03038807977], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3873600.0000, 
sim time next is 3874200.0000, 
raw observation next is [0.6666666666666667, 52.5, 0.0, 0.0, 26.0, 25.12715634946045, 0.4413147172790832, 0.0, 1.0, 65800.72633635027], 
processed observation next is [1.0, 0.8695652173913043, 0.4810710987996307, 0.525, 0.0, 0.0, 0.6666666666666666, 0.5939296957883707, 0.6471049057596944, 0.0, 1.0, 0.3133367920778584], 
reward next is 0.6867, 
noisyNet noise sample is [array([-0.9779168], dtype=float32), 0.8191588]. 
=============================================
[2019-04-03 22:41:15,397] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4882772e-11 2.2312405e-03 8.3809387e-10 1.6344345e-07 3.7842907e-04
 5.4323580e-11 9.9739021e-01], sum to 1.0000
[2019-04-03 22:41:15,397] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0781
[2019-04-03 22:41:15,477] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.66666666666667, 56.33333333333333, 94.33333333333333, 486.3333333333333, 26.0, 26.11435087617321, 0.4727619206263241, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4005600.0000, 
sim time next is 4006200.0000, 
raw observation next is [-11.33333333333333, 54.66666666666667, 95.66666666666666, 528.6666666666667, 26.0, 26.25907882164697, 0.4852952858493607, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.14866112650046176, 0.5466666666666667, 0.31888888888888883, 0.5841620626151014, 0.6666666666666666, 0.6882565684705808, 0.6617650952831202, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44534522], dtype=float32), -0.6417407]. 
=============================================
[2019-04-03 22:41:16,833] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7922686e-12 9.1921123e-05 3.2977570e-12 3.4510862e-08 7.4077298e-06
 4.4607991e-13 9.9990070e-01], sum to 1.0000
[2019-04-03 22:41:16,836] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4037
[2019-04-03 22:41:16,900] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.166666666666666, 40.66666666666667, 114.3333333333333, 792.6666666666667, 26.0, 26.55444375753664, 0.5844407455009332, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4013400.0000, 
sim time next is 4014000.0000, 
raw observation next is [-8.0, 40.0, 115.5, 798.5, 26.0, 26.55372568638396, 0.5789127388764141, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.24099722991689754, 0.4, 0.385, 0.8823204419889503, 0.6666666666666666, 0.7128104738653299, 0.6929709129588048, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.27139446], dtype=float32), -0.008914157]. 
=============================================
[2019-04-03 22:41:16,921] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[83.443085]
 [83.38319 ]
 [83.20097 ]
 [83.050095]
 [82.95133 ]], R is [[83.63362885]
 [83.79729462]
 [83.95932007]
 [84.11972809]
 [84.27853394]].
[2019-04-03 22:41:42,630] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.9108264e-11 2.2298694e-03 5.0015642e-10 7.7757733e-08 6.0736715e-05
 1.1570526e-10 9.9770927e-01], sum to 1.0000
[2019-04-03 22:41:42,631] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2300
[2019-04-03 22:41:42,665] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.35516635469298, 0.3243307560762079, 0.0, 1.0, 39807.46443128338], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4257000.0000, 
sim time next is 4257600.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.34907396775345, 0.3233527724916394, 0.0, 1.0, 39413.3451343027], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6124228306461209, 0.6077842574972131, 0.0, 1.0, 0.1876825958776319], 
reward next is 0.8123, 
noisyNet noise sample is [array([-1.0271931], dtype=float32), 0.74665505]. 
=============================================
[2019-04-03 22:41:48,281] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3214562e-12 4.6788286e-05 1.9276289e-11 6.5507981e-09 7.9816027e-06
 2.0628111e-12 9.9994516e-01], sum to 1.0000
[2019-04-03 22:41:48,282] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9946
[2019-04-03 22:41:48,310] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.25, 74.5, 0.0, 0.0, 26.0, 25.5330812923055, 0.4099946900186339, 0.0, 1.0, 19306.39495275544], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4324200.0000, 
sim time next is 4324800.0000, 
raw observation next is [4.300000000000001, 74.0, 0.0, 0.0, 26.0, 25.57729045491084, 0.4210795077389792, 0.0, 1.0, 18742.09200348783], 
processed observation next is [1.0, 0.043478260869565216, 0.5817174515235458, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6314408712425701, 0.6403598359129931, 0.0, 1.0, 0.08924805715946585], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.7557654], dtype=float32), 0.29528967]. 
=============================================
[2019-04-03 22:41:48,531] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5094885e-11 1.4552416e-03 9.5587663e-11 2.7996288e-07 2.1951259e-03
 6.2351973e-12 9.9634922e-01], sum to 1.0000
[2019-04-03 22:41:48,532] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1101
[2019-04-03 22:41:48,544] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.2, 64.0, 0.0, 0.0, 26.0, 25.33682397749876, 0.3548104099056906, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4298400.0000, 
sim time next is 4299000.0000, 
raw observation next is [6.166666666666667, 64.83333333333334, 0.0, 0.0, 26.0, 25.26867563881269, 0.3386822169304834, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6334256694367498, 0.6483333333333334, 0.0, 0.0, 0.6666666666666666, 0.6057229699010577, 0.6128940723101611, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8826003], dtype=float32), 2.2211561]. 
=============================================
[2019-04-03 22:41:48,548] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.7615  ]
 [83.92351 ]
 [84.42234 ]
 [84.988686]
 [85.63449 ]], R is [[82.2853241 ]
 [82.46247101]
 [82.6378479 ]
 [82.81147003]
 [82.98335266]].
[2019-04-03 22:41:53,486] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.8279638e-13 1.2632906e-04 5.9285674e-11 2.5127076e-08 4.9985247e-05
 5.9195361e-12 9.9982375e-01], sum to 1.0000
[2019-04-03 22:41:53,487] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2738
[2019-04-03 22:41:53,499] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.966666666666667, 70.66666666666667, 0.0, 0.0, 26.0, 25.62504619849974, 0.4146767170712226, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4332000.0000, 
sim time next is 4332600.0000, 
raw observation next is [3.95, 70.5, 0.0, 0.0, 26.0, 25.66489266540983, 0.4037470706167876, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.57202216066482, 0.705, 0.0, 0.0, 0.6666666666666666, 0.6387410554508192, 0.6345823568722625, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06193685], dtype=float32), 0.6034715]. 
=============================================
[2019-04-03 22:41:57,892] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3588299e-12 6.2164571e-04 1.5276665e-10 4.3944777e-08 7.3852498e-05
 2.1580883e-11 9.9930441e-01], sum to 1.0000
[2019-04-03 22:41:57,893] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1562
[2019-04-03 22:41:57,926] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.22008219178851, 0.3922061957712379, 0.0, 1.0, 40636.10870427138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4516800.0000, 
sim time next is 4517400.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.26330770058906, 0.3934558987696254, 0.0, 1.0, 40542.33755761449], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 0.6666666666666666, 0.605275641715755, 0.6311519662565418, 0.0, 1.0, 0.1930587502743547], 
reward next is 0.8069, 
noisyNet noise sample is [array([0.8245494], dtype=float32), -0.35432392]. 
=============================================
[2019-04-03 22:42:15,677] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3714996e-10 3.5401564e-02 1.1190982e-08 1.8934386e-06 4.2005549e-03
 2.2226814e-09 9.6039593e-01], sum to 1.0000
[2019-04-03 22:42:15,678] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9200
[2019-04-03 22:42:15,702] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666667, 92.83333333333333, 0.0, 0.0, 26.0, 23.91535900343071, 0.1037318918238916, 0.0, 1.0, 41870.97826331233], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4777800.0000, 
sim time next is 4778400.0000, 
raw observation next is [-6.133333333333334, 92.66666666666667, 0.0, 0.0, 26.0, 23.88456037210125, 0.09691253262898564, 0.0, 1.0, 41899.5333149286], 
processed observation next is [0.0, 0.30434782608695654, 0.2927054478301016, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.49038003100843763, 0.5323041775429952, 0.0, 1.0, 0.19952158721394572], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.5518291], dtype=float32), 0.5266368]. 
=============================================
[2019-04-03 22:42:16,769] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.06208185e-11 2.15019754e-04 8.69459366e-11 6.49704788e-08
 8.74830948e-05 7.38806802e-13 9.99697447e-01], sum to 1.0000
[2019-04-03 22:42:16,769] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4186
[2019-04-03 22:42:16,788] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 24.33333333333334, 0.0, 0.0, 26.0, 27.11195916917418, 0.7913526164719119, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4990800.0000, 
sim time next is 4991400.0000, 
raw observation next is [6.0, 24.0, 0.0, 0.0, 26.0, 27.07382220938915, 0.6567655524276049, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.24, 0.0, 0.0, 0.6666666666666666, 0.7561518507824291, 0.7189218508092017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0872717], dtype=float32), 0.7249264]. 
=============================================
[2019-04-03 22:42:18,192] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.9522318e-11 1.1444339e-03 6.3410949e-10 2.3485347e-07 1.4391768e-03
 1.5728658e-11 9.9741614e-01], sum to 1.0000
[2019-04-03 22:42:18,192] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0072
[2019-04-03 22:42:18,219] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 65.0, 71.66666666666666, 245.0, 26.0, 25.58896378885387, 0.4230025070129811, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5039400.0000, 
sim time next is 5040000.0000, 
raw observation next is [-2.0, 65.0, 78.0, 317.0, 26.0, 25.58487790860521, 0.4292187367513505, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.65, 0.26, 0.35027624309392263, 0.6666666666666666, 0.6320731590504343, 0.6430729122504502, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.91997206], dtype=float32), 1.0330828]. 
=============================================
[2019-04-03 22:42:18,224] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[85.79299 ]
 [82.83501 ]
 [83.4766  ]
 [80.09506 ]
 [75.765724]], R is [[87.79852295]
 [87.92053986]
 [88.04133606]
 [88.16092682]
 [88.27931976]].
[2019-04-03 22:42:22,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:22,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:22,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run5
[2019-04-03 22:42:22,847] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:22,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:22,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run5
[2019-04-03 22:42:23,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:23,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:23,306] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run5
[2019-04-03 22:42:24,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:24,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:24,958] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run5
[2019-04-03 22:42:29,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:29,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:29,243] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run5
[2019-04-03 22:42:31,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:31,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:31,428] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run5
[2019-04-03 22:42:31,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:31,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:31,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run5
[2019-04-03 22:42:32,350] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.8210265e-13 4.6484598e-05 5.0183226e-11 2.1484093e-08 3.7998936e-05
 6.4140624e-12 9.9991548e-01], sum to 1.0000
[2019-04-03 22:42:32,350] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6339
[2019-04-03 22:42:32,386] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 54.16666666666667, 0.0, 0.0, 26.0, 25.36043055260692, 0.3892778403437605, 0.0, 1.0, 63253.78552055294], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5026200.0000, 
sim time next is 5026800.0000, 
raw observation next is [-1.0, 53.33333333333334, 0.0, 0.0, 26.0, 25.3349651927617, 0.3964859972721874, 0.0, 1.0, 48473.7438084036], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.5333333333333334, 0.0, 0.0, 0.6666666666666666, 0.6112470993968083, 0.6321619990907291, 0.0, 1.0, 0.23082735146858854], 
reward next is 0.7692, 
noisyNet noise sample is [array([-0.3133517], dtype=float32), -1.2116688]. 
=============================================
[2019-04-03 22:42:32,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:32,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:32,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run5
[2019-04-03 22:42:32,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:32,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:32,954] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run5
[2019-04-03 22:42:33,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:33,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:33,724] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run5
[2019-04-03 22:42:33,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:33,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:33,856] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run5
[2019-04-03 22:42:35,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:35,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:35,716] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run5
[2019-04-03 22:42:36,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:36,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:36,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run5
[2019-04-03 22:42:37,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:37,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:37,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run5
[2019-04-03 22:42:37,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:37,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:37,907] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run5
[2019-04-03 22:42:41,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-03 22:42:41,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:42:41,662] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run5
[2019-04-03 22:42:49,540] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3321597e-11 3.5636124e-04 1.7326008e-09 5.1285849e-07 3.3421340e-04
 3.8213233e-10 9.9930882e-01], sum to 1.0000
[2019-04-03 22:42:49,540] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8911
[2019-04-03 22:42:49,582] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.71178837462841, -0.2542553896173845, 0.0, 1.0, 44953.25353150383], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 195000.0000, 
sim time next is 195600.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.65610158040396, -0.2598757296748834, 0.0, 1.0, 44957.35091117842], 
processed observation next is [1.0, 0.2608695652173913, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.38800846503366326, 0.4133747567750388, 0.0, 1.0, 0.21408262338656392], 
reward next is 0.7859, 
noisyNet noise sample is [array([0.1453601], dtype=float32), 0.20052657]. 
=============================================
[2019-04-03 22:42:50,789] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3722735e-13 7.6804041e-05 2.9809065e-11 1.4199278e-08 1.7292623e-05
 1.3688019e-11 9.9990594e-01], sum to 1.0000
[2019-04-03 22:42:50,789] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1575
[2019-04-03 22:42:50,842] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.683333333333334, 85.33333333333334, 12.0, 0.0, 26.0, 24.54705520768697, 0.1885587474897131, 0.0, 1.0, 34866.34361374741], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 60600.0000, 
sim time next is 61200.0000, 
raw observation next is [5.5, 86.0, 0.0, 0.0, 26.0, 24.54550005086362, 0.1942964111072328, 0.0, 1.0, 42213.77513613884], 
processed observation next is [0.0, 0.7391304347826086, 0.6149584487534627, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5454583375719683, 0.5647654703690775, 0.0, 1.0, 0.20101797683875636], 
reward next is 0.7990, 
noisyNet noise sample is [array([-0.0574689], dtype=float32), 1.1266402]. 
=============================================
[2019-04-03 22:43:08,300] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.4515496e-13 5.3107278e-06 3.1947341e-11 3.2778566e-08 3.3700526e-06
 4.1711193e-13 9.9999130e-01], sum to 1.0000
[2019-04-03 22:43:08,301] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5535
[2019-04-03 22:43:08,356] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.9345717789821, 0.254527360551221, 0.0, 1.0, 50351.05393597209], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 246600.0000, 
sim time next is 247200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.89164812783088, 0.2442620471577862, 0.0, 1.0, 46740.79993686373], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5743040106525733, 0.5814206823859287, 0.0, 1.0, 0.2225752377945892], 
reward next is 0.7774, 
noisyNet noise sample is [array([-2.2037544], dtype=float32), 1.2973809]. 
=============================================
[2019-04-03 22:43:27,891] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1083210e-13 4.3805339e-06 3.3466351e-11 2.9776577e-09 6.2517092e-06
 1.7006178e-11 9.9998939e-01], sum to 1.0000
[2019-04-03 22:43:27,891] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2068
[2019-04-03 22:43:27,940] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 87.0, 0.0, 0.0, 26.0, 24.94341702800877, 0.289773327911177, 0.0, 1.0, 56483.26170906942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 581400.0000, 
sim time next is 582000.0000, 
raw observation next is [-2.1, 87.0, 0.0, 0.0, 26.0, 24.95242024973768, 0.2902364223350555, 0.0, 1.0, 45156.56541811807], 
processed observation next is [0.0, 0.7391304347826086, 0.404432132963989, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5793683541448068, 0.5967454741116852, 0.0, 1.0, 0.21503126389580035], 
reward next is 0.7850, 
noisyNet noise sample is [array([0.9309318], dtype=float32), -0.10871811]. 
=============================================
[2019-04-03 22:43:27,960] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.84101]
 [81.30531]
 [80.98277]
 [81.04219]
 [80.86535]], R is [[82.40362549]
 [82.31062317]
 [82.25906372]
 [82.28154755]
 [82.33310699]].
[2019-04-03 22:43:28,761] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.7957764e-15 1.5458731e-07 4.7882217e-13 7.8635576e-10 1.2151901e-07
 2.2694247e-15 9.9999976e-01], sum to 1.0000
[2019-04-03 22:43:28,761] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3722
[2019-04-03 22:43:28,823] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 24.72626913955947, 0.1784726496004304, 1.0, 1.0, 102230.3681227753], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 502200.0000, 
sim time next is 502800.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 24.66555811382598, 0.2037195575255476, 1.0, 1.0, 108768.6452053885], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.6666666666666666, 0.555463176152165, 0.5679065191751825, 1.0, 1.0, 0.517945929549469], 
reward next is 0.4821, 
noisyNet noise sample is [array([-1.5754589], dtype=float32), 1.2826862]. 
=============================================
[2019-04-03 22:43:31,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.2072964e-12 5.4152304e-04 3.5738540e-10 3.7517339e-08 5.4763368e-05
 6.6862529e-11 9.9940360e-01], sum to 1.0000
[2019-04-03 22:43:31,203] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1402
[2019-04-03 22:43:31,239] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 65.5, 0.0, 0.0, 26.0, 23.7124307865888, -0.01235491610844162, 0.0, 1.0, 43985.30666043075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 625800.0000, 
sim time next is 626400.0000, 
raw observation next is [-4.5, 65.0, 0.0, 0.0, 26.0, 23.72189753451286, -0.01661821261597634, 0.0, 1.0, 43895.12979217836], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.65, 0.0, 0.0, 0.6666666666666666, 0.47682479454273824, 0.49446059579467455, 0.0, 1.0, 0.20902442758180173], 
reward next is 0.7910, 
noisyNet noise sample is [array([-0.8138195], dtype=float32), 1.2214686]. 
=============================================
[2019-04-03 22:43:34,696] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6259237e-15 1.1291840e-07 2.2104550e-13 2.5685920e-10 1.1032127e-07
 6.6453466e-15 9.9999976e-01], sum to 1.0000
[2019-04-03 22:43:34,700] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7187
[2019-04-03 22:43:34,724] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 96.0, 0.0, 0.0, 26.0, 24.83589931883902, 0.2335312388634035, 0.0, 1.0, 40390.96628448059], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 514800.0000, 
sim time next is 515400.0000, 
raw observation next is [3.383333333333333, 96.16666666666666, 0.0, 0.0, 26.0, 24.83846333596739, 0.2335783803970969, 0.0, 1.0, 40247.04249967411], 
processed observation next is [1.0, 1.0, 0.5563250230840259, 0.9616666666666666, 0.0, 0.0, 0.6666666666666666, 0.5698719446639492, 0.5778594601323657, 0.0, 1.0, 0.19165258333178148], 
reward next is 0.8083, 
noisyNet noise sample is [array([0.3355669], dtype=float32), -2.4195917]. 
=============================================
[2019-04-03 22:43:38,818] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6684136e-12 2.3614604e-05 4.4491133e-11 3.0418413e-08 7.7073106e-05
 9.3129792e-12 9.9989927e-01], sum to 1.0000
[2019-04-03 22:43:38,820] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0790
[2019-04-03 22:43:38,851] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.95534319523085, 0.2057362213578684, 0.0, 1.0, 42391.46497055359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 680400.0000, 
sim time next is 681000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.93218653322971, 0.1983825815039489, 0.0, 1.0, 42323.9410842012], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5776822111024759, 0.566127527167983, 0.0, 1.0, 0.2015425765914343], 
reward next is 0.7985, 
noisyNet noise sample is [array([1.6367769], dtype=float32), 0.81206465]. 
=============================================
[2019-04-03 22:43:38,887] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.52597]
 [80.60134]
 [80.69782]
 [80.76401]
 [80.89075]], R is [[80.41208649]
 [80.40609741]
 [80.39987183]
 [80.39356995]
 [80.38611603]].
[2019-04-03 22:43:46,528] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4660516e-11 1.7045445e-03 8.0772500e-10 1.5973202e-07 1.0299688e-03
 4.0741355e-10 9.9726534e-01], sum to 1.0000
[2019-04-03 22:43:46,551] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3847
[2019-04-03 22:43:46,565] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 23.80190521137781, 0.006288519104980318, 0.0, 1.0, 44395.75759693926], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 622800.0000, 
sim time next is 623400.0000, 
raw observation next is [-4.5, 67.5, 0.0, 0.0, 26.0, 23.77562291684596, 0.004009932870546361, 0.0, 1.0, 44320.82560163779], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.675, 0.0, 0.0, 0.6666666666666666, 0.4813019097371634, 0.5013366442901821, 0.0, 1.0, 0.2110515504839895], 
reward next is 0.7889, 
noisyNet noise sample is [array([0.06912275], dtype=float32), 0.018857181]. 
=============================================
[2019-04-03 22:43:56,917] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1319617e-13 1.2692493e-05 2.0430287e-12 7.8401410e-09 9.9973922e-06
 4.1552411e-14 9.9997723e-01], sum to 1.0000
[2019-04-03 22:43:56,917] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4120
[2019-04-03 22:43:56,934] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 47.0, 82.5, 372.5, 26.0, 25.94479421960897, 0.4267617651475511, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 745200.0000, 
sim time next is 745800.0000, 
raw observation next is [-0.09999999999999999, 46.66666666666667, 83.33333333333333, 258.6666666666666, 26.0, 25.86579965471508, 0.276112395169583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4598337950138504, 0.46666666666666673, 0.27777777777777773, 0.28581952117863707, 0.6666666666666666, 0.6554833045595899, 0.5920374650565277, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05566621], dtype=float32), -0.39804634]. 
=============================================
[2019-04-03 22:44:07,160] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.2928951e-14 2.7273226e-04 1.0957882e-12 4.5628901e-10 4.0341511e-06
 2.7464352e-14 9.9972326e-01], sum to 1.0000
[2019-04-03 22:44:07,161] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1450
[2019-04-03 22:44:07,185] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 91.33333333333333, 0.0, 0.0, 26.0, 25.31538461331616, 0.4212413689741921, 0.0, 1.0, 38034.58644570113], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 952800.0000, 
sim time next is 953400.0000, 
raw observation next is [5.416666666666667, 90.16666666666667, 0.0, 0.0, 26.0, 25.31646274046922, 0.4205714182111109, 0.0, 1.0, 38043.11356783254], 
processed observation next is [1.0, 0.0, 0.6126500461680519, 0.9016666666666667, 0.0, 0.0, 0.6666666666666666, 0.609705228372435, 0.640190472737037, 0.0, 1.0, 0.1811576836563454], 
reward next is 0.8188, 
noisyNet noise sample is [array([-0.43893218], dtype=float32), 0.37962052]. 
=============================================
[2019-04-03 22:44:14,277] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.0860542e-13 1.6217816e-04 1.7644835e-11 1.8140360e-08 1.0087085e-04
 5.0338444e-12 9.9973696e-01], sum to 1.0000
[2019-04-03 22:44:14,282] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8860
[2019-04-03 22:44:14,291] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 86.0, 0.0, 26.0, 24.77696554066742, 0.448875753451608, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1260000.0000, 
sim time next is 1260600.0000, 
raw observation next is [13.8, 100.0, 83.0, 0.0, 26.0, 24.76189946286332, 0.4466410715553518, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.27666666666666667, 0.0, 0.6666666666666666, 0.5634916219052766, 0.6488803571851173, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34732744], dtype=float32), -0.72548085]. 
=============================================
[2019-04-03 22:44:15,329] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5744547e-14 1.3091051e-03 6.3970990e-12 6.2367627e-09 5.5553692e-06
 2.0746593e-13 9.9868530e-01], sum to 1.0000
[2019-04-03 22:44:15,329] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9683
[2019-04-03 22:44:15,341] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 26.0, 25.7354152610196, 0.5783831428259015, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1059600.0000, 
sim time next is 1060200.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 26.0, 25.66772719070872, 0.5789715453115704, 0.0, 1.0, 61962.83648896827], 
processed observation next is [1.0, 0.2608695652173913, 0.8310249307479226, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6389772658923935, 0.6929905151038568, 0.0, 1.0, 0.29506112613794416], 
reward next is 0.7049, 
noisyNet noise sample is [array([0.3782236], dtype=float32), -1.2558452]. 
=============================================
[2019-04-03 22:44:20,659] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4821581e-12 4.1220261e-04 4.6624687e-11 6.1999117e-08 1.3109426e-05
 2.4483452e-11 9.9957460e-01], sum to 1.0000
[2019-04-03 22:44:20,665] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3414
[2019-04-03 22:44:20,673] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.46666666666667, 64.33333333333334, 88.0, 0.0, 26.0, 25.05931077393005, 0.4921197692192287, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1178400.0000, 
sim time next is 1179000.0000, 
raw observation next is [18.55, 64.0, 80.0, 0.0, 26.0, 25.045771035041, 0.488817314077735, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.976454293628809, 0.64, 0.26666666666666666, 0.0, 0.6666666666666666, 0.5871475862534167, 0.6629391046925783, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9725016], dtype=float32), 0.41646072]. 
=============================================
[2019-04-03 22:44:20,685] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.4785 ]
 [84.60979]
 [84.76346]
 [84.93777]
 [85.11419]], R is [[84.5260849 ]
 [84.68082428]
 [84.83401489]
 [84.985672  ]
 [85.13581848]].
[2019-04-03 22:44:21,192] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6485339e-15 3.2899436e-05 9.8332388e-13 1.6418424e-09 7.6601879e-07
 7.5434294e-14 9.9996638e-01], sum to 1.0000
[2019-04-03 22:44:21,195] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2578
[2019-04-03 22:44:21,240] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 9.0, 0.0, 26.0, 25.41889772240422, 0.4415864414770851, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1411200.0000, 
sim time next is 1411800.0000, 
raw observation next is [-0.6, 100.0, 12.0, 0.0, 26.0, 25.36559653900025, 0.428119985412359, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.04, 0.0, 0.6666666666666666, 0.6137997115833542, 0.6427066618041196, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6125834], dtype=float32), 0.03930848]. 
=============================================
[2019-04-03 22:44:21,436] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7523658e-14 3.4823253e-05 3.9645622e-12 5.2957967e-09 3.3614018e-05
 3.8260481e-13 9.9993157e-01], sum to 1.0000
[2019-04-03 22:44:21,439] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0131
[2019-04-03 22:44:21,452] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.68333333333333, 76.0, 0.0, 0.0, 26.0, 25.64900218269987, 0.6424397943303458, 0.0, 1.0, 34483.30313114921], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1126200.0000, 
sim time next is 1126800.0000, 
raw observation next is [10.5, 77.0, 0.0, 0.0, 26.0, 25.65036193176282, 0.6422468373032667, 0.0, 1.0, 27061.03723463996], 
processed observation next is [0.0, 0.043478260869565216, 0.7534626038781165, 0.77, 0.0, 0.0, 0.6666666666666666, 0.637530160980235, 0.7140822791010889, 0.0, 1.0, 0.12886208206971408], 
reward next is 0.8711, 
noisyNet noise sample is [array([0.4197989], dtype=float32), -0.420065]. 
=============================================
[2019-04-03 22:44:23,651] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4785521e-15 2.7851783e-07 2.6959262e-13 6.6726798e-11 1.2785999e-07
 3.4361372e-15 9.9999964e-01], sum to 1.0000
[2019-04-03 22:44:23,651] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9098
[2019-04-03 22:44:23,669] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666667, 94.0, 95.0, 0.0, 26.0, 25.71778115595882, 0.474072985260627, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1426800.0000, 
sim time next is 1427400.0000, 
raw observation next is [0.25, 93.5, 96.0, 0.0, 26.0, 25.66790112912408, 0.4457885290068311, 1.0, 1.0, 18680.8043927615], 
processed observation next is [1.0, 0.5217391304347826, 0.46952908587257625, 0.935, 0.32, 0.0, 0.6666666666666666, 0.6389917607603399, 0.6485961763356104, 1.0, 1.0, 0.08895621139410238], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.9000891], dtype=float32), 0.24195167]. 
=============================================
[2019-04-03 22:44:26,439] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2398935e-12 1.9897483e-04 6.4643443e-11 3.8102353e-08 1.3665894e-05
 9.2076451e-12 9.9978727e-01], sum to 1.0000
[2019-04-03 22:44:26,439] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6716
[2019-04-03 22:44:26,443] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 100.0, 76.0, 0.0, 26.0, 23.30472534653298, 0.1230740888890166, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1245600.0000, 
sim time next is 1246200.0000, 
raw observation next is [14.9, 100.0, 76.66666666666667, 0.0, 26.0, 23.30085875325023, 0.1224842962833061, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.8753462603878118, 1.0, 0.2555555555555556, 0.0, 0.6666666666666666, 0.441738229437519, 0.540828098761102, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15585455], dtype=float32), -0.81832904]. 
=============================================
[2019-04-03 22:44:31,717] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.0758167e-15 5.2386739e-08 2.1292894e-12 5.3674643e-10 1.5257957e-07
 5.1374900e-14 9.9999988e-01], sum to 1.0000
[2019-04-03 22:44:31,717] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0665
[2019-04-03 22:44:31,762] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.41128669834539, 0.4829851146447678, 0.0, 1.0, 59419.24187177058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1386000.0000, 
sim time next is 1386600.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.30777632944009, 0.4852049479551219, 0.0, 1.0, 72306.19909039894], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6089813607866743, 0.6617349826517073, 0.0, 1.0, 0.34431523376380446], 
reward next is 0.6557, 
noisyNet noise sample is [array([0.27431196], dtype=float32), 0.7364369]. 
=============================================
[2019-04-03 22:44:33,674] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1179889e-14 1.6836668e-06 1.6196080e-12 4.0680055e-09 1.0602402e-06
 3.4923331e-14 9.9999726e-01], sum to 1.0000
[2019-04-03 22:44:33,674] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8683
[2019-04-03 22:44:33,751] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.4166666666666667, 92.5, 94.0, 0.0, 26.0, 25.31756444656965, 0.3170492065348053, 1.0, 1.0, 16605.69518837373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1428600.0000, 
sim time next is 1429200.0000, 
raw observation next is [0.5, 92.0, 93.0, 0.0, 26.0, 24.93872140415865, 0.413037931625526, 1.0, 1.0, 169883.6876283516], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.92, 0.31, 0.0, 0.6666666666666666, 0.5782267836798874, 0.637679310541842, 1.0, 1.0, 0.8089699410873885], 
reward next is 0.1910, 
noisyNet noise sample is [array([-0.40637022], dtype=float32), -0.37469074]. 
=============================================
[2019-04-03 22:44:48,784] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.5592412e-11 3.7417546e-04 1.1308584e-09 5.8027115e-07 4.5247674e-05
 3.4705411e-10 9.9957997e-01], sum to 1.0000
[2019-04-03 22:44:48,784] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5695
[2019-04-03 22:44:48,795] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.616666666666667, 78.16666666666666, 0.0, 0.0, 26.0, 23.41869652185456, -0.06698592155754672, 0.0, 1.0, 47109.3492635877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1839000.0000, 
sim time next is 1839600.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 23.38500275954588, -0.07418306208310112, 0.0, 1.0, 47142.55689489856], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4487502299621567, 0.47527231263896624, 0.0, 1.0, 0.22448836616618362], 
reward next is 0.7755, 
noisyNet noise sample is [array([1.3384999], dtype=float32), -0.6414938]. 
=============================================
[2019-04-03 22:45:16,937] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.3558083e-13 3.4896388e-05 7.6505954e-11 1.9979968e-08 3.0995423e-07
 4.2333021e-12 9.9996471e-01], sum to 1.0000
[2019-04-03 22:45:16,937] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9872
[2019-04-03 22:45:16,951] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.33002248514952, 0.1566966284545257, 0.0, 1.0, 43830.69704096475], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2087400.0000, 
sim time next is 2088000.0000, 
raw observation next is [-5.6, 91.0, 0.0, 0.0, 26.0, 24.41168876910437, 0.1649026159851507, 0.0, 1.0, 43575.6144060193], 
processed observation next is [1.0, 0.17391304347826086, 0.30747922437673136, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5343073974253642, 0.5549675386617169, 0.0, 1.0, 0.20750292574294904], 
reward next is 0.7925, 
noisyNet noise sample is [array([0.16712879], dtype=float32), 0.26833254]. 
=============================================
[2019-04-03 22:45:16,955] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.80988]
 [76.89298]
 [76.97713]
 [77.05712]
 [77.13418]], R is [[76.77355957]
 [76.79710388]
 [76.82221985]
 [76.84745026]
 [76.87277985]].
[2019-04-03 22:45:18,044] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-03 22:45:18,050] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-03 22:45:18,050] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:45:18,052] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run7
[2019-04-03 22:45:18,069] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-03 22:45:18,070] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-03 22:45:18,071] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:45:18,071] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-03 22:45:18,073] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run7
[2019-04-03 22:45:18,085] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/3/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run7
